[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"\nFigure 0.1: Telling stories data\nbook help tell stories data. establishes foundation can build share knowledge aspect world interest based data observe. Telling stories small groups around fire played critical role development humans society (Wiessner 2014). Today stories, based data, can influence millions.book explore, prod, push, manipulate, knead, ultimately, try understand implications data. variety features drive choices book.motto university took PhD naturam primum cognoscere rerum roughly ‘learn first nature things’. original quote continues temporis aeterni quoniam, roughly ‘eternal time’. things. focus tools, approaches, workflows enable establish lasting reproducible knowledge.talk data book, typically related humans. Humans center stories, tell social, cultural, economic stories. particular, throughout book draw attention inequity social phenomena data. data analysis reflects world . Many least well-face double burden regard: disadvantaged, extent difficult measure. Respecting whose data dataset primary concern, thinking systematically dataset.data often specific various contexts disciplines, approaches used understand tend similar. Data also increasingly global resources opportunities available variety sources. Hence, draw examples many disciplines geographies.become knowledge, findings must communicated , understood, trusted people. Scientific economic progress can made building work others. possible can understand . Similarly, create knowledge world, must enable others understand precisely , found, went tasks. , book particularly prescriptive communication reproducibility.Improving quality quantitative work enormous challenge, yet challenge time. Data around us, little enduring knowledge created. book hopes contribute, small way, changing .","code":""},{"path":"index.html","id":"audience-and-assumed-background","chapter":"Preface","heading":"Audience and assumed background","text":"typical person reading book familiarity first-year statistics, instance run regression. targeted particular level, instead providing aspects relevant almost quantitative course. taught book high school, undergraduate, graduate, professional, levels. Everyone unique needs, hopefully aspect book speaks .Enthusiasm interest taken folks far. , worry much else. successful students quantitative coding background.book covers lot ground, go depth particular aspect. book especially complements books : Data Science: First Introduction (T.-. Timbers, Campbell, Lee 2022), R Data Science (Wickham Grolemund 2017), Introduction Statistical Learning (James et al. 2017), Statistical Rethinking (McElreath 2020), Causal Inference: Mixtape (Cunningham 2021), Effect: Introduction Research Design Causality (Huntington-Klein 2021), Building Software Together (Wilson 2021). interested books, might good one start .","code":""},{"path":"index.html","id":"structure-and-content","chapter":"Preface","heading":"Structure and content","text":"book structured around six parts: ) Foundations, II) Communication, III) Acquisition, IV) Preparation, V) Modelling, VI) Enrichment.Part – Foundations – begins Chapter 1, provides overview trying achieve book read . Chapter 2 provides worked examples. intention can experience full workflow recommended book without worrying much specifics happening. workflow : plan, simulate, acquire, model, communicate. normal follow everything chapter, go , typing executing code . time read one chapter book, recommend one. Chapter 3 goes essential tasks R, statistical programming language used book. reference chapter, may find returning time time. Chapter 4 introduces key tools reproducibility used workflow advocate. things like using command line, R Markdown, R Projects, Git GitHub, using R practice.Part II – Communication – considers three types communication: written, static, interactive. Chapter 5 details features quantitative writing go writing crisp, technical, paper. Static communication Chapter 6 introduces features like graphs, tables, maps. Interactive communication Chapter 7 covers aspects websites, web applications, maps can manipulated.Part III – Acquisition – focuses three aspects: farming data, gathering data, hunting data. Farming data Chapter 8 begins essential concepts sampling govern approach data. focuses datasets explicitly provided us use data, instance censuses government statistics. typically clean, pre-packaged datasets, sometimes complete. Gathering data Chapter 9 covers things like using Application Programming Interface (APIs), scraping data, getting data PDFs, Optical Character Recognition (OCR). idea data available, necessarily designed datasets, must go get . Finally, hunting data Chapter 10 covers aspects expected us. instance, may need conduct experiment, run /B test, surveys.Part IV – Preparation – covers respectfully transform raw data something can explored shared. Chapter 11 begins detailing principles follow approaching task cleaning preparing data, goes specific steps take checks implement. Chapter 12 focuses methods storing retrieving datasets, including use R packages, continues onto considerations steps take wanting disseminate datasets broadly possible, time respecting whose data based .Part V – Modelling – begins exploratory data analysis Chapter 13. critical process coming understand nature dataset, something typically finds final product. Chapter 14 use statistical models explore data introduced. Chapter 15 first three applications modelling. focuses attempts make causal claims observational data covers approaches difference--differences, regression discontinuity, instrumental variables. Chapter 16 second modelling applications chapters focuses multilevel regression post-stratification use statistical model adjust sample known biases. Chapter 17 third final modelling application focused text--data.Part VI – Enrichment – introduces various next steps improve aspects workflow approaches introduced previous chapters. Chapter 18 goes moving away computer toward using cloud discusses deploying models use packages, web applications, APIs. Chapter 19 discusses various alternatives storage data including feather SQL; also covers ways improve performance code. Finally, Chapter 20 offers concluding remarks, details open problems, suggests next steps.","code":""},{"path":"index.html","id":"pedagogy-and-key-features","chapter":"Preface","heading":"Pedagogy and key features","text":"work. actively go material code . S. King (2000) says ‘[]mateurs sit wait inspiration, rest us just get go work’. passively read book. role best described Hamming (1996, 2–3):, , coach. run mile ; best can discuss styles criticize . know must run mile athletics course benefit —hence must think carefully hear read book effective changing —must obviously purpose…book structured around dense 12-week course. provides enough material advanced readers challenged, establishing core readers master. Typically courses cover material Chapter 14, pick another couple chapters particular interest.early Chapter 2 workflow—plan, simulate, acquire, model, communicate—allowing tell convincing story data. subsequent chapter add aspects depth workflow allow speak increasing sophistication credibility. workflow expands addresses skills typically sought industry. instance, features : communication, ethics, reproducibility, research question development, data collection, data cleaning, data protection dissemination, exploratory data analysis, statistical modelling, scaling.One defining aspects book ethics inequity concerns integrated throughout, rather clustered one, easily ignorable, chapter. aspects critical, yet can difficult immediately see value, hence tight integration.book also designed enable build portfolio work show potential employer. want industry job, arguably important thing . E. Robinson Nolis (2020, 55) describe portfolio collection projects show can something can help successful job search.novel Last Samurai (DeWitt 2000, 326), character says:[] scholar able look word passage instantly think another passage occurred; … [] text like pack icebergs word snowy peak huge frozen mass cross-references beneath surface.analogous way, book provides text instruction self-contained, also helps develop critical masses knowledge expertise built. chapter positions last word, instead written relation work.chapter following features:list required materials go read chapter. clear, first read material return book. chapter also contains recommended materials particularly interested topic want starting place exploration.summary key concepts skills developed chapter. Technical chapters additionally contain list main packages functions used chapter. combination features acts checklist learning, return completing chapter.series short exercises complete going required materials, going chapter, test knowledge. completing chapter, go back exercises make sure understand aspect.One two tutorial questions included end chapter encourage actively engage material. consider forming small groups discuss answers questions.chapters additionally feature:section called ‘Oh, think good data !’ focuses particular setting, cause death, often assumed unimpeachable unambiguous data reality tends quite far .section called ‘Shoulders giants’, focuses created intellectual foundation build.Finally, set six papers included Appendix B. write , conducting original research topic interest . Although open-ended research may new , extent able : develop questions, use quantitative methods explore , communicates findings, measure success book.","code":""},{"path":"index.html","id":"software-information-and-conventions","chapter":"Preface","heading":"Software information and conventions","text":"software use book R (R Core Team 2021). language chosen open source, widely used, general enough cover entire workflow, yet specific enough plenty well-developed features. assume used R , another reason selecting R book community R users. community especially welcoming new-comers lot complementary beginner-friendly material available. R package, DoSSToolkit (R. Alexander et al. 2021), contains learnr modules (Schloerke et al. 2021). may useful newer R especially complementary book.programming language, R great one start . preferred programming language already, wouldn’t hurt pick R well. said, good reason prefer another open-source programming language (instance use Python daily work) may wish stick . However, examples book R.Please download R R Studio onto computer. can download R free : http://cran.utstat.utoronto.ca/, can download R Studio Desktop free : https://rstudio.com/products/rstudio/download/#download.\nPlease also create account R Studio Cloud: https://rstudio.cloud/. allow run R cloud.Packages typewriter text, instance, tidyverse, functions also typewriter text, include brackets, instance dplyr::filter().","code":""},{"path":"index.html","id":"acknowledgments","chapter":"Preface","heading":"Acknowledgments","text":"Many people generously gave code, data, examples, guidance, opportunities, thoughts, time, helped develop book.Thank David Grubbs team CRC Press taking chance providing invaluable support.Thank Michael Chong Sharla Gelfand greatly helping shape approaches advocate. However, much contribute enormous way spirit generosity characterizes R community.Thank Kelly Lyons support, guidance, mentorship, friendship. Every day demonstrates academic , broadly, aspire person.Thank Greg Wilson providing structure think teaching, catalyst book, helpful comments drafts. Every day provides example contribute intellectual community.Thank anonymous reviewer Isabella Ghement, thoroughly went early draft book provided detailed feedback improved book.Thank Hareem Naveed helpful feedback encouragement. industry experience invaluable resource grappled questions coverage focus.Thank Leslie Root, came idea around ‘Oh, think good data !’.Thank Lauren Kennedy whose generous sharing code, data, countless conversations thoughts MRP developed.Thank PhD supervisory panel John Tang, Martine Mariotti, Tim Hatton, Zach Ward gave freedom explore intellectual space interest , support follow interests, guidance ensure resulted something tangible.Thank Elle Côtè enabling book written.book greatly benefited notes teaching materials others freely available online, especially: Chris Bail, Scott Cunningham, Andrew Heiss, Lisa Lendway, Grant McDermott, Nathan Matias, David Mimno, Ed Rubin. Thank folks. changed norm academics making materials freely available online great one one hope free online version book helps contribute .Thank Samantha-Jo Caetano, helped develop assessment items. also, Lisa Romkey Alan Chong allowed adapt aspects rubric.Thank students contributed substantially development book, including: Mahfouz, Faria Khandaker, Keli Chiu, Paul Hodgetts, Thomas William Rosenthal. discussed aspects book , made specific contributions, also changed sharpened way thought almost everything covered . Paul additionally made art book.Thank students identified specific improvements, including: Aaron Miller, Amy Farrow, Arsh Lakhanpal, Cesar Villarreal Guzman, Flavia López, Hong Shi, Laura Cline, Lorena Almaraz De La Garza, Mounica Thanam, Reem Alasadi, Wijdan Tariq, Yang Wu, Yewon Han.Christmas 2021 book disparate collection notes. Thank Mum Dad, dropped everything came side world two months give opportunity re-write put together cohesive book.Finally, thank Monica Alexander. Without written book; even thought possible. Many best ideas book , , made better. Thank inestimable help writing book, providing base builds (remember library showing many times get certain rows R!), giving time needed write, encouragement turned writing book just meant endlessly re-writing perfect day , reading everything book many times, making coffee cocktails appropriate, .can contact : rohan.alexander@utoronto.ca.\nRohan Alexander\nToronto, Canada\nFebruary 2022\n","code":""},{"path":"about-the-author.html","id":"about-the-author","chapter":"About the author","heading":"About the author","text":"Rohan Alexander assistant professor University Toronto Information Statistical Sciences. also assistant director CANSSI Ontario, senior fellow Massey College, faculty affiliate Schwartz Reisman Institute Technology Society, co-lead Data Sciences Institute Thematic Program Reproducibility. holds PhD Economics Australian National University supervised John Tang (chair), Martine Mariotti, Tim Hatton, Zach Ward.interested using statistical models try understand world. particularly get data go models; whose data systematically missing; clean, prepare, tidy data modelled; effects implications models; can reproducibly share totality process. tries develop students skilled using statistical methods across various disciplines, also appreciate limitations, think deeply broader contexts work.enjoys teaching aims help students wide range backgrounds learn use data tell convincing stories. teaches Faculty Information Department Statistical Sciences undergraduate graduate levels. RStudio Certified Tidyverse Trainer.married Monica Alexander two children. probably spends much money books, certainly much time libraries. book recommendations , ’d love hear .","code":""},{"path":"telling-stories-with-data.html","id":"telling-stories-with-data","chapter":"1 Telling stories with data","heading":"1 Telling stories with data","text":"Required materialRead Counting Countless, (Keyes 2019).Watch Data Science Ethics 6 Minutes, (Register 2020).","code":""},{"path":"telling-stories-with-data.html","id":"on-telling-stories","chapter":"1 Telling stories with data","heading":"1.1 On telling stories","text":"One first things many parents regularly children born read stories . carry tradition occurred millennia. Myths, fables, fairy tales can seen heard around us. entertaining enable us learn something world. Hungry Caterpillar (Carle 1969) may seem quite far world dealing data, similarities. trying tell story teaching us something world.using data, try tell convincing story. may exciting predicting elections, banal increasing internet advertising click rates, serious finding cause disease, fun forecasting basketball games. case key elements . English author, E. M. Forster, described aspects common novels : story, people, plot, fantasy, prophecy, pattern, rhythm (Forster 1927). Similarly, tell stories data, common concerns, regardless setting:dataset? generated dataset ?process underpins dataset? Given process, missing dataset poorly measured? datasets generated, , different one ?dataset trying say, can let say ? else say? decide ?hoping others see dataset, can convince ? much work must convince ? extent can share came believe ?affected processes outcomes related dataset? extent represented dataset, part conducting analysis?past, certain elements telling stories data easier. instance, experimental design long robust tradition within agricultural medical sciences, physics, chemistry. Student’s t-distribution identified early 1900s chemist, William Sealy Gosset, working Guinness, beer manufacturer (Boland 1984)! possible randomly sample beer change one aspect time.Many fundamentals statistical methods use today developed settings. , typically possible establish control groups randomize; settings fewer ethical concerns. story told resulting data likely fairly convincing.Unfortunately, little applies days, given diversity settings statistical methods applied. hand, many advantages. instance, well-developed statistical techniques, easier access large datasets, open-source statistical languages R (R Core Team 2021). difficulty conducting traditional experiments means must also turn aspects tell convincing story.","code":""},{"path":"telling-stories-with-data.html","id":"workflow-components","chapter":"1 Telling stories with data","heading":"1.2 Workflow components","text":"five core components workflow needed tell stories data:Plan sketch endpoint.Simulate reasonable data consider .Acquire prepare real data.Explore understand dataset.Share found.begin planning sketching endpoint ensures think carefully want go. forces us deeply consider situation, acts keep us focused efficient, helps reduce scope creep. Alice’s Adventures Wonderland (Carroll 1865), Alice asks Cheshire Cat way go. Cheshire Cat replies asking Alice like go. Alice replies mind, long gets somewhere, Cheshire Cat says direction matter always get somewhere ‘walk long enough’. issue, case, typically afford walk aimlessly long. may endpoint needs change, important deliberate, reasoned, decision. possible given initial target. need spend much time get lot value . Often five minutes paper pen, enough.next step simulate data forces us details. helps cleaning preparing dataset focuses us classes dataset distribution values expect. instance, interested effect age-groups political preferences, may expect age-group column factor, four possible values: ‘18-29’, ‘30-44’, ‘45-59’, ‘60+’. process simulation provides us clear features real dataset satisfy. use features define tests guide data cleaning preparation. instance, check real dataset age-groups one four values. tests pass, confident age-group column contains values expect.Simulating data also important turn statistical modelling stage. stage, concerned whether model reflects dataset. issue go straight modelling real dataset, know whether problem model. simulate data precisely know underlying data generation process. apply model simulated dataset. get put , know model performing appropriately, can turn real dataset. Without initial application simulated data, difficult confidence model.Simulation often cheap—almost free given modern computing resources statistical programming languages—fast. provides ‘intimate feeling situation’ (Hamming 1996, 239). way proceed start simulation just contains essentials, get working, complicate .Acquiring preparing data interested often-overlooked stage workflow. surprising can one difficult stages requires many decisions made. increasingly subject research. instance, found decisions made stage greatly affect statistical results (Huntington-Klein et al. 2021).stage workflow, common feel little overwhelmed. Typically, data can acquire leave us little scared. may little , case worry going able make statistical machinery work. Alternatively, may problem worried can even begin deal large amount data.Perhaps dragons lives princesses waiting see us act, just , beauty courage. Perhaps everything frightens us , deepest essence, something helpless wants love.Rilke (1929)Developing comfort stage workflow unlocks rest . dataset needed tell convincing story , need iteratively remove everything data need, shape .dataset, want explore understand  certain relationships dataset. use statistical models understand implications data free bias, ‘truth’; tell . Within workflow tell stories data, statistical models tools approaches use explore dataset, way may use graphs tables. something provide us definitive result enable us understand dataset clearly particular way.time get step workflow, large extent, model reflect decisions made earlier stages, especially acquisition cleaning, much reflects type underlying process. Sophisticated modelers know statistical models like bit iceberg surface; build , possible due , majority underneath, case, data. expert whole workflow uses modelling, recognize results obtained additionally due choices whose data matters, decisions measure record data, aspects reflect world , well data available specific workflow.Finally, must share found, high fidelity possible. Talking knowledge , make knowledgeable, includes knowledge ‘past ’ . communicating, need clear decisions made, made , findings, weaknesses approach. aiming uncover something important (otherwise, bother) write everything, first instance, although written communication may supplemented forms communication later. many decisions must make workflow want sure open entire thing—start finish. means much just statistical modelling creation graphs tables, everything. Without , stories based data credibility.world rational meritocracy everything carefully judiciously evaluated. Instead, use shortcuts, hacks, heuristics, based experience. Unclear communication render even best work moot, thoroughly engaged . minimum comes communication, upper limit impressive can . culmination thought-workflow, best, obtains certain sprezzatura, studied carelessness. Achieving mastery, work years.","code":""},{"path":"telling-stories-with-data.html","id":"telling-stories-with-data-1","chapter":"1 Telling stories with data","heading":"1.3 Telling stories with data","text":"compelling story based data can likely told around ten--twenty pages. Much less , likely light details. easy write much , often reflection enables succinctness multiple stories separated. best stories typically based research independent learning.possible tell convincing stories even possible conduct traditional experiments. approaches rely ‘big data’—panacea (Meng 2018)—instead better using data available. blend theory application, combined practical skills, sophisticated workflow, appreciation one know, often enough create lasting knowledge.best stories based data tend multi-disciplinary. take whatever field need , almost always draw : statistics, data visualization, computer science, experimental design, economics, engineering, information science (name ). , end--end workflow requires blend skills areas. best way learn skills use real-world data conduct research projects :obtain clean relevant datasets;develop research questions;use statistical techniques explore questions; andcommunicate meaningful way.key elements telling convincing stories data :Communication.Ethics.Reproducibility.Questions.Measurement.Data collection.Data cleaning.Exploratory data analysis.Modelling.Scaling.elements foundation workflow built (Figure 1.1).\nFigure 1.1: workflow builds various elements\nlot master, communication important. Simple analysis, communicated well, valuable complicated analysis communicated poorly. latter understood trusted others. lack clear communication sometimes reflects failure researcher understand going , even . , level analysis match dataset, instruments, task, skillset, trade-required clarity complication, can sensible err side clarity.Clear communication means writing plain language, help tables, graphs, technical terms, way brings audience along . means setting done , well found. minimum hurdle way enables another person independently find found. One challenge immerse data, can difficult remember like first came . audience coming . Learning provide appropriate level nuance detail especially difficult made easier trying write audience’s benefit.Active consideration ethics needed dataset likely concerns humans. means considering things like: dataset, missing, ? extent story perpetuate past? something happen? Even dataset concern humans, story likely put together humans, affect almost everything else. means moral responsibility use data ethically, concern environmental impact, inequity.many definitions ethics, comes telling stories data, minimum means considering full context dataset (D’Ignazio Klein 2020). jurisprudence, textual approach law means literally considering words law printed, purposive approach means laws interpreted within broader context. ethical approach telling stories data means adopting latter approach, considering social, cultural, historical, political forces shape world, hence data (Crawford 2021).Reproducibility required create lasting knowledge world. means everything done—, end--end—can independently redone. Ideally, autonomous end--end reproducibility possible; anyone can get code, data, environment, verify everything done. Unfettered access code almost always possible. default data also, always reasonable. instance, studies psychology may small, personally identifying, samples. One way forward openly share simulated data similar properties, along defining process real data accessed, given appropriate bona fides.Curiosity provides internal motivation explore dataset, associated process, proper extent. Questions tend beget questions, usually improve refine process coming understand dataset carries . contrast stock Popperian approach hypothesis testing often taught, questions typically developed continuous evolving process (Franklin 2005). can difficult find initial question. Selecting area interest can help, can sketching broad claim intent evolving specific question, finally, bringing together two different areas.Developing comfort ease messiness real-world data means getting ask new questions time data update. knowing dataset detail tends surface unexpected groupings values can work subject-area experts understand. Becoming bit ‘mongrel’ developing base knowledge across variety areas especially valuable, becoming comfortable possibility initially asking dumb questions.Measurement data collection deciding world become data. challenging. world vibrant difficult reduce something possible consistently measure collect. Take, instance, someone’s height. can, probably, agree take shoes measure height. height changes course day. measuring someone’s height tape measure give different results using laser. comparing heights people time, therefore becomes important measure time day, using method. quickly becomes unfeasible.questions interested use data complicated height. measure sad someone ? measure pain? decides measure measure ? certain arrogance required think can reduce world value compare . Ultimately, must, difficult consistently define measured. process value-free. way reasonably come terms brutal reduction deeply understand, respect measuring collecting. central essence, can stripped away?Pablo Picasso, twentieth century Spanish painter, series drawings depicts outline animal using one line (Figure 1.2). Despite simplicity, recognize animal depicted—drawing sufficient tell animal dog, cat. used determine whether dog sick? Probably . likely want detailed drawing. decision features measured collected, ignore, turns context purpose.\nFigure 1.2: drawing clearly dog, even though just one line\nData cleaning preparation critical part using data. need massage data available us dataset can use. requires making lot decisions. data cleaning preparation stage critical, worthy much attention care .Consider survey collected information gender using four options: ‘man’, ‘woman’, ‘prefer say’, ‘’, ‘’ dissolved open textbox. come dataset, likely find responses either ‘man’ ‘woman’. need decide ‘prefer say’. drop dataset, actively ignoring respondents. drop , makes analysis complicated. Similarly, need decide deal open text responses. , drop responses, ignores experiences respondents. Another option merge ‘prefer say’, shows disregard respondents, specifically choose option.easy, always-correct, choice many data cleaning preparation situations. depends context purpose. Data cleaning preparation involves making many choices like , vital record every step others can understand done . Data never speak ; dummies ventriloquists cleaned prepared .process coming understand look feel dataset termed exploratory data analysis (EDA). open-ended process. need understand shape dataset can formally model . process EDA iterative one involves producing summary statistics, graphs, tables, sometimes even modelling. process never formally finishes requires variety skills.difficult delineate EDA ends formal statistical modelling begins, especially considering beliefs understanding develop (Hullman Gelman 2021). core, ‘EDA starts data’, involves immersing (Cook, Reid, Tanaka 2021). EDA something typically explicitly part final story. central role come understand story telling. , critical steps taken EDA recorded shared.Statistical modelling long robust history. knowledge statistics built hundreds years. Statistics series dry theorems proofs instead way exploring world. analogous ‘knowledge foreign languages algebra: may prove use time circumstances’ (Bowley 1901, 4). statistical model recipe blindly followed ---way instead way understanding data (James et al. 2017). Modelling usually required infer statistical patterns data. formally, ‘statistical inference, “learning” called computer science, process using data infer distribution generated data’ (Wasserman 2005, 87).Statistical significance scientific significance, realizing cost dominant paradigm. rarely appropriate put data arbitrary pass/fail statistical test. Instead, proper use statistical modelling kind echolocation. listen comes back us model, help learn shape world, recognizing one representation world.use statistical programming languages, R, enables us rapidly scale work. refers inputs outputs. basically just easy consider 10 observations 1,000, even 1,000,000. enables us quickly see extent stories apply. also case outputs can consumed easily one person 10, 100. Using Application Programming Interface (API) even possible stories considered many thousands times second.","code":""},{"path":"telling-stories-with-data.html","id":"how-do-our-worlds-become-data","chapter":"1 Telling stories with data","heading":"1.4 How do our worlds become data?","text":"famous story Eddington people went fishing sea net. Upon examining size fish caught, decided minimum size fish sea! conclusion arose tool used reality.Hamming (1996, 177)certain extent wasting time. perfect model world—world! complicated. knew perfectly everything affected uncountable factors influence , perfectly forecast coin toss, dice roll, every seemingly random process time. . Instead, must simplify things plausibly measurable, define data. data simplification messy, complex, world generated.different approximations ‘plausibly measurable’. Hence, datasets always result choices. must decide whether nonetheless reasonable task hand. use statistical models help us think deeply , explore, hopefully come better understand, data.Much statistics focused considering, thoroughly, data . appropriate data predominately agricultural, astronomical, physical sciences. rise data science, mostly value application datasets generated humans, must also actively consider dataset. systematically missing dataset? Whose data fit nicely approach using hence inappropriately simplified? process world becoming data necessarily involves abstraction simplification, need clear points can reasonably simplify, inappropriate, recognizing application specific.process world becoming data necessarily involves measurement. Paradoxically, often measurement deeply immersed details less trust data removed . Even seemingly clear tasks, measuring distance, defining boundaries, counting populations, surprisingly difficult practice. Turning world data requires many decisions imposes much error. Among many considerations, need decide measured, accurately , measurement.Oh, think good data ! important example something seemingly simple quickly becomes difficult maternal mortality. refers number women die pregnant, soon termination, cause related pregnancy management (2019). difficult critical turn tragedy death cause-specific data helps mitigate future deaths. countries well-developed civil registration vital statistics (CRVS). collect data every death. many countries CRVS every death recorded. Even death recorded, defining cause death may difficult, especially lack qualified medical personal equipment. Maternal mortality especially difficult typically many causes. CRVS checkbox form specify whether death counted maternal mortality. even developed countries recently adopted . instance, introduced US 2003, even 2015 Alabama, California, West Virginia adopted standard question (MacDorman Declercq 2018).typically use various instruments turn world data. astronomy, development better telescopes, eventually satellites probes, enabled new understanding worlds. Similarly, new instruments turning world data developed day. census generational-defining event, now regular surveys, transactions data available second, almost interactions internet become data kind. development instruments enabled exciting new stories.world imperfectly becomes data. nonetheless use data learn world, need actively seek understand ways imperfect implications imperfections.","code":""},{"path":"telling-stories-with-data.html","id":"what-is-data-science-and-how-should-we-use-it-to-learn-about-the-world","chapter":"1 Telling stories with data","heading":"1.5 What is data science and how should we use it to learn about the world","text":"agreed definition data science, lot people tried. instance, Wickham Grolemund (2017) say ‘…exciting discipline allows turn raw data understanding, insight, knowledge.’ Similarly, Leek Peng (2020) say ‘…process formulating quantitative question can answered data, collecting cleaning data, analyzing data, communicating answer question relevant audience.’ Baumer, Kaplan, Horton (2021) say ‘…science extracting meaningful information data’. T.-. Timbers, Campbell, Lee (2022) say define ‘data science process generating insight data reproducible auditable processes’. Foster (1968) points clearly data science says: ‘(s)tatistics concerned processing analysis masses data development mathematical methods extracting information data. Combine activity computer methods something sum parts.’Craiu (2019) argues lack certainty data science might matter ‘…can really say makes someone poet scientist?’ goes broadly say data scientist ‘…someone data driven research agenda, adheres aspires using principled implementation statistical methods uses efficient computation skills.’case, alongside specific, technical, definitions, value simple definition, even lose bit specificity. Probability often informally defined ‘counting things’ (McElreath 2020, 10). similar informal sense, data science can defined something like: humans measuring stuff, typically related humans, using sophisticated averaging explain predict.may sound touch cute, Francis Edgeworth, nineteenth century statistician economist, considered statistics science ‘Means presented social phenomena’, good company (Edgeworth 1885). case, one feature definition treat data terra nullius, nobody’s land. Statisticians tend see data result process can never know, try use data come understand. Many statisticians care deeply data measurement, many cases statistics data kind just appear; belong one. never actually case.Data generated, must gathered, cleaned, prepared, decisions matter. Every dataset sui generis, class , come know one dataset well, just know one dataset, datasets.Much data science focuses ‘science’, important also focus ‘data’. another feature cutesy definition data science. lot data scientists generalists, interested broad range problems. Often, thing unites need gather, clean, prepare messy data. often specifics data requires time, updates often, worthy full attention.Jordan (2019) describes medical office given probability, based prenatal screening, child, fetus, syndrome. way background, one can test know sure, test comes risk fetus surviving, initial screening probability matters. Jordan (2019) found probabilities determined based study done decade earlier UK. issue ensuing 10 years, imaging technology improved test expecting high-resolution images subsequent (false) increase syndrome diagnoses images improved. problem science, data.Shoulders giants Dr Michael Jordan Pehong Chen Distinguished Professor University California, Berkeley. taking PhD Cognitive Science University California, San Diego, 1985, appointed assistant professor MIT, promoted full professor 1997, 1998 moved Berkeley. One area research statistical machine learning. One particularly important paper Blei, Ng, Jordan (2003), enables text grouped together define topics.just ‘science’ bit hard, ‘data’ bit well. instance, researchers went back examined one popular text datasets computer science, found around 30 per cent data inappropriately duplicated (Bandy Vincent 2021). entire field—linguistics—specializes types datasets, inappropriate use data one dangers one field hegemonic. strength data science brings together folks variety backgrounds training task learning dataset. constrained done past. means must go way show respect come tradition, nonetheless similarly interested dataset . Data science multi-disciplinary increasingly critical; hence must reflect world. pressing need diversity backgrounds, approaches, disciplines data science.world messy, data. successfully tell stories data need become comfortable fact process difficult. Hannah Fry, British mathematician, describes spending six months rewriting code solved problem (Thornhill 2021). need learn stick . also need countenance failure, developing resilience intrinsic motivation. world data considering possibilities probabilities, learning make trade-offs . almost never anything know certain, perfect analysis.Ultimately, just telling stories data, stories increasingly among important world.","code":""},{"path":"telling-stories-with-data.html","id":"exercises-and-tutorial","chapter":"1 Telling stories with data","heading":"1.6 Exercises and tutorial","text":"","code":""},{"path":"telling-stories-with-data.html","id":"exercises","chapter":"1 Telling stories with data","heading":"1.6.1 Exercises","text":"According Register (2020) data decisions affect (pick one)?\nReal people.\none.\ntraining set.\ntest set.\nReal people.one.training set.test set.data science (words)?According Keyes (2019) perhaps accurate definition data science (pick one)?\ninhumane reduction humanity can counted.\nquantitative analysis large amounts data purpose decision-making.\nData science inter-disciplinary field uses scientific methods, processes, algorithms, systems extract knowledge insights many structural unstructured data.\ninhumane reduction humanity can counted.quantitative analysis large amounts data purpose decision-making.Data science inter-disciplinary field uses scientific methods, processes, algorithms, systems extract knowledge insights many structural unstructured data.Imagine job including ‘race’ /sexuality explanatory variables improves performance model. types issues consider deciding whether include variable production (words)?Re-order following steps workflow correct:\nSimulate.\nAcquire.\nShare.\nPlan.\nExplore.\nSimulate.Acquire.Share.Plan.Explore.According Crawford (2021), following forces shape world, hence data (select apply)?\nPolitical.\nHistorical.\nCultural.\nSocial.\nPolitical.Historical.Cultural.Social.required tell convincing stories (select apply)?\nSophisticated workflow.\nPractical skills.\nBig data.\nHumility one’s knowledge.\nTheory application.\nSophisticated workflow.Practical skills.Big data.Humility one’s knowledge.Theory application.fair coin tossed twice, probability getting two heads?\n0\n0.25\n0.5\n0.75\n1\n00.250.50.751Why ethics key element telling convincing stories (words)?","code":""},{"path":"telling-stories-with-data.html","id":"tutorial","chapter":"1 Telling stories with data","heading":"1.6.2 Tutorial","text":"purpose tutorial clarify mind difficulty measurement, even seemingly simple things, hence likelihood measurement issues complicated areas.Please obtain seeds fast-growing plant radishes, mustard greens, arugula. Plant seeds measure much soil used. Water measure water used. day take note changes. generally, measure record much can. Note thoughts difficulty measurement. Eventually seeds sprout, measure big . return use data put together.waiting seeds sprout, one week , please measure length hair daily. Write one--two-page paper found learned difficulty measurement.","code":""},{"path":"drinking-from-a-fire-hose.html","id":"drinking-from-a-fire-hose","chapter":"2 Drinking from a fire hose","heading":"2 Drinking from a fire hose","text":"Required materialRead Data science atomic habit, (Barrett 2021a)Read AI bias really happens—’s hard fix, (Hao 2019)Read mundanity excellence: ethnographic report stratification Olympic swimmers, (Chambliss 1989)Key concepts skillsThe statistical programming language R enables us tell interesting stories using data. language like , path mastery can slow.framework use approach projects : plan, simulate, gather, explore, share.way learn R start small project break required achieve tiny steps, look people’s code, draw achieve step. Complete project move onto next project. project get little better.key start actively working regularly.Key librariesggplot2 (Wickham 2016)janitor (Firke 2020)opendatatoronto (Gelfand 2020)tidyr (Wickham 2021c)tidyverse (Wickham 2017)Key functions<- ‘assign’|> ‘pipe’+ ‘add’c()citation()class()dplyr::arrange()dplyr::filter()dplyr::mutate()dplyr::recode()dplyr::rename()dplyr::select()dplyr::summarize()ggplot2::geom_bar()ggplot2::geom_point()ggplot2::ggplot()head()janitor::clean_names()library()names()readr::read_csv()readr::write_csv()rep()rpois()runif()sample()set.seed()stringr::str_remove()sum()tail()tidyr::separate()","code":""},{"path":"drinking-from-a-fire-hose.html","id":"hello-world","chapter":"2 Drinking from a fire hose","heading":"2.1 Hello, World!","text":"way start, start. chapter go three complete examples workflow advocated book. means : plan, simulate, acquire, explore, share. new R, code may bit unfamiliar . new statistics, concepts may unfamiliar. worry. soon become familiar.way learn tell stories, start telling stories . means try get examples working. sketches , type everything (using R Studio Cloud new R installed computer), execute . important, normal, realize challenging start.Whenever ’re learning new tool, long time, ’re going suck… good news typical; ’s something happens everyone, ’s temporary.Hadley Wickham quoted Barrett (2021a).guided thoroughly . Hopefully experiencing power telling stories data, feel empowered stick .get started, go R Studio Cloud – https://rstudio.cloud/ – create account. anything involved free version fine now. account log , look something like Figure 2.1.\nFigure 2.1: Opening R Studio Cloud first time\n(‘Workspace’, ‘Example Workspace’.) start ‘New Project’. can give project name clicking ‘Untitled Project’ replacing .now go three worked examples: Canadian elections, Toronto homelessness, neonatal mortality. examples build increasing complexity, first one, telling story data.","code":""},{"path":"drinking-from-a-fire-hose.html","id":"canadian-elections","chapter":"2 Drinking from a fire hose","heading":"2.2 Canadian elections","text":"Canada parliamentary democracy 338 seats House Commons, lower house government formed. two major parties – ‘Liberal’ ‘Conservative’ – three minor parties – ‘Bloc Québécois’, ‘New Democratic’, ‘Green’ – many smaller parties independents. example create graph number seats party won 2019 Federal Election.","code":""},{"path":"drinking-from-a-fire-hose.html","id":"plan","chapter":"2 Drinking from a fire hose","heading":"2.2.1 Plan","text":"example, need plan two aspects. first dataset need look like, second final graph look like.basic requirement dataset name seat (sometimes called ‘riding’ Canada) party person elected. , quick sketch dataset need look something like Figure 2.2.\nFigure 2.2: Quick sketch dataset useful analysing Canadian elections\nalso need plan graph interested . Given want display number seats party won, quick sketch might aim Figure 2.3.\nFigure 2.3: Quick sketch possible graph number ridings won party\n","code":""},{"path":"drinking-from-a-fire-hose.html","id":"simulate","chapter":"2 Drinking from a fire hose","heading":"2.2.2 Simulate","text":"now simulate data, bring specificity sketches.get started, within R Studio Cloud, make new R Markdown file (‘File’ -> ‘New File’ -> ‘R Markdown’). , asked install packages, agree . example, put everything one R Markdown document. save ‘canadian_elections.Rmd’ (‘File’ -> ‘Save …’)R Markdown document create new R code chunk (‘Code’ -> ‘Insert Chunk’) add preamble documentation explains:purpose document;author contact details;file written last updated; andpre-requisites file relies .R, lines start ‘#’ comments. means run code R, instead designed read humans. line preamble start ‘#’. Also make clear preamble section surrounding ‘####’.need set-workspace. involves installing loading packages needed. package needs installed computer, needs loaded time used. case going use tidyverse (Wickham 2017), janitor (Firke 2020), tidyr (Wickham 2021c). need installed first time used, need loaded.example installing packages follows (excessive comments added clear going ; general, level commenting unnecessary). Run code clicking small green arrow associated R code chunk.Now packages installed, need loaded. installation step needs done per computer, code can commented accidentally run.packages contain help file provides information functions. can accessed appending question mark package name running code. instance ?tidyverse.simulate data, need create dataset two columns: ‘Riding’ ‘Party’, values . case ‘Riding’ reasonable values name one 338 Canadian ridings. case ‘Party’ reasonable values one following six: ‘Liberal’, ‘Conservative’, ‘Bloc Québécois’, ‘New Democratic’, ‘Green’, ‘’. , code can run clicking small green arrow associated R code chunk.","code":"\n#### Preamble ####\n# Purpose: Read in data from the 2019 Canadian Election and make a\n# graph of the number of ridings each party won.\n# Author: Rohan Alexander\n# Email: rohan.alexander@utoronto.ca\n# Date: 1 January 2022\n# Prerequisites: Need to know where to get Canadian elections data.\n#### Workspace set-up ####\ninstall.packages(\"tidyverse\") # Only need to do this once per computer\ninstall.packages(\"janitor\") # Only need to do this once per computer\ninstall.packages(\"tidyr\") # Only need to do this once per computer\n#### Workspace set-up ####\n# install.packages(\"tidyverse\") # Only need to do this once per computer\n# install.packages(\"janitor\") # Only need to do this once per computer\n# install.packages(\"tidyr\") # Only need to do this once per computer\n\nlibrary(tidyverse) # A collection of data-related packages\nlibrary(janitor) # Helps clean datasets\nlibrary(tidyr) # Helps make tidy datasets\nsimulated_data <-\n  tibble(\n    # Use 1 through to 338 to represent each riding\n    'Riding' = 1:338,\n    # Randomly choose one of six options, with replacement, 338 times\n    'Party' = sample(\n      x = c(\n        'Liberal',\n        'Conservative',\n        'Bloc Québécois',\n        'New Democratic',\n        'Green',\n        'Other'\n      ),\n      size = 338,\n      replace = TRUE\n    ))\n\nsimulated_data\n#> # A tibble: 338 × 2\n#>    Riding Party         \n#>     <int> <chr>         \n#>  1      1 New Democratic\n#>  2      2 Liberal       \n#>  3      3 Liberal       \n#>  4      4 Other         \n#>  5      5 Green         \n#>  6      6 Bloc Québécois\n#>  7      7 New Democratic\n#>  8      8 Other         \n#>  9      9 Green         \n#> 10     10 Green         \n#> # … with 328 more rows"},{"path":"drinking-from-a-fire-hose.html","id":"acquire","chapter":"2 Drinking from a fire hose","heading":"2.2.3 Acquire","text":"Now want get actual data. data need Elections Canada, non-partisan agency organizes Canadian Federal elections. can pass website read_csv() readr package (Wickham, Hester, Bryan 2021). need explicitly load readr package part tidyverse. ‘<-’ ‘assignment operator’ allocating output read_csv() object called ‘raw_elections_data’.can take quick look dataset using head() show first six rows, tail() show last six rows.need clean data can use . trying make similar dataset thought wanted planning stage. fine move away plan, needs deliberate, reasoned, decision. reading dataset saved, first thing adjust names make easier type. Removing spaces helps type column names. using clean_names() janitor (Firke 2020) changes names ‘snake_case’.names faster type R Studio auto-complete . , begin typing name column use ‘tab’ auto-complete .many columns dataset, primarily interested two: ‘electoral_district_name_nom_de_circonscription’, ‘elected_candidate_candidat_elu’. can choose certain columns interest using select() dplyr (Wickham et al. 2020) loaded part tidyverse. ‘pipe operator’, |>, pushes output one line first input function next line.names columns still quite long English French . can look names columns names(). can change names using rename() dplyr (Wickham et al. 2020).now look dataset, ‘elected_candidate’ column particular.can see surname elected candidate, followed comma, followed first name, followed space, followed name party English French, separated slash. can break-column pieces using separate() tidyr (Wickham 2021c).Finally want change party names French English match simulated, using recode() dplyr (Wickham et al. 2020).data now matches plan (Figure 2.2) pretty well. every electoral district party person won .now nicely cleaned dataset, save , can start cleaned dataset next stage.","code":"\n#### Read in the data ####\nraw_elections_data <- \n  read_csv(\n    file =\n      \"https://www.elections.ca/res/rep/off/ovr2019app/51/data_donnees/table_tableau11.csv\",\n    show_col_types = FALSE\n    ) \n\n# We have read the data from the Elections Canada website. We may like\n# to save it in case something happens or they move it. \nwrite_csv(\n  x = raw_elections_data, \n  file = \"canadian_voting.csv\"\n  )\nhead(raw_elections_data)\n#> # A tibble: 6 × 13\n#>   Province      `Electoral Dis…` `Electoral Dis…` Population\n#>   <chr>         <chr>                       <dbl>      <dbl>\n#> 1 Newfoundland… Avalon                      10001      86494\n#> 2 Newfoundland… Bonavista--Buri…            10002      74116\n#> 3 Newfoundland… Coast of Bays--…            10003      77680\n#> 4 Newfoundland… Labrador                    10004      27197\n#> 5 Newfoundland… Long Range Moun…            10005      86553\n#> 6 Newfoundland… St. John's East…            10006      85697\n#> # … with 9 more variables: `Electors/Électeurs` <dbl>,\n#> #   `Polling Stations/Bureaux de scrutin` <dbl>,\n#> #   `Valid Ballots/Bulletins valides` <dbl>,\n#> #   `Percentage of Valid Ballots /Pourcentage des bulletins valides` <dbl>,\n#> #   `Rejected Ballots/Bulletins rejetés` <dbl>,\n#> #   `Percentage of Rejected Ballots /Pourcentage des bulletins rejetés` <dbl>,\n#> #   `Total Ballots Cast/Total des bulletins déposés` <dbl>, …\ntail(raw_elections_data)\n#> # A tibble: 6 × 13\n#>   Province      `Electoral Dis…` `Electoral Dis…` Population\n#>   <chr>         <chr>                       <dbl>      <dbl>\n#> 1 British Colu… Vancouver South…            59040     102927\n#> 2 British Colu… Victoria                    59041     117133\n#> 3 British Colu… West Vancouver-…            59042     119113\n#> 4 Yukon         Yukon                       60001      35874\n#> 5 Northwest Te… Northwest Terri…            61001      41786\n#> 6 Nunavut       Nunavut                     62001      35944\n#> # … with 9 more variables: `Electors/Électeurs` <dbl>,\n#> #   `Polling Stations/Bureaux de scrutin` <dbl>,\n#> #   `Valid Ballots/Bulletins valides` <dbl>,\n#> #   `Percentage of Valid Ballots /Pourcentage des bulletins valides` <dbl>,\n#> #   `Rejected Ballots/Bulletins rejetés` <dbl>,\n#> #   `Percentage of Rejected Ballots /Pourcentage des bulletins rejetés` <dbl>,\n#> #   `Total Ballots Cast/Total des bulletins déposés` <dbl>, …\n#### Basic cleaning ####\nraw_elections_data <- \n  read_csv(file = \"canadian_voting.csv\",\n           show_col_types = FALSE\n           )\n# Make the names easier to type\ncleaned_elections_data <- \n  clean_names(raw_elections_data)\n\n# Have a look at the first six rows\nhead(cleaned_elections_data)\n#> # A tibble: 6 × 13\n#>   province      electoral_distr… electoral_distr… population\n#>   <chr>         <chr>                       <dbl>      <dbl>\n#> 1 Newfoundland… Avalon                      10001      86494\n#> 2 Newfoundland… Bonavista--Buri…            10002      74116\n#> 3 Newfoundland… Coast of Bays--…            10003      77680\n#> 4 Newfoundland… Labrador                    10004      27197\n#> 5 Newfoundland… Long Range Moun…            10005      86553\n#> 6 Newfoundland… St. John's East…            10006      85697\n#> # … with 9 more variables: electors_electeurs <dbl>,\n#> #   polling_stations_bureaux_de_scrutin <dbl>,\n#> #   valid_ballots_bulletins_valides <dbl>,\n#> #   percentage_of_valid_ballots_pourcentage_des_bulletins_valides <dbl>,\n#> #   rejected_ballots_bulletins_rejetes <dbl>,\n#> #   percentage_of_rejected_ballots_pourcentage_des_bulletins_rejetes <dbl>,\n#> #   total_ballots_cast_total_des_bulletins_deposes <dbl>, …\ncleaned_elections_data <- \n  cleaned_elections_data |> \n  # Select only certain columns\n  select(electoral_district_name_nom_de_circonscription,\n         elected_candidate_candidat_elu\n         )\n\n# Have a look at the first six rows\nhead(cleaned_elections_data)\n#> # A tibble: 6 × 2\n#>   electoral_district_name_nom_de_circonscr… elected_candida…\n#>   <chr>                                     <chr>           \n#> 1 Avalon                                    McDonald, Kenne…\n#> 2 Bonavista--Burin--Trinity                 Rogers, Churenc…\n#> 3 Coast of Bays--Central--Notre Dame        Simms, Scott Li…\n#> 4 Labrador                                  Jones, Yvonne L…\n#> 5 Long Range Mountains                      Hutchings, Gudi…\n#> 6 St. John's East/St. John's-Est            Harris, Jack ND…\nnames(cleaned_elections_data)\n#> [1] \"electoral_district_name_nom_de_circonscription\"\n#> [2] \"elected_candidate_candidat_elu\"\ncleaned_elections_data <- \n  cleaned_elections_data |> \n  rename(\n    riding = electoral_district_name_nom_de_circonscription,\n    elected_candidate = elected_candidate_candidat_elu\n    )\n\nhead(cleaned_elections_data)\n#> # A tibble: 6 × 2\n#>   riding                             elected_candidate      \n#>   <chr>                              <chr>                  \n#> 1 Avalon                             McDonald, Kenneth Libe…\n#> 2 Bonavista--Burin--Trinity          Rogers, Churence Liber…\n#> 3 Coast of Bays--Central--Notre Dame Simms, Scott Liberal/L…\n#> 4 Labrador                           Jones, Yvonne Liberal/…\n#> 5 Long Range Mountains               Hutchings, Gudie Liber…\n#> 6 St. John's East/St. John's-Est     Harris, Jack NDP-New D…\nhead(cleaned_elections_data$elected_candidate)\n#> [1] \"McDonald, Kenneth Liberal/Libéral\"                                   \n#> [2] \"Rogers, Churence Liberal/Libéral\"                                    \n#> [3] \"Simms, Scott Liberal/Libéral\"                                        \n#> [4] \"Jones, Yvonne Liberal/Libéral\"                                       \n#> [5] \"Hutchings, Gudie Liberal/Libéral\"                                    \n#> [6] \"Harris, Jack NDP-New Democratic Party/NPD-Nouveau Parti démocratique\"\ncleaned_elections_data <- \n  cleaned_elections_data |> \n  # Separate the column into two based on the slash\n  separate(col = elected_candidate,\n           into = c('other', 'party'),\n           sep = '/') |> \n  # Remove the 'other' column\n  select(-other)\n\nhead(cleaned_elections_data)\n#> # A tibble: 6 × 2\n#>   riding                             party                  \n#>   <chr>                              <chr>                  \n#> 1 Avalon                             Libéral                \n#> 2 Bonavista--Burin--Trinity          Libéral                \n#> 3 Coast of Bays--Central--Notre Dame Libéral                \n#> 4 Labrador                           Libéral                \n#> 5 Long Range Mountains               Libéral                \n#> 6 St. John's East/St. John's-Est     NPD-Nouveau Parti démo…\ncleaned_elections_data <-\n  cleaned_elections_data |>\n  mutate(\n    party =\n      recode(\n        party,\n        'Conservateur' = 'Conservative',\n        'Indépendant(e)' = 'Other',\n        'Libéral' = 'Liberal',\n        'NPD-Nouveau Parti démocratique' = 'New Democratic',\n        'Parti Vert' = 'Green'\n      )\n  )\n\nhead(cleaned_elections_data)\n#> # A tibble: 6 × 2\n#>   riding                             party         \n#>   <chr>                              <chr>         \n#> 1 Avalon                             Liberal       \n#> 2 Bonavista--Burin--Trinity          Liberal       \n#> 3 Coast of Bays--Central--Notre Dame Liberal       \n#> 4 Labrador                           Liberal       \n#> 5 Long Range Mountains               Liberal       \n#> 6 St. John's East/St. John's-Est     New Democratic\nwrite_csv(\n  x = cleaned_elections_data,\n  file = \"cleaned_elections_data.csv\"\n  )"},{"path":"drinking-from-a-fire-hose.html","id":"explore","chapter":"2 Drinking from a fire hose","heading":"2.2.4 Explore","text":"point like explore dataset created. One way better understand dataset make graph. particular, like build graph planned Figure 2.3.First, read dataset just created.can get quick count many seats party won using count() dplyr (Wickham et al. 2020).build graph interested , rely ggplot2 package (Wickham 2016). key aspect package build graphs adding layers using ‘+’, call ‘add operator’. particular create bar chart using geom_bar() ggplot2 (Wickham 2016).accomplishes set . can make look bit nicer modifying default options (Figure 2.4).\nFigure 2.4: Number seats won, political party, 2019 Canadian Federal Election\n","code":"\n#### Read in the data ####\ncleaned_elections_data <- \n  read_csv(\n    file = \"cleaned_elections_data.csv\",\n    show_col_types = FALSE\n    )\ncleaned_elections_data |> \n  count(party)\n#> # A tibble: 6 × 2\n#>   party              n\n#>   <chr>          <int>\n#> 1 Bloc Québécois    32\n#> 2 Conservative     121\n#> 3 Green              3\n#> 4 Liberal          157\n#> 5 New Democratic    24\n#> 6 Other              1\ncleaned_elections_data |> \n  ggplot(aes(x = party)) + # aes abbreviates aesthetics and enables us \n  # to specify the x axis variable\n  geom_bar()\ncleaned_elections_data |> \n  ggplot(aes(x = party)) +\n  geom_bar() +\n  theme_minimal() + # Make the theme neater\n  coord_flip() + # Swap the x and y axis to make parties easier to read\n  labs(x = \"Party\",\n       y = \"Number of seats\") # Make the labels more meaningful"},{"path":"drinking-from-a-fire-hose.html","id":"communicate","chapter":"2 Drinking from a fire hose","heading":"2.2.5 Communicate","text":"point downloaded data, cleaned , made graph. typically need communicate done length. case, can write paragraphs , , found. example follows.Canada parliamentary democracy 338 seats House Commons, house forms government. two major parties—‘Liberal’ ‘Conservative’—three minor parties—‘Bloc Québécois’, ‘New Democratic’, ‘Green’—many smaller parties. 2019 Federal Election occurred 21 October, 17 million votes cast. interested number seats won party.downloaded results, seat-specific basis, Elections Canada website. cleaned tidied dataset using statistical programming language R (R Core Team 2021) well packages tidyverse (Wickham et al. 2019a) janitor (Firke 2020). created graph number seats political party won (Figure 2.4).found Liberal Party won 157 seats, followed Conservative Party 121 seats. minor parties won following number seats: Bloc Québécois, 32 seats, New Democratic Party, 24 seats, Green Party, 3 seats. Finally, one independent candidate won seat.distribution seats skewed toward two major parties reflect relatively stable preferences part Canadian voters, possibly inertia due benefits already major party national network funding, reason. better understanding reasons distribution interest future work. dataset consists everyone voted, worth noting Canada systematically excluded voting; much difficult vote others.","code":""},{"path":"drinking-from-a-fire-hose.html","id":"toronto-homelessness","chapter":"2 Drinking from a fire hose","heading":"2.3 Toronto homelessness","text":"Toronto large homeless population (City Toronto 2021). Freezing winters mean important enough places shelters. example make table shelter usage second half 2021 compares average use month. expectation greater usage colder months, instance, December, compared warmer months, instance, July.","code":""},{"path":"drinking-from-a-fire-hose.html","id":"plan-1","chapter":"2 Drinking from a fire hose","heading":"2.3.1 Plan","text":"dataset interested need date, shelter, number beds occupied night. quick sketch dataset work Figure 2.5.\nFigure 2.5: Quick sketch dataset useful understanding shelter usage Toronto\ninterested creating table monthly average number beds occupied night. table probably look something like Figure 2.6.\nFigure 2.6: Quick sketch table average number beds occupied month\n","code":""},{"path":"drinking-from-a-fire-hose.html","id":"simulate-1","chapter":"2 Drinking from a fire hose","heading":"2.3.2 Simulate","text":"next step simulate data resemble dataset.Within R Studio Cloud make new R Markdown file, save , make new R code chunk add preamble documentation. install /load libraries needed. use tidyverse (Wickham 2017), janitor (Firke 2020), tidyr (Wickham 2021c). installed earlier, need installed . example also use lubridate (Grolemund Wickham 2011), part tidyverse need installed independently. also use opendatatoronto (Gelfand 2020), knitr (Xie 2021) need installed.add bit detail earlier example, libraries contain code people written. common ones see regularly, especially tidyverse. use package, must first install need load . package needs installed per computer must loaded every time. , packages installed earlier need reinstalled .Given folks freely gave time make R packages use, important cite . get information needed, can use citation(). run without arguments, provides citation information R , run argument name package, provides citation information package.Turning simulation, need three columns: ‘date’, ‘shelter’, ‘occupancy’. example build earlier one adding seed using set.seed(). seed enables us always generate random data. integer can used seed. case seed 853. use seed, get random numbers example. use different seed, expect different random numbers. Finally, use rep() repeat something certain number times. instance, repeat ‘Shelter 1’, 184 times accounts half year.simulation first create list dates 2021. repeat list three times. assume data three shelters every day year. simulate number beds occupied night, draw Poisson distribution, assuming mean number 30 beds occupied per shelter.","code":"\n#### Preamble ####\n# Purpose: Get data about 2021 houseless shelter usage and make a table\n# Author: Rohan Alexander\n# Email: rohan.alexander@utoronto.ca\n# Date: 1 January 2022\n# Prerequisites: - \n\n#### Workspace set-up ####\ninstall.packages(\"opendatatoronto\")\ninstall.packages(\"lubridate\")\ninstall.packages(\"knitr\")\n\nlibrary(knitr)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(opendatatoronto)\nlibrary(tidyverse)\nlibrary(tidyr)\ncitation() # Get the citation information for R\n#> \n#> To cite R in publications use:\n#> \n#>   R Core Team (2021). R: A language and environment\n#>   for statistical computing. R Foundation for\n#>   Statistical Computing, Vienna, Austria. URL\n#>   https://www.R-project.org/.\n#> \n#> A BibTeX entry for LaTeX users is\n#> \n#>   @Manual{,\n#>     title = {R: A Language and Environment for Statistical Computing},\n#>     author = {{R Core Team}},\n#>     organization = {R Foundation for Statistical Computing},\n#>     address = {Vienna, Austria},\n#>     year = {2021},\n#>     url = {https://www.R-project.org/},\n#>   }\n#> \n#> We have invested a lot of time and effort in creating\n#> R, please cite it when using it for data analysis.\n#> See also 'citation(\"pkgname\")' for citing R packages.\ncitation('tidyverse') # Get the citation information for a particular package\n#> \n#>   Wickham et al., (2019). Welcome to the tidyverse.\n#>   Journal of Open Source Software, 4(43), 1686,\n#>   https://doi.org/10.21105/joss.01686\n#> \n#> A BibTeX entry for LaTeX users is\n#> \n#>   @Article{,\n#>     title = {Welcome to the {tidyverse}},\n#>     author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},\n#>     year = {2019},\n#>     journal = {Journal of Open Source Software},\n#>     volume = {4},\n#>     number = {43},\n#>     pages = {1686},\n#>     doi = {10.21105/joss.01686},\n#>   }\n#### Simulate ####\nset.seed(853)   \n\nsimulated_occupancy_data <- \n  tibble(\n    date = rep(x = as.Date(\"2021-07-01\") + c(0:183), times = 3), \n    # Based on Dirk Eddelbuettel: https://stackoverflow.com/a/21502386\n    shelter = c(rep(x = \"Shelter 1\", times = 184), \n                rep(x = \"Shelter 2\", times = 184),\n                rep(x = \"Shelter 3\", times = 184)),\n    number_occupied = \n      rpois(n = 184*3,\n            lambda = 30) # Draw 552 times from the Poisson distribution\n    )\n\nhead(simulated_occupancy_data)\n#> # A tibble: 6 × 3\n#>   date       shelter   number_occupied\n#>   <date>     <chr>               <int>\n#> 1 2021-07-01 Shelter 1              28\n#> 2 2021-07-02 Shelter 1              29\n#> 3 2021-07-03 Shelter 1              35\n#> 4 2021-07-04 Shelter 1              25\n#> 5 2021-07-05 Shelter 1              21\n#> 6 2021-07-06 Shelter 1              30"},{"path":"drinking-from-a-fire-hose.html","id":"acquire-1","chapter":"2 Drinking from a fire hose","heading":"2.3.3 Acquire","text":"use data made available Toronto homeless shelters City Toronto. premise data night 4am count made occupied beds. access data, use opendatatoronto (Gelfand 2020) save copy.much needs done make similar dataset interested (Figure 2.5). need change names make easier type using clean_names(), reduce columns relevant using select(), keep second half year using filter().remains save cleaned dataset.","code":"\n#### Acquire ####\n# Based on code from: \n# https://open.toronto.ca/dataset/daily-shelter-overnight-service-occupancy-capacity/\n# Thank you to Heath Priston for assistance\ntoronto_shelters <- \n  # Each package is associated with a unique id which can be found in \n  # 'For Developers':\n  # https://open.toronto.ca/dataset/daily-shelter-overnight-service-occupancy-capacity/\n  list_package_resources(\"21c83b32-d5a8-4106-a54f-010dbe49f6f2\") |> \n  # Within that package, we are interested in the 2021 dataset\n  filter(name == \"daily-shelter-overnight-service-occupancy-capacity-2021\") |> \n  # Having reduce the dataset down to one row we can get the resource\n  get_resource()\n\nwrite_csv(\n  x = toronto_shelters, \n  file = \"toronto_shelters.csv\"\n  )\n\nhead(toronto_shelters)#> # A tibble: 6 × 32\n#>     `_id` OCCUPANCY_DATE ORGANIZATION_ID ORGANIZATION_NAME  \n#>     <dbl> <date>                   <dbl> <chr>              \n#> 1 7272806 2021-01-01                  24 COSTI Immigrant Se…\n#> 2 7272807 2021-01-01                  24 COSTI Immigrant Se…\n#> 3 7272808 2021-01-01                  24 COSTI Immigrant Se…\n#> 4 7272809 2021-01-01                  24 COSTI Immigrant Se…\n#> 5 7272810 2021-01-01                  24 COSTI Immigrant Se…\n#> 6 7272811 2021-01-01                  24 COSTI Immigrant Se…\n#> # … with 28 more variables: SHELTER_ID <dbl>,\n#> #   SHELTER_GROUP <chr>, LOCATION_ID <dbl>,\n#> #   LOCATION_NAME <chr>, LOCATION_ADDRESS <chr>,\n#> #   LOCATION_POSTAL_CODE <chr>, LOCATION_CITY <chr>,\n#> #   LOCATION_PROVINCE <chr>, PROGRAM_ID <dbl>,\n#> #   PROGRAM_NAME <chr>, SECTOR <chr>, PROGRAM_MODEL <chr>,\n#> #   OVERNIGHT_SERVICE_TYPE <chr>, PROGRAM_AREA <chr>, …\ntoronto_shelters_clean <- \n  clean_names(toronto_shelters) |> \n  select(occupancy_date, id, occupied_beds) |> \n  filter(occupancy_date >= as_date(\"2021-07-01\"))\n\nhead(toronto_shelters_clean)\n#> # A tibble: 6 × 3\n#>   occupancy_date      id occupied_beds\n#>   <date>           <dbl>         <dbl>\n#> 1 2021-12-27     7323151            50\n#> 2 2021-12-27     7323152            18\n#> 3 2021-12-27     7323153            28\n#> 4 2021-12-27     7323154            50\n#> 5 2021-12-27     7323155            NA\n#> 6 2021-12-27     7323156            NA\nwrite_csv(\n  x = toronto_shelters_clean, \n  file = \"cleaned_toronto_shelters.csv\"\n  )"},{"path":"drinking-from-a-fire-hose.html","id":"explore-1","chapter":"2 Drinking from a fire hose","heading":"2.3.4 Explore","text":"First, load dataset just created.dataset daily basis shelter. interested understanding average usage month. , need add month column, using month() lubridate (Grolemund Wickham 2011). default, month() provides number month, include two arguments ‘label’ ‘abbr’ get full name month. remove rows data number beds using drop_na() tidyr. create summary statistic basis monthly groups, using summarize() dplyr (Wickham et al. 2020). use kable() knitr (Xie 2021) create table., looks fine, achieves set . can make tweaks defaults make look even better (Table 2.1). can add caption, make column names easier read, show appropriate level decimal places, improve formatting.Table 2.1: Homeless shelter usage Toronto 2021","code":"\n#### Explore ####\ntoronto_shelters_clean <- \n  read_csv(\n    \"cleaned_toronto_shelters.csv\",\n    show_col_types = FALSE\n    )\n# Based on code from Florence Vallée-Dubois and Lisa Lendway\ntoronto_shelters_clean |>\n  mutate(occupancy_month = month(occupancy_date, \n                                 label = TRUE, \n                                 abbr = FALSE)) |>\n  drop_na(occupied_beds) |> # We only want rows that have data\n  group_by(occupancy_month) |> # We want to know the occupancy by month\n  summarize(number_occupied = mean(occupied_beds)) |>\n  kable()\ntoronto_shelters_clean |>\n  mutate(occupancy_month = month(occupancy_date, \n                                 label = TRUE, \n                                 abbr = FALSE)) |>\n  drop_na(occupied_beds) |> # We only want rows that have data\n  group_by(occupancy_month) |> # We want to know the occupancy by month\n  summarize(number_occupied = mean(occupied_beds)) |> \n  kable(caption = \"Homeless shelter usage in Toronto in 2021\", \n        col.names = c(\"Month\", \"Average daily number of occupied beds\"),\n        digits = 1,\n        booktabs = TRUE,\n        linesep = \"\"\n        )"},{"path":"drinking-from-a-fire-hose.html","id":"communicate-1","chapter":"2 Drinking from a fire hose","heading":"2.3.5 Communicate","text":"need write brief paragraphs , , found. example follows.Toronto large homeless population. Freezing winters mean critical enough places shelters. interested understand usage shelters changes colder months, compared warmer months.use data provided City Toronto Toronto homeless shelter bed occupancy. Specifically, 4am night count made occupied beds. interested averaging month. cleaned, tidied, analyzed dataset using statistical programming language R (R Core Team 2021) well packages tidyverse (Wickham 2017), janitor (Firke 2020), tidyr (Wickham 2021c), opendatatoronto (Gelfand 2020), lubridate (Grolemund Wickham 2011), knitr (Xie 2021). made table average number occupied beds night month (Table 2.1).found daily average number occupied beds higher December 2021 July 2021, 34 occupied beds December, compared 30 July (Table 2.1). generally, steady increase daily average number occupied beds July December, slight increase month.dataset basis shelters, results may skewed changes specific especially large especially small shelters. may particular shelters especially attractive colder months. Additionally, concerned counts number occupied beds, supply beds changes season, additional statistic interest proportion occupied.Although example paragraphs, reduced form abstract, increased form full report. first paragraph general overview, second focuses data, third results, fourth discussion. expanded form sections short report.","code":""},{"path":"drinking-from-a-fire-hose.html","id":"neonatal-mortality","chapter":"2 Drinking from a fire hose","heading":"2.4 Neonatal mortality","text":"Neonatal mortality refers death occurs within first month life, particular, neonatal mortality rate (NMR) number neonatal deaths per 1,000 live births (UN IGME 2021). Reducing part third Sustainable Development Goal (Hug et al. 2019). example create graph estimated NMR past fifty years : Argentina, Australia, Canada, Kenya.","code":""},{"path":"drinking-from-a-fire-hose.html","id":"plan-2","chapter":"2 Drinking from a fire hose","heading":"2.4.1 Plan","text":"example, need think dataset look like, graph look like.dataset needs columns specify country, year. also needs column NMR estimate year country. Roughly, look like Figure 2.7.\nFigure 2.7: Quick sketch potentially useful NMR dataset\ninterested make graph year x-axis estimated NMR y-axis. country series Roughly similar Figure 2.8.\nFigure 2.8: Quick sketch graph NMR country time\n","code":""},{"path":"drinking-from-a-fire-hose.html","id":"simulate-2","chapter":"2 Drinking from a fire hose","heading":"2.4.2 Simulate","text":"like simulate data aligns plan. case need three columns: country, year, NMR.Within R Studio Cloud make new R Markdown file save . Add preamble documentation set-workspace. use tidyverse (Wickham 2017), janitor (Firke 2020), lubridate (Grolemund Wickham 2011).code contained libraries can change time time authors update release new versions. can see version package using packageVersion(). instance, using version 1.3.1 tidyverse version 2.1.0 janitor.update version package, use update.packages().need run, say, every day, time--time worth updating packages. many packages take care ensure backward compatibility, certain point become reasonable, important aware updating packages can result old code needing updated.Returning simulation, repeat name country 50 times rep(), enable passing 50 years. Finally, draw uniform distribution runif() simulate estimated NMR value year country.simulation works, time-consuming error-prone decided instead fifty years, interested simulating, say, sixty years. One way make easier replace instances 50 variable. example follows.result , now want change fifty sixty years make change one place.can confidence simulated dataset relatively straight-forward, wrote code . turn real dataset, difficult sure claims . Even trust data, important can share confidence others. One way forward establish checks prove data . instance, expect:‘country’ , , one four: ‘Argentina’, ‘Australia’, ‘Canada’, ‘Kenya’.Conversely, ‘country’ contains four countries.‘year’ smaller 1971 larger 2020, number, letter.‘nmr’ value somewhere 0 1,000, number.can write series tests based features, expect dataset pass.passed tests, can confidence simulated dataset. importantly, can apply tests real dataset. enables us greater confidence dataset share confidence others.","code":"\n#### Preamble ####\n# Purpose: Obtain and prepare data about neonatal mortality for four\n# countries for the past fifty years and create a graph.\n# Author: Rohan Alexander\n# Email: rohan.alexander@utoronto.ca\n# Date: 1 January 2022\n# Prerequisites: - \n\n#### Workspace set-up ####\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(tidyverse)\npackageVersion('tidyverse')\n#> [1] '1.3.1'\npackageVersion('janitor')\n#> [1] '2.1.0'\nupdate.packages()\n#### Simulate data ####\nset.seed(853)\n\nsimulated_nmr_data <- \n  tibble(\n    country = \n      c(\n        rep('Argentina', 50),\n        rep('Australia', 50),\n        rep('Canada', 50),\n        rep('Kenya', 50)\n        ),\n    year = \n      rep(c(1971:2020), 4),\n    nmr = \n      runif(n = 200,\n            min = 0, \n            max = 100)\n  )\n\nhead(simulated_nmr_data)\n#> # A tibble: 6 × 3\n#>   country    year   nmr\n#>   <chr>     <int> <dbl>\n#> 1 Argentina  1971 35.9 \n#> 2 Argentina  1972 12.0 \n#> 3 Argentina  1973 48.4 \n#> 4 Argentina  1974 31.6 \n#> 5 Argentina  1975  3.74\n#> 6 Argentina  1976 40.4\n#### Simulate data ####\nset.seed(853)\n\nnumber_of_years <- 50\n\nsimulated_nmr_data <- \n  tibble(\n    country = \n      c(\n        rep('Argentina', number_of_years),\n        rep('Australia', number_of_years),\n        rep('Canada', number_of_years),\n        rep('Kenya', number_of_years)\n        ),\n    year = \n      rep(c(1:number_of_years + 1970), 4),\n    nmr = \n      runif(n = number_of_years * 4,\n            min = 0, \n            max = 100)\n  )\n\nhead(simulated_nmr_data)\n#> # A tibble: 6 × 3\n#>   country    year   nmr\n#>   <chr>     <dbl> <dbl>\n#> 1 Argentina  1971 35.9 \n#> 2 Argentina  1972 12.0 \n#> 3 Argentina  1973 48.4 \n#> 4 Argentina  1974 31.6 \n#> 5 Argentina  1975  3.74\n#> 6 Argentina  1976 40.4\n# Tests for simulated data\nsimulated_nmr_data$country |> \n  unique() == c(\"Argentina\", \n                \"Australia\", \n                \"Canada\", \n                \"Kenya\")\n#> [1] TRUE TRUE TRUE TRUE\n\nsimulated_nmr_data$country |> unique() |> length() == 4\n#> [1] TRUE\n\nsimulated_nmr_data$year |> min() == 1971\n#> [1] TRUE\n\nsimulated_nmr_data$year |> max() == 2020\n#> [1] TRUE\n\nsimulated_nmr_data$nmr |> min() >= 0\n#> [1] TRUE\n\nsimulated_nmr_data$nmr |> max() <= 1000\n#> [1] TRUE\n\nsimulated_nmr_data$nmr |> class() == \"numeric\"\n#> [1] TRUE"},{"path":"drinking-from-a-fire-hose.html","id":"acquire-2","chapter":"2 Drinking from a fire hose","heading":"2.4.3 Acquire","text":"UN Inter-agency Group Child Mortality Estimation (IGME) provides estimates NMR – https://childmortality.org/ – can download save.can take quick look get better sense . might interested dataset seems look like (using head() tail()), names columns (using names()).like clean names keep rows columns interested . Based plan, interested rows ‘Sex’ ‘Total’, ‘Series Name’ ‘UN IGME estimate’, ‘Geographic area’ one ‘Argentina’, ‘Australia’, ‘Canada’, ‘Kenya’, ‘Indicator’ ‘Neonatal mortality rate’. interested just columns: ‘geographic_area’, ‘time_period’, ‘obs_value’.Finally, need fix two final aspects: class ‘time_period’ character need year, name ‘obs_value’ ‘nmr’ informative.Finally, can check dataset passes tests developed based simulated dataset.remains save nicely cleaned dataset.","code":"\n#### Acquire data ####\nraw_igme_data <- \n  read_csv(\n    file =\n      \"https://childmortality.org/wp-content/uploads/2021/09/UNIGME-2021.csv\",\n    show_col_types = FALSE) \n\nwrite_csv(\n  x = raw_igme_data, \n  file = \"igme.csv\"\n  )\nhead(raw_igme_data)\n#> # A tibble: 6 × 29\n#>   `Geographic area` Indicator         Sex   `Wealth Quinti…`\n#>   <chr>             <chr>             <chr> <chr>           \n#> 1 Afghanistan       Neonatal mortali… Total Total           \n#> 2 Afghanistan       Neonatal mortali… Total Total           \n#> 3 Afghanistan       Neonatal mortali… Total Total           \n#> 4 Afghanistan       Neonatal mortali… Total Total           \n#> 5 Afghanistan       Neonatal mortali… Total Total           \n#> 6 Afghanistan       Neonatal mortali… Total Total           \n#> # … with 25 more variables: `Series Name` <chr>,\n#> #   `Series Year` <chr>, `Regional group` <chr>,\n#> #   TIME_PERIOD <chr>, OBS_VALUE <dbl>,\n#> #   COUNTRY_NOTES <chr>, CONNECTION <lgl>,\n#> #   DEATH_CATEGORY <lgl>, CATEGORY <chr>,\n#> #   `Observation Status` <chr>, `Unit of measure` <chr>,\n#> #   `Series Category` <chr>, `Series Type` <chr>, …\nnames(raw_igme_data)\n#>  [1] \"Geographic area\"        \"Indicator\"             \n#>  [3] \"Sex\"                    \"Wealth Quintile\"       \n#>  [5] \"Series Name\"            \"Series Year\"           \n#>  [7] \"Regional group\"         \"TIME_PERIOD\"           \n#>  [9] \"OBS_VALUE\"              \"COUNTRY_NOTES\"         \n#> [11] \"CONNECTION\"             \"DEATH_CATEGORY\"        \n#> [13] \"CATEGORY\"               \"Observation Status\"    \n#> [15] \"Unit of measure\"        \"Series Category\"       \n#> [17] \"Series Type\"            \"STD_ERR\"               \n#> [19] \"REF_DATE\"               \"Age Group of Women\"    \n#> [21] \"Time Since First Birth\" \"DEFINITION\"            \n#> [23] \"INTERVAL\"               \"Series Method\"         \n#> [25] \"LOWER_BOUND\"            \"UPPER_BOUND\"           \n#> [27] \"STATUS\"                 \"YEAR_TO_ACHIEVE\"       \n#> [29] \"Model Used\"\ncleaned_igme_data <- \n  clean_names(raw_igme_data) |> \n  filter(sex == 'Total',\n         series_name == 'UN IGME estimate',\n         geographic_area %in% \n           c('Argentina', 'Australia', 'Canada', 'Kenya'),\n         indicator == 'Neonatal mortality rate') |> \n  select(geographic_area,\n         time_period,\n         obs_value)\n\nhead(cleaned_igme_data)\n#> # A tibble: 6 × 3\n#>   geographic_area time_period obs_value\n#>   <chr>           <chr>           <dbl>\n#> 1 Argentina       1970-06          24.9\n#> 2 Argentina       1971-06          24.7\n#> 3 Argentina       1972-06          24.6\n#> 4 Argentina       1973-06          24.6\n#> 5 Argentina       1974-06          24.5\n#> 6 Argentina       1975-06          24.1\ncleaned_igme_data <- \n  cleaned_igme_data |> \n  mutate(time_period = str_remove(time_period, \"-06\"),\n         time_period = as.integer(time_period)) |> \n  filter(time_period >= 1971) |> \n  rename(nmr = obs_value,\n         year = time_period,\n         country = geographic_area)\n\nhead(cleaned_igme_data)\n#> # A tibble: 6 × 3\n#>   country    year   nmr\n#>   <chr>     <int> <dbl>\n#> 1 Argentina  1971  24.7\n#> 2 Argentina  1972  24.6\n#> 3 Argentina  1973  24.6\n#> 4 Argentina  1974  24.5\n#> 5 Argentina  1975  24.1\n#> 6 Argentina  1976  23.3\n# Test the cleaned dataset\ncleaned_igme_data$country |> \n  unique() == c(\"Argentina\", \n                \"Australia\", \n                \"Canada\", \n                \"Kenya\")\n#> [1] TRUE TRUE TRUE TRUE\n\ncleaned_igme_data$country |> unique() |> length() == 4\n#> [1] TRUE\n\ncleaned_igme_data$year |> min() == 1971\n#> [1] TRUE\n\ncleaned_igme_data$year |> max() == 2020\n#> [1] TRUE\n\ncleaned_igme_data$nmr |> min() >= 0\n#> [1] TRUE\n\ncleaned_igme_data$nmr |> max() <= 1000\n#> [1] TRUE\n\ncleaned_igme_data$nmr |> class() == \"numeric\"\n#> [1] TRUE\nwrite_csv(\n  x = cleaned_igme_data, \n  file = \"cleaned_igme_data.csv\"\n  )"},{"path":"drinking-from-a-fire-hose.html","id":"explore-2","chapter":"2 Drinking from a fire hose","heading":"2.4.4 Explore","text":"like make graph estimated NMR using cleaned dataset. First, read dataset.can now make graph interested (Figure 2.9). interested showing NMR changed time difference countries.\nFigure 2.9: Neonatal Mortality Rate (NMR), Argentina, Australia, Canada, Kenya, (1971-2020)\n","code":"\n#### Explore ####\ncleaned_igme_data <- \n  read_csv(\n    file = \"cleaned_igme_data.csv\",\n    show_col_types = FALSE\n    )\ncleaned_igme_data |> \n  ggplot(aes(x = year, y = nmr, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Year\",\n       y = \"Neonatal Mortality Rate (NMR)\",\n       color = \"Country\") +\n  scale_color_brewer(palette = \"Set1\")"},{"path":"drinking-from-a-fire-hose.html","id":"communicate-2","chapter":"2 Drinking from a fire hose","heading":"2.4.5 Communicate","text":"point downloaded data, cleaned , wrote tests, made graph. typically need communicate done length. case, write paragraphs , , found.Neonatal mortality refers death occurs within first month life. particular, neonatal mortality rate (NMR) number neonatal deaths per 1,000 live births (M. Alexander Alkema 2018). obtain estimates NMR four countries—Argentina, Australia, Canada, China, Kenya—past fifty years.UN Inter-agency Group Child Mortality Estimation (IGME) provides estimates NMR website: https://childmortality.org/. downloaded estimates cleaned tidied dataset using statistical programming language R (R Core Team 2021).found considerable change estimated NMR time four countries interest (Figure 2.9). found 1970s tended associated reductions estimated NMR. Australia Canada estimated low NMR point remained 2020, slight improvements. estimates Argentina Kenya continued substantial reductions 2020. Data available 1990 China estimates show substantial reduction NMR, especially 1990s 2000s.results suggest considerable improvements estimated NMR time. worth emphasizing estimates NMR based statistical model underlying data. paradox data availability often high-quality data less easily available countries worse outcomes. instance, M. Alexander Alkema (2018) say ‘[t]large variability availability data neonatal mortality’. conclusions subject model underpins estimates, quality underlying data independently verify either .","code":""},{"path":"drinking-from-a-fire-hose.html","id":"exercises-and-tutorial-1","chapter":"2 Drinking from a fire hose","heading":"2.5 Exercises and tutorial","text":"","code":""},{"path":"drinking-from-a-fire-hose.html","id":"exercises-1","chapter":"2 Drinking from a fire hose","heading":"2.5.1 Exercises","text":"Following Barrett (2021a), please write stack four five atomic habits implement week.one four challenges mitigating bias mentioned Hao (2019) (pick one)?\nUnknown unknowns.\nImperfect processes.\ndefinitions fairness.\nLack social context.\nDisinterest given profit considerations.\nUnknown unknowns.Imperfect processes.definitions fairness.Lack social context.Disinterest given profit considerations.dataset underpins Chambliss (1989) collected (pick one)?\nAugust 1983 August 1984\nJanuary 1983 August 1984\nJanuary 1983 January 1984\nAugust 1983 January 1984\nAugust 1983 August 1984January 1983 August 1984January 1983 January 1984August 1983 January 1984When Chambliss (1989) talks stratification, talking ?Chambliss (1989) define ‘excellence’ (pick one)?\nProlonged performance world-class level.\nOlympic medal winners.\nConsistent superiority performance.\nnational-level athletes.\nProlonged performance world-class level.Olympic medal winners.Consistent superiority performance.national-level athletes.Think following quote Chambliss (1989, 81) list three small skills activities help achieve excellence data science.Excellence mundane. Superlative performance really confluence dozens small skills activities, one learned stumbled upon, carefully drilled habit fitted together synthesized whole. nothing extraordinary super-human one actions; fact done consistently correctly, together, produce excellence.following arguments read_csv() readr (Wickham, Hester, Bryan 2021) (select apply)? (Hint: can access help function ?readr::read_csv().)\n‘all_cols’\n‘file’\n‘show_col_types’\n‘number’\n‘all_cols’‘file’‘show_col_types’‘number’used rpois() runif() draw Poisson Uniform distributions, respectively. following can used draw Normal Binomial distributions (select apply)?\nrnormal() rbinom()\nrnorm() rbinomial()\nrnormal() rbinomial()\nrnorm() rbinom()\nrnormal() rbinom()rnorm() rbinomial()rnormal() rbinomial()rnorm() rbinom()result sample(x = letters, size = 2) seed set ‘853’? seed set ‘1234’ (pick one)?\n‘“” “q”’ ‘“p” “v”’\n‘“e” “l”’ ‘“e” “r”’\n‘“” “q”’ ‘“e” “r”’\n‘“e” “l”’ ‘“p” “v”’\n‘“” “q”’ ‘“p” “v”’‘“e” “l”’ ‘“e” “r”’‘“” “q”’ ‘“e” “r”’‘“e” “l”’ ‘“p” “v”’function provides recommended citation cite R (pick one)?\ncite('R').\ncite().\ncitation('R').\ncitation().\ncite('R').cite().citation('R').citation().get citation information opendatatoronto (pick one)?\ncite()\ncitation()\ncite(‘opendatatoronto’)\ncitation(‘opendatatoronto’)\ncite()citation()cite(‘opendatatoronto’)citation(‘opendatatoronto’)argument needs changed change headings kable() knitr (Xie 2021) (pick one)?\n‘booktabs’\n‘col.names’\n‘digits’\n‘linesep’\n‘caption’\n‘booktabs’‘col.names’‘digits’‘linesep’‘caption’function used update packages (pick one)?\nupdate.packages()\nupgrade.packages()\nrevise.packages()\nrenovate.packages()\nupdate.packages()upgrade.packages()revise.packages()renovate.packages()features might typically expect column claimed year (select apply)?\nclass ‘character’.\nnegative numbers.\nletters column.\nentry four digits.\nclass ‘character’.negative numbers.letters column.entry four digits.","code":""},{"path":"drinking-from-a-fire-hose.html","id":"tutorial-1","chapter":"2 Drinking from a fire hose","heading":"2.5.2 Tutorial","text":"Please pick either seed-growing, hair-length, example Chapter 1. already started record data, probably much . First sketch example dataset look like. Please use sample() create tibble twelve weeks’ worth simulated data.Pretend dataset just generated actual data end . Please write page communicate , , found.Reflecting Chambliss (1989), please write page stratification excellence relates using programming languages, R Python, data science.","code":""},{"path":"r-essentials.html","id":"r-essentials","chapter":"3 R essentials","heading":"3 R essentials","text":"Required materialRead Kitchen Counter Observatory, (Healy 2020)Read R Data Science, Chapter 5 ‘Data transformation’, (Wickham Grolemund 2017)Read Data Feminism, Chapter 6 ‘Numbers Don’t Speak ’, (D’Ignazio Klein 2020)Key concepts skillsUnderstanding foundational aspects R R Studio.able use key dplyr verbs.Know fundamentals class manipulate .Ability simulate data.Ability make graphs ggplot2.Comfort aspects tidyverse including importing data, dataset manipulation, string manipulation, factors.Develop strategies things work.Key librariesforcats (Wickham 2020a)ggplot2 (Wickham 2016)haven (Wickham Miller 2020)stringr (Wickham 2019e)tidyr (Wickham 2021c)tidyverse (Wickham et al. 2019a)Key functions| ‘’& ‘’|> ‘pipe’$ ‘extract’.character().integer()c()citation()class()dplyr::arrange()dplyr::case_when()dplyr::count()dplyr::filter()dplyr::group_by()dplyr::if_else()dplyr::left_join()dplyr::mutate()dplyr::pull()dplyr::rename()dplyr::select()dplyr::slice()dplyr::summarise()forcats::as_factor()forcats::fct_relevel()function()ggplot2::facet_wrap()ggplot2::geom_density()ggplot2::geom_histogram()ggplot2::geom_point()ggplot2::ggplot()head()janitor::clean_names()library()lubridate::ymd()max()mean()print()readr::read_csv()rnorm()round()runif()sample()set.seed()stringr::str_detect()stringr::str_replace()stringr::str_squish()sum()tibble::tibble()tidyr::pivot_longer()tidyr::pivot_wider()","code":""},{"path":"r-essentials.html","id":"background","chapter":"3 R essentials","heading":"3.1 Background","text":"chapter focus foundational skills needed use statistical programming language R (R Core Team 2021) tell stories data. may make sense first, skills approaches often use. initially just go chapter quickly, noting aspects understand. come back chapter time time continue rest book. way see various bits fit context.R open-source language statistical programming. can download R free Comprehensive R Archive Network (CRAN): https://cran.r-project.org. R Studio Integrated Development Environment (IDE) R makes language easier use can downloaded free: https://www.rstudio.com/products/rstudio/.past ten years , characterized increased use tidyverse. ‘…opinionated collection R packages designed data science. packages share underlying design philosophy, grammar, data structures’ (Wickham 2020b). three distinctions clear : original R language, typically referred ‘base’; ‘tidyverse’ coherent collection packages build top base, packages.Essentially everything can tidyverse, can also base. , tidyverse built especially data science often easier use tidyverse, especially learning. Additionally, often everything can tidyverse, can also packages. , tidyverse coherent collection packages, often easier use tidyverse, , especially learning. Eventually cases makes sense trade-convenience coherence tidyverse features base packages. Indeed, see various points later book. instance, tidyverse can slow, one needs import thousands CSVs can make sense switch away read_csv(). appropriate use base non-tidyverse packages, even languages, rather dogmatic insistence particular solution, sign intellectual maturity.Central use statistical programming language R data, data use humans heart . Sometimes, dealing human-centered data way can numbing effect, results -generalization, potentially problematic work. Another sign intellectual maturity opposite effect.practice, find far distancing questions meaning, quantitative data forces confront . numbers draw . Working data like unending exercise humility, constant compulsion think can see, standing invitation understand measures really capture—mean, .Healy (2020)","code":""},{"path":"r-essentials.html","id":"broader-impacts","chapter":"3 R essentials","heading":"3.2 Broader impacts","text":"“shouldn’t think societal impact work ’s hard people can us” really bad argument. stopped CV [computer vision] research saw impact work . loved work military applications privacy concerns eventually became impossible ignore. basically facial recognition work get published took Broader Impacts sections seriously. almost upside enormous downside risk. fair though lot humility . grad school bought myth science apolitical research objectively moral good matter subject .Joe Redmon, 20 February 2020Although term ‘data science’ ubiquitous academia, industry, even generally, difficult define. One deliberately antagonistic definition data science ‘[t]inhumane reduction humanity can counted’ (Keyes 2019). purposefully controversial, definition highlights one reason increased demand data science quantitative methods past decade—individuals behavior now heart . Many techniques around many decades, makes popular now human focus.Unfortunately, even though much work may focused individuals, issues privacy consent, ethical concerns broadly, rarely seem front mind. exceptions, general, even time claiming AI, machine learning, data science going revolutionize society, consideration types issues appears largely treated something nice , rather something may like think embrace revolution.part, types issues new. sciences, considerable recent ethical consideration around CRISPR technology gene editing (Brokowski Adli 2019), earlier time similar conversations , instance, Wernher von Braun allowed building rockets US (Neufeld 2002). medicine, course, concerns front--mind time (Association Medicine 1848). Data science seems determined Tuskegee-moment rather think , proactively deal appropriately , issues, based experiences fields.said, evidence data scientists beginning concerned ethics surrounding practice. instance, NeurIPS, prestigious machine learning conference, required statement ethics accompany submissions since 2020.order provide balanced perspective, authors required include statement potential broader impact work, including ethical aspects future societal consequences. Authors take care discuss positive negative outcomes.NeurIPS 2020 Conference Call PapersThe purpose ethical consideration concern broader impact data science prescriptively rule things , provide opportunity raise issues paramount. variety data science applications, relative youth field, speed change, mean considerations sometimes knowingly set aside, acceptable rest field. contrasts fields science, medicine, engineering, accounting. Possibly fields self-aware (Figure 3.1).\nFigure 3.1: Probability, XKCD\n","code":""},{"path":"r-essentials.html","id":"r-r-studio-and-r-studio-cloud","chapter":"3 R essentials","heading":"3.3 R, R Studio, and R Studio Cloud","text":"R R Studio complementary, thing. Dr Liza Bolton, Assistant Professor, Teaching Stream, University Toronto explains relationship analogy R like engine R Studio like car. Although us use car engine directly, us use car interact engine.","code":""},{"path":"r-essentials.html","id":"r","chapter":"3 R essentials","heading":"3.3.1 R","text":"R – https://www.r-project.org/ – open-source free programming language focused general statistics. Free context refer price zero, instead freedom creators give users largely want , although also price zero. contrast open-source programming language designed general purpose, Python, open-source programming language focused probability, Stan. created Ross Ihaka Robert Gentleman University Auckland 1990s, traces provenance S, developed Bell Labs 1970s. maintained R Core Team changes ‘base’ code occur methodically concern given variety different priorities.Many people build stable base, extend capabilities R better quickly suit needs. creating packages. Typically, although always, package collection R code, mostly functions, allows us easily things want . packages managed repositories CRAN Bioconductor.want use package first need install computer, need load want use . Dr Di Cook, Professor Business Analytics Monash University, describes analogous lightbulb. want light house, first need fit lightbulb, need turn switch . Installing package, say, install.packages(\"tidyverse\"), akin fitting lightbulb socket—need lightbulb. time want light need turn switch lightbulb, R packages case, means calling library, say, library(tidyverse).Shoulders giants Dr Di Cook Professor Business Analytics Monash University. taking PhD statistics Rutgers University 1993 focused statistical graphics, appointed assistant professor Iowa State University, promoted full professor 2005, 2015 moved Monash. One area research data visualisation, especially interactive dynamic graphics. One particularly important paper Buja, Cook, Swayne (1996) proposes taxonomy interactive data visualization associated software XGobi.install package computer (, need per computer) use install.packages().want use package, use library().downloaded , can open R use directly. primarily designed interacted command line. functional, can useful richer environment command line provides. particular, can useful install Integrated Development Environment (IDE), application brings together various bits pieces used often. One common IDE R R Studio, although others Visual Studio also used.","code":"\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)"},{"path":"r-essentials.html","id":"r-studio","chapter":"3 R essentials","heading":"3.3.2 R Studio","text":"R Studio distinct R, different entities. R Studio builds top R make easier use R. way one use internet command line, folks use browser Chrome, Firefox, Safari.R Studio free sense pay . also free sense able take code, modify , distribute code. important recognize R Studio company possible current situation change. can downloaded: https://www.rstudio.com/products/rstudio/.open R Studio look like Figure 3.2.\nFigure 3.2: Opening R Studio first time\nleft pane console can type execute R code line line. Try 2+2 clicking next prompt ‘>’, typing ‘2+2’, pressing ‘return/enter’.pane top right information environment. instance, create variables list names properties appear . Try type following code, replacing name name, next prompt, press enter:notice new value environment pane variable name value.pane bottom right file manager. moment just two files: R History file R Project file. get later, now create save file.Run following code, without worrying much details now. see new ‘.rds’ file list files.","code":"\n2 + 2\n#> [1] 4\nmy_name <- \"Rohan\"\nsaveRDS(object = my_name, file = \"my_first_file.rds\")"},{"path":"r-essentials.html","id":"r-studio-cloud","chapter":"3 R essentials","heading":"3.3.3 R Studio Cloud","text":"can download R Studio computer, initially use R Studio Cloud: https://rstudio.cloud/. online version provided R Studio. use can focus getting comfortable R R Studio environment consistent. way worry computer installation permissions, amongst things.free version R Studio Cloud free ‘financial cost’. trade-powerful, sometimes slow, purposes getting started enough.","code":""},{"path":"r-essentials.html","id":"getting-started","chapter":"3 R essentials","heading":"3.4 Getting started","text":"now start going code. important actively write .working line--line console fine, easier write whole script can run. making R Script (‘File’ -> ‘New File’ -> ‘R Script’). console pane fall bottom left R Script open top left. write code get Australian federal politicians construct small table genders prime ministers. code make sense stage, just type get habit run . run whole script, can click ‘Run’ can highlight certain lines click ‘Run’ just run lines.can see , end 2021, one female prime minister (Julia Gillard), 29 prime ministers maleOne critical operator programming ‘pipe’: |>. read ‘’. takes output line code uses first input next line code. makes code easier read.idea pipe take dataset, something . used earlier example. Another example follows look first six lines dataset piping head(). Notice head() explicitly take arguments example. knows data display pipe implicitly.can save R Script ‘my_first_r_script.R’ (‘File’ -> ‘Save ’). point, workspace look something like Figure 3.3.\nFigure 3.3: running R Script\nOne thing aware R Studio Cloud workspace essentially new computer. , need install package want use workspace. instance, can use tidyverse, need install install.packages(\"tidyverse\"). contrasts using one’s computer.final notes R Studio Cloud:Australian politician’s example, got data website GitHub using R package, can get data workspace local computer variety ways. One way use ‘upload’ button ‘Files’ panel.R Studio Cloud allows degree collaboration. instance, can give someone else access workspace create. useful collaborating assignment, although quite full featured yet workspace time, contrast , say, Google Docs.variety weaknesses R Studio Cloud, particular RAM limits. Additionally, like web application, things break time time go .","code":"\n# Install the packages that we need\ninstall.packages(\"tidyverse\")\ninstall.packages(\"AustralianPoliticians\")\n# Load the packages that we need to use this time\nlibrary(tidyverse)\nlibrary(AustralianPoliticians)\n\n# Make a table of the counts of genders of the prime ministers\nAustralianPoliticians::get_auspol('all') |> \n  as_tibble() |> \n  filter(wasPrimeMinister == 1) |> \n  count(gender)\n#> # A tibble: 2 × 2\n#>   gender     n\n#>   <chr>  <int>\n#> 1 female     1\n#> 2 male      29\nAustralianPoliticians::get_auspol('all') |> \n  head()\n#> # A tibble: 6 × 20\n#>   uniqueID   surname allOtherNames      firstName commonName\n#>   <chr>      <chr>   <chr>              <chr>     <chr>     \n#> 1 Abbott1859 Abbott  Richard Hartley S… Richard   <NA>      \n#> 2 Abbott1869 Abbott  Percy Phipps       Percy     <NA>      \n#> 3 Abbott1877 Abbott  Macartney          Macartney Mac       \n#> 4 Abbott1886 Abbott  Charles Lydiard A… Charles   Aubrey    \n#> 5 Abbott1891 Abbott  Joseph Palmer      Joseph    <NA>      \n#> 6 Abbott1957 Abbott  Anthony John       Anthony   Tony      \n#> # … with 15 more variables: displayName <chr>,\n#> #   earlierOrLaterNames <chr>, title <chr>, gender <chr>,\n#> #   birthDate <date>, birthYear <dbl>, birthPlace <chr>,\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>, wikidataID <chr>,\n#> #   wikipedia <chr>, adb <chr>, comments <chr>"},{"path":"r-essentials.html","id":"the-dplyr-verbs","chapter":"3 R essentials","heading":"3.5 The dplyr verbs","text":"One key packages use tidyverse (Wickham et al. 2019b). tidyverse actually package packages, means install tidyverse, actually install whole bunch different packages. key package tidyverse terms manipulating data dplyr (Wickham et al. 2020).five dplyr functions regularly used, now go . commonly referred dplyr verbs.select()filter()arrange()mutate()summarise() equally summarize()also cover group_by(), count() closely related.already installed tidyverse, just need load .begin using data Australian politicians AustralianPoliticians package (R. Alexander Hodgetts 2021).","code":"\nlibrary(tidyverse)\nlibrary(AustralianPoliticians)\n\naustralian_politicians <- \n  get_auspol('all')\n\nhead(australian_politicians)\n#> # A tibble: 6 × 20\n#>   uniqueID   surname allOtherNames      firstName commonName\n#>   <chr>      <chr>   <chr>              <chr>     <chr>     \n#> 1 Abbott1859 Abbott  Richard Hartley S… Richard   <NA>      \n#> 2 Abbott1869 Abbott  Percy Phipps       Percy     <NA>      \n#> 3 Abbott1877 Abbott  Macartney          Macartney Mac       \n#> 4 Abbott1886 Abbott  Charles Lydiard A… Charles   Aubrey    \n#> 5 Abbott1891 Abbott  Joseph Palmer      Joseph    <NA>      \n#> 6 Abbott1957 Abbott  Anthony John       Anthony   Tony      \n#> # … with 15 more variables: displayName <chr>,\n#> #   earlierOrLaterNames <chr>, title <chr>, gender <chr>,\n#> #   birthDate <date>, birthYear <dbl>, birthPlace <chr>,\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>, wikidataID <chr>,\n#> #   wikipedia <chr>, adb <chr>, comments <chr>"},{"path":"r-essentials.html","id":"select","chapter":"3 R essentials","heading":"3.5.1 select()","text":"use select() pick particular columns dataset. instance, might like select ‘firstName’ column.R, many ways things. Sometimes different ways thing, times different ways almost thing. instance, another way pick particular column dataset use ‘extract’ operator ‘$’. base, opposed select() tidyverse.two appear similar—pick ‘firstName’ column—differ class return. sake completeness, combine select() pull() get class output used extract operator.can also use select() remove columns, negating column name.Finally, can select() based conditions. instance, can select() columns start , say, ‘birth’.variety similar ‘selection helpers’ including starts_with(), ends_with(), contains(). information available help page select() can accessed running ?select().point, use select() reduce width dataset.","code":"\naustralian_politicians |> \n  select(firstName) |> \n  head()\n#> # A tibble: 6 × 1\n#>   firstName\n#>   <chr>    \n#> 1 Richard  \n#> 2 Percy    \n#> 3 Macartney\n#> 4 Charles  \n#> 5 Joseph   \n#> 6 Anthony\naustralian_politicians$firstName |> \n  head()\n#> [1] \"Richard\"   \"Percy\"     \"Macartney\" \"Charles\"  \n#> [5] \"Joseph\"    \"Anthony\"\naustralian_politicians |> \n  select(firstName) |> \n  pull() |> \n  head()\n#> [1] \"Richard\"   \"Percy\"     \"Macartney\" \"Charles\"  \n#> [5] \"Joseph\"    \"Anthony\"\naustralian_politicians |> \n  select(-firstName) |> \n  head()\n#> # A tibble: 6 × 19\n#>   uniqueID   surname allOtherNames    commonName displayName\n#>   <chr>      <chr>   <chr>            <chr>      <chr>      \n#> 1 Abbott1859 Abbott  Richard Hartley… <NA>       Abbott, Ri…\n#> 2 Abbott1869 Abbott  Percy Phipps     <NA>       Abbott, Pe…\n#> 3 Abbott1877 Abbott  Macartney        Mac        Abbott, Mac\n#> 4 Abbott1886 Abbott  Charles Lydiard… Aubrey     Abbott, Au…\n#> 5 Abbott1891 Abbott  Joseph Palmer    <NA>       Abbott, Jo…\n#> 6 Abbott1957 Abbott  Anthony John     Tony       Abbott, To…\n#> # … with 14 more variables: earlierOrLaterNames <chr>,\n#> #   title <chr>, gender <chr>, birthDate <date>,\n#> #   birthYear <dbl>, birthPlace <chr>, deathDate <date>,\n#> #   member <dbl>, senator <dbl>, wasPrimeMinister <dbl>,\n#> #   wikidataID <chr>, wikipedia <chr>, adb <chr>,\n#> #   comments <chr>\naustralian_politicians |> \n  select(starts_with(\"birth\")) |> \n  head()\n#> # A tibble: 6 × 3\n#>   birthDate  birthYear birthPlace  \n#>   <date>         <dbl> <chr>       \n#> 1 NA              1859 Bendigo     \n#> 2 1869-05-14        NA Hobart      \n#> 3 1877-07-03        NA Murrurundi  \n#> 4 1886-01-04        NA St Leonards \n#> 5 1891-10-18        NA North Sydney\n#> 6 1957-11-04        NA London\naustralian_politicians <-\n  australian_politicians |>\n  select(uniqueID,\n         surname,\n         firstName,\n         gender,\n         birthDate,\n         birthYear,\n         deathDate,\n         member,\n         senator,\n         wasPrimeMinister)\n\naustralian_politicians |> head()\n#> # A tibble: 6 × 10\n#>   uniqueID   surname firstName gender birthDate  birthYear\n#>   <chr>      <chr>   <chr>     <chr>  <date>         <dbl>\n#> 1 Abbott1859 Abbott  Richard   male   NA              1859\n#> 2 Abbott1869 Abbott  Percy     male   1869-05-14        NA\n#> 3 Abbott1877 Abbott  Macartney male   1877-07-03        NA\n#> 4 Abbott1886 Abbott  Charles   male   1886-01-04        NA\n#> 5 Abbott1891 Abbott  Joseph    male   1891-10-18        NA\n#> 6 Abbott1957 Abbott  Anthony   male   1957-11-04        NA\n#> # … with 4 more variables: deathDate <date>, member <dbl>,\n#> #   senator <dbl>, wasPrimeMinister <dbl>"},{"path":"r-essentials.html","id":"filter","chapter":"3 R essentials","heading":"3.5.2 filter()","text":"use filter() pick particular rows dataset. instance, might interested politicians became prime minister.also give filter() two conditions. instance, look politicians become prime minister named Joseph, using ‘’ operator ‘&’.get result use comma instead ampersand.Similarly, look politicians named, say, Myles Ruth using ‘’ operator ‘|’also pipe result. instance pipe filter() select().happen know particular row number interest filter() particular row. instance, say row 853 interest.also dedicated function , slice().may seem somewhat esoteric, especially useful like remove particular row using negation, duplicate specific rows. instance, remove first row.also , say, keep first three rows.Finally, duplicate first two rows.","code":"\naustralian_politicians |> \n  filter(wasPrimeMinister == 1)\n#> # A tibble: 30 × 10\n#>    uniqueID    surname firstName gender birthDate  birthYear\n#>    <chr>       <chr>   <chr>     <chr>  <date>         <dbl>\n#>  1 Abbott1957  Abbott  Anthony   male   1957-11-04        NA\n#>  2 Barton1849  Barton  Edmund    male   1849-01-18        NA\n#>  3 Bruce1883   Bruce   Stanley   male   1883-04-15        NA\n#>  4 Chifley1885 Chifley Joseph    male   1885-09-22        NA\n#>  5 Cook1860    Cook    Joseph    male   1860-12-07        NA\n#>  6 Curtin1885  Curtin  John      male   1885-01-08        NA\n#>  7 Deakin1856  Deakin  Alfred    male   1856-08-03        NA\n#>  8 Fadden1894  Fadden  Arthur    male   1894-04-13        NA\n#>  9 Fisher1862  Fisher  Andrew    male   1862-08-29        NA\n#> 10 Forde1890   Forde   Francis   male   1890-07-18        NA\n#> # … with 20 more rows, and 4 more variables:\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>\naustralian_politicians |> \n  filter(wasPrimeMinister == 1 & firstName == \"Joseph\")\n#> # A tibble: 3 × 10\n#>   uniqueID    surname firstName gender birthDate  birthYear\n#>   <chr>       <chr>   <chr>     <chr>  <date>         <dbl>\n#> 1 Chifley1885 Chifley Joseph    male   1885-09-22        NA\n#> 2 Cook1860    Cook    Joseph    male   1860-12-07        NA\n#> 3 Lyons1879   Lyons   Joseph    male   1879-09-15        NA\n#> # … with 4 more variables: deathDate <date>, member <dbl>,\n#> #   senator <dbl>, wasPrimeMinister <dbl>\naustralian_politicians |> \n  filter(wasPrimeMinister == 1, firstName == \"Joseph\")\n#> # A tibble: 3 × 10\n#>   uniqueID    surname firstName gender birthDate  birthYear\n#>   <chr>       <chr>   <chr>     <chr>  <date>         <dbl>\n#> 1 Chifley1885 Chifley Joseph    male   1885-09-22        NA\n#> 2 Cook1860    Cook    Joseph    male   1860-12-07        NA\n#> 3 Lyons1879   Lyons   Joseph    male   1879-09-15        NA\n#> # … with 4 more variables: deathDate <date>, member <dbl>,\n#> #   senator <dbl>, wasPrimeMinister <dbl>\naustralian_politicians |> \n  filter(firstName == \"Myles\" | firstName == \"Ruth\")\n#> # A tibble: 3 × 10\n#>   uniqueID     surname firstName gender birthDate  birthYear\n#>   <chr>        <chr>   <chr>     <chr>  <date>         <dbl>\n#> 1 Coleman1931  Coleman Ruth      female 1931-09-27        NA\n#> 2 Ferricks1875 Ferric… Myles     male   1875-11-12        NA\n#> 3 Webber1965   Webber  Ruth      female 1965-03-24        NA\n#> # … with 4 more variables: deathDate <date>, member <dbl>,\n#> #   senator <dbl>, wasPrimeMinister <dbl>\naustralian_politicians |> \n  filter(firstName == \"Ruth\" | firstName == \"Myles\") |> \n  select(firstName, surname)\n#> # A tibble: 3 × 2\n#>   firstName surname \n#>   <chr>     <chr>   \n#> 1 Ruth      Coleman \n#> 2 Myles     Ferricks\n#> 3 Ruth      Webber\naustralian_politicians |> \n  filter(row_number() == 853)\n#> # A tibble: 1 × 10\n#>   uniqueID     surname firstName gender birthDate  birthYear\n#>   <chr>        <chr>   <chr>     <chr>  <date>         <dbl>\n#> 1 Jakobsen1947 Jakobs… Carolyn   female 1947-09-11        NA\n#> # … with 4 more variables: deathDate <date>, member <dbl>,\n#> #   senator <dbl>, wasPrimeMinister <dbl>\naustralian_politicians |> \n  slice(853)\n#> # A tibble: 1 × 10\n#>   uniqueID     surname firstName gender birthDate  birthYear\n#>   <chr>        <chr>   <chr>     <chr>  <date>         <dbl>\n#> 1 Jakobsen1947 Jakobs… Carolyn   female 1947-09-11        NA\n#> # … with 4 more variables: deathDate <date>, member <dbl>,\n#> #   senator <dbl>, wasPrimeMinister <dbl>\naustralian_politicians |> \n  slice(-1)\n#> # A tibble: 1,782 × 10\n#>    uniqueID    surname firstName gender birthDate  birthYear\n#>    <chr>       <chr>   <chr>     <chr>  <date>         <dbl>\n#>  1 Abbott1869  Abbott  Percy     male   1869-05-14        NA\n#>  2 Abbott1877  Abbott  Macartney male   1877-07-03        NA\n#>  3 Abbott1886  Abbott  Charles   male   1886-01-04        NA\n#>  4 Abbott1891  Abbott  Joseph    male   1891-10-18        NA\n#>  5 Abbott1957  Abbott  Anthony   male   1957-11-04        NA\n#>  6 Abel1939    Abel    John      male   1939-06-25        NA\n#>  7 Abetz1958   Abetz   Eric      male   1958-01-25        NA\n#>  8 Adams1943   Adams   Judith    female 1943-04-11        NA\n#>  9 Adams1951   Adams   Dick      male   1951-04-29        NA\n#> 10 Adamson1857 Adamson John      male   1857-02-18        NA\n#> # … with 1,772 more rows, and 4 more variables:\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>\naustralian_politicians |> \n  slice(1:3)\n#> # A tibble: 3 × 10\n#>   uniqueID   surname firstName gender birthDate  birthYear\n#>   <chr>      <chr>   <chr>     <chr>  <date>         <dbl>\n#> 1 Abbott1859 Abbott  Richard   male   NA              1859\n#> 2 Abbott1869 Abbott  Percy     male   1869-05-14        NA\n#> 3 Abbott1877 Abbott  Macartney male   1877-07-03        NA\n#> # … with 4 more variables: deathDate <date>, member <dbl>,\n#> #   senator <dbl>, wasPrimeMinister <dbl>\naustralian_politicians |> \n  slice(1:2, 1:n())\n#> # A tibble: 1,785 × 10\n#>    uniqueID   surname firstName gender birthDate  birthYear\n#>    <chr>      <chr>   <chr>     <chr>  <date>         <dbl>\n#>  1 Abbott1859 Abbott  Richard   male   NA              1859\n#>  2 Abbott1869 Abbott  Percy     male   1869-05-14        NA\n#>  3 Abbott1859 Abbott  Richard   male   NA              1859\n#>  4 Abbott1869 Abbott  Percy     male   1869-05-14        NA\n#>  5 Abbott1877 Abbott  Macartney male   1877-07-03        NA\n#>  6 Abbott1886 Abbott  Charles   male   1886-01-04        NA\n#>  7 Abbott1891 Abbott  Joseph    male   1891-10-18        NA\n#>  8 Abbott1957 Abbott  Anthony   male   1957-11-04        NA\n#>  9 Abel1939   Abel    John      male   1939-06-25        NA\n#> 10 Abetz1958  Abetz   Eric      male   1958-01-25        NA\n#> # … with 1,775 more rows, and 4 more variables:\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>"},{"path":"r-essentials.html","id":"arrange","chapter":"3 R essentials","heading":"3.5.3 arrange()","text":"use arrange() change order dataset based values particular columns. instance, arrange politicians birthday.modify arrange() desc() change ascending descending order.arrange based one column. instance, two politicians first name, arrange based birthday.achieve result piping two instances arrange().use arrange() important clear precedence. instance, changing birthday first name give different arrangement.nice way arrange variety columns use across(). enables us use ‘selection helpers’ starts_with() mentioned association select().","code":"\naustralian_politicians |> \n  arrange(birthDate)\n#> # A tibble: 1,783 × 10\n#>    uniqueID    surname firstName gender birthDate  birthYear\n#>    <chr>       <chr>   <chr>     <chr>  <date>         <dbl>\n#>  1 Braddon1829 Braddon Edward    male   1829-06-11        NA\n#>  2 Ferguson18… Fergus… John      male   1830-03-15        NA\n#>  3 Zeal1830    Zeal    William   male   1830-12-05        NA\n#>  4 Fraser1832  Fraser  Simon     male   1832-08-21        NA\n#>  5 Groom1833   Groom   William   male   1833-03-09        NA\n#>  6 Sargood1834 Sargood Frederick male   1834-05-30        NA\n#>  7 Fysh1835    Fysh    Philip    male   1835-03-01        NA\n#>  8 Playford18… Playfo… Thomas    male   1837-11-26        NA\n#>  9 Solomon1839 Solomon Elias     male   1839-09-02        NA\n#> 10 McLean1840  McLean  Allan     male   1840-02-03        NA\n#> # … with 1,773 more rows, and 4 more variables:\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>\naustralian_politicians |> \n  arrange(desc(birthDate))\n#> # A tibble: 1,783 × 10\n#>    uniqueID    surname firstName gender birthDate  birthYear\n#>    <chr>       <chr>   <chr>     <chr>  <date>         <dbl>\n#>  1 SteeleJohn… Steele… Jordon    male   1994-10-14        NA\n#>  2 Chandler19… Chandl… Claire    female 1990-06-01        NA\n#>  3 Roy1990     Roy     Wyatt     male   1990-05-22        NA\n#>  4 Thompson19… Thomps… Phillip   male   1988-05-07        NA\n#>  5 Paterson19… Paters… James     male   1987-11-21        NA\n#>  6 Burns1987   Burns   Joshua    male   1987-02-06        NA\n#>  7 Smith1986   Smith   Marielle  female 1986-12-30        NA\n#>  8 KakoschkeM… Kakosc… Skye      female 1985-12-19        NA\n#>  9 Simmonds19… Simmon… Julian    male   1985-08-29        NA\n#> 10 Gorman1984  Gorman  Patrick   male   1984-12-12        NA\n#> # … with 1,773 more rows, and 4 more variables:\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>\naustralian_politicians |> \n  arrange(firstName, birthDate)\n#> # A tibble: 1,783 × 10\n#>    uniqueID    surname firstName gender birthDate  birthYear\n#>    <chr>       <chr>   <chr>     <chr>  <date>         <dbl>\n#>  1 Blain1894   Blain   Adair     male   1894-11-21        NA\n#>  2 Dein1889    Dein    Adam      male   1889-03-04        NA\n#>  3 Armstrong1… Armstr… Adam      male   1909-07-01        NA\n#>  4 Bandt1972   Bandt   Adam      male   1972-03-11        NA\n#>  5 Ridgeway19… Ridgew… Aden      male   1962-09-18        NA\n#>  6 Bennett1933 Bennett Adrian    male   1933-01-21        NA\n#>  7 Gibson1935  Gibson  Adrian    male   1935-11-03        NA\n#>  8 Wynne1850   Wynne   Agar      male   1850-01-15        NA\n#>  9 Robertson1… Robert… Agnes     female 1882-07-31        NA\n#> 10 Pittard1902 Pittard Alan      male   1902-11-15        NA\n#> # … with 1,773 more rows, and 4 more variables:\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>\naustralian_politicians |> \n  arrange(birthDate) |> \n  arrange(firstName)\n#> # A tibble: 1,783 × 10\n#>    uniqueID    surname firstName gender birthDate  birthYear\n#>    <chr>       <chr>   <chr>     <chr>  <date>         <dbl>\n#>  1 Blain1894   Blain   Adair     male   1894-11-21        NA\n#>  2 Dein1889    Dein    Adam      male   1889-03-04        NA\n#>  3 Armstrong1… Armstr… Adam      male   1909-07-01        NA\n#>  4 Bandt1972   Bandt   Adam      male   1972-03-11        NA\n#>  5 Ridgeway19… Ridgew… Aden      male   1962-09-18        NA\n#>  6 Bennett1933 Bennett Adrian    male   1933-01-21        NA\n#>  7 Gibson1935  Gibson  Adrian    male   1935-11-03        NA\n#>  8 Wynne1850   Wynne   Agar      male   1850-01-15        NA\n#>  9 Robertson1… Robert… Agnes     female 1882-07-31        NA\n#> 10 Pittard1902 Pittard Alan      male   1902-11-15        NA\n#> # … with 1,773 more rows, and 4 more variables:\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>\naustralian_politicians |> \n  arrange(birthYear, firstName)\n#> # A tibble: 1,783 × 10\n#>    uniqueID    surname firstName gender birthDate birthYear\n#>    <chr>       <chr>   <chr>     <chr>  <date>        <dbl>\n#>  1 Edwards1842 Edwards Richard   male   NA             1842\n#>  2 Sawers1844  Sawers  William   male   NA             1844\n#>  3 Barker1846  Barker  Stephen   male   NA             1846\n#>  4 Corser1852  Corser  Edward    male   NA             1852\n#>  5 Lee1856     Lee     Henry     male   NA             1856\n#>  6 Grant1857   Grant   John      male   NA             1857\n#>  7 Palmer1859  Palmer  Albert    male   NA             1859\n#>  8 Riley1859   Riley   Edward    male   NA             1859\n#>  9 Abbott1859  Abbott  Richard   male   NA             1859\n#> 10 Kennedy1860 Kennedy Thomas    male   NA             1860\n#> # … with 1,773 more rows, and 4 more variables:\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>\naustralian_politicians |> \n  arrange(across(c(firstName, birthYear))) |> \n  head()\n#> # A tibble: 6 × 10\n#>   uniqueID     surname firstName gender birthDate  birthYear\n#>   <chr>        <chr>   <chr>     <chr>  <date>         <dbl>\n#> 1 Blain1894    Blain   Adair     male   1894-11-21        NA\n#> 2 Armstrong19… Armstr… Adam      male   1909-07-01        NA\n#> 3 Bandt1972    Bandt   Adam      male   1972-03-11        NA\n#> 4 Dein1889     Dein    Adam      male   1889-03-04        NA\n#> 5 Ridgeway1962 Ridgew… Aden      male   1962-09-18        NA\n#> 6 Bennett1933  Bennett Adrian    male   1933-01-21        NA\n#> # … with 4 more variables: deathDate <date>, member <dbl>,\n#> #   senator <dbl>, wasPrimeMinister <dbl>\n\naustralian_politicians |> \n  arrange(across(starts_with('birth'))) |> \n  head()\n#> # A tibble: 6 × 10\n#>   uniqueID     surname firstName gender birthDate  birthYear\n#>   <chr>        <chr>   <chr>     <chr>  <date>         <dbl>\n#> 1 Braddon1829  Braddon Edward    male   1829-06-11        NA\n#> 2 Ferguson1830 Fergus… John      male   1830-03-15        NA\n#> 3 Zeal1830     Zeal    William   male   1830-12-05        NA\n#> 4 Fraser1832   Fraser  Simon     male   1832-08-21        NA\n#> 5 Groom1833    Groom   William   male   1833-03-09        NA\n#> 6 Sargood1834  Sargood Frederick male   1834-05-30        NA\n#> # … with 4 more variables: deathDate <date>, member <dbl>,\n#> #   senator <dbl>, wasPrimeMinister <dbl>"},{"path":"r-essentials.html","id":"mutate","chapter":"3 R essentials","heading":"3.5.4 mutate()","text":"use mutate() want make new column. instance, perhaps want make new column 1 person member senator 0 otherwise. say new column denote politicians served upper lower house.use mutate() math, addition subtraction. instance, calculate age politicians () 2022.variety functions especially useful constructing new columns. include log() compute natural logarithm, lead() bring values one row, lag() push values one row, cumsum() creates cumulative sum column.earlier examples, can also use mutate() combination across(). includes potential use selection helpers. instance, count number characters first last names time.Finally, use case_when() need make new column basis two conditional statements. instance, may years want group decades.accomplish series if_else() statements, case_when() clear. cases evaluated order soon match case_when() continue remainder cases. can useful catch-end signal potential issue might like know .","code":"\naustralian_politicians <- \n  australian_politicians |> \n  mutate(was_both = if_else(member == 1 & senator == 1, 1, 0))\n\naustralian_politicians |> \n  select(member, senator, was_both)\n#> # A tibble: 1,783 × 3\n#>    member senator was_both\n#>     <dbl>   <dbl>    <dbl>\n#>  1      0       1        0\n#>  2      1       1        1\n#>  3      0       1        0\n#>  4      1       0        0\n#>  5      1       0        0\n#>  6      1       0        0\n#>  7      1       0        0\n#>  8      0       1        0\n#>  9      0       1        0\n#> 10      1       0        0\n#> # … with 1,773 more rows\naustralian_politicians <- \n  australian_politicians |> \n  mutate(age = 2022 - lubridate::year(birthDate))\n\naustralian_politicians |> \n  select(uniqueID, age)\n#> # A tibble: 1,783 × 2\n#>    uniqueID     age\n#>    <chr>      <dbl>\n#>  1 Abbott1859    NA\n#>  2 Abbott1869   153\n#>  3 Abbott1877   145\n#>  4 Abbott1886   136\n#>  5 Abbott1891   131\n#>  6 Abbott1957    65\n#>  7 Abel1939      83\n#>  8 Abetz1958     64\n#>  9 Adams1943     79\n#> 10 Adams1951     71\n#> # … with 1,773 more rows\naustralian_politicians |> \n  select(uniqueID, age) |> \n  mutate(log_age = log(age)) |> \n  head()\n#> # A tibble: 6 × 3\n#>   uniqueID     age log_age\n#>   <chr>      <dbl>   <dbl>\n#> 1 Abbott1859    NA   NA   \n#> 2 Abbott1869   153    5.03\n#> 3 Abbott1877   145    4.98\n#> 4 Abbott1886   136    4.91\n#> 5 Abbott1891   131    4.88\n#> 6 Abbott1957    65    4.17\n\naustralian_politicians |> \n  select(uniqueID, age) |> \n  mutate(lead_age = lead(age)) |> \n  head()\n#> # A tibble: 6 × 3\n#>   uniqueID     age lead_age\n#>   <chr>      <dbl>    <dbl>\n#> 1 Abbott1859    NA      153\n#> 2 Abbott1869   153      145\n#> 3 Abbott1877   145      136\n#> 4 Abbott1886   136      131\n#> 5 Abbott1891   131       65\n#> 6 Abbott1957    65       83\n\naustralian_politicians |> \n  select(uniqueID, age) |> \n  mutate(lag_age = lag(age)) |> \n  head()\n#> # A tibble: 6 × 3\n#>   uniqueID     age lag_age\n#>   <chr>      <dbl>   <dbl>\n#> 1 Abbott1859    NA      NA\n#> 2 Abbott1869   153      NA\n#> 3 Abbott1877   145     153\n#> 4 Abbott1886   136     145\n#> 5 Abbott1891   131     136\n#> 6 Abbott1957    65     131\n\naustralian_politicians |> \n  select(uniqueID, age) |> \n  filter(!is.na(age)) |> \n  mutate(cumulative_age = cumsum(age)) |> \n  head()\n#> # A tibble: 6 × 3\n#>   uniqueID     age cumulative_age\n#>   <chr>      <dbl>          <dbl>\n#> 1 Abbott1869   153            153\n#> 2 Abbott1877   145            298\n#> 3 Abbott1886   136            434\n#> 4 Abbott1891   131            565\n#> 5 Abbott1957    65            630\n#> 6 Abel1939      83            713\naustralian_politicians |> \n  mutate(across(c(firstName, surname), str_count)) |> \n  select(uniqueID, firstName, surname)\n#> # A tibble: 1,783 × 3\n#>    uniqueID   firstName surname\n#>    <chr>          <int>   <int>\n#>  1 Abbott1859         7       6\n#>  2 Abbott1869         5       6\n#>  3 Abbott1877         9       6\n#>  4 Abbott1886         7       6\n#>  5 Abbott1891         6       6\n#>  6 Abbott1957         7       6\n#>  7 Abel1939           4       4\n#>  8 Abetz1958          4       5\n#>  9 Adams1943          6       5\n#> 10 Adams1951          4       5\n#> # … with 1,773 more rows\naustralian_politicians |> \n  mutate(year_of_birth = lubridate::year(birthDate),\n         decade_of_birth = \n           case_when(\n             year_of_birth <= 1929 ~ \"pre-1930\",\n             year_of_birth <= 1939 ~ \"1930s\",\n             year_of_birth <= 1949 ~ \"1940s\",\n             year_of_birth <= 1959 ~ \"1950s\",\n             year_of_birth <= 1969 ~ \"1960s\",\n             year_of_birth <= 1979 ~ \"1970s\",\n             year_of_birth <= 1989 ~ \"1980s\",\n             TRUE ~ \"Unknown or error\"\n             )\n  ) |> \n  select(uniqueID, year_of_birth, decade_of_birth)\n#> # A tibble: 1,783 × 3\n#>    uniqueID   year_of_birth decade_of_birth \n#>    <chr>              <dbl> <chr>           \n#>  1 Abbott1859            NA Unknown or error\n#>  2 Abbott1869          1869 pre-1930        \n#>  3 Abbott1877          1877 pre-1930        \n#>  4 Abbott1886          1886 pre-1930        \n#>  5 Abbott1891          1891 pre-1930        \n#>  6 Abbott1957          1957 1950s           \n#>  7 Abel1939            1939 1930s           \n#>  8 Abetz1958           1958 1950s           \n#>  9 Adams1943           1943 1940s           \n#> 10 Adams1951           1951 1950s           \n#> # … with 1,773 more rows"},{"path":"r-essentials.html","id":"summarise","chapter":"3 R essentials","heading":"3.5.5 summarise()","text":"use summarise() like make new, condensed, summary variables. instance, perhaps like know minimum, average, maximum column.aside, summarise() summarize() equivalent can use either.default, summarise() provide one row output whole dataset. instance, earlier example found youngest, oldest, average across politicians. However, can create groups dataset using group_by(). can apply another function within context groups. use many functions basis groups, summarise() function particularly powerful conjunction group_by(). instance, group gender, get age-based summary statistics.Similarly, look youngest, oldest, mean age death gender.learn female members parliament average lived slightly longer male members parliament.can use group_by() basis one group. instance, look average number days lived gender house.can use count() create counts groups. instance, number politicians gender.addition count(), make proportion.Using count() essentially using group_by() summarise(), get result way.similarly helpful function mutate(), add_count(). difference number added column.","code":"\naustralian_politicians |> \n  summarise(youngest = min(age, na.rm = TRUE),\n            oldest = max(age, na.rm = TRUE),\n            average = mean(age, na.rm = TRUE))\n#> # A tibble: 1 × 3\n#>   youngest oldest average\n#>      <dbl>  <dbl>   <dbl>\n#> 1       28    193    101.\naustralian_politicians |> \n  summarize(youngest = min(age, na.rm = TRUE),\n            oldest = max(age, na.rm = TRUE),\n            average = mean(age, na.rm = TRUE))\n#> # A tibble: 1 × 3\n#>   youngest oldest average\n#>      <dbl>  <dbl>   <dbl>\n#> 1       28    193    101.\naustralian_politicians |> \n  group_by(gender) |> \n  summarise(youngest = min(age, na.rm = TRUE),\n            oldest = max(age, na.rm = TRUE),\n            average = mean(age, na.rm = TRUE))\n#> # A tibble: 2 × 4\n#>   gender youngest oldest average\n#>   <chr>     <dbl>  <dbl>   <dbl>\n#> 1 female       32    140    66.0\n#> 2 male         28    193   106.\naustralian_politicians |>\n  mutate(days_lived = deathDate - birthDate) |> \n  filter(!is.na(days_lived)) |> \n  group_by(gender) |> \n  summarise(\n    min_days = min(days_lived),\n    mean_days = mean(days_lived) |> round(),\n    max_days = max(days_lived)\n    )\n#> # A tibble: 2 × 4\n#>   gender min_days   mean_days  max_days  \n#>   <chr>  <drtn>     <drtn>     <drtn>    \n#> 1 female 14856 days 28857 days 35560 days\n#> 2 male   12380 days 27376 days 36416 days\naustralian_politicians |>\n  mutate(days_lived = deathDate - birthDate) |> \n  filter(!is.na(days_lived)) |> \n  group_by(gender, member) |> \n  summarise(\n    min_days = min(days_lived),\n    mean_days = mean(days_lived) |> round(),\n    max_days = max(days_lived)\n    )\n#> # A tibble: 4 × 5\n#> # Groups:   gender [2]\n#>   gender member min_days   mean_days  max_days  \n#>   <chr>   <dbl> <drtn>     <drtn>     <drtn>    \n#> 1 female      0 21746 days 29517 days 35560 days\n#> 2 female      1 14856 days 27538 days 33442 days\n#> 3 male        0 13619 days 27133 days 36416 days\n#> 4 male        1 12380 days 27496 days 36328 days\naustralian_politicians |> \n  group_by(gender) |> \n  count()\n#> # A tibble: 2 × 2\n#> # Groups:   gender [2]\n#>   gender     n\n#>   <chr>  <int>\n#> 1 female   240\n#> 2 male    1543\naustralian_politicians |> \n  group_by(gender) |> \n  count() |> \n  ungroup() |> \n  mutate(proportion = n/(sum(n)))\n#> # A tibble: 2 × 3\n#>   gender     n proportion\n#>   <chr>  <int>      <dbl>\n#> 1 female   240      0.135\n#> 2 male    1543      0.865\naustralian_politicians |> \n  group_by(gender) |> \n  summarise(n = n())\n#> # A tibble: 2 × 2\n#>   gender     n\n#>   <chr>  <int>\n#> 1 female   240\n#> 2 male    1543\naustralian_politicians |> \n  group_by(gender) |> \n  add_count() |> \n  select(uniqueID, gender, n)\n#> # A tibble: 1,783 × 3\n#> # Groups:   gender [2]\n#>    uniqueID   gender     n\n#>    <chr>      <chr>  <int>\n#>  1 Abbott1859 male    1543\n#>  2 Abbott1869 male    1543\n#>  3 Abbott1877 male    1543\n#>  4 Abbott1886 male    1543\n#>  5 Abbott1891 male    1543\n#>  6 Abbott1957 male    1543\n#>  7 Abel1939   male    1543\n#>  8 Abetz1958  male    1543\n#>  9 Adams1943  female   240\n#> 10 Adams1951  male    1543\n#> # … with 1,773 more rows"},{"path":"r-essentials.html","id":"base","chapter":"3 R essentials","heading":"3.6 Base","text":"tidyverse established relatively recently help data science, R existed long . host functionality built R especially around core needs programming statisticians.particular, cover:class()data simulationfunction(), (), apply()need install additional packages, functionality comes R.","code":""},{"path":"r-essentials.html","id":"class","chapter":"3 R essentials","heading":"3.6.1 class()","text":"everyday usage ‘, b, c, …’ letters ‘1, 2, 3,…’ numbers. use letters numbers differently, instance add letters. Similarly, R needs way distinguishing different classes content. define properties class , ‘behaves, relates types objects’ (Wickham 2019a).Classes hierarchy. instance, ‘human’, ‘animal’. ‘humans’ ‘animals’, ‘animals’ ‘humans’. Similarly, integers numbers, numbers integers. can find class object R class().classes cover ‘numeric’, ‘character’, ‘factor’, ‘date’, ‘data.frame’.first thing know , way frog can become prince, can sometimes change class object R. instance, start ‘numeric’, change ‘character’ .character(), ‘factor’ .factor(). tried make date .Date() get error numbers properties needed date.Compared ‘numeric’ ‘character’ classes, ‘factor’ class might less familiar. ‘factor’ used categorical data can take certain values (Wickham 2019a). instance, typical usage factor variable binary, ‘day’ ‘night’. also often used age-groups, ‘18-29’, ‘30-44’, ‘45-60’, ‘60+’ (opposed age, often ‘numeric’); sometimes level education: ‘less high school’, ‘high school’, ‘college’, ‘undergraduate degree’, ‘postgraduate degree’. can find allowed levels ‘factor’ using levels().Dates especially tricky class quickly become complicated. Nonetheless, foundational level, can use .Date() convert character looks like date actual date. enables us , say, perform addition subtraction, able character.final class discuss ‘data.frame’. looks like spreadsheet commonly used store data analyze. Formally, ‘data frame list equal-length vectors’ (Wickham 2019a). column row names can see using colnames() rownames(), although often names rows just numbers.illustrate , use ‘ResumeNames’ dataset AER (Kleiber Zeileis 2008). package can installed way package CRAN. dataset comprises cross-sectional data resume content, especially name used resume, associated information whether candidate received call-back 4,870 fictitious resumes. dataset created Bertrand Mullainathan (2004) sent fictitious resumes response job advertisements Boston Chicago differed whether resume assigned ‘African American sounding name White sounding name’. found considerable discrimination whereby ‘White names receive 50 percent callbacks interviews’.can examine class vectors, .e. columns, make-data frame specifying column name.Sometimes helpful able change classes many columns . can using mutate() across().many ways code run issue class always among first things check. Common issues variables think ‘character’ ‘numeric’, actually ‘factor’. variables think ‘numeric’ actually ‘character’.","code":"\na_number <- 8\nclass(a_number)\n#> [1] \"numeric\"\n\na_letter <- \"a\"\nclass(a_letter)\n#> [1] \"character\"\na_number <- 8\na_number\n#> [1] 8\nclass(a_number)\n#> [1] \"numeric\"\n\na_number <- as.character(a_number)\na_number\n#> [1] \"8\"\nclass(a_number)\n#> [1] \"character\"\n\na_number <- as.factor(a_number)\na_number\n#> [1] 8\n#> Levels: 8\nclass(a_number)\n#> [1] \"factor\"\nage_groups <- factor(\n  c('18-29', '30-44', '45-60', '60+')\n)\nage_groups\n#> [1] 18-29 30-44 45-60 60+  \n#> Levels: 18-29 30-44 45-60 60+\nclass(age_groups)\n#> [1] \"factor\"\nlevels(age_groups)\n#> [1] \"18-29\" \"30-44\" \"45-60\" \"60+\"\nlooks_like_a_date_but_is_not <- \"2022-01-01\"\nlooks_like_a_date_but_is_not\n#> [1] \"2022-01-01\"\nclass(looks_like_a_date_but_is_not)\n#> [1] \"character\"\nis_a_date <- as.Date(looks_like_a_date_but_is_not)\nis_a_date\n#> [1] \"2022-01-01\"\nclass(is_a_date)\n#> [1] \"Date\"\nis_a_date + 3\n#> [1] \"2022-01-04\"\ninstall.packages(\"AER\")\nlibrary(AER)\ndata(\"ResumeNames\", package = \"AER\")\nResumeNames |> \n  head()\n#>      name gender ethnicity quality call    city jobs\n#> 1 Allison female      cauc     low   no chicago    2\n#> 2 Kristen female      cauc    high   no chicago    3\n#> 3 Lakisha female      afam     low   no chicago    1\n#> 4 Latonya female      afam    high   no chicago    4\n#> 5  Carrie female      cauc    high   no chicago    3\n#> 6     Jay   male      cauc     low   no chicago    2\n#>   experience honors volunteer military holes school email\n#> 1          6     no        no       no   yes     no    no\n#> 2          6     no       yes      yes    no    yes   yes\n#> 3          6     no        no       no    no    yes    no\n#> 4          6     no       yes       no   yes     no   yes\n#> 5         22     no        no       no    no    yes   yes\n#> 6          6    yes        no       no    no     no    no\n#>   computer special college minimum equal     wanted\n#> 1      yes      no     yes       5   yes supervisor\n#> 2      yes      no      no       5   yes supervisor\n#> 3      yes      no     yes       5   yes supervisor\n#> 4      yes     yes      no       5   yes supervisor\n#> 5      yes      no      no    some   yes  secretary\n#> 6       no     yes     yes    none   yes      other\n#>   requirements reqexp reqcomm reqeduc reqcomp reqorg\n#> 1          yes    yes      no      no     yes     no\n#> 2          yes    yes      no      no     yes     no\n#> 3          yes    yes      no      no     yes     no\n#> 4          yes    yes      no      no     yes     no\n#> 5          yes    yes      no      no     yes    yes\n#> 6           no     no      no      no      no     no\n#>                           industry\n#> 1                    manufacturing\n#> 2                    manufacturing\n#> 3                    manufacturing\n#> 4                    manufacturing\n#> 5 health/education/social services\n#> 6                            trade\nclass(ResumeNames)\n#> [1] \"data.frame\"\ncolnames(ResumeNames)\n#>  [1] \"name\"         \"gender\"       \"ethnicity\"   \n#>  [4] \"quality\"      \"call\"         \"city\"        \n#>  [7] \"jobs\"         \"experience\"   \"honors\"      \n#> [10] \"volunteer\"    \"military\"     \"holes\"       \n#> [13] \"school\"       \"email\"        \"computer\"    \n#> [16] \"special\"      \"college\"      \"minimum\"     \n#> [19] \"equal\"        \"wanted\"       \"requirements\"\n#> [22] \"reqexp\"       \"reqcomm\"      \"reqeduc\"     \n#> [25] \"reqcomp\"      \"reqorg\"       \"industry\"\nclass(ResumeNames$name)\n#> [1] \"factor\"\nclass(ResumeNames$jobs)\n#> [1] \"integer\"\nclass(ResumeNames$name)\n#> [1] \"factor\"\nclass(ResumeNames$gender)\n#> [1] \"factor\"\nclass(ResumeNames$ethnicity)\n#> [1] \"factor\"\n\nResumeNames |>\n  mutate(across(c(name, gender, ethnicity), as.character)) |>\n  head()\n#>      name gender ethnicity quality call    city jobs\n#> 1 Allison female      cauc     low   no chicago    2\n#> 2 Kristen female      cauc    high   no chicago    3\n#> 3 Lakisha female      afam     low   no chicago    1\n#> 4 Latonya female      afam    high   no chicago    4\n#> 5  Carrie female      cauc    high   no chicago    3\n#> 6     Jay   male      cauc     low   no chicago    2\n#>   experience honors volunteer military holes school email\n#> 1          6     no        no       no   yes     no    no\n#> 2          6     no       yes      yes    no    yes   yes\n#> 3          6     no        no       no    no    yes    no\n#> 4          6     no       yes       no   yes     no   yes\n#> 5         22     no        no       no    no    yes   yes\n#> 6          6    yes        no       no    no     no    no\n#>   computer special college minimum equal     wanted\n#> 1      yes      no     yes       5   yes supervisor\n#> 2      yes      no      no       5   yes supervisor\n#> 3      yes      no     yes       5   yes supervisor\n#> 4      yes     yes      no       5   yes supervisor\n#> 5      yes      no      no    some   yes  secretary\n#> 6       no     yes     yes    none   yes      other\n#>   requirements reqexp reqcomm reqeduc reqcomp reqorg\n#> 1          yes    yes      no      no     yes     no\n#> 2          yes    yes      no      no     yes     no\n#> 3          yes    yes      no      no     yes     no\n#> 4          yes    yes      no      no     yes     no\n#> 5          yes    yes      no      no     yes    yes\n#> 6           no     no      no      no      no     no\n#>                           industry\n#> 1                    manufacturing\n#> 2                    manufacturing\n#> 3                    manufacturing\n#> 4                    manufacturing\n#> 5 health/education/social services\n#> 6                            trade\n\nclass(ResumeNames$name)\n#> [1] \"factor\"\nclass(ResumeNames$gender)\n#> [1] \"factor\"\nclass(ResumeNames$ethnicity)\n#> [1] \"factor\""},{"path":"r-essentials.html","id":"simulating-data","chapter":"3 R essentials","heading":"3.6.2 Simulating data","text":"Simulating data key skill telling believable stories data. order simulate data, need able randomly draw statistical distributions collections. R variety functions make easier, including: normal distribution, rnorm(); uniform distribution, runif(); Poisson distribution, rpois; binomial distribution, rbinom; many others. randomly sample collection items, can use sample().dealing randomness, need reproducibility makes important, paradoxically, randomness repeatable. say, another person needs able draw random numbers draw. setting seed random draws using set.seed().get observations standard normal distribution put data frame.add draws uniform, Poisson, binomial distributions, using cbind() bring columns original dataset new one together.Finally, add favorite color observation sample().set option ‘replace’ ‘TRUE’ choosing two items, time choose want possibility either chosen. Depending simulation may need think whether ‘replace’ ‘TRUE’ ‘FALSE’. Another useful optional argument sample() adjust probability item drawn. default options equally likely, specify particular probabilities wanted ‘prob’. always functions, can find help file, instance ?sample.","code":"\nset.seed(853)\n\nnumber_of_observations <- 5\n\nsimulated_data <- \n  data.frame(\n    person = c(1:number_of_observations),\n    std_normal_observations = rnorm(n = number_of_observations,\n                                    mean = 0,\n                                    sd = 1)\n    )\n\nsimulated_data\n#>   person std_normal_observations\n#> 1      1             -0.35980342\n#> 2      2             -0.04064753\n#> 3      3             -1.78216227\n#> 4      4             -1.12242282\n#> 5      5             -1.00278400\nsimulated_data <-\n  data.frame(\n    uniform_observations = \n      runif(n = number_of_observations, min = 0, max = 10),\n    poisson_observations = \n      rpois(n = number_of_observations, lambda = 100),\n    binomial_observations = \n      rbinom(n = number_of_observations, size = 2, prob = 0.5)\n  ) |>\n  cbind(simulated_data)\n\nsimulated_data\n#>   uniform_observations poisson_observations\n#> 1            9.6219155                   81\n#> 2            7.2269016                   91\n#> 3            0.8252921                   84\n#> 4            1.0379810                  100\n#> 5            3.0942004                   97\n#>   binomial_observations person std_normal_observations\n#> 1                     2      1             -0.35980342\n#> 2                     1      2             -0.04064753\n#> 3                     1      3             -1.78216227\n#> 4                     1      4             -1.12242282\n#> 5                     1      5             -1.00278400\nsimulated_data <- \n  data.frame(\n    favorite_color = sample(x = c(\"blue\", \" white \"), \n                             size = number_of_observations,\n                             replace = TRUE)\n    ) |>\n  cbind(simulated_data)\n\nsimulated_data\n#>   favorite_color uniform_observations poisson_observations\n#> 1           blue            9.6219155                   81\n#> 2           blue            7.2269016                   91\n#> 3           blue            0.8252921                   84\n#> 4         white             1.0379810                  100\n#> 5           blue            3.0942004                   97\n#>   binomial_observations person std_normal_observations\n#> 1                     2      1             -0.35980342\n#> 2                     1      2             -0.04064753\n#> 3                     1      3             -1.78216227\n#> 4                     1      4             -1.12242282\n#> 5                     1      5             -1.00278400"},{"path":"r-essentials.html","id":"function-for-and-apply","chapter":"3 R essentials","heading":"3.6.3 function(), for(), and apply()","text":"R ‘functional programming language’ (Wickham 2019a). means foundationally write, use, compose functions, collections code accomplish something specific.lot functions R people written, can use. Almost common statistical data science task might need accomplish likely already function written someone else made available us, either part base R installation package. need write functions time time, especially -specific tasks.\ndefine function using function(), assign name. likely need include inputs outputs function. Inputs specified round brackets. specific task function accomplish goes braces.can specify defaults inputs case person using function supply .One common scenario want apply function multiple times. Like many programming languages, can use () loop . look () loop R similar function(), define iterating round brackets, function apply braces.R programming language focused statistics, often interested arrays matrices. us apply() apply function rows (‘MARGIN = 1’) columns (‘MARGIN = 2’).","code":"\nprint_names <- function(some_names) {\n  print(some_names)\n}\n\nprint_names(c(\"rohan\", \"monica\"))\n#> [1] \"rohan\"  \"monica\"\nprint_names <- function(some_names = c(\"edward\", \"hugo\")) {\n  print(some_names)\n}\n\nprint_names()\n#> [1] \"edward\" \"hugo\"\nfor (i in 1:3) {\n  print(i)\n}\n#> [1] 1\n#> [1] 2\n#> [1] 3\nx <- cbind(x1 = 66, x2 = c(4:1, 2:5))\ndimnames(x)[[1]] <- letters[1:8]\nclass(x)\n#> [1] \"matrix\" \"array\"\napply(x, 2, mean, trim = .2)\n#> x1 x2 \n#> 66  3\nsimulated_data\n#>   favorite_color uniform_observations poisson_observations\n#> 1           blue            9.6219155                   81\n#> 2           blue            7.2269016                   91\n#> 3           blue            0.8252921                   84\n#> 4         white             1.0379810                  100\n#> 5           blue            3.0942004                   97\n#>   binomial_observations person std_normal_observations\n#> 1                     2      1             -0.35980342\n#> 2                     1      2             -0.04064753\n#> 3                     1      3             -1.78216227\n#> 4                     1      4             -1.12242282\n#> 5                     1      5             -1.00278400\napply(X = simulated_data, MARGIN = 2, FUN = unique)\n#> $favorite_color\n#> [1] \"blue\"    \" white \"\n#> \n#> $uniform_observations\n#> [1] \"9.6219155\" \"7.2269016\" \"0.8252921\" \"1.0379810\"\n#> [5] \"3.0942004\"\n#> \n#> $poisson_observations\n#> [1] \" 81\" \" 91\" \" 84\" \"100\" \" 97\"\n#> \n#> $binomial_observations\n#> [1] \"2\" \"1\"\n#> \n#> $person\n#> [1] \"1\" \"2\" \"3\" \"4\" \"5\"\n#> \n#> $std_normal_observations\n#> [1] \"-0.35980342\" \"-0.04064753\" \"-1.78216227\" \"-1.12242282\"\n#> [5] \"-1.00278400\""},{"path":"r-essentials.html","id":"making-graphs-with-ggplot2","chapter":"3 R essentials","heading":"3.7 Making graphs with ggplot2","text":"key package tidyverse terms manipulating data dplyr (Wickham et al. 2020), key package tidyverse terms creating graphs ggplot2 (Wickham 2016). part tidyverse collection packages need explicitly installed loaded tidyverse loaded.formally, ggplot2 works defining layers build form graph, based around ‘grammar graphics’ (hence, ‘gg’). Instead pipe operator (|>) ggplot uses add operator +.three key aspects need specified build graph ggplot2:data;aesthetics / mapping; andtype.get started obtain GDP data OECD countries (OECD 2022).interested, firstly, making bar chart GDP change third quarter 2021 ten countries: Australia, Canada, Chile, Indonesia, Germany, Great Britain, New Zealand, South Africa, Spain, US.start ggplot specify mapping/aesthetic, case means specifying x-axis y-axis.Now need specify type graph interested . case want bar chart adding geom_bar().can color bars whether country European adding another aesthetic, ‘fill’.Finally, make look nicer : adding labels, labs(); changing color, scale_fill_brewer(); background, theme_classic().Facets enable us create subplots focus specific aspects data. invaluable allow us add another variable graph without make 3D graph. use facet_wrap() add facet specify variable like facet . case, facet hemisphere.","code":"\nlibrary(tidyverse)\n\noecd_gdp <- \n  read_csv(\"https://stats.oecd.org/sdmx-json/data/DP_LIVE/.QGDP.../OECD?contentType=csv&detail=code&separator=comma&csv-lang=en\")\n\nwrite_csv(oecd_gdp, 'inputs/data/oecd_gdp.csv')#> # A tibble: 6 × 8\n#>   LOCATION INDICATOR SUBJECT MEASURE  FREQUENCY TIME  Value\n#>   <chr>    <chr>     <chr>   <chr>    <chr>     <chr> <dbl>\n#> 1 OECD     QGDP      TOT     PC_CHGPP A         1962   5.70\n#> 2 OECD     QGDP      TOT     PC_CHGPP A         1963   5.20\n#> 3 OECD     QGDP      TOT     PC_CHGPP A         1964   6.38\n#> 4 OECD     QGDP      TOT     PC_CHGPP A         1965   5.35\n#> 5 OECD     QGDP      TOT     PC_CHGPP A         1966   5.75\n#> 6 OECD     QGDP      TOT     PC_CHGPP A         1967   3.96\n#> # … with 1 more variable: `Flag Codes` <chr>\noecd_gdp_most_recent <- \n  oecd_gdp |> \n  filter(TIME == \"2021-Q3\",\n         SUBJECT == \"TOT\",\n         LOCATION %in% c(\"AUS\", \"CAN\", \"CHL\", \"DEU\", \"GBR\",\n                         \"IDN\", \"ESP\", \"NZL\", \"USA\", \"ZAF\"),\n         MEASURE == \"PC_CHGPY\") |> \n  mutate(european = if_else(LOCATION %in% c(\"DEU\", \"GBR\", \"ESP\"),\n                             \"European\",\n                             \"Not european\"),\n         hemisphere = if_else(LOCATION %in% c(\"CAN\", \"DEU\", \"GBR\", \"ESP\", \"USA\"),\n                             \"Northern Hemisphere\",\n                             \"Southern Hemisphere\"),\n         )\noecd_gdp_most_recent |> \n  ggplot(mapping = aes(x = LOCATION, y = Value))\noecd_gdp_most_recent |> \n  ggplot(mapping = aes(x = LOCATION, y = Value)) +\n  geom_bar(stat=\"identity\")\noecd_gdp_most_recent |> \n  ggplot(mapping = aes(x = LOCATION, y = Value, fill = european)) +\n  geom_bar(stat=\"identity\")\noecd_gdp_most_recent |> \n  ggplot(mapping = aes(x = LOCATION, y = Value, fill = european)) +\n  geom_bar(stat=\"identity\") + \n  labs(title = \"Quarterly change in GDP for ten OECD countries in 2021Q3\", \n       x = \"Countries\", \n       y = \"Change (%)\",\n       fill = \"Is European?\") +\n  theme_classic() +\n  scale_fill_brewer(palette = \"Set1\")\noecd_gdp_most_recent |> \n  ggplot(mapping = aes(x = LOCATION, y = Value, fill = european)) +\n  geom_bar(stat=\"identity\") + \n  labs(title = \"Quarterly change in GDP for ten OECD countries in 2021Q3\", \n       x = \"Countries\", \n       y = \"Change (%)\",\n       fill = \"Is European?\") +\n  theme_classic() +\n  scale_fill_brewer(palette = \"Set1\") +\n  facet_wrap(~hemisphere, \n              scales = \"free_x\")"},{"path":"r-essentials.html","id":"exploring-the-tidyverse","chapter":"3 R essentials","heading":"3.8 Exploring the tidyverse","text":"focused two aspects tidyverse: dplyr, ggplot2. However, tidyverse comprises variety different packages functions. now go four common aspects:Importing data tibble().Joining pivoting datasets.String manipulation stringr.Factor variables forcats.However, first task deal nomenclature, particular specific ‘tidy’ ‘tidyverse’. name refers tidy data, benefit variety ways data messy, tidy data satisfy three rules. means structure datasets consistent regardless specifics, makes easier apply functions expect certain types input. Tidy data refers dataset (Wickham Grolemund 2017; Wickham 2014, 4):Every variable column .Every observation row.Every value cell.Table 3.1 tidy. Table 3.2 tidy age hair share column.Table 3.1: Example tidy dataTable 3.2: Example data tidy","code":"\ntibble(\n  person = c(\"Rohan\", \"Monica\", \"Edward\", \"Hugo\"),\n  age = c(35, 35, 2, 0),\n  hair = c(\"Black\", \"Blonde\", \"Brown\", \"None\")\n  ) |>\n  knitr::kable(\n    caption = \"Example of tidy data\",\n    col.names = c(\"Person\", \"Age\", \"Hair\"),\n    digits = 1,\n    booktabs = TRUE,\n    linesep = \"\"\n    )\ntibble(\n  person = c(\n    \"Rohan\",\n    \"Rohan\",\n    \"Monica\",\n    \"Monica\",\n    \"Edward\",\n    \"Edward\",\n    \"Hugo\",\n    \"Hugo\"\n  ),\n  variable = c(\"Age\", \"Hair\", \"Age\", \"Hair\", \"Age\", \"Hair\", \"Age\", \"Hair\"),\n  value = c(\"35\", \"Black\", \"35\", \"Blonde\", \"2\", \"Brown\", \"0\", \"None\")\n) |>\n  knitr::kable(\n    caption = \"Example of data that are not tidy\",\n    col.names = c(\"Person\", \"Variable\", \"Value\"),\n    digits = 1,\n    booktabs = TRUE,\n    linesep = \"\"\n  )"},{"path":"r-essentials.html","id":"importing-data-and-tibble","chapter":"3 R essentials","heading":"3.8.1 Importing data and tibble()","text":"variety ways get data R can use . CSV files, read_csv() readr (Wickham, Hester, Bryan 2021), dta files, read_dta() haven (Wickham Miller 2020).CSVs common format many advantages including fact typically modify data. column separated comma, row record. can provide read_csv() URL local file read. variety different options can passed read_csv() including ability specify whether dataset column names, types columns, many lines skip. specify types columns read_csv() make guess looking dataset.use read_dta() read .dta files, commonly produced statistical program Stata. means common fields sociology, political science, economics. format separates data labels typically reunite using to_factor() labelled (Larmarange 2021). haven part tidyverse, automatically loaded default, contrast package ggplot2, need run library(haven).Typically dataset enters R ‘data.frame’. can useful, another helpful class dataset ‘tibble’. can created using tibble() tibble package part tidyverse. tibble data frame, particular changes make easier work , including converting strings factors default, showing class columns, printing nicely.can make tibble manually, need , instance, simulate data. typically import data directly tibble, instance, use read_csv().","code":"\npeople_as_dataframe <- \n  data.frame(names = c(\"rohan\", \"monica\"),\n             website = c(\"rohanalexander.com\", \"monicaalexander.com\"),\n             fav_color = c(\"blue\", \" white \")\n             )\nclass(people_as_dataframe)\n#> [1] \"data.frame\"\npeople_as_dataframe\n#>    names             website fav_color\n#> 1  rohan  rohanalexander.com      blue\n#> 2 monica monicaalexander.com    white\n\npeople_as_tibble <- \n  tibble(names = c(\"rohan\", \"monica\"),\n         website = c(\"rohanalexander.com\", \"monicaalexander.com\"),\n         fav_color = c(\"blue\", \" white \")\n         )\npeople_as_tibble\n#> # A tibble: 2 × 3\n#>   names  website             fav_color\n#>   <chr>  <chr>               <chr>    \n#> 1 rohan  rohanalexander.com  \"blue\"   \n#> 2 monica monicaalexander.com \" white \"\nclass(people_as_tibble)\n#> [1] \"tbl_df\"     \"tbl\"        \"data.frame\""},{"path":"r-essentials.html","id":"dataset-manipulation-with-joins-and-pivots","chapter":"3 R essentials","heading":"3.8.2 Dataset manipulation with joins and pivots","text":"two dataset manipulations often needed: joins pivots.often situation two, , datasets interested combining . can join datasets together variety ways. common way use left_join() dplyr (Wickham et al. 2020). useful one main dataset using another dataset useful variables want add . critical aspect column/s can use link two datasets. create two tibbles join basis names.variety options join datasets, including inner_join(), right_join(), full_join().Another common dataset manipulation task pivoting . Datasets tend either long wide. Generally, tidyverse, certainly ggplot2, need long data. go one use pivot_longer() pivot_wider() tidyr (Wickham 2021c).create wide data whether ‘mark’ ‘lauren’ won running race three years.dataset wide format moment. get long format, need column specifies person, another specifies result. use pivot_longer() achieve .Occasionally, need go long data wide data. use pivot_wider() .","code":"\nmain_dataset <- \n  tibble(\n    names = c('rohan', 'monica', 'edward', 'hugo'),\n    status = c('adult', 'adult', 'child', 'infant')\n  )\nmain_dataset\n#> # A tibble: 4 × 2\n#>   names  status\n#>   <chr>  <chr> \n#> 1 rohan  adult \n#> 2 monica adult \n#> 3 edward child \n#> 4 hugo   infant\n\nsupplementary_dataset <- \n  tibble(\n    names = c('rohan', 'monica', 'edward', 'hugo'),\n    favorite_food = c('pasta', 'salmon', 'pizza', 'milk')\n  )\nsupplementary_dataset\n#> # A tibble: 4 × 2\n#>   names  favorite_food\n#>   <chr>  <chr>        \n#> 1 rohan  pasta        \n#> 2 monica salmon       \n#> 3 edward pizza        \n#> 4 hugo   milk\n\nmain_dataset <- \n  main_dataset |> \n  left_join(supplementary_dataset, by = \"names\")\n\nmain_dataset\n#> # A tibble: 4 × 3\n#>   names  status favorite_food\n#>   <chr>  <chr>  <chr>        \n#> 1 rohan  adult  pasta        \n#> 2 monica adult  salmon       \n#> 3 edward child  pizza        \n#> 4 hugo   infant milk\npivot_example_data <- \n  tibble(year = c(2019, 2020, 2021),\n         mark = c(\"first\", \"second\", \"first\"),\n         lauren = c(\"second\", \"first\", \"second\"))\n\npivot_example_data\n#> # A tibble: 3 × 3\n#>    year mark   lauren\n#>   <dbl> <chr>  <chr> \n#> 1  2019 first  second\n#> 2  2020 second first \n#> 3  2021 first  second\ndata_pivoted_longer <- \n  pivot_example_data |> \n  tidyr::pivot_longer(cols = c(\"mark\", \"lauren\"),\n              names_to = \"person\",\n               values_to = \"position\")\n\nhead(data_pivoted_longer)\n#> # A tibble: 6 × 3\n#>    year person position\n#>   <dbl> <chr>  <chr>   \n#> 1  2019 mark   first   \n#> 2  2019 lauren second  \n#> 3  2020 mark   second  \n#> 4  2020 lauren first   \n#> 5  2021 mark   first   \n#> 6  2021 lauren second\ndata_pivoted_wider <- \n  data_pivoted_longer |> \n  tidyr::pivot_wider(names_from = \"person\",\n                     values_from = \"position\")\n\nhead(data_pivoted_wider)\n#> # A tibble: 3 × 3\n#>    year mark   lauren\n#>   <dbl> <chr>  <chr> \n#> 1  2019 first  second\n#> 2  2020 second first \n#> 3  2021 first  second"},{"path":"r-essentials.html","id":"string-manipulation-and-stringr","chapter":"3 R essentials","heading":"3.8.3 String manipulation and stringr","text":"R often create string double quotes, although using single quotes works . instance c(\"\", \"b\") consists two strings ‘’ ‘b’, contained character vector. variety ways manipulate strings R focus stringr (Wickham 2019e). automatically loaded load tidyverse.want look whether string contains certain content, can use str_detect(). want remove change particular content can use str_remove() str_replace().variety functions often especially useful data cleaning. instance, can use str_length() find long string , str_c() bring strings together.Finally, separate() tidyr, although part stringr, indispensable string manipulation. turns one character column many.","code":"\ndataset_of_strings <- \n  tibble(\n    names = c(\"rohan alexander\", \n              \"monica alexander\", \n              \"edward alexander\", \n              \"hugo alexander\")\n  )\n\ndataset_of_strings |> \n  mutate(is_rohan = str_detect(names, \"rohan\"),\n         make_howlett = str_replace(names, \"alexander\", \"howlett\"),\n         remove_rohan = str_remove(names, \"rohan\")\n         )\n#> # A tibble: 4 × 4\n#>   names            is_rohan make_howlett   remove_rohan     \n#>   <chr>            <lgl>    <chr>          <chr>            \n#> 1 rohan alexander  TRUE     rohan howlett  \" alexander\"     \n#> 2 monica alexander FALSE    monica howlett \"monica alexande…\n#> 3 edward alexander FALSE    edward howlett \"edward alexande…\n#> 4 hugo alexander   FALSE    hugo howlett   \"hugo alexander\"\ndataset_of_strings |> \n  mutate(length_is = str_length(string = names),\n         name_and_length = str_c(names, length_is, sep = \" - \")\n         )\n#> # A tibble: 4 × 3\n#>   names            length_is name_and_length      \n#>   <chr>                <int> <chr>                \n#> 1 rohan alexander         15 rohan alexander - 15 \n#> 2 monica alexander        16 monica alexander - 16\n#> 3 edward alexander        16 edward alexander - 16\n#> 4 hugo alexander          14 hugo alexander - 14\ndataset_of_strings |> \n  separate(col = names,\n           into = c(\"first\", \"last\"),\n           sep = \" \",\n           remove = FALSE)\n#> # A tibble: 4 × 3\n#>   names            first  last     \n#>   <chr>            <chr>  <chr>    \n#> 1 rohan alexander  rohan  alexander\n#> 2 monica alexander monica alexander\n#> 3 edward alexander edward alexander\n#> 4 hugo alexander   hugo   alexander"},{"path":"r-essentials.html","id":"factor-variables-and-forcats","chapter":"3 R essentials","heading":"3.8.4 Factor variables and forcats","text":"factor collection strings categories. Sometimes inherent ordering. instance, days week order – Monday, Tuesday, Wednesday, … – alphabetical. requirement case, instance gender: female, male, ; pregnancy status: pregnant pregnant. Factors feature prominently base R. can useful ensure appropriate strings allowed. instance, ‘days_of_the_week’ factor variable ‘January’ allowed. can add great deal complication, less prominent role tidyverse. Nonetheless taking advantage factors useful certain circumstances. instance, plotting days week probably want usual ordering alphabetical ordering result character variable. factors built base R, one tidyverse package especially useful using factors forcats (Wickham 2020a).Sometimes character vector, want ordered particular way. default character vector ordered alphabetically, may want . instance, days week look strange graph alphabetically ordered: Friday, Monday, Saturday, Sunday, Thursday, Tuesday, Wednesday!way change ordering change variable character factor. can use fct_relevel() forcats (Wickham 2020a) specify ordering.can compare results graphing first original character vector x-axis, another graph factor vector x-axis.","code":"\nset.seed(853)\n\ndays_data <-\n  tibble(\n    days =\n      c(\n        \"Monday\",\n        \"Tuesday\",\n        \"Wednesday\",\n        \"Thursday\",\n        \"Friday\",\n        \"Saturday\",\n        \"Sunday\"\n      ),\n    some_value = c(sample.int(100, 7))\n  )\n\ndays_data <-\n  days_data |>\n  mutate(\n    days_as_factor = factor(days),\n    days_as_factor = fct_relevel(\n      days,\n      \"Monday\",\n      \"Tuesday\",\n      \"Wednesday\",\n      \"Thursday\",\n      \"Friday\",\n      \"Saturday\",\n      \"Sunday\"\n    )\n  )\ndays_data |> \n  ggplot(aes(x = days, y = some_value)) +\n  geom_point()\n\ndays_data |> \n  ggplot(aes(x = days_as_factor, y = some_value)) +\n  geom_point()"},{"path":"r-essentials.html","id":"exercises-and-tutorial-2","chapter":"3 R essentials","heading":"3.9 Exercises and tutorial","text":"","code":""},{"path":"r-essentials.html","id":"exercises-2","chapter":"3 R essentials","heading":"3.9.1 Exercises","text":"R?\nopen-source statistical programming language\nprogramming language created Guido van Rossum\nclosed source statistical programming language\nintegrated development environment (IDE)\nopen-source statistical programming languageA programming language created Guido van RossumA closed source statistical programming languageAn integrated development environment (IDE)three advantages R? three disadvantages?R Studio?\nintegrated development environment (IDE).\nclosed source paid program.\nprogramming language created Guido van Rossum\nstatistical programming language.\nintegrated development environment (IDE).closed source paid program.programming language created Guido van RossumA statistical programming language.class output 2 + 2 (pick one)?\ncharacter\nfactor\nnumeric\ndate\ncharacterfactornumericdateSay run: my_name <- 'Rohan'. result running print(my_name) (pick one)?\n‘Edward’\n‘Monica’\n‘Hugo’\n‘Rohan’\n‘Edward’‘Monica’‘Hugo’‘Rohan’Say dataset two columns: ‘name’, ‘age’. verb use pick just ‘name’ (pick one)?\ntidyverse::select()\ntidyverse::mutate()\ntidyverse::filter()\ntidyverse::rename()\ntidyverse::select()tidyverse::mutate()tidyverse::filter()tidyverse::rename()Say loaded AustralianPoliticians tidyverse run following code: australian_politicians <- AustralianPoliticians::get_auspol(''). select columns end ‘Name’ (pick one)?\naustralian_politicians |> select(contains(\"Name\"))\naustralian_politicians |> select(starts_with(\"Name\"))\naustralian_politicians |> select(matches(\"Name\"))\naustralian_politicians |> select(ends_with(\"Name\"))\naustralian_politicians |> select(contains(\"Name\"))australian_politicians |> select(starts_with(\"Name\"))australian_politicians |> select(matches(\"Name\"))australian_politicians |> select(ends_with(\"Name\"))circumstances, terms names columns, use ‘contains()’ potentially give different answers using ‘ends_with()’ question?following tidyverse verbs (pick one)?\nselect()\nfilter()\narrange()\nmutate()\nvisualize()\nselect()filter()arrange()mutate()visualize()function make new column (pick one)?\nselect()\nfilter()\narrange()\nmutate()\nvisualize()\nselect()filter()arrange()mutate()visualize()function focus particular rows (pick one)?\nselect()\nfilter()\narrange()\nmutate()\nsummarise()\nselect()filter()arrange()mutate()summarise()combination two functions provide mean dataset, sex (pick two)?\nsummarise()\nfilter()\narrange()\nmutate()\ngroup_by()\nsummarise()filter()arrange()mutate()group_by()Assume variable called ‘age’ integer. line code create column exponential (pick one)?\nmutate(exp_age = exponential(age))\nmutate(exp_age = exponent(age))\nmutate(exp_age = exp(age))\nmutate(exp_age = expon(age))\nmutate(exp_age = exponential(age))mutate(exp_age = exponent(age))mutate(exp_age = exp(age))mutate(exp_age = expon(age))Assume column called ‘age’. line code create column contains value five rows ?\nmutate(five_before = lag(age))\nmutate(five_before = lead(age))\nmutate(five_before = lag(age, n = 5))\nmutate(five_before = lead(age, n = 5))\nmutate(five_before = lag(age))mutate(five_before = lead(age))mutate(five_before = lag(age, n = 5))mutate(five_before = lead(age, n = 5))output class('edward') (pick one)?\n‘numeric’\n‘character’\n‘data.frame’\n‘vector’\n‘numeric’‘character’‘data.frame’‘vector’function enable us draw three options ‘blue, white, red’, 10 per cent probability ‘blue’ ‘white’, remainder ‘red’?\nsample(c('blue', 'white', 'red'), prob = c(0.1, 0.1, 0.8))\nsample(c('blue', 'white', 'red'), size = 1)\nsample(c('blue', 'white', 'red'), size = 1, prob = c(0.8, 0.1, 0.1))\nsample(c('blue', 'white', 'red'), size = 1, prob = c(0.1, 0.1, 0.8))\nsample(c('blue', 'white', 'red'), prob = c(0.1, 0.1, 0.8))sample(c('blue', 'white', 'red'), size = 1)sample(c('blue', 'white', 'red'), size = 1, prob = c(0.8, 0.1, 0.1))sample(c('blue', 'white', 'red'), size = 1, prob = c(0.1, 0.1, 0.8))code simulates 10,000 draws normal distribution mean 27 standard deviation 3 (pick one)?\nrnorm(10000, mean = 27, sd = 3)\nrnorm(27, mean = 10000, sd = 3)\nrnorm(3, mean = 10000, sd = 27)\nrnorm(27, mean = 3, sd = 1000)\nrnorm(10000, mean = 27, sd = 3)rnorm(27, mean = 10000, sd = 3)rnorm(3, mean = 10000, sd = 27)rnorm(27, mean = 3, sd = 1000)three key aspects grammar graphics (select )?\ndata\naesthetics\ntype\ngeom_histogram()\ndataaestheticstypegeom_histogram()","code":""},{"path":"r-essentials.html","id":"tutorial-2","chapter":"3 R essentials","heading":"3.9.2 Tutorial","text":"think suspicious find attracted data—, thin weak data—seem justify beliefs held great currency lots societies throughout history, way conducive oppression large segments populationAmia Srinivasan, 22 September 2021Reflect quote Amia Srinivasan, Chichele Professor Social Political Theory, Souls College, Oxford, D’Ignazio Klein (2020), especially Chapter 6, spend least two pages discussing relation dataset familiar .","code":""},{"path":"reproducible-workflows.html","id":"reproducible-workflows","chapter":"4 Reproducible workflows","heading":"4 Reproducible workflows","text":"Required materialRead happened winds changed, (Gelman 2016)Read Good enough practices scientific computing, (Wilson et al. 2017)Watch Overcoming barriers sharing code, (M. Alexander 2021)Watch Make reprex… Please, (Gelfand 2021)Read tidyverse style guide, ‘Part: Analyses’, (Wickham 2021b)Key concepts skillsReproducibility requirement, implies sharing data, code, environment.Reproducibility enhanced using R Markdown, R Projects, Git GitHub.R Markdown involves marking text certain types building document.R Projects enable file structure dependent specific directory set-.Git GitHub make easier share code data.\nGet latest changes: git pull.\nAdd updates: git add -.\nCheck everything: git status.\nCommit changes: git commit -m \"Short description changes\".\nPush changes GitHub: git push.\nGet latest changes: git pull.Add updates: git add -.Check everything: git status.Commit changes: git commit -m \"Short description changes\".Push changes GitHub: git push.Restart R often (‘Session’ -> ‘Restart R Clear Output’).Debugging skill, improves practice.One key debugging skill able make reproducible example reproduces issue others.Appropriate code structure comments critical aspect reproducibility help others understand.","code":""},{"path":"reproducible-workflows.html","id":"introduction","chapter":"4 Reproducible workflows","heading":"4.1 Introduction","text":"Suppose cancer choose black box AI surgeon explain works 90% cure rate human surgeon 80% cure rate. want AI surgeon illegal?Geoffrey Hinton, 20 February 2020.number one thing keep mind machine learning performance evaluated samples one dataset, model used production samples may necessarily follow characteristics… finance industry saying : “past performance guarantee future results”. model scoring X test dataset doesn’t mean perform level X next N situations encounters real world. future may like past.asking question, “rather use model evaluated 90% accurate, human evaluated 80% accurate”, answer depends whether data typical per evaluation process. Humans adaptable, models . significant uncertainty involved, go human. may inferior pattern recognition capabilities (versus models trained enormous amounts data), understand , can reason , can improvise faced noveltyIf every possible situation known want prioritize scalability cost-reduction, go model. Models exist encode operationalize human cognition well-understood situations. (“well understood” meaning either can explicitly described programmer, can amass dataset densely samples distribution possible situations – must static)François Chollet, 20 February 2020.science systematically building organizing knowledge terms testable explanations predictions, data science takes focuses data. means building, organizing, sharing knowledge critical aspect. Creating knowledge, , way can , meet standard. Hence, need reproducible workflows data science.M. Alexander (2019a) says ‘research reproducible can reproduced exactly, given materials used study… [hence] materials need provided!… [M]aterials usually means data, code software.’ minimum requirement another person able ‘reproduce data, methods results (including figures, tables)’. Similarly, Gelman (2016) identifies large issue various social sciences. problem work reproducible, contribute stock knowledge world. Since Gelman (2016), great deal work done many social sciences situation improved little, much work remains. situation similar life sciences (Heil et al. 2021) computer science (Pineau et al. 2021).examples Gelman (2016) talks , turned reproduce important scheme things. time, saw, continue see, similar approaches used areas big impacts. instance, many governments created ‘nudge’ units implement public policy (Sunstein Reisch 2017) increasingly using algorithms make open (Chouldechova et al. 2018). Similarly, businesses increasingly implementing machine learning methods.minimum, exceptions, must release code, datasets, environment. Without data, know finding speaks (Miyakawa 2020). banally, also know mistakes aspects inadvertently overlooked (Merali 2010) (Hillel 2017) (Silver 2020).specific, consider Y. Wang Kosinski (2018) use deep neural networks train model distinguish gay heterosexual men. (Murphy (2017) provides summary paper associated issues, along comments authors.) , Y. Wang Kosinski (2018, 248) needed dataset photos folks ‘adult, Caucasian, fully visible, gender matched one reported user’s profile’. verified using Amazon Mechanical Turk, online platform pays workers small amount money complete specific tasks. Figure 4.1, Y. Wang Kosinski (2018) supplemental materials, shows instructions provided Mechanical Turk workers task. issues instructions include Obama white mother black father classified ‘Black’; Latino ethnicity, rather race (Mattson 2017). classification task may seem objective, , perhaps unthinkingly, echoes views Americans certain class background.\nFigure 4.1: Instructions given Mechanical Turk workers removing incomplete, non-Caucasian, nonadult, nonhuman male faces\njust one specific concern one part Y. Wang Kosinski (2018) workflow. Broader concerns raised others including Gelman, Mattson, Simpson (2018). main issue statistical models specific data trained. reason can identify likely issues model Y. Wang Kosinski (2018) , despite releasing specific dataset used, nonetheless open procedure. work credible, needs reproducible others.steps can take make work reproducible include:Ensure entire workflow documented may involve addressing questions :\nraw dataset obtained access likely persistent available others?\nspecific steps taken transform raw data data analyzed, can made available others?\nanalysis done, clearly can shared?\nfinal paper report built extent can others follow process ?\nraw dataset obtained access likely persistent available others?specific steps taken transform raw data data analyzed, can made available others?analysis done, clearly can shared?final paper report built extent can others follow process ?worrying perfect reproducibility initially, instead focusing trying improve successive project. instance, following requirements increasingly onerous need concerned able last, can first:\nCan run entire workflow ?\nCan ‘another person’ run entire workflow ?\nCan ‘future’ run entire workflow ?\nCan ‘future’ ‘another person’ run entire workflow ?\nCan run entire workflow ?Can ‘another person’ run entire workflow ?Can ‘future’ run entire workflow ?Can ‘future’ ‘another person’ run entire workflow ?Including detailed discussion limitations dataset approach final paper report.workflow follow summarized Figure 4.2.\nFigure 4.2: Workflow telling stories data\nvarious tools can use different stages improve reproducibility workflow. includes use R Markdown, R Projects, Git GitHub.","code":""},{"path":"reproducible-workflows.html","id":"r-markdown","chapter":"4 Reproducible workflows","heading":"4.2 R Markdown","text":"","code":""},{"path":"reproducible-workflows.html","id":"getting-started-1","chapter":"4 Reproducible workflows","heading":"4.2.1 Getting started","text":"R Markdown mark-language similar HyperText Markup Language (HTML) LaTeX, comparison ‘See Get’ (WYSIWYG) language, Microsoft Word. means aspects consistent, instance, top-level heading look . , means must use symbols designate like certain aspects appear. build mark-get see looks like.R Markdown variant Markdown specifically designed allow R code chunks included. One advantage can get ‘live’ document code executes forms part document. Another advantage R Markdown similar code can compile variety documents, including html pages PDFs. R Markdown also default options set including title, author, date sections. One disadvantage can take document compile code needs run. N. Tierney (2020) especially useful instructions achieve specific outcomes R Markdown.can create new R Markdown document within R Studio (‘File’ -> ‘New File’ -> ‘R Markdown Document’).","code":""},{"path":"reproducible-workflows.html","id":"essential-commands","chapter":"4 Reproducible workflows","heading":"4.2.2 Essential commands","text":"Essential markdown commands include emphasis, headers, lists, links, images. reminder included R Studio (‘Help’ -> ‘Markdown Quick Reference’).Emphasis: *italic*, **bold**Headers (go line blank line ): # First level header, ## Second level header, ### Third level headerUnordered list, sub-lists:Ordered list, sub-lists:URLs can added including address auto-link: https://www.tellingstorieswithdata.com, linking text [address book](https://www.tellingstorieswithdata.com) results address book.added aspects, may want see actual document. build document click ‘Knit’.","code":"* Item 1\n* Item 2\n    + Item 2a\n    + Item 2b1. Item 1\n2. Item 2\n3. Item 3\n    + Item 3a\n    + Item 3b"},{"path":"reproducible-workflows.html","id":"r-chunks","chapter":"4 Reproducible workflows","heading":"4.2.3 R chunks","text":"can include code R many languages code chunks within R Markdown document. knit document, code run included document.create R chunk, start three backticks within curly braces tell R Markdown R chunk. Anything inside chunk considered R code run . instance, load tidyverse AER make graph number times survey respondent visited doctor past two weeks.output code Figure 4.3.\nFigure 4.3: Number doctor visits past two weeks, based 1977–1978 Australian Health Survey\nvarious evaluation options available chunks. include putting comma r specifying options closing curly brace. Helpful options include:echo = FALSE: run code include output, print code document.include = FALSE: run code output anything print code document.eval = FALSE: run code, hence include outputs, print code document.warning = FALSE: display warnings.message = FALSE: display messages.instance, include output, code, suppress warnings.","code":"```{r}\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |>\n  ggplot(aes(x = illness)) +\n  geom_histogram(stat = \"count\")\n``````{r, echo = FALSE, warning = FALSE}\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits %>%\n  ggplot(aes(x = visits)) +\n  geom_histogram(stat = \"count\")\n```"},{"path":"reproducible-workflows.html","id":"abstracts-and-pdf-outputs","chapter":"4 Reproducible workflows","heading":"4.2.4 Abstracts and PDF outputs","text":"abstract short summary paper. default preamble, can add section abstract. Similarly, can change output html_document pdf_document produce PDF. uses LaTeX background may require installation supporting packages.","code":"---\ntitle: My document\nauthor: Rohan Alexander\ndate: 1 January 2022\noutput: pdf_document\nabstract: \"This is my abstract.\"\n---"},{"path":"reproducible-workflows.html","id":"references","chapter":"4 Reproducible workflows","heading":"4.2.5 References","text":"can include references specifying bib file preamble calling within text, needed.need make separate file called ‘bibliography.bib’ save next R Markdown file. bib file need entry item referenced. instance, citation R can obtained citation() can added ‘bibliography.bib’ file. Similarly, citation package can found including package name, instance citation('tidyverse'). can helpful use Google Scholar, doi2bib, get citations books articles.need create unique key use refer item text. can anything, provided unique, meaningful ones can easier remember, instance ‘citeR’.cite R R Markdown document include @citeR, put brackets around year, like : R Core Team (2021) [@citeR], put brackets around whole thing, like : (Wickham et al. 2019a).","code":"---\ntitle: My document\nauthor: Rohan Alexander\ndate: 1 January 2022\noutput: pdf_document\nabstract: \"This is my abstract.\"\nbibliography: bibliography.bib\n---@Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2021},\n    url = {https://www.R-project.org/},\n  }\n@Article{,\n    title = {Welcome to the {tidyverse}},\n    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},\n    year = {2019},\n    journal = {Journal of Open Source Software},\n    volume = {4},\n    number = {43},\n    pages = {1686},\n    doi = {10.21105/joss.01686},\n  }@Manual{citeR,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2021},\n    url = {https://www.R-project.org/},\n  }\n@Article{citetidyverse,\n    title = {Welcome to the {tidyverse}},\n    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},\n    year = {2019},\n    journal = {Journal of Open Source Software},\n    volume = {4},\n    number = {43},\n    pages = {1686},\n    doi = {10.21105/joss.01686},\n  }"},{"path":"reproducible-workflows.html","id":"cross-references","chapter":"4 Reproducible workflows","heading":"4.2.6 Cross-references","text":"can useful cross-reference figures, tables, equations. makes easier refer text. figure refer name R chunk creates contains figure. instance, (Figure \\@ref(fig:uniquename)) produce: (Figure 4.4) name R chunk uniquename. also need add ‘fig’ front chunk name R Markdown knows figure. include ‘fig.cap’ R chunk specifies caption.\nFigure 4.4: Number illnesses past two weeks, based 1977–1978 Australian Health Survey\ncan take similar, slightly different, approach cross-reference tables. instance, (Table \\@ref(tab:docvisittable)) produce: (Table 4.1). case specify ‘tab’ unique reference table, R Markdown knows table. tables need include caption main content, ‘caption’, rather ‘fig.cap’ chunk option case figures.Table 4.1: Number visits doctor past two weeks, based 1977–1978 Australian Health SurveyFinally, can also cross-reference equations. need add tag (\\#eq:macroidentity) reference. instance, use Equation \\@ref(eq:macroidentity). produce Equation (4.1).\\[\\begin{equation}\nY = C + + G + (X - M) \\tag{4.1}\n\\end{equation}\\]using cross-references, important R chunks simple labels. general, try keep names simple unique, possible, avoid punctuation stick letters. use underbars labels cause error.","code":"```{r uniquename, fig.cap = \"Number of illnesses in the past two weeks, based on the 1977--1978 Australian Health Survey\", echo = TRUE}\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |>\n  ggplot(aes(x = illness)) +\n  geom_histogram(stat = \"count\")\nDoctorVisits |> \n  count(visits) |> \n  knitr::kable(caption = \"Number of visits to the doctor in the past two weeks, based on the 1977--1978 Australian Health Survey\")\\begin{equation}\nY = C + I + G + (X - M) (\\#eq:macroidentity)\n\\end{equation}"},{"path":"reproducible-workflows.html","id":"r-projects-and-file-structure","chapter":"4 Reproducible workflows","heading":"4.3 R projects and file structure","text":"can use R Studio create R project. means can keep files (data, analysis, report, etc) associated particular project together. project can created R Studio ‘File’ -> ‘New Project’, select ‘empty project’, name project decide save . instance, project focused maternal mortality, may called ‘maternalmortality’, might saved within folder projects. use R Projects ‘practical convention creates reliable, polite behavior across different computers users time.’ (Jennifer Bryan Hester 2020). , projects ‘neither new, unique R’, well-established part software development.project created, new file extension ‘.RProj’ appear folder. example, folder R Project, example R Markdown document, appropriate file structure available: https://github.com/RohanAlexander/starter_folder. can downloaded: ‘Code’ -> ‘Download ZIP’.main advantage using R Project easily able reference files self-contained way. means others want reproduce work, know file references structure need changed. means files referenced relation ‘.Rproj’ file . instance, instead reading csv , say, \"~/Documents/projects/book/data/\" can read book/data/. may someone else ‘projects’ folder, former work , latter .use R projects required meet minimal level reproducibility. use functions setwd(), computer-specific file paths, bind work computer way appropriate.variety ways set-folder. variant Wilson et al. (2017) often useful shown example: https://github.com/RohanAlexander/starter_folder. ‘inputs’ folder contains raw data (never modified (Wilson et al. 2017)) literature related project (modified). ‘outputs’ folder contains data create using R, well paper writing. ‘scripts’ folder modifies raw data saves ‘outputs’. Useful aspects include ‘README.md’ specify overview details project, LICENSE.","code":""},{"path":"reproducible-workflows.html","id":"git-and-github","chapter":"4 Reproducible workflows","heading":"4.4 Git and GitHub","text":"use combination Git GitHub :enhance reproducibility work making easier share code data;make easier share work;improve workflow encouraging systematic approaches; andmake easier work teams.Git version control system. One way one often starts version control various versions one file: ‘first_go.R’, ‘first_go-fixed.R’, ‘first_go-fixed--mons-edits.R’. soon becomes cumbersome. One often soon turns dates, instance: ‘2022-01-01-analysis.R’, ‘2022-01-02-analysis.R’, ‘2022-01-03-analysis.R’, etc. keeps record can difficult search need go back, can difficult remember date change made. case, quickly gets unwieldy project regularly worked .Instead , use Git can one version file, say, ‘analysis.R’ use Git keep record changes file, snapshot file given point time.determine Git takes snapshot, take snapshot, additionally include message saying changed snapshot last. way, ever one version file, history can easily searched.issue Git designed software developers. , works, can little ungainly non-developers (Figure 4.5).\nFigure 4.5: infamous response launch Dropbox 2007, trivializing use-case Dropbox, user’s approach probably work , probably folks.\nHence, GitHub, GitLab, various companies offer easier--use services build Git. introduce GitHub ‘far dominant code-hosting platform’ (Eghbal 2020, 21) built R Studio, options advantages.One challenging aspects Git terminology. Folders called ‘repos’. Saving called ‘commit’. One gets used eventually, initially feeling confused entirely normal.Jenny Bryan (2020) especially useful setting using Git GitHub.","code":""},{"path":"reproducible-workflows.html","id":"git","chapter":"4 Reproducible workflows","heading":"4.4.1 Git","text":"need git check whether Git installed. Open R Studio, go Terminal, type following, enter/return.get version number, done (Figure 4.6).\nFigure 4.6: access Terminal within R Studio\nMac Git come pre-installed, Windows chance, Linux probably need guide. get version number, need install . follow instructions specific operating system Chapter 5 Jenny Bryan (2020).Git, need tell username email. need Git adds information whenever take ‘snapshot’, use Git’s language, whenever make commit., within Terminal, type following, replacing details , enter/return line.details enter public. various ways hide email address need GitHub provides instructions . , issues, need detailed instructions step, please see Chapter 7 Jenny Bryan (2020).","code":"git --versiongit config --global user.name 'Rohan Alexander'\ngit config --global user.email 'rohan.alexander@utoronto.ca'\ngit config --global --list"},{"path":"reproducible-workflows.html","id":"github","chapter":"4 Reproducible workflows","heading":"4.4.2 GitHub","text":"Now Git set-need set-GitHub. first step create account GitHub: https://github.com (Figure 4.7).\nFigure 4.7: GitHub sign-screen\nnow need make new folder (called ‘repo’ Git). Look ‘+’ top right, select ‘New Repository’ (Figure 4.8).\nFigure 4.8: Start process creating new repository\npoint can add sensible name repo. Leave public now, can always deleted later. check box ‘Initialize repository README’. Leave ‘Add .gitignore’ set ‘None’. , click ‘Create repository’ (Figure 4.9).\nFigure 4.9: Finish creating new repository\ntake us screen fairly empty, details need green ‘Clone Download’ button, can copy clicking clipboard (Figure 4.10).\nFigure 4.10: Get details new repository\nNow returning R Studio, open Terminal, use cd navigate directory want create folder. type following, replacing repo details , enter/return.point, new folder created can now use .use GitHub project actively working follow procedure:first thing almost always pull latest changes git pull. , open Terminal, navigate folder using cd. type git pull enter/return.can make change folder, instance, update README, save normal.done, need ‘add’, ‘commit’, ‘push’.\n, use cd navigate folder, type git status enter/return see changes (see reference change made).\ntype git add -enter/return. adds changes staging area.\ntype git status enter/return verify happy added.\ntype git commit -m \"Minor update README\" enter/return. message informative change made.\n, type git status enter/return check everything.\nFinally, type git push enter/return push everything GitHub.\n, use cd navigate folder, type git status enter/return see changes (see reference change made).type git add -enter/return. adds changes staging area.type git status enter/return verify happy added.type git commit -m \"Minor update README\" enter/return. message informative change made., type git status enter/return check everything.Finally, type git push enter/return push everything GitHub.summarize workflow (assuming already relevant folder):","code":"git clone https://github.com/RohanAlexander/test.gitgit pull\ngit status\ngit add -A\ngit status\ngit commit -m \"Short commit message\"\ngit status\ngit push"},{"path":"reproducible-workflows.html","id":"using-github-within-r-studio-with-the-git-pane","chapter":"4 Reproducible workflows","heading":"4.4.3 Using GitHub within R Studio with the Git pane","text":"procedure went useful better understand happening use Git GitHub can bit cumbersome. Usefully, GitHub built R Studio can use Git pane move away Terminal.Get started creating new repo GitHub, , copy repo information, . point, open R Studio, create new project using version control (‘Files’ -> ‘New Project’ -> ‘Version Control’ -> ‘Git’, paste information repo). Follow rest set-naming project something sensible, saving somewhere sensible, clicking ‘Open new session’, creating project. create new folder R Project Git-initialized linked GitHub repo.open R Project, ‘Git’ tab (Figure 4.11).\nFigure 4.11: Git pane R Studio\ncan use Git tab. , first want ‘pull’, can clicking blue arrow. , want commit files changes, selecting ‘staged’ checkbox files like commit. ‘Commit’. , want include message commit, typing message ‘Commit message’ box ‘Commit’. Finally, can ‘Push’. details workflow available Chapter 12 Jenny Bryan (2020).","code":""},{"path":"reproducible-workflows.html","id":"using-github-with-usethis","chapter":"4 Reproducible workflows","heading":"4.4.4 Using GitHub with usethis","text":"used Git pane R Studio reduce need use Terminal, remove need go GitHub set-new project. set-Git GitHub, can improve workflow using usethis much work (Wickham Bryan 2020).installing loading usethis need check set-Git usethis::git_sitrep(). print information user. can use usethis::use_git_config() update username email address. instance,can create new R Project (‘File’ -> ‘New Project’ -> ‘New Directory’ -> ‘New Project’ -> Add name save sensible location click ‘Open new session’.)use usethis::use_git() initiate things. ask happy commit files.committed, can use usethis::use_github() push GitHub.","code":"\nlibrary(usethis)\n\nuse_git_config(user.name = \"Rohan Alexander\", user.email = \"rohan.alexander@utoronto.ca\")"},{"path":"reproducible-workflows.html","id":"using-r-in-practice","chapter":"4 Reproducible workflows","heading":"4.5 Using R in practice","text":"","code":""},{"path":"reproducible-workflows.html","id":"dealing-with-errors","chapter":"4 Reproducible workflows","heading":"4.5.1 Dealing with errors","text":"programming, eventually code break, say eventually, mean like probably 10 20 times day.Gelfand (2021)Everyone uses R, programming language matter, trouble find point. normal. Programming hard everyone struggles sometimes. point code run throw error. normal, happens everyone. Everyone gets frustrated.move forward develop strategies work issues:get error message, sometimes useful. Try read carefully see anything use .Try search, say Google, error message. can useful include ‘tidyverse’ ‘R’ search help make results appropriate. Sometimes Stack Overflow results can useful.Look help file function, putting ‘?’ function, instance, ?pivot_wider(). common issue use slightly incorrect argument name format, accidentally including string instead object name.Look error happening remove code error resolved, slowly add code back .Check class object, class(), instance, class(data_set$data_column). Ensuring expected.Restart R (‘Session’ -> ‘Restart R Clear Output’) load everything .Restart computer.Search trying , rather error, sure include ‘tidyverse’ ‘R’ search help make results appropriate. instance, ‘save PDF graph R made using ggplot’. Sometimes relevant blog posts Stack Overflow answers help.Making small, self-contained, reproducible example ‘reprex’ see issue can isolated enable others help.generally, rarely possible , almost always helpful take break come back next day.","code":""},{"path":"reproducible-workflows.html","id":"reproducible-examples","chapter":"4 Reproducible workflows","heading":"4.5.2 Reproducible examples","text":"one can advise help —one. one thing . Go .Rilke (1929)Asking help skill like . get better practice. important try say ‘doesn’t work’, ‘tried everything’, ‘code work’, ‘error message, ?’. general, possible help based comments, many possible issues. need make easy others help . involves steps.Provide small, self-contained, example data, code, detail going wrong.Document tried far, including Stack Overflow R Studio Community pages looked , quite ?clear outcome like.Begin creating minimal REPRoducible EXample, ‘reprex’. code contains needed reproduce error, needed. means code likely smaller, simpler, version nonetheless reproduces error.Sometimes process enables one solve problem. , gives someone else fighting chance able help. important recognize almost chance got problem someone addressed . likely main difficulty trying communicate trying happening, way allows others recognize . Developing tenacity important.develop reproducible examples, reprex package (Jennifer Bryan et al. 2019) especially useful. use :Load reprex package: library(reprex).Highlight copy code giving issues.Run reprex() Console.code self-contained, preview Viewer. , error, code needs re-written self-contained.need data reproduce error, use data built R. large number datasets built R can seen using library(help = \"datasets\"). possible, use common option ‘mtcars’ ‘faithful’.","code":""},{"path":"reproducible-workflows.html","id":"mentality","chapter":"4 Reproducible workflows","heading":"4.5.3 Mentality","text":"(Y)ou real, valid, competent user programmer matter IDE develop tools use make work work (L)et’s break gates, ’s enough room everyoneSharla Gelfand, 10 March 2020.write code, programmer regardless , using , . traits one tends notice great programmers common.Focused: Often aim ‘learn R’ something similar tends problematic, real end point . tends efficient smaller, specific goals, ‘make histogram 2019 Canadian Election ggplot’. something can focused achieved hours. issue goals nebulous, ‘want learn R’, becomes easy get lost tangents, much difficult get help. can demoralizing lead folks quitting early.Curious: almost always useful go. general, worst happens waste time. can rarely break something irreparably code. want know happens pass ‘vector’ instead ‘dataframe’ ggplot() try .Pragmatic: time, can useful stick within reasonable bounds, make one small change time. instance, say want run regressions, curious possibility using tidymodels package (Kuhn Wickham 2020) instead lm(). pragmatic way proceed use one aspect tidymodels package initially make another change next time.Tenacious: , balancing act. always unexpected problems issues every project. one hand, persevering despite good tendency. hand, sometimes one need prepared give something seem like break-possible. mentors can useful tend better judge reasonable. also appropriate planning useful.Planned: almost always useful excessively plan going . instance, may want make histogram 2019 Canadian Election. plan steps needed even sketch step might implemented. instance, first step get data. packages might useful? might data ? back-plan data exist ?Done better perfect: various perfectionist tendencies certain extent, can useful initially try turn certain extent. first instance, try write code works, especially early days. can always come back improve aspects . important actually ship. Ugly code gets job done, better beautiful code never finished.","code":""},{"path":"reproducible-workflows.html","id":"code-comments-and-style","chapter":"4 Reproducible workflows","heading":"4.5.4 Code comments and style","text":"Code must commented (Lee 2018). Comments focus certain code written, (lesser extent, common option selected).one way write code, especially R. However, general guidelines make easier even just working . important recognize projects evolve time, one purpose served code comments ‘[m]essages left future self (near-future others) [] help retrace justify decisions’ (Bowers 2011).Comments R can added including # symbol. put comment start line, can midway . general, need comment every aspect code comment parts obvious. instance, read value may like comment coming .comment something (Wickham 2021b). trying achieve?must comment explain weird things. Like removing specific row, say row 27, removing row? may seem obvious moment, future-six months remember.break code sections. instance, setting workspace, reading datasets, manipulating cleaning dataset, analyzing datasets, finally producing tables figures. separated comments explaining going , sometimes separate files, depending length.Additionally, top file important basic information, purpose file, pre-requisites dependencies, date, author contact information, finally red-flags todos.least every R script needs preamble clear demarcation sections.","code":"#### Preamble ####\n# Purpose: Brief sentence about what this script does\n# Author: Your name\n# Data: The date it was written\n# Contact: Add your email\n# License: Think about how your code may be used\n# Pre-requisites: \n# - Maybe you need some data or some other script to have been run?\n\n\n#### Workspace setup ####\n# do not keep the install.packages line - comment out if need be\n# Load libraries\nlibrary(tidyverse)\n\n# Read in the raw data. \nraw_data <- readr::read_csv(\"inputs/data/raw_data.csv\")\n\n\n#### Next section ####\n...\n"},{"path":"reproducible-workflows.html","id":"exercises-and-tutorial-3","chapter":"4 Reproducible workflows","heading":"4.6 Exercises and tutorial","text":"","code":""},{"path":"reproducible-workflows.html","id":"exercises-3","chapter":"4 Reproducible workflows","heading":"4.6.1 Exercises","text":"According M. Alexander (2019a) research reproducible (pick one)?\npublished peer-reviewed journals.\nmaterials used study provided.\ncan reproduced exactly without authors providing materials.\ncan reproduced exactly, given materials used study.\npublished peer-reviewed journals.materials used study provided.can reproduced exactly without authors providing materials.can reproduced exactly, given materials used study.According timeline Gelman (2016), ) Paul Meehl identify various issues; b) null hypothesis significance testing (NHST) become controversial (pick one)?\n1970s-1980s; 1990s-2000.\n1960s-1970s; 1980s-1990.\n1970s-1980s; 1980s-1990.\n1960s-1970s; 1990s-2000.\n1970s-1980s; 1990s-2000.1960s-1970s; 1980s-1990.1970s-1980s; 1980s-1990.1960s-1970s; 1990s-2000.following components project layout recommended Wilson et al. (2017) (select apply)?\nrequirements.txt\ndoc\ndata\nLICENSE\nCITATION\nREADME\nsrc\nresults\nrequirements.txtdocdataLICENSECITATIONREADMEsrcresultsBased M. Alexander (2021) please write paragraph barriers overcame, still face, regard sharing code wrote.According Gelfand (2021), key part ‘need help getting unstuck, first step create reprex, reproducible example. goal reprex package problematic code way people can run feel pain. , hopefully, can provide solution put misery.’ (pick one)?\npackage problematic code\npeople can run feel pain\nfirst step create reprex\ncan provide solution put misery\npackage problematic codeother people can run feel painthe first step create reprexthey can provide solution put miseryAccording Gelfand (2021), three key aspects reprex (select apply)?\ndata\nlibraries necessary libraries necessary\nrelevant code relevant code\ndataonly libraries necessary libraries necessaryrelevant code relevant codeAccording Wickham (2021b) naming files, files ‘00_get_data.R’, ‘get data.R’ classified (pick one)?\nbad; bad.\ngood; bad.\nbad; good.\ngood; good.\nbad; bad.good; bad.bad; good.good; good.following result bold text R Markdown (pick one)?\n**bold**\n##bold##\n*bold*\n#bold#\n**bold**##bold##*bold*#bold#option hide warnings R Markdown R chunk (pick one)?\necho = FALSE\ninclude = FALSE\neval = FALSE\nwarning = FALSE\nmessage = FALSE\necho = FALSEinclude = FALSEeval = FALSEwarning = FALSEmessage = FALSEWhich options run R code chunk display results, show code R Markdown R chunk (pick one)?\necho = FALSE\ninclude = FALSE\neval = FALSE\nwarning = FALSE\nmessage = FALSE\necho = FALSEinclude = FALSEeval = FALSEwarning = FALSEmessage = FALSEWhy R Projects important (select apply)?\nhelp reproducibility.\nmake easier share code.\nmake workspace organized.\nensure reproducibility.\nhelp reproducibility.make easier share code.make workspace organized.ensure reproducibility.Please discuss circumstance R Project useful.Consider sequence: ‘git pull, git status, ________, git status, git commit -m \"message\", git push’. missing step (pick one)?\ngit add -.\ngit status.\ngit pull.\ngit push.\ngit add -.git status.git pull.git push.Assuming libraries datasets loaded, mistake code: DoctorVisits |> select(\"visits\") (pick one)?\n\"visits\"\nDoctorVisits\nselect\n|>\n\"visits\"DoctorVisitsselect|>reprex important able make one (select apply)?\nreproducible example enables error reproduced.\nreproducible example helps others help .\nreproducible example construction may solve problem.\nreproducible example demonstrates actually tried help .\nreproducible example enables error reproduced.reproducible example helps others help .reproducible example construction may solve problem.reproducible example demonstrates actually tried help .following code produces error. Please use reprex (Jennifer Bryan et al. 2019) build reproducible example use get help , submit reprex.","code":"\nlibrary(tidyverse)\n\noecd_gdp <- \n  read_csv(\"https://stats.oecd.org/sdmx-json/data/DP_LIVE/.QGDP.../OECD?contentType=csv&detail=code&separator=comma&csv-lang=en\")\n\nhead(oecd_gdp)\n\nlibrary(forcats)\nlibrary(dplyr)\n\noecd_gdp_most_recent <- \n  oecd_gdp |> \n  filter(TIME == \"2021-Q3\",\n         SUBJECT == \"TOT\",\n         LOCATION %in% c(\"AUS\", \"CAN\", \"CHL\", \"DEU\", \"GBR\",\n                         \"IDN\", \"ESP\", \"NZL\", \"USA\", \"ZAF\"),\n         MEASURE == \"PC_CHGPY\") |> \n  mutate(european = if_else(LOCATION %in% c(\"DEU\", \"GBR\", \"ESP\"),\n                             \"European\",\n                             \"Not european\"),\n         hemisphere = if_else(LOCATION %in% c(\"CAN\", \"DEU\", \"GBR\", \"ESP\", \"USA\"),\n                             \"Northern Hemisphere\",\n                             \"Southern Hemisphere\"),\n         )\n\nlibrary(ggplot)\nlibrary(patchwork)\n\noecd_gdp_most_recent |> \n  ggplot(mapping = aes(x = LOCATION, y = Value)) |> \n  geom_bar(stat=\"identity\")"},{"path":"reproducible-workflows.html","id":"tutorial-3","chapter":"4 Reproducible workflows","heading":"4.6.2 Tutorial","text":"Please put together small R Markdown file downloads dataset using opendatatoronto, cleans , makes graph. exchange someone else. Ask read code run , provide feedback aspects. Write one--two pages single-spaced content, comments received changes make going forward.","code":""},{"path":"reproducible-workflows.html","id":"paper","chapter":"4 Reproducible workflows","heading":"4.6.3 Paper","text":"point, Paper One (Appendix B.1) appropriate.","code":""},{"path":"on-writing.html","id":"on-writing","chapter":"5 On writing","heading":"5 On writing","text":"want writer, must two things others: read lot write lot. ’s way around two things ’m aware , shortcut.S. King (2000, 145)Required materialRead Writing Well, (edition fine) Parts ‘Principles’, II ‘Methods’, (Zinsser 1976).Read Publication, publication, (G. King 2006).Read two following well-written quantitative papers:\nAsset prices exchange economy, (Lucas Jr 1978).\nIndividuals, institutions, innovation debates French Revolution, (Barron et al. 2018).\nModeling: optimal marathon performance basis physiological factors, (Joyner 1991).\nreproducible econometric research, (Koenker Zeileis 2009).\nPrevented mortality greenhouse gas emissions historical projected nuclear power, (Kharecha Hansen 2013).\nSample selection bias specification error, (Heckman 1979).\nSeeing like market, (Fourcade Healy 2017).\nSimpson’s paradox hot hand basketball, (Wardrop 1995).\nSmoking carcinoma lung, (Doll Hill 1950).\nstudies machine learning using game checkers, (Samuel 1959).\nStatistical methods assessing agreement two methods clinical measurement, (Bland Altman 1986).\nmundanity excellence: ethnographic report stratification Olympic swimmers, (Chambliss 1989).\nprobable error mean, (Student 1908).\nAsset prices exchange economy, (Lucas Jr 1978).Individuals, institutions, innovation debates French Revolution, (Barron et al. 2018).Modeling: optimal marathon performance basis physiological factors, (Joyner 1991).reproducible econometric research, (Koenker Zeileis 2009).Prevented mortality greenhouse gas emissions historical projected nuclear power, (Kharecha Hansen 2013).Sample selection bias specification error, (Heckman 1979).Seeing like market, (Fourcade Healy 2017).Simpson’s paradox hot hand basketball, (Wardrop 1995).Smoking carcinoma lung, (Doll Hill 1950).studies machine learning using game checkers, (Samuel 1959).Statistical methods assessing agreement two methods clinical measurement, (Bland Altman 1986).mundanity excellence: ethnographic report stratification Olympic swimmers, (Chambliss 1989).probable error mean, (Student 1908).Read two following articles New Yorker:\nFunny Like Guy, Tad Friend\nGoing Distance, David Remnick\nHappy Feet, Alexandra Jacobs\nLevels Game, John McPhee\nReporting Hiroshima, John Hersey\nCatastrophist, Elizabeth Kolbert\nQuiet German, George Packer\nFunny Like Guy, Tad FriendGoing Distance, David RemnickHappy Feet, Alexandra JacobsLevels Game, John McPheeReporting Hiroshima, John HerseyThe Catastrophist, Elizabeth KolbertThe Quiet German, George PackerRead two following articles miscellaneous publications:\nBlades Glory, Holly Anderson\nBorn Run, Walt Harrington\nDropped, Jason Fagone\nFederer Religious Experience, David Foster Wallace\nGeneration ?, Zadie Smith\nOne hundred years arm bars, David Samuels\nGreat Alone, Brian Phillips\nPearls Breakfast, Gene Weingarten\nCult ‘Jurassic Park’, Bryan Curtis\nHouse Hova Built, Zadie Smith\nRe-Education Chris Copeland, Flinder Boyd\nSea Crisis, Brian Phillips\nBlades Glory, Holly AndersonBorn Run, Walt HarringtonDropped, Jason FagoneFederer Religious Experience, David Foster WallaceGeneration ?, Zadie SmithOne hundred years arm bars, David SamuelsOut Great Alone, Brian PhillipsPearls Breakfast, Gene WeingartenThe Cult ‘Jurassic Park’, Bryan CurtisThe House Hova Built, Zadie SmithThe Re-Education Chris Copeland, Flinder BoydThe Sea Crisis, Brian PhillipsKey concepts skillsTo get better writing, write, ideally every day.Write reader.one message want communicate.Get first draft quickly possible.Rewrite brutally.Remove many words possible.","code":""},{"path":"on-writing.html","id":"introduction-1","chapter":"5 On writing","heading":"5.1 Introduction","text":"[T]duty scientist find new things, communicate successfully least three forms: 1) Writing papers books. 2) Prepared public talks. 3) Impromptu talks.Hamming (1996, 65)People need write: founders, VCs, lawyers, software engineers, designers, painters, data scientists, musicians, filmmakers, creative directors, physical trainers, teachers, writers.\nLearn write.Sahil Lavingia, 3 February 2020.Writing well done just much knowing code. ’d add ’re intimidated writing, start blog write often something ’re interested . ’ll get better. least ’s ’ve done past 10 years. :)Vicki Boykis, 3 February 2020.need write order tell stories. Writing allows us communicate efficiently. also way work believe allows us get feedback ideas. Effective papers tightly written well-organized, makes story flow easy follow. Proper sentence structure, spelling, vocabulary, grammar important remove distractions enable point clearly articulated. Effective papers demonstrate understanding topic confidently using relevant terms techniques considering issues without overly verbose. Graphs, tables, references used enhance story credibility.chapter writing. end , better idea write short, detailed, quantitative papers communicate want , waste reader’s time. write reader, . Specifically, write useful reader, ‘[u]seful writing tells people something true important didn’t already know, tells unequivocally possible’ (Graham 2020). said, greatest benefit writing nonetheless often accrues writer, even write audience. process writing way work think came believe .way piece writing three four times , never . , hardest part comes first, getting something—anything—front . Sometimes nervous frenzy just fling words flinging mud wall. Blurt , heave , babble something—anything—first draft. , achieved sort nucleus. , work alter , begin shape sentences score higher ear eye. Edit —top bottom. chances now ’ll seeing something sort eager others see. takes times. left interstitial time. finish first awful blurting, put thing aside. get car drive home. way, mind still knitting words. think better way say something, good phrase correct certain problem. Without drafted version—exist—obviously thinking things improve . short, may actually writing two three hours day, mind, one way another, working twenty-four hours day—yes, sleep—sort draft earlier version already exists. exists, writing really begun.McPhee (2017, 159)process writing process re-writing. critical task get first draft quickly possible. complete first draft five--ten-page quantitative paper can done day. complete first draft exists, useful try delete even revise anything written, regardless bad may seem. Just write.One intimidating things world blank page, deal immediately adding headings : ‘Introduction’, ‘Data’, ‘Model’, ‘Results’, ‘Discussion’. add fields top matter various bits pieces needed, ‘title’, ‘date’, ‘author’ ‘abstract’. creates generic outline, role akin placing counter, ingredients use prepare dinner (McPhee 2017).established generic outline, need develop understanding exploring developing research question. theory, develop research question, answer , writing; rarely actually happens (Franklin 2005). Instead, typically idea question, answer, become less vague write. process writing refine thinking (S. King 2000, 131). put thoughts research question, can start add dot points sections, adding sub-sections, informative sub-headings needed. go back expand dot points paragraphs.writing first draft important ignore feeling one good enough, impossible. Just write. need words paper, even bad, first draft accomplish . Remove distractions just write. Perfectionism enemy, set aside. Sometimes can accomplished getting early write, creating deadline, glass two wine. One friend puts baby sleep sound typing, result must keep typing otherwise baby wake . Creating sense urgency can useful rather adding proper citations go, slow us , just add something like ‘[TODO: CITE R ]’. similar graphs tables. , include textual descriptions ‘[TODO: ADD GRAPH SHOWS COUNTRY TIME ]’ instead actual graphs tables. Focus adding content, even poorly written, ideal. done, first draft exists!first draft bad. writing bad first draft can get good second draft, great third draft, eventually excellence (Lamott 1994, 20). first draft long, make sense, contain claims supported, claims . focused adding content writing first draft, turn second draft, use ‘delete’ key extensively, well ‘cut’ ‘paste’. Printing paper using red pen move remove especially helpful. process going first draft second draft best done one sitting, help flow consistency story. One aspect first re-write enhancing story want tell. another aspect taking everything story (S. King 2000, 57).go written sections, try bring sense , special consideration supports story. revision process essence writing (McPhee 2017, 160). also fix references, add real graphs tables. part re-writing process, paper’s central message tends develop, answers research questions tend become clearer. point, aspects introduction can returned , finally, abstract. Typos issues affect credibility work, important fixed part second draft.now paper sensible. job now make brilliant. Print , go paper. especially important brutally remove everything contribute story. stage, may starting get close paper. write reader, great opportunity give someone else comments. ask feedback enables us better understand weak parts story. addressing , can helpful go paper , time reading aloud. paper tends never ‘done’ certain point either run time become sick sight .","code":""},{"path":"on-writing.html","id":"developing-research-questions","chapter":"5 On writing","heading":"5.2 Developing research questions","text":"qualitative quantitative approaches place, focus quantitative approaches. Qualitative research important well, often interesting work little . conducting quantitative analysis, subject issues data quality, scales, measurement, sources. often especially interested trying tease causality. Regardless, trying learn something world. research questions need take account.Broadly, two ways go research:data-first; orquestion-first.data-first, main issue working questions can reasonably answered available data. deciding , useful consider:Theory: reasonable expectation something causal determined? instance, question involves charting stock market, might better consider haruspex least way something eat. Questions usually need plausible theoretical underpinning help avoid spurious relationships.Importance: plenty trivial questions can answered, important waste time reader. important question can also help motivation find , say, fourth straight week cleaning data de-bugging code. can also make easier attract talented employees funding. said, balance needed. important question decent chance answered. attacking generational-defining question might best broken smaller chunks.Availability: reasonable expectation additional data available future? allow us answer related questions turn one paper research agenda.Iteration: something run multiple times, -analysis? former, becomes possible start answering specific research questions iterate. can get access data need think broader questions.’s saying, sometimes attributed Xiao-Li Meng statistics missing data problem. paradoxically, another way ask data-first questions think data . instance, returning neonatal maternal mortality examples discussed , respectively, Chapters 1 2, fundamental problem perfect complete data cause death. , count number relevant deaths. established missing data problem, can take data-driven approach looking data , ask research questions speak extent can use approximate hypothetical perfect complete dataset.One way researchers data-first, develop particular expertise data geographical historical circumstance. instance, may especially knowledgeable present-day UK, late nineteenth century Japan. look questions researchers asking circumstances, bring data question. instance, common see particular question initially asked US, host researchers answer question UK, Canada, Australia, many countries.variant data-driven research model-driven research. researcher becomes expert particular statistical approach applies approach whenever appropriate datasets.trying question-first, inverse different issue concerned data availability. ‘FINER framework’ used medicine help guide development research questions. recommends asking questions : Feasible, Interesting, Novel, Ethical, Relevant (Hulley 2007). Farrugia et al. (2010) builds FINER PICOT, recommends additional considerations: Population, Intervention, Comparison group, Outcome interest, Time. can feel overwhelming trying write question. One way go ask specific question. Another decide whether interested descriptive, predictive, inferential, causal analysis.lead different types questions, instance, descriptive analysis: ‘\\(x\\) look like?’; predictive analysis: ‘happen \\(x\\)?’; inferential: ‘can explain \\(x\\)?’; causal: ‘impact \\(x\\) \\(y\\)?’. role play.Often time constrained, possibly interesting ways can guide specifics research question. interested effect Trump’s tweets stock market, can done just looking minutes (milliseconds?) tweets. interested effect cancer drug long term outcomes? effect takes 20 years, must either wait , need look people treated 2000, selection effects different circumstances give drug today. Often reasonable thing build statistical model, need adequate sample sizes, etc.answering questions usually, creation counterfactual crucial. Briefly, counterfactual -statement ‘’ false. Consider example Humpty Dumpty Lewis Carroll’s Looking-Glass (Carroll 1871).‘tremendously easy riddles ask!’ Humpty Dumpty growled . ‘course don’t think ! , ever fall —’s chance ——’ pursed lips looked solemn grand Alice hardly help laughing. ‘fall,’ went , ‘King promised —mouth---’Humpty satisfied happen fall , even though similarly satisfied never happen. comparison group often determines answer question. instance, consider effect VO2 max outcome bike race. compare general population important variable, compare elite athletes, less important, selection.Two aspects especially aware deciding research question selection bias, measurement bias.Selection bias occurs results depend sample. One pernicious aspects selection bias need know existence order anything . many default diagnostics identify selection bias. /B testing set-, discuss Chapter 10, /testing can help us identify selection bias. generally, comparing properties sample, age-group, gender, education, characteristics population. fundamental problem selection bias observational data, , Dr Jill Sheppard, Lecturer, Australian National University, says, people respond surveys weird. weirdness likely generalizes almost method collecting data.pernicious aspect selection bias, pervades every aspect analysis. Even sample starts representative, may become selected time. instance, survey panels used polling, discussed Chapter 8, need updated time time folks get anything stop responding.Another bias aware measurement bias, results affected data collected.instance, ask respondents income, may get different answers -person, compared online survey.","code":""},{"path":"on-writing.html","id":"writing","chapter":"5 On writing","heading":"5.3 Writing","text":"indeed published anything commenced “Professor”, many crude effort, destroyed almost soon composed, got taste might ornamented redundant composition, come prefer plain homely.Professor (Brontë 1857).discuss following components: title, abstract, introduction, data, results, discussion, figures, tables, equations, technical terms. Throughout sections paper important brief specific possible.","code":""},{"path":"on-writing.html","id":"title","chapter":"5 On writing","heading":"5.3.1 Title","text":"title first opportunity engage reader story. Ideally, able tell reader exactly found. Effective titles critical otherwise papers ignored readers. title ‘cute’, need effective. means needs make story clear.One example title good enough ‘2016 Brexit referendum’. title useful reader least knows paper . particular informative enticing. slightly better variant ‘’Vote Leave’ outcome 2016 Brexit referendum’. variant adds specifically particularly informative. Finally, another variant ‘Vote Leave outperforms rural areas 2016 Brexit referendum: Evidence Bayesian hierarchical model’. reader knows approach paper also main take-away.consider examples particularly effective titles. Hug et al. (2019) uses ‘National, regional, global levels trends neonatal mortality 1990 2017, scenario-based projections 2030: systematic analysis’. clear paper methods used. R. Alexander Alexander (2021) uses ‘Increased Effect Elections Changing Prime Ministers Topics Discussed Australian Federal Parliament 1901 2018’. method used paper clear title, main finding , along good deal information content . finally, M. J. Alexander, Kiang, Barbieri (2018) uses ‘Trends Black White Opioid Mortality United States, 1979–2015’.title often among last aspects paper finalized. getting first draft, typically just use working title good enough get job done. refine course redrafting. title needs reflect final story paper, usually something know start. interested striking balance getting reader interested enough read paper, conveying enough content useful (Hayot 2014). can think classic books, Macaulay’s History England Accession James Second, Churchill’s History English-Speaking Peoples. clear content , , target audience, spark interest.One specific approach form: ‘Exciting content: Specific content’, instance, ‘Returning roots: Examining performance ’Vote Leave’ 2016 Brexit referendum’. Kennedy Gelman (2020) provides particular nice example approach ‘Know population know model: Using model-based regression poststratification generalize findings beyond observed sample’, Craiu (2019) ‘Hiring Gambit: Search Twofer Data Scientist’. close variant ‘question? answer’. instance, Cahill, Weinberger, Alkema (2020) ‘increase modern contraceptive use needed FP2020 countries reach 75% demand satisfied 2030? assessment using Accelerated Transition Method Family Planning Estimation Model’. one gains experience variant, becomes possible know appropriate drop answer part yet remain effective, Briggs (2021) ‘Aid Target Poorest?’. Another specific approach ‘Specific content broad content’ inversely. instance, ‘Rurality, elites, support ’Vote Leave’ 2016 Brexit referendum’ ‘Support ’Vote Leave’ 2016 Brexit referendum, rurality elites. approach used Tolley Paquet (2021) ‘Gender, municipal party politics, Montreal’s first woman mayor’.","code":""},{"path":"on-writing.html","id":"abstract","chapter":"5 On writing","heading":"5.3.2 Abstract","text":"five--ten-page paper, good abstract three five sentence paragraph. longer paper abstract can slightly longer. abstract needs specify story paper, objective abstract convey done matters. abstract typically touches context work, objectives, approach, findings.specifically, good recipe abstract : first sentence: specify general area paper encourage reader; second sentence: specify dataset methods general level; third sentence: specify headline result; fourth sentence implications.see pattern variety abstracts. instance, Tolley Paquet (2021) draw reader first sentence mentioning election first woman mayor 400 years. second sentence clear done paper. third paper tells reader done .e. survey. fourth sentence adds detail. fifth final sentence makes main take-away paper clear.2017, Montreal elected Valérie Plante, first woman mayor city’s 400-year history. Using election case study, show gender influence outcome. survey Montreal electors suggests gender salient factor vote choice. Although gender matter much voters, shape organization campaign party. argue Plante’s victory can explained part strategy showcased less leader-centric party degendered campaign helped counteract stereotypes women’s unsuitability positions political leadership.Similarly, Beauregard Sheppard (2021) make broader environment clear within first two sentences, specific contribution paper environment. third fourth sentences makes data source clear also main findings. fifth sixth sentences add specificity interest likely readers abstract .e. academic political science experts. final sentence makes clear position authors.Previous research support gender quotas focuses attitudes toward gender equality government intervention explanations. argue role attitudes toward women understanding support policies aiming increase presence women politics ambivalent—hostile benevolent forms sexism contribute understanding support, albeit different ways. Using original data survey conducted probability-based sample Australian respondents, findings demonstrate hostile sexists likely oppose increasing women’s presence politics adoption gender quotas. Benevolent sexists, hand, likely support policies respondents exhibiting low levels benevolent sexism. argue benevolent sexism holds women pure need protection; takes succeed politics without assistance quotas. Finally, show women likely support quotas, ambivalent sexism relationship support among women men. findings suggest aggregate levels public support gender quotas necessarily represent greater acceptance gender equality generally.finally, Briggs (2021) begins claim seems unquestionably true. second sentence claims found false. third sentence specifies extent claim, fourth sentence details comes position, providing detail. final two sentences speak broad implications importance.Foreign-aid projects typically local effects, need placed close poor reduce poverty. show , conditional local population levels, World Bank (WB) project aid targets richer parts countries. relationship holds time across world regions. test five donor-side explanations pro-rich targeting using pre-registered conjoint experiment WB Task Team Leaders (TTLs). TTLs perceive aid-receiving governments interested targeting aid politically controlling implementation. also believe aid works better poorer remote areas, implementation areas uniquely difficult. results speak debates distributive politics, international bargaining aid, principal-agent issues international organizations. results also suggest tweaks WB incentive structures make ease project implementation less important may encourage aid flow poorer parts countries.journal Nature provides guide constructing abstract. recommend structure results abstract six parts, add around 200 words.basic introductory sentence comprehensible wide audience.detailed sentence background relevant likely readers.sentence states general problem.Sentences summarize explain main results.sentence general context.finally, sentence broader perspective.critical first sentence abstract vacuous. Assuming reader continued past title, first sentence next opportunity implore keep reading paper. second sentence abstract, . Work re-work abstract good fine thing read.","code":""},{"path":"on-writing.html","id":"introduction-2","chapter":"5 On writing","heading":"5.3.3 Introduction","text":"introduction needs self-contained convey everything reader needs know. important recognize writing mystery story. Instead, want give-away important points introduction. six-page paper, introduction may two three paragraphs main content. Hayot (2014, 90) describes goal introduction engage reader, locate discipline background, tell happens rest paper. completely reader-focused.introduction set scene give reader background. instance, typically start little broader. provides context paper. describe paper fits context, give high-level results, especially focused one key result main part story. provide detail provided abstract, full extent. final bit main content broadly discuss next steps. Finally, finish introduction additional short final paragraph highlights structure paper.example (made-details):UK Conservative Party always done well rural electorates. 2016 Brexit vote different significant different support rural urban areas. even standard rural support conservative issues, support ‘Vote Leave’ unusually strong ‘Vote Leave’ heavily supported East Midlands East England, strongest support ‘Remain’ Greater London.paper look performance ‘Vote Leave’ 2016 Brexit referendum correlated rurality. construct model support ‘Vote Leave’ voting area level, explained number farms area, average internet connectivity, median age. find median age area increases, likelihood area supported ‘Vote Leave’ decreases 14 percentage points. Future work look effect Conservative MP allow nuanced understanding effects.remainder paper structured follows: Section 2 discusses data, Section 3 discusses model, Section 4 presents results, finally Section 5 discusses findings weaknesses.introduction needs self-contained tell reader everything need know. reader able read introduction accurate picture major aspects read whole paper. rare include graphs tables introduction. introduction always closes structure paper. instance (just rough guide) introduction 10-page paper, probably 3 4 paragraphs, 10 per cent, depends specifics.","code":""},{"path":"on-writing.html","id":"data","chapter":"5 On writing","heading":"5.3.4 Data","text":"Robert Caro, Lyndon B. Johnson’s biographer, describes importance conveying ‘sense place’ writing biography (Caro 2019, 141). defines ‘physical setting book’s action occurring: see clearly enough, sufficient detail, feels present action occurring.’ provides following example:Rebekah walked front door little house, nothing—roadrunner streaking behind rocks something long wet dangling beak, perhaps, rabbit disappearing around bush fast really saw flash white tail—otherwise nothing. movement except ripple leaves scattered trees, sound except constant whisper wind… Rebekah climbed, almost desperation, hill back house, saw crest hills, endless vista hills, hills visible single house… hills nothing moved, empty hills , , empty sky; hawk circling silently overhead event. Bus , nothing human, one talk .Caro (2019, 146)thoroughly can imagine circumstances Rebekah Baines Johnson (Lyndon B. Johnson’s mother). need provide reader sense place dataset.\n\nwriting papers, need achieve sense place, data, Caro able provide Hill county. explicit possible showing dataset. typically whole section designed show reader, closely possible, actual data underpin story.writing data section, beginning answer critical question claims, , possible know ? (McPhee 2017, 78). preeminent example data section provided Doll Hill (1950), interested effect smoking control treatment groups. begin clearly describing dataset. use tables display relevant cross-tabs. use graphs contrast groups.data section need thoroughly discuss variables dataset using. datasets used, , mentioned choices justified. variables constructed combined, process motivation explained.get sense data, important reader able understand data underpin results look like. means graph actual data used analysis, close possible. also include tables summary statistics. dataset created source, can also help include example original source. instance, dataset created survey responses survey form included, potentially appendix.data section also figures tables. judgment required. important reader opportunity understand details, may better placed appendix. Figure tables critical aspect convincing people story. graph can show data let reader decide . using table, can easily summarize dataset. least, every variable needs shown graph summarized table. Figures tables numbered cross-referenced text, instance, “Figure 1 shows…”, “Table 1 describes…”. every graph table extensive accompanying text describes main aspects, adds additional detail.discuss components graphs tables, including titles labels, Chapter 6. discuss captions, text graph table. Captions need informative self-contained. Cleveland (1994, 57) says, ‘interplay graph, caption, text delicate one’, however reader able read caption understand graph table shows. caption two three lines long necessarily inappropriate. aspects graph table explained. instance, consider Figures 5.1 5.2 Bowley (1901, 151), exceptionally clear, self-contained.\nFigure 5.1: Example well-captioned figure\n\nFigure 5.2: Example well-captioned table\nchoice table graph comes much information conveyed. general, specific information considered, summary statistic, table good option, interested reader making comparisons understanding trends graph good option (Gelman, Pasarica, Dodhia 2002).Finally, relevant literature discuss throughout paper appropriate. instance, literature relevant data discussed section, literature relevant model, results, discussion mentioned appropriate sections. rarely necessary separate literature review section.","code":""},{"path":"on-writing.html","id":"model","chapter":"5 On writing","heading":"5.3.5 Model","text":"often build statistical model use explore data, often specific section . minimum important clearly specify equation/s describe model used, explain components plain language cross-references.model section typically begins model written , explained, justified. Depending expected reader, background may needed. specifying model appropriate mathematical notation cross-referencing , components model typically defined explained. especially important define aspect notation. helps convince reader model well-chosen enhances credibility paper. model’s variables correspond discussed data section, making clear link two sections.discussion features enter model . instance, examples include, use ages rather age-groups, state/province levels effect, gender categorical variable. general, trying convey sense model situation. want reader understand aspects discussed data section assert modelling decisions made.model section close discussion assumptions underpin model, brief discussion alternative models, variants, strengths weaknesses made clear. clear reader’s mind model chosen.point section, usually appropriate specify software used run model, provide evidence thought circumstances model may appropriate. later point typically expanded discussion. evidence model validation checking, model convergence, /diagnostic issues. , balance needed , content may appropriate placed appendices.technical terms used, briefly explained plain language readers might familiar . instance, M. Alexander (2019b) integrates explanation Gini coefficient brings reader along.look concentration baby names, let’s calculate Gini coefficient country, sex year. Gini coefficient measures dispersion inequality among values frequency distribution. can take value 0 1. case income distributions, Gini coefficient 1 mean one person income. case, Gini coefficient 1 mean babies name. contrast, Gini coefficient 0 mean names evenly distributed across babies.","code":""},{"path":"on-writing.html","id":"results","chapter":"5 On writing","heading":"5.3.6 Results","text":"Two excellent examples results sections provided Kharecha Hansen (2013) Kiang et al. (2021). results section, want communicate outcomes model clear way without much way discussion implications. results section likely requires summary statistics, tables, graphs. aspects cross-referenced text associated details seen . section strictly relay results; , interested results , rather mean.section also typically include table/s coefficient estimates based modelling used explore data. Various features estimates discussed, differences models explained. may different subsets data considered separately. , graphs tables need plain language text accompany . rough guide amount text least equal amount space taken tables graphs. instance, full page used display table coefficient estimates, cross-referenced accompanied least full page text table.","code":""},{"path":"on-writing.html","id":"discussion","chapter":"5 On writing","heading":"5.3.7 Discussion","text":"discussion section may final section paper typically four five sub-sections.discussion section typically begin sub-section comprises one- two-paragraph summary done paper. followed two three sub-sections devoted key things learn world paper. instance, typically implications come modelling results. sub-sections main opportunity justify detail implications story told paper. Typically, sub-sections see newly introduced graphs tables, instead focused learn introduced earlier sections. may results discussed relation others found, differences attempted reconciled .Following sub-sections learn world, typically sub-section focused weaknesses done. concern aspects data used, approach, model. final sub-section typically paragraphs specify left learn, future work proceed.general, expect section take least twenty-five per cent total paper. instance, eight-page paper, expect least two pages discussion.","code":""},{"path":"on-writing.html","id":"brevity-typos-and-grammar","chapter":"5 On writing","heading":"5.3.8 Brevity, typos, and grammar","text":"Brevity important. Partly write reader, reader priorities. also writer focuses us consider important points , can best support , arguments weakest. Jean Chrétien, former Canadian Prime Minister, describes ‘[t]o allow get heart issue quickly, asked officials summarize documents two three pages attach rest materials background information. soon discovered problem didn’t really know talking .’ (Chrétien 2007, 105).experience unique Canada. instance, Oliver Letwin, former British Conservative Cabinet member, describes ‘huge amount terrible guff, huge, colossal, humongous length coming departments’ asked ‘one quarter length’ (Hughes Rutter 2016). found departments able accommodate request without losing anything important.experience also new. instance, Churchill asked brevity Second World War, saying ‘discipline setting real points concisely prove aid clearer thinking’. letter Szilard Einstein FDR catalyst Manhattan Project two pages.experience also unique academia. instance, one foundations Amazon, one world’s largest companies, clear writing. Specifically, instead PowerPoint presentations, Jeff Bezos asked ‘[w]ell structured, narrative text… [] forces better thought better understanding ’s important , things related.’Zinsser (1976) goes describes ‘secret good writing’ ‘strip every sentence cleanest components.’ Every sentence simplified essence. every word contribute removed.Typos grammatical mistakes affect credibility claims. reader trust us use spell-checker, trust us use logistic regression? Microsoft Word Google Docs useful spell-checkers: copy/paste R Markdown, look red green lines, fix R Markdown.worried n-th degree grammatical content. Instead, interested grammar sentence structure occurs conversational language use (S. King 2000, 118). way develop comfort reading lot asking others read work also.Unnecessary words, typos, grammatical issues removed papers fanatical zeal.","code":""},{"path":"on-writing.html","id":"rules","chapter":"5 On writing","heading":"5.3.9 Rules","text":"variety authors established rules writing, including famously, Orwell (1946), reimagined Economist (2013). Fiske Kuriwaki (2021) list rules scientific papers. reimagining, focused telling stories data, :Focus reader needs. Everything else comment.Establish logical structure rely structure tell story.Write first draft quickly possible.Re-write extensively without favor.Aim concise direct. Remove many words possible.Using words precisely. Stock-markets rise fall, improve worsen.Use short sentence possible.Avoid jargon.Write though work front page newspaper. .","code":""},{"path":"on-writing.html","id":"exercises-and-tutorial-4","chapter":"5 On writing","heading":"5.4 Exercises and tutorial","text":"","code":""},{"path":"on-writing.html","id":"exercises-4","chapter":"5 On writing","heading":"5.4.1 Exercises","text":"According Introduction Zinsser (1976), whose picture hangs Zinsser’s office?\nCharlotte Bronte\nE. M. Forster\nE. B. White\nStephen King\nCharlotte BronteE. M. ForsterE. B. WhiteStephen KingAccording Chapter 2 Zinsser (1976), secret good writing?\nCorrect sentence structure grammar.\nuse long words, adverbs, passive voice.\nThorough planning.\nStrip every sentence cleanest components.\nCorrect sentence structure grammar.use long words, adverbs, passive voice.Thorough planning.Strip every sentence cleanest components.According Chapter 2 Zinsser (1976), must writer constantly ask?\ntrying say?\nwriting ?\ncan re-written?\nmatter?\ntrying say?writing ?can re-written?matter?two repeated words, instance Chapter 3, characterize advice Zinsser (1976)?\nRe-write, re-write.\nRemove, remove.\nSimplify, simplify.\nLess, less.\nRe-write, re-write.Remove, remove.Simplify, simplify.Less, less.According Chapter 5 Zinsser (1976), writer never say anything writing wouldn’t say ?\nPrivate\nPublic\nConversation\nSpeeches\nPrivatePublicConversationSpeechesAccording Chapter 6 Zinsser (1976), tools writer ?\nPapers\nWords\nParagraphs\nSentences\nPapersWordsParagraphsSentencesAccording G. King (2006), key task subheadings (pick one)?\nEnable reader randomly falls asleep keeps turning pages know .\nbroad sweeping reader impressed importance paper.\nUse acronyms integrate paper literature.\nEnable reader randomly falls asleep keeps turning pages know .broad sweeping reader impressed importance paper.Use acronyms integrate paper literature.According G. King (2006), maximum length abstract (pick one)?\nTwo hundred words.\nTwo hundred fifty words.\nOne hundred words.\nOne hundred fifty words.\nTwo hundred words.Two hundred fifty words.One hundred words.One hundred fifty words.According G. King (2006), paper, raw computer output (pick one)?\nCommented .\nincluded.\nIncluded.\nCommented .included.Included.According G. King (2006), standard error 0.05 following specificity coefficient silly (select apply)?\n2.7182818\n2.718282\n2.72\n2.7\n2.7183\n2.718\n3\n2.71828\n2.71828182.7182822.722.72.71832.71832.71828When try use ‘delete’ key (pick one)?\nwriting first draft.\nwriting second draft.\nwriting third draft.\n‘delete’ key always used.\nwriting first draft.writing second draft.writing third draft.‘delete’ key always used.long first draft take write five--ten-page paper (pick one)?\nOne hour\nOne day\nOne week\nOne month\nOne hourOne dayOne weekOne monthWhat key aspect re-drafting process (select apply)?\nGoing red pen remove unneeded words.\nPrinting paper reading physical copy.\nCutting pasting enhance flow.\nReading aloud.\nExchanging others.\nGoing red pen remove unneeded words.Printing paper reading physical copy.Cutting pasting enhance flow.Reading aloud.Exchanging others.three features good research question (write paragraph two)?challenges ‘data-first’ (write paragraph two)?challenges ‘question-first’ (write paragraph two)?counterfactual (pick one)?\n-statements happen.\n-statements happens.\nStatements either true false.\nStatements neither true false.\n-statements happen.-statements happens.Statements either true false.Statements neither true false.following best title (pick one)?\n“Problem Set 1”\n“Unemployment”\n“Examining England’s Unemployment (2010-2020)”\n“England’s Unemployment Increased 2010 2020”\n“Problem Set 1”“Unemployment”“Examining England’s Unemployment (2010-2020)”“England’s Unemployment Increased 2010 2020”following best title (pick one)?\n“Problem Set 2”\n“Standard errors”\n“standard errors small samples”\n“Problem Set 2”“Standard errors”“standard errors small samples”word/s can removed following sentence without substantially affecting meaning (select apply)? ‘Like many parents, children born, one first things wife regularly read stories .’\nfirst\nregularly\nstories\nfirstregularlystoriesPlease write new title either Barron et al. (2018) Fourcade Healy (2017).Please write new title first article list articles New Yorker read.Please write new title article list articles New Yorker read.Please write new four-sentence abstract Chambliss (1989)Please write new four-sentence abstract Doll Hill (1950) Student (1908) Kharecha Hansen (2013).Please write abstract first article list ‘miscellaneous’ articles read.Please write abstract article list ‘miscellaneous’ articles read.Using 1000-popular words English language – https://xkcd.com/simplewriter/ – re-write following retains original meaning:using data, try tell convincing story. may exciting predicting elections, banal increasing internet advertising click rates, serious finding cause disease, fun forecasting basketball games. case key elements .","code":""},{"path":"on-writing.html","id":"tutorial-4","chapter":"5 On writing","heading":"5.4.2 Tutorial","text":"Caro (2019, xii) writes least one thousand words almost every day. tutorial write every day week. day pick one papers specified required materials complete following tasks:Transcribe, writing word , entire introduction.(idea comes McPhee (2017, 186).) Re-write introduction five lines (10 per cent, whichever less) shorter.Transcribe, writing word , abstract.Re-write new, four-sentence, abstract paper.(idea comes comes Chelsea Parlett-Pelleriti.) Write second version new abstract using one thousand popular words English language: https://xkcd.com/simplewriter/.Detail three points way paper written likeDetail one point way paper written like.Please use R Markdown produce single PDF whole week. Make judicious use headings sub-headings structure submission. Submit PDF.","code":""},{"path":"static-communication.html","id":"static-communication","chapter":"6 Static communication","heading":"6 Static communication","text":"Required materialRead R Data Science, Chapter 28 ‘Graphics communication’, (Wickham Grolemund 2017).Read Data Visualization: Practical Introduction, Chapters 3 ‘Make plot’, 4 ‘Show right numbers’, 5 ‘Graph tables, add labels, make notes’, (Healy 2018).Read Testing Statistical Charts: Makes Good Graph?, (Vanderplas, Cook, Hofmann 2020).Read Data Feminism, Chapter 3 ‘Rational, Scientific, Objective Viewpoints Mythical, Imaginary, Impossible Standpoints’, (D’Ignazio Klein 2020).Key concepts skillsKnowing importance showing reader actual dataset, close possible.Using variety different graph options, including bar charts, scatterplots, line plots, histograms.Knowing use tables show part dataset, communicate summary statistics, display regression results.Approaching maps type graph.Comfort geocoding places.Key librariesdatasauRus (Locke D’Agostino McGowan 2018)ggmap (Kahle Wickham 2013)kableExtra (Zhu 2020)knitr (Xie 2021)mapsmodelsummary (Arel-Bundock 2021a)opendatatoronto (Gelfand 2020)patchwork (Pedersen 2020)tidyverse (Wickham et al. 2019a)viridis (Garnier et al. 2021)WDI (Arel-Bundock 2021b)Key functionsggmap::get_googlemap()ggmap::get_stamenmap()ggmap::ggmap()ggplot2::coord_map()ggplot2::facet_wrap()ggplot2::geom_abline()ggplot2::geom_bar()ggplot2::geom_boxplot()ggplot2::geom_dotplot()ggplot2::geom_freqpoly()ggplot2::geom_histogram()ggplot2::geom_jitter()ggplot2::geom_line()ggplot2::geom_path()ggplot2::geom_point()ggplot2::geom_polygon()ggplot2::geom_smooth()ggplot2::geom_step()ggplot2::ggplot()ggplot2::ggsave()ggplot2::labeller()ggplot2::labs()ggplot2::map_data()ggplot2::scale_color_brewer()ggplot2::scale_colour_viridis_d()ggplot2::scale_fill_brewer()ggplot2::scale_fill_viridis()ggplot2::stat_qq()ggplot2::stat_qq_line()ggplot2::theme()ggplot2::theme_bw()ggplot2::theme_classic()ggplot2::theme_linedraw()ggplot2::theme_minimal()kableExtra::add_header_above()knitr::kable()lm()maps::map()modelsummary::datasummary()modelsummary::datasummary_balance()modelsummary::datasummary_correlation()modelsummary::datasummary_skim()modelsummary::modelsummary()WDI::WDI()WDI::WDIsearch()","code":""},{"path":"static-communication.html","id":"introduction-3","chapter":"6 Static communication","heading":"6.1 Introduction","text":"telling stories data, like data much work convincing reader. paper medium, data message. end, want try show reader data allowed us come understanding story. use graphs, tables, maps help achieve .critical task show actual data underpin analysis, close can. instance, dataset consists 2,500 responses survey, point paper expect graph contains 2,500 points. build graphs using ggplot2 (Wickham 2016). go variety different options including bar charts, scatterplots, line plots, histograms.contrast role graphs, show actual data, close possible, role tables typically show extract dataset convey various summary statistics. build tables using knitr (Xie 2021) kableExtra (Zhu 2020) initially, modelsummary (Arel-Bundock 2021a).Finally, cover maps variant graphs used show particular type data. build static maps using ggmap (Kahle Wickham 2013), obtained geocoded data need using tidygeocoder (Cambon Belanger 2021).","code":""},{"path":"static-communication.html","id":"graphs","chapter":"6 Static communication","heading":"6.2 Graphs","text":"Graphs critical aspect compelling stories told data.Graphs allow us explore data see overall patterns see detailed behavior; approach can compete revealing structure data thoroughly. Graphs allow us view complex mathematical models fitted data, allow us assess validity models.Cleveland (1994, 5)way, graphing data information coding process create glyph, purposeful mark, mean convey information audience. audience must decode glyph. success graph turns much information lost process. decoding critical aspect (Cleveland 1994, 221), means creating graphs audience. nothing else possible, important feature convey much actual data possible.see important begin using dataset ‘datasaurus_dozen’ datasauRus (Locke D’Agostino McGowan 2018). installing loading necessary packages, can take quick look dataset.can see dataset consists values ‘x’ ‘y’, plotted x-axis y-axis, respectively. can see thirteen different values variable ‘dataset’ including: “dino”, “star”, “away”, “bullseye”. focus four generate summary statistics (Table 6.1).Table 6.1: Mean standard deviation four ‘datasaurus’ datasetsDespite similarities summary statistics, turns different ‘datasets’ actually different beasts graph actual data (Figure 6.1).\nFigure 6.1: Graph four ‘datasaurus’ datasets\nvariant famous ‘Anscombe’s Quartet’. key takeaway important plot actual data rely summary statistics. ‘anscombe’ dataset built R.consists six observations four different datasets, x y values observation. need manipulate dataset pivot_longer() get ‘tidy format’.can first create summary statistics (Table 6.2) graph data (Figure 6.2). see importance graphing actual data, rather relying summary statistics.Table 6.2: Mean standard deviation Anscombe\nFigure 6.2: Recreation Anscombe’s Quartet\n","code":"\ninstall.packages('datasauRus')\nlibrary(tidyverse)\nlibrary(datasauRus)\n\nhead(datasaurus_dozen)\n#> # A tibble: 6 × 3\n#>   dataset     x     y\n#>   <chr>   <dbl> <dbl>\n#> 1 dino     55.4  97.2\n#> 2 dino     51.5  96.0\n#> 3 dino     46.2  94.5\n#> 4 dino     42.8  91.4\n#> 5 dino     40.8  88.3\n#> 6 dino     38.7  84.9\ndatasaurus_dozen |> \n  count(dataset)\n#> # A tibble: 13 × 2\n#>    dataset        n\n#>    <chr>      <int>\n#>  1 away         142\n#>  2 bullseye     142\n#>  3 circle       142\n#>  4 dino         142\n#>  5 dots         142\n#>  6 h_lines      142\n#>  7 high_lines   142\n#>  8 slant_down   142\n#>  9 slant_up     142\n#> 10 star         142\n#> 11 v_lines      142\n#> 12 wide_lines   142\n#> 13 x_shape      142\n# From Julia Silge: \n# https://juliasilge.com/blog/datasaurus-multiclass/\ndatasaurus_dozen |>\n  filter(dataset %in% c(\"dino\", \"star\", \"away\", \"bullseye\")) |>\n  group_by(dataset) |>\n  summarise(across(c(x, y),\n                   list(mean = mean,\n                        sd = sd)),\n            x_y_cor = cor(x, y)) |>\n  knitr::kable(\n    caption = \n      \"Mean and standard deviation for four 'datasaurus' datasets\",\n    col.names = c(\"Dataset\", \n                  \"x mean\", \n                  \"x sd\", \n                  \"y mean\", \n                  \"y sd\", \n                  \"correlation\"),\n    digits = 1,\n    booktabs = TRUE,\n    linesep = \"\"\n  )\ndatasaurus_dozen |> \n  filter(dataset %in% c(\"dino\", \"star\", \"away\", \"bullseye\")) |>\n  ggplot(aes(x=x, y=y, colour=dataset)) +\n  geom_point() +\n  theme_minimal() +\n  facet_wrap(vars(dataset), nrow = 2, ncol = 2) +\n  labs(colour = \"Dataset\")\nhead(anscombe)\n#>   x1 x2 x3 x4   y1   y2    y3   y4\n#> 1 10 10 10  8 8.04 9.14  7.46 6.58\n#> 2  8  8  8  8 6.95 8.14  6.77 5.76\n#> 3 13 13 13  8 7.58 8.74 12.74 7.71\n#> 4  9  9  9  8 8.81 8.77  7.11 8.84\n#> 5 11 11 11  8 8.33 9.26  7.81 8.47\n#> 6 14 14 14  8 9.96 8.10  8.84 7.04\n# From Nick Tierney: \n# https://www.njtierney.com/post/2020/06/01/tidy-anscombe/\n# Code from pivot_longer() vignette.\ntidy_anscombe <- \n  anscombe |>\n  pivot_longer(everything(),\n               names_to = c(\".value\", \"set\"),\n               names_pattern = \"(.)(.)\"\n               )\ntidy_anscombe |>\n  group_by(set) |>\n  summarise(across(c(x, y),\n                   list(mean = mean, sd = sd)),\n            x_y_cor = cor(x, y)) |>\n  knitr::kable(\n    caption = \"Mean and standard deviation for Anscombe\",\n    col.names = c(\"Dataset\", \n                  \"x mean\", \n                  \"x sd\", \n                  \"y mean\", \n                  \"y sd\", \n                  \"correlation\"),\n    digits = 1,\n    booktabs = TRUE,\n    linesep = \"\"\n  )\ntidy_anscombe |> \n  ggplot(aes(x = x, y = y, colour = set)) +\n  geom_point() +\n  theme_minimal() +\n  facet_wrap(vars(set), nrow = 2, ncol = 2) +\n  labs(colour = \"Dataset\")"},{"path":"static-communication.html","id":"bar-charts","chapter":"6 Static communication","heading":"6.2.1 Bar charts","text":"typically use bar chart categorical variable want focus . saw example Chapter 2 constructed graph number occupied beds. geom primarily use geom_bar(), many variants cater specific situations.use dataset 1997-2001 British Election Panel Study put together Fox Andersen (2006).dataset consists party person supports, along various demographic, economic, political variables. particular, age respondents. begin creating age-groups ages, making bar chart age-groups using geom_bar() (Figure 6.3).\nFigure 6.3: Distribution ages 1997-2001 British Election Panel Study\ndefault, geom_bar() created count number times age-group appears dataset. default ‘stat’ geom_bar() ‘count’. saves us create statistic . already constructed count (instance, beps |> count(age)), also specify column values y-axis use stat = \"identity\".may also like consider different groupings data, instance, looking age-groups party respondent supports (Figure 6.4).\nFigure 6.4: Distribution ages, vote preference, 1997-2001 British Election Panel Study\ndefault different groups stacked, can placed side--side position = \"dodge\" (Figure 6.5).\nFigure 6.5: Distribution age-groups, vote preference, 1997-2001 British Election Panel Study\npoint, may like address general look graph. various themes built ggplot2. include theme_bw(), theme_classic(), theme_dark(), theme_minimal(). full list available ggplot2 cheatsheet. can use themes adding layer (Figure 6.6). can use patchwork (Pedersen 2020) bring together multiple graphs. assign graph name, use ‘+’ signal next , ‘/’ signal top, brackets precedence.\nFigure 6.6: Distribution age-groups, vote preference, 1997-2001 British Election Panel Study, illustrating different themes\ncan install themes packages, including ggthemes (Arnold 2021), hrbrthemes (Rudis 2020). can also build .default labels use dby ggplot2 name relevant variable, often useful add detail. add title caption point. caption can useful add information source dataset. title can useful graph going considered outside context paper. case graph included paper, need cross-reference graphs paper means included title within labs() unnecessary (Figure 6.7).\nFigure 6.7: Distribution age-groups, vote preference, 1997-2001 British Election Panel Study\nuse facets create ‘many little graphics variations single graphic’ (L. Wilkinson 2005, 219). especially useful want specifically compare across variable, already used color. instance, may interested explain vote, age gender (Figure 6.8).\nFigure 6.8: Distribution age-group gender, vote preference, 1997-2001 British Election Panel Study\nchange facet_wrap() wrap vertically instead horizontally dir = \"v\". Alternatively, specify number rows, say nrow = 2, number columns, say ncol = 2. Additionally, default, facets scales. enable facets different scales scales = \"free\", just x-axis scales = \"free_x\", just y-axis scales = \"free_y\" (Figure 6.9).\nFigure 6.9: Distribution age-group gender, vote preference, 1997-2001 British Election Panel Study\nFinally, can change labels facets using labeller() (Figure 6.10).\nFigure 6.10: Distribution age-group gender, vote preference, 1997-2001 British Election Panel Study\nvariety different ways change colors, many palettes available including RColorBrewer (Neuwirth 2014), specify scale_fill_brewer(), viridis (Garnier et al. 2021), specify scale_fill_viridis() particularly focused color-blind palettes (Figure 6.11).\nFigure 6.11: Distribution age-group vote preference, 1997-2001 British Election Panel Study\nDetails variety palettes available RColorBrewer viridis available help files. Many different palettes available, can also build . said, color something considered great deal care added increase amount information communicated (Cleveland 1994). Colors added graphs unnecessarily—say, must play role. Typically, role distinguish different groups, implies making colors dissimilar. Colors may also appropriate relationship color variable, instance making graph sales , say, mangoes raspberries, help reader colors yellow red, respectively (Franconeri et al. 2021, 121).","code":"\n# Vincent Arel Bundock provides access to this dataset.\nbeps <- \n  read_csv(\n    file = \n    \"https://vincentarelbundock.github.io/Rdatasets/csv/carData/BEPS.csv\"\n    )\nhead(beps)\n#> # A tibble: 6 × 11\n#>    ...1 vote     age economic.cond.n… economic.cond.h… Blair\n#>   <dbl> <chr>  <dbl>            <dbl>            <dbl> <dbl>\n#> 1     1 Liber…    43                3                3     4\n#> 2     2 Labour    36                4                4     4\n#> 3     3 Labour    35                4                4     5\n#> 4     4 Labour    24                4                2     2\n#> 5     5 Labour    41                2                2     1\n#> 6     6 Labour    47                3                4     4\n#> # … with 5 more variables: Hague <dbl>, Kennedy <dbl>,\n#> #   Europe <dbl>, political.knowledge <dbl>, gender <chr>\nbeps <- \n  beps |> \n  mutate(age_group = \n           case_when(age < 35 ~ \"<35\",\n                     age < 50 ~ \"35-49\",\n                     age < 65 ~ \"50-64\",\n                     age < 80 ~ \"65-79\",\n                     age < 100 ~ \"80-99\"\n                     ),\n         age_group = factor(age_group,\n                            levels = c(\"<35\",\n                                       \"35-49\",\n                                       \"50-64\",\n                                       \"65-79\",\n                                       \"80-99\"\n                                       )\n                            )\n         )\n\nbeps |>  \n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar()\nbeps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar()\nbeps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar(position = \"dodge\")\nlibrary(patchwork)\n\ntheme_bw <- \n  beps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar(position = \"dodge\") +\n  theme_bw()\n\ntheme_classic <- \n  beps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar(position = \"dodge\") +\n  theme_classic()\n\ntheme_dark <- \n  beps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar(position = \"dodge\") +\n  theme_dark()\n\ntheme_minimal <- \n  beps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar(position = \"dodge\") +\n  theme_minimal()\n\n(theme_bw + theme_classic) / (theme_dark + theme_minimal)\nbeps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group of respondent\",\n       y = \"Number of respondents\",\n       fill = \"Voted for\",\n       title = \"Distribution of age-groups, and vote preference, in\n       the 1997-2001 British Election Panel Study\",\n       caption = \"Source: 1997-2001 British Election Panel Study.\")\nbeps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group of respondent\",\n       y = \"Number of respondents\",\n       fill = \"Voted for\") +\n  facet_wrap(vars(gender))\nbeps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group of respondent\",\n       y = \"Number of respondents\",\n       fill = \"Voted for\") +\n  facet_wrap(vars(gender),\n             dir = \"v\",\n             scales = \"free\")\nnew_labels <- c(female = \"Female\", male = \"Male\")\n\nbeps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group of respondent\",\n       y = \"Number of respondents\",\n       fill = \"Voted for\") +\n  facet_wrap(vars(gender),\n             dir = \"v\",\n             scales = \"free\",\n             labeller = labeller(gender = new_labels))\nlibrary(viridis)\nlibrary(patchwork)\n\nRColorBrewerBrBG <- \n  beps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group of respondent\",\n       y = \"Number of respondents\",\n       fill = \"Voted for\") + \n  scale_fill_brewer(palette = \"Blues\")\n\nRColorBrewerSet2 <- \n  beps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group of respondent\",\n       y = \"Number of respondents\",\n       fill = \"Voted for\") +\n  scale_fill_brewer(palette = \"Set1\")\n\nviridis <- \n  beps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group of respondent\",\n       y = \"Number of respondents\",\n       fill = \"Voted for\") + \n  scale_fill_viridis(discrete = TRUE)\n\nviridismagma <- \n  beps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group of respondent\",\n       y = \"Number\",\n       fill = \"Voted for\") +\n   scale_fill_viridis(discrete = TRUE, \n                      option = \"magma\")\n\n(RColorBrewerBrBG + RColorBrewerSet2) /\n  (viridis + viridismagma)"},{"path":"static-communication.html","id":"scatterplots","chapter":"6 Static communication","heading":"6.2.2 Scatterplots","text":"often interested relationship two variables. can use scatterplots show information. Unless good reason move different option, scatterplot almost always best choice (Weissgerber et al. 2015). Indeed, ‘among forms statistical graphics, scatterplot may considered versatile generally useful invention entire history statistical graphics.’ (Friendly Wainer 2021, 121) illustrate scatterplots, use WDI (Arel-Bundock 2021b) download economic indicators World Bank, particular WDIsearch() find unique key need pass WDI() facilitate download.Oh, think good data ! Gross Domestic Product (GDP) ‘combines single figure, double counting, output (production) carried firms, non-profit institutions, government bodies households given country given period, regardless type goods services produced, provided production takes place within country’s economic territory’ (OECD (2014), p. 15). modern concept developed Simon Kuznets widely used reported. certain comfort definitive concrete single number describe something complicated entire economic activity country. crucial summary statistics. summary statistic, strength also weakness. single number necessarily loses information constituent components, distributional differences critical. highlights short term economic progress longer term improvements. ‘quantitative definiteness estimates makes easy forget dependence upon imperfect data consequently wide margins possible error totals components liable’ (Kuznets 1941, xxvi). Reliance one summary measure economic performance presents misguided picture country’s economy, also peoples.may like change names meaningful, keep columns need.get started can use geom_point() make scatterplot showing GDP growth inflation, country (Figure 6.12).\nFigure 6.12: Relationship inflation GDP growth Australia, Ethiopia, India, US\nbar charts, change theme, update labels (Figure 6.13), although , normally need caption title just use one.\nFigure 6.13: Relationship inflation GDP growth Australia, Ethiopia, India, US\nuse ‘color’ instead ‘fill’ using dots rather bars. also slightly affects change palette (Figure 6.14).\nFigure 6.14: Relationship inflation GDP growth Australia, Ethiopia, India, US\npoints scatterplot sometimes overlap. can address situation one two ways:Adding degree transparency dots ‘alpha’ (Figure 6.15). value ‘alpha’ can vary 0, fully transparent, 1, completely opaque.Adding small noise, slightly moves points, using geom_jitter() (Figure 6.16). default, movement uniform directions, can specify direction movement occurs ‘width’ ‘height’. decision two options turns degree exact accuracy matters, number points.\nFigure 6.15: Relationship inflation GDP growth Australia, Ethiopia, India, US\n\nFigure 6.16: Relationship inflation GDP growth Australia, Ethiopia, India, US\ncommon use case scatterplot illustrate relationship two variables. can useful add line best fit using geom_smooth() (Figure 6.17). default, geom_smooth() use LOESS smoothing used datasets less 1,000 observations, can specify relationship using ‘method’, change color ‘color’ remove standard errors ‘se’. Using geom_smooth() adds layer graph, inherits aesthetics ggplot(). instance, initially one line country Figure 6.17. overwrite specifying particular color, third graph Figure 6.17.\nFigure 6.17: Relationship inflation GDP growth Australia, Ethiopia, India, US\n","code":"\ninstall.packages('WDI')\nlibrary(tidyverse)\nlibrary(WDI)\nWDIsearch(\"gdp growth\")\n#>      indicator             \n#> [1,] \"5.51.01.10.gdp\"      \n#> [2,] \"6.0.GDP_growth\"      \n#> [3,] \"NV.AGR.TOTL.ZG\"      \n#> [4,] \"NY.GDP.MKTP.KD.ZG\"   \n#> [5,] \"NY.GDP.MKTP.KN.87.ZG\"\n#>      name                                    \n#> [1,] \"Per capita GDP growth\"                 \n#> [2,] \"GDP growth (annual %)\"                 \n#> [3,] \"Real agricultural GDP growth rates (%)\"\n#> [4,] \"GDP growth (annual %)\"                 \n#> [5,] \"GDP growth (annual %)\"\nWDIsearch(\"inflation\")\n#>      indicator             \n#> [1,] \"FP.CPI.TOTL.ZG\"      \n#> [2,] \"FP.FPI.TOTL.ZG\"      \n#> [3,] \"FP.WPI.TOTL.ZG\"      \n#> [4,] \"NY.GDP.DEFL.87.ZG\"   \n#> [5,] \"NY.GDP.DEFL.KD.ZG\"   \n#> [6,] \"NY.GDP.DEFL.KD.ZG.AD\"\n#>      name                                               \n#> [1,] \"Inflation, consumer prices (annual %)\"            \n#> [2,] \"Inflation, food prices (annual %)\"                \n#> [3,] \"Inflation, wholesale prices (annual %)\"           \n#> [4,] \"Inflation, GDP deflator (annual %)\"               \n#> [5,] \"Inflation, GDP deflator (annual %)\"               \n#> [6,] \"Inflation, GDP deflator: linked series (annual %)\"\nWDIsearch(\"population, total\")\n#>           indicator                name \n#>       \"SP.POP.TOTL\" \"Population, total\"\nWDIsearch(\"Unemployment, total\")\n#>      indicator          \n#> [1,] \"SL.UEM.TOTL.NE.ZS\"\n#> [2,] \"SL.UEM.TOTL.ZS\"   \n#>      name                                                                 \n#> [1,] \"Unemployment, total (% of total labor force) (national estimate)\"   \n#> [2,] \"Unemployment, total (% of total labor force) (modeled ILO estimate)\"\nworld_bank_data <- \n  WDI(indicator = c(\"FP.CPI.TOTL.ZG\",\n                    \"NY.GDP.MKTP.KD.ZG\",\n                    \"SP.POP.TOTL\",\n                    \"SL.UEM.TOTL.NE.ZS\"\n                    ),\n      country = c(\"AU\", \"ET\", \"IN\", \"US\")\n      )\nworld_bank_data <- \n  world_bank_data |> \n  rename(inflation = FP.CPI.TOTL.ZG,\n         gdp_growth = NY.GDP.MKTP.KD.ZG,\n         population = SP.POP.TOTL,\n         unemployment_rate = SL.UEM.TOTL.NE.ZS\n         ) |> \n  select(-iso2c)\n\nhead(world_bank_data)\n#> # A tibble: 6 × 6\n#>   country    year inflation gdp_growth population\n#>   <chr>     <dbl>     <dbl>      <dbl>      <dbl>\n#> 1 Australia  1960     3.73       NA      10276477\n#> 2 Australia  1961     2.29        2.48   10483000\n#> 3 Australia  1962    -0.319       1.29   10742000\n#> 4 Australia  1963     0.641       6.21   10950000\n#> 5 Australia  1964     2.87        6.98   11167000\n#> 6 Australia  1965     3.41        5.98   11388000\n#> # … with 1 more variable: unemployment_rate <dbl>\nworld_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point()\nworld_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",\n       y = \"Inflation\",\n       color = \"Country\",\n       title = \"Relationship between inflation and GDP growth\",\n       caption = \"Data source: World Bank.\")\nlibrary(patchwork)\n\nRColorBrewerBrBG <-\n  world_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",\n       y = \"Inflation\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\") +\n  scale_color_brewer(palette = \"Blues\")\n\nRColorBrewerSet2 <- \n  world_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",\n       y = \"Inflation\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\") +\n  scale_color_brewer(palette = \"Set1\")\n\nviridis <- \n  world_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",\n       y = \"Inflation\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\") +\n  scale_colour_viridis_d()\n\nviridismagma <- \n  world_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",\n       y = \"Inflation\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\") +\n  scale_colour_viridis_d(option = \"magma\")\n\nRColorBrewerBrBG / \n  RColorBrewerSet2 /\n  viridis /\n  viridismagma\nworld_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point(alpha = 0.5) +\n  theme_minimal() +\n  labs(x = \"GDP growth\",\n       y = \"Inflation\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\")\nworld_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",\n       y = \"Inflation\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\")\ndefaults <- \n  world_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  geom_smooth() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",\n       y = \"Inflation\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\")\n\nstraightline <- \n  world_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  geom_smooth(method = lm, se = FALSE) +\n  theme_minimal() +\n  labs(x = \"GDP growth\",\n       y = \"Inflation\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\")\n\nonestraightline <- \n  world_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  geom_smooth(method = lm, color = \"black\", se = FALSE) +\n  theme_minimal() +\n  labs(x = \"GDP growth\",\n       y = \"Inflation\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\")\n\ndefaults / \n  straightline /\n  onestraightline"},{"path":"static-communication.html","id":"line-plots","chapter":"6 Static communication","heading":"6.2.3 Line plots","text":"can use line plot variables joined together, instance, economic time series. continue dataset World Bank focus US GDP growth using geom_line() (Figure 6.18).\nFigure 6.18: US GDP growth (1961-2020)\n, can adjust theme, say theme_minimal() labels labs() (Figure 6.19).\nFigure 6.19: US GDP growth (1961-2020)\ncan use slight variant geom_line(), geom_step() focus attention change year year (Figure 6.20).\nFigure 6.20: US GDP growth (1961-2020)\nPhillips curve name given plot relationship unemployment inflation time. inverse relationship sometimes found data, instance UK 1861 1957 (Phillips 1958). variety ways investigate including:Adding second line graph. instance, add inflation (Figure 6.21). may require use use pivot_longer() ensure data tidy format.Using geom_path() links values order appear dataset. Figure 6.22 show Phillips curve US 1960 2020. Figure 6.22 appear show clear relationship unemployment inflation.\nFigure 6.21: Unemployment inflation US (1960-2020)\n\nFigure 6.22: Phillips curve US (1960-2020)\n","code":"\nworld_bank_data |>\n  filter(country == \"United States\") |>\n  ggplot(mapping = aes(x = year, y = gdp_growth)) +\n  geom_line()\nworld_bank_data |>\n  filter(country == \"United States\") |>\n  ggplot(mapping = aes(x = year, y = gdp_growth)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Year\",\n       y = \"GDP growth\",\n       caption = \"Data source: World Bank.\")\nworld_bank_data |>\n  filter(country == \"United States\") |>\n  ggplot(mapping = aes(x = year, y = gdp_growth)) +\n  geom_step() +\n  theme_minimal() +\n  labs(x = \"Year\",\n       y = \"GDP growth\",\n       caption = \"Data source: World Bank.\")\nworld_bank_data |>\n  filter(country == \"United States\") |>\n  select(-population, -gdp_growth) |>\n  pivot_longer(cols = c(\"inflation\", \"unemployment_rate\"),\n               names_to = \"series\",\n               values_to = \"value\"\n               ) |>\n  ggplot(mapping = aes(x = year, y = value, color = series)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Year\",\n       y = \"Value\",\n       color = \"Economic indicator\",\n       caption = \"Data source: World Bank.\") +\n  scale_color_brewer(palette = \"Set1\", labels = c(\"Inflation\", \"Unemployment\")) +\n  theme(legend.position = \"bottom\")\nworld_bank_data |>\n  filter(country == \"United States\") |>\n  ggplot(mapping = aes(x = unemployment_rate, y = inflation)) +\n  geom_path() +\n  theme_minimal() +\n  labs(x = \"Unemployment rate\",\n       y = \"Inflation\",\n       caption = \"Data source: World Bank.\")"},{"path":"static-communication.html","id":"histograms","chapter":"6 Static communication","heading":"6.2.4 Histograms","text":"histogram useful show shape continuous variable works constructing counts number observations different subsets support, called ‘bins’. Figure 6.23 examine distribution GDP Ethiopia.\nFigure 6.23: Distribution GDP Ethiopia (1960-2020)\ncan add theme labels (Figure 6.24.\nFigure 6.24: Distribution GDP Ethiopia (1960-2020)\nkey component determining shape histogram number bins. can specified one two ways (Figure 6.25):specifying number ‘bins’ include, orspecifying wide ‘binwidth’.\nFigure 6.25: Distribution GDP Ethiopia (1960-2020)\nhistogram smoothing data, number bins affects much smoothing occurs. two bins data smooth, lost great deal accuracy. specifically, ‘histogram estimator piecewise constant function height function proportional number observations bin’ (Wasserman 2005, 303). bins result biased estimator, many bins results estimator high variance. decision number bins, width, concerned trying balance bias variance. depend variety concerns including subject matter goal (Cleveland 1994, 135).Finally, can use ‘fill’ distinguish different types observations, can get quite messy. usually better give away showing distribution columns instead trace outline distribution, using geom_freqpoly() (Figure 6.26) build using dots geom_dotplot() (Figure 6.27) .\nFigure 6.26: Distribution GDP four countries (1960-2020)\n\nFigure 6.27: Distribution GDP four countries (1960-2020)\n","code":"\nworld_bank_data |> \n  filter(country == \"Ethiopia\") |> \n  ggplot(mapping = aes(x = gdp_growth)) +\n  geom_histogram()\nworld_bank_data |> \n  filter(country == \"Ethiopia\") |> \n  ggplot(mapping = aes(x = gdp_growth)) +\n  geom_histogram() +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       caption = \"Data source: World Bank.\")\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n\ntwobins <- \n  world_bank_data |> \n  filter(country == \"Ethiopia\") |> \n  ggplot(mapping = aes(x = gdp_growth)) +\n  geom_histogram(bins = 2) +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       caption = \"Data source: World Bank.\")\n\nfivebins <- \n  world_bank_data |> \n  filter(country == \"Ethiopia\") |> \n  ggplot(mapping = aes(x = gdp_growth)) +\n  geom_histogram(bins = 5) +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       caption = \"Data source: World Bank.\")\n\ntwentybins <- \n  world_bank_data |> \n  filter(country == \"Ethiopia\") |> \n  ggplot(mapping = aes(x = gdp_growth)) +\n  geom_histogram(bins = 20) +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       caption = \"Data source: World Bank.\")\n\nhalfbinwidth <- \n  world_bank_data |> \n  filter(country == \"Ethiopia\") |> \n  ggplot(mapping = aes(x = gdp_growth)) +\n  geom_histogram(binwidth = 0.5) +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       caption = \"Data source: World Bank.\")\n\ntwobinwidth <- \n  world_bank_data |> \n  filter(country == \"Ethiopia\") |> \n  ggplot(mapping = aes(x = gdp_growth)) +\n  geom_histogram(binwidth = 2) +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       caption = \"Data source: World Bank.\")\n\nfivebinwidth <- \n  world_bank_data |> \n  filter(country == \"Ethiopia\") |> \n  ggplot(mapping = aes(x = gdp_growth)) +\n  geom_histogram(binwidth = 5) +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       caption = \"Data source: World Bank.\")\n\n(twobins + fivebins + twentybins) / \n  (halfbinwidth + twobinwidth + fivebinwidth)\nworld_bank_data |> \n  ggplot(mapping = aes(x = gdp_growth, color = country)) +\n  geom_freqpoly() +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\") +\n  scale_color_brewer(palette = \"Set1\") \nworld_bank_data |> \n  ggplot(mapping = aes(x = gdp_growth, group = country, fill = country)) +\n  geom_dotplot(method = 'histodot', alpha = 0.4) +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       fill = \"Country\",\n       caption = \"Data source: World Bank.\") +\n  scale_color_brewer(palette = \"Set1\") "},{"path":"static-communication.html","id":"boxplots","chapter":"6 Static communication","heading":"6.2.5 Boxplots","text":"Boxplots almost never appropriate choice hide distribution data, rather show . Unless need compare summary statistics many variables , almost never used. boxplot can apply different distributions. see , consider simulated data beta distribution two types. One type data contains draws two beta distributions: one right skewed another left skewed. type data contains draws beta distribution skew.can first compare boxplots two series (Figure 6.28.\nFigure 6.28: Data drawn beta distributions different parameters\nplot actual data can see different (Figure 6.29).\nFigure 6.29: Data drawn beta distributions different parameters\nOne way forward, boxplot must included, include actual data layer top boxplot. instance, Figure 6.30 show distribution inflation across four countries.\nFigure 6.30: Distribution unemployment data four countries (1960-2020)\n","code":"\nset.seed(853)\n\nboth_left_and_right_skew <- \n  c(\n    rbeta(500, 5, 2),\n    rbeta(500, 2, 5)\n    )\n\nno_skew <- \n  rbeta(1000, 1, 1)\n\nbeta_distributions <- \n  tibble(\n    observation = c(both_left_and_right_skew, no_skew),\n    source = c(rep(\"Left and right skew\", 1000),\n               rep(\"No skew\", 1000)\n               )\n  )\nbeta_distributions |> \n  ggplot(aes(x = source, y = observation)) +\n  geom_boxplot() +\n  theme_classic()\nbeta_distributions |> \n  ggplot(aes(x = observation, color = source)) +\n  geom_freqpoly(binwidth = 0.05) +\n  theme_classic()\nworld_bank_data |> \n  ggplot(mapping = aes(x = country, y = inflation)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.3, width = 0.15, height = 0) +\n  theme_minimal() +\n  labs(x = \"Country\",\n       y = \"Inflation\",\n       caption = \"Data source: World Bank.\") +\n  scale_color_brewer(palette = \"Set1\") "},{"path":"static-communication.html","id":"tables","chapter":"6 Static communication","heading":"6.3 Tables","text":"Tables critical telling compelling story. Tables can communicate less information graph, can high fidelity. primarily use tables three ways:show actual dataset, use kable() knitr (Xie 2021), alongside kableExtra (Zhu 2020).communicate summary statistics, use modelsummary (Arel-Bundock 2021a).display regression results, also use modelsummary (Arel-Bundock 2021a).","code":""},{"path":"static-communication.html","id":"showing-part-of-a-dataset","chapter":"6 Static communication","heading":"6.3.1 Showing part of a dataset","text":"illustrate showing part dataset using kable() knitr drawing kableExtra enhancement. use World Bank dataset downloaded earlier.begin, can display first ten rows default kable() settings.order able cross-reference text, need add caption ‘caption’. can also make column names information ‘col.names’ specify number digits displayed (Table 6.3).Table 6.3: First ten rows dataset economic indicators \nAustralia, Ethiopia, India, USWhen producing PDFs, ‘booktabs’ option makes host small changes default display results tables look better (Table 6.4). using ‘booktabs’ additionally specify ‘linesep’ otherwise kable() adds small space every five lines. (None show html output.)Table 6.4: First ten rows dataset economic indicators \nAustralia, Ethiopia, India, USTable 6.5: First ten rows dataset economic indicators \nAustralia, Ethiopia, India, USWe can specify alignment columns using character vector ‘l’ (left), ‘c’ (centre), ‘r’ (right) (Table 6.6). Additionally, can change formatting. instance, specify groupings numbers least one thousand using ‘format.args = list(big.mark = “,”)’.Table 6.6: First ten rows dataset economic indicators \nAustralia, Ethiopia, India, USWe can use kableExtra (Zhu 2020) add extra functionality kable. instance, add row groups columns (Table 6.6).\nTable 6.7: First ten rows dataset economic indicators \nAustralia, Ethiopia, India, US\nAnother especially nice way build tables use gt (Iannone, Cheng, Schloerke 2020)., can add caption informative column labels (Table 6.8).Table 6.8: First ten rows dataset economic indicators Australia, Ethiopia, India, US","code":"\nlibrary(knitr)\nhead(world_bank_data)\n#> # A tibble: 6 × 6\n#>   country    year inflation gdp_growth population\n#>   <chr>     <dbl>     <dbl>      <dbl>      <dbl>\n#> 1 Australia  1960     3.73       NA      10276477\n#> 2 Australia  1961     2.29        2.48   10483000\n#> 3 Australia  1962    -0.319       1.29   10742000\n#> 4 Australia  1963     0.641       6.21   10950000\n#> 5 Australia  1964     2.87        6.98   11167000\n#> 6 Australia  1965     3.41        5.98   11388000\n#> # … with 1 more variable: unemployment_rate <dbl>\nworld_bank_data |> \n  slice(1:10) |> \n  kable() \nworld_bank_data |> \n  slice(1:10) |> \n  kable(\n    caption = \"First ten rows of a dataset of economic indicators for \n    Australia, Ethiopia, India, and the US\",\n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \"Population\", \"Unemployment rate\"),\n    digits = 1\n  )\nworld_bank_data |> \n  slice(1:10) |> \n  kable(\n    caption = \"First ten rows of a dataset of economic indicators for \n    Australia, Ethiopia, India, and the US\",\n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \"Population\", \"Unemployment rate\"),\n    digits = 1,\n    booktabs = TRUE, \n    linesep = \"\"\n  )\nworld_bank_data |> \n  slice(1:10) |> \n  kable(\n    caption = \"First ten rows of a dataset of economic indicators for\n    Australia, Ethiopia, India, and the US\",\n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \"Population\", \"Unemployment rate\"),\n    digits = 1,\n    booktabs = TRUE\n  )\nworld_bank_data |> \n  slice(1:10) |> \n  mutate(year = as.factor(year)) |>\n  kable(\n    caption = \"First ten rows of a dataset of economic indicators for \n    Australia, Ethiopia, India, and the US\",\n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \n                  \"Population\", \"Unemployment rate\"),\n    digits = 1,\n    booktabs = TRUE, \n    linesep = \"\",\n    align = c('l', 'l', 'c', 'c', 'r', 'r'),\n    format.args = list(big.mark = \",\")\n  )\nlibrary(kableExtra)\n\nworld_bank_data |> \n  slice(1:10) |> \n  kable(\n    caption = \"First ten rows of a dataset of economic indicators for \n    Australia, Ethiopia, India, and the US\",\n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \n                  \"Population\", \"Unemployment rate\"),\n    digits = 1,\n    booktabs = TRUE, \n    linesep = \"\",\n    align = c('l', 'l', 'c', 'c', 'r', 'r'),\n  ) |> \n  add_header_above(c(\" \" = 2, \"Economic indicators\" = 4))\nlibrary(gt)\n\nworld_bank_data |> \n  slice(1:10) |> \n  gt() \nworld_bank_data |>\n  slice(1:10) |>\n  gt(\n    caption = \"First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the US\") |>\n  cols_label(\n      country = \"Country\",\n      year = \"Year\",\n      inflation = \"Inflation\",\n      gdp_growth = \"GDP growth\",\n      population = \"Population\",\n      unemployment_rate = \"Unemployment rate\"\n    )"},{"path":"static-communication.html","id":"communicating-summary-statistics","chapter":"6 Static communication","heading":"6.3.2 Communicating summary statistics","text":"can use datasummary() modelsummary create tables summary statistics dataset.default, datasummary() summarizes ‘numeric’ variables, can ask ‘categorical’ variables (Table 6.9). Additionally can add cross-references way kable(), , include title cross-reference name R chunk.\nTable 6.9: Summary categorical economic indicator\nvariables four countries\ncan create table shows correlation variables using datasummary_correlation() (Table 6.10).\nTable 6.10: Correlation economic indicator variables \nfour countries (Australia, Ethiopia, India, US)\ntypically need table descriptive statistics add paper (Table 6.11). contrasts Table 6.9 likely included paper. can add note source data using ‘notes’.\nTable 6.11: Descriptive statistics inflation GDP dataset\n","code":"\nlibrary(modelsummary)\n#> \n#> Attaching package: 'modelsummary'\n#> The following object is masked from 'package:gt':\n#> \n#>     escape_latex\n\nworld_bank_data |> \n  datasummary_skim()\nworld_bank_data |> \n  datasummary_skim(type = \"categorical\",\n                   title = \"Summary of categorical economic indicator \n                   variables for four countries\")\nworld_bank_data |> \n  datasummary_correlation(\n    title = \"Correlation between the economic indicator variables for \n    four countries (Australia, Ethiopia, India, and the US)\"\n    )\ndatasummary_balance(formula = ~country,\n                    data = world_bank_data,\n                    title = \"Descriptive statistics for the inflation and GDP dataset\",\n                    notes = \"Data source: World Bank.\")"},{"path":"static-communication.html","id":"display-regression-results","chapter":"6 Static communication","heading":"6.3.3 Display regression results","text":"Finally, one common reason needing table report regression results. using modelsummary() modelsummary (Arel-Bundock 2021a).can put variety different different models together (Table 6.12).\nTable 6.12: Explaining GDP function inflation\ncan adjust number significant digits (Table 6.13).\nTable 6.13: Two models GDP function inflation\n","code":"\nfirst_model <- lm(formula = gdp_growth ~ inflation, \n                  data = world_bank_data)\n\nmodelsummary(first_model)\nsecond_model <- lm(formula = gdp_growth ~ inflation + country, \n                  data = world_bank_data)\n\nthird_model <- lm(formula = gdp_growth ~ inflation + country + population, \n                  data = world_bank_data)\n\nmodelsummary(list(first_model, second_model, third_model),\n             title = \"Explaining GDP as a function of inflation\")\nmodelsummary(list(first_model, second_model, third_model),\n             fmt = 1,\n             title = \"Two models of GDP as a function of inflation\")"},{"path":"static-communication.html","id":"maps","chapter":"6 Static communication","heading":"6.4 Maps","text":"many ways maps can thought another type graph, x-axis latitude, y-axis longitude, outline background image. seen type set-used type set-, instance, ggplot2 setting, quite familiar.small complications, part straight-forward . first step get data. geographic data built ggplot2, additional information ‘world.cities’ dataset maps.information hand, can create map France shows larger cities. use geom_polygon() ggplot2 draw shapes connecting points within groups. coord_map() adjusts fact making 2D map represent world 3D.)often case R, many different ways get started creating static maps. seen can built using ggplot2, ggmap brings additional functionality (Kahle Wickham 2013).two essential components map:border background image (sometimes called tile); andsomething interest within border, top tile.ggmap, use open-source option tile, Stamen Maps. use plot points based latitude longitude.","code":"\nggplot() +\n  geom_polygon( # First draw an outline\n    data = some_data, \n    aes(x = latitude, \n        y = longitude,\n        group = group\n        )) +\n  geom_point( # Then add points of interest\n    data = some_other_data, \n    aes(x = latitude, \n        y = longitude)\n    )\nlibrary(maps)\n\nfrance <- map_data(map = \"france\")\n\nhead(france)\n#>       long      lat group order region subregion\n#> 1 2.557093 51.09752     1     1   Nord      <NA>\n#> 2 2.579995 51.00298     1     2   Nord      <NA>\n#> 3 2.609101 50.98545     1     3   Nord      <NA>\n#> 4 2.630782 50.95073     1     4   Nord      <NA>\n#> 5 2.625894 50.94116     1     5   Nord      <NA>\n#> 6 2.597699 50.91967     1     6   Nord      <NA>\n\nfrench_cities <- \n  world.cities |>\n  filter(country.etc == \"France\")\n\nhead(french_cities)\n#>              name country.etc    pop   lat long capital\n#> 1       Abbeville      France  26656 50.12 1.83       0\n#> 2         Acheres      France  23219 48.97 2.06       0\n#> 3            Agde      France  23477 43.33 3.46       0\n#> 4            Agen      France  34742 44.20 0.62       0\n#> 5 Aire-sur-la-Lys      France  10470 50.64 2.39       0\n#> 6 Aix-en-Provence      France 148622 43.53 5.44       0\nggplot() +\n  geom_polygon(data = france,\n               aes(x = long,\n                   y = lat,\n                   group = group),\n               fill = \"white\", \n               colour = \"grey\") +\n  coord_map() +\n  geom_point(aes(x = french_cities$long, \n                 y = french_cities$lat),\n             alpha = 0.3,\n             color = \"black\") +\n  theme_classic() +\n  labs(x = \"Longitude\",\n       y = \"Latitude\")"},{"path":"static-communication.html","id":"australian-polling-places","chapter":"6 Static communication","heading":"6.4.1 Australian polling places","text":"Australia people go specific locations, called booths, vote. booths latitudes longitudes can plot . One reason may like notice patterns geographies.get started need get tile. going use ggmap get tile Stamen Maps, builds OpenStreetMap (openstreetmap.org). main argument function specify bounding box. requires two latitudes - one top box one bottom box - two longitudes - one left box one right box. can useful use Google Maps, alternative, find values need. bounding box provides coordinates edges interested . case provided coordinates centered around Canberra, Australia, small city created purposes capital.defined bounding box, function get_stamenmap() get tiles area. number tiles needs get depends zoom, type tiles gets depends maptype. used black--white type map helpfile specifies others. point can map maps ggmap() plot tile! actively downloading tiles, needs internet connection.map can use ggmap() plot . Now want get data plot top tiles. just plot location polling places, based ‘division’ . available . Australian Electoral Commission (AEC) official government agency responsible elections Australia.dataset whole Australia, just going plot area around Canberra filter booths geographic (AEC various options people hospital, able get booth, etc, still ‘booths’ dataset).Now can use ggmap way plot underlying tiles, build using geom_point() add points interest.may like save map draw every time, can way graph, using ggsave().Finally, reason used Stamen Maps OpenStreetMap open source, also used Google Maps. requires first register credit card Google, specify key, low usage free. Using Google Maps, get_googlemap(), brings advantages get_stamenmap(), instance attempt find placename, rather needing specify bounding box.","code":"\nlibrary(ggmap)\n\nbbox <- c(left = 148.95, bottom = -35.5, right = 149.3, top = -35.1)\ncanberra_stamen_map <- get_stamenmap(bbox, zoom = 11, maptype = \"toner-lite\")\n\nggmap(canberra_stamen_map)\n# Read in the booths data for each year\nbooths <-\n  readr::read_csv(\n    \"https://results.aec.gov.au/24310/Website/Downloads/GeneralPollingPlacesDownload-24310.csv\",\n    skip = 1,\n    guess_max = 10000\n  )\n\nhead(booths)\n#> # A tibble: 6 × 15\n#>   State DivisionID DivisionNm PollingPlaceID\n#>   <chr>      <dbl> <chr>               <dbl>\n#> 1 ACT          318 Bean                93925\n#> 2 ACT          318 Bean                93927\n#> 3 ACT          318 Bean                11877\n#> 4 ACT          318 Bean                11452\n#> 5 ACT          318 Bean                 8761\n#> 6 ACT          318 Bean                 8763\n#> # … with 11 more variables: PollingPlaceTypeID <dbl>,\n#> #   PollingPlaceNm <chr>, PremisesNm <chr>,\n#> #   PremisesAddress1 <chr>, PremisesAddress2 <chr>,\n#> #   PremisesAddress3 <chr>, PremisesSuburb <chr>,\n#> #   PremisesStateAb <chr>, PremisesPostCode <chr>,\n#> #   Latitude <dbl>, Longitude <dbl>\n# Reduce the booths data to only rows with that have latitude and longitude\nbooths_reduced <-\n  booths |>\n  filter(State == \"ACT\") |> \n  select(PollingPlaceID, DivisionNm, Latitude, Longitude) |> \n  filter(!is.na(Longitude)) |> # Remove rows that do not have a geography\n  filter(Longitude < 165) # Remove Norfolk Island\nggmap(canberra_stamen_map,\n      extent = \"normal\",\n      maprange = FALSE) +\n  geom_point(data = booths_reduced,\n             aes(x = Longitude,\n                 y = Latitude,\n                 colour = DivisionNm),) +\n  scale_color_brewer(name = \"2019 Division\", palette = \"Set1\") +\n  coord_map(\n    projection = \"mercator\",\n    xlim = c(attr(map, \"bb\")$ll.lon, attr(map, \"bb\")$ur.lon),\n    ylim = c(attr(map, \"bb\")$ll.lat, attr(map, \"bb\")$ur.lat)\n  ) +\n  labs(x = \"Longitude\",\n       y = \"Latitude\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank())\nggsave(\"map.pdf\", width = 20, height = 10, units = \"cm\")"},{"path":"static-communication.html","id":"us-troop-deployment","chapter":"6 Static communication","heading":"6.4.2 US troop deployment","text":"Let us see another example static map, time using data US military deployments troopdata (Flynn 2021). can access data US overseas military bases back start Cold War using get_basedata().look locations US military bases : Germany, Japan, Australia. troopdata dataset already latitude longitude base. use item interest. first step define bounding box country.need get tiles using get_stamenmap() ggmap.finally, can bring together maps show US military bases Germany (Figure 6.31), Japan (Figure 6.32), Australia (Figure 6.33).\nFigure 6.31: Map US military bases Germany\n\nFigure 6.32: Map US military bases Japan\n\nFigure 6.33: Map US military bases Australia\n","code":"\ninstall.packages(\"troopdata\")\nlibrary(troopdata)\n\nbases <- get_basedata()\n\nhead(bases)\n#> # A tibble: 6 × 9\n#>   countryname ccode iso3c basename   lat   lon  base lilypad\n#>   <chr>       <dbl> <chr> <chr>    <dbl> <dbl> <dbl>   <dbl>\n#> 1 Afghanistan   700 AFG   Bagram …  34.9  69.3     1       0\n#> 2 Afghanistan   700 AFG   Kandaha…  31.5  65.8     1       0\n#> 3 Afghanistan   700 AFG   Mazar-e…  36.7  67.2     1       0\n#> 4 Afghanistan   700 AFG   Gardez    33.6  69.2     1       0\n#> 5 Afghanistan   700 AFG   Kabul     34.5  69.2     1       0\n#> 6 Afghanistan   700 AFG   Herat     34.3  62.2     1       0\n#> # … with 1 more variable: fundedsite <dbl>\nlibrary(ggmap)\n\n# Based on: https://data.humdata.org/dataset/bounding-boxes-for-countries\nbbox_germany <-\n  c(\n    left = 5.867,\n    bottom = 45.967,\n    right = 15.033,\n    top = 55.133\n  )\n\nbbox_japan <-\n  c(\n    left = 127,\n    bottom = 30,\n    right = 146,\n    top = 45\n  )\n\nbbox_australia <-\n  c(\n    left = 112.467,\n    bottom = -45,\n    right = 155,\n    top = -9.133\n  )\ngermany_stamen_map <-\n  get_stamenmap(bbox_germany, zoom = 6, maptype = \"toner-lite\")\n\njapan_stamen_map <-\n  get_stamenmap(bbox_japan, zoom = 6, maptype = \"toner-lite\")\n\naustralia_stamen_map <-\n  get_stamenmap(bbox_australia, zoom = 5, maptype = \"toner-lite\")\nggmap(germany_stamen_map) +\n  geom_point(data = bases,\n             aes(x = lon,\n                 y = lat)\n             ) +\n  labs(x = \"Longitude\",\n       y = \"Latitude\") +\n  theme_minimal() \nggmap(japan_stamen_map) +\n  geom_point(data = bases,\n             aes(x = lon,\n                 y = lat)\n             ) +\n  labs(x = \"Longitude\",\n       y = \"Latitude\") +\n  theme_minimal() \nggmap(australia_stamen_map) +\n  geom_point(data = bases,\n             aes(x = lon,\n                 y = lat)\n             ) +\n  labs(x = \"Longitude\",\n       y = \"Latitude\") +\n  theme_minimal() "},{"path":"static-communication.html","id":"geocoding","chapter":"6 Static communication","heading":"6.4.3 Geocoding","text":"point assumed already geocoded data, means latitude longitude. place names, ‘Canberra, Australia’, ‘Ottawa, Canada’, ‘Accra, Ghana’, ‘Quito, Ecuador’ just names, actually inherently location. plot need get latitude longitude . process going names coordinates called geocoding.range options geocode data R, tidygeocoder especially useful (Cambon Belanger 2021). first need dataframe locations.can now plot label cities (Figure 6.34).\nFigure 6.34: Map Accra, Canberra, Ottawa, Quito geocoding obtain locations\n","code":"\nplace_names <-\n  tibble(\n    city = c('Canberra', 'Ottawa', 'Accra', 'Quito'),\n    country = c('Australia', 'Canada', 'Ghana', 'Ecuador')\n  )\n\nplace_names\n#> # A tibble: 4 × 2\n#>   city     country  \n#>   <chr>    <chr>    \n#> 1 Canberra Australia\n#> 2 Ottawa   Canada   \n#> 3 Accra    Ghana    \n#> 4 Quito    Ecuador\nlibrary(tidygeocoder)\n\nplace_names <-\n  geo(city = place_names$city,\n      country = place_names$country,\n      method = 'osm')\n\nplace_names\n#> # A tibble: 4 × 4\n#>   city     country       lat    long\n#>   <chr>    <chr>       <dbl>   <dbl>\n#> 1 Canberra Australia -35.3   149.   \n#> 2 Ottawa   Canada     45.4   -75.7  \n#> 3 Accra    Ghana       5.56   -0.201\n#> 4 Quito    Ecuador    -0.220 -78.5\nworld <- map_data(map = \"world\")\n\nggplot() +\n  geom_polygon(\n    data = world,\n    aes(x = long,\n        y = lat,\n        group = group),\n    fill = \"white\",\n    colour = \"grey\"\n  ) +\n  coord_map(ylim = c(47,-47)) +\n  geom_point(aes(x = place_names$long,\n                 y = place_names$lat),\n             color = \"black\") +\n  geom_text(aes(\n    x = place_names$long,\n    y = place_names$lat,\n    label = place_names$city\n  ),\n  nudge_y = -5,) +\n  theme_classic() +\n  labs(x = \"Longitude\",\n       y = \"Latitude\")"},{"path":"static-communication.html","id":"exercises-and-tutorial-5","chapter":"6 Static communication","heading":"6.5 Exercises and tutorial","text":"","code":""},{"path":"static-communication.html","id":"exercises-5","chapter":"6 Static communication","heading":"6.5.1 Exercises","text":"Assume tidyverse datasauRus installed loaded. outcome following code?\ndatasaurus_dozen |> filter(dataset == \"v_lines\") |> ggplot(aes(x=x, y=y)) + geom_point()\nFour vertical lines\nFive vertical lines\nThree vertical lines\nTwo vertical lines\nFour vertical linesFive vertical linesThree vertical linesTwo vertical linesAssume tidyverse ‘beps’ dataset installed loaded. change made following make bars different parties next rather top ?\nbeps |> ggplot(mapping = aes(x = age, fill = vote)) + geom_bar()\nposition = \"side_by_side\"\nposition = \"dodge\"\nposition = \"adjacent\"\nposition = \"closest\"\nposition = \"side_by_side\"position = \"dodge\"position = \"adjacent\"position = \"closest\"theme used remove solid lines along x y axes?\ntheme_minimal()\ntheme_classic()\ntheme_bw()\ntheme_dark()\ntheme_minimal()theme_classic()theme_bw()theme_dark()Assume tidyverse ‘beps’ dataset installed loaded. added ‘labs()’ change text legend?\nbeps |> ggplot(mapping = aes(x = age, fill = vote)) + geom_bar() + theme_minimal() + labs(x = \"Age respondent\", y = \"Number respondents\")\ncolor = \"Voted \"\nlegend = \"Voted \"\nscale = \"Voted \"\nfill = \"Voted \"\ncolor = \"Voted \"legend = \"Voted \"scale = \"Voted \"fill = \"Voted \"palette scale_colour_brewer() divergent?\n‘Accent’\n‘RdBu’\n‘GnBu’\n‘Set1’\n‘Accent’‘RdBu’‘GnBu’‘Set1’geom used make scatter plot?\ngeom_smooth()\ngeom_point()\ngeom_bar()\ngeom_dotplot()\ngeom_smooth()geom_point()geom_bar()geom_dotplot()result largest number bins?\ngeom_histogram(binwidth = 5)\ngeom_histogram(binwidth = 2)\ngeom_histogram(binwidth = 5)geom_histogram(binwidth = 2)dataset contains heights 100 birds one three different species. interested understanding distribution heights, paragraph two, please explain type graph used ?Assume dataset columns exist. code work? data |> ggplot(aes(x = col_one)) |> geom_point() (pick one)?\nYes\n\nYesNoWhich geom used plot categorical data (pick one)?\ngeom_bar()\ngeom_point()\ngeom_abline()\ngeom_boxplot()\ngeom_bar()geom_point()geom_abline()geom_boxplot()boxplots often inappropriate (pick one)?\nhide full distribution data.\nhard make.\nugly.\nmode clearly displayed.\nhide full distribution data.hard make.ugly.mode clearly displayed.following, , elements layered grammar graphics (Wickham 2010) (select apply)?\ndefault dataset set mappings variables aesthetics.\nOne layers, layer one geometric object, one statistical transformation, one position adjustment, optionally, one dataset set aesthetic mappings.\nColors enable reader understand main point.\ncoordinate system.\nfacet specification.\nOne scale aesthetic mapping used.\ndefault dataset set mappings variables aesthetics.One layers, layer one geometric object, one statistical transformation, one position adjustment, optionally, one dataset set aesthetic mappings.Colors enable reader understand main point.coordinate system.facet specification.One scale aesthetic mapping used.function modelsummary used create table descriptive statistics?\ndatasummary_descriptive()\ndatasummary_skim()\ndatasummary_crosstab()\ndatasummary_balance()\ndatasummary_descriptive()datasummary_skim()datasummary_crosstab()datasummary_balance()","code":""},{"path":"static-communication.html","id":"tutorial-5","chapter":"6 Static communication","heading":"6.5.2 Tutorial","text":"Using R Markdown, please create graph using ggplot2 map using ggmap add explanatory text accompany . sure include cross-references captions, etc. take one two pages ., graph, please reflect Vanderplas, Cook, Hofmann (2020) add paragraphs different options considered graph effective. (’ve now got least two pages graph ’ve likely written little.)finally, map, please reflect following quote Heather Krause: ‘maps show people aren’t invisible makers’ well Chapter 3 D’Ignazio Klein (2020) add paragraphs related . (, ’ve now got least two pages map ’ve likely written little.)Please submit PDF.","code":""},{"path":"interactive-communication.html","id":"interactive-communication","chapter":"7 Interactive communication","heading":"7 Interactive communication","text":"Required materialRead M-F-E-O: postcards + distill, (Presmanes Hill 2021a).\nRead Geocomputation R, Chapter 2 ‘Geographic data R’, (Lovelace, Nowosad, Muenchow 2019).Read Mastering Shiny, Chapter 1 ‘first Shiny app’, (Wickham 2021a).Read Still Can’t See American Slavery , (Bouie 2022).Key concepts skillsBuilding website using within R environment using: postcards (Kross 2021), distill (Allaire et al. 2021), blogdown (Xie, Dervieux, Hill 2021).Adding interaction maps, using leaflet (Cheng, Karambelkar, Xie 2021) mapdeck (Cooley 2020).Adding interaction graphs, using Shiny (Chang et al. 2021).Key librariesblogdown (Xie, Dervieux, Hill 2021)distill (Allaire et al. 2021)leaflet (Cheng, Karambelkar, Xie 2021)mapdeck (Cooley 2020)postcards (Kross 2021)tidyverse (Wickham et al. 2019a)usethis (Wickham Bryan 2020)Key functionsblogdown::serve_site()distill::create_article()leaflet::addCircleMarkers()leaflet::addLegend()leaflet::addMarkers()leaflet::addTiles()leaflet::leaflet()mapdeck:::add_scatterplot()mapdeck::mapdeck()mapdeck::mapdeck_style()shiny::fluidPage()shiny::mainPanel()shiny::plotOutput()shiny::renderPlot()shiny::shinyApp()usethis::edit_r_environ()usethis::use_git()usethis::use_github()","code":""},{"path":"interactive-communication.html","id":"introduction-4","chapter":"7 Interactive communication","heading":"7.1 Introduction","text":"Books papers primary mediums communication thousands years. rise computers, especially internet, recent decades, static approaches complemented interactive approaches. Fundamentally, internet making files available others. additionally allow something make available, need take variety additional aspects consideration.chapter begin covering create publish website. serves place host portfolio work. cover adding interaction maps graphs, two nicely lend .","code":""},{"path":"interactive-communication.html","id":"making-a-website","chapter":"7 Interactive communication","heading":"7.2 Making a website","text":"website critical part communication. instance, place make portfolio work publicly available. One way make website use blogdown (Xie, Dervieux, Hill 2021). blogdown package allows make websites, just blogs, notwithstanding name, largely within R Studio. builds ‘Hugo’, popular general framework making websites. blogdown enables us freely quickly get website --running. easy add content time--time. integrates R Markdown, makes easy share work. blogdown brittle. dependent Hugo, features work today may work tomorrow. Also, owners Hugo templates can update time, without thought existing users. blogdown good option know , specific use-case, style, mind. two alternatives better starting points.first distill (Allaire et al. 2021). , R package wraps around another framework, case ‘Distill’. contrast Hugo, Distill focused common needs data science, also maintained one group, may considered stable choice. said, default distill site little plain looking. , following Presmanes Hill (2021a), pair third option, postcards (Kross 2021).third option, one start , postcards (Kross 2021). tailored solution creates simple biographical websites look great. set-GitHub R Studio, literally possible postcards website online five minutes.","code":""},{"path":"interactive-communication.html","id":"postcards","chapter":"7 Interactive communication","heading":"7.2.1 Postcards","text":"Begin installing postcards, install.packages('postcards') creating new project website (‘File’ -> ‘New Project’ -> ‘New Directory’ -> ‘Postcards Website’). can pick name location project, select postcards theme. case, can start ‘trestles’ can changed later. Click option ‘Open new session’ create project.open new file can now build site clicking ‘Knit’. result one-page website (Figure 7.1).\nFigure 7.1: Example website made postcards using ‘trestles’ theme\ncan now update basic content name, bio links, match (Figure 7.2).\nFigure 7.2: Example Trestles website updated details\ndetails personalized, can push GitHub act host website. default, GitHub try build site, want, need first add hidden file turn , running console:, assuming GitHub set-Chapter 4, can use usethis (Wickham Bryan 2020) get newly created project onto GitHub. use use_git() initialize Git repository, use_github() pushes GitHub.project GitHub. can use GitHub pages host : ‘Settings -> Pages’ change source ‘main’ ‘master’, depending settings. GitHub let know address can share visit site.","code":"\nfile.create('.nojekyll')\nlibrary(usethis)\nuse_git()\nuse_github()"},{"path":"interactive-communication.html","id":"distill","chapter":"7 Interactive communication","heading":"7.2.2 Distill","text":"now use distill (Allaire et al. 2021) build additional infrastructure around postcards site, following Presmanes Hill (2021a). explore aspects distill make nice choice, mention trade-offs. First, install distill install.packages('distill'), , create new project website (‘File’ -> ‘New Project’ -> ‘New Directory’ -> ‘Distill Blog’).can pick name location project, set title. Select ‘Configure GitHub Pages’ also ‘Open new session’. options can changed ex post. look something like Figure 7.3.\nFigure 7.3: Example settings setting distill\npoint can click ‘Build Website’ Build tab, see default website (Figure 7.4).\nFigure 7.4: Example default distill website\n, now need update reflect details. default ‘Distill Blog’ blog homepage. can change use postcards page homepage. First change name ‘index.Rmd’ ‘blog.Rmd’ create new ‘trestles’ page:trestles page open, need add following line yaml file: site: distill::distill_website. Figure 7.5 added line 16, can rebuild website.\nFigure 7.5: Updating yaml change homepage\ncan make changes default content earlier, instance, updating links, image, bio. advantage using distill now additional pages, just one-page website, also blog. default, ‘’ page, pages may useful, depending particular use-case, include: ‘research’, ‘teaching’, ‘talks’, ‘projects’, ‘software’, ‘datasets’. example, add edit page called ‘software’ using distill::create_article(file = 'software').create open R Markdown document. add website, open ’_site.yml’ add line ‘navbar’ (Figure 7.6. done can rebuild website, ‘software’ page added.\nFigure 7.6: Adding another page website\ncan continue process happy website. instance, may want add blog. follow pattern , ‘blog’ instead ‘software’.happy website, can push GitHub use GitHub Pages host , way postcards site.Using distill good option need multi-page website, still want fairly controlled environment. many options can changed, Presmanes Hill (2021a) good starting point, addition distill homepage: https://rstudio.github.io/distill/.said, distill opinionated. great option, want something little flexible blogdown might better option.","code":"\npostcards::create_postcard(file = \"index.Rmd\", template = \"trestles\")"},{"path":"interactive-communication.html","id":"blogdown","chapter":"7 Interactive communication","heading":"7.2.3 Blogdown","text":"Using blogdown (Xie, Dervieux, Hill 2021) work Google sites Squarespace. requires little knowledge using basic Wordpress site. need customize absolutely every aspect website, need everything ‘just ’ blogdown may good option. blogdown allows variety level expression possible distill. Presmanes Hill (2021b) Xie, Thomas, Presmanes Hill (2021) useful learning blogdown.First need install blogdown install.packages(\"blogdown\"). create new project website (‘File’ -> ‘New Project’ -> ‘New Directory’ -> ‘Website using blogdown’). point can set name location, also select ‘Open new session’ (Figure 7.7).\nFigure 7.7: Example settings setting blogdown\ncan click ‘Build Website’ ‘Build’ pane, extra step needed; need serve site blogdown:::serve_site(). , site show ‘Viewer’ pane (Figure 7.8).\nFigure 7.8: Serving default blogdown site\ndefault website now ‘served’ locally. means changes make reflected website see Viewer pane. see website web browser, click ‘Show new window’ top left Viewer. open website using address R Studio also provides.now want update content, starting ‘’ section. go ‘content -> .md’ modify add content. One nice aspect blogdown automatically reload content save, changes appear immediately modify aspects also. instance, change logo, adding square image ‘public/images/’ changing call ‘logo.png’ ‘config.yaml’. happy , can make website public way postcards.One advantage using blogdown allows us use Hugo templates. provides large number beautifully crafted websites. pick theme go Hugo themes page: https://themes.gohugo.io. hundreds different themes. general, can made work blogdown, sometimes can bit hassle.One nice option Apéro: https://hugo-apero-docs.netlify.app. can specify use theme part creating new site (‘File’ -> ‘New Project’ -> ‘New Directory’ -> ‘Website using blogdown’). point, addition setting name location, can specify theme. Specifically, ‘Hugo theme’ field, can specify GitHub username repository, case ‘hugo-apero/apero’ (Figure 7.9).\nFigure 7.9: Using Apéro theme\n","code":""},{"path":"interactive-communication.html","id":"interactive-maps","chapter":"7 Interactive communication","heading":"7.3 Interactive maps","text":"nice thing interactive maps can let user decide interested . instance, case map, people interested , say, Toronto, others interested Chennai even Auckland. difficult present map focused , interactive map way allow users focus want.said, important cognizant build maps, broadly, done scale enable us able build maps. instance, regard Google, McQuire (2019) says:Google began life 1998 company famously dedicated organising vast amounts data Internet. last two decades ambitions changed crucial way. Extracting data words numbers physical world now merely stepping-stone towards apprehending organizing physical world data. Perhaps shift surprising moment become possible comprehend human identity form (genetic) ‘code’. However, apprehending organizing world data current settings likely take us well beyond Heidegger’s ‘standing reserve’ modern technology enframed ‘nature’ productive resource. 21st century, stuff human life —genetics bodily appearances, mobility, gestures, speech, behaviour—progressively rendered productive resource can harvested continuously subject modulation time.mean use build interactive maps? course . important aware fact frontier, boundaries appropriate use still determined. Indeed, literal boundaries maps consistently determined updated. move digital maps, compared physical printed maps, means possible different users presented different realities. instance, ‘…Google routinely takes sides border disputes. Take, instance, representation border Ukraine Russia. Russia, Crimean Peninsula represented hard-line border Russian-controlled, whereas Ukrainians others see dotted-line border. strategically important peninsula claimed nations violently seized Russia 2014, one many skirmishes control’ Bensinger (2020).","code":""},{"path":"interactive-communication.html","id":"leaflet","chapter":"7 Interactive communication","heading":"7.3.1 Leaflet","text":"can use leaflet (Cheng, Karambelkar, Xie 2021) make interactive maps. essentials similar ggmap (Kahle Wickham 2013), many additional aspects beyond . can redo US military deployments map Chapter 6 used troopdata (Flynn 2021). advantage interactive map can plot bases allow user focus area want, comparison Chapter 6 just picked particular countries.way graph ggplot2 begins ggplot(), map leaflet begins leaflet(). can specify data, options width height. , add ‘layers’ way added ggplot2. first layer add tile, using addTiles(). case, default OpenStreeMap. add markers addMarkers() show location base (Figure 7.10).\nFigure 7.10: Interactive map US bases\ntwo new arguments, compared ggmap. first ‘popup’, behavior occurs user clicks marker. case, name base provided. second ‘label’, happens user hovers marker. case name country.can try another example, time amount spent building bases. introduce different type marker , circles. allow us use different colors outcomes type. four possible outcomes: “$100,000”, “$10,000”, “$1,000”, “$1,000 less” (Figure 7.11).\nFigure 7.11: Interactive map US bases colored circules indicate spend\n","code":"\nlibrary(leaflet)\nlibrary(tidyverse)\nlibrary(troopdata)\n\nbases <- get_basedata()\n\n# Some of the bases include unexpected characters which we need to address\nEncoding(bases$basename) <- 'latin1'\n\nleaflet(data = bases) |>\n  addTiles() |>  # Add default OpenStreetMap map tiles\n  addMarkers(lng = bases$lon, \n             lat = bases$lat, \n             popup = bases$basename,\n             label = bases$countryname)\nbuild <- \n  get_builddata(startyear = 2008, endyear = 2019) |>\n  filter(!is.na(lon)) |>\n  mutate(\n    cost = case_when(\n      spend_construction > 100000 ~ \"More than $100,000\",\n      spend_construction > 10000 ~ \"More than $10,000\",\n      spend_construction > 1000 ~ \"More than $1,000\",\n      TRUE ~ \"$1,000 or less\"\n      )\n    )\n\npal <-\n  colorFactor(\"Dark2\", domain = build$cost |> unique())\n\nleaflet() |>\n  addTiles() |>  # Add default OpenStreetMap map tiles\n  addCircleMarkers(\n    data = build,\n    lng = build$lon,\n    lat = build$lat,\n    color = pal(build$cost),\n    popup = paste(\n      \"<b>Location:<\/b>\",\n      as.character(build$location),\n      \"<br>\",\n      \"<b>Amount:<\/b>\",\n      as.character(build$spend_construction),\n      \"<br>\"\n    )\n  ) |>\n  addLegend(\n    \"bottomright\",\n    pal = pal,\n    values = build$cost |> unique(),\n    title = \"Type\",\n    opacity = 1\n  )"},{"path":"interactive-communication.html","id":"mapdeck","chapter":"7 Interactive communication","heading":"7.3.2 Mapdeck","text":"mapdeck (Cooley 2020) based WebGL. means web browser lot work us. enables us accomplish things mapdeck leaflet struggles , larger datasets.point used ‘stamen maps’ underlying tile, mapdeck uses ‘Mapbox’: https://www.mapbox.com/. requires registering account obtaining token. free needs done . token add R environment (details process covered Chapter 9) running usethis::edit_r_environ(), open text file. can add Mapbox secret token.save ‘.Renviron’ file, restart R (‘Session’ -> ‘Restart R’).obtained token, can create plot base spend data earlier (Figure 7.12).\nFigure 7.12: Interactive map US bases using Mapdeck\n","code":"\nMAPBOX_TOKEN = 'PUT_YOUR_MAPBOX_SECRET_HERE'\nlibrary(mapdeck)\n\nmapdeck(style = mapdeck_style('dark')\n        ) |>\n  add_scatterplot(\n    data = build, \n    lat = \"lat\", \n    lon = \"lon\", \n    layer_id = 'scatter_layer',\n    radius = 10,\n    radius_min_pixels = 5,\n    radius_max_pixels = 100,\n    tooltip = \"location\"\n  )"},{"path":"interactive-communication.html","id":"shiny","chapter":"7 Interactive communication","heading":"7.4 Shiny","text":"shiny (Chang et al. 2021) way making interactive web applications using R. fun, fiddly. going step one way take advantage shiny. quickly add interactivity graphs. return shiny detail Chapter 18.going make interactive graph based ‘babynames’ dataset babynames (Wickham 2019b). First, build static version (Figure 7.13).\nFigure 7.13: Popular baby names\ncan see popular boys names tend clustered, compared -popular girls names, may spread . However, one thing might interested effect ‘bins’ parameter shapes see. might like use interactivity explore different values.get started, create new shiny app (‘File -> New File -> Shiny Web App’). Give name, ‘not_my_first_shiny’ leave options default. new file ‘app.R’ open click ‘Run app’ see looks like.Now replace content file, ‘app.R’, content , click ‘Run app’just build interactive graph number bins can changed. look like Figure 7.14.\nFigure 7.14: Example Shiny app user controls number bins\n","code":"\nlibrary(babynames)\nlibrary(tidyverse)\n\ntop_five_names_by_year <-\n  babynames |>\n  group_by(year, sex) |>\n  arrange(desc(n)) |>\n  slice_head(n = 5)\n\ntop_five_names_by_year |>\n  ggplot(aes(x = n, fill = sex)) +\n  geom_histogram(position = \"dodge\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(x = \"Babies with that name\",\n       y = \"Occurances\",\n       fill = \"Sex\")\nlibrary(shiny)\n\n# Define UI for application that draws a histogram\nui <- fluidPage(\n  # Application title\n  titlePanel(\"Count of names for five most popular names each year.\"),\n  \n  # Sidebar with a slider input for number of bins\n  sidebarLayout(sidebarPanel(\n    sliderInput(\n      inputId = \"number_of_bins\",\n      label = \"Number of bins:\",\n      min = 1,\n      max = 50,\n      value = 30\n    )\n  ),\n  \n  # Show a plot of the generated distribution\n  mainPanel(plotOutput(\"distPlot\")))\n)\n\n# Define server logic required to draw a histogram\nserver <- function(input, output) {\n  output$distPlot <- renderPlot({\n    # Draw the histogram with the specified number of bins\n    top_five_names_by_year |>\n      ggplot(aes(x = n, fill = sex)) +\n      geom_histogram(position = \"dodge\", bins = input$number_of_bins) +\n      theme_minimal() +\n      scale_fill_brewer(palette = \"Set1\") +\n      labs(x = \"Babies with that name\",\n           y = \"Occurances\",\n           fill = \"Sex\")\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)"},{"path":"interactive-communication.html","id":"exercises-and-tutorial-6","chapter":"7 Interactive communication","heading":"7.5 Exercises and tutorial","text":"","code":""},{"path":"interactive-communication.html","id":"exercises-6","chapter":"7 Interactive communication","heading":"7.5.1 Exercises","text":"Based Presmanes Hill (2021a), posts distill re-built automatically (pick one)?\n\nYes\nDepends settings\nNoYesDepends settingsBased Lovelace, Nowosad, Muenchow (2019), please explain paragraph two, difference vector data raster data context geographic data?Based Wickham (2021a), shiny uses:\nObject-oriented programming\nFunctional programming\nReactive programming\nObject-oriented programmingFunctional programmingReactive programmingIn paragraph two, important website?following packages use make website (select apply)?\ndistill\nblogdown\npostcards\nhugo\ndistillblogdownpostcardshugoLooking help file postcards, following themes use (select apply)?\njolla\njolla-blue\njolla-red\ntrestles\nmjolnir\nonofre\nsolana\njollajolla-bluejolla-redtrestlesmjolnironofresolanaWhich function use stop GitHub trying build site instead just serving (pick one)?\nfile.create('.nojekyll')\nfile.remove('.nojekyll')\nfile.create('.jekyll')\nfile.remove('.jekyll')\nfile.create('.nojekyll')file.remove('.nojekyll')file.create('.jekyll')file.remove('.jekyll')argument addMarkers() used specify behavior occurs marker clicked (pick one)?\nlayerId\nicon\npopup\nlabel\nlayerIdiconpopuplabel","code":""},{"path":"interactive-communication.html","id":"tutorial-6","chapter":"7 Interactive communication","heading":"7.5.2 Tutorial","text":"catalyst tutorial work Mauricio Vargas Sepúlveda (‘Pachá’) Andrew Whitby.Please obtain data ethnic origins number victims Auschwitz. use shiny create interactive graph interactive table. show number people murdered nationality/category allow user specify groups interested seeing data . Publish . , based themes brought Bouie (2022), discuss work least two pages. Submit PDF created using R Markdown, ensure contains link app GitHub repo contains code data.","code":""},{"path":"interactive-communication.html","id":"paper-1","chapter":"7 Interactive communication","heading":"7.5.3 Paper","text":"point, Paper Two (Appendix B.2) appropriate.","code":""},{"path":"farm-data.html","id":"farm-data","chapter":"8 Farm data","heading":"8 Farm data","text":"Required materialRead Atlas AI, Chapter 3 ‘Data’, (Crawford 2021).Read Guide Census Population, 2016, Chapter 10 ‘Data quality assessment’, (Statistics Canada 2017).Read Working-Class Households Reading, (focus method approach, necessarily specific results) (Bowley 1913).Read Representative Method: Method Stratified Sampling Method Purposive Selection, Parts ‘Introduction’, III ‘Different Aspects Representative Method’, V ‘Conclusion’ Bowley’s discussion p. 607 - 610, (Neyman 1934).Read Using sex gender survey adjustment, (Kennedy et al. 2020).Key concepts skillsWhy conduct sampling two approaches: probability non-probability.Terminology concepts including: target population, sampling frame, sample, simple random sampling, systematic sampling, stratified sampling, cluster sampling.Ratio regression estimators.Non-probability sampling including convenience quota sampling also snowball respondent-driven sampling.Obtain data censuses datasets provided governments.Key librariescancensus (von Bergmann, Shkolnik, Jacobs 2021)canlang (T. Timbers 2020)tidyverse (Wickham et al. 2019a)Key functionscancensus::get_census()cancensus::list_census_datasets()cancensus::list_census_regions()cancensus::list_census_vectors()cancensus::set_api_key()canlang::can_langcanlang::region_langdplyr::left_join()dplyr::mutate()dplyr::row_number()dplyr::select()dplyr::slice_max()dplyr::slice_min()dplyr::slice_sample()readr::read_csv()set.seed()","code":""},{"path":"farm-data.html","id":"introduction-5","chapter":"8 Farm data","heading":"8.1 Introduction","text":"think world telling stories , one difficult aspects reduce beautiful complexity dataset can use. need know giving . Often, interested understanding implications dataset, making forecasts based , using dataset make claims broader world. Regardless turn world data, ever sample data need. Statistics provides formal approaches use keep issues front mind.chapter first introduce statistical notions around sampling provide framework use guide data gathering. discuss censuses.","code":""},{"path":"farm-data.html","id":"sampling-essentials","chapter":"8 Farm data","heading":"8.2 Sampling essentials","text":"Statistics heart telling stories data. Statisticians spent considerable time effort thinking properties various samples data enable us speak implications broader population.Let us say data. instance, particular toddler goes sleep 6:00pm every night. might interested know whether bedtime common among toddlers, unusual toddler. one toddler ability use bedtime speak toddlers limited.One approach talk friends also toddlers. talk friends friends. many friends, friends friends, ask can begin feel comfortable speaking underlying truth toddler bedtime?Wu Thompson (2020, 3) describe statistics ‘science collect analyze data draw statements conclusions unknown populations.’ ‘population’ refers infinite group can never know exactly, can use probability distributions random variables describe characteristics . Another way say statistics involves getting data trying say something sensible based .critical terminology includes:‘Target population’: collection items like speak.‘Sampling frame’: list items target population get data .‘Sample’: items sampling frame get data .target population finite set labelled items, size \\(N\\). instance, hypothetically add label books world: ‘Book 1’, ‘Book 2’, ‘Book 3’, …, ‘Book \\(N\\)’. difference use term population , everyday usage. instance, one sometimes hears work census data say need worry sampling whole population country. conflation terms, sample gathered census population country.can difficult define target population. instance, say asked find consumption habits hipsters. can define target population? someone regularly eats avocado toast, never drunk bullet coffee, population? aspects might interested formally defined extent always commonly realized. instance, whether area classified rural often formally defined country’s statistical agency. aspects less clear. instance, classify someone ‘smoker’? 15-year-old 100 cigarettes lifetime, need treat differently none. 90-year-old 100 cigarettes lifetime, likely different 90-year-old none? age, number cigarettes answers change?Consider want speak titles books ever written. target population books ever written. almost impossible us imagine get information title book written nineteenth century, author locked desk never told anyone . One sampling frame books Library Congress Online Catalog, another 25 million digitized Google (Somers 2017). finally, sample may tens thousands available Project Gutenberg, can access using gutenbergr (D. Robinson 2021).consider another example, consider wanting speak attitudes Brazilians live Germany. target population Brazilians live Germany. One possible source information Facebook case, sampling frame might Brazilians live Germany Facebook. sample might Brazilians live Germany Facebook can gather data . target population sampling frame different Brazilians live Germany Facebook. sampling frame different sample likely able gather data Brazilians live Germany Facebook.","code":""},{"path":"farm-data.html","id":"sampling-in-dublin-and-reading","chapter":"8 Farm data","heading":"8.2.1 Sampling in Dublin and Reading","text":"clearer, consider two examples: 1798 count number inhabitants Dublin, Ireland (Whitelaw 1905), 1912 count working-class households Reading, England (Bowley 1913).1798 Reverend James Whitelaw conducted survey Dublin, Ireland, count population. Whitelaw (1905) describes population estimates wide variation, instance estimated size London time ranged 128,570 300,000. Reverend Whitelaw expected Lord Mayor Dublin compel person charge house affix list inhabitants house door, Reverend Whitelaw simply use .Instead, found lists ‘frequently illegible, generally short actual number third, even one-half’. instead recruited assistants, went door--door making counts. resulting estimates particularly informative (Figure 8.1). total population Dublin 1798 estimated 182,370.\nFigure 8.1: Extract results Reverend Whitelaw found 1798\nOne aspect worth noticing Reverend Whitelaw includes information class. difficult know determined, played large role data collection. Reverend Whitelaw describes houses ‘middle upper classes always contained individual competent task [making list]’. ‘among lower class, forms great mass population city, case different’. difficult know Reverend Whitelaw known upper middle classes representing number, lower class . also difficult imagine Reverend Whitelaw going houses upper class counting number, assistants lower classes. always, issue defining target population difficult one, seems may slightly different approaches class.little one hundred years later, Bowley (1913) interested counting number working-class households Reading, England. Bowley selects sample using following procedure (Bowley 1913, 672):One building ten marked throughout local directory alphabetical order streets, making 1,950 . 300 marked shops, factories, institutions non-residential buildings, 300 found indexed among Principal Residents, marked. remaining 1,350 working-class houses, number volunteers set visit every one … []t decided take one house 20, rejecting incomplete information intermediate tenths. visitors instructed never substitute another house marked, however difficult proved get information, whatever type house.Bowley (1913) continues ended information 622 working-class households. , judged, based census 18,000 households Reading, Bowley (1913) applies ‘[t]multiplier twenty-one… sample data give estimates whole Reading.’ Bowley (1913) explains reasonableness estimates depends ‘proportion whole, magnitude, conditions random sampling secured, believed inquiry’. Bowley , instance, able furnish information rent paid per week (Figure 8.2).\nFigure 8.2: Extract results Bowley found rent paid working-class Reading\n","code":""},{"path":"farm-data.html","id":"probabilistic-sampling","chapter":"8 Farm data","heading":"8.2.2 Probabilistic sampling","text":"identified target population sampling frame, need distinguish probability non-probability sampling, Neyman (1934) describes ‘random sampling’ ‘purposive selection’:‘Probability sampling’: Every unit sampling frame , known, chance sampled specific sample obtained randomly based chances. Note chances necessarily need unit.‘Non-probability sampling’: Units sampling frame sampled based convenience, quotas, judgement, non-random processes.Often difference probability non-probability sampling one degree. instance, often forcibly obtain data almost always aspect volunteering. Even penalties providing data, case completing census form many countries, difficult even government force people fill completely truthfully. One reason Randomized Control Trial revolution, discussed Chapter 10, needed due lack probability sampling. important aspect clear probability sampling role uncertainty. allows us make claims population, based sample, known amounts error. trade-probability sampling often expensive difficult.add specificity discussion, following Lohr (2019, 27) may help consider numbers 1 100 let us define target population. simple random sampling, every unit chance included. case 20 per cent. expect around 20 units sample, around 1 5 compared target population.systematic sampling, used Bowley (1913), proceed selecting value, say 5. randomly pick starting point units 1 5, say 3. include every fifth unit. starting point usually randomly selecting.consider population, typically grouping. may straight-forward country states, provinces, counties, statistical districts; university faculties departments; humans age-groups. stratified structure one can divide population mutually exclusive collectively exhaustive sub-populations, strata.use stratification help efficiency sampling balance survey. instance, population US around 335 million, 40 million California, Wyoming around half million. even survey 10,000 responses expect 15 responses Wyoming, make inference Wyoming difficult. use stratification ensure 200 responses 50 US states. use random sampling within state select person data gathered.case, stratify illustration, consider strata 10s, , 1 10 one stratum, 11 20 another, . use simple random sampling within strata select two units.finally, can also take advantage clusters may exist dataset. Like strata, clusters collectively exhaustive mutually exclusive. examples earlier, states, departments, age-groups remain valid clusters. However, intentions toward groups different. Specific, cluster sampling, intend collect data every cluster, whereas stratified sampling . stratified sampling look every stratum conduct simple random sampling within strata select sample. cluster sampling conduct simple random sampling select clusters interest. can either sample every unit selected clusters use simple random sampling, within selected clusters, select units. said, difference can become less clear practice, especially ex post.case, cluster illustration based 10s. use simple random sampling select two clusters use entire cluster.point can illustrate differences approaches (Figure 8.3).\nFigure 8.3: Illustrative example simple random sampling, systematic sampling, stratified sampling, cluster sampling numbers 1 100\nestablished sample, typically want use make claims population. Neyman (1934, 561) goes says ‘[o]bviously problem representative method par excellence problem statistical estimation. interested characteristics certain population, \\(\\pi\\), either impossible least difficult study detail, try estimate characteristics basing judgment sample.’particular, typically interested estimate population mean variance.Scaling can used interested using count sample imply total count population. saw Bowley (1913) ratio number households sample, compared number households known census, 21, information used scale sample.consider example, perhaps interested sum numbers 1 100. know samples size 20, need scaled five times (Table 8.1).Table 8.1: Sum numbers sample, implied sum populationThe actual sum population 5,050. can obtain using trick, attributed Euler, noticed sum 1 number can quickly obtained finding middle number multiplying one plus number. , case, 50*101. Alternatively can use R: sum(1:100).estimate population sum, based scaling, especially revealing. closest stratified sample, closely followed systematic sampling. Cluster sampling little 10 per cent , simple random sampling little away. get close, important sampling method gets many higher values possible. stratified systematic sampling, ensured unit larger numbers particularly well. performance cluster simple random sampling depend particular clusters, units, selected. case, stratified systematic sampling ensured estimate sum population, far away actual population sum.approach long history. instance, Adolphe Quetelet, nineteenth century astronomer, mathematician, statistician, sociologist proposed one. Stigler (1986, 163) describes 1826 Quetelet become involved statistical bureau, planning census. Quetelet argued births deaths well known, migration . proposed approach based counts specific geographies, scaled whole country. criticism plan focused difficulty selecting appropriate geographies, saw also example cluster sampling. criticism reasonable, even today, two hundred years later, something keep front mind, (Stigler 1986):[Quetelet] acutely aware infinite number factors affect quantities wished measure, lacked information tell indeed important. … reluctant group together homogenous, data reason believe … aware myriad potentially important factors, without knowing truly important effect may felt, often fear worst’…. [Quetelet] bring treat large regions homogeneous, [] think single rate applying large areaWe able scaling know population total, know , concerns around precision approach may use ratio estimator.Ratio estimators also long history. instance, 1802 used Pierre-Simon Laplace estimate total population France, based ratio number registered births, known throughout country, number inhabitants, know certain communes. calculated ratio three communes, scaled , based knowing number births across whole country produce estimate population France (Lohr 2019).particular, ratio estimator population parameter ratio two means. instance, may information number hours toddler sleeps overnight, \\(x\\), number hours parents sleep overnight \\(y\\) 30 day period.average :ratio estimate proportion sleep parent gets compared toddler :\\[\\hat{B} = \\frac{\\bar{y}}{\\bar{x}} = \\frac{4.9}{6.16} \\approx 0.8\\]acknowledging spectrum, much statistics developed based probability sampling. considerable amount modern sampling done using non-probability sampling. common approach use Facebook advertisements recruit panel respondents exchange compensation. panel group sent various surveys necessary. think moment implications . instance, type people likely respond advertisement? richest person world likely respond? especially young especially old people likely respond? cases, possible census. Nation-states typically one every five ten years. reason nation states —expensive, time-consuming, surprisingly, sometimes accurate may hope general need .","code":"\nlibrary(tidyverse)\n\nset.seed(853)\n\nillustrative_sampling <-\n  tibble(unit = 1:100,\n         simple_random_sampling = \n           sample(\n             x = c(\"Included\", \"Not included\"),\n             size = 100,\n             replace = TRUE,\n             prob = c(0.2, 0.8)\n             ))\n\nillustrative_sampling\n#> # A tibble: 100 × 2\n#>     unit simple_random_sampling\n#>    <int> <chr>                 \n#>  1     1 Not included          \n#>  2     2 Not included          \n#>  3     3 Not included          \n#>  4     4 Not included          \n#>  5     5 Not included          \n#>  6     6 Not included          \n#>  7     7 Not included          \n#>  8     8 Not included          \n#>  9     9 Not included          \n#> 10    10 Not included          \n#> # … with 90 more rows\nset.seed(853)\n\nstarting_point <- sample(x = c(1:5), \n                         size = 1)\n\nillustrative_sampling <-\n  illustrative_sampling |>\n  mutate(systematic_sampling = \n           if_else(row_number() %in% seq.int(from = starting_point, \n                                             to = 100, \n                                             by = 5), \n                   \"Included\", \n                   \"Not included\")\n         )\n\nillustrative_sampling\n#> # A tibble: 100 × 3\n#>     unit simple_random_sampling systematic_sampling\n#>    <int> <chr>                  <chr>              \n#>  1     1 Not included           Included           \n#>  2     2 Not included           Not included       \n#>  3     3 Not included           Not included       \n#>  4     4 Not included           Not included       \n#>  5     5 Not included           Not included       \n#>  6     6 Not included           Included           \n#>  7     7 Not included           Not included       \n#>  8     8 Not included           Not included       \n#>  9     9 Not included           Not included       \n#> 10    10 Not included           Not included       \n#> # … with 90 more rows\nset.seed(853)\n\nselected_within_strata <-\n  illustrative_sampling |>\n  mutate(strata = (row_number() - 1) %/% 10) |>\n  group_by(strata) |>\n  slice_sample(n = 2) |>\n  pull(unit)\n\nillustrative_sampling <-\n  illustrative_sampling |>\n  mutate(\n    stratified_sampling = if_else(\n      row_number() %in% selected_within_strata,\n      \"Included\",\n      \"Not included\"\n    )\n  )\n\nillustrative_sampling\n#> # A tibble: 100 × 4\n#>     unit simple_random_sa… systematic_samp… stratified_samp…\n#>    <int> <chr>             <chr>            <chr>           \n#>  1     1 Not included      Included         Included        \n#>  2     2 Not included      Not included     Not included    \n#>  3     3 Not included      Not included     Not included    \n#>  4     4 Not included      Not included     Not included    \n#>  5     5 Not included      Not included     Not included    \n#>  6     6 Not included      Included         Not included    \n#>  7     7 Not included      Not included     Not included    \n#>  8     8 Not included      Not included     Not included    \n#>  9     9 Not included      Not included     Included        \n#> 10    10 Not included      Not included     Not included    \n#> # … with 90 more rows\nset.seed(853)\n\nselected_clusters <- \n  sample(x = c(0:9),\n         size = 2)\n\nillustrative_sampling <-\n  illustrative_sampling |>\n  mutate(cluster = (row_number() - 1) %/% 10, \n         cluster_sampling = if_else(\n           cluster %in% selected_clusters,\n           \"Included\",\n           \"Not included\"\n           )\n         ) %>% \n  select(-cluster)\n\nillustrative_sampling\n#> # A tibble: 100 × 5\n#>     unit simple_random_sa… systematic_samp… stratified_samp…\n#>    <int> <chr>             <chr>            <chr>           \n#>  1     1 Not included      Included         Included        \n#>  2     2 Not included      Not included     Not included    \n#>  3     3 Not included      Not included     Not included    \n#>  4     4 Not included      Not included     Not included    \n#>  5     5 Not included      Not included     Not included    \n#>  6     6 Not included      Included         Not included    \n#>  7     7 Not included      Not included     Not included    \n#>  8     8 Not included      Not included     Not included    \n#>  9     9 Not included      Not included     Included        \n#> 10    10 Not included      Not included     Not included    \n#> # … with 90 more rows, and 1 more variable:\n#> #   cluster_sampling <chr>\nnew_labels <- c(simple_random_sampling = \"Simple random sampling\", \n                systematic_sampling = \"Systematic sampling\",\n                stratified_sampling = \"Stratified sampling\",\n                cluster_sampling = \"Cluster sampling\")\n\nillustrative_sampling_long <- \n  illustrative_sampling |>\n  pivot_longer(\n    cols = c(\n      simple_random_sampling,\n      systematic_sampling,\n      stratified_sampling,\n      cluster_sampling),\n    names_to = \"sampling_method\",\n    values_to = \"in_sample\"\n  ) |>\n  mutate(sampling_method = factor(sampling_method,\n                                  levels = c(\"simple_random_sampling\",\n                                             \"systematic_sampling\",\n                                             \"stratified_sampling\",\n                                             \"cluster_sampling\"))\n         ) \n\nillustrative_sampling_long |>\n  filter(in_sample == \"Included\") |>\n  ggplot(aes(x = unit, y = in_sample)) +\n  geom_point() +\n  facet_wrap(vars(sampling_method),\n             dir = \"v\",\n             ncol = 1,\n             labeller = labeller(sampling_method = new_labels)\n             ) +\n  theme_minimal() +\n  labs(x = \"Unit\",\n       y = \"Is included in sample\") +\n  theme(axis.text.y = element_blank())\nillustrative_sampling_long |>\n  filter(in_sample == \"Included\") |>\n  group_by(sampling_method) |>\n  summarize(sum_from_sample = sum(unit)) |>\n  mutate(scaled_by_five = sum_from_sample * 5) |>\n  knitr::kable(\n    caption = \"Sum of the numbers in each sample, and implied sum of population\",\n    col.names = c(\"Sampling method\", \"Sum of sample\", \"Implied population sum\"),\n    format.args = list(big.mark = \",\")\n  )\nset.seed(853)\n\nsleep <- \n  tibble(\n    toddler_sleep = sample(x = c(2:14), size = 30, replace = TRUE),\n    difference = sample(x = c(0:3), size = 30, replace = TRUE),\n    parent_sleep = toddler_sleep - difference\n  )\n\nsleep\n#> # A tibble: 30 × 3\n#>    toddler_sleep difference parent_sleep\n#>            <int>      <int>        <int>\n#>  1            10          1            9\n#>  2            11          0           11\n#>  3            14          2           12\n#>  4             2          2            0\n#>  5             6          1            5\n#>  6            14          2           12\n#>  7             3          0            3\n#>  8             5          2            3\n#>  9             4          3            1\n#> 10             4          1            3\n#> # … with 20 more rows\nsleep %>% \n  summarize(toddler_sleep_average = mean(toddler_sleep),\n            parent_sleep_average = mean(parent_sleep))\n#> # A tibble: 1 × 2\n#>   toddler_sleep_average parent_sleep_average\n#>                   <dbl>                <dbl>\n#> 1                  6.17                  4.9"},{"path":"farm-data.html","id":"non-probability-samples","chapter":"8 Farm data","heading":"8.2.3 Non-probability samples","text":"Non-probability samples important role play typically cheaper quicker obtain probability samples. , discussed, difference probability non-probability samples sometimes one degree, rather dichotomy. case, non-probability samples legitimate appropriate tasks provided one clear trade-offs ensure transparency (Baker et al. 2013).Convenience sampling involves gathering data sample easy access. instance, one often asks one’s friends family fill survey way testing wide-scale distribution. instead analyze sample, likely using convenience sampling.main issue convenience sampling unlikely able speak much broader population filled survey. also tricky ethical considerations, typically lack anonymity may bias results. hand, can useful cheaply get quick sense situation rolling sampling approaches likely broadly useful.Quota sampling occurs strata, use random sampling within strata select unit. instance, stratified US based state, instead ensuring everyone Wyoming chance chosen stratum, just picked people Jackson Hole. , advantages approach, especially terms speed cost, resulting sample likely biased various ways.saying goes, birds feather flock together. can take advantage sampling. Although Handcock Gile (2011) describe various uses , notoriously difficult define attribution multidisciplinary work, snowball sampling nicely defined Goodman (1961). Following Goodman (1961), conduct snowball sampling, first draw random sample sampling frame. asked name \\(k\\) others also sample population, initial draw, form ‘first stage’. individual first stage similarly asked name \\(k\\) others also sample population, random draw first stage, form ‘second stage’. need specified number stages, \\(s\\), also \\(k\\) ahead time.Respondent-driven sampling developed Heckathorn (1997) focus hidden populations, : 1) sampling frame 2) known sampling population negative effect. instance, imagine various countries difficult sample gay population abortions illegal. Respondent-driven sampling differs snowball sampling two ways: 1) addition compensation response, case snowball sampling, respondent-driven sampling typically also involves compensation recruiting others. 2) Respondents asked provide information others investigator, instead recruit study. Selection sample occurs sampling frame, instead networks already sample (M. J. Salganik Heckathorn 2004).established foundations sampling, remain front mind, turn describe approaches gathering data. largely represent convenience samples.","code":""},{"path":"farm-data.html","id":"censuses","chapter":"8 Farm data","heading":"8.3 Censuses","text":"variety sources data produced purposes used datasets. One thinks especially censuses. Whitby (2020, 30–31) provides enthralling overview, describing earliest censuses written suggestions China’s Yellow River valley, used just purposes taxation conscription. Whitby (2020) also highlights links censuses religion, quoting Book Luke ‘days Caesar Augustus issued decree census taken entire Roman world’, led David Mary travelling Bethlehem. TheTaxation substantial motivator censuses. Jones (1953) describes census records survive ‘probably engraved late third early fourth century .D., Diocletian colleagues successors known active carrying censuses serve basis new system taxation’. detailed records sort abused. instance, Luebke Milton (1994) say ‘(t)Nazi regime gathered information two relatively conventional tools modern administration: national census police registration’.Another source data deliberately put together dataset include economic conditions unemployment, inflation, GDP. Interestingly, Rockoff (2019) describes economic statistics actually developed federal government, even though federal governments typically eventually took role. Typically, sources data put together governments. powers state behind enables thorough way datasets , similarly bring specific perspective. say census data unimpeachable, common errors include - -enumeration, well misreporting [steckel1991quality].Another, similarly, large established source data long-running large surveys. conducted regular basis, usually directly conducted government, usually funded, one way another, government. instance, often think electoral surveys, Canadian Election Study, run association every federal election since 1965, similarly British Election Study associated every general election since 1964.Finally, large push toward open data government. term become contentious occurred practice, underlying principle—government make available data —undeniable.\nchapter cover datasets, term ‘farmed data’. typically fairly nicely put together work collecting, preparing cleaning datasets typically done. also, usually, conducted known release cycle. instance, developed countries release unemployment inflation dataset monthly basis, GDP quarterly basis, census every five ten years.datasets always useful, developed time much analysis conducted without use scripts programming languages. cottage industry R package development sprung around making easier get datasets R. chapter cover especially useful.important recognize data neutral. Thinking clearly included dataset, systematically excluded, critical. Crawford (2021, 121) says:way data understood, captured, classified, named fundamentally act world-making containment…. myth data collection benevolent practice… obscured operations power, protecting profit avoiding responsibility consequences.point, worth briefly discussing role sex gender survey research, following Kennedy et al. (2020). Sex based biological attributes, gender socially constructed. likely interested effect gender dependent variable. Moving away non-binary concept gender, terms official statistics, something happened recently. researcher one problems insisting binary , Kennedy et al. (2020, 2) say ‘…measuring gender simply two categories, failure capture unique experiences identify either male female, whose gender align sex classification.’. researcher variety ways proceeding, Kennedy et al. (2020) discuss based : ethics, accuracy, practicality, flexibility. However, ‘single good solution can applied situations. Instead, important recognize compromise ethical concerns, statistical concerns, appropriate decision reflective ’ [p. 16]. important consideration ensure appropriate ‘respect consideration survey respondent’.","code":""},{"path":"farm-data.html","id":"canada","chapter":"8 Farm data","heading":"8.3.1 Canada","text":"first census Canada conducted 1666. 3,215 inhabitants counted, census asked age, sex, marital status, occupation (Statistics Canada 2017). association Confederation, 1867 decennial census required political representatives allocated new Parliament. Regular censuses occurred since , recent 2021.can explore data languages spoken Canada 2016 Census using canlang (T. Timbers 2020). package yet available CRAN, install GitHub, using devtools (Wickham, Hester, Chang 2020).start ‘can_lang’ dataset, provides number Canadians use language 214 languages.can quickly see top-10 common languages mother tongue.combine two datasets together ‘region_lang’ ‘region_data’, see five -common languages differ largest region, Toronto, smallest, Belleville.can see considerable difference proportions, little 50 per cent Toronto English mother tongue, case around 90 per cent Belleville.general, data Canadian censuses easily available countries. Statistics Canada, government agency responsible census official statistics freely provides Individuals File 2016 census Public Use Microdata File (PUMF), response request. 2.7 per cent sample 2016 census, PUMF provides limited detail.Another way access data Canadian census use cancensus (von Bergmann, Shkolnik, Jacobs 2021). package can installed CRAN. requires API key, can requested creating account going ‘edit profile’. package helper function set_api_key(\"ADD_YOUR_API_KEY_HERE\", install = TRUE) makes easier add API key ‘.Renviron’ file, way Chapter 9.can use get_census() get census data. need specify census interest, variety arguments. instance, get data 2016 census Ontario, largest Canadian province population.Data 1996, 2001, 2006, 2011, 2016 censuses available, list_census_datasets() provides metadata need provide get_census() access . Data available based variety regions, list_census_regions() provides metadata need. finally, list_census_vectors() provides metadata variables available.","code":"\ninstall.packages(\"devtools\")\ndevtools::install_github(\"ttimbers/canlang\")\nlibrary(tidyverse)\nlibrary(canlang)\n\ncan_lang\n#> # A tibble: 214 × 6\n#>    category language mother_tongue most_at_home most_at_work\n#>    <chr>    <chr>            <dbl>        <dbl>        <dbl>\n#>  1 Aborigi… Aborigi…           590          235           30\n#>  2 Non-Off… Afrikaa…         10260         4785           85\n#>  3 Non-Off… Afro-As…          1150          445           10\n#>  4 Non-Off… Akan (T…         13460         5985           25\n#>  5 Non-Off… Albanian         26895        13135          345\n#>  6 Aborigi… Algonqu…            45           10            0\n#>  7 Aborigi… Algonqu…          1260          370           40\n#>  8 Non-Off… America…          2685         3020         1145\n#>  9 Non-Off… Amharic          22465        12785          200\n#> 10 Non-Off… Arabic          419890       223535         5585\n#> # … with 204 more rows, and 1 more variable:\n#> #   lang_known <dbl>\ncan_lang |>\n  slice_max(mother_tongue, n = 10) |>\n  select(language, mother_tongue)\n#> # A tibble: 10 × 2\n#>    language                     mother_tongue\n#>    <chr>                                <dbl>\n#>  1 English                           19460850\n#>  2 French                             7166700\n#>  3 Mandarin                            592040\n#>  4 Cantonese                           565270\n#>  5 Punjabi (Panjabi)                   501680\n#>  6 Spanish                             458850\n#>  7 Tagalog (Pilipino, Filipino)        431385\n#>  8 Arabic                              419890\n#>  9 German                              384040\n#> 10 Italian                             375635\nregion_lang |>\n  left_join(region_data, by = \"region\") |>\n  slice_max(c(population)) |>\n  slice_max(mother_tongue, n = 5) |>\n  select(region, language, mother_tongue, population) |>\n  mutate(prop = mother_tongue / population)\n#> # A tibble: 5 × 5\n#>   region  language          mother_tongue population   prop\n#>   <chr>   <chr>                     <dbl>      <dbl>  <dbl>\n#> 1 Toronto English                 3061820    5928040 0.516 \n#> 2 Toronto Cantonese                247710    5928040 0.0418\n#> 3 Toronto Mandarin                 227085    5928040 0.0383\n#> 4 Toronto Punjabi (Panjabi)        171225    5928040 0.0289\n#> 5 Toronto Italian                  151415    5928040 0.0255\n\nregion_lang |>\n  left_join(region_data, by = \"region\") |>\n  slice_min(c(population)) |>\n  slice_max(mother_tongue, n = 5) |>\n  select(region, language, mother_tongue, population) |>\n  mutate(prop = mother_tongue / population)\n#> # A tibble: 5 × 5\n#>   region     language mother_tongue population    prop\n#>   <chr>      <chr>            <dbl>      <dbl>   <dbl>\n#> 1 Belleville English          93655     103472 0.905  \n#> 2 Belleville French            2675     103472 0.0259 \n#> 3 Belleville German             635     103472 0.00614\n#> 4 Belleville Dutch              600     103472 0.00580\n#> 5 Belleville Spanish            350     103472 0.00338library(tidyverse)\nlibrary(cancensus)\n\nontario_population <- \n  get_census(dataset = \"CA16\",\n             level = \"Regions\",\n             vectors = \"v_CA16_1\", \n             regions = list(PR=c('35')\n                            )\n             )\n#> \nDownloading: 120 B     \nDownloading: 120 B     \nDownloading: 120 B     \nDownloading: 120 B     \nDownloading: 120 B     \nDownloading: 120 B\n\nontario_population\n#> # A tibble: 0 × 0"},{"path":"farm-data.html","id":"usa","chapter":"8 Farm data","heading":"8.3.2 USA","text":"requirement US Census included US Constitution, decent, though clunky, access provided. US envious situation usually better approach going national statistical agency IPUMS. IPUMS provides access wide range datasets, including international census microdata. specific case US, American Community Survey (ACS) survey comparable questions asked many censuses, available annual basis, compared census quite --date time data available. ends millions responses year. Although ACS smaller census, advantage available timely basis. access ACS IPUMS.Go IPUMS, ‘IPUMS USA’, select ‘get data’ (Figure 8.4).\nFigure 8.4: IPUMS homepage, IPUMS USA shown top left box\ninterested sample, go ‘SELECT SAMPLE’, un-select ‘Default sample year’ instead select ‘2019 ACS’ ‘SUBMIT SAMPLE SELECTIONS’ (Figure 8.5).\nFigure 8.5: Selecting sample IPUMS USA specifying interest 2019 ACS\nmight interested data based state. begin looking ‘HOUSEHOLD’ variables selecting ‘GEOGRAPHIC’ (Figure 8.6).\nFigure 8.6: Specifying interested state\nadd ‘STATEICP’ ‘cart’ clicking plus, turn tick (Figure 8.7).\nFigure 8.7: Adding STATEICP cart\nmight interested data ‘PERSON’ basis, instance, ‘DEMOGRAPHIC’ variables ‘AGE’, add cart. Still ‘PERSON’ basis, might interested ‘INCOME’, instance, ‘Total personal income’ ‘INCTOT’ add cart (Figure 8.8).\nFigure 8.8: Adding additional demographic variables available individual basis\ndone, can ‘VIEW CART’, ‘CREATE DATA EXTRACT’ (Figure 8.9). point two aspects likely want change:Change ‘DATA FORMAT’ dat csv (Figure 8.10).Customize sample size likely need three million responses, just change , say, 500,000 (Figure 8.11).\nFigure 8.9: Beginning checkout process\n\nFigure 8.10: Specifying interested CSV files\n\nFigure 8.11: Reducing sample size three million responses half million\nFinally, want include descriptive name extract, instance, ‘2022-02-06: Income based state age’, specifies date made extract extract. can ‘SUBMIT EXTRACT’.asked log create account, able submit request. IPUMS email extract available, can download read R usual way. critical cite dataset use (Ruggles et al. 2021).Incredibly, full count, entire census, data available IPUMS US censuses conducted : 1850, 1860, 1870, 1880, 1900, 1910, 1920, 1930, 1940. 1890 census records destroyed due fire 1921. 1 per cent samples available years, 1990. ACS data available 2000.","code":"\nlibrary(tidyverse)\nipums_extract <- read_csv(\"usa_00010.csv\")\n\nipums_extract#> # A tibble: 6 × 4\n#>    YEAR STATEICP   AGE INCTOT\n#>   <dbl>    <dbl> <dbl>  <dbl>\n#> 1  2019       41    39   9000\n#> 2  2019       41    35   9300\n#> 3  2019       41    39  60000\n#> 4  2019       41    32  14400\n#> 5  2019       41    21      0\n#> 6  2019       41    61  11100"},{"path":"farm-data.html","id":"exercises-and-tutorial-7","chapter":"8 Farm data","heading":"8.4 Exercises and tutorial","text":"","code":""},{"path":"farm-data.html","id":"exercises-7","chapter":"8 Farm data","heading":"8.4.1 Exercises","text":"Please identify three sources data interested describe available (please include link code)?Please focus one sources. steps go order get dataset can analyzed R?Let us say take job RBC (Canadian bank) already quantitative data use. questions explore deciding whether data useful ?Write three points (welcome use dot points) government data may especially useful?Please pick government interest find inflation statistics. extent know data gathered?reference Chen et al. (2019) Martinez (2019) extent think can trust government statistics? Please mention least three governments answer.2021 census Canada asked, firstly, ‘person’s sex birth? Sex refers sex assigned birth. Male/Female’, ‘person’s gender? Refers current gender may different sex assigned birth may different indicated legal documents. Male/Female/please specify person’s gender (space typed handwritten answer)’. reference Statistics Canada (2020), please discuss extent think appropriate way census proceeded. welcome discuss case different country familiar .Pretend conducted survey everyone Canada, asked age, sex, gender. friend claims need worry uncertainty ‘whole population’. friend right wrong, ?","code":""},{"path":"farm-data.html","id":"tutorial-7","chapter":"8 Farm data","heading":"8.4.2 Tutorial","text":"Use IPUMS access ACS. Download data interest write two--three page paper analyzing .","code":""},{"path":"gather-data.html","id":"gather-data","chapter":"9 Gather data","heading":"9 Gather data","text":"Required materialRead Turning History Data: Data Collection, Measurement, Inference HPE, (Cirone Spirling 2021).Read Two Regimes Prison Data Collection, (K. R. Johnson 2021).Key concepts skillsInitial usage APIs, directly, including dealing semi-structured data, indirectly R Packages.Use R environments manage keys.Web scraping, especially reasonable use ethical concerns.Cleaning data unstructured data structured, tidy, data.Extracting data PDFs, able parsed image require OCR.Key librariesbabynames (Wickham 2019b)httr (Wickham 2019c)jsonlite (Ooms 2014)lubridate (Grolemund Wickham 2011)pdftools (Ooms 2018a)purrr (Henry Wickham 2020)rtweet (Kearney 2019)rvest (Wickham 2019d)spotifyr (Thompson et al. 2020)tesseract (Ooms 2018b)tidyverse (Wickham et al. 2019a)usethis (Wickham Bryan 2020)xml2 (Wickham, Hester, Ooms 2021)Key functionsdownload.file()factor()function()httr::GET()pdftools::pdf_text()purrr::walk2()rtweet::get_favorites()rtweet::get_friends()rtweet::get_timelines()rtweet::search_tweets()rvest::html_nodes()rvest::html_text()set.seed()spotifyr::get_artist_audio_features()sys.sleep()tesseract::ocr()usethis::edit_r_environ()","code":""},{"path":"gather-data.html","id":"introduction-6","chapter":"9 Gather data","heading":"9.1 Introduction","text":"chapter first\ngo variety approaches gathering data, including use APIs semi-structured data, JSON XML, web scraping, converting PDFs,\nusing optical character recognition, especially obtain text data.","code":""},{"path":"gather-data.html","id":"apis","chapter":"9 Gather data","heading":"9.2 APIs","text":"everyday language, purposes, Application Programming Interface (API) situation someone set specific files computer can follow instructions get . instance, use gif Slack, Slack asks Giphy’s server appropriate gif, Giphy’s server gives gif Slack Slack inserts chat. way Slack Giphy interact determined Giphy’s API. strictly, API just application runs server access using HTTP protocol.focus using APIs gathering data. , focus, API website set-another computer able access, rather person. instance, go Google Maps: https://www.google.com/maps. scroll click drag center map Canberra, Australia. paste browser: https://www.google.com/maps/@-35.2812958,149.1248113,16z. just used Google Maps API, result map similar Figure 9.1.\nFigure 9.1: Example Google Maps, 29 January 2022\nadvantage using API data provider specifies exactly data willing provide, terms provide . terms may include aspects rate limits (.e. often can ask data), can data, instance, might allowed use commercial purposes, republish . Additionally, API provided specifically us use , less likely subject unexpected changes legal issues. ethically legally clear API available try use rather web scraping.now go case studies using APIs. first deal directly API using httr (Wickham 2019c). second access data Twitter using rtweet (Kearney 2019). third access data Spotify using spotifyr (Thompson et al. 2020). Developing comfort gathering data APIs enables access exciting datasets. instance, Wong (2020) use Facebook Political Ad API gather 218,100 Trump 2020 campaign ads better understand campaign.","code":""},{"path":"gather-data.html","id":"case-study-gathering-data-from-arxiv-nasa-and-dataverse","chapter":"9 Gather data","heading":"9.2.1 Case study: Gathering data from arXiv, NASA, and Dataverse","text":"use GET() httr (Wickham 2019c) obtain data API directly. try get specific data main argument ‘url’. way, similar earlier Google Maps example. example, specific information interested map.case study use API provided arXiv: https://arxiv.org. arXiv online repository academic papers go peer-review, typically referred ‘pre-prints’. installing loading httr, use GET() ask arXiv obtain information pre-print R. Alexander Alexander (2021).can use status_code() check whether received error server. assuming received something back server, can use content() display information. case received XML formatted data, can read using read_xml() xml2 (Wickham, Hester, Ooms 2021). XML semi-formatted structure, can useful start look using html_structure().might interested create dataset based extracting various aspects XML tree. instance, might interested look ‘entry’, eighth item, particular obtain title URL, fourth ninth items, respectively, within entry.day NASA provides Astronomy Picture Day (APOD) APOD API. can use GET() obtain URL photo particular dates display .Examining returned data using content(), can see provided various fields, date, title, explanation, URL. can provide URL include_graphics() knitr display (Figure 9.2).\nFigure 9.2: Photo James Webb Space Telescope Earth another Tranquility Base obtained NASA APOD API\nFinally, another common API response semi-structured form JSON. can parse JSON jsonlite (Ooms 2014). Dataverse web application makes easier share dataset. can use API go query demonstration dataverse. instance might interested datasets related politics.can also look dataset using View(politics_datasets), allows us expand tree based interested even get code need focus different aspects hovering item clicking icon green arrow (Figure 9.3).\nFigure 9.3: Example hovering JSON element, ‘items’, icon green arrow can clicked get code focus element\ntells us obtain dataset interest.","code":"\nlibrary(httr)\nlibrary(tidyverse)\n#> ── Attaching packages ─────────────────── tidyverse 1.3.1 ──\n#> ✓ ggplot2 3.3.5     ✓ purrr   0.3.4\n#> ✓ tibble  3.1.6     ✓ dplyr   1.0.7\n#> ✓ tidyr   1.2.0     ✓ stringr 1.4.0\n#> ✓ readr   2.1.1     ✓ forcats 0.5.1\n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> x dplyr::filter() masks stats::filter()\n#> x dplyr::lag()    masks stats::lag()\nlibrary(xml2)\n\narxiv <-\n  GET(\"http://export.arxiv.org/api/query?id_list=2111.09299\")\n\nstatus_code(arxiv)\n#> [1] 200\ncontent(arxiv) |>\n  read_xml() |>\n  html_structure()\n#> <feed [xmlns]>\n#>   <link [href, rel, type]>\n#>   <title [type]>\n#>     {text}\n#>   <id>\n#>     {text}\n#>   <updated>\n#>     {text}\n#>   <totalResults [xmlns:opensearch]>\n#>     {text}\n#>   <startIndex [xmlns:opensearch]>\n#>     {text}\n#>   <itemsPerPage [xmlns:opensearch]>\n#>     {text}\n#>   <entry>\n#>     <id>\n#>       {text}\n#>     <updated>\n#>       {text}\n#>     <published>\n#>       {text}\n#>     <title>\n#>       {text}\n#>     <summary>\n#>       {text}\n#>     <author>\n#>       <name>\n#>         {text}\n#>     <author>\n#>       <name>\n#>         {text}\n#>     <comment [xmlns:arxiv]>\n#>       {text}\n#>     <link [href, rel, type]>\n#>     <link [title, href, rel, type]>\n#>     <primary_category [term, scheme, xmlns:arxiv]>\n#>     <category [term, scheme]>\ndata_from_arxiv <-\n  tibble(\n    title = content(arxiv) |>\n      read_xml() |>\n      xml_child(search = 8) |>\n      xml_child(search = 4) |>\n      xml_text(),\n    link = content(arxiv) |>\n      read_xml() |>\n      xml_child(search = 8) |>\n      xml_child(search = 9) |>\n      xml_attr(\"href\")\n  )\ndata_from_arxiv\n#> # A tibble: 1 × 2\n#>   title                                                link \n#>   <chr>                                                <chr>\n#> 1 \"The Increased Effect of Elections and Changing Pri… http…\nNASA_APOD_20211226 <-\n  GET(\"https://api.nasa.gov/planetary/apod?api_key=DEMO_KEY&date=2021-12-26\")\n\nNASA_APOD_20190719 <-\n  GET(\"https://api.nasa.gov/planetary/apod?api_key=DEMO_KEY&date=2019-07-19\")\ncontent(NASA_APOD_20211226)$date\n#> [1] \"2021-12-26\"\ncontent(NASA_APOD_20211226)$title\n#> [1] \"James Webb Space Telescope over Earth\"\ncontent(NASA_APOD_20211226)$explanation\n#> [1] \"There's a big new telescope in space. This one, the James Webb Space Telescope (JWST), not only has a mirror over five times larger than Hubble's in area, but can see better in infrared light. The featured picture shows JWST high above the Earth just after being released by the upper stage of an Ariane V rocket, launched yesterday from French Guiana. Over the next month, JWST will move out near the Sun-Earth L2 point where it will co-orbit the Sun with the Earth. During this time and for the next five months, JWST will unravel its segmented mirror and an array of sophisticated scientific instruments -- and test them. If all goes well, JWST will start examining galaxies across the universe and planets orbiting stars across our Milky Way Galaxy in the summer of 2022.   APOD Gallery: Webb Space Telescope Launch\"\ncontent(NASA_APOD_20211226)$url\n#> [1] \"https://apod.nasa.gov/apod/image/2112/JwstLaunch_Arianespace_1080.jpg\"\n\ncontent(NASA_APOD_20190719)$date\n#> [1] \"2019-07-19\"\ncontent(NASA_APOD_20190719)$title\n#> [1] \"Tranquility Base Panorama\"\ncontent(NASA_APOD_20190719)$explanation\n#> [1] \"On July 20, 1969 the Apollo 11 lunar module Eagle safely touched down on the Moon. It landed near the southwestern corner of the Moon's Mare Tranquillitatis at a landing site dubbed Tranquility Base. This panoramic view of Tranquility Base was constructed from the historic photos taken from the lunar surface. On the far left astronaut Neil Armstrong casts a long shadow with Sun is at his back and the Eagle resting about 60 meters away ( AS11-40-5961). He stands near the rim of 30 meter-diameter Little West crater seen here to the right ( AS11-40-5954). Also visible in the foreground is the top of the camera intended for taking stereo close-ups of the lunar surface.\"\ncontent(NASA_APOD_20190719)$url\n#> [1] \"https://apod.nasa.gov/apod/image/1907/apollo11TranquilitybasePan600h.jpg\"\nlibrary(jsonlite)\n\npolitics_datasets <- fromJSON(\"https://demo.dataverse.org/api/search?q=politics\")\nas_tibble(politics_datasets[[\"data\"]][[\"items\"]])\n#> # A tibble: 1 × 6\n#>   name       type  url   identifier description published_at\n#>   <chr>      <chr> <chr> <chr>      <chr>       <chr>       \n#> 1 China Arc… data… http… china-arc… Introducti… 2016-12-09T…"},{"path":"gather-data.html","id":"case-study-gathering-data-from-twitter","chapter":"9 Gather data","heading":"9.2.2 Case study: Gathering data from Twitter","text":"Twitter rich source text data. Twitter API way Twitter asks gather data. rtweet (Kearney 2019) built around API allows us interact ways similar using R package. Initially, can use Twitter API just regular Twitter account.Begin installing loading rtweet tidyverse. need authorize rtweet start process calling function package, instance get_favorites() return tibble user’s favorites. open browser, log regular Twitter account (Figure 9.4).\nFigure 9.4: rtweet authorisation page\napplication authorized, can use get_favorites() actually get favorites user save .look recent favorites, keeping mind may different depending accessed.can use search_tweets() search tweets particular topic. instance, look tweets using hashtag commonly associated R: ‘#rstats’.useful functions can used include get_friends() get accounts user follows, get_timelines() get user’s recent tweets. Registering developer enables access API functionality.using APIs, even wrapped R package, case rtweet, important read terms access provided. Twitter API docs surprisingly readable, developer policy especially clear: https://developer.twitter.com/en/developer-terms/policy. see easy violate terms API provider makes data available, consider saved tweets downloaded. push GitHub, possible may accidentally stored sensitive information happened tweets. Twitter also explicit asking use API especially careful sensitive information matching Twitter users -Twitter folks. , documentation around restricted uses clear readable: https://developer.twitter.com/en/developer-terms/--restricted-use-cases.","code":"\nlibrary(rtweet)\nlibrary(tidyverse)\nget_favorites(user = \"RohanAlexander\")\nrohans_favorites <- get_favorites(\"RohanAlexander\")\n\nsaveRDS(rohans_favorites, \"rohans_favorites.rds\")\nrohans_favorites |> \n  arrange(desc(created_at)) |> \n  slice(1:10) |> \n  select(screen_name, text)\n#> # A tibble: 10 × 2\n#>    screen_name text                                         \n#>    <chr>       <chr>                                        \n#>  1 EconAndrew  \"How much better are the investment opportun…\n#>  2 simonpcouch \"There's a new release of #rstats broom up o…\n#>  3 MineDogucu  \"🚨 New manuscript🚨\\n📕 Content and Computi…\n#>  4 reid_nancy  \"Latest issue. From the intro: \\\"... it has …\n#>  5 tjmahr      \"bathing is good, folks\"                     \n#>  6 andrewheiss \"finished hand washing that load in the bath…\n#>  7 monkmanmh   \"@CMastication https://t.co/3Eh0mLy44v\"      \n#>  8 eplusgg     \"Stares from Ontario https://t.co/swzYhaptF9\"\n#>  9 ryancbriggs \"Same. https://t.co/C9pNpXO0F9\"              \n#> 10 flynnpolsci \"I’m not great at coming up with assignments…\nrstats_tweets <- search_tweets(\n  q = \"#rstats\",\n  include_rts = FALSE\n)\n\nsaveRDS(rstats_tweets, \"rstats_tweets.rds\")\nrstats_tweets |> \n  select(screen_name, text) |> \n  head()\n#> # A tibble: 6 × 2\n#>   screen_name     text                                      \n#>   <chr>           <chr>                                     \n#> 1 SuccessAnalytiX \"The Science of Success \\n\\nhttps://t.co/…\n#> 2 babycoin_dev    \"BabyCoin (BABY)\\n\\nGUI wallet v2.05 =&gt…\n#> 3 rstatsdata      \"#rdata #rstats: Yield of 6 barley variet…\n#> 4 PDH_SciTechNews \"#Coding Arm Puts Security Architecture t…\n#> 5 PDH_SciTechNews \"#Coding Network Engineer: Skills, Roles …\n#> 6 PDH_SciTechNews \"#Coding CockroachDB Strengthens Change D…"},{"path":"gather-data.html","id":"case-study-gathering-data-from-spotify","chapter":"9 Gather data","heading":"9.2.3 Case study: Gathering data from Spotify","text":"final case study, use spotifyr (Thompson et al. 2020), wrapper around Spotify API. Install install.packages('spotifyr') load package.access Spotify API, need Spotify Developer Account: https://developer.spotify.com/dashboard/. require logging Spotify account accepting Developer Terms (Figure 9.5).\nFigure 9.5: Spotify Developer Account Terms agreement page\nContinuing registration process, case, ‘know’ building Spotify requires us use non-commercial agreement. use Spotify API need ‘Client ID’ ‘Client Secret’. things want keep anyone details use developer account though us. One way keep details secret minimum hassle keep ‘System Environment’. way, push GitHub included. (followed process without explanation Chapter 7 used mapdeck.) use usethis (Wickham Bryan 2020) modify System Environment. particular, file called ‘.Renviron’ open using edit_r_environ() add ‘Client ID’ ‘Client Secret’ .run edit_r_environ(), ‘.Renviron’ file open can add ‘Spotify Client ID’ ‘Client Secret’. important use names, spotifyr look environment keys specific names.Save ‘.Renviron’ file, restart R (‘Session’ -> ‘Restart R’). can now use ‘Spotify Client ID’ ‘Client Secret’ needed. functions require details arguments work without explicitly specified . get save information Radiohead, English rock band, using get_artist_audio_features(). One required arguments authorization, set, default, look ‘.Renviron’ file, need specify .variety information available based songs. might interested see whether songs getting longer time (Figure 9.6).\nFigure 9.6: Length Radiohead song, time, gathered Spotify\nOne interesting variable provided Spotify song ‘valence’. Spotify documentation describe measure 0 1 signals ‘musical positiveness’ track higher values positive. details available documentation: https://developer.spotify.com/documentation/web-api/reference/#/operations/get-audio-features. might interested compare valence time artists, instance, American rock band National, American singer Taylor Swift.First, need gather data.can bring together make graph (Figure 9.7). appears show Taylor Swift Radiohead largely maintained level valence overtime, National decreased .\nFigure 9.7: Comparing valence, time, Radiohead, Taylor Swift, National\namazing live world information available little effort cost. gathered data, lot done. instance, Pavlik (2019) uses expanded dataset classify musical genres Economist (2022) looks language associated music streaming Spotify. ability gather data enables us answer questions considered experimentally past, instance M. J. Salganik, Dodds, Watts (2006) use experimental data rather real data able access. time, worth thinking valence purporting represent. Little information available Spotify documentation created. doubtful one number can completely represent positive song .","code":"\nlibrary(spotifyr)\nlibrary(usethis)\n\nedit_r_environ()\nSPOTIFY_CLIENT_ID = 'PUT_YOUR_CLIENT_ID_HERE'\nSPOTIFY_CLIENT_SECRET = 'PUT_YOUR_SECRET_HERE'\nradiohead <- get_artist_audio_features('radiohead')\nsaveRDS(radiohead, \"radiohead.rds\")\nradiohead <- readRDS(\"radiohead.rds\")\nradiohead |> \n  select(artist_name, track_name, album_name) |> \n  head()\n#>   artist_name                    track_name   album_name\n#> 1   Radiohead Everything In Its Right Place KID A MNESIA\n#> 2   Radiohead                         Kid A KID A MNESIA\n#> 3   Radiohead           The National Anthem KID A MNESIA\n#> 4   Radiohead   How to Disappear Completely KID A MNESIA\n#> 5   Radiohead                   Treefingers KID A MNESIA\n#> 6   Radiohead                    Optimistic KID A MNESIA\nlibrary(lubridate)\n\nradiohead |> \n  mutate(album_release_date = ymd(album_release_date)) |> \n  ggplot(aes(x = album_release_date, y = duration_ms)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Album release date\",\n       y = \"Duration of song (ms)\"\n       ) \ntaylor_swift <- get_artist_audio_features('taylor swift')\nthe_national <- get_artist_audio_features('the national')\n\nsaveRDS(taylor_swift, \"taylor_swift.rds\")\nsaveRDS(the_national, \"the_national.rds\")\nthree_artists <-\n  rbind(taylor_swift, the_national, radiohead) |>\n  select(artist_name, album_release_date, valence) |>\n  mutate(album_release_date = ymd(album_release_date))\n\nthree_artists |>\n  ggplot(aes(x = album_release_date,\n             y = valence,\n             color = artist_name)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth() +\n  theme_minimal() +\n  labs(x = \"Album release date\",\n       y = \"Valence\",\n       color = \"Artist\") +\n  scale_color_brewer(palette = \"Set1\")"},{"path":"gather-data.html","id":"web-scraping","chapter":"9 Gather data","heading":"9.3 Web scraping","text":"Web scraping way get data websites. Rather going website using browser copy pasting, write code us. opens lot data us, hand, typically data made available purposes. means important respectful . generally illegal, specifics legality web scraping depend jurisdictions specifics , also important mindful . use rarely commercially competitive, particular concern conflict need work reproducible need respect terms service may disallow data republishing (Luscombe, Dick, Walby 2021). finally, web scraping imposes cost website host, important reduce extent possible.said, web scraping invaluable source data. typically datasets can created -product someone trying achieve another aim. instance, retailer may website products prices. created deliberately source data, can scrape create dataset. , following principles useful guide web scraping.Avoid . Try use API wherever possible.Abide desires. websites ‘robots.txt’ file contains information comfortable scrapers , instance ‘https://www.google.com/robots.txt’.Reduce impact.\nFirstly, slow scraper, instance, rather visit website every second, slow using sys.sleep(). need hundred files, just visit website times minute, running background overnight?\nSecondly, consider timing run scraper. instance, scraping retailer maybe set script run 10pm morning, fewer customers likely using site. Similarly, government website big monthly release, might polite avoid day.\nFirstly, slow scraper, instance, rather visit website every second, slow using sys.sleep(). need hundred files, just visit website times minute, running background overnight?Secondly, consider timing run scraper. instance, scraping retailer maybe set script run 10pm morning, fewer customers likely using site. Similarly, government website big monthly release, might polite avoid day.Take needed. instance, need scrape entire Wikipedia need names ten largest cities Croatia. reduces impact website, allows us easily justify actions.scrape . means save everything go re-collect data. Similarly, data, keep separate modify . course, need data time need go back, different needlessly re-scraping page.republish pages scraped. (contrasts datasets create .)Take ownership ask permission possible. minimum level scripts contact details . Depending circumstances, may worthwhile asking permission scrape.Web scraping possible taking advantage underlying structure webpage. use patterns HTML/CSS get data want. look underlying HTML/CSS can either:open browser, right-click, choose something like ‘Inspect’; orsave website open text editor rather browser.HTML/CSS markup language comprised matching tags. want text bold, use something like:Similarly, want list start end list, well item.scraping search tags.get started, can pretend obtained HTML website, want get name . can see name bold, want focus feature extract .use read_html() rvest (Wickham 2019d) read data.language used rvest look tags ‘node’, focus bold nodes. default html_nodes() returns tags well. can focus text contain, html_text().","code":"<b>My bold text<\/b><ul>\n  <li>Learn webscraping<\/li>\n  <li>Do data science<\/li>\n  <li>Proft<\/li>\n<\/ul>\nwebsite_extract <- \"<p>Hi, I’m <b>Rohan<\/b> Alexander.<\/p>\"\n# install.packages(\"rvest\")\nlibrary(rvest)\n\nrohans_data <- read_html(website_extract)\n\nrohans_data\n#> {html_document}\n#> <html>\n#> [1] <body><p>Hi, I’m <b>Rohan<\/b> Alexander.<\/p><\/body>\nrohans_data |> \n  html_nodes(\"b\")\n#> {xml_nodeset (1)}\n#> [1] <b>Rohan<\/b>\n\nfirst_name <- \n  rohans_data |> \n  html_nodes(\"b\") |>\n  html_text()\n\nfirst_name\n#> [1] \"Rohan\""},{"path":"gather-data.html","id":"case-study-web-scraping-book-information","chapter":"9 Gather data","heading":"9.3.1 Case study: Web scraping book information","text":"case study scrape list books : https://rohanalexander.com/bookshelf.html. clean data look distribution first letters author surnames. slightly complicated example , underlying approach : download website, look nodes interest, extract information, clean .use rvest (Wickham 2019d) download website, navigate html find aspects interested . use tidyverse clean dataset. first need go website save local copy.Now need navigate HTML get aspects want, put sensible structure. start trying get data tibble quickly possible allow us easily use dplyr verbs tidyverse functions.get data tibble first need identify data interested using html tags. look website need focus list items (Figure 9.8). can look source, focusing particularly looking list (Figure 9.9).\nFigure 9.8: Books website displayed\n\nFigure 9.9: HTML top books website list books\ntag list item ‘li’, can use focus list.now need clean data. First want separate title author using separate() clean author title columns.end need get rid ‘best ’.Finally, make table distribution first letter names (Table 9.1).Table 9.1: Distribution first letter author names collection books","code":"\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(xml2)\n\nbooks_data <- read_html(\"https://rohanalexander.com/bookshelf.html\")\n\nwrite_html(books_data, \"raw_data.html\") \nbooks_data <- read_html(\"inputs/my_website/raw_data.html\")\nbooks_data\n#> {html_document}\n#> <html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"\" xml:lang=\"\">\n#> [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text ...\n#> [2] <body>\\n\\n<!--radix_placeholder_front_matter-->\\n\\n<s ...\ntext_data <- \n  books_data |>\n  html_nodes(\"li\") |>\n  html_text()\n\nall_books <- \n  tibble(books = text_data)\n\nhead(all_books)\n#> # A tibble: 6 × 1\n#>   books                     \n#>   <chr>                     \n#> 1 Academic                  \n#> 2 Non-fiction               \n#> 3 Fiction                   \n#> 4 Cookbooks                 \n#> 5 Want to buy               \n#> 6 Best books that I read in:\nall_books <-\n  all_books |>\n  slice(7:nrow(all_books)) |>\n  separate(books, into = c(\"author\", \"title\"), sep = \", ‘\")\n\nall_books <-\n  all_books |>\n  separate(title, into = c(\"title\", \"debris\"), sep = \"’.\") |>\n  select(-debris) |>\n  mutate(author = str_remove_all(author, \"^, \"),\n         author = str_squish(author),\n         title = str_remove(title, \"“\"),\n         title = str_remove(title, \"^-\")\n         )\n\nhead(all_books)\n#> # A tibble: 6 × 2\n#>   author                               title                \n#>   <chr>                                <chr>                \n#> 1 Bryant, John, and Junni L. Zhang     Bayesian Demographic…\n#> 2 Chan, Ngai Hang                      Time Series          \n#> 3 Clark, Greg                          The Son Also Rises   \n#> 4 Duflo, Esther                        Expérience, science …\n#> 5 Foster, Ghani, Jarmin, Kreuter, Lane Big Data and Social …\n#> 6 Francois Chollet with JJ Allaire     Deep Learning with R\nall_books <- \n  all_books |> \n  slice(1:142) |>\n  filter(author != \"‘150 Years of Stats Canada!’.\")\nall_books |> \n  mutate(\n    first_letter = str_sub(author, 1, 1)\n    ) |> \n  group_by(first_letter) |> \n  count() |>\n  knitr::kable(\n    caption = \"Distribution of first letter of author names in a collection of books\",\n    col.names = c(\"First letter\", \"Number of times\"),\n    booktabs = TRUE, \n    linesep = \"\"\n    )"},{"path":"gather-data.html","id":"case-study-web-scraping-uk-prime-ministers-from-wikipedia","chapter":"9 Gather data","heading":"9.3.2 Case study: Web scraping UK Prime Ministers from Wikipedia","text":"case study interested long UK prime ministers lived, based year born. scrape data Wikipedia using rvest (Wickham 2019d), clean , make graph. Every time scrape website things change. scrape largely bespoke, even can borrow code earlier projects. completely normal feel frustrated times. helps begin end mind.end, can start generating simulated data. Ideally, want table row prime minister, column name, column birth death years. still alive, death year can empty. know birth death years somewhere 1700 1990, death year larger birth year. Finally, also know years integers, names characters. , want something looks roughly like :One advantages generating simulated dataset working groups one person can start making graph, using simulated dataset, person gathers data. terms graph, aiming something like Figure 9.10.\nFigure 9.10: Sketch planned graph showing long UK prime ministers lived\nstarting question interest, long UK prime minister lived. , need identify source data plenty data sources births deaths prime minister, want one can trust, going scraping, want one structure . Wikipedia page UK prime ministers fits criteria: https://en.wikipedia.org/wiki/List_of_prime_ministers_of_the_United_Kingdom. popular page information likely correct, data available table.load rvest download page using read_html(). Saving locally provides us copy need reproducibility case website changes, also means keep visiting website. likely property, typically something necessarily redistributed.earlier case study looking patterns HTML can use help us get closer data want. iterative process requires lot trial error. Even simple examples take time.One tool may help SelectorGadget: https://rvest.tidyverse.org/articles/articles/selectorgadget.html. allows us pick choose elements want, gives us input give html_nodes() (Figure 9.11)\nFigure 9.11: Using Selector Gadget identify tag, 13 March 2020.\ncase blank lines need filter away.Now parsed data, need clean match wanted. particular want names column, well columns birth year death year. use separate() take advantage fact looks like dates distinguished brackets.Finally, need clean columns.dataset looks similar one said wanted start (Table 9.2).Table 9.2: UK Prime Ministers, old diedAt point like make graph illustrates long prime minister lived. still alive like run end, like color differently.","code":"\nlibrary(babynames)\n\nset.seed(853)\n\nsimulated_dataset <-\n  tibble(\n    prime_minister = sample(\n      x = babynames |> filter(prop > 0.01) |>\n        select(name) |> unique() |> unlist(),\n      size = 10,\n      replace = FALSE\n    ),\n    birth_year = sample(\n      x = c(1700:1990),\n      size = 10,\n      replace = TRUE\n    ),\n    years_lived = sample(\n      x = c(50:100),\n      size = 10,\n      replace = TRUE\n    ),\n    death_year = birth_year + years_lived\n  ) |>\n  select(prime_minister, birth_year, death_year, years_lived) |>\n  arrange(birth_year)\nlibrary(rvest)\nlibrary(tidyverse)\nraw_data <-\n  read_html(\"https://en.wikipedia.org/wiki/List_of_prime_ministers_of_the_United_Kingdom\")\nwrite_html(raw_data, \"pms.html\")\n# Read in our saved data\nraw_data <- read_html(\"pms.html\")\n# We can parse tags in order\nparse_data_selector_gadget <- \n  raw_data |> \n  html_nodes(\"td:nth-child(3)\") |> \n  html_text()\n\nhead(parse_data_selector_gadget)\n#> [1] \"\\nSir Robert Walpole(1676–1745)\\n\"                       \n#> [2] \"\\nSpencer Compton1st Earl of Wilmington(1673–1743)\\n\"    \n#> [3] \"\\nHenry Pelham(1694–1754)\\n\"                             \n#> [4] \"\\nThomas Pelham-Holles1st Duke of Newcastle(1693–1768)\\n\"\n#> [5] \"\\nWilliam Cavendish4th Duke of Devonshire(1720–1764)\\n\"  \n#> [6] \"\\nThomas Pelham-Holles1st Duke of Newcastle(1693–1768)\\n\"\nparsed_data <-\n  tibble(raw_text = parse_data_selector_gadget) |>\n  filter(raw_text != \"—\\n\") |>\n  filter(\n    !raw_text %in% c(\n      \"\\n1868\\n\",\n      \"\\n1874\\n\",\n      \"\\n1880\\n\",\n      \"\\n1885\\n\",\n      \"\\n1892\\n\",\n      \"\\n1979\\n\",\n      \"\\n1997\\n\",\n      \"\\n2010\\n\"\n    )\n  ) |>\n  filter(\n    !raw_text %in% c(\n      \"\\nNational Labour\\n\",\n      \"\\nWilliam Pulteney1st Earl of Bath(1684–1764)\\n\",\n      \"\\nJames Waldegrave2nd Earl Waldegrave(1715–1763)\\n\",\n      \"\\nEdward VII\\n\\n\\n1901–1910\\n\\n\", \n      \"\\nGeorge V\\n\\n\\n1910–1936\\n\\n\"\n    )\n  )\n\nhead(parsed_data)\n#> # A tibble: 6 × 1\n#>   raw_text                                                  \n#>   <chr>                                                     \n#> 1 \"\\nSir Robert Walpole(1676–1745)\\n\"                       \n#> 2 \"\\nSpencer Compton1st Earl of Wilmington(1673–1743)\\n\"    \n#> 3 \"\\nHenry Pelham(1694–1754)\\n\"                             \n#> 4 \"\\nThomas Pelham-Holles1st Duke of Newcastle(1693–1768)\\n\"\n#> 5 \"\\nWilliam Cavendish4th Duke of Devonshire(1720–1764)\\n\"  \n#> 6 \"\\nThomas Pelham-Holles1st Duke of Newcastle(1693–1768)\\n\"\ninitial_clean <- \n  parsed_data |> \n  mutate(raw_text = str_remove_all(raw_text, \"\\n\")) |>\n  separate(raw_text, \n            into = c(\"Name\", \"not_name\"), \n            sep = \"\\\\(\",\n            remove = FALSE) |> # The remove = FALSE option here means that we \n  # keep the original column that we are separating.\n  separate(not_name, \n            into = c(\"Date\", \"all_the_rest\"), \n            sep = \"\\\\)\",\n            remove = FALSE)\n\nhead(initial_clean)\n#> # A tibble: 6 × 5\n#>   raw_text                 Name  not_name Date  all_the_rest\n#>   <chr>                    <chr> <chr>    <chr> <chr>       \n#> 1 Sir Robert Walpole(1676… Sir … 1676–17… 1676… \"\"          \n#> 2 Spencer Compton1st Earl… Spen… 1673–17… 1673… \"\"          \n#> 3 Henry Pelham(1694–1754)  Henr… 1694–17… 1694… \"\"          \n#> 4 Thomas Pelham-Holles1st… Thom… 1693–17… 1693… \"\"          \n#> 5 William Cavendish4th Du… Will… 1720–17… 1720… \"\"          \n#> 6 Thomas Pelham-Holles1st… Thom… 1693–17… 1693… \"\"\ninitial_clean <- \n initial_clean |> \n  separate(col = Name, \n           into = c(\"Name\", \"Title\"),\n           sep = \"[[:digit:]]\",\n           extra = \"merge\",\n           fill = \"right\") |>\n  separate(col = Name, \n           into = c(\"Name\", \"Title\"),\n           sep = \"MP for\",\n           extra = \"merge\",\n           fill = \"right\") |>\n  mutate(Name = str_remove(Name, \"\\\\[b\\\\]\"))\n\nhead(initial_clean)\n#> # A tibble: 6 × 6\n#>   raw_text           Name  Title not_name Date  all_the_rest\n#>   <chr>              <chr> <chr> <chr>    <chr> <chr>       \n#> 1 Sir Robert Walpol… Sir … <NA>  1676–17… 1676… \"\"          \n#> 2 Spencer Compton1s… Spen… <NA>  1673–17… 1673… \"\"          \n#> 3 Henry Pelham(1694… Henr… <NA>  1694–17… 1694… \"\"          \n#> 4 Thomas Pelham-Hol… Thom… <NA>  1693–17… 1693… \"\"          \n#> 5 William Cavendish… Will… <NA>  1720–17… 1720… \"\"          \n#> 6 Thomas Pelham-Hol… Thom… <NA>  1693–17… 1693… \"\"\ncleaned_data <- \n  initial_clean |> \n  select(Name, Date) |> \n  separate(Date, into = c(\"Birth\", \"Died\"), sep = \"–\", remove = FALSE) |> # The \n  # PMs who have died have their birth and death years separated by a hyphen, but \n  # we need to be careful with the hyphen as it seems to be a slightly odd type of \n  # hyphen and we need to copy/paste it.\n  mutate(Birth = str_remove_all(Birth, \"born\"),\n         Birth = str_trim(Birth)\n         ) |> # Alive PMs have slightly different format\n  select(-Date) |> \n  mutate(Name = str_remove(Name, \"\\n\")) |> # Remove some html tags that remain\n  mutate_at(vars(Birth, Died), ~as.integer(.)) |> # Change birth and death to integers\n  mutate(Age_at_Death = Died - Birth) |>  # Add column of the number of years they lived\n  distinct() # Some of the PMs had two goes at it.\n\nhead(cleaned_data)\n#> # A tibble: 6 × 4\n#>   Name                 Birth  Died Age_at_Death\n#>   <chr>                <int> <int>        <int>\n#> 1 Sir Robert Walpole    1676  1745           69\n#> 2 Spencer Compton       1673  1743           70\n#> 3 Henry Pelham          1694  1754           60\n#> 4 Thomas Pelham-Holles  1693  1768           75\n#> 5 William Cavendish     1720  1764           44\n#> 6 John Stuart           1713  1792           79\ncleaned_data |> \n  knitr::kable(\n    caption = \"UK Prime Ministers, by how old they were when they died\",\n    col.names = c(\"Prime Minister\", \"Birth year\", \"Death year\", \"Age at death\"),\n    booktabs = TRUE, \n    linesep = \"\"\n    )\ncleaned_data |> \n  mutate(still_alive = if_else(is.na(Died), \"Yes\", \"No\"),\n         Died = if_else(is.na(Died), as.integer(2022), Died)) |> \n  mutate(Name = as_factor(Name)) |> \n  ggplot(aes(x = Birth, \n             xend = Died,\n             y = Name,\n             yend = Name, \n             color = still_alive)) +\n  geom_segment() +\n  labs(x = \"Year of birth\",\n       y = \"Prime minister\",\n       color = \"PM is alive\",\n       title = \"How long each UK Prime Minister lived, by year of birth\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")"},{"path":"gather-data.html","id":"case-study-downloading-multiple-files","chapter":"9 Gather data","heading":"9.3.3 Case study: Downloading multiple files","text":"Considering text data exciting opens lot different research questions. Many guides assume already nicely formatted text dataset, rarely actually case. case study download files different pages. already seen two examples web scraping, focused just one page, whereas often need many. focus iteration. use download.file() download, purrr (Henry Wickham 2020) apply function across multiple sites.Reserve Bank Australia (RBA) Australia’s central bank sets monetary policy. responsibility setting cash rate, interest rate used loans banks. interest rate especially important one, large impact interest rates economy. Four times year – February, May, August, November – RBA publishes statement monetary policy, available PDFs. example download four statements published 2021.First set-dataframe information need.can apply function download.files() fourThen can write function download file, let us know downloaded, wait polite amount time, go get next file.now apply function list URLs.result downloaded four PDFs saved computer. next section build discuss getting information PDFs.","code":"\nlibrary(tidyverse)\n\nstatements_of_interest <- \n  tibble(\n    address = c(\"https://www.rba.gov.au/publications/smp/2021/nov/pdf/00-overview.pdf\",\n                \"https://www.rba.gov.au/publications/smp/2021/aug/pdf/00-overview.pdf\",\n                \"https://www.rba.gov.au/publications/smp/2021/may/pdf/00-overview.pdf\",\n                \"https://www.rba.gov.au/publications/smp/2021/feb/pdf/00-overview.pdf\"\n                ),\n    local_save_name = c(\n      \"2021-11.pdf\",\n      \"2021-08.pdf\",\n      \"2021-05.pdf\",\n      \"2021-02.pdf\"\n    )\n  )\n\nstatements_of_interest\n#> # A tibble: 4 × 2\n#>   address                                    local_save_name\n#>   <chr>                                      <chr>          \n#> 1 https://www.rba.gov.au/publications/smp/2… 2021-11.pdf    \n#> 2 https://www.rba.gov.au/publications/smp/2… 2021-08.pdf    \n#> 3 https://www.rba.gov.au/publications/smp/2… 2021-05.pdf    \n#> 4 https://www.rba.gov.au/publications/smp/2… 2021-02.pdf\nvisit_download_and_wait <-\n  function(the_address_to_visit, where_to_save_it_locally) {\n    \n    download.file(url = the_address_to_visit,\n                  destfile = where_to_save_it_locally\n                  )\n\n    print(paste(\"Done with\", the_address_to_visit, \"at\", Sys.time()))\n    \n    Sys.sleep(sample(5:10, 1))\n  }\nwalk2(statements_of_interest$address,\n      statements_of_interest$local_save_name,\n      ~visit_download_and_wait(.x, .y))"},{"path":"gather-data.html","id":"pdfs","chapter":"9 Gather data","heading":"9.4 PDFs","text":"contrast API, PDF usually produced human rather computer consumption. nice thing PDFs static constant. nice make data available . trade-:overly useful larger-scale statistical analysis.know PDF put together know whether can trust .manipulate data get results interested .Indeed, sometimes governments publish data PDFs actually want us able analyze . able get data PDFs opens large number datasets.two important aspects keep mind approaching PDF mind extracting data :Begin end mind. Planning literally sketching want final dataset/graph/paper stops us wasting time keeps us focused.Start simple, iterate. quickest way make complicated model often first build simple model complicate . Start just trying get one page PDF working even just one line. iterate .start walking several examples go case study gather data US Total Fertility Rate, state.Figure 9.12 PDF consists just first sentence Jane Eyre taken Project Gutenberg (Bronte 1847).\nFigure 9.12: First sentence Jane Eyre\nassume saved ‘first_example.pdf’, can pdftools (Ooms 2019a) get text one-page PDF R.can see PDF correctly read , character vector.now try slightly complicated example consists first paragraphs Jane Eyre (Figure 9.13). Also notice now chapter heading well.\nFigure 9.13: First paragraphs Jane Eyre\nuse function ., character vector. end line signaled ‘\\n’, looks pretty good. Finally, consider first two pages.Notice first page first element character vector, second page second element. familiar rectangular data, try get format quickly possible. can use regular tidyverse functions deal .First want convert character vector tibble. point may like add page numbers well.want separate lines line observation. can looking ‘\\n’ remembering need escape backslash special character.","code":"\nlibrary(pdftools)\nlibrary(tidyverse)\n\nfirst_example <- pdf_text(\"first_example.pdf\")\n\nfirst_example\n\nclass(first_example)\nsecond_example <- pdftools::pdf_text(\"second_example.pdf\")\n\nsecond_example\n\nclass(second_example)\nthird_example <- pdftools::pdf_text(\"third_example.pdf\")\n\nthird_example\n\nclass(third_example)\njane_eyre <- tibble(raw_text = third_example,\n                    page_number = c(1:2))\njane_eyre <- \n  separate_rows(jane_eyre, raw_text, sep = \"\\\\n\", convert = FALSE)\njane_eyre\n#> # A tibble: 93 × 2\n#>    raw_text                                      page_number\n#>    <chr>                                               <int>\n#>  1 \"CHAPTER I\"                                             1\n#>  2 \"There was no possibility of taking a walk t…           1\n#>  3 \"leafless shrubbery an hour in the morning; …           1\n#>  4 \"company, dined early) the cold winter wind …           1\n#>  5 \"penetrating, that further out-door exercise…           1\n#>  6 \"\"                                                      1\n#>  7 \"I was glad of it: I never liked long walks,…           1\n#>  8 \"coming home in the raw twilight, with nippe…           1\n#>  9 \"chidings of Bessie, the nurse, and humbled …           1\n#> 10 \"Eliza, John, and Georgiana Reed.\"                      1\n#> # … with 83 more rows"},{"path":"gather-data.html","id":"case-study-gathering-data-on-the-us-total-fertility-rate","chapter":"9 Gather data","heading":"9.4.1 Case-study: Gathering data on the US Total Fertility Rate","text":"US Department Health Human Services Vital Statistics Report provides information total fertility rate (average number births per woman women experience current age-specific fertility rates throughout reproductive years) state nineteen years. US persists making data available PDFs, hinders research. can use approaches get data nice dataset.instance, case year 2000 table interested page 40 PDF available https://www.cdc.gov/nchs/data/nvsr/nvsr50/nvsr50_05.pdf. column interest labelled: “Total fertility rate” (Figure 9.14).\nFigure 9.14: Example Vital Statistics Report, 2000\nfirst step getting data PDF sketch eventually want. PDF typically contains lot information, handy clear need. helps keep focused, prevents scope creep, also helpful thinking data checks. literally write paper mind. case, needed table column state, year TFR (Figure 9.15).\nFigure 9.15: Planned dataset TFR year US state\n19 different PDFs, interested particular column particular table . Unfortunately, nothing magical coming. first step requires working link , page column name interest. end, looks like .first step get code works one . ’ll step code lot detail normal ’re going use pieces lot.choose year 2000. first download save PDF using download.file().read PDF character vector using pdf_text() pdftools. convert tibble, can use familiar verbs .Grab page interest (remembering page element character vector, hence row tibble).Now want separate rows.Now searching patterns can use. Let us look first ten lines content.get much better :dots separating states data.space columns.can now separate separate columns. First want match least two dots (remembering dot special character needs escaped).get expected warnings top bottom multiple dots. (Another option use pdf_data() allow us use location rather delimiters.)can now separate data based spaces. inconsistent number spaces, first squish example one space just one.looking fairly great. thing left clean .’re done year. Now want take pieces, put function run function 19 years.first part downloading 19 PDFs need. going build code used . code :modify need:iterate lines dataset contains CSVs (.e. says 1, want 1, 2, 3, etc.).filename, need iterate desired filenames (.e. year_2000, year_2001, year_2002, etc).like way little robust errors. instance, one URLs wrong internet drops like just move onto next PDF, warn us end missed one, stop. (really matter 19 files, easy find oneself thousands files).draw purrr (Henry Wickham 2020).take download.file() pass two arguments: .x .y. walk2() applies function inputs give , case URLs columns .x pdf_names column .y. Finally, safely() means failures just moves onto next file instead throwing error.now PDFs saved can move onto getting data .Now need get data PDFs. , going build code used . code (overly condensed) :first thing want iterate argument pdf_text(), number slice() also need change (work get page interested ).Two aspects hardcoded, may need updated. particular:separate works tables columns order; andthe slice (restricts data just states) works case.Finally, add year end, whereas need bring earlier process.start writing function go files, grab data, get page interest, expand rows. use pmap_dfr() purrr apply function PDFs output tibble.Now need clean state names filter .next step separate data get correct column . going separate based spaces cleaned .can now grab correct column.Finally, need convert case.run checks.particular want 51 states 19 years.done (Table 9.3)!Table 9.3: First ten rows dataset TFR US state, 2000-2019","code":"\nsummary_tfr_dataset |> \n  select(year, page, table, column_name, url) |> \n  gt()\ndownload.file(url = summary_tfr_dataset$url[1], \n              destfile = \"year_2000.pdf\")\n# INTERNAL\ndownload.file(url = summary_tfr_dataset$url[1], \n              destfile = \"inputs/pdfs/dhs/year_2000.pdf\")\nlibrary(pdftools)\ndhs_2000 <- pdf_text(\"year_2000.pdf\")\ndhs_2000 <- tibble(raw_data = dhs_2000)\n\nhead(dhs_2000)\n#> # A tibble: 6 × 1\n#>   raw_data                                                  \n#>   <chr>                                                     \n#> 1 \"Volume 50, Number 5                                     …\n#> 2 \"2   National Vital Statistics Report, Vol. 50, No. 5, Fe…\n#> 3 \"                                                        …\n#> 4 \"4   National Vital Statistics Report, Vol. 50, No. 5, Fe…\n#> 5 \"                                                        …\n#> 6 \"6   National Vital Statistics Report, Vol. 50, No. 5, Fe…\ndhs_2000 <- \n  dhs_2000 |> \n  slice(summary_tfr_dataset$page[1])\n\nhead(dhs_2000)\n#> # A tibble: 1 × 1\n#>   raw_data                                                  \n#>   <chr>                                                     \n#> 1 \"40 National Vital Statistics Report, Vol. 50, No. 5, Rev…\ndhs_2000 <- \n  dhs_2000 |> \n  separate_rows(raw_data, sep = \"\\\\n\", convert = FALSE)\n\nhead(dhs_2000)\n#> # A tibble: 6 × 1\n#>   raw_data                                                  \n#>   <chr>                                                     \n#> 1 \"40 National Vital Statistics Report, Vol. 50, No. 5, Rev…\n#> 2 \"\"                                                        \n#> 3 \"Table 10. Number of births, birth rates, fertility rates…\n#> 4 \"United States, each State and territory, 2000\"           \n#> 5 \"[By place of residence. Birth rates are live births per …\n#> 6 \"estimated in each area; total fertility rates are sums o…\ndhs_2000[13:22,]\n#> # A tibble: 10 × 1\n#>    raw_data                                                 \n#>    <chr>                                                    \n#>  1 \"                                  State                …\n#>  2 \"                                                       …\n#>  3 \"                                                       …\n#>  4 \"\"                                                       \n#>  5 \"\"                                                       \n#>  6 \"United States 1 .......................................…\n#>  7 \"\"                                                       \n#>  8 \"Alabama ...............................................…\n#>  9 \"Alaska ................................................…\n#> 10 \"Arizona ...............................................…\ndhs_2000 <- \n  dhs_2000 |> \n  separate(col = raw_data, \n           into = c(\"state\", \"data\"), \n           sep = \"\\\\.{2,}\", \n           remove = FALSE,\n           fill = \"right\"\n           )\n\nhead(dhs_2000)\n#> # A tibble: 6 × 3\n#>   raw_data                                       state data \n#>   <chr>                                          <chr> <chr>\n#> 1 \"40 National Vital Statistics Report, Vol. 50… \"40 … <NA> \n#> 2 \"\"                                             \"\"    <NA> \n#> 3 \"Table 10. Number of births, birth rates, fer… \"Tab… <NA> \n#> 4 \"United States, each State and territory, 200… \"Uni… <NA> \n#> 5 \"[By place of residence. Birth rates are live… \"[By… <NA> \n#> 6 \"estimated in each area; total fertility rate… \"est… <NA>\ndhs_2000 <- \n  dhs_2000 |>\n  mutate(data = str_squish(data)) |> \n  tidyr::separate(col = data, \n           into = c(\"number_of_births\", \n                    \"birth_rate\", \n                    \"fertility_rate\", \n                    \"TFR\", \n                    \"teen_births_all\", \n                    \"teen_births_15_17\", \n                    \"teen_births_18_19\"), \n           sep = \"\\\\s\", \n           remove = FALSE\n           )\n\nhead(dhs_2000)\n#> # A tibble: 6 × 10\n#>   raw_data           state data  number_of_births birth_rate\n#>   <chr>              <chr> <chr> <chr>            <chr>     \n#> 1 \"40 National Vita… \"40 … <NA>  <NA>             <NA>      \n#> 2 \"\"                 \"\"    <NA>  <NA>             <NA>      \n#> 3 \"Table 10. Number… \"Tab… <NA>  <NA>             <NA>      \n#> 4 \"United States, e… \"Uni… <NA>  <NA>             <NA>      \n#> 5 \"[By place of res… \"[By… <NA>  <NA>             <NA>      \n#> 6 \"estimated in eac… \"est… <NA>  <NA>             <NA>      \n#> # … with 5 more variables: fertility_rate <chr>, TFR <chr>,\n#> #   teen_births_all <chr>, teen_births_15_17 <chr>,\n#> #   teen_births_18_19 <chr>\ndhs_2000 <- \n  dhs_2000 |> \n  select(state, TFR) |> \n  slice(13:69) |> \n  mutate(year = 2000)\n\ndhs_2000\n#> # A tibble: 57 × 3\n#>    state                                         TFR    year\n#>    <chr>                                         <chr> <dbl>\n#>  1 \"                                  State    … <NA>   2000\n#>  2 \"                                           … <NA>   2000\n#>  3 \"                                           … <NA>   2000\n#>  4 \"\"                                            <NA>   2000\n#>  5 \"\"                                            <NA>   2000\n#>  6 \"United States 1 \"                            2,13…  2000\n#>  7 \"\"                                            <NA>   2000\n#>  8 \"Alabama \"                                    2,02…  2000\n#>  9 \"Alaska \"                                     2,43…  2000\n#> 10 \"Arizona \"                                    2,65…  2000\n#> # … with 47 more rows\ndownload.file(url = summary_tfr_dataset$url[1], destfile = \"year_2000.pdf\")\nlibrary(purrr)\n\nsummary_tfr_dataset <- \n  summary_tfr_dataset |> \n  mutate(pdf_name = paste0(\"dhs/year_\", year, \".pdf\"))\nwalk2(\n  summary_tfr_dataset$url,\n  summary_tfr_dataset$pdf_name,\n  safely( ~ download.file(.x , .y))\n)\ndhs_2000 <- pdftools::pdf_text(\"year_2000.pdf\")\n\ndhs_2000 <-\n  tibble(raw_data = dhs_2000) |>\n  slice(summary_tfr_dataset$page[1]) |>\n  separate_rows(raw_data, sep = \"\\\\n\", convert = FALSE) |>\n  separate(\n    col = raw_data,\n    into = c(\"state\", \"data\"),\n    sep = \"\\\\.{2,}\",\n    remove = FALSE\n  ) |>\n  mutate(data = str_squish(data)) |>\n  separate(\n    col = data,\n    into = c(\n      \"number_of_births\",\n      \"birth_rate\",\n      \"fertility_rate\",\n      \"TFR\",\n      \"teen_births_all\",\n      \"teen_births_15_17\",\n      \"teen_births_18_19\"\n    ),\n    sep = \"\\\\s\",\n    remove = FALSE\n  ) |>\n  select(state, TFR) |>\n  slice(13:69) |>\n  mutate(year = 2000)\n\ndhs_2000\nget_pdf_convert_to_tibble <- function(pdf_name, page, year){\n  \n  dhs_table_of_interest <- \n    tibble(raw_data = pdftools::pdf_text(pdf_name)) |> \n    slice(page) |> \n    separate_rows(raw_data, sep = \"\\\\n\", convert = FALSE) |> \n    separate(col = raw_data, \n             into = c(\"state\", \"data\"), \n             sep = \"[�|\\\\.]\\\\s+(?=[[:digit:]])\", \n             remove = FALSE) |> \n    mutate(\n      data = str_squish(data),\n      year_of_data = year)\n\n  print(paste(\"Done with\", year))\n  \n  return(dhs_table_of_interest)\n}\n\nraw_dhs_data <- purrr::pmap_dfr(summary_tfr_dataset |> select(pdf_name, page, year),\n                                get_pdf_convert_to_tibble)\n#> [1] \"Done with 2000\"\n#> [1] \"Done with 2001\"\n#> [1] \"Done with 2002\"\n#> [1] \"Done with 2003\"\n#> [1] \"Done with 2004\"\n#> [1] \"Done with 2005\"\n#> [1] \"Done with 2006\"\n#> [1] \"Done with 2007\"\n#> [1] \"Done with 2008\"\n#> [1] \"Done with 2009\"\n#> [1] \"Done with 2010\"\n#> [1] \"Done with 2011\"\n#> [1] \"Done with 2012\"\n#> [1] \"Done with 2013\"\n#> [1] \"Done with 2014\"\n#> [1] \"Done with 2015\"\n#> [1] \"Done with 2016\"\n#> [1] \"Done with 2016\"\n#> [1] \"Done with 2017\"\n#> [1] \"Done with 2017\"\n#> [1] \"Done with 2018\"\n\nhead(raw_dhs_data)\n#> # A tibble: 6 × 4\n#>   raw_data                          state data  year_of_data\n#>   <chr>                             <chr> <chr>        <dbl>\n#> 1 \"40 National Vital Statistics Re… \"40 … 50, …         2000\n#> 2 \"\"                                \"\"    <NA>          2000\n#> 3 \"Table 10. Number of births, bir… \"Tab… <NA>          2000\n#> 4 \"United States, each State and t… \"Uni… <NA>          2000\n#> 5 \"[By place of residence. Birth r… \"[By… <NA>          2000\n#> 6 \"estimated in each area; total f… \"est… <NA>          2000\nstates <- c(\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \n            \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \n            \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \n            \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \n            \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \n            \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \n            \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \n            \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \n            \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \n            \"Wyoming\", \"District of Columbia\")\n\nraw_dhs_data <- \n  raw_dhs_data |> \n  mutate(state = str_remove_all(state, \"\\\\.\"),\n         state = str_remove_all(state, \"�\"),\n         state = str_remove_all(state, \"\\u0008\"),\n         state = str_replace_all(state, \"United States 1\", \"United States\"),\n         state = str_replace_all(state, \"United States1\", \"United States\"),\n         state = str_replace_all(state, \"United States 2\", \"United States\"),\n         state = str_replace_all(state, \"United States2\", \"United States\"),\n         state = str_replace_all(state, \"United States²\", \"United States\"),\n         ) |> \n  mutate(state = str_squish(state)) |> \n  filter(state %in% states)\n\nhead(raw_dhs_data)\n#> # A tibble: 6 × 4\n#>   raw_data                          state data  year_of_data\n#>   <chr>                             <chr> <chr>        <dbl>\n#> 1 Alabama ........................… Alab… 63,2…         2000\n#> 2 Alaska .........................… Alas… 9,97…         2000\n#> 3 Arizona ........................… Ariz… 85,2…         2000\n#> 4 Arkansas .......................… Arka… 37,7…         2000\n#> 5 California .....................… Cali… 531,…         2000\n#> 6 Colorado .......................… Colo… 65,4…         2000\nraw_dhs_data <- \n  raw_dhs_data |> \n  mutate(data = str_remove_all(data, \"\\\\*\")) |> \n  separate(data, into = c(\"col_1\", \"col_2\", \"col_3\", \"col_4\", \"col_5\", \n                          \"col_6\", \"col_7\", \"col_8\", \"col_9\", \"col_10\"), \n           sep = \" \",\n           remove = FALSE)\nhead(raw_dhs_data)\n#> # A tibble: 6 × 14\n#>   raw_data   state data  col_1 col_2 col_3 col_4 col_5 col_6\n#>   <chr>      <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n#> 1 Alabama .… Alab… 63,2… 63,2… 14.4  65.0  2,02… 62.9  37.9 \n#> 2 Alaska ..… Alas… 9,97… 9,974 16.0  74.6  2,43… 42.4  23.6 \n#> 3 Arizona .… Ariz… 85,2… 85,2… 17.5  84.4  2,65… 69.1  41.1 \n#> 4 Arkansas … Arka… 37,7… 37,7… 14.7  69.1  2,14… 68.5  36.7 \n#> 5 Californi… Cali… 531,… 531,… 15.8  70.7  2,18… 48.5  28.6 \n#> 6 Colorado … Colo… 65,4… 65,4… 15.8  73.1  2,35… 49.2  28.6 \n#> # … with 5 more variables: col_7 <chr>, col_8 <chr>,\n#> #   col_9 <chr>, col_10 <chr>, year_of_data <dbl>\ntfr_data <- \n  raw_dhs_data |> \n  mutate(TFR = if_else(year_of_data < 2008, col_4, col_3)) |> \n  select(state, year_of_data, TFR) |> \n  rename(year = year_of_data)\nhead(tfr_data)\n#> # A tibble: 6 × 3\n#>   state       year TFR    \n#>   <chr>      <dbl> <chr>  \n#> 1 Alabama     2000 2,021.0\n#> 2 Alaska      2000 2,437.0\n#> 3 Arizona     2000 2,652.5\n#> 4 Arkansas    2000 2,140.0\n#> 5 California  2000 2,186.0\n#> 6 Colorado    2000 2,356.5\nhead(tfr_data)\n#> # A tibble: 6 × 3\n#>   state       year TFR    \n#>   <chr>      <dbl> <chr>  \n#> 1 Alabama     2000 2,021.0\n#> 2 Alaska      2000 2,437.0\n#> 3 Arizona     2000 2,652.5\n#> 4 Arkansas    2000 2,140.0\n#> 5 California  2000 2,186.0\n#> 6 Colorado    2000 2,356.5\n\ntfr_data <- \n  tfr_data |> \n  mutate(TFR = str_remove_all(TFR, \",\"),\n         TFR = as.numeric(TFR))\n\nhead(tfr_data)\n#> # A tibble: 6 × 3\n#>   state       year   TFR\n#>   <chr>      <dbl> <dbl>\n#> 1 Alabama     2000 2021 \n#> 2 Alaska      2000 2437 \n#> 3 Arizona     2000 2652.\n#> 4 Arkansas    2000 2140 \n#> 5 California  2000 2186 \n#> 6 Colorado    2000 2356.\ntfr_data$state %>% unique() %>% length() == 51\n#> [1] TRUE\n\ntfr_data$year %>% unique() %>% length() == 19\n#> [1] TRUE\ntfr_data |>\n  slice(1:10) |>\n  knitr:: kable(\n    caption = \"First ten rows of a dataset of TFR by US state, 2000-2019\",\n    col.names = c(\"State\", \"Year\", \"TFR\"),\n    digits = 0,\n    booktabs = TRUE, \n    linesep = \"\",\n    format.args = list(big.mark = \",\")\n  )"},{"path":"gather-data.html","id":"optical-character-recognition","chapter":"9 Gather data","heading":"9.4.2 Optical Character Recognition","text":"predicated PDF already ‘digitized’. images? case need first use Optical Character Recognition (OCR) using tesseract (Ooms 2019c). R wrapper around Tesseract open-source OCR engine.Let us see example scan first page Jane Eyre (Figure 9.16).\nFigure 9.16: Scan first page Jane Eyre\n","code":"\nlibrary(tesseract)\n\ntext <- tesseract::ocr(here::here(\"jane_scan.png\"), engine = tesseract(\"eng\"))\ncat(text)"},{"path":"gather-data.html","id":"exercises-and-tutorial-8","chapter":"9 Gather data","heading":"9.5 Exercises and tutorial","text":"","code":""},{"path":"gather-data.html","id":"exercises-8","chapter":"9 Gather data","heading":"9.5.1 Exercises","text":"types probability sampling, circumstances might want implement (write two three pages)?substantial political polling ‘misses’ recent years (Trump Brexit come mind). extent think non-response bias cause (write page two, sure ground writing citations)?seems like lot businesses closed since pandemic. investigate , walk along blocks downtown count number businesses closed open. decide blocks walk, open map, start lake, pick every 10th street. type sampling (pick one)?\nCluster sampling.\nSystematic sampling.\nStratified sampling.\nConvenience sampling.\nCluster sampling.Systematic sampling.Stratified sampling.Convenience sampling.Please name reasons may wish use cluster sampling (select )?\nBalance responses.\nAdministrative convenience.\nEfficiency terms money.\nUnderlying systematic concerns.\nEstimation sub-populations.\nBalance responses.Administrative convenience.Efficiency terms money.Underlying systematic concerns.Estimation sub-populations.Please consider Beaumont, 2020, ‘probability surveys bound disappear production official statistics?’. reference paper, think probability surveys disappear, (please write paragraph two)?words, API (write paragraph two)?Find two APIs discuss use tell interesting stories (write paragraph two )?Find two APIs R packages written around . use tell interesting stories (write paragraph two)?main argument httr::GET() (pick one)?\n‘url’\n‘website’\n‘domain’\n‘location’\n‘url’‘website’‘domain’‘location’three reasons respectful getting scraping data websites (write paragraph two)?features website typically take advantage parse code (pick )?\nHTML/CSS mark-.\nCookies.\nFacebook beacons.\nCode comments.\nHTML/CSS mark-.Cookies.Facebook beacons.Code comments.three advantages three disadvantages scraping compared using API (write paragraph two)?three delimiters useful trying bring order PDF read character vector (write paragraph two)?following, used part regular expression, match full stop (hint: see ‘strings’ cheat sheet) (pick one)?\n‘.’\n‘.’\n‘\\.’\n‘\\.’\n‘.’‘.’‘\\.’‘\\.’Name three reasons sketching want starting try extract data PDF (write paragraph two )?three checks might like use demographic data, number births country particular year (write paragraph two check)?three checks might like use economic data, GDP particular country particular year (write paragraph two check)?purrr package (select apply)?\nEnhances R’s functional programming toolkit.\nMakes loops easier code read.\nChecks consistency datasets.\nIdentifies issues data structures proposes replacements.\nEnhances R’s functional programming toolkit.Makes loops easier code read.Checks consistency datasets.Identifies issues data structures proposes replacements.functions purrr package (select apply)?\nmap()\nwalk()\nrun()\nsafely()\nmap()walk()run()safely()principles follow scraping (select apply)?\nAvoid possible\nFollow site’s guidance\nSlow \nUse scalpel axe.\nAvoid possibleFollow site’s guidanceSlow downUse scalpel axe.robots.txt file (pick one)?\ninstructions Frankenstein followed.\nNotes web scrapers follow scraping.\ninstructions Frankenstein followed.Notes web scrapers follow scraping.html tag item list (pick one)?\nli\nbody\nb\nem\nlibodybemWhich function use following text data: ‘rohan_alexander’ column called ‘names’ want split first name surname based underbar (pick one)?\nseparate()\nslice()\nspacing()\ntext_to_columns()\nseparate()slice()spacing()text_to_columns()","code":""},{"path":"gather-data.html","id":"tutorial-8","chapter":"9 Gather data","heading":"9.5.2 Tutorial","text":"Please redo web scraping example, one : Australia, Canada, India, New Zealand.Plan, gather, clean data, use create similar table one created . Write paragraphs findings. write paragraphs data source, gathered, went . took longer expected? become fun? differently next time ? submission least two pages likely .Please submit link PDF produced using R Markdown includes link GitHub repo.","code":""},{"path":"hunt-data.html","id":"hunt-data","chapter":"10 Hunt data","heading":"10 Hunt data","text":"Required materialRead Big tech testing , (Fry 2020).Read Inventing randomized double-blind trial: Nuremberg salt test 1835, (Stolberg 2006).Read Impact evaluation practice, Chapters 3 4, (Gertler et al. 2016).Read Statistics causal inference, Parts 1-3, (Holland 1986).Key concepts skillsTreatment control groups.Internal external validity.Average treatment effect.Generating simulated datasets.Informed consent establishing need experiment./B testingPutting together surveys Google FormsKey librariestidyverse (Wickham et al. 2019a)Key functionscount()filter()ggplot()group_by()head()()if_else()left_join()length()mean()mutate()names()nrow()pivot_wider()read_csv()ref()rename()rnorm()rowwise()sample()scale_fill_brewer()seed()select()str_detect()sum()summarize()test()theme_classic()theme_minimal()tibble()ungroup()","code":""},{"path":"hunt-data.html","id":"experiments-and-randomized-controlled-trials","chapter":"10 Hunt data","heading":"10.1 Experiments and randomized controlled trials","text":"","code":""},{"path":"hunt-data.html","id":"introduction-7","chapter":"10 Hunt data","heading":"10.1.1 Introduction","text":"Ronald Fisher, twentieth century statistician, Francis Galton, nineteenth century statistician, intellectual grandfathers much work cover chapter. cases directly work, cases work built contributions. men believed eugenics, amongst things generally reprehensible.\nway art history must acknowledge, say Caravaggio murderer, also considering work influence, must statistics data sciences generally concern past, time try build better future.chapter experiments. situation can explicitly control vary interested . advantage identification clear. treatment group subject interested , control group . randomly split treatment. , end different must treatment. Unfortunately, life rarely smooth. Arguing similar treatment control groups tends carry indefinitely. ability speak whether measured effect treatment, affects ability speak effect treatment .chapter cover experiments, especially constructing treatment control groups, appropriately considering results. discuss aspects ethical behaviour experiments reference abhorrent Tuskegee Syphilis Study ECMO. go Oregon Health Insurance Experiment case study. turn /B testing, extensively used industry, consider case study based Upworthy data. Finally, go actually implementing survey using Google Forms.\n","code":""},{"path":"hunt-data.html","id":"motivation-and-notation","chapter":"10 Hunt data","heading":"10.1.2 Motivation and notation","text":"Professional sports big deal North America. Consider situation someone moves San Francisco 2014, soon moved Giants win World Series Golden State Warriors begin historic streak World Championships. move Chicago, immediately Cubs win World Series first time hundred years. move Massachusetts, Patriots win Super Bowl , , . finally, move Toronto, Raptors immediately win World Championship. city pay move, municipal funds better spent elsewhere?One way get answer run experiment. Make list North American cities major sports teams, roll dice send live year. enough lifetimes, work . fundamental issue live city live city. fundamental problem causal inference: person treated untreated. Experiments randomized controlled trials circumstances try randomly allocate treatment, belief everything else (least ignorable). framework use formalize situation Neyman-Rubin model (Holland 1986).treatment, \\(t\\), often binary variable either 0 1. 0 person, \\(\\), treated, say control group, 1 treated. typically outcome, \\(Y_i\\), interest person, binary, multinomial, continuous. instance, vote choice, case measure whether person : ‘Conservative’ ‘Conservative’; party support, say: ‘Conservative’, ‘Liberal’, ‘Democratic’, ‘Green’; maybe probability support.treatment causal \\((Y_i|t=0) \\neq (Y_i|t=1)\\). say, outcome person \\(\\), given treated, different outcome given treated. treat control one individual one time, know treatment caused change outcome, factor explain . fundamental problem causal inference treat control one individual one time. want know effect treatment, need compare counterfactual. counterfactual happened individual treated. turns , means one way think causal inference missing data problem, missing counterfactual.compared treatment control one individual, instead compare average two groups—treated . looking estimate counterfactual group level impossibility individual level. Making trade-allows us move forward comes cost certainty. must instead rely randomization, probabilities, expectations.usually consider default effect look evidence cause us change mind. interested happening groups, turn expectations, notions probability express . Hence, make claims talk, average. Maybe wearing fun socks really make lucky day, average, across group, probably case.worth pointing just interested average effect. may consider median, variance, whatever. Nonetheless, interested average effect, one way proceed :divide dataset two—treated treated—binary effect column;sum column, divide length column; andthen look ratio.estimator, touched Chapter 5, way putting together guess something interest. estimand thing interest, case average effect, estimate whatever guess turns . can simulate data illustrate situation.broadly, tell causal stories need bring together theory detailed knowledge interested (Cunningham 2021, 4). Chapter 9 discussed gathering data observed world. chapter going active turning world data need. researcher decide measure , need define interested . active participants data generating process. , want use data, researchers must go hunt , like.","code":"\nlibrary(tidyverse)\n\nset.seed(853)\n\ntreatment_control <- \n  tibble(\n    binary_effect = sample(x = c(0, 1), size = 10, replace = TRUE)\n  )\n\ntreatment_control\n#> # A tibble: 10 × 1\n#>    binary_effect\n#>            <dbl>\n#>  1             0\n#>  2             1\n#>  3             1\n#>  4             0\n#>  5             0\n#>  6             0\n#>  7             0\n#>  8             0\n#>  9             1\n#> 10             1\nestimate <-\n  sum(treatment_control$binary_effect) / length(treatment_control$binary_effect)\nestimate\n#> [1] 0.4"},{"path":"hunt-data.html","id":"randomization","chapter":"10 Hunt data","heading":"10.1.3 Randomization","text":"Correlation can enough settings, order able make forecasts things change, circumstances slightly different need understand causation. key counterfactual: happened absence treatment. Ideally, keep everything else constant, randomly divide world two groups, treat one . can pretty confident difference two groups due treatment. reason population randomly select two groups , two groups (provided big enough) characteristics population. Randomized controlled trials (RCTs) /B testing attempt get us close ‘gold standard’ can hope. , others Athey Imbens (2017), use language like gold standard refer approaches, mean imply perfect. Just can better options.hope able find treatment control groups , treatment. means establishing control group critical , establish counterfactual. might worried , say, underlying trends, one issue --comparison, selection bias, occur allow self-selection. Either issues result biased estimators. use randomization go way addressing .get started, simulate population, randomly sample . set half population likes blue, half likes white. , someone likes blue almost surely prefer dogs, like white almost surely prefer cats. approach heavily using simulation critical part workflow advocated book. know roughly outcomes analysis simulated data. Whereas go straight analyzing real data know unexpected outcomes due analysis errors, actual results. Another good reason useful take approach simulation working teams analysis can get started data collection cleaning completed. simulation also help collection cleaning team think tests run data.now construct frame, assuming frame contains 80 per cent population.now, set aside dog cat preferences focus creating treatment control groups basis favorite color .look mean two groups, can see proportions prefer blue white similar specified (Table 10.1).Table 10.1: Proportion treatment control group prefer blue whiteWe randomized based favorite color. also find took dog cat preferences along time ‘representative’ share people prefer dogs cats. happen randomized variables? Let’s start looking dataset (Table 10.2).Table 10.2: Proportion treatment control group prefer dogs catsIt exciting representative share ‘unobservables’. case, ‘observe’ —illustrate point—select . get variables correlated. break several ways discuss. also assumes large enough groups. instance, considered specific dog breeds, instead dogs entity, may find situation. check two groups look see can identify difference two groups based observables. case looked mean, look aspects well.brings us Analysis Variation (ANOVA). ANOVA introduced Fisher working statistical problems agriculture. less unexpected may seem historically agricultural research closely tied statistical innovation. mention ANOVA importance historically, variant linear regression cover detail Chapter 14. , general, usually use ANOVA day--day. nothing wrong right circumstances. hundred years old number modern use-case best option small.case, approach ANOVA expectation groups distribution conduct using aov(). case, fail reject default hypothesis samples .","code":"\nset.seed(853)\n\nnumber_of_people <- 5000\n\npopulation <-\n  tibble(\n    person = c(1:number_of_people),\n    favorite_color = sample(\n      x = c(\"Blue\", \"White\"),\n      size  = number_of_people,\n      replace = TRUE\n    )\n  ) |>\n  mutate(\n    prefers_dogs_to_cats =\n      if_else(favorite_color == \"Blue\", \"Yes\", \"No\"),\n    noise = sample(1:10, size = 1),\n    prefers_dogs_to_cats =\n      if_else(\n        noise <= 8,\n        prefers_dogs_to_cats,\n        sample(c(\"Yes\", \"No\"), \n               size = 1)\n        )\n  ) |> \n  select(-noise)\n\n\npopulation\n#> # A tibble: 5,000 × 3\n#>    person favorite_color prefers_dogs_to_cats\n#>     <int> <chr>          <chr>               \n#>  1      1 Blue           Yes                 \n#>  2      2 White          No                  \n#>  3      3 White          No                  \n#>  4      4 Blue           Yes                 \n#>  5      5 Blue           Yes                 \n#>  6      6 Blue           Yes                 \n#>  7      7 Blue           Yes                 \n#>  8      8 Blue           Yes                 \n#>  9      9 White          No                  \n#> 10     10 White          No                  \n#> # … with 4,990 more rows\n\npopulation |>\n  group_by(favorite_color) |> \n  count()\n#> # A tibble: 2 × 2\n#> # Groups:   favorite_color [2]\n#>   favorite_color     n\n#>   <chr>          <int>\n#> 1 Blue            2547\n#> 2 White           2453\nset.seed(853)\n\nframe <-\n  population |>\n  mutate(\n    in_frame = sample(\n      x = c(0, 1),\n      size  = number_of_people,\n      replace = TRUE,\n      prob = c(0.2, 0.8)\n  )) |>\n  filter(in_frame == 1)\n\nframe |>\n  group_by(favorite_color) |> \n  count()\n#> # A tibble: 2 × 2\n#> # Groups:   favorite_color [2]\n#>   favorite_color     n\n#>   <chr>          <int>\n#> 1 Blue            2023\n#> 2 White           1980\nset.seed(853)\n\nsample <-\n  frame |>\n  select(-prefers_dogs_to_cats) |>\n  mutate(group = sample(\n    x = c(\"Treatment\", \"Control\"),\n    size  = nrow(frame),\n    replace = TRUE\n  ))\nsample |>\n  group_by(group, favorite_color) |>\n  count() |>\n  ungroup() |>\n  group_by(group) |>\n  mutate(prop = n / sum(n)) |>\n  knitr::kable(\n    caption = \"Proportion of the treatment and control group that prefer blue or white\",\n    col.names = c(\"Group\", \"Preferred color\", \"Number\", \"Proportion\"),\n    digits = 2,\n    booktabs = TRUE,\n    linesep = \"\"\n  )\nsample |> \n  left_join(frame |> select(person, prefers_dogs_to_cats), \n            by = \"person\") |>\n  group_by(group, prefers_dogs_to_cats) |> \n  count() |>\n  ungroup() |>\n  group_by(group) |>\n  mutate(prop = n / sum(n)) |>\n  knitr::kable(\n    caption = \"Proportion of the treatment and control group that prefer dogs or cats\",\n    col.names = c(\"Group\", \"Prefers dogs to cats\", \"Number\", \"Proportion\"),\n    digits = 2,\n    booktabs = TRUE,\n    linesep = \"\"\n  )"},{"path":"hunt-data.html","id":"treatment-and-control","chapter":"10 Hunt data","heading":"10.1.4 Treatment and control","text":"treated control groups ways remain way, treatment, internal validity, say control work counterfactual results can speak difference groups study. Internal validity means estimates effect treatment speaking treatment aspect. mean can use results make claims happened experiment.group applied randomization representative broader population, experimental set-fairly similar outside conditions, external validity. mean difference find just apply experiment, also broader population. External validity means can use experiment make claims happen outside experiment. randomization allowed happen.means need randomization twice. Firstly, group subject experiment, secondly, treatment control. think randomization, extent matter?interested effect treated. may charge different prices, continuous treatment variable, compare different colors website, discrete treatment variable. Either way, need make sure groups otherwise . can convinced ? One way ignore treatment variable examine variables, looking whether can detect difference groups based variables. instance, conducting experiment website, groups roughly similar terms , say:Microsoft Apple users?Safari, Chrome, Firefox users?Mobile desktop users?Users certain locations?, groups representative broader population? threats validity claims.done properly, treatment truly independent, can estimate average treatment effect (ATE). binary treatment variable setting :\\[\\mbox{ATE} = \\mathbb{E}[Y|t=1] - \\mathbb{E}[Y|t=0].\\], difference treated group, \\(t = 1\\), control group, \\(t = 0\\), measured expected value outcome, \\(Y\\). ATE becomes difference two expectations.illustrate concept, first simulate data shows difference one treatment control groups.can see difference, simulated one, two groups Figure (Figure 10.1). can compute average groups difference see also get back result put (Table 10.3).\nFigure 10.1: Simulated data showing difference treatment control groups\nTable 10.3: Average difference treatment control groups data simulated average difference oneUnfortunately, often difference simulated data reality. instance, experiment run long otherwise people may treated many times, become inured treatment; short otherwise measure longer term outcomes. ‘representative’ sample across every facet population, , treatment control different. Practical difficulties may make difficult follow certain groups end biased collection. questions explore working real experimental data include:participants selected frame consideration?selected treatment? hope done randomly, term applied variety situations. Additionally, early ‘success’ can lead pressure treat everyone, especially medical settings.treatment assessed?extent random allocation ethical fair? argue shortages mean reasonable randomly allocate, may depend linear benefits . may also difficult establish definitions, power imbalance making decisions treated considered.Bias issues end world. need think carefully. well-known example, Abraham Wald, twentieth century Hungarian mathematician, given data planes came back Britain shot WW2. question place armor. One option place bullet holes. Wald recognized selection effect —planes made back examined—holes necessarily need armor. Arguably armor better placed bullet holes.instance, results survey difficulty university course differ students completed course surveyed, dropped ? work try make dataset good possible, may possible use model control bias. instance, variable correlated say, attrition, added model either -, interaction. Similarly, correlation individuals. instance, ‘hidden variable’ know meant individuals correlated, use ‘wider’ standard errors. needs done carefully discuss Chapter 15. said, issues can anticipated, can better change experiment. instance, perhaps possibly stratify hidden variable.","code":"\nset.seed(853)\n\nate_example <- tibble(person = c(1:1000),\n                      was_treated = sample(\n                        x = c(\"Yes\", \"No\"),\n                        size  = 1000,\n                        replace = TRUE\n                      ))\n\n# Make outcome a bit more likely if treated.\nate_example <-\n  ate_example |>\n  rowwise() |>\n  mutate(outcome = if_else(\n    was_treated == \"No\",\n    rnorm(n = 1, mean = 5, sd = 1),\n    rnorm(n = 1, mean = 6, sd = 1)\n  ))\nate_example |>\n  ggplot(aes(x = outcome,\n             fill = was_treated)) +\n  geom_histogram(position = \"dodge\",\n                 binwidth = 0.2) +\n  theme_minimal() +\n  labs(x = \"Outcome\",\n       y = \"Number of people\",\n       fill = \"Person was treated\") +\n  scale_fill_brewer(palette = \"Set1\")\nate_example |>\n  group_by(was_treated) |>\n  summarize(mean = mean(outcome)) |>\n  pivot_wider(names_from = was_treated, values_from = mean) |>\n  mutate(difference = Yes - No) |>\n  knitr::kable(\n    caption = \"Average difference between the treatment and control groups for data simulated to have an average difference of one\",\n    col.names = c(\"Average for treated\", \"Average for not treated\", \"Difference\"),\n    digits = 2,\n    booktabs = TRUE,\n    linesep = \"\"\n  )"},{"path":"hunt-data.html","id":"fishers-tea-party","chapter":"10 Hunt data","heading":"10.1.5 Fisher’s tea party","text":"Fisher introduced experiment designed see person can distinguish cup tea milk added first, last. begin preparing eight cups tea: four milk added first four milk added last. randomize order eight cups. tell taster, call ‘Ian’, experimental set-: eight cups tea, four type, given cups tea random order, task group two groups.One nice aspects experiment can . things careful practice, including : quantities milk tea consistent; groups marked way taster see; order randomized.Another nice aspect experiment can calculate chance Ian able randomly get groupings correct. decide groupings likely occurred random, need calculate probability happen. First, count number successes four chosen. Fisher (1935, 14) says : \\({8 \\choose 4} = \\frac{8!}{4!(8-4)!}=70\\) possible outcomes.asking Ian group cups, identify , two ways perfectly correct. either correctly identify ones milk-first (one outcome 70) correctly identify ones tea-first (one outcome 70). means probability event : \\(\\frac{2}{70} \\approx 0.028\\) 3 per cent.Fisher (1935, 15) makes clear, now becomes judgement call. need consider weight evidence require accept groupings occur chance Ian well-aware . need decide evidence takes us convinced. possible evidence dissuade us view held coming experiment, say, difference milk-first tea-first, point experiment? expect Ian got completely right, accept able tell difference.almost perfect? chance, 16 ways person ‘--one’. Either Ian thinks one cup milk-first tea-first—, \\({4 \\choose 1} = 4\\), four ways happen—thinks one cup tea-first milk-first—, , \\({4 \\choose 1}\\) = 4, four ways happen. outcomes independent, probability \\(\\frac{4\\times 4}{70} \\approx 0.228\\). . Given almost 23 per cent chance --one just randomly grouping teacups, outcome probably convince us Ian can tell difference tea-first milk-first.looking , order claim something experimentally demonstrable results just shown , instead know features experiment result reliably found (Fisher 1935, 16). looking thoroughly interrogate data experiments, think precisely analysis methods using. Rather searching meaning constellations stars, want make easy possible others reproduce work. way conclusions stand chance holding long-term.","code":""},{"path":"hunt-data.html","id":"informed-consent-and-the-need-for-an-experiment","chapter":"10 Hunt data","heading":"10.1.6 Informed consent and the need for an experiment","text":"One foundations ethical experimental practice informed consent ensuring experiment actually needed. now detail two cases human life potentially lost due issues. One issue experiments medical settings weight evidence measured lost lives. Ethical practice experiments develops many people may unnecessarily lost life due experiments. Two cases dramatically informed practice Tuskegee Syphilis Study ECMO.Tuskegee Syphilis Study infamous medical trial began 1932. part experiment 400 Black Americans syphilis, control group without, given appropriate treatment, even told syphilis (case treatment group), well standard treatment syphilis established widely available sometime mid-1940s early 1950s (Brandt 1978; Alsan Wanamaker 2018). Like treatment group, control group also given non-effective drugs. financially-poor Black Americans US South identified offered compensation including ‘hot meals, guise treatment, burial payments’ (Alsan Wanamaker 2018). men actually treated syphilis (Brandt 1978; Alsan Wanamaker 2018). men told part experiment (Brandt 1978; Alsan Wanamaker 2018). , extensive work undertaken ensure men receive treatment anywhere including writing local doctors, local health department, , incredibly, men drafted told immediately get treatment, draft board complied request men excluded treatment (Brandt 1978, 25). time study stopped 1972, half men deceased many deaths syphilis-related causes (Alsan Wanamaker 2018).effect Tuskegee Syphilis Study felt just men study, broadly. Alsan Wanamaker (2018) found associated decrease life expectancy age 45 1.5 years Black men. response US established requirements Institutional Review Boards President Clinton made formal apology 1997. Brandt (1978, 27) says:retrospect Tuskegee Study revealed pathology racism pathology syphilis; nature scientific inquiry nature disease process… [T]notion science value-free discipline must rejected. need greater vigilance assessing specific ways social values attitudes affect professional behavior clearly indicated.Turning evaluation extracorporeal membrane oxygenation (ECMO), J. H. Ware (1989) describes viewed ECMO possible treatment persistent pulmonary hypertension newborns (PPHN). enrolled 19 patients used conventional medical therapy ten , ECMO nine . found six ten control group survived treatment group survived. J. H. Ware (1989) used randomized consent whereby parents infants randomly selected treated ECMO asked consent.J. H. Ware (1989) concerned ‘equipoise’, refer situation genuine uncertainty whether treatment effective existing procedures. note medical settings even initial equipoise undermined treatment found effective early study. J. H. Ware (1989) describe results first 19 patients, randomization stopped ECMO used. recruiters treating patients initially told randomization stopped. decided complete allocation ECMO continue ‘either 28th survivor 4th death observed’. 19 20 additional patients survived ECMO trial terminated. , actual result experiment divided two phases: first randomized use ECMO, second ECMO used.One approach settings ‘randomized play--winner’ rule following Wei Durham (1978). Treatment still randomized, weight probability shifts successful treatment make treatment likely stopping rule. Berry (1989) argues stopping rule case J. H. Ware (1989) occurred study started need study equipoise never existed. Berry (1989) re-visit literature mentioned J. H. Ware (1989) find extensive evidence ECMO known effective. Berry (1989) points almost never complete consensus one almost always argue existence equipoise even face substantial weight evidence. Berry (1989) criticizes J. H. Ware (1989) use randomized consent potential may different outcomes infants subject conventional therapy parents known options. instead, Berry (1989) argues need comprehensive patient registries, enabling analysis large datasets.Tuskegee Syphilis Study ECMO may seem quite far present circumstances, Dr Monica Alexander, Assistant Professor, University Toronto explains may illegal exact research days, mean unethical research still happen. see time machine learning applications health areas. meant explicitly discriminate meant get consent, mean implicitly discriminate without type buy-. instance, Obermeyer et al. (2019) describes US health care systems use algorithms score severity sick patient . show score, ‘Black patients considerably sicker White patients, evidenced signs uncontrolled illnesses’ Black patients scored way White patients, receive considerably help now. find discrimination occurs algorithm based health care costs, rather sickness. access healthcare unequally distributed Black White patients, algorithm, however inadvertently, perpetuates racial bias.","code":""},{"path":"hunt-data.html","id":"case-study-the-oregon-health-insurance-experiment","chapter":"10 Hunt data","heading":"10.1.7 Case study: The Oregon Health Insurance Experiment","text":"US, unlike many developed countries, basic health insurance necessarily available residents even low incomes. Oregon Health Insurance Experiment involved low-income adults Oregon, state north-west US, 2008 2010 (Finkelstein et al. 2012).Oregon funded 10,000 places state-run Medicaid program, provides health insurance people low incomes. lottery used allocate places judged fair expected, correctly turned , demand places exceed supply. People month sign enter draw. lottery used determine 89,824 individuals signed allowed apply Medicaid.draws conducted six-month period selected opportunity sign . 35,169 individuals selected (household actually won draw given opportunity) 30 per cent completed paperwork eligible (typically earned much). insurance lasted indefinitely. random allocation insurance allowed researchers understand effect health insurance.reason random allocation important usually possible compare without insurance type people sign get health insurance differ . decision ‘confounded’ variables results selection effect.opportunity apply health insurance randomly allocated, researchers able evaluate health earnings received health insurance compare . used administrative data, hospital discharge data, credit reports matched 68.5 per cent lottery participants, mortality records, uncommon. Interestingly collection data fairly restrained included survey conducted via mail.specifics important, say Chapter 14, use statistical model analyze results (Finkelstein et al. 2012):\\[\\begin{equation}\ny_{ihj} = \\beta_0 + \\beta_1\\mbox{Lottery} + X_{ih}\\beta+2 + V_{ih}\\beta_3 + \\epsilon_{ihj} \\tag{10.1}\n\\end{equation}\\]Equation (10.1) explains various \\(j\\) outcomes (health) individual \\(\\) household \\(h\\) function indicator variable whether household \\(h\\) selected lottery. Hence, ‘(t)coefficient Lottery, \\(\\beta_1\\), main coefficient interest, gives average difference (adjusted) means treatment group (lottery winners) control group (selected lottery).’complete specification Equation (10.1), \\(X_{ih}\\) set variables correlated probability treated. adjust impact certain extent. example number individuals household. finally, \\(V_{ih}\\) set variables correlated lottery. variables include demographics, hospital discharge lottery draw.found earlier studies Brook et al. (1984), Finkelstein et al. (2012) found , treatment group 25 per cent likely insurance control group. treatment group used health care including primary preventive care well hospitalizations lower --pocket medical expenditures. generally, treatment group reported better physical mental health.","code":""},{"path":"hunt-data.html","id":"ab-testing","chapter":"10 Hunt data","heading":"10.2 A/B testing","text":"past decade probably seen experiments ever run several orders magnitude extensive use /B testing websites. Large tech companies typically extensive infrastructure experiments, term /B tests comparison two groups: one gets treatment either gets treatment B see change (M. Salganik 2018, 185). Every time online probably subject tens, hundreds, potentially thousands, different /B tests. use apps like TikTok run tens thousands. , heart, still just surveys result data need analysed, several interesting features discuss.instance, Kohavi, Tang, Xu (2020, 3) discusses example Microsoft’s search engine Bing increased amount content displayed ads. change triggered alert usually signaled bug billing. bug, instead case revenue increase 12 per cent, around $100 million annually US, without significant trade-measured.use term /B test strictly refer situation primarily implementing experiment technology stack something primarily internet, instance change website similar. heart just experiments, /B testing range specific concerns. something different tens thousands small experiments time, compared normal experimental set-conducting one experiment course months. Additionally, tech firms distinct cultures can difficult shift toward experimental set-. Sometimes can easier experiment delivering, delaying, change decided create control group rather treatment group (M. Salganik 2018, 188). Often difficult aspect /B testing conducting experiments generally, statistics, ’s politics.first aspect concern delivery /B test (Kohavi, Tang, Xu 2020, 153–61). case experiment, usually clear delivered. instance, may person come doctor’s clinic inject either drug placebo. case /B testing, less obvious. instance, run ‘server-side’, meaning make change website, ‘client-side’, meaning change app. decision affects ability conduct experiment gather data .case effect conducting experiment, relatively easy normal update website time. means small changes can easily implemented experiment conducted server-side. case client-side implementation app, conducting experiment becomes bigger deal. instance, release may need go app store, usually happen time. Instead, need part regular release cycle. also selection concern users update app possibility different regularly update app.Turning effect delivery decision ability gather data experiment. , server-side less big deal get data anyway part user interacting website. case app, user may use app offline limited data upload, requires data transmission protocol caching, affect user experience, especially phones place limits various aspects.effect need plan. instance, results unlikely available day change app, whereas likely available day change website. , may need consider results context different devices platforms, potentially using, say, multilevel regression covered Chapter 14.second aspect concern ‘instrumentation’ method measurement (Kohavi, Tang, Xu 2020, 162 - 165). conduct traditional experiment might, instance, ask respondents fill survey. usually done /B testing. One approach put cookie user’s device, different users clear different rates. Another approach use beacon, forcing user download tiny image server, know completed action. instance, commonly used approach know user opened email. practical concerns around beacon loads, instance, main content loads user experience may worse, sample may biased.third aspect concern randomizing (Kohavi, Tang, Xu 2020, 162 - 165). case traditional experiments, usually clear often person, sometimes various groups people. case /B testing can less clear. instance, randomizing page, session, user?think , let us consider color. instance, say interested whether change logo red blue homepage. randomizing page level, user goes page website, back homepage logo back red. randomizing session level, blue use website time, close come back red. Finally, randomizing user level possibly always red one used, always blue another.extent matters depends trade-consistency importance. instance, /B testing product prices consistency likely feature. /B testing background colors consistency might important. hand, /B testing position log-button might important move around much one user, users might matter less.Interestingly, /B testing, traditional experiments, concerned treatment control groups , treatment. case traditional experiments, satisfy making conducting analysis basis data experiment conducted. usually can weird treat control groups. case /B testing, pace experimentation allows us randomly create treatment control groups, check, subject treatment group treatment, groups . instance, show group website, expect outcomes across two groups. found different outcomes know may randomization issue (Taddy 2019, 129).One interesting aspects /B testing usually running desperately care specific outcome, feeds measure care . instance, care whether website quite-dark-blue slightly-dark-blue? Probably , probably care lot company share price. picking best blue comes cost share price? example bit contrived, let us pretend work food delivery app concerned driver retention. Say /B tests find drivers always likely retained can deliver food customer faster. finding faster better, driver retention, always. one way achieve faster deliveries, put food hot box maintain food’s temperature. Something like might save 30 seconds, significant 10-15 minute delivery. Unfortunately, although decide encourage basis /B tests designed optimize driver-retention, decision likely make customer experience worse. customers receive cold food, meant hot, may stop using app, ultimately bad business.trade-may obvious run driver experiment look customer complaints. possible small team exposed tickets, larger team may . Ensuring /B tests resulting false optimization especially important. something typically worry normal experiments.","code":""},{"path":"hunt-data.html","id":"case-study-upworthy","chapter":"10 Hunt data","heading":"10.2.1 Case study: Upworthy","text":"trouble much /B testing done firms typically datasets can use. Matias et al. (2019) provide access dataset /B tests Upworthy, clickbait media website used /B testing optimize content. Fitts (2014) provides background information Upworthy. datasets /B tests available: https://osf.io/jd64p/.can look dataset looks like, get sense looking names extract.also useful look documentation dataset. describes structure dataset, packages within tests. package collection headlines images shown randomly different visitors website, part test. test can include many packages. row dataset package test part specified ‘clickability_test_id’ column.variety variables. focus :‘created_at’,‘clickability_test_id’ can create comparison groups,‘headline’,‘impressions’ number people saw package, ‘clicks’ number clicked package.Within batch tests, interested effect varied headlines impressions clicks.focus text contained headlines, look whether headlines asked question got clicks . want remove effect different images focus tests image. identify whether headline asks question, search question mark. Although complicated constructions use, enough get started.every test, every picture, want know whether asking question affected number clicks.find general, question headline may slightly decrease number clicks headline, although effect appear large (Figure 10.2).\nFigure 10.2: Comparison average number clicks headline contains question mark \n","code":"\nupworthy <- read_csv(\"https://osf.io/vy8mj/download\")\nupworthy |> \n  names()\n#>  [1] \"...1\"                 \"created_at\"          \n#>  [3] \"updated_at\"           \"clickability_test_id\"\n#>  [5] \"excerpt\"              \"headline\"            \n#>  [7] \"lede\"                 \"slug\"                \n#>  [9] \"eyecatcher_id\"        \"impressions\"         \n#> [11] \"clicks\"               \"significance\"        \n#> [13] \"first_place\"          \"winner\"              \n#> [15] \"share_text\"           \"square\"              \n#> [17] \"test_week\"\n\nupworthy |> \n  head()\n#> # A tibble: 6 × 17\n#>    ...1 created_at          updated_at         \n#>   <dbl> <dttm>              <dttm>             \n#> 1     0 2014-11-20 06:43:16 2016-04-02 16:33:38\n#> 2     1 2014-11-20 06:43:44 2016-04-02 16:25:54\n#> 3     2 2014-11-20 06:44:59 2016-04-02 16:25:54\n#> 4     3 2014-11-20 06:54:36 2016-04-02 16:25:54\n#> 5     4 2014-11-20 06:54:57 2016-04-02 16:31:45\n#> 6     5 2014-11-20 06:55:07 2016-04-02 16:25:54\n#> # … with 14 more variables: clickability_test_id <chr>,\n#> #   excerpt <chr>, headline <chr>, lede <chr>, slug <chr>,\n#> #   eyecatcher_id <chr>, impressions <dbl>, clicks <dbl>,\n#> #   significance <dbl>, first_place <lgl>, winner <lgl>,\n#> #   share_text <chr>, square <chr>, test_week <dbl>\nupworthy_restricted <- \n  upworthy |> \n  select(created_at, clickability_test_id, headline, impressions, clicks)\n\nhead(upworthy_restricted)\n#> # A tibble: 6 × 5\n#>   created_at          clickability_tes… headline impressions\n#>   <dttm>              <chr>             <chr>          <dbl>\n#> 1 2014-11-20 06:43:16 546d88fb84ad38b2… They're…        3052\n#> 2 2014-11-20 06:43:44 546d88fb84ad38b2… They're…        3033\n#> 3 2014-11-20 06:44:59 546d88fb84ad38b2… They're…        3092\n#> 4 2014-11-20 06:54:36 546d902c26714c6c… This Is…        3526\n#> 5 2014-11-20 06:54:57 546d902c26714c6c… This Is…        3506\n#> 6 2014-11-20 06:55:07 546d902c26714c6c… This Is…        3380\n#> # … with 1 more variable: clicks <dbl>\nupworthy_restricted <-\n  upworthy_restricted |>\n  mutate(asks_question = str_detect(string = headline, pattern = \"\\\\?\"))\n\nupworthy_restricted |> \n  count(asks_question)\n#> # A tibble: 2 × 2\n#>   asks_question     n\n#>   <lgl>         <int>\n#> 1 FALSE         19130\n#> 2 TRUE           3536\nto_question_or_not_to_question <- \n  upworthy_restricted |> \n  group_by(clickability_test_id, asks_question) |> \n  summarize(ave_clicks = mean(clicks)) |> \n  ungroup()\n#> `summarise()` has grouped output by 'clickability_test_id'.\n#> You can override using the `.groups` argument.\n\nlook_at_differences <- \n  to_question_or_not_to_question |> \n  pivot_wider(id_cols = clickability_test_id,\n              names_from = asks_question,\n              values_from = ave_clicks) |> \n  rename(ave_clicks_not_question = `FALSE`,\n         ave_clicks_is_question = `TRUE`) |> \n  filter(!is.na(ave_clicks_not_question)) |>\n  filter(!is.na(ave_clicks_is_question)) |> \n  mutate(difference_in_clicks = ave_clicks_is_question - ave_clicks_not_question)\n\nlook_at_differences$difference_in_clicks |> mean()\n#> [1] -4.890435"},{"path":"hunt-data.html","id":"implementing-surveys","chapter":"10 Hunt data","heading":"10.3 Implementing surveys","text":"many ways implement surveys. instance, dedicated survey platforms Survey Monkey Qualtrics. general, focus platforms putting together survey form expect already contact details sample interest. platforms, Mechanical Turk Prolific, focus providing audience, can ask audience just take survey. useful, usually comes higher costs. Finally, platforms Facebook also provide ability run survey. One especially common approach, free, use Google Forms.create survey Google Forms, sign Google Account, go Google Drive, click ‘New’ ‘Google Form’. default, form largely empty (Figure 10.3), add title description.\nFigure 10.3: default view new Google Form created contains many empty fields\ndefault, multiple-choice question included, can update content clicking question field. Helpfully, often suggestions can help provide options. can make question required toggling (Figure 10.4).\nFigure 10.4: Updating multiple-choice question included default\ncan add another question, clicking plus circle around , select different types question, instance, ‘Short answer’, ‘Checkboxes’, ‘Linear scale’ (Figure 10.5). can especially useful use ‘Short answer’ aspect name email address, checkboxes linear scale understand preferences.\nFigure 10.5: Different options questions include short answer, checkboxes, linear scale\nhappy survey, make like preview , clicking icon looks like eye. checking way, can click ‘Send’. Usually especially useful use second option, send via link, can handy shorten URL (Figure 10.6).\nFigure 10.6: variety ways share survey, one helpful one get link short URL\nshare survey, results accrue ‘Responses’ tab can especially useful create spreadsheet view responses, clicking ‘Sheets’ icon. collected enough responses can turn ‘Accepting responds’ (Figure 10.7)\nFigure 10.7: Responses show alongside survey can helpful add separate spreadsheet\n","code":""},{"path":"hunt-data.html","id":"exercises-and-tutorial-9","chapter":"10 Hunt data","heading":"10.4 Exercises and tutorial","text":"","code":""},{"path":"hunt-data.html","id":"exercises-9","chapter":"10 Hunt data","heading":"10.4.1 Exercises","text":"words, role randomization constructing counterfactual (write two three paragraphs)?external validity (pick one)?\nFindings experiment hold setting.\nFindings experiment hold outside setting.\nFindings experiment repeated many times.\nFindings experiment code data available.\nFindings experiment hold setting.Findings experiment hold outside setting.Findings experiment repeated many times.Findings experiment code data available.internal validity (pick one)?\nFindings experiment hold setting.\nFindings experiment hold outside setting.\nFindings experiment repeated many times.\nFindings experiment code data available.\nFindings experiment hold setting.Findings experiment hold outside setting.Findings experiment repeated many times.Findings experiment code data available.dataset named ‘netflix_data’, columns ‘person’ ‘tv_show’ ‘hours’, (person character class uniqueID every person, tv_show character class name tv show, hours double expressing number hours person watched tv show). please write code randomly assign people one two groups? data looks like :context randomization, stratification mean (write paragraph two)?check randomization done appropriately (write two three paragraphs)?Identify three companies conduct /B testing commercially write one paragraph work trade-offs involved.Pretend work junior analyst large consulting firm. , pretend consulting firm taken contract put together facial recognition model Canada Border Services Agency’s Inland Enforcement branch. Taking page two, please discuss thoughts matter. ?estimate (pick one)?\nrule calculating estimate given quantity based observed data.\nquantity interest.\nresult.\nUnknown numbers determine statistical model.\nrule calculating estimate given quantity based observed data.quantity interest.result.Unknown numbers determine statistical model.estimator (pick one)?\nrule calculating estimate given quantity based observed data.\nquantity interest.\nresult.\nUnknown numbers determine statistical model.\nrule calculating estimate given quantity based observed data.quantity interest.result.Unknown numbers determine statistical model.estimand (pick one)?\nrule calculating estimate given quantity based observed data.\nquantity interest.\nresult.\nUnknown numbers determine statistical model.\nrule calculating estimate given quantity based observed data.quantity interest.result.Unknown numbers determine statistical model.parameter (pick one)?\nrule calculating estimate given quantity based observed data.\nquantity interest.\nresult.\nUnknown numbers determine statistical model.\nrule calculating estimate given quantity based observed data.quantity interest.result.Unknown numbers determine statistical model.J. Ware (1989, 298) mentions ‘randomized play winner design’. ?J. Ware (1989, 299) mentions ‘adaptive randomization’. , words?J. Ware (1989, 299) mentions ‘randomized-consent’. continues ‘attractive setting standard approach informed consent require parents infants near death approached give informed consent invasive surgical procedure , instances, administered. familiar agonizing experience child neonatal intensive care unit can appreciate process obtaining informed consent frightening stressful parents’. extent agree position, especially given, Ware (1989), p. 305, mentions ‘need withhold information study parents infants receiving CMT’?J. Ware (1989, 300) mentions ‘equipoise’. words, please define discuss , using example experience.","code":"\nlibrary(tidyverse)\nnetflix_data <- \n  tibble(person = c(\"Rohan\", \"Rohan\", \"Monica\", \"Monica\", \"Monica\", \n                    \"Patricia\", \"Patricia\", \"Helen\"),\n         tv_show = c(\"Broadchurch\", \"Duty-Shame\", \"Broadchurch\", \"Duty-Shame\", \n                     \"Shetland\", \"Broadchurch\", \"Shetland\", \"Duty-Shame\"),\n         hours = c(6.8, 8.0, 0.8, 9.2, 3.2, 4.0, 0.2, 10.2)\n         )"},{"path":"hunt-data.html","id":"tutorial-9","chapter":"10 Hunt data","heading":"10.4.2 Tutorial","text":"Please build website using postcards (Kross 2021). Add Google Analytics. Deploy using Netlify. Change aspect website, add different tracker, push new branch. use Netlify conduct /B test. Write one--two page paper found.","code":""},{"path":"hunt-data.html","id":"paper-2","chapter":"10 Hunt data","heading":"10.4.3 Paper","text":"point, Paper Three (Appendix B.3) appropriate.","code":""},{"path":"clean-and-prepare.html","id":"clean-and-prepare","chapter":"11 Clean and prepare","heading":"11 Clean and prepare","text":"Required materialRead Data Feminism, Chapter 5 ‘Unicorns, Janitors, Ninjas, Wizards, Rock Stars’, (D’Ignazio Klein 2020).Read R Data Science, Chapter 12 ‘Tidy data’, (Wickham Grolemund 2017).Read Gave Four Good Pollsters Raw Data. Four Different Results, (Cohn 2016).Read Column Names Contracts, (Riederer 2020)\n\nKey concepts skillsPlanning end-point simulating dataset like end , key elements cleaning preparing data.Begin small sample dataset, write code fix , iterate generalize additional tranches.Develop series tests checks dataset pass features dataset clear.especially concerned class variables, clear names, values variable expected given .Key librariesconvo (Riederer 2022)janitor (Firke 2020)pointblank (Iannone Vargas 2022)purrr (Henry Wickham 2020)stringr (Wickham 2019e)tidyr (Wickham 2021c)tidyverse (Wickham 2017)Key functionsdplyr::count()dplyr::mutate()dplyr::select()janitor::clean_names()stringr::str_replace_all()stringr::str_trim()tidyr::pivot_longer()tidyr::separate()tidyr::separate_rows()","code":""},{"path":"clean-and-prepare.html","id":"introduction-8","chapter":"11 Clean and prepare","heading":"11.1 Introduction","text":"“Well, Lyndon, may right may every bit intelligent say,” said Rayburn, “’d feel whole lot better just one run sheriff .”Sam Rayburn’s reaction Lyndon Johnson’s enthusiasm Kennedy’s incoming cabinet, quoted Best Brightest (Halberstam 1972, 41).earlier chapters done data cleaning preparation, chapter put place formal approaches. large extent, role data cleaning preparation great people can trust understand data, cleaned . , paradoxically, often cleaning preparation often trust least. point every data science workflow, modelling get hands dirty data cleaning. clean prepare data make many decisions, may important effects results.long time, data cleaning preparation largely overlooked. now realize mistake. difficult trust results disciplines apply statistics. reproducibility crisis, started psychology now extended many fields physical social sciences, brought light issues p-value ‘hacking’, researcher degrees freedom, file-drawer issues, even data results fabrication (Gelman Loken 2013). Steps now put place address . , relatively little focus data gathering, cleaning, preparation aspects applied statistics, despite evidence decisions made steps greatly affect statistical results (Huntington-Klein et al. 2020). chapter focus issues.statistical practices underpin data science correct robust applied simulated datasets, data science typically conducted types datasets. instance, data scientists interested ‘messy, unfiltered, possibly unclean data—tainted heteroskedasticity, complex dependence missingness patterns—recently avoided polite conversations traditional statisticians’ (Craiu 2019). Big data resolve issue, may even exacerbate , instance ‘without taking data quality account, population inferences Big Data subject Big Data Paradox: data, surer fool ’ (Meng 2018). important note issues found much applied statistics research necessarily associated researcher quality, biases (Silberzahn et al. 2018). Instead, result environment within data science conducted. chapter provides approach tools explicitly think work.Gelman Vehtari (2020) writing important statistical ideas past 50 years say enabled new ways thinking data analysis brought tent statistics, approaches ‘considered matter taste philosophy’. focus data cleaning preparation chapter analogous, insofar, represents codification, bringing inside tent, aspects typically, incorrectly, considered taste rather statistics.workflow advocate :Save raw data.Begin end mind.Execute plan small sample.Write tests documentation.Iterate plan.Generalize execution.Update tests documentation.need variety skills effective, stuff statistical sciences. approach needed combination dogged sensible. Perfect much enemy good enough comes data cleaning. specific, better 90 per cent data cleaned prepared, start exploring , deciding whether worth effort clean prepare remaining 10 per cent remainder likely take awful lot time effort.data regardless whether obtained hunting, gathering, farming, issues critical approaches can deal variety issues, importantly, understand might affect modelling (Van den Broeck et al. 2005). clean data analyze data. process forces us make choices value results (Au 2020).","code":""},{"path":"clean-and-prepare.html","id":"workflow","chapter":"11 Clean and prepare","heading":"11.2 Workflow","text":"","code":""},{"path":"clean-and-prepare.html","id":"save-a-copy-of-the-raw-data","chapter":"11 Clean and prepare","heading":"11.2.1 Save a copy of the raw data","text":"first step save raw data separate, local, folder. important save raw data, extent possible, establishes foundation reproducibility (Wilson et al. 2017). obtaining data third-party, government website, control whether continue host data, whether update , address available. also want reduce burden impose servers, saving local copy.locally saved raw data must maintain state, modify . begin clean prepare , instead create another dataset. Maintaining initial, raw, state dataset, using scripts create dataset interested analyzing, ensures entire workflow reproducible.","code":""},{"path":"clean-and-prepare.html","id":"begin-with-an-end-in-mind","chapter":"11 Clean and prepare","heading":"11.2.2 Begin with an end in mind","text":"Planning end state forcing begin end mind important variety reasons. scraping data, helps us proactive scope-creep, data cleaning additionally forces us really think want final dataset look like.first step sketch dataset interested . key features sketch aspects names columns, class, possible range values. instance, might interested populations US states. case sketch might look like Figure 11.1.\nFigure 11.1: Planned dataset US states populations\ncase, sketch forces us decide whether want full names abbreviations state names, population measured millions. process sketching end-point forced us make decisions early , clear desired end state.implement using code simulate data. , process forces us think reasonable values look like dataset literally forced decide functions use. Thinking carefully membership column , instance column meant ‘gender’ values ‘male’, ‘female’, ‘’, ‘unknown’ may expected, number ‘1,000’ likely unexpected. also forces us explicit variable names assign outputs functions variable. instance, simulate data population data.purpose, data cleaning preparation, bring raw data close plan. Ideally, plan desired end-state dataset ‘tidy data’, introduced Chapter 3.","code":"\nlibrary(tidyverse)\n\nset.seed(853)\n\nsimulated_tfr <- \n  tibble(\n    state = state.name,\n    population = runif(n = 50, min = 0, max = 50) |> round(digits = 2)\n  )\n\nsimulated_tfr\n#> # A tibble: 50 × 2\n#>    state       population\n#>    <chr>            <dbl>\n#>  1 Alabama          18.0 \n#>  2 Alaska            6.01\n#>  3 Arizona          24.2 \n#>  4 Arkansas         15.8 \n#>  5 California        1.87\n#>  6 Colorado         20.2 \n#>  7 Connecticut       6.54\n#>  8 Delaware         12.1 \n#>  9 Florida           7.9 \n#> 10 Georgia           9.44\n#> # … with 40 more rows"},{"path":"clean-and-prepare.html","id":"start-small","chapter":"11 Clean and prepare","heading":"11.2.3 Start small","text":"thoroughly planned can turn raw data dealing . Usually, regardless raw data look like, want manipulate rectangular dataset quickly possible. allows us use family dplyr verbs tidyverse approaches. instance, let us assume starting .txt file.first step look regularities dataset. wanting end tabular data, means need type delimiter distinguish different columns. Ideally might features comma, semicolon, tab, double space, line break.worse cases may regular feature dataset can take advantage . instance, sometimes various text repeated.case, although traditional delimiter can use regularity ‘State ’ ’ population ’ get need. difficult case line breaks.One way approach take advantage different classes values looking . instance, case, know US states, 50 possible options, use existence delimiter. also use fact population number , split based space followed number.now go process converting last example tidy data using tidyr (Wickham 2021c).","code":"Alabama, 5\nAlaska, 0.7\nArizona, 7\nArkansas, 3\nCalifornia, 40State is Alabama and population is 5 million.\nState is Alaska and population is 0.7 million.\nState is Arizona and population is 7 million.\nState is Arkansas and population is 3 million.\nState is California and population is 40 million.Alabama 5 Alaska 0.7 Arizona 7 Arkansas 3 California 40\nraw_data <-\n  c('Alabama 5 Alaska 0.7 Arizona 7 Arkansas 3 California 40')\n\ndata_as_tibble <-\n  tibble(raw = raw_data)\n\ntidy_data <-\n  data_as_tibble |>\n  separate(col = raw,\n           into = letters[1:5],\n           sep = \"(?<=[[:digit:]]) \") |>\n  pivot_longer(cols = letters[1:5],\n               names_to = \"drop_me\",\n               values_to = \"separate_me\") |>\n  separate(col = separate_me,\n           into = c('state', 'population'),\n           sep = \" (?=[[:digit:]])\") |>\n  select(-drop_me)\n\ntidy_data\n#> # A tibble: 5 × 2\n#>   state      population\n#>   <chr>      <chr>     \n#> 1 Alabama    5         \n#> 2 Alaska     0.7       \n#> 3 Arizona    7         \n#> 4 Arkansas   3         \n#> 5 California 40"},{"path":"clean-and-prepare.html","id":"write-tests-and-documentation","chapter":"11 Clean and prepare","heading":"11.2.4 Write tests and documentation","text":"established rectangular dataset, albeit messy one, begin look classes . necessarily want fix classes point, can result us losing data. look class see , compare simulated dataset see needs get . note columns different.changing class going onto bespoke issues, deal common issues class. common issues :Commas punctuation, denomination signs columns numeric.Inconsistent formatting dates, ‘December’ ‘Dec’ ‘12’.Unexpected characters, especially unicode, may display consistently.Typically, want fix anything immediately obvious. instance, remove commas used group digits currencies. However, situation typically quickly become dire. need look membership group, triage fix. probably make decision triage based likely largest impact. usually means starting counts, sorting descending order, dealing come.tests membership passed, finally can change class, run tests . adapting idea software development approach unit testing. Tests crucial enable us understand whether software (case data) fit purpose (Wilson 2021).Let us run example collection strings, slightly wrong. type output typical OCR, often gets way , quite., first want get rectangular dataset.now need decide errors going fix. help us decide important, create count.common element correct one, great. next one - ‘PatricIa’ - looks like ‘’ incorrectly capitalized, one - ‘8atricia’ - distinguished ‘8’ instead ‘P’. Let us quickly fix issues redo count.Already much better 60 per cent values correct, compared earlier 30 per cent. two obvious errors - ‘Ptricia’ ‘Patncia’ - first missing ‘’ second ‘n’ ‘ri’ . , can quickly update fix .achieved 80 per cent fix much effort. two remaining issues subtle. first - ‘Patric1a’ - occurred ‘’ incorrectly coded ‘1’. fonts show , others difficult see. common issue, especially OCR, something aware . second - ‘Patricia’ - similarly subtle occurring trailing space. , trailing leading spaces common issue can address str_trim(). fix two remaining issues entries corrected.tests head example. know hoping ‘Patricia’. can start document test well. One way look see values ‘Patricia’ exist dataset.can make things little imposing stopping code execution condition met stopifnot(). use define condition like met. implement type check throughout code. instance, expected certain number rows dataset, certain column various properties, integer, factor.can use stopifnot() ensure script working expected goes . Another way especially usedAnother way write tests dataset use testthat (Wickham 2011). Although developed testing packages, can use functionality test datasets. instance, can use expect_length() check length dataset use expect_equal() check content.tests pass nothing happens, tests fail script stop.","code":"\nmessy_string <-\n  c('Patricia, Ptricia, PatricIa, Patncia, PatricIa, Patricia, Patricia, Patric1a, Patricia , 8atricia')\n\nmessy_string\n#> [1] \"Patricia, Ptricia, PatricIa, Patncia, PatricIa, Patricia, Patricia, Patric1a, Patricia , 8atricia\"\nmessy_data <- \n  tibble(names = messy_string) |> \n  separate_rows(names, sep = \", \") \n\nmessy_data\n#> # A tibble: 10 × 1\n#>    names      \n#>    <chr>      \n#>  1 \"Patricia\" \n#>  2 \"Ptricia\"  \n#>  3 \"PatricIa\" \n#>  4 \"Patncia\"  \n#>  5 \"PatricIa\" \n#>  6 \"Patricia\" \n#>  7 \"Patricia\" \n#>  8 \"Patric1a\" \n#>  9 \"Patricia \"\n#> 10 \"8atricia\"\nmessy_data |> \n  count(names, sort = TRUE)\n#> # A tibble: 7 × 2\n#>   names           n\n#>   <chr>       <int>\n#> 1 \"Patricia\"      3\n#> 2 \"PatricIa\"      2\n#> 3 \"8atricia\"      1\n#> 4 \"Patncia\"       1\n#> 5 \"Patric1a\"      1\n#> 6 \"Patricia \"     1\n#> 7 \"Ptricia\"       1\nmessy_data <- \n  messy_data |> \n  mutate(names = str_replace_all(names, 'PatricIa', 'Patricia'),\n         names = str_replace_all(names, '8atricia', 'Patricia')\n         )\n\nmessy_data |> \n  count(names, sort = TRUE)\n#> # A tibble: 5 × 2\n#>   names           n\n#>   <chr>       <int>\n#> 1 \"Patricia\"      6\n#> 2 \"Patncia\"       1\n#> 3 \"Patric1a\"      1\n#> 4 \"Patricia \"     1\n#> 5 \"Ptricia\"       1\nmessy_data <- \n  messy_data |> \n  mutate(names = str_replace_all(names, 'Ptricia', 'Patricia'),\n         names = str_replace_all(names, 'Patncia', 'Patricia')\n         )\n\nmessy_data |> \n  count(names, sort = TRUE)\n#> # A tibble: 3 × 2\n#>   names           n\n#>   <chr>       <int>\n#> 1 \"Patricia\"      8\n#> 2 \"Patric1a\"      1\n#> 3 \"Patricia \"     1\ncleaned_data <- \n  messy_data |> \n  mutate(names = str_replace_all(names, 'Patric1a', 'Patricia'),\n         names = str_trim(names, side = c(\"right\"))\n         )\n\ncleaned_data |> \n  count(names, sort = TRUE)\n#> # A tibble: 1 × 2\n#>   names        n\n#>   <chr>    <int>\n#> 1 Patricia    10\ncheck_me <- \n  cleaned_data |> \n  filter(names != \"Patricia\")\n\nif (nrow(check_me) > 0) {\n  print(\"Still have values that are not Patricia!\")\n}\nstopifnot(nrow(check_me) == 0)\nlibrary(testthat)\n\nexpect_length(check_me, 1)\nexpect_equal(class(cleaned_data$names), \"character\")\nexpect_equal(unique(cleaned_data$names), \"Patricia\")"},{"path":"clean-and-prepare.html","id":"iterate-generalize-and-update","chapter":"11 Clean and prepare","heading":"11.2.5 Iterate, generalize and update","text":"now iterate plan. recent case, started 10 entries. reason increase 100 even 1,000. may need generalize cleaning procedures tests. eventually start dataset sort order.","code":""},{"path":"clean-and-prepare.html","id":"case-study-kenya-census","chapter":"11 Clean and prepare","heading":"11.3 Case study: Kenya census","text":"","code":""},{"path":"clean-and-prepare.html","id":"gather-and-clean-data","chapter":"11 Clean and prepare","heading":"11.3.1 Gather and clean data","text":"make clear, let us gather, clean, prepare data 2019 Kenyan census. distribution population age, sex, administrative unit 2019 Kenyan census can downloaded . format PDF makes easy look particular result, overly useful want model data. order able , need convert PDF Kenyan census results counts, age sex, county sub-county, tidy dataset can analyzed. use janitor (Firke 2020), pdftools (Ooms 2019b), tidyverse (Wickham et al. 2019b), stringi (Gagolewski 2020).can download1 read PDF 2019 Kenyan census. PDF want read content R, pdf_text() pdftools (Ooms 2019b) useful. works well many recently produced PDFs content text can extract. PDF image, pdf_text() work. Instead, PDF first need go OCR, covered Chapter 9.can see example page PDF 2019 Kenyan census (Figure 11.2).\nFigure 11.2: Example page 2019 Kenyan census\nfirst challenge get dataset format can easily manipulate. consider page PDF extract relevant parts. , first write function, apply page.now function need page PDF. use map_dfr() purrr (Henry Wickham 2020) apply function page, combine outputs one tibble.got rectangular format, now need clean dataset make useful.first step make numbers actual numbers, rather characters. can convert type, need remove anything number otherwise cell converted NA. first identify values numbers can remove .use janitor , worthwhile least first looking going sometimes odd stuff janitor (packages) deal , way want. case, Kenyan government used Excel similar, converted two entries dates. just took numbers column 23 15 , inspecting column can use Excel reverse process enter correct values 4,923 4,611, respectively.identified everything needs removed, can actual removal convert character column numbers integers.next thing clean areas. know 47 counties Kenya, large number sub-counties. Kenyan government purports provide list pages 19 22 PDF (document pages 7 10). list complete, minor issues deal later. case, first need fix inconsistencies.Kenya 47 counties, sub-counties. PDF arranged county data sub-counties, without designating . can use names, certain extent, handful cases, sub-county name county, need first fix .PDF made-three tables. can first get names counties based final two tables reconcile get list counties.hoped, 47 . can add flag based names, need deal sub-counties share name. based page, looking deciding county page sub-county page.Now can add flag whether area county, adjust ones troublesome,dealt areas, can deal ages. First, need fix clear errors.census done work putting together age-groups us, want make easy just focus counts single-year-age. add flag type age : age group, “ages 0 5”, single age, “1”.moment, age character variable. decision make . want character variable (graph properly), want numeric, total 100+ . now, just make factor, least able nicely graphed.","code":"\nlibrary(janitor)\nlibrary(pdftools)\nlibrary(purrr)\nlibrary(tidyverse)\nlibrary(stringi)\ndownload.file(\n  \"https://www.knbs.or.ke/download/2019-kenya-population-and-housing-census-volume-iii-distribution-of-population-by-age-sex-and-administrative-units/?wpdmdl=5729&refresh=620561f1ce3ad1644519921\", \n  \"2019_Kenya_census.pdf\",\n  mode=\"wb\")\n\nall_content <- pdf_text(\"2019_Kenya_census.pdf\")\n# The function is going to take an input of a page\nget_data <- function(i){\n  # i = 467\n  # Just look at the page of interest\n  # Based on Bob Rudis: https://stackoverflow.com/a/47793617\n  just_page_i <- stri_split_lines(all_content[[i]])[[1]] \n  \n  just_page_i <- just_page_i[just_page_i != \"\"]\n  \n  # Grab the name of the location\n  area <- just_page_i[3] |> str_squish()\n  area <- str_to_title(area)\n  \n  # Grab the type of table\n  type_of_table <- just_page_i[2] |> str_squish()\n  \n  # Get rid of the top matter\n  # Manually for now, but could create some rules if needed\n  just_page_i_no_header <- just_page_i[5:length(just_page_i)] \n  \n  # Get rid of the bottom matter\n  # Manually for now, but could create some rules if needed\n  just_page_i_no_header_no_footer <- just_page_i_no_header[1:62] \n  \n  # Convert into a tibble\n  demography_data <- tibble(all = just_page_i_no_header_no_footer)\n  \n  # Split columns\n  demography_data <-\n    demography_data |>\n    mutate(all = str_squish(all)) |> # Any space more than two spaces is reduced\n    mutate(all = str_replace(all, \"10 -14\", \"10-14\")) |> # One specific issue\n    mutate(all = str_replace(all, \"Not Stated\", \"NotStated\")) |> # And another\n    separate(col = all,\n             into = c(\"age\", \"male\", \"female\", \"total\", \"age_2\", \"male_2\", \"female_2\", \"total_2\"),\n             sep = \" \", # Works fine because the tables are nicely laid out\n             remove = TRUE,\n             fill = \"right\",\n             extra = \"drop\"\n    )\n  \n  # They are side by side at the moment, need to append to bottom\n  demography_data_long <-\n    rbind(demography_data |> select(age, male, female, total),\n          demography_data |>\n            select(age_2, male_2, female_2, total_2) |>\n            rename(age = age_2, male = male_2, female = female_2, total = total_2)\n    )\n  \n  # There is one row of NAs, so remove it\n  demography_data_long <- \n    demography_data_long |> \n    remove_empty(which = c(\"rows\"))\n  \n  # Add the area and the page\n  demography_data_long$area <- area\n  demography_data_long$table <- type_of_table\n  demography_data_long$page <- i\n  \n  rm(just_page_i,\n     i,\n     area,\n     type_of_table,\n     just_page_i_no_header,\n     just_page_i_no_header_no_footer,\n     demography_data)\n  \n  return(demography_data_long)\n}\n# Run through each relevant page and get the data\npages <- c(30:513)\nall_tables <- map_dfr(pages, get_data)\nrm(pages, get_data)\nall_tables\n#> # A tibble: 59,532 × 7\n#>    age   male    female  total     area    table        page\n#>    <chr> <chr>   <chr>   <chr>     <chr>   <chr>       <int>\n#>  1 Total 610,257 598,046 1,208,303 Mombasa Table 2.3:…    30\n#>  2 0     15,111  15,009  30,120    Mombasa Table 2.3:…    30\n#>  3 1     15,805  15,308  31,113    Mombasa Table 2.3:…    30\n#>  4 2     15,088  14,837  29,925    Mombasa Table 2.3:…    30\n#>  5 3     14,660  14,031  28,691    Mombasa Table 2.3:…    30\n#>  6 4     14,061  13,993  28,054    Mombasa Table 2.3:…    30\n#>  7 0-4   74,725  73,178  147,903   Mombasa Table 2.3:…    30\n#>  8 5     13,851  14,023  27,874    Mombasa Table 2.3:…    30\n#>  9 6     12,889  13,216  26,105    Mombasa Table 2.3:…    30\n#> 10 7     13,268  13,203  26,471    Mombasa Table 2.3:…    30\n#> # … with 59,522 more rows\n# Need to convert male, female, and total to integers\n# First find the characters that should not be in there\nall_tables |> \n  select(male, female, total) |>\n  mutate_all(~str_remove_all(., \"[:digit:]\")) |> \n  mutate_all(~str_remove_all(., \",\")) |>\n  mutate_all(~str_remove_all(., \"_\")) |>\n  mutate_all(~str_remove_all(., \"-\")) |> \n  distinct()\n#> # A tibble: 3 × 3\n#>   male  female total\n#>   <chr> <chr>  <chr>\n#> 1 \"\"    \"\"     \"\"   \n#> 2 \"Aug\" \"\"     \"\"   \n#> 3 \"Jun\" \"\"     \"\"\n\n# We clearly need to remove \",\", \"_\", and \"-\". \n# This also highlights a few issues on p. 185 that need to be manually adjusted\n# https://twitter.com/RohanAlexander/status/1244337583016022018\nall_tables$male[all_tables$male == \"23-Jun\"] <- 4923\nall_tables$male[all_tables$male == \"15-Aug\"] <- 4611\nall_tables <-\n  all_tables |>\n  mutate_at(vars(male, female, total), ~str_remove_all(., \",\")) |>\n  mutate_at(vars(male, female, total), ~str_replace(., \"_\", \"0\")) |>\n  mutate_at(vars(male, female, total), ~str_replace(., \"-\", \"0\")) |>\n  mutate_at(vars(male, female, total), ~as.integer(.))\n\nall_tables\n#> # A tibble: 59,532 × 7\n#>    age     male female   total area    table            page\n#>    <chr>  <int>  <int>   <int> <chr>   <chr>           <int>\n#>  1 Total 610257 598046 1208303 Mombasa Table 2.3: Dis…    30\n#>  2 0      15111  15009   30120 Mombasa Table 2.3: Dis…    30\n#>  3 1      15805  15308   31113 Mombasa Table 2.3: Dis…    30\n#>  4 2      15088  14837   29925 Mombasa Table 2.3: Dis…    30\n#>  5 3      14660  14031   28691 Mombasa Table 2.3: Dis…    30\n#>  6 4      14061  13993   28054 Mombasa Table 2.3: Dis…    30\n#>  7 0-4    74725  73178  147903 Mombasa Table 2.3: Dis…    30\n#>  8 5      13851  14023   27874 Mombasa Table 2.3: Dis…    30\n#>  9 6      12889  13216   26105 Mombasa Table 2.3: Dis…    30\n#> 10 7      13268  13203   26471 Mombasa Table 2.3: Dis…    30\n#> # … with 59,522 more rows\n# Fix some area names\nall_tables$area[all_tables$area == \"Taita/ Taveta\"] <- \"Taita/Taveta\"\nall_tables$area[all_tables$area == \"Elgeyo/ Marakwet\"] <- \"Elgeyo/Marakwet\"\nall_tables$area[all_tables$area == \"Nairobi City\"] <- \"Nairobi\"\nall_tables$table |> \n  table()\n#> \n#> Table 2.3: Distribution of Population by Age, Sex*, County and Sub- County \n#>                                                                      48216 \n#>       Table 2.4a: Distribution of Rural Population by Age, Sex* and County \n#>                                                                       5535 \n#>       Table 2.4b: Distribution of Urban Population by Age, Sex* and County \n#>                                                                       5781\nlist_counties <- \n  all_tables |> \n  filter(table %in% c(\"Table 2.4a: Distribution of Rural Population by Age, Sex* and County\",\n                      \"Table 2.4b: Distribution of Urban Population by Age, Sex* and County\")\n         ) |> \n  select(area) |> \n  distinct()\n\nlist_counties\n#> # A tibble: 47 × 1\n#>    area        \n#>    <chr>       \n#>  1 Kwale       \n#>  2 Kilifi      \n#>  3 Tana River  \n#>  4 Lamu        \n#>  5 Taita/Taveta\n#>  6 Garissa     \n#>  7 Wajir       \n#>  8 Mandera     \n#>  9 Marsabit    \n#> 10 Isiolo      \n#> # … with 37 more rows\nall_tables |> \n  filter(table == \"Table 2.3: Distribution of Population by Age, Sex*, County and Sub- County\") |> \n  filter(area %in% c(\"Busia\",\n                     \"Garissa\",\n                     \"Homa Bay\",\n                     \"Isiolo\",\n                     \"Kiambu\",\n                     \"Machakos\",\n                     \"Makueni\",\n                     \"Samburu\",\n                     \"Siaya\",\n                     \"Tana River\",\n                     \"Vihiga\",\n                     \"West Pokot\")\n         ) |> \n  select(area, page) |> \n  distinct()\n#> # A tibble: 24 × 2\n#>    area        page\n#>    <chr>      <int>\n#>  1 Samburu       42\n#>  2 Tana River    53\n#>  3 Tana River    56\n#>  4 Garissa       65\n#>  5 Garissa       69\n#>  6 Isiolo        98\n#>  7 Isiolo       100\n#>  8 Machakos     149\n#>  9 Machakos     154\n#> 10 Makueni      159\n#> # … with 14 more rows\nall_tables <- \n  all_tables |> \n  mutate(area_type = if_else(area %in% list_counties$area, \"county\", \"sub-county\"))\n\nall_tables <- \n  all_tables |> \n  mutate(area_type = case_when(\n    area == \"Samburu\" & page == 42 ~ \"sub-county\",\n    area == \"Tana River\" & page == 56 ~ \"sub-county\",\n    area == \"Garissa\" & page == 69 ~ \"sub-county\",\n    area == \"Isiolo\" & page == 100 ~ \"sub-county\",\n    area == \"Machakos\" & page == 154 ~ \"sub-county\",\n    area == \"Makueni\" & page == 164 ~ \"sub-county\",\n    area == \"Kiambu\" & page == 213 ~ \"sub-county\",\n    area == \"West Pokot\" & page == 233 ~ \"sub-county\",\n    area == \"Vihiga\" & page == 333 ~ \"sub-county\",\n    area == \"Busia\" & page == 353 ~ \"sub-county\",\n    area == \"Siaya\" & page == 360 ~ \"sub-county\",\n    area == \"Homa Bay\" & page == 375 ~ \"sub-county\",\n    TRUE ~ area_type\n    )\n  )\n\nrm(list_counties)\n\nall_tables\n#> # A tibble: 59,532 × 8\n#>    age     male female   total area    table  page area_type\n#>    <chr>  <int>  <int>   <int> <chr>   <chr> <int> <chr>    \n#>  1 Total 610257 598046 1208303 Mombasa Tabl…    30 county   \n#>  2 0      15111  15009   30120 Mombasa Tabl…    30 county   \n#>  3 1      15805  15308   31113 Mombasa Tabl…    30 county   \n#>  4 2      15088  14837   29925 Mombasa Tabl…    30 county   \n#>  5 3      14660  14031   28691 Mombasa Tabl…    30 county   \n#>  6 4      14061  13993   28054 Mombasa Tabl…    30 county   \n#>  7 0-4    74725  73178  147903 Mombasa Tabl…    30 county   \n#>  8 5      13851  14023   27874 Mombasa Tabl…    30 county   \n#>  9 6      12889  13216   26105 Mombasa Tabl…    30 county   \n#> 10 7      13268  13203   26471 Mombasa Tabl…    30 county   \n#> # … with 59,522 more rows\ntable(all_tables$age) |> head()\n#> \n#>     0   0-4     1    10 10-14 10-19 \n#>   484   484   484   484   482     1\nunique(all_tables$age) |> head()\n#> [1] \"Total\" \"0\"     \"1\"     \"2\"     \"3\"     \"4\"\n\n# Looks like there should be 484, so need to follow up on some:\nall_tables$age[all_tables$age == \"NotStated\"] <- \"Not Stated\"\nall_tables$age[all_tables$age == \"43594\"] <- \"5-9\"\nall_tables$age[all_tables$age == \"43752\"] <- \"10-14\"\nall_tables$age[all_tables$age == \"9-14\"] <- \"5-9\"\nall_tables$age[all_tables$age == \"10-19\"] <- \"10-14\"\nall_tables$age_type <-\n  if_else(str_detect(all_tables$age, c(\"-\")), \"age-group\", \"single-year\")\nall_tables$age_type <-\n  if_else(str_detect(all_tables$age, c(\"Total\")),\n          \"age-group\",\n          all_tables$age_type)\nall_tables$age <- as_factor(all_tables$age)"},{"path":"clean-and-prepare.html","id":"check-data","chapter":"11 Clean and prepare","heading":"11.3.2 Check data","text":"gathered cleaned data, like run checks. Given format data, can check ‘total’ sum ‘male’ ‘female’. (prefer use different groupings, Kenyan government collected makes available.)can adjust one looks wrong.Kenyan census provides different tables total county sub-county; within county, number urban area county, number urban area county. counties urban count, like make sure sum rural urban counts equals total count. requires pivoting data long wide.First, construct different tables three.constructed constituent parts, can join based age, area, whether county.can now check sum rural urban total.just , difference 1, just move .Finally, want check single age counts sum age-groups.Mt. Kenya Forest, Aberdare Forest, Kakamega Forest slightly dodgy. seem documentation, looks like Kenyan government apportioned various countries. understandable, unlikely big deal, , , just move .","code":"\nfollow_up <- \n  all_tables |> \n  mutate(check_sum = male + female,\n         totals_match = if_else(total == check_sum, 1, 0)\n         ) |> \n  filter(totals_match == 0)\n# There is just one that looks wrong\nall_tables$male[all_tables$age == \"10\" & all_tables$page == 187] <- as.integer(1)\n\nrm(follow_up)\n# Table 2.3\ntable_2_3 <- all_tables |> \n  filter(table == \"Table 2.3: Distribution of Population by Age, Sex*, County and Sub- County\")\ntable_2_4a <- all_tables |> \n  filter(table == \"Table 2.4a: Distribution of Rural Population by Age, Sex* and County\")\ntable_2_4b <- all_tables |> \n  filter(table == \"Table 2.4b: Distribution of Urban Population by Age, Sex* and County\")\nboth_2_4s <-\n  full_join(\n    table_2_4a,\n    table_2_4b,\n    by = c(\"age\", \"area\", \"area_type\"),\n    suffix = c(\"_rural\", \"_urban\")\n  )\n\nall <-\n  full_join(\n    table_2_3,\n    both_2_4s,\n    by = c(\"age\", \"area\", \"area_type\"),\n    suffix = c(\"_all\", \"_\")\n  )\n\nall <-\n  all |>\n  mutate(\n    page = glue::glue(\n      'Total from p. {page}, rural from p. {page_rural}, urban from p. {page_urban}'\n    )\n  ) |>\n  select(\n    -page,\n    -page_rural,\n    -page_urban,-table,\n    -table_rural,\n    -table_urban,-age_type_rural,\n    -age_type_urban\n  )\n\nrm(both_2_4s, table_2_3, table_2_4a, table_2_4b)\nfollow_up <- \n  all |> \n  mutate(total_from_bits = total_rural + total_urban,\n         check_total_is_rural_plus_urban = if_else(total == total_from_bits, 1, 0),\n         total_from_bits - total) |> \n  filter(check_total_is_rural_plus_urban == 0)\n\nhead(follow_up)\n#> # A tibble: 3 × 16\n#>   age          male female  total area   area_type age_type \n#>   <fct>       <int>  <int>  <int> <chr>  <chr>     <chr>    \n#> 1 Not Stated     31     10     41 Nakuru county    single-y…\n#> 2 Total      434287 441379 875666 Bomet  county    age-group\n#> 3 Not Stated      3      2      5 Bomet  county    single-y…\n#> # … with 9 more variables: male_rural <int>,\n#> #   female_rural <int>, total_rural <int>,\n#> #   male_urban <int>, female_urban <int>,\n#> #   total_urban <int>, total_from_bits <int>,\n#> #   check_total_is_rural_plus_urban <dbl>,\n#> #   `total_from_bits - total` <int>\nrm(follow_up)\nfollow_up <- \n  all |> \n  mutate(groups = case_when(age %in% c(\"0\", \"1\", \"2\", \"3\", \"4\", \"0-4\") ~ \"0-4\",\n                            age %in% c(\"5\", \"6\", \"7\", \"8\", \"9\", \"5-9\") ~ \"5-9\",\n                            age %in% c(\"10\", \"11\", \"12\", \"13\", \"14\", \"10-14\") ~ \"10-14\",\n                            age %in% c(\"15\", \"16\", \"17\", \"18\", \"19\", \"15-19\") ~ \"15-19\",\n                            age %in% c(\"20\", \"21\", \"22\", \"23\", \"24\", \"20-24\") ~ \"20-24\",\n                            age %in% c(\"25\", \"26\", \"27\", \"28\", \"29\", \"25-29\") ~ \"25-29\",\n                            age %in% c(\"30\", \"31\", \"32\", \"33\", \"34\", \"30-34\") ~ \"30-34\",\n                            age %in% c(\"35\", \"36\", \"37\", \"38\", \"39\", \"35-39\") ~ \"35-39\",\n                            age %in% c(\"40\", \"41\", \"42\", \"43\", \"44\", \"40-44\") ~ \"40-44\",\n                            age %in% c(\"45\", \"46\", \"47\", \"48\", \"49\", \"45-49\") ~ \"45-49\",\n                            age %in% c(\"50\", \"51\", \"52\", \"53\", \"54\", \"50-54\") ~ \"50-54\",\n                            age %in% c(\"55\", \"56\", \"57\", \"58\", \"59\", \"55-59\") ~ \"55-59\",\n                            age %in% c(\"60\", \"61\", \"62\", \"63\", \"64\", \"60-64\") ~ \"60-64\",\n                            age %in% c(\"65\", \"66\", \"67\", \"68\", \"69\", \"65-69\") ~ \"65-69\",\n                            age %in% c(\"70\", \"71\", \"72\", \"73\", \"74\", \"70-74\") ~ \"70-74\",\n                            age %in% c(\"75\", \"76\", \"77\", \"78\", \"79\", \"75-79\") ~ \"75-79\",\n                            age %in% c(\"80\", \"81\", \"82\", \"83\", \"84\", \"80-84\") ~ \"80-84\",\n                            age %in% c(\"85\", \"86\", \"87\", \"88\", \"89\", \"85-89\") ~ \"85-89\",\n                            age %in% c(\"90\", \"91\", \"92\", \"93\", \"94\", \"90-94\") ~ \"90-94\",\n                            age %in% c(\"95\", \"96\", \"97\", \"98\", \"99\", \"95-99\") ~ \"95-99\",\n                            TRUE ~ \"Other\")\n         ) |> \n  group_by(area_type, area, groups) |> \n  mutate(group_sum = sum(total, na.rm = FALSE),\n         group_sum = group_sum / 2,\n         difference = total - group_sum) |> \n  ungroup() |> \n  filter(age == groups) |> \n  filter(total != group_sum) \n\nhead(follow_up)\n#> # A tibble: 6 × 16\n#>   age    male female total area           area_type age_type\n#>   <fct> <int>  <int> <int> <chr>          <chr>     <chr>   \n#> 1 0-4       1      5     6 Mt. Kenya For… sub-coun… age-gro…\n#> 2 5-9       1      2     3 Mt. Kenya For… sub-coun… age-gro…\n#> 3 10-14     6      0     6 Mt. Kenya For… sub-coun… age-gro…\n#> 4 15-19     9      1    10 Mt. Kenya For… sub-coun… age-gro…\n#> 5 20-24    21      4    25 Mt. Kenya For… sub-coun… age-gro…\n#> 6 25-29    59      9    68 Mt. Kenya For… sub-coun… age-gro…\n#> # … with 9 more variables: male_rural <int>,\n#> #   female_rural <int>, total_rural <int>,\n#> #   male_urban <int>, female_urban <int>,\n#> #   total_urban <int>, groups <chr>, group_sum <dbl>,\n#> #   difference <dbl>\n\nrm(follow_up)"},{"path":"clean-and-prepare.html","id":"tidy-up","chapter":"11 Clean and prepare","heading":"11.3.3 Tidy-up","text":"Now confident everything looking good, can just convert tidy format. make easier work .original purpose cleaning dataset make table M. Alexander Alkema (2021). Just bring together, make graph single-year counts, gender, Nairobi (Figure 11.3).\nFigure 11.3: Distribution age gender Nairobi 2019, based Kenyan census\nvariety features clear Figure 11.3, including age-heaping, slight difference ratio male-female birth, substantial difference ages 15 25.","code":"\nall <-\n  all |>\n  rename(male_total = male,\n         female_total = female,\n         total_total = total) |>\n  pivot_longer(\n    cols = c(\n      male_total,\n      female_total,\n      total_total,\n      male_rural,\n      female_rural,\n      total_rural,\n      male_urban,\n      female_urban,\n      total_urban\n    ),\n    names_to = \"type\",\n    values_to = \"number\"\n  ) |>\n  separate(\n    col = type,\n    into = c(\"gender\", \"part_of_area\"),\n    sep = \"_\"\n  ) |>\n  select(area, area_type, part_of_area, age, age_type, gender, number)\n\nhead(all)\n#> # A tibble: 6 × 7\n#>   area  area_type part_of_area age   age_type gender  number\n#>   <chr> <chr>     <chr>        <fct> <chr>    <chr>    <int>\n#> 1 Momb… county    total        Total age-gro… male    610257\n#> 2 Momb… county    total        Total age-gro… female  598046\n#> 3 Momb… county    total        Total age-gro… total  1208303\n#> 4 Momb… county    rural        Total age-gro… male        NA\n#> 5 Momb… county    rural        Total age-gro… female      NA\n#> 6 Momb… county    rural        Total age-gro… total       NA\nmonicas_dataset <- \n  all |> \n  filter(area_type == \"county\") |> \n  filter(part_of_area == \"total\") |>\n  filter(age_type == \"single-year\") |> \n  select(area, age, gender, number)\n\nhead(monicas_dataset)\n#> # A tibble: 6 × 4\n#>   area    age   gender number\n#>   <chr>   <fct> <chr>   <int>\n#> 1 Mombasa 0     male    15111\n#> 2 Mombasa 0     female  15009\n#> 3 Mombasa 0     total   30120\n#> 4 Mombasa 1     male    15805\n#> 5 Mombasa 1     female  15308\n#> 6 Mombasa 1     total   31113\nmonicas_dataset |>\n  filter(area == \"Nairobi\") |>\n  filter(gender != \"total\") |>\n  ggplot(aes(x = age, y = number, fill = gender)) +\n  geom_col(aes(x = age, y = number, fill = gender), position = \"dodge\") +\n  scale_y_continuous(labels = scales::comma) +\n  scale_x_discrete(breaks = c(seq(from = 0, to = 99, by = 5), \"100+\")) +\n  theme_classic()+\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(y = \"Number\",\n       x = \"Age\",\n       fill = \"Gender\",\n       caption = \"Data source: 2019 Kenya Census\")"},{"path":"clean-and-prepare.html","id":"checks-and-tests","chapter":"11 Clean and prepare","heading":"11.4 Checks and tests","text":"Robert Caro, biographer Lyndon Johnson, spent years tracking everyone connected 36th President United States. went far live Texas Hill Country three years better understand LBJ . heard story LBJ used run Senate senator, ran route multiple times try understand LBJ running. Caro eventually understood ran route sun rising, just LBJ done, sun hits Senate Rotunda particularly inspiring way (Caro 2019). background work enabled uncover aspects one else knew. instance, turns LBJ almost surely stole first election win Texas Senator (Caro 2019). need understand data extent. must turn every page go every extreme.idea negative space well established design. refers surrounds subject. Sometimes negative space used effect, instance logo FedEx, American logistics company, negative space E x creates arrow. similar way, want cognizant data , data . worried data somehow meaning, potentially even extent changing conclusions. cleaning data, looking anomalies. interested values , also opposite situation—values missing . four tools use identify situations: graphs, counts, green/red conditions, targets.","code":""},{"path":"clean-and-prepare.html","id":"graphs-1","chapter":"11 Clean and prepare","heading":"11.4.1 Graphs","text":"Graphs invaluable tool cleaning data, show point dataset, relation points. especially useful identifying value belong. instance, value expected numerical, still character plot warning displayed.Graphs especially useful numerical data, still useful text categorical data. Let us pretend situation interested person’s age, youth survey. following data:graph clearly shows unexpected value 150. likely explanation data incorrectly entered trailing 0, 15. can fix , document , redo graph, see everything seems reasonable now.","code":"\nraw_data <- \n  tibble(ages = c(11, 17, 22, 13, 21, 16, 16, 6, 16, 11, 150))\n\nraw_data |> \n  ggplot(aes(y = ages, x = 0)) +\n  geom_point()"},{"path":"clean-and-prepare.html","id":"counts","chapter":"11 Clean and prepare","heading":"11.4.2 Counts","text":"want focus getting data right. interested counts unique values. Hopefully majority data concentrated common counts. can also useful invert , see especially uncommon. extent want deal depends need. Ultimately, time fix one getting additional observations, potentially even just one! Counts especially useful text categorical data, can helpful numerical well.Let us see example.use count clearly identifies spend time - changing ‘Australie’ ‘Australia’ almost double amount usable data.","code":"\nraw_data <- \n  tibble(country = c('Australie', 'Austrelia', 'Australie', 'Australie', 'Aeustralia', 'Austraia', 'Australia', 'Australia', 'Australia', 'Australia'\n                  )\n         )\n\nraw_data |> \n  count(country, sort = TRUE)\n#> # A tibble: 5 × 2\n#>   country        n\n#>   <chr>      <int>\n#> 1 Australia      4\n#> 2 Australie      3\n#> 3 Aeustralia     1\n#> 4 Austraia       1\n#> 5 Austrelia      1"},{"path":"clean-and-prepare.html","id":"gono-go","chapter":"11 Clean and prepare","heading":"11.4.3 Go/no-go","text":"things important require cleaned dataset . go/-go conditions. typically come experience, expert knowledge, planning simulation exercises. example may negative numbers age column, ages 140.specifically require condition met. examples include:cross-country analysis, list country names know dataset useful. -go conditions : 1) values list dataset, , vice versa; 2) countries expected .concrete example, let us consider analysis five largest counties Kenya: ‘Nairobi’, ‘Kiambu’, ‘Nakuru’, ‘Kakamega’, ‘Bungoma’. Let us create array first.begin following dataset.Based count know fix two numbers obvious fixes.point can use go/-go conditions decide whether finished .clear still cleaning !may also find similar conditions experts experience particular field.","code":"\ncorrect_counties <- c('Nairobi', 'Kiambu', 'Nakuru', 'Kakamega', 'Bungoma')\ntop_five_kenya <- \n  tibble(county = c('Nairobi', 'Nairob1', 'Nakuru', 'Kakamega', 'Nakuru', \n                      'Kiambu', 'Kiambru', 'Kabamega', 'Bun8oma', 'Bungoma')\n  )\n\ntop_five_kenya |> \n  count(county, sort = TRUE)\n#> # A tibble: 9 × 2\n#>   county       n\n#>   <chr>    <int>\n#> 1 Nakuru       2\n#> 2 Bun8oma      1\n#> 3 Bungoma      1\n#> 4 Kabamega     1\n#> 5 Kakamega     1\n#> 6 Kiambru      1\n#> 7 Kiambu       1\n#> 8 Nairob1      1\n#> 9 Nairobi      1\ntop_five_kenya <- \n  top_five_kenya |> \n  mutate(county = str_replace_all(county, 'Nairob1', 'Nairobi'),\n         county = str_replace_all(county, 'Bun8oma', 'Bungoma')\n  )\n\ntop_five_kenya |> \n  count(county, sort = TRUE)\n#> # A tibble: 7 × 2\n#>   county       n\n#>   <chr>    <int>\n#> 1 Bungoma      2\n#> 2 Nairobi      2\n#> 3 Nakuru       2\n#> 4 Kabamega     1\n#> 5 Kakamega     1\n#> 6 Kiambru      1\n#> 7 Kiambu       1\ntop_five_kenya$county |> unique()\n#> [1] \"Nairobi\"  \"Nakuru\"   \"Kakamega\" \"Kiambu\"   \"Kiambru\" \n#> [6] \"Kabamega\" \"Bungoma\"\n\nif(all(top_five_kenya$county |> unique() == top_five_kenya)) {\n  \"Oh no\"\n}\nif(all(top_five_kenya==top_five_kenya$county |> unique()) ) {\n  \"Oh no\"\n}"},{"path":"clean-and-prepare.html","id":"class-1","chapter":"11 Clean and prepare","heading":"11.4.4 Class","text":"often said American society obsessed money, British society obsessed class. case data cleaning preparation need British. Explicit checks class variables essential. Accidentally assigning wrong class variable can large effect subsequent analysis. particular:check whether value number factor; andcheck dates correctly formatted.understand important clear whether value number factor, consider following situation:Let us start ‘group’ integer look logistic regression.Now can try factor. interpretation variable completely different.Another critical aspect check dates. particular want try make following format: YYYY-MM-DD. course differences opinion appropriate date format broader world, reasonable people can differ whether 1 July 2010 July 1, 2020, better, YYYY-MM-DD format generally appropriate data.","code":"\nsome_data <- \n  tibble(response = c(1, 1, 0, 1, 0, 1, 1, 0, 0),\n         group = c(1, 2, 1, 1, 2, 3, 1, 2, 3)) |> \n  mutate(group_as_integer = as.integer(group),\n         group_as_factor = as.factor(group),\n         )\nlm(response~group_as_integer, data = some_data) |> \n  summary()\n#> \n#> Call:\n#> lm(formula = response ~ group_as_integer, data = some_data)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#>  -0.68  -0.52   0.32   0.32   0.64 \n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)        0.8400     0.4495   1.869    0.104\n#> group_as_integer  -0.1600     0.2313  -0.692    0.511\n#> \n#> Residual standard error: 0.5451 on 7 degrees of freedom\n#> Multiple R-squared:  0.064,  Adjusted R-squared:  -0.06971 \n#> F-statistic: 0.4786 on 1 and 7 DF,  p-value: 0.5113\nlm(response~group_as_factor, data = some_data) |> \n  summary()\n#> \n#> Call:\n#> lm(formula = response ~ group_as_factor, data = some_data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -0.7500 -0.3333  0.2500  0.2500  0.6667 \n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)        0.7500     0.2826   2.654   0.0378 *\n#> group_as_factor2  -0.4167     0.4317  -0.965   0.3717  \n#> group_as_factor3  -0.2500     0.4895  -0.511   0.6278  \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.5652 on 6 degrees of freedom\n#> Multiple R-squared:  0.1375, Adjusted R-squared:  -0.15 \n#> F-statistic: 0.4783 on 2 and 6 DF,  p-value: 0.6416"},{"path":"clean-and-prepare.html","id":"naming-things","chapter":"11 Clean and prepare","heading":"11.5 Naming things","text":"improved scanning software developed identified gene name errors 30.9% (3,436/11,117) articles supplementary Excel gene lists; figure significantly higher previously estimated. due gene names converted just dates floating-point numbers, also internal date format (five-digit numbers).Abeysooriya et al. (2021)Names matter. land much book written today named Toronto, within country named Canada, long time known Turtle Island. common, days people sometimes still refer Turtle Island. tells us something , use name Canada tells something us. big rock center Australia. long time, called Uluru, known Ayers Rock. Today dual name combines , choice name use tells someone something . Even British Royal Family recognize power names. 1917 changed House Saxe-Coburg Gotha House Windsor, due feeling former Germanic given World War ongoing. Names matter everyday life. matter data science .importance names, ignoring existing claims re-naming clear cases, see data science well. need careful name datasets, variables, functions. tendency, days, call variable ‘gender’ even though may male female, want say word ‘sex’. Tukey (1962) essentially defines today call data science, popularized folks computer science 2010s ignored, either deliberately ignorance, came . past ten years characteristic renaming concepts well-established fields computer science recently expanded . instance, use binary variables regression, sometimes called ‘dummy variables’, called one-hot encoding computer science. Like fashions, one pass also. recently saw 1980s early 2010s economics. Economists described ‘queen social sciences’ self-described imperialistic (Lazear 2000). now recognizing costs imperialism social sciences, future look back count cost computer science imperialism data science. key area study ever terra nullius, nobody’s land. important recognize, adopt, use existing names, practices.Names give places meaning, ignoring existing names, ignore come us. Kimmerer (2012, 34) describes ‘Tahawus Algonquin name Mount Marcy, highest peak Adirondacks. ’s called Mount March commemorate governor never set foot wild slopes.’ continues ‘[w]hen call place name transformed wilderness homeland.’ talking regard physical places, true function names, variable names dataset names. use gender instead sex want say sex front others, ignore preferences provided data.addition respecting nature data, names need satisfy two additional considerations:need machine readable, andthey need human readable.Machine readable names easier standard meet, usually means avoiding spaces special characters. space can replaced underbar. Usually, special characters just removed can inconsistent different computers languages. names also unique within dataset, unique within collection datasets unless particular column deliberately used key join different datasets.especially useful function use get closer machine readable names janitor::clean_names() janitor package (Firke 2020). deals issues mentioned well others. can see example.Human readable names require additional layer. need consider cultures may interpret names using. also need consider different experience levels subsequent users dataset may . terms experience programming statistics, also experience similar datasets. instance, column ‘flag’ often used signal column contains data needs followed treated carefully way. experienced analyst know , beginner . Try use meaningful names wherever possible (Lin, Ali, Wilson 2020). found shorter names may take longer comprehend (Hofmeister, Siegmund, Holt 2017), often useful avoid abbreviations possible.One interesting feature R certain cases partial matching names possible. instance:behavior possible within tidyverse, instance data.frame replaced tibble code. Partial matching almost never used. makes difficult understand code break, others come fresh.Riederer (2020) advises using column names contracts, establishing controlled vocabulary column names. way, define set words can use column names. controlled vocabulary Riederer (2020) column start abbreviation class, something specific pertains , various details. instance, Kenyan data example earlier following column names: “area”, “age”, “gender”, “number”. use column names contracts, : “chr_area”, “fctr_group_age”, “chr_group_gender”, “int_group_count”.can use pointblank (Iannone Vargas 2022) set-tests us.\n    &marker;chr_area\n  ✓————\n    &marker;chr_group_gender\n  ✓————\n    &marker;fctr_group_age\n  ✓————\n    &marker;int_group_count\n  ✓————\n    &marker;chr_group_gender\n  male, female, total✓————","code":"\nbad_names_good_names <- \n  tibble(\n    'First' = c(1),\n    'second name has spaces' = c(1),\n    'weird#symbol' = c(1),\n    'InCoNsIsTaNtCaPs' = c(1)\n  )\n\nbad_names_good_names\n#> # A tibble: 1 × 4\n#>   First `second name has s…` `weird#symbol` InCoNsIsTaNtCaPs\n#>   <dbl>                <dbl>          <dbl>            <dbl>\n#> 1     1                    1              1                1\n\nbad_names_good_names <- \n  bad_names_good_names |> \n  janitor::clean_names()\n  \nbad_names_good_names\n#> # A tibble: 1 × 4\n#>   first second_name_has_s… weird_number_sy… in_co_ns_is_ta_…\n#>   <dbl>              <dbl>            <dbl>            <dbl>\n#> 1     1                  1                1                1\nnever_use_partial_matching <- \n  data.frame(\n    my_first_name = c(1, 2),\n    another_name = c(\"wow\", \"great\")\n  )\n\nnever_use_partial_matching$my_first_name\n#> [1] 1 2\nnever_use_partial_matching$my\n#> [1] 1 2\ncolumn_names_as_contracts <- \n  monicas_dataset |> \n  rename(\n    \"chr_area\" = \"area\",\n    \"fctr_group_age\" = \"age\",\n    \"chr_group_gender\" = \"gender\",\n    \"int_group_count\" = \"number\"\n  )\nlibrary(pointblank)\n\nagent <-\n  create_agent(tbl = column_names_as_contracts) |>\n  col_is_character(columns = vars(chr_area, chr_group_gender)) |>\n  col_is_factor(columns = vars(fctr_group_age)) |>\n  col_is_integer(columns = vars(int_group_count)) |>\n  col_vals_in_set(columns = chr_group_gender,\n                  set = c(\"male\", \"female\", \"total\")) |>\n  interrogate()\n\nagent"},{"path":"clean-and-prepare.html","id":"exercises-and-tutorial-10","chapter":"11 Clean and prepare","heading":"11.6 Exercises and tutorial","text":"","code":""},{"path":"clean-and-prepare.html","id":"exercises-10","chapter":"11 Clean and prepare","heading":"11.6.1 Exercises","text":"following example tidy data?dealing ages likely class variable? [Select apply.]\ninteger\nmatrix\nnumeric\nfactor\nintegermatrixnumericfactor","code":"\ntibble(name = c('Anne', 'Bethany', 'Stephen', 'William'),\n       age_group = c('18-29', '30-44', '45-60', '60+'),\n       )\n#> # A tibble: 4 × 2\n#>   name    age_group\n#>   <chr>   <chr>    \n#> 1 Anne    18-29    \n#> 2 Bethany 30-44    \n#> 3 Stephen 45-60    \n#> 4 William 60+"},{"path":"clean-and-prepare.html","id":"tutorial-10","chapter":"11 Clean and prepare","heading":"11.6.2 Tutorial","text":"regard Jordan (2019), D’Ignazio Klein (2020), Chapter 6, Au (2020), relevant work, extent think let data speak ? [Please write page two.]","code":""},{"path":"store-and-share.html","id":"store-and-share","chapter":"12 Store and share","heading":"12 Store and share","text":"Required materialRead Datasheets datasets, (Gebru et al. 2021).Read Data (dis)contents: survey dataset development use machine learning research, (Paullada et al. 2021).Read Ten simple rules responsible big data research, (Zook et al. 2017).Read Data Can Used People: Classification Personal Data Misuses, (Kröger, Miceli, Müller 2021).Key concepts skillsFAIR principles data sharing management.Sharing data using: GitHub, R data packages, depositing data.Data documentation especially datasheets datasets.Understanding personally identifying information, depends context.Implementing data simulation share data.Know differential privacy likely effect data science.Key librariesopenssl (Ooms 2021)Key functionsopenssl::md5()openssl::sha512()","code":""},{"path":"store-and-share.html","id":"introduction-9","chapter":"12 Store and share","heading":"12.1 Introduction","text":"put together dataset, important part responsible storing appropriately enabling easy retrieval. certainly possible especially concerned , entire careers based storage retrieval data, certain extent, baseline onerous. can get dataset computer, much way . confirming someone else can retrieve use , puts us much .said, FAIR principles useful come think formally data sharing management. (M. D. Wilkinson et al. 2016):Findable. one, unchanging, identifier dataset dataset high-quality descriptions explanations.Accessible. Standardized approaches can used retrieve data, open free, possibly authentication, metadata persists even dataset removed.Interoperable. dataset metadata use broadly applicable language, vocabulary.Reusable. plenty description dataset usage conditions made clear along provenance.important recognize just dataset FAIR, necessarily unbiased representation world. FAIR reflects whether dataset appropriately available, whether appropriate.One reason rise data science humans heart . often data interested directly concern humans. means tension sharing data facilitate reproducibility maintaining privacy. Medicine developed approaches long time. seen Health Insurance Portability Accountability Act (HIPAA) US, related General Data Protection Regulation (GDPR) Europe. concerns data science tend personally identifying information (PII). variety ways protect especially private information datasets, emails, including hashing. sometimes simulate data distribute instead sharing actual dataset. recently, differential privacy implemented. usually inappropriate choice, anything massive datasets ensures level privacy, expense population minorities.chapter consider plan organize datasets meet essential requirements. large extent put place make life easier come back use dataset later. go putting dataset GitHub, building R packages data, finally depositing various archives. consider documentation, particular focus datasheets.","code":""},{"path":"store-and-share.html","id":"plan-3","chapter":"12 Store and share","heading":"12.2 Plan","text":"storage retrieval information long history. especially connected libraries existed since antiquity established protocols deciding information store discard, well retrieval. One defining aspects libraries deliberate curation organization. cataloging system ensures books similar topics located close , typically also deliberate plans ensuring collection --date.Vannevar Bush defines ‘memex’ 1945 device used store books, records, communications, way supplements memory (Bush 1945). key indexing, linking together items. can see concept echoed Tim Berners-Lee proposal hypertext (Berners-Lee 1989), led World Wide Web. way resources identified. transported Internet, using HTTP.fundamental, storing retrieving data. instance, make various files computer available others. internet famously brittle, considering storage retrieval datasets want consider especially, long important data stored (Michener 2015). instance, want data available decade widely available becomes important store data open persistent formats, CSVs (Hart et al. 2016). just using data part intermediate step, raw data, scripts create , might fine worry much considerations.Storing raw data important many cases raw data revealed hinted fraud (Simonsohn 2013). Shared data also enhances credibility work, enabling others verify , can lead generation new knowledge others use answer different questions (Christensen, Freese, Miguel 2019). Finally, research shares data may highly cited (Christensen et al. 2019).","code":""},{"path":"store-and-share.html","id":"share-data","chapter":"12 Store and share","heading":"12.3 Share data","text":"","code":""},{"path":"store-and-share.html","id":"github-1","chapter":"12 Store and share","heading":"12.3.1 GitHub","text":"easiest place store datasets GitHub already built workflow. instance, push, dataset becomes available. One great benefit , set-workspace appropriately, likely store raw data, tidy data, well scripts needed transform one .example stored data, can access ‘raw_data.csv’ file ‘starter_folder’. get file pass read_csv() navigate file GitHub, click ‘Raw’.One issue dataset much documentation. can store retrieve dataset easily way, lacks much explanation, formal dictionary, aspects license bring dataset closer aligning FAIR principles.","code":"\nlibrary(tidyverse)\n\nstarter_data <- read_csv(\"https://raw.githubusercontent.com/RohanAlexander/starter_folder/main/inputs/data/raw_data.csv\")\n\nstarter_data\n#> # A tibble: 1 × 3\n#>   first_col second_col third_col\n#>   <chr>     <chr>      <chr>    \n#> 1 some      raw        data"},{"path":"store-and-share.html","id":"r-packages-for-data","chapter":"12 Store and share","heading":"12.3.2 R Packages for data","text":"point largely used R packages code, although seen focused sharing data, instance, Flynn (2021). can build R Package dataset add GitHub. make easy store retrieve can obtain dataset loading package. first R package build. Chapter 18, return R packages use deploy models.get started, create new package (‘File’ -> ‘New project’ -> ‘New Directory’ -> ‘R Package’). Give package name, ‘favcolordata’ select ‘Open new session’. Create new folder called ‘data’. simulate dataset include.point largely trying use CSV files datasets. include data R package, save dataset particular format, ‘rda’, using save().create R file ‘data.R’ ‘R’ folder. R file contain documentation using roxygen2 comments, start #', follow documentation troopdata closely.Finally, add README provides summary someone coming project outside.can go ‘Build’ tab ‘Install Restart’. happens, package ‘favcolordata’, loaded data can accessed using ‘color_data’. push GitHub, anyone able install package using devtools (Wickham, Hester, Chang 2020) use dataset.addressed many issues faced earlier. instance, included README data dictionary sorts terms descriptions added. try put package onto CRAN, might face issues. instance, maximum size package 5MB quickly come . also largely forced users use R, considerable benefits , may like language agnostic (N. J. Tierney Ram 2020).","code":"\nlibrary(tidyverse)\n\nset.seed(853)\n\ncolor_data <-\n    tibble(\n        name =\n            c(\"Edward\",\n              \"Helen\",\n              \"Hugo\",\n              \"Ian\",\n              \"Monica\",\n              \"Myles\",\n              \"Patricia\",\n              \"Roger\",\n              \"Rohan\",\n              \"Ruth\"\n            ),\n        fav_color =\n            sample(\n                x = c(\"Black\", \"White\", \"Rainbow\"),\n                size = 10,\n                replace = TRUE\n            )\n    )\nsave(color_data, file=\"data/color_data.rda\")\n#' Favorite color of various people data\n#'\n#' @description \\code{favcolordata} returns a dataframe of the favorite color of various people.\n#' \n#' @return Returns a dataframe of the favorite color of various people.\n#' \n#' @docType data\n#'\n#' @usage data(color_data)\n#'\n#' @format An dataframe of individual-level observations with the following variables: \n#'\n#' \\describe{\n#' \\item{\\code{name}}{A character vector of individual names.}\n#' \\item{\\code{fav_color}}{A character vector of one of: black, white, rainbow.}\n#' }\n#'\n#' @keywords datasets\n#'\n#' @source \\url{https://www.tellingstorieswithdata.com/storing-and-retrieving-data.html}\n#'\n\"color_data\"\ndevtools::install_github(\"RohanAlexander/favcolordata\")\n\nlibrary(favcolordata)\n\ncolor_data"},{"path":"store-and-share.html","id":"depositing-data","chapter":"12 Store and share","heading":"12.3.3 Depositing data","text":"possible dataset cited available GitHub R package, becomes likely dataset deposited somewhere. several reasons , one seems bit formal. Zenodo Open Science Framework (OSF) two commonly used. instance, C. Carleton (2021) use Zenodo share dataset analysis supporting W. C. Carleton, Campbell, Collard (2021). Similarly Michael et al. (2021) use Zenodo share dataset underpins Geuenich et al. (2021).\nAnother option dataverse, Harvard Dataverse, common requirement journal publications. One nice aspect can use dataverse (Kuriwaki, Beasley, Leeper 2022) retrieve dataset part reproducible workflow.","code":""},{"path":"store-and-share.html","id":"documentation","chapter":"12 Store and share","heading":"12.4 Documentation","text":"Datasheets (Gebru et al. 2021) increasingly critical aspect data science. Datasheets basically nutrition labels datasets. process creating enables us think carefully feed model. importantly, enable others better understand fed model. One important task going back putting together datasheets datasets widely used. instance, researchers went back wrote datasheet one popular datasets computer science, found around 30 per cent data duplicated (Bandy Vincent 2021).Instead telling us unhealthy various foods , datasheet tells us things like:put dataset together?paid dataset created?complete dataset?fields present, equally present, particular observations?Sometimes done lot work create dataset. case, may like publish share , instance, Biderman, Bicheno, Gao (2022) Bandy Vincent (2021). typically datasheet might live appendix paper, included file adjacent dataset.put together datasheet dataset underpins R. Alexander Hodgetts (2021). text questions directly comes Gebru et al. (2021). create datasheets dataset, especially dataset put together , possible answer questions simply “Unknown”.MotivationFor purpose dataset created? specific task mind? specific gap needed filled? Please provide description.\ndataset created enable analysis Australian politicians. unable find publicly available dataset structured format biographical political information Australian politicians needed modelling.\ndataset created enable analysis Australian politicians. unable find publicly available dataset structured format biographical political information Australian politicians needed modelling.created dataset (example, team, research group) behalf entity (example, company, institution, organization)?\nRohan Alexander, working Australian National University University Toronto\nRohan Alexander, working Australian National University University TorontoWho funded creation dataset? associated grant, please provide name grantor grant name number.\ndirect funding received project, Rohan received salary University Toronto.\ndirect funding received project, Rohan received salary University Toronto.comments?\n.\n.CompositionWhat instances comprise dataset represent (example, documents, photos, people, countries)? multiple types instances (example, movies, users, ratings; people interactions ; nodes edges)? Please provide description.\nrow main dataset individual, link datasets row refers various information person.\nrow main dataset individual, link datasets row refers various information person.many instances total (type, appropriate)?\nlittle 1700.\nlittle 1700.dataset contain possible instances sample (necessarily random) instances larger set? dataset sample, larger set? sample representative larger set (example, geographic coverage)? , please describe representativeness validated/verified. representative larger set, please describe (example, cover diverse range instances, instances withheld unavailable).\nindividuals elected appointed Australian Federal Parliament dataset.\nindividuals elected appointed Australian Federal Parliament dataset.data instance consist ? “Raw” data (example, unprocessed text images) features? either case, please provide description.\ninstance consists biographical information birthdate, political information, political party membership.\ninstance consists biographical information birthdate, political information, political party membership.label target associated instance? , please provide description.\nYes unique key comprising surname year birth, individuals needing additional demarcation.\nYes unique key comprising surname year birth, individuals needing additional demarcation.information missing individual instances? , please provide description, explaining information missing (example, unavailable). include intentionally removed information, might include, example, redacted text.\nBirthdate available cases, especially earlier dataset.\nBirthdate available cases, especially earlier dataset.relationships individual instances made explicit (example, users’ movie ratings, social network links)? , please describe relationships made explicit.\nYes, uniqueID.\nYes, uniqueID.recommended data splits (example, training, development/validation, testing)? , please provide description splits, explaining rationale behind .\n.\n.errors, sources noise, redundancies dataset? , please provide description.\nuncertainty cabinet ministries. instance, different sources differ. also little bit uncertainty birthdates.\nuncertainty cabinet ministries. instance, different sources differ. also little bit uncertainty birthdates.dataset self-contained, link otherwise rely external resources (example, websites, tweets, datasets)? links relies external resources, ) guarantees exist, remain constant, time; b) official archival versions complete dataset (, including external resources existed time dataset created); c) restrictions (example, licenses, fees) associated external resources might apply dataset consumer? Please provide descriptions external resources restrictions associated , well links access points, appropriate.\nSelf-contained.\nSelf-contained.dataset contain data might considered confidential (example, data protected legal privilege doctor-patient confidentiality, data includes content individuals’ non-public communications)? , please provide description.\n, data gathered public sources.\n, data gathered public sources.dataset contain data , viewed directly, might offensive, insulting, threatening, might otherwise cause anxiety? , please describe .\n.\n.dataset identify sub-populations (example, age, gender)? , please describe subpopulations identified provide description respective distributions within dataset.\nYes, age gender.\nYes, age gender.possible identify individuals (, one natural persons), either directly indirectly (, combination data) dataset? , please describe .\nYes, individuals identified name.\nYes, individuals identified name.dataset contain data might considered sensitive way (example, data reveals race ethnic origins, sexual orientations, religious beliefs, political opinions union memberships, locations; financial health data; biometric genetic data; forms government identification, social security numbers; criminal history)? , please provide description.\ndataset contains sensitive information, political membership, however public knowledge federal politicians.\ndataset contains sensitive information, political membership, however public knowledge federal politicians.comments?\n.\n.Collection processHow data associated instance acquired? data directly observable (example, raw text, movie ratings), reported subjects (example, survey responses), indirectly inferred/derived data (example, part--speech tags, model-based guesses age language)? data reported subjects indirectly inferred/derived data, data validated/verified? , please describe .\ndata gathered Australian Parliamentary Handbook first instance, augmented information parliaments, especially Victoria New South Wales, Wikipedia.\ndata gathered Australian Parliamentary Handbook first instance, augmented information parliaments, especially Victoria New South Wales, Wikipedia.mechanisms procedures used collect data (example, hardware apparatuses sensors, manual human curation, software programs, software APIs)? mechanisms procedures validated?\nScraping parsing using R.\nScraping parsing using R.dataset sample larger set, sampling strategy (example, deterministic, probabilistic specific sampling probabilities)?\ndataset sample.\ndataset sample.involved data collection process (example, students, crowdworkers, contractors) compensated (example, much crowdworkers paid)?\nRohan Alexander. Paid post-doc assistant professor, although tied specific project.\nRohan Alexander. Paid post-doc assistant professor, although tied specific project.timeframe data collected? timeframe match creation timeframe data associated instances (example, recent crawl old news articles)? , please describe timeframe data associated instances created.\nThree years, updated time time.\nThree years, updated time time.ethical review processes conducted (example, institutional review board)? , please provide description review processes, including outcomes, well link access point supporting documentation.\n.\n.collect data individuals question directly, obtain via third parties sources (example, websites)?\nThird parties almost cases.\nThird parties almost cases.individuals question notified data collection? , please describe (show screenshots information) notice provided, provide link access point , otherwise reproduce, exact language notification .\n.\n.individuals question consent collection use data? , please describe (show screenshots information) consent requested provided, provide link access point , otherwise reproduce, exact language individuals consented.\n.\n.consent obtained, consenting individuals provided mechanism revoke consent future certain uses? , please provide description, well link access point mechanism (appropriate).\nConsent obtained.\nConsent obtained.analysis potential impact dataset use data subjects (example, data protection impact analysis) conducted? , please provide description analysis, including outcomes, well link access point supporting documentation.\n.\n.comments?\n.\n.Preprocessing/cleaning/labelingWas preprocessing/cleaning/labeling data done (example, discretization bucketing, tokenization, part--speech tagging, SIFT feature extraction, removal instances, processing missing values)? , please provide description. , may skip remaining questions section.\nYes cleaning data done.\nYes cleaning data done.“raw” data saved addition preprocessed/cleaned/labeled data (example, support unanticipated future uses)? , please provide link access point “raw” data.\ngeneral, . scripts got data parliamentary handbook CSV available. scripts go Wikipedia check things available.\ngeneral, . scripts got data parliamentary handbook CSV available. scripts go Wikipedia check things available.software used preprocess/clean/label data available? , please provide link access point.\nR used.\nR used.comments?\n\nNoUsesHas dataset used tasks already? , please provide description.\nYes, papers Australian politics, instance, https://arxiv.org/abs/2111.09299.\nYes, papers Australian politics, instance, https://arxiv.org/abs/2111.09299.repository links papers systems use dataset? , please provide link access point.\n\nNoWhat () tasks dataset used ?\nLinking elections interesting.\nLinking elections interesting.anything composition dataset way collected preprocessed/cleaned/labeled might impact future uses? example, anything dataset consumer might need know avoid uses result unfair treatment individuals groups (example, stereotyping, quality service issues) risks harms (example, legal risks, financial harms)? , please provide description. anything dataset consumer mitigate risks harms?\n.\n.tasks dataset used? , please provide description.\n.\n.comments?\n.\n.DistributionWill dataset distributed third parties outside entity (example, company, institution, organization) behalf dataset created? , please provide description.\ndataset available GitHub.\ndataset available GitHub.dataset distributed (example, tarball website, API, GitHub)? dataset digital object identifier (DOI)?\nGitHub now, eventually deposit.\nGitHub now, eventually deposit.dataset distributed?\ndataset available now.\ndataset available now.dataset distributed copyright intellectual property (IP) license, /applicable terms use (ToU)? , please describe license / ToU, provide link access point , otherwise reproduce, relevant licensing terms ToU, well fees associated restrictions.\n. MIT license.\n. MIT license.third parties imposed IP-based restrictions data associated instances? , please describe restrictions, provide link access point , otherwise reproduce, relevant licensing terms, well fees associated restrictions.\nNone known.\nNone known.export controls regulatory restrictions apply dataset individual instances? , please describe restrictions, provide link access point , otherwise reproduce, supporting documentation.\nNone known.\nNone known.comments?\n.\n.MaintenanceWho supporting/hosting/maintaining dataset?\nRohan Alexander\nRohan AlexanderHow can owner/curator/manager dataset contacted (example, email address)?\nrohan.alexander@utoronto\nrohan.alexander@utorontoIs erratum? , please provide link access point.\n, dataset just updated.\n, dataset just updated.dataset updated (example, correct labeling errors, add new instances, delete instances)? , please describe often, , updates communicated dataset consumers (example, mailing list, GitHub)?\nYes, roughly quarterly.\nYes, roughly quarterly.dataset relates people, applicable limits retention data associated instances (example, individuals question told data retained fixed period time deleted)? , please describe limits explain enforced.\n.\n.older versions dataset continue supported/hosted/maintained? , please describe . , please describe obsolescence communicated dataset consumers.\ndataset just updated. Although history available GitHub.\ndataset just updated. Although history available GitHub.others want extend/augment/build /contribute dataset, mechanism ? , please provide description. contributions validated/verified? , please describe . , ? process communicating/distributing contributions dataset consumers? , please provide description.\nPull request GitHub.\nPull request GitHub.comments?\n\n","code":""},{"path":"store-and-share.html","id":"personally-identifying-information","chapter":"12 Store and share","heading":"12.5 Personally identifying information","text":"Personally identifying information (PII) enables us link row dataset actual person. instance, email addresses often PII, names addresses. Interestingly, sometimes combination several variables, none PII , can PII. instance, age unlikely PII , age combined city, education, variables . One concern re-identification can occur across datasets. Another interesting aspect variable may PII almost everyone dataset, can become PII extreme. instance, age, many people ages, fewer people ages 100 likely becomes PII. scenario happens income wealth. One response data censored. instance, may record age 0 100, group everyone ‘101+’.Zook et al. (2017) recommend considering whether data even need gathered first place. instance, phone number absolutely required might better gather first place, rather need worry protecting data dissemination.GDPR HIPAA two legal structures govern data Europe US, respectively. Due influence regions, significant effect outside regions also. GDPR concerns data generally, HIPAA focused healthcare. GDPR applies personal data, defined :‘personal data’ means information relating identified identifiable natural person (‘data subject’); identifiable natural person one can identified, directly indirectly, particular reference identifier name, identification number, location data, online identifier one factors specific physical, physiological, genetic, mental, economic, cultural social identity natural person;Council European Union (2016), Article 4, ‘Definitions’HIPAA refers privacy medical records US codify idea patient access medical records, patient able authorize access medical records (Annas 2003). applies covered entities, sets standard informs practice, yet piecemeal given variety applications. instance, person’s social media posts health generally subject , knowledge person’s location active , even though based information may able get idea health (Cohen Mello 2018). data hugely valuable (Ross 2022).variety ways protecting PII, sharing data, now go .","code":""},{"path":"store-and-share.html","id":"hashing-and-salting","chapter":"12 Store and share","heading":"12.5.1 Hashing and salting","text":"hash one-way transformation data, input always provides output, given output, reasonably possible obtain input. instance, function doubled input always gives output, input, also easy reverse.Knuth (1998, 514) relates interesting etymology ‘hash’ first defining ‘hash’ relating chop make mess, explaining hashing relates scrambling input using partial information define output. collision different inputs map output, one feature good hashing algorithm collisions reduced. One simple approach rely modulo operator. instance, interested 10 different groupings integers 1 10. better approach number groupings prime number, 11 853.Rather worry things , can use various hash functions openssl (Ooms 2021) including sha512() md5().share either comfortable , general, difficult someone use information recover names respondents. say impossible. made mistake, accidentally committing original dataset GitHub recovered. course, likely various governments ability reverse cryptographic hashes used .One issue remains anyone can take advantage key feature hashing input always gets output, test various options inputs. instance, , try ‘Rohan’, noticing hash one published dataset, know data relates particular individual. try keep hashing approach secret, difficult widely used. One approach add salt keep secret. slightly changes input. instance, add salt ’_is_a_person’ names hash , although large random number might better option. Provided salt shared, difficult folks reverse approach way.","code":"\nlibrary(tidyverse)\n\nhashing <- \n  tibble(ppi_data = c(1:10),\n         modulo_ten = ppi_data %% 10,\n         modulo_eleven = ppi_data %% 11,\n         modulo_eightfivethree = ppi_data %% 853)\n\nhashing\n#> # A tibble: 10 × 4\n#>    ppi_data modulo_ten modulo_eleven modulo_eightfivethree\n#>       <int>      <dbl>         <dbl>                 <dbl>\n#>  1        1          1             1                     1\n#>  2        2          2             2                     2\n#>  3        3          3             3                     3\n#>  4        4          4             4                     4\n#>  5        5          5             5                     5\n#>  6        6          6             6                     6\n#>  7        7          7             7                     7\n#>  8        8          8             8                     8\n#>  9        9          9             9                     9\n#> 10       10          0            10                    10\nlibrary(openssl)\n\nopenssl_hashing <- \n  tibble(names =\n            c(\"Edward\",\n              \"Helen\",\n              \"Hugo\",\n              \"Ian\",\n              \"Monica\",\n              \"Myles\",\n              \"Patricia\",\n              \"Roger\",\n              \"Rohan\",\n              \"Ruth\"\n            )) |> \n  mutate(md5 = md5(names),\n         sha512 = sha512(names)\n  )\n\nopenssl_hashing\n#> # A tibble: 10 × 3\n#>    names    md5                              sha512         \n#>    <chr>    <hash>                           <hash>         \n#>  1 Edward   243f63354f4c1cc25d50f6269b844369 5759ada975e7cb…\n#>  2 Helen    29e00d3659d1c5e75f99e892f0c1a1f1 6ee4156ca7e8e9…\n#>  3 Hugo     1b3840b0b70d91c17e70014c8537dbba b1e441a5486690…\n#>  4 Ian      245a58a5dc42397caf57bc06c2c0afd2 d3cf9cdaea6ffd…\n#>  5 Monica   09084cc0cda34fd80bfa3cc0ae8fe3dc 84250b971b8772…\n#>  6 Myles    fafdf519cb5877d4751b4cbe6f3f534a 4eae7c19d5c5d4…\n#>  7 Patricia 54a7b18f26374fc200ddedde0844f8ec e511593a2db805…\n#>  8 Roger    efc5c58b9a85926a31587140cbeb0220 f63ab236a5b013…\n#>  9 Rohan    02df8936eee3d4d2568857ed530671b2 5111e18391d41f…\n#> 10 Ruth     8e06843ec162b74a7902867dd4bca8c8 d7e7d23b69e372…\nopenssl_hashing_with_salt <- \n  tibble(names =\n            c(\"Edward\",\n              \"Helen\",\n              \"Hugo\",\n              \"Ian\",\n              \"Monica\",\n              \"Myles\",\n              \"Patricia\",\n              \"Roger\",\n              \"Rohan\",\n              \"Ruth\"\n            )\n         ) |> \n  mutate(names = paste0(names, \"_is_a_person\")) |> \n  mutate(md5 = md5(names),\n         sha512 = sha512(names)\n         )\n\nopenssl_hashing_with_salt\n#> # A tibble: 10 × 3\n#>    names                md5                           sha512\n#>    <chr>                <hash>                        <hash>\n#>  1 Edward_is_a_person   9845500d4070c0cbba7c6b81ed30… e8ce0…\n#>  2 Helen_is_a_person    7e4a77b41fb6e108618f93fb9f47… b066b…\n#>  3 Hugo_is_a_person     b9b8c4e9870aca482cf062da4681… 07c18…\n#>  4 Ian_is_a_person      9b1ad8fbbc190c2e3ce74372029f… eb992…\n#>  5 Monica_is_a_person   50bb9dfffa926c855b830845ac61… ef429…\n#>  6 Myles_is_a_person    3635be5fe758ed1fc0d9c78fc0b2… 795dc…\n#>  7 Patricia_is_a_person 4e4a5ed8842fd7caad320c3a92bf… 1b446…\n#>  8 Roger_is_a_person    ea1d56e89771d8b0a7b598132442… 2f753…\n#>  9 Rohan_is_a_person    3ab064d7f746fde604122d072fd4… cc9c7…\n#> 10 Ruth_is_a_person     8b83f4285ac30a3efa5ede3636b7… 6d329…"},{"path":"store-and-share.html","id":"data-simulation","chapter":"12 Store and share","heading":"12.5.2 Data simulation","text":"One common approach deal issue unable share actual data underpins analysis, use data simulation. used data simulation throughout book toward start workflow help us think deeply dataset turn . can use data simulation end, ensure others think actual dataset. workflow advocated book makes relatively straight-forward.approach understand critical features dataset appropriate distribution. instance, data ages population, may want use Poisson distribution experiment different parameters lambda. simulated dataset, conduct analysis using simulated dataset ensure results broadly similar use real data. can release simulated dataset along code.nuanced situations, Koenecke Varian (2020) recommend using synthetic data vault (Patki, Wedge, Veeramachaneni 2016) use Generative Adversarial Networks, implemented Athey et al. (2021).","code":""},{"path":"store-and-share.html","id":"differential-privacy","chapter":"12 Store and share","heading":"12.5.3 Differential privacy","text":"Differential privacy implements mathematical definition privacy, means even datasets combined, certain level privacy maintained. dataset differentially private different levels, based much changes one person’s results removed.variant differential privacy recently implemented US census. shown universally protect respondent privacy, yet expected significant effect redistricting (Kenny et al. 2021). Suriyakumar et al. (2021) found model disproportionately affected large demographic groups. implementation differential privacy expected result publicly available data unusable social sciences (Ruggles et al. 2019).","code":""},{"path":"store-and-share.html","id":"exercises-and-tutorial-11","chapter":"12 Store and share","heading":"12.6 Exercises and tutorial","text":"","code":""},{"path":"store-and-share.html","id":"exercises-11","chapter":"12 Store and share","heading":"12.6.1 Exercises","text":"Following M. D. Wilkinson et al. (2016), following FAIR principles (please select apply)?\nFindable.\nApproachable.\nInteroperable.\nReusable.\nIntegrated.\nFungible.\nReduced.\nAccessible.\nFindable.Approachable.Interoperable.Reusable.Integrated.Fungible.Reduced.Accessible.Please create R package simulated dataset, push GitHub, submit link.Please simulate data, add GitHub repository submit link.According Gebru et al. (2021), datasheet document dataset’s (please select apply):\ncomposition.\nrecommended uses.\nmotivation.\ncollection process.\ncomposition.recommended uses.motivation.collection process.think person’s name PII?Yes..circumstances think income PII (please write paragraph two)?Using openssl::md5() hash “Rohan” (pick one)?\n243f63354f4c1cc25d50f6269b844369\n09084cc0cda34fd80bfa3cc0ae8fe3dc\n02df8936eee3d4d2568857ed530671b2\n1b3840b0b70d91c17e70014c8537dbba\n243f63354f4c1cc25d50f6269b84436909084cc0cda34fd80bfa3cc0ae8fe3dc02df8936eee3d4d2568857ed530671b21b3840b0b70d91c17e70014c8537dbba","code":""},{"path":"store-and-share.html","id":"tutorial-11","chapter":"12 Store and share","heading":"12.6.2 Tutorial","text":"Please identify dataset consider interesting important, datasheet (Gebru et al. 2021). reminder, datasheets accompany datasets document ‘motivation, composition, collection process, recommended uses,’ among aspects. Please put together datasheet dataset. welcome use template starting point. datasheet completely contained GitHub repository. Please submit PDF.","code":""},{"path":"store-and-share.html","id":"paper-3","chapter":"12 Store and share","heading":"12.6.3 Paper","text":"point, Paper Four (Appendix B.4) appropriate.","code":""},{"path":"exploratory-data-analysis.html","id":"exploratory-data-analysis","chapter":"13 Exploratory data analysis","heading":"13 Exploratory data analysis","text":"Required materialRead Future Data Analysis, Part 1 ‘General Considerations’, (Tukey 1962).Key concepts skillsQuickly coming terms new dataset constructing graphs tables.Understanding issues features dataset may affect analysis decisions.Thinking missing values outliers.Key librariesbroomggrepelherejanitorlubridateopendatatorontotidymodelstidyversevisdatKey functionsaugment()clean_names()coord_flip()count()distinct()facet_grid()facet_wrap()geom_bar()geom_col()geom_density()geom_histogram()geom_line()geom_point()geom_smooth()geom_text_repel()get_dupes()glance()if_else()ifelse()initial_split()left_join()mutate()mutate_all()names()ncol()nrow()pivot_wider()scale_color_brewer()scale_fill_brewer()scale_x_log10()scale_y_log10()str_detect()str_extract()str_remove()str_split()str_starts()summarise()summarise_all()theme_classic()theme_minimal()vis_dat()vis_miss()","code":""},{"path":"exploratory-data-analysis.html","id":"introduction-10","chapter":"13 Exploratory data analysis","heading":"13.1 Introduction","text":"future data analysis can involve great progress, overcoming real difficulties, provision great service fields science technology. ? remains us, willingness take rocky road real problems preference smooth road unreal assumptions, arbitrary criteria, abstract results without real attachments. challenge?Tukey (1962, 64).Exploratory data analysis never finished, just die. active process exploring becoming familiar data. Like farmer hands earth, need know every contour aspect data. need know changes, shows, hides, limits. Exploratory data analysis (EDA) unstructured process .EDA means end. inform entire paper, especially data section, typically something ends final paper.\nway proceed make separate R Markdown file, add code well brief notes --go. delete previous code, just add . end created useful notebook captures exploration dataset. document guide subsequent analysis modelling.EDA draws variety skills lot options EDA (Staniak Biecek 2019). Every tool considered. Look data scroll . Make tables, plots, summary statistics, even models. key iterate, move quickly rather perfectly, come thorough understanding data.chapter go various examples EDA including TTC subway delays, Airbnb.\n","code":""},{"path":"exploratory-data-analysis.html","id":"case-study-ttc-subway-delays","chapter":"13 Exploratory data analysis","heading":"13.2 Case study: TTC subway delays","text":"can use opendatatoronto (Gelfand 2020) tidyverse (Wickham et al. 2019a) obtain data Toronto subway system, especially delays occurred. idea case study comes Monica Alexander.begin, download data Toronto Transit Commission (TTC) subway delays 2020. data available separate dataset month. interested 2020, create column year, filter resources just months 2020. download using get_resource(), iterating month using map_dfr purrr (Henry Wickham 2020) brings twelve datasets together, save .\nsave .dataset variety columns, can find downloading codebook. reason delay coded, can also download explanations. One particular variable interest appears ‘min_delay’, gives extent delay minutes.one way explore dataset conducting EDA, usually especially interested :variables look like? instance, type, values, distribution look like?aspects surprising, terms data expect, outliers, also terms data may expect missing data.Developing goal analysis. instance, case, might understanding factors stations time day, associated delays.important document aspects go make note anything surprising. looking create record steps assumptions made going important come modelling.","code":"\nlibrary(janitor)\nlibrary(opendatatoronto)\nlibrary(tidyverse)\n\n# We know this unique key by looking the 'id' of the interest.\nttc_resources <- \n  list_package_resources(\"996cfe8d-fb35-40ce-b569-698d51fc683b\") |>\n  mutate(year = str_extract(name, \"20..?\")) |>  \n  filter(year == 2020)\n\nall_2020_ttc_data <- \n  map_dfr(ttc_resources$id, get_resource)\n\nall_2020_ttc_data <- clean_names(all_2020_ttc_data)\n\nwrite_csv(all_2020_ttc_data, \"all_2020_ttc_data.csv\")\n\nall_2020_ttc_data\n# Data codebook\ndelay_data_codebook <- get_resource(\"54247e39-5a7d-40db-a137-82b2a9ab0708\")\ndelay_data_codebook <- clean_names(delay_data_codebook)\nwrite_csv(delay_data_codebook, \"delay_data_codebook.csv\")\n\n# Explanation for delay codes\ndelay_codes <- get_resource(\"fece136b-224a-412a-b191-8d31eb00491e\")\ndelay_codes <- clean_names(delay_codes)\nwrite_csv(delay_codes, \"delay_codes.csv\")"},{"path":"exploratory-data-analysis.html","id":"checking-data","chapter":"13 Exploratory data analysis","heading":"13.2.1 Checking data","text":"check variables say . , need work , instance, recode , even remove ? also important ensure class variables expect, instance variables factor factor character character. also accidentally , say, factors numbers, vice versa. One way use unique(), another use table().clear likely issues terms lines. clear re-code, . One option drop , need think whether errors might correlated something interest, may dropping important information. usually one right answer, usually depend using data . note issue, continued EDA decide later . now, remove lines ones know correct.Exploring missing data course , presence, lack, missing values can haunt analysis. get started look known-unknowns, NAs variable. vis_dat() vis_miss() visdat (N. Tierney 2017) can useful get feel missing values distributed.case many missing values ‘bound’ two ‘line’. known-unknowns, interested whether missing random. want , ideally, show data happened just drop . unlikely, usually trying look systematic data missing.Sometime data happen duplicated. notice analysis wrong ways ’d able consistently expect. variety ways look duplicated rows, get_dupes() janitor (Firke 2020) especially useful.dataset many duplicates. , interested whether something systematic going . Remembering EDA trying quickly come terms dataset, one way forward flag issue come back explore later, just remove duplicates now using distinct().station names mess. try quickly bring little order chaos just taking just first word (, first two starts ‘ST’).","code":"\nunique(all_2020_ttc_data$day)\n#> [1] \"Wednesday\" \"Thursday\"  \"Friday\"    \"Saturday\" \n#> [5] \"Sunday\"    \"Monday\"    \"Tuesday\"\n\nunique(all_2020_ttc_data$line)\n#>  [1] \"SRT\"                    \"YU\"                    \n#>  [3] \"BD\"                     \"SHP\"                   \n#>  [5] \"YU/BD\"                  \"YU / BD\"               \n#>  [7] \"999\"                    NA                      \n#>  [9] \"29 DUFFERIN\"            \"95 YORK MILLS\"         \n#> [11] \"35 JANE\"                \"YU-BD\"                 \n#> [13] \"BLOOR - DANFORTH\"       \"YU/BD LINE\"            \n#> [15] \"YUS\"                    \"YUS/BD\"                \n#> [17] \"40 JUNCTION-DUNDAS WES\" \"71 RUNNYMEDE\"          \n#> [19] \"BD/YU\"                  \"102 MARKHAM ROAD\"      \n#> [21] \"YUS/DB\"                 \"YU & BD\"               \n#> [23] \"SHEP\"\n\ntable(all_2020_ttc_data$day)\n#> \n#>    Friday    Monday  Saturday    Sunday  Thursday   Tuesday \n#>      2174      2222      1867      1647      2353      2190 \n#> Wednesday \n#>      2329\n\ntable(all_2020_ttc_data$line)\n#> \n#>       102 MARKHAM ROAD            29 DUFFERIN \n#>                      1                      1 \n#>                35 JANE 40 JUNCTION-DUNDAS WES \n#>                      1                      1 \n#>           71 RUNNYMEDE          95 YORK MILLS \n#>                      1                      1 \n#>                    999                     BD \n#>                      2                   5473 \n#>                  BD/YU       BLOOR - DANFORTH \n#>                      1                      1 \n#>                   SHEP                    SHP \n#>                      1                    619 \n#>                    SRT                     YU \n#>                    644                   7620 \n#>                YU / BD                YU & BD \n#>                     22                      2 \n#>                  YU-BD                  YU/BD \n#>                      1                    338 \n#>             YU/BD LINE                    YUS \n#>                      1                      2 \n#>                 YUS/BD                 YUS/DB \n#>                      1                      1\nall_2020_ttc_data <- \n  all_2020_ttc_data |> \n  filter(line %in% c(\"BD\", \"YU\", \"SHP\", \"SRT\"))\nlibrary(visdat)\n\nall_2020_ttc_data |>\n  summarise_all(list( ~ sum(is.na(.))))\n#> # A tibble: 1 × 10\n#>    date  time   day station  code min_delay min_gap bound\n#>   <int> <int> <int>   <int> <int>     <int>   <int> <int>\n#> 1     0     0     0       0     0         0       0  3272\n#> # … with 2 more variables: line <int>, vehicle <int>\n\nvis_dat(x = all_2020_ttc_data,\n         palette = \"cb_safe\"\n         )\n\nvis_miss(all_2020_ttc_data)\nget_dupes(all_2020_ttc_data)\n#> No variable names specified - using all columns.\n#> # A tibble: 37 × 11\n#>    date                time   day    station code  min_delay\n#>    <dttm>              <time> <chr>  <chr>   <chr>     <dbl>\n#>  1 2020-02-10 00:00:00 06:00  Monday TORONT… MRO           0\n#>  2 2020-02-10 00:00:00 06:00  Monday TORONT… MRO           0\n#>  3 2020-02-10 00:00:00 06:00  Monday TORONT… MUO           0\n#>  4 2020-02-10 00:00:00 06:00  Monday TORONT… MUO           0\n#>  5 2020-03-10 00:00:00 23:00  Tuesd… YORK M… MUO           0\n#>  6 2020-03-10 00:00:00 23:00  Tuesd… YORK M… MUO           0\n#>  7 2020-03-26 00:00:00 13:20  Thurs… VAUGHA… MUNOA         3\n#>  8 2020-03-26 00:00:00 13:20  Thurs… VAUGHA… MUNOA         3\n#>  9 2020-03-26 00:00:00 18:32  Thurs… VAUGHA… MUNOA         3\n#> 10 2020-03-26 00:00:00 18:32  Thurs… VAUGHA… MUNOA         3\n#> # … with 27 more rows, and 5 more variables: min_gap <dbl>,\n#> #   bound <chr>, line <chr>, vehicle <dbl>,\n#> #   dupe_count <int>\nall_2020_ttc_data <- \n  all_2020_ttc_data |> \n  distinct()\nall_2020_ttc_data <-\n  all_2020_ttc_data |>\n  mutate(station_clean = if_else(str_starts(station, \"ST\"), \n                                 word(station, 1, 2), \n                                 word(station, 1)))"},{"path":"exploratory-data-analysis.html","id":"visualizing-data","chapter":"13 Exploratory data analysis","heading":"13.2.2 Visualizing data","text":"need see data original state understand use bar charts, scatterplots, line plots histograms extensively . EDA concerned whether graph aesthetically pleasing, instead trying acquire sense data quickly possible. can start looking distribution ‘min_delay’, one outcome interest.largely empty graph suggests presence outliers. variety ways try understand going , one quick way proceed use log, remembering expect values 0 drop away.initial exploration hints issue might like explore . join dataset ‘delay_codes’ understand going .can see 450 minute delay due ‘Transit Control Related Problems’, 446 minute delay due ‘Miscellaneous ’, seem outliers, even among outliers.","code":"\nall_2020_ttc_data |>\n  ggplot(aes(x = min_delay)) + \n  geom_bar()\nall_2020_ttc_data |>\n  ggplot(aes(x = min_delay)) + \n  geom_bar() +\n  scale_x_log10()\nall_2020_ttc_data <-\n  all_2020_ttc_data |>\n  left_join(\n    delay_codes |>\n      rename(code = sub_rmenu_code, \n             code_desc = code_description_3) |>\n      select(code, code_desc),\n    by = \"code\"\n  )\n\nall_2020_ttc_data <-\n  all_2020_ttc_data |>\n  mutate(code_srt = ifelse(line == \"SRT\", code, \"NA\")) |>\n  left_join(\n    delay_codes |>\n      rename(code_srt = sub_rmenu_code, code_desc_srt = code_description_7) |>\n      select(code_srt, code_desc_srt),\n    by = \"code_srt\"\n  ) |>\n  mutate(\n    code = ifelse(code_srt == \"NA\", code, code_srt),\n    code_desc = ifelse(is.na(code_desc_srt), code_desc, code_desc_srt)\n  ) |>\n  select(-code_srt,-code_desc_srt)\nall_2020_ttc_data |> \n  left_join(delay_codes |> \n              rename(code = sub_rmenu_code, code_desc = code_description_3) |>\n              select(code, code_desc)) |> \n  arrange(-min_delay) |> \n  select(date, time, station, line, min_delay, code, code_desc)\n#> Joining, by = c(\"code\", \"code_desc\")\n#> # A tibble: 14,335 × 7\n#>    date                time   station  line  min_delay code \n#>    <dttm>              <time> <chr>    <chr>     <dbl> <chr>\n#>  1 2020-02-13 00:00:00 05:30  ST GEOR… YU          450 TUCC \n#>  2 2020-05-08 00:00:00 16:16  ST CLAI… YU          446 MUO  \n#>  3 2020-01-22 00:00:00 05:57  KEELE S… BD          258 EUTR \n#>  4 2020-03-19 00:00:00 11:26  ROYAL Y… BD          221 MUPR1\n#>  5 2020-11-12 00:00:00 23:10  SHEPPAR… YU          197 PUCSC\n#>  6 2020-12-13 00:00:00 21:37  MCCOWAN… SRT         167 PRSP \n#>  7 2020-12-04 00:00:00 16:23  MCCOWAN… SRT         165 PRSP \n#>  8 2020-01-18 00:00:00 05:48  SCARBOR… SRT         162 PRSL \n#>  9 2020-02-22 00:00:00 05:16  SPADINA… YU          159 PUSWZ\n#> 10 2020-09-03 00:00:00 14:35  CASTLE … BD          150 MUPR1\n#> # … with 14,325 more rows, and 1 more variable:\n#> #   code_desc <chr>"},{"path":"exploratory-data-analysis.html","id":"groups-of-small-counts","chapter":"13 Exploratory data analysis","heading":"13.2.3 Groups of small counts","text":"Another thing looking various groupings data, especially sub-groups may end small numbers observations . analysis especially influenced . One quick way group data variable interest, instance ‘line’, using color.\nFigure 13.1: Density distribution delay, minutes\nFigure 13.1 uses density can look distributions comparably, also aware differences frequency (Figure 13.2). case, see ‘SHP’ ‘SRT’ much smaller counts.\nFigure 13.2: Frequency distribution delay, minutes\ngroup another variable can add facets (Figure 13.3).\nFigure 13.3: Frequency distribution delay, minutes, day\ncan now plot top five stations mean delay.","code":"\nggplot(data = all_2020_ttc_data) + \n  geom_histogram(aes(x = min_delay, y = ..density.., fill = line), \n                 position = 'dodge', \n                 bins = 10) + \n  scale_x_log10()\nggplot(data = all_2020_ttc_data) + \n  geom_histogram(aes(x = min_delay, fill = line), \n                 position = 'dodge', \n                 bins = 10) + \n  scale_x_log10()\nggplot(data = all_2020_ttc_data) + \n  geom_density(aes(x = min_delay, color = line), \n               bw = .08) + \n  scale_x_log10() + \n  facet_wrap(vars(day))\nall_2020_ttc_data |> \n  group_by(line, station_clean) |> \n  summarise(mean_delay = mean(min_delay), n_obs = n()) |> \n  filter(n_obs>1) |> \n  arrange(line, -mean_delay) |> \n  slice(1:5) |> \n  ggplot(aes(station_clean, mean_delay)) + \n    geom_col() + \n    coord_flip() + \n    facet_wrap(vars(line), \n               scales = \"free_y\")\n#> `summarise()` has grouped output by 'line'. You can override\n#> using the `.groups` argument."},{"path":"exploratory-data-analysis.html","id":"dates","chapter":"13 Exploratory data analysis","heading":"13.2.4 Dates","text":"Dates often difficult work prone issues. reason, especially important consider EDA. create graph week, see seasonality. using dates, lubridate (Grolemund Wickham 2011) especially useful. instance, can look average delay, delayed, week drawing week() construct weeks (Figure 13.4).\nFigure 13.4: Average delay, minutes, week, Toronto subway\nNow let us look proportion delays greater 10 minutes (Figure 13.5).\nFigure 13.5: Delays longer ten minutes, week, Toronto subway\nfigures, tables, analysis place final paper. Instead, allow us become comfortable data. note aspects stand , well warnings implications aspects return .","code":"\nlibrary(lubridate)\n\nall_2020_ttc_data |>\n  filter(min_delay > 0) |>\n  mutate(week = week(date)) |>\n  group_by(week, line) |>\n  summarise(mean_delay = mean(min_delay)) |>\n  ggplot(aes(week, mean_delay, color = line)) +\n  geom_point() +\n  geom_smooth() +\n  facet_wrap(vars(line),\n              scales = \"free_y\"\n             )\nall_2020_ttc_data |> \n  mutate(week = week(date)) |> \n  group_by(week, line) |> \n  summarise(prop_delay = sum(min_delay>10)/n()) |> \n  ggplot(aes(week, prop_delay, color = line)) + \n    geom_point() + \n    geom_smooth() + \n      facet_wrap(vars(line),\n              scales = \"free_y\"\n             )"},{"path":"exploratory-data-analysis.html","id":"relationships","chapter":"13 Exploratory data analysis","heading":"13.2.5 Relationships","text":"also interested looking relationship two variables. Scatter plots especially useful continuous variables, good precursor modeling. instance, may interested relationship delay gap (Figure 13.6).\nFigure 13.6: Relationship delay gap Toronto subway 2020\nrelationship categorical variables takes work, also, instance, look top five reasons delay station. particular, may interested whether differ, difference modeled (Figure 13.7).\nFigure 13.7: Relationship categorical variables Toronto subway 2020\nPrincipal components analysis (PCA) another powerful exploratory tool. allows us pick potential clusters outliers can help inform modeling. see , can look types delay station. delay categories messy lot , trying come terms dataset, just take first word.Let us also just restrict analysis causes happen least 50 times 2019. PCA, dataframe also needs switched wide format.Now can quickly PCA.can plot first two principal components, add labels outlying stations.also plot factor loadings. se evidence perhaps one public, compared another operator.","code":"\nall_2020_ttc_data |>\n  ggplot(aes(x = min_delay, y = min_gap)) +\n  geom_point() +\n  scale_x_log10() +\n  scale_y_log10()\nall_2020_ttc_data |>\n  group_by(line, code_desc) |>\n  summarise(mean_delay = mean(min_delay)) |>\n  arrange(-mean_delay) |>\n  slice(1:5) |>\n  ggplot(aes(x = code_desc,\n             y = mean_delay)) +\n  geom_col() + \n  facet_wrap(vars(line), \n             scales = \"free_y\",\n             nrow = 4) +\n  coord_flip()\nall_2020_ttc_data <-\n  all_2020_ttc_data |>\n  mutate(code_red = case_when(\n    str_starts(code_desc, \"No\") ~ word(code_desc, 1, 2),\n    str_starts(code_desc, \"Operator\") ~ word(code_desc, 1, 2),\n    TRUE ~ word(code_desc, 1)\n  ))\ndwide <-\n  all_2020_ttc_data |>\n  group_by(line, station_clean) |>\n  mutate(n_obs = n()) |>\n  filter(n_obs > 1) |>\n  group_by(code_red) |>\n  mutate(tot_delay = n()) |>\n  arrange(tot_delay) |>\n  filter(tot_delay > 50) |>\n  group_by(line, station_clean, code_red) |>\n  summarise(n_delay = n()) |>\n  pivot_wider(names_from = code_red, values_from = n_delay) |>\n  mutate_all(.funs = funs(ifelse(is.na(.), 0, .)))\n#> `summarise()` has grouped output by 'line', 'station_clean'. You can override using the `.groups` argument.\n#> `mutate_all()` ignored the following grouping variables:\n#> Columns `line`, `station_clean`\n#> Use `mutate_at(df, vars(-group_cols()), myoperation)` to silence the message.\n#> Warning: `funs()` was deprecated in dplyr 0.8.0.\n#> Please use a list of either functions or lambdas: \n#> \n#>   # Simple named list: \n#>   list(mean = mean, median = median)\n#> \n#>   # Auto named with `tibble::lst()`: \n#>   tibble::lst(mean, median)\n#> \n#>   # Using lambdas\n#>   list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n#> This warning is displayed once every 8 hours.\n#> Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\ndelay_pca <- prcomp(dwide[,3:ncol(dwide)])\n\ndf_out <- as_tibble(delay_pca$x)\ndf_out <- bind_cols(dwide |> select(line, station_clean), df_out)\nhead(df_out)\n#> # A tibble: 6 × 32\n#> # Groups:   line, station_clean [6]\n#>   line  station_clean    PC1    PC2     PC3   PC4    PC5\n#>   <chr> <chr>          <dbl>  <dbl>   <dbl> <dbl>  <dbl>\n#> 1 BD    BATHURST       10.9   17.2   2.04   12.9   4.60 \n#> 2 BD    BAY            14.6    6.54  4.76   14.4   0.406\n#> 3 BD    BLOOR          22.8  -18.6  19.7    -7.37 -1.54 \n#> 4 BD    BLOOR-DANFORTH 23.4  -20.2  20.4    -4.85 -0.429\n#> 5 BD    BROADVIEW       9.29  22.0  -0.0365  6.72  4.31 \n#> 6 BD    CASTLE         15.1    5.21  7.62   11.6  -1.17 \n#> # … with 25 more variables: PC6 <dbl>, PC7 <dbl>,\n#> #   PC8 <dbl>, PC9 <dbl>, PC10 <dbl>, PC11 <dbl>,\n#> #   PC12 <dbl>, PC13 <dbl>, PC14 <dbl>, PC15 <dbl>,\n#> #   PC16 <dbl>, PC17 <dbl>, PC18 <dbl>, PC19 <dbl>,\n#> #   PC20 <dbl>, PC21 <dbl>, PC22 <dbl>, PC23 <dbl>,\n#> #   PC24 <dbl>, PC25 <dbl>, PC26 <dbl>, PC27 <dbl>,\n#> #   PC28 <dbl>, PC29 <dbl>, PC30 <dbl>\nlibrary(ggrepel)\nggplot(df_out,aes(x=PC1,y=PC2,color=line )) + \n  geom_point() + \n  geom_text_repel(data = df_out |> filter(PC2>100|PC1<100*-1), \n                  aes(label = station_clean)\n                  )\ndf_out_r <- as_tibble(delay_pca$rotation)\ndf_out_r$feature <- colnames(dwide[,3:ncol(dwide)])\n\ndf_out_r\n#> # A tibble: 30 × 31\n#>         PC1    PC2      PC3      PC4      PC5     PC6\n#>       <dbl>  <dbl>    <dbl>    <dbl>    <dbl>   <dbl>\n#>  1 -0.0279  0.125  -0.0576   0.0679  -0.0133   0.0307\n#>  2 -0.108   0.343  -0.150    0.205   -0.100    0.317 \n#>  3 -0.0196  0.0604 -0.0207   0.0195   0.0189   0.0574\n#>  4 -0.0244  0.0752 -0.0325  -0.0237   0.00121 -0.0263\n#>  5 -0.0128  0.0113 -0.00340 -0.00977 -0.0255   0.0186\n#>  6 -0.231   0.650  -0.309    0.245    0.222   -0.309 \n#>  7 -0.0871  0.233  -0.0904  -0.692   -0.311   -0.414 \n#>  8 -0.00377 0.0193 -0.00201 -0.0140  -0.0424   0.0751\n#>  9 -0.0167  0.120  -0.0367  -0.578    0.336    0.563 \n#> 10 -0.0708  0.276  -0.118    0.116   -0.368    0.435 \n#> # … with 20 more rows, and 25 more variables: PC7 <dbl>,\n#> #   PC8 <dbl>, PC9 <dbl>, PC10 <dbl>, PC11 <dbl>,\n#> #   PC12 <dbl>, PC13 <dbl>, PC14 <dbl>, PC15 <dbl>,\n#> #   PC16 <dbl>, PC17 <dbl>, PC18 <dbl>, PC19 <dbl>,\n#> #   PC20 <dbl>, PC21 <dbl>, PC22 <dbl>, PC23 <dbl>,\n#> #   PC24 <dbl>, PC25 <dbl>, PC26 <dbl>, PC27 <dbl>,\n#> #   PC28 <dbl>, PC29 <dbl>, PC30 <dbl>, feature <chr>\n\nggplot(df_out_r,aes(x=PC1,y=PC2,label=feature )) + geom_text_repel()\n#> Warning: ggrepel: 22 unlabeled data points (too many\n#> overlaps). Consider increasing max.overlaps"},{"path":"exploratory-data-analysis.html","id":"case-study-airbnb-listing-in-toronto","chapter":"13 Exploratory data analysis","heading":"13.3 Case study: Airbnb listing in Toronto","text":"case study look Airbnb listings Toronto. dataset Inside Airbnb (M. Cox 2021) read website, save local copy. can give read_csv() link dataset download . helps reproducibility source clear. , link change time, longer-term reproducibility, well wanting minimize effect Inside Airbnb servers, suggest also save local copy data use . original data , make public without first getting written permission. ‘guess_max’ option read_csv helps us avoid specify column types. Usually read_csv() takes best guess column types based first rows. sometimes first ones misleading ‘guess_max’ forces look larger number rows try work going .","code":"\nlibrary(tidyverse)\n\nairbnb_data <- \n  read_csv(\"http://data.insideairbnb.com/canada/on/toronto/2021-01-02/data/listings.csv.gz\", \n           guess_max = 20000)\n\nwrite_csv(airbnb_data, \"airbnb_data.csv\")\n\nairbnb_data"},{"path":"exploratory-data-analysis.html","id":"explore-individual-variables","chapter":"13 Exploratory data analysis","heading":"13.3.1 Explore individual variables","text":"large number columns, just select .First might interested price. character moment need convert numeric. common problem, need little careful just convert NAs. case just force price data numeric go NA lot characters unclear numeric equivalent , ‘$’. need remove characters first.Now can look distribution prices (Figure 13.8).\nFigure 13.8: Distribution prices Toronto Airbnb rentals January 2021\nclear outliers, might like consider log scale (Figure 13.9).\nFigure 13.9: Distribution log prices Toronto Airbnb rentals January 2021\nfocus prices less $1,000 see majority properties nightly price less $250. Interestingly looks like bunching prices, possible around numbers ending zero nine. Let us just zoom prices $90 $210, interest, change bins smaller (Figure 13.10).\nFigure 13.10: Distribution prices less $1000 Toronto Airbnb rentals January 2021 shows bunching\nnow, just remove prices $999.Superhosts especially experienced Airbnb hosts, might interested learn . instance, host either superhost, expect NAs. can see . might host removed listing similar. now, remove .also want create binary variable . true/false moment, fine modelling, handful situations easier 0/1. now just remove anyone NA whether super host.Airbnb, guests can give 1-5 star ratings across variety different aspects, including cleanliness, accuracy, value, others. look reviews dataset, less clear constructed, appears rating 20 100 also NA values (Figure 13.11).\nFigure 13.11: Distribution review scores rating Toronto Airbnb rentals January 2021\nlike deal NAs ‘review_scores_rating’, complicated lot . may just reviews.properties review rating yet enough reviews. large proportion total, almost fifth might like look detail using vis_miss() visdat (N. Tierney 2017). interested see whether something systematic happening properties. instance, NAs driven , say, requirement minimum number reviews, expect missing.Given looks convincing almost cases, different types reviews missing observation. One approach just focus missing main review score. clear almost reviews 80. Let us just zoom 60 80 range check distribution looks like range (Figure 13.12).\nFigure 13.12: Distribution review scores, 60 80, Toronto Airbnb rentals January 2021\nnow, remove anyone NA main review score, even though remove roughly 20 per cent observations. also focus hosts main review score least 70. ended using dataset actual analysis, want justify decision appendix similar.Another important factor quickly host responds enquiry. Airbnb allows hosts 24 hours respond, encourages responses within hour.unclear host response time NA. may related variable. Interestingly seems like looks like ‘NAs’ ‘host_response_time’ variable coded proper NAs, instead treated another category. recode actual NAs also change variable factor.clearly issue NAs lot . instance, might interested see relationship review score (Figure 13.13). lot overall review 100.\nFigure 13.13: Distribution review scores properties NA response time, Toronto Airbnb rentals January 2021\nnow, remove anyone NA response time. removes roughly another 20 per cent observations.two versions variable suggest many properties host Airbnb. might interested know whether difference .differences dataset, can just remove one variable now look one (Figure 13.14).\nFigure 13.14: Distribution number properties host Airbnb, Toronto Airbnb rentals January 2021\nlarge number somewhere 2-10 properties range, usual long tail. number 0 listings unexpected worth following . bunch NA need deal .nothing immediately jumps odd people zero listings, must something going . now, focus one property.","code":"\nnames(airbnb_data) |> \n  length()\n#> [1] 74\n\nairbnb_data_selected <- \n  airbnb_data |> \n  select(host_id, \n         host_since, \n         host_response_time, \n         host_is_superhost, \n         host_listings_count,\n         host_total_listings_count,\n         host_neighbourhood, \n         host_listings_count, \n         neighbourhood_cleansed, \n         room_type, \n         bathrooms, \n         bedrooms, \n         price, \n         number_of_reviews, \n         has_availability, \n         review_scores_rating, \n         review_scores_accuracy, \n         review_scores_cleanliness, \n         review_scores_checkin, \n         review_scores_communication, \n         review_scores_location, \n         review_scores_value\n         )\n\nairbnb_data_selected\n#> # A tibble: 18,265 × 21\n#>    host_id host_since host_response_time host_is_superhost\n#>      <dbl> <date>     <chr>              <lgl>            \n#>  1    1565 2008-08-08 N/A                FALSE            \n#>  2   22795 2009-06-22 N/A                FALSE            \n#>  3   48239 2009-10-25 N/A                FALSE            \n#>  4   93825 2010-03-15 N/A                FALSE            \n#>  5  118124 2010-05-04 within a day       FALSE            \n#>  6   22795 2009-06-22 N/A                FALSE            \n#>  7  174063 2010-07-20 within an hour     TRUE             \n#>  8  183071 2010-07-28 within an hour     TRUE             \n#>  9  187320 2010-08-01 within a few hours TRUE             \n#> 10  192364 2010-08-05 N/A                FALSE            \n#> # … with 18,255 more rows, and 17 more variables:\n#> #   host_listings_count <dbl>,\n#> #   host_total_listings_count <dbl>,\n#> #   host_neighbourhood <chr>, neighbourhood_cleansed <chr>,\n#> #   room_type <chr>, bathrooms <lgl>, bedrooms <dbl>,\n#> #   price <chr>, number_of_reviews <dbl>,\n#> #   has_availability <lgl>, review_scores_rating <dbl>, …\nairbnb_data_selected$price |> head()\n#> [1] \"$469.00\" \"$96.00\"  \"$64.00\"  \"$70.00\"  \"$45.00\" \n#> [6] \"$127.00\"\n\nairbnb_data_selected$price |> str_split(\"\") |> unlist() |> unique()\n#>  [1] \"$\" \"4\" \"6\" \"9\" \".\" \"0\" \"7\" \"5\" \"1\" \"2\" \"3\" \"8\" \",\"\n\nairbnb_data_selected |> \n  select(price) |> \n  filter(str_detect(price, \",\"))\n#> # A tibble: 145 × 1\n#>    price    \n#>    <chr>    \n#>  1 $1,724.00\n#>  2 $1,000.00\n#>  3 $1,100.00\n#>  4 $1,450.00\n#>  5 $1,019.00\n#>  6 $1,000.00\n#>  7 $1,300.00\n#>  8 $2,142.00\n#>  9 $2,000.00\n#> 10 $1,200.00\n#> # … with 135 more rows\n\nairbnb_data_selected <- \n  airbnb_data_selected |> \n  mutate(price = str_remove(price, \"\\\\$\"),\n         price = str_remove(price, \",\"),\n         price = as.integer(price)\n         )\nairbnb_data_selected |>\n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 10) +\n  theme_classic() +\n  labs(x = \"Price per night\",\n       y = \"Number of properties\")\nairbnb_data_selected |>\n  filter(price > 1000) |> \n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 10) +\n  theme_classic() +\n  labs(x = \"Price per night\",\n       y = \"Number of properties\") +\n  scale_y_log10()\nairbnb_data_selected |>\n  filter(price < 1000) |> \n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 10) +\n  theme_classic() +\n  labs(x = \"Price per night\",\n       y = \"Number of properties\")\n\nairbnb_data_selected |>\n  filter(price > 90) |> \n  filter(price < 210) |> \n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 1) +\n  theme_classic() +\n  labs(x = \"Price per night\",\n       y = \"Number of properties\")\nairbnb_data_selected <- \n  airbnb_data_selected |> \n  filter(price < 1000)\nairbnb_data_selected |>\n  filter(is.na(host_is_superhost))\n#> # A tibble: 11 × 21\n#>      host_id host_since host_response_time host_is_superhost\n#>        <dbl> <date>     <chr>              <lgl>            \n#>  1  23472830 NA         <NA>               NA               \n#>  2  31675651 NA         <NA>               NA               \n#>  3  75779190 NA         <NA>               NA               \n#>  4  47614482 NA         <NA>               NA               \n#>  5 201103629 NA         <NA>               NA               \n#>  6 111820893 NA         <NA>               NA               \n#>  7  23472830 NA         <NA>               NA               \n#>  8 196269219 NA         <NA>               NA               \n#>  9  23472830 NA         <NA>               NA               \n#> 10 266594170 NA         <NA>               NA               \n#> 11 118516038 NA         <NA>               NA               \n#> # … with 17 more variables: host_listings_count <dbl>,\n#> #   host_total_listings_count <dbl>,\n#> #   host_neighbourhood <chr>, neighbourhood_cleansed <chr>,\n#> #   room_type <chr>, bathrooms <lgl>, bedrooms <dbl>,\n#> #   price <int>, number_of_reviews <dbl>,\n#> #   has_availability <lgl>, review_scores_rating <dbl>,\n#> #   review_scores_accuracy <dbl>, …\nairbnb_data_selected <- \n  airbnb_data_selected |>\n  filter(!is.na(host_is_superhost)) |>\n  mutate(host_is_superhost_binary = if_else(\n    host_is_superhost == TRUE, 1, 0)\n    )\nairbnb_data_selected |>\n  ggplot(aes(x = review_scores_rating)) +\n  geom_bar() +\n  theme_classic() +\n  labs(x = \"Reviews scores rating\",\n       y = \"Number of properties\")\nairbnb_data_selected |>\n  filter(is.na(review_scores_rating)) |>\n  nrow()\n#> [1] 4308\n\nairbnb_data_selected |>\n  filter(is.na(review_scores_rating)) |> \n  select(number_of_reviews) |> \n  table()\n#> \n#>    0    1    2    3    4 \n#> 4046  226   23   10    3\nlibrary(visdat)\nairbnb_data_selected |> \n  select(review_scores_rating, \n         review_scores_accuracy, \n         review_scores_cleanliness, \n         review_scores_checkin, \n         review_scores_communication, \n         review_scores_location, \n         review_scores_value) |> \n  vis_miss()\nairbnb_data_selected |>\n  filter(!is.na(review_scores_rating)) |> \n  filter(review_scores_rating > 60) |>\n  filter(review_scores_rating < 80) |> \n  ggplot(aes(x = review_scores_rating)) +\n  geom_histogram(binwidth = 1) +\n  theme_classic() +\n  labs(x = \"Average review score\",\n       y = \"Number of properties\")\nairbnb_data_selected <- \n  airbnb_data_selected |>\n  filter(!is.na(review_scores_rating)) |>\n  filter(review_scores_rating >= 70)\nairbnb_data_selected |>\n  count(host_response_time)\n#> # A tibble: 5 × 2\n#>   host_response_time     n\n#>   <chr>              <int>\n#> 1 a few days or more   494\n#> 2 N/A                 5708\n#> 3 within a day         952\n#> 4 within a few hours  1649\n#> 5 within an hour      4668\nairbnb_data_selected <- \n  airbnb_data_selected |>\n  mutate(host_response_time = if_else(host_response_time == \"N/A\", NA_character_, host_response_time),\n         host_response_time = factor(host_response_time)\n  )\nairbnb_data_selected |> \n  filter(is.na(host_response_time)) |> \n  ggplot(aes(x = review_scores_rating)) +\n  geom_histogram(binwidth = 1) +\n  theme_classic() +\n  labs(x = \"Average review score\",\n       y = \"Number of properties\")\nairbnb_data_selected <- \n  airbnb_data_selected |> \n  filter(!is.na(host_response_time))\nairbnb_data_selected |> \n  mutate(listings_count_is_same = if_else(host_listings_count == host_total_listings_count, 1, 0)) |> \n  filter(listings_count_is_same == 0)\n#> # A tibble: 0 × 23\n#> # … with 23 variables: host_id <dbl>, host_since <date>,\n#> #   host_response_time <fct>, host_is_superhost <lgl>,\n#> #   host_listings_count <dbl>,\n#> #   host_total_listings_count <dbl>,\n#> #   host_neighbourhood <chr>, neighbourhood_cleansed <chr>,\n#> #   room_type <chr>, bathrooms <lgl>, bedrooms <dbl>,\n#> #   price <int>, number_of_reviews <dbl>, …\nairbnb_data_selected <- \n  airbnb_data_selected |> \n  select(-host_listings_count)\n\nairbnb_data_selected |> \n  ggplot(aes(x = host_total_listings_count)) +\n  geom_bar() +\n  scale_x_log10()\nairbnb_data_selected |> \n  filter(host_total_listings_count == 0) |> \n  head()\n#> # A tibble: 6 × 21\n#>   host_id host_since host_response_time host_is_superhost\n#>     <dbl> <date>     <fct>              <lgl>            \n#> 1 3783106 2012-10-06 within an hour     FALSE            \n#> 2 3814089 2012-10-09 within an hour     FALSE            \n#> 3 3827668 2012-10-10 within a day       FALSE            \n#> 4 2499198 2012-05-30 within a day       FALSE            \n#> 5 3268493 2012-08-15 within a day       FALSE            \n#> 6 8675040 2013-09-06 within an hour     TRUE             \n#> # … with 17 more variables:\n#> #   host_total_listings_count <dbl>,\n#> #   host_neighbourhood <chr>, neighbourhood_cleansed <chr>,\n#> #   room_type <chr>, bathrooms <lgl>, bedrooms <dbl>,\n#> #   price <int>, number_of_reviews <dbl>,\n#> #   has_availability <lgl>, review_scores_rating <dbl>,\n#> #   review_scores_accuracy <dbl>, …\nairbnb_data_selected <- \n  airbnb_data_selected |> \n  add_count(host_id) |> \n  filter(n == 1) |> \n  select(-n)"},{"path":"exploratory-data-analysis.html","id":"explore-relationships-between-variables","chapter":"13 Exploratory data analysis","heading":"13.3.2 Explore relationships between variables","text":"might like make graphs see relationships become clear. aspects come mind looking prices reviews super hosts, number properties neighborhood.Look relationship price reviews, whether super-host (Figure 13.15).\nFigure 13.15: Relationship price review whether host super host, Toronto Airbnb rentals January 2021\nOne aspects may make someone super host quickly respond inquiries. One imagine superhost involves quickly saying yes inquiries. Let us look data. First, want look possible values superhost response times.Fortunately, looks like removed reviews rows removed NAs whether super host, go back look may need check .\nbuild table looks hosts response time whether superhost using tabyl() janitor (Firke 2020). clear host respond within hour unlikely super host.Finally, look neighborhood. data provider attempted clean neighborhood variable us, just use variable now. Although ended using variable actual analysis want check made errors.","code":"\nairbnb_data_selected |>\n  ggplot(aes(x = price, y = review_scores_rating, color = host_is_superhost)) +\n  geom_point(size = 1, alpha = 0.1) + \n  theme_classic() +\n  labs(x = \"Price per night\",\n       y = \"Average review score\",\n       color = \"Super host\") +\n  scale_color_brewer(palette = \"Set1\")\nairbnb_data_selected |> \n  count(host_is_superhost) |>\n  mutate(proportion = n / sum(n),\n         proportion = round(proportion, digits = 2))\n#> # A tibble: 2 × 3\n#>   host_is_superhost     n proportion\n#>   <lgl>             <int>      <dbl>\n#> 1 FALSE              1677       0.58\n#> 2 TRUE               1234       0.42\nairbnb_data_selected |> \n  tabyl(host_response_time, host_is_superhost) |> \n  adorn_percentages(\"row\") |>\n  adorn_pct_formatting(digits = 0) |>\n  adorn_ns() |> \n  adorn_title()\n#>                     host_is_superhost          \n#>  host_response_time             FALSE      TRUE\n#>  a few days or more         90% (223) 10%  (26)\n#>        within a day         70% (348) 30% (149)\n#>  within a few hours         55% (354) 45% (284)\n#>      within an hour         49% (752) 51% (775)\nairbnb_data_selected |> \n  tabyl(neighbourhood_cleansed) |> \n  adorn_totals(\"row\") |> \n  adorn_pct_formatting() |> \n  nrow()\n#> [1] 140\nairbnb_data_selected |> \n  tabyl(neighbourhood_cleansed) |> \n  adorn_pct_formatting() |> \n  arrange(-n) |> \n  filter(n > 100) |> \n  adorn_totals(\"row\") |> \n  head()\n#>             neighbourhood_cleansed   n percent\n#>  Waterfront Communities-The Island 488   16.8%\n#>                            Niagara 129    4.4%\n#>                              Annex 102    3.5%\n#>                              Total 719       -"},{"path":"exploratory-data-analysis.html","id":"explore-multiple-relationships-with-a-model","chapter":"13 Exploratory data analysis","heading":"13.3.3 Explore multiple relationships with a model","text":"now run models dataset. cover modeling detail Chapter 14, can use models EDA help get better sense relationships may exist multiple variables dataset. instance, may like see whether can forecast whether someone super host, factors go explaining . dependent variable binary, good opportunity use logistic regression. expect better reviews associated faster responses higher reviews. Specifically, model estimate :\\[\\mbox{Prob(super host} = 1) = \\beta_0 + \\beta_1 \\mbox{Response time} + \\beta_2 \\mbox{Reviews} + \\epsilon\\]estimate model using glm R language (R Core Team 2021).can quick look results using modelsummary() modelsummary (Arel-Bundock 2021a)","code":"\nlogistic_reg_superhost_response_review <- glm(host_is_superhost ~ \n                                                host_response_time + \n                                                review_scores_rating,\n                                              data = airbnb_data_selected,\n                                              family = binomial\n                                              )\nlibrary(modelsummary)\nmodelsummary(logistic_reg_superhost_response_review)"},{"path":"exploratory-data-analysis.html","id":"exercises-and-tutorial-12","chapter":"13 Exploratory data analysis","heading":"13.4 Exercises and tutorial","text":"","code":""},{"path":"exploratory-data-analysis.html","id":"exercises-12","chapter":"13 Exploratory data analysis","heading":"13.4.1 Exercises","text":"words exploratory data analysis (difficult, please write one nuanced paragraph)?Tukey’s words, exploratory data analysis (please write one paragraph)?Tukey (please write paragraph two)?dataset called ‘my_data’, two columns: ‘first_col’ ‘second_col’, please write rough R code generate graph (type graph matter).Consider dataset 500 rows 3 columns, 1,500 cells. 100 cells missing data least one columns, remove whole row dataset try run analysis data , procedure? dataset 10,000 rows instead, number missing cells?Please note three ways identifying unusual values.difference categorical continuous variable?difference factor integer variable?can think systematically excluded dataset?Using opendatatoronto package, download data mayoral campaign contributions 2014. (note: 2014 file get get_resource, just keep sheet relates Mayor election).\nClean data format (fixing parsing issue standardizing column names using janitor)\nSummarize variables dataset. missing values, , worried ? every variable format ? , create new variable(s) right format.\nVisually explore distribution values contributions. contributions notable outliers? share similar characteristic(s)? may useful plot distribution contributions without outliers get better sense majority data.\nList top five candidates categories: 1) total contributions; 2) mean contribution; 3) number contributions.\nRepeat process, without contributions candidates .\nmany contributors gave money one candidate?\nClean data format (fixing parsing issue standardizing column names using janitor)Summarize variables dataset. missing values, , worried ? every variable format ? , create new variable(s) right format.Visually explore distribution values contributions. contributions notable outliers? share similar characteristic(s)? may useful plot distribution contributions without outliers get better sense majority data.List top five candidates categories: 1) total contributions; 2) mean contribution; 3) number contributions.Repeat process, without contributions candidates .many contributors gave money one candidate?Name three geoms produce graphs bars ggplot().Consider dataset 10,000 observations 27 variables. observation, least one missing variable. Please discuss, paragraph two, steps take understand going .Known missing data, leave holes dataset. data never collected? Please look McClelland (2019) Luscombe McClelland (2020). Look gathered dataset took put together. dataset ? missing ? affect results? might similar biases enter datasets used read ?","code":""},{"path":"exploratory-data-analysis.html","id":"tutorial-12","chapter":"13 Exploratory data analysis","heading":"13.4.2 Tutorial","text":"Redo Airbnb EDA Paris. Please submit PDF.","code":""},{"path":"ijalm.html","id":"ijalm","chapter":"14 It’s Just A Linear Model","heading":"14 It’s Just A Linear Model","text":"Required materialRead Introduction Statistical Learning Applications R, Chapters 3 ‘Linear Regression’, 4 ‘Classification’, (James et al. 2017)Read Data Analysis Using Regression Multilevel/Hierarchical Models, Chapters 3 ‘Linear regression: basics’, 4 ‘Linear regression: fitting model’, 5 ‘Logistic regression’, 6 ‘Generalized linear models’, (Gelman Hill 2007)Read published research findings false, (Ioannidis 2005)Key concepts skillsSimple multiple linear regression.Logistic Poisson regression.key role uncertainty.Threats validity inference.Overfitting.Key librariesbroom (D. Robinson, Hayes, Couch 2021)modelsummary (Arel-Bundock 2021a)rstanarm (Goodrich et al. 2020)tidymodels (Kuhn Wickham 2020)tidyverse (Wickham et al. 2019a)Key functionsbroom::augment()broom::glance()broom::tidy()glm()lm()modelsummary::modelsummary()parsnip::fit()parsnip::linear_reg()parsnip::logistic_reg()parsnip::set_engine()poissonreg::poisson_reg()rnorm()rpois()rsample::initial_split()rsample::testing()rsample::training()sample()set.seed()summary()","code":""},{"path":"ijalm.html","id":"introduction-11","chapter":"14 It’s Just A Linear Model","heading":"14.1 Introduction","text":"Linear models around long time. instance, speaking development least squares, one way fit linear models, 1700s, Stigler (1986, 16) describes associated foundational problems astronomy, determining motion moon reconciling non-periodic motion Jupiter Saturn. fundamental issue time least squares hesitancy combine different observations. Astronomers early develop comfort typically gathered observations knew conditions data gathering similar, even though value observation different. took longer social scientists become comfortable linear models, possibly hesitant group together data worried alike. sense, astronomers advantage compare predictions actually happened whereas difficult social scientists (Stigler 1986, 163).Galton others generation, eugenicists, used linear regression earnest late 1800s early 1900s. Binary outcomes quickly became interest needed special treatment, leading development wide adaption logistic regression similar methods mid-1900s (Cramer 2002). generalized linear model framework came , formal sense, 1970s Nelder Wedderburn (1972) brought together. Generalized linear models (GLM) broaden types outcomes allowed. still model outcomes linear function, constrained outcome normally distributed. outcome can anything exponential family, popular choices include logistic distribution, Poisson distribution. generalization GLMs generalized additive models broaden structure explanatory side. still explain dependent variable additive function various bits pieces, bits pieces can functions. framework, way, came 1990s, Hastie Tibshirani (1990).important recognize build models, discovering ‘truth’. using model help us explore understand data . one best model, just useful models help us learn something data hence, hopefully, something world data generated. use models, trying understand world, enormous constraints perspective bring . silly expect one model universal. , just blindly throw data regression model hope sort . ‘Regression sort . Regression indeed oracle, cruel one. speaks riddles delights punishing us asking bad questions’ (McElreath 2020, 162).use models understand world. poke, push, test . build rejoice beauty, seek understand limits ultimately destroy . process important, process allows us better understand world; outcome. build models, need keep mind world model broader world want able speak . extent model trained experiences straight, cis, men, speak world ? worthless, also unimpeachable. extent model teach us data ? extent data reflect world like draw conclusions? need keep questions front mind.Much statistics developed without concern broader implications. reasonable developed situations astronomy agriculture. Folks literally able randomize order fields planting literally worked agricultural stations. many subsequent applications twentieth twenty-first centuries, properties. Statistical science often taught though proceeds idealized process hypothesis appears, tested, either confirmed . happens. react incentives. dabble, guess, test, follow intuition, backfilling need. fine. world traditional null hypothesis holds completely, means concepts p-values power lose meaning. need understand ‘old world’, also need sophisticated enough know need move away . can appreciate beauty ingenuity Ford Model T, time recognizing use win Le Mans.chapter begin simple linear regression, move multiple linear regression, difference number explanatory variables allow. consider logistic Poisson regression. consider three approaches : base R, useful want quickly use models EDA; tidymodels (Kuhn Wickham 2020) useful interested forecasting; rstanarm (Goodrich et al. 2020) interested understanding. Regardless approach use, important thing remember modelling way just fancy averaging. chapter named quote Daniela Witten, Professor, University Washington, identifies far can get linear models huge extent underpin statistics.","code":""},{"path":"ijalm.html","id":"simple-linear-regression","chapter":"14 It’s Just A Linear Model","heading":"14.2 Simple linear regression","text":"interested relationship two continuous variables, say \\(y\\) \\(x\\), can use simple linear regression. based Normal, also ‘Gaussian’, distribution. shape Normal distribution determined two parameters, mean \\(\\mu\\) standard deviation, \\(\\sigma\\):\\[y = \\frac{1}{\\sqrt{2\\pi\\sigma}}e^{-\\frac{1}{2}z^2},\\]\n\\(z = (x - \\mu)/\\sigma\\) difference mean, \\(\\mu\\), \\(x\\) terms standard deviation (Pitman 1993, 94).discussed Chapter 3, use rnorm() simulate data Normal distribution.specified \\(n=20\\) draws Normal distribution mean 0 standard deviation 1. deal real data, know parameters want use data estimate . can estimate mean, \\(\\bar{x}\\), standard deviation, \\(\\hat{\\sigma}_x\\), following estimators:\\[\n\\begin{aligned}\n\\bar{x} &= \\frac{1}{n} \\times \\sum_{= 1}^{n}x_i\\\\\n\\hat{\\sigma}_{x} &= \\sqrt{\\frac{1}{n} \\times \\sum_{= 1}^{n}\\left(x_i - \\bar{x}\\right)^2}\n\\end{aligned}\n\\]\\(\\hat{\\sigma}_x\\) standard deviation, standard error estimate, say, \\(\\bar{x}\\) :\n\\[\\mbox{SE}(\\bar{x})^2 = \\frac{\\sigma^2}{n}\\]worried estimates slightly . typically take larger number draws get expected shape, estimated parameters get close actual parameters, happen (Figure 14.1). Wasserman (2005, 76) describes certainty , due Law Large Numbers, ‘crowning achievement probability’.\nFigure 14.1: Normal distribution takes familiar shape number draws increases\nuse simple linear regression, assume relationship characterized variables parameters. two variables, \\(Y\\) \\(X\\), characterize linear relationship :\n\\[Y \\approx \\beta_0 + \\beta_1 X.\\]two coefficients, also ‘parameters’: ‘intercept’, \\(\\beta_0\\), ‘slope’, \\(\\beta_1\\). saying \\(Y\\) value, \\(\\beta_0\\), even \\(X\\) 0, \\(Y\\) change \\(\\beta_1\\) units every one unit change \\(X\\). language use ‘X regressed Y’. may take relationship data , order estimate coefficients, particular data .make example concrete, simulate data discuss context. instance, consider time takes someone run five kilometers, compared time takes run marathon (Figure 14.2). impute relationship 8.4, roughly ratio distance marathon five-kilometer race.\nFigure 14.2: Simulated data relationship time run five kilometers marathon\nsimulated example, know \\(\\beta_0\\) \\(\\beta_1\\) . challenge see can use data, simple linear regression, recover . , can use \\(x\\), five-kilometer time, produce estimates \\(y\\), marathon time, put hat denote estimate:\\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x.\\]\nhats used indicate estimated values.involves estimating values \\(\\beta_0\\) \\(\\beta_1\\), , estimates denoted hat . estimate coefficients? Even impose linear relationship many options, large number straight lines drawn. lines fit data better others.One way may define line ‘better’ another, close possible \\(x\\) \\(y\\) combinations know. lot candidates define ‘close possible’, one minimize residual sum squares. produce estimates \\(\\hat{y}\\) based guesses \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\), given \\(x\\). work ‘wrong’, every point \\(\\), :\n\\[e_i = y_i - \\hat{y}_i.\\]compute residual sum squares (RSS), sum across points, taking square account negative differences:\n\\[\\mbox{RSS} = e^2_1+ e^2_2 +\\dots + e^2_n.\\]\nresults one ‘linear best-fit’ line (Figure 14.3), worth thinking assumptions decisions took get us point.\nFigure 14.3: Simulated data relationship time run five kilometers marathon\nUnderpinning use simple linear regression belief ‘true’ relationship \\(X\\) \\(Y\\), :\\[Y = f(X) + \\epsilon.\\]going say function, \\(f()\\), linear, simple linear regression:\\[\\hat{Y} = \\beta_0 + \\beta_1 X + \\epsilon.\\]‘true’ relationship \\(X\\) \\(Y\\), know . can use sample data estimate . understanding depends sample, every possible sample, get slightly different relationship, measured coefficients.\\(\\epsilon\\) measure error—model know? going plenty model know, hope error depend \\(X\\), error normally distributed.can conduct simple linear regression lm() base R. specify dependent variable first, ~, followed independent variables. Finally, specify dataset.see result regression, can use summary() base R.first part result tells us regression specified, information residuals, estimated coefficients. finally useful diagnostics.intercept marathon time expect five-kilometer time 0 minutes. Hopefully example illustrates need carefully interpret intercept coefficient! coefficient five-kilometer run time shows expect marathon time change five-kilometer run time changed one unit. case 8.4, makes sense seeing marathon roughly many times longer five-kilometer run.use augment() broom (D. Robinson, Hayes, Couch 2021) add fitted values residuals original dataset. allows us plot residuals (Figure 14.4).\nFigure 14.4: Residuals simple linear regression simulated data time someone takes run five kilometers marathon\nwant estimate unbiased. say estimate unbiased, trying say even though sample estimate might high, another sample estimate might low, eventually lot data estimate population. estimator unbiased systematically - -estimate (James et al. 2017, 65).want try speak ‘true’ relationship, need try capture much think understanding depends particular sample analyze. standard error comes . tells us estimate compared actual (Figure 14.5).\nFigure 14.5: Simple linear regression simulated data time someone takes run five kilometers marathon, along standard errors\nstandard errors, can compute confidence interval. 95 per cent confidence interval range, roughly 0.95 probability interval happens contain population parameter, typically unknown. lower end range : \\(\\hat{\\beta_1} - 2 \\times \\mbox{SE}\\left(\\hat{\\beta_1}\\right)\\) upper end range : \\(\\hat{\\beta_1} + 2 \\times \\mbox{SE}\\left(\\hat{\\beta_1}\\right)\\).Now range, can say roughly 95 per cent probability range contains true population parameter, test claims. instance, claim relationship \\(X\\) \\(Y\\), .e. \\(\\beta_1 = 0\\), alternative claim relationship \\(X\\) \\(Y\\), .e. \\(\\beta_1 \\neq 0\\).way Chapter 10 needed decide much evidence take convince us tea taster distinguish whether milk tea added first, need decide whether estimate \\(\\beta_1\\), \\(\\hat{\\beta}_1\\), ‘far enough’ away zero us comfortable claiming \\(\\beta_1 \\neq 0\\). far ‘far enough’? confident estimate \\(\\beta_1\\) far, substantial. standard error \\(\\hat{\\beta}_1\\) awful lot work accounting variety factors, can actually account .compare standard error \\(\\hat{\\beta}_1\\) get t-statistic:\n\\[t = \\frac{\\hat{\\beta}_1 - 0}{\\mbox{SE}(\\hat{\\beta}_1)}.\\]\ncompare t-statistic t-distribution compute probability getting absolute t-statistic larger one, actually case \\(\\beta_1 = 0\\). probability p-value. smaller p-value means less likely observe data due chance relationship.Words! Mere words! terrible ! clear, vivid, cruel! One escape . yet subtle magic ! seemed able give plastic form formless things, music sweet viol lute. Mere words! anything real words?Picture Dorian Gray (Wilde 1891).make much use p-values book specific subtle concept. difficult understand easy abuse. main issue embody, assume correct, every assumption model, including everything went gathering cleaning data. smaller p-values imply data unusual assumptions correct; consider full data science workflow usually awful lot assumptions. get guidance p-values reasonableness specific assumptions (Greenland et al. 2016, 339). p-value may reject null hypothesis null hypothesis actually false, may also data incorrectly gathered prepared. can sure p-value speaks hypothesis interested testing, assumptions correct. nothing inherently wrong using p-values, important use sophisticated thoughtful ways. D. Cox (2018) provides lovely discussion requires.One application easy see abuse p-values power analysis. Power, statistical sense, refers probability rejecting null hypothesis actually false. power relates hypothesis testing, also related sample size. often worry study ‘-powered’, meaning large enough sample, rarely worry , say, data inappropriately cleaned, even though distinguish based p-value.","code":"\nlibrary(tidyverse)\n\nset.seed(853)\n\ntwenty_draws_from_normal_distribution <- \n  tibble(draws = rnorm(n = 20, mean = 0, sd = 1))\n  \ntwenty_draws_from_normal_distribution\n#> # A tibble: 20 × 1\n#>      draws\n#>      <dbl>\n#>  1 -0.360 \n#>  2 -0.0406\n#>  3 -1.78  \n#>  4 -1.12  \n#>  5 -1.00  \n#>  6  1.78  \n#>  7 -1.39  \n#>  8 -0.497 \n#>  9 -0.558 \n#> 10 -0.824 \n#> 11  1.67  \n#> 12 -0.682 \n#> 13  0.0652\n#> 14 -0.260 \n#> 15  0.329 \n#> 16 -0.437 \n#> 17 -0.323 \n#> 18  0.115 \n#> 19  0.842 \n#> 20  0.342\nestimated_mean <-\n  sum(twenty_draws_from_normal_distribution$draws) / nrow(twenty_draws_from_normal_distribution)\n\nestimated_mean\n#> [1] -0.2069253\n\nestimated_mean <-\n  twenty_draws_from_normal_distribution |> \n  mutate(draws_diff_square = (draws - estimated_mean)^2)\n\nestimated_standard_deviation <- \n  sqrt(\n    sum(estimated_mean$draws_diff_square) / nrow(twenty_draws_from_normal_distribution)\n  )\n\nestimated_standard_deviation\n#> [1] 0.8832841\nlibrary(tidyverse)\n\nset.seed(853)\n\ntibble(\n  number_of_draws = c(\n    rep.int(x = \"2 draws\", times = 2),\n    rep.int(x = \"5 draws\", times = 5),\n    rep.int(x = \"10 draws\", times = 10),\n    rep.int(x = \"50 draws\", times = 50),\n    rep.int(x = \"100 draws\", times = 100),\n    rep.int(x = \"500 draws\", times = 500),\n    rep.int(x = \"1,000 draws\", times = 1000),\n    rep.int(x = \"10,000 draws\", times = 10000),\n    rep.int(x = \"100,000 draws\", times = 100000)\n  ),\n  draws = c(\n    rnorm(n = 2, mean = 0, sd = 1),\n    rnorm(n = 5, mean = 0, sd = 1),\n    rnorm(n = 10, mean = 0, sd = 1),\n    rnorm(n = 50, mean = 0, sd = 1),\n    rnorm(n = 100, mean = 0, sd = 1),\n    rnorm(n = 500, mean = 0, sd = 1),\n    rnorm(n = 1000, mean = 0, sd = 1),\n    rnorm(n = 10000, mean = 0, sd = 1),\n    rnorm(n = 100000, mean = 0, sd = 1)\n  )\n) |>\n  mutate(number_of_draws = as_factor(number_of_draws)) |>\n  ggplot(aes(x = draws)) +\n  geom_density() +\n  theme_minimal() +\n  facet_wrap(vars(number_of_draws),\n             scales = \"free_y\") +\n  labs(x = 'Draw',\n       y = 'Density')\nset.seed(853)\n\nnumber_of_observations <- 100\nexpected_relationship <- 8.4\n\nsimulated_running_data <- \n  tibble(five_km_time = \n           runif(n = number_of_observations, \n                 min = 15, \n                 max = 30),\n         noise = rnorm(number_of_observations, 0, 20),\n         marathon_time = five_km_time * expected_relationship + noise\n         ) |>\n  mutate(five_km_time = round(five_km_time, digits = 1),\n         marathon_time = round(marathon_time, digits = 1)) |>\n  select(-noise)\n\nsimulated_running_data\n#> # A tibble: 100 × 2\n#>    five_km_time marathon_time\n#>           <dbl>         <dbl>\n#>  1         20.4          152.\n#>  2         16.8          134.\n#>  3         22.3          198.\n#>  4         19.7          166.\n#>  5         15.6          163.\n#>  6         21.1          168.\n#>  7         17            131.\n#>  8         18.6          150.\n#>  9         17.4          158.\n#> 10         17.8          147.\n#> # … with 90 more rows\nsimulated_running_data |> \n  ggplot(aes(x = five_km_time, y = marathon_time)) +\n  geom_point() +\n  labs(x = \"Five-kilometer time (minutes)\",\n       y = \"Marathon time (minutes)\") +\n  theme_classic()\nsimulated_running_data |> \n  ggplot(aes(x = five_km_time, y = marathon_time)) +\n  geom_point() + \n  geom_smooth(method = \"lm\", \n              se = FALSE, \n              color = \"black\", \n              linetype = \"dashed\",\n              formula = 'y ~ x') +\n  labs(x = \"Five-kilometer time (minutes)\",\n       y = \"Marathon time (minutes)\") +\n  theme_classic()\nsimulated_running_data_first_model <- \n  lm(marathon_time ~ five_km_time, \n     data = simulated_running_data)\nsummary(simulated_running_data_first_model)\n#> \n#> Call:\n#> lm(formula = marathon_time ~ five_km_time, data = simulated_running_data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -49.654  -9.278   0.781  12.606  56.898 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)    8.2393     8.9550    0.92     0.36    \n#> five_km_time   7.9407     0.4072   19.50   <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 16.96 on 98 degrees of freedom\n#> Multiple R-squared:  0.7951, Adjusted R-squared:  0.793 \n#> F-statistic: 380.3 on 1 and 98 DF,  p-value: < 2.2e-16\nlibrary(broom)\n\nsimulated_running_data <- \n  augment(simulated_running_data_first_model,\n          data = simulated_running_data)\n\nsimulated_running_data\n#> # A tibble: 100 × 8\n#>    five_km_time marathon_time .fitted .resid   .hat .sigma\n#>           <dbl>         <dbl>   <dbl>  <dbl>  <dbl>  <dbl>\n#>  1         20.4          152.    170. -17.8  0.0108   17.0\n#>  2         16.8          134.    142.  -7.84 0.0232   17.0\n#>  3         22.3          198.    185.  13.1  0.0103   17.0\n#>  4         19.7          166.    165.   1.83 0.0121   17.1\n#>  5         15.6          163.    132.  31.3  0.0307   16.7\n#>  6         21.1          168.    176.  -8.09 0.0101   17.0\n#>  7         17            131.    143. -11.8  0.0222   17.0\n#>  8         18.6          150.    156.  -6.04 0.0152   17.0\n#>  9         17.4          158.    146.  11.1  0.0201   17.0\n#> 10         17.8          147.    150.  -2.68 0.0183   17.0\n#> # … with 90 more rows, and 2 more variables: .cooksd <dbl>,\n#> #   .std.resid <dbl>\nggplot(simulated_running_data, \n       aes(x = .resid)) + \n  geom_histogram(binwidth = 1) +\n  theme_classic() +\n  labs(y = \"Number of occurrences\",\n       x = \"Residuals\")\n\nggplot(simulated_running_data, \n       aes(x = five_km_time, y = .resid)) + \n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dotted\", color = \"grey\") +\n  theme_classic() +\n  labs(y = \"Residuals\",\n       x = \"Five-kilometer time (minutes)\")\n\nggplot(simulated_running_data, \n       aes(x = marathon_time, .fitted)) + \n  geom_point() +\n  theme_classic() +\n  labs(y = \"Estimated marathon time\",\n       x = \"Actual marathon time\")\nsimulated_running_data |> \n  ggplot(aes(x = five_km_time, y = marathon_time)) +\n  geom_point() + \n  geom_smooth(method = \"lm\", \n              se = TRUE, \n              color = \"black\", \n              linetype = \"dashed\",\n              formula = 'y ~ x') +\n  labs(x = \"Five-kilometer time (minutes)\",\n       y = \"Marathon time (minutes)\") +\n  theme_classic()"},{"path":"ijalm.html","id":"multiple-linear-regression","chapter":"14 It’s Just A Linear Model","heading":"14.3 Multiple linear regression","text":"point just considered one explanatory variable. usually one. One approach run separate regressions explanatory variable. compared separate linear regressions , adding explanatory variables allows us better understanding intercept accounts interaction. Often results quite different.may also like consider explanatory variables inherent ordering. instance: pregnant ; day night. two options can use binary variable, considered either 0 1. column character values two values, : c(\"Myles\", \"Ruth\", \"Ruth\", \"Myles\", \"Myles\", \"Ruth\"), using explanatory variable usual regression set , mean treated binary variable. two levels can use combination binary variables, ‘missing’ outcome (baseline) gets pushed intercept.can add additional explanatory variables lm() +.result probably surprising look plot data (Figure 14.6).\nFigure 14.6: Simple linear regression simulated data time someone takes run five kilometers marathon, binary variable whether raining\naddition wanting include additional explanatory variables, may think related another. instance, wanting explain amount snowfall, may interested humidity temperature, two variables may also interact. can using * instead + specify model. interact variables way, almost always need include individual variables well lm() default.variety threats validity linear regression estimates, aspects think . need address use , usually four graphs associated text sufficient assuage . Aspects concern include:Linearity explanatory variables. concerned whether independent variables enter linear way. Sometimes worried might multiplicative relationship explanatory variables, rather additive one, may consider logarithmic transform. can usually convinced enough linearity explanatory variables purposes using graphs variables.Independence errors. concerned errors correlated. instance, interested weather-related measurement average daily temperature, may find pattern temperature one day likely similar temperature another. can convinced satisfied condition making graphs errors, Figure 14.4.Homoscedasticity errors. concerned errors becoming systematically larger smaller throughout sample. happening, term heteroscedasticity. , graphs errors, Figure 14.4 used convince us .Normality errors. concerned errors normally distributed interested making individual-level predictions.Outliers high-impact observations. Finally, might worried results driven handful observations. instance, thinking back Chapter 6 Anscombe’s Quartet, notice linear regression estimates heavily influenced inclusion one two particular points. can become comfortable considering analysis various sub-setsThose aspects statistical concerns relate whether model working. important threat validity hence aspect must addressed length, speaking fact model appropriate circumstances addresses research question hand.point, just quick look regression results using summary(). better approach use modelsummary() modelsummary (Arel-Bundock 2021a) (Table 14.1).\nTable 14.1: Explaining marathon time based five-kilometer run times weather features\nfocused prediction, often want fit many models. One way copy paste code many times. nothing wrong . way people get started. need approach :scales easily;enables us think carefully -fitting; andadds model evaluation.use tidymodels (Kuhn Wickham 2020) satisfies criteria providing coherent grammar allows us easily fit variety models. Like tidyverse, package packages.focused prediction, worried -fitting data, limit ability make claims datasets. One way partially address split dataset training test datasets using initial_split().split data, create training test datasets.look training test datasets, can see placed dataset training dataset. use estimate parameters model. kept small amount back, use evaluate model.use tidymodels forecasting. focused inference, instead, use Bayesian approaches. use probabilistic programming language ‘Stan’, interface using rstanarm (Goodrich et al. 2020). keep separate, rather adapting Bayesian approaches within tidymodels, point ecosystems developed separately, best books go onto next also separate.order use Bayesian approaches need specify starting point, prior. another reason workflow advocated book; simulate stage leads directly priors. also thoroughly specify model interested :\\[\n\\begin{aligned}\ny_i &\\sim \\mbox{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 +\\beta_1x_i\\\\\n\\beta_0 &\\sim \\mbox{Normal}(0, 3) \\\\\n\\beta_1 &\\sim \\mbox{Normal}(0, 3) \\\\\n\\sigma &\\sim \\mbox{Normal}(0, 3) \\\\\n\\end{aligned}\n\\]practical note, one aspect different Bayesian approaches way modelling point, Bayesian models usually take longer run. , can useful run model, either within R Markdown document separate R script, save saveRDS(). sensible R Markdown chunk options, model can read R Markdown document readRDS(). way, model, hence delay, imposed given model.","code":"\nsimulated_running_data <-\n  simulated_running_data |>\n  mutate(was_raining = sample(\n    c(\"Yes\", \"No\"),\n    size = number_of_observations,\n    replace = TRUE,\n    prob = c(0.2, 0.8)\n  )) |> \n  select(five_km_time, marathon_time, was_raining)\n\nsimulated_running_data\n#> # A tibble: 100 × 3\n#>    five_km_time marathon_time was_raining\n#>           <dbl>         <dbl> <chr>      \n#>  1         20.4          152. No         \n#>  2         16.8          134. No         \n#>  3         22.3          198. No         \n#>  4         19.7          166. No         \n#>  5         15.6          163. No         \n#>  6         21.1          168. No         \n#>  7         17            131. No         \n#>  8         18.6          150. No         \n#>  9         17.4          158. No         \n#> 10         17.8          147. No         \n#> # … with 90 more rows\nsimulated_running_data_rain_model <-\n  lm(marathon_time ~ five_km_time + was_raining,\n     data = simulated_running_data)\n\nsummary(simulated_running_data_rain_model)\n#> \n#> Call:\n#> lm(formula = marathon_time ~ five_km_time + was_raining, data = simulated_running_data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -49.150  -8.828   0.968  10.522  58.224 \n#> \n#> Coefficients:\n#>                Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)      9.1030     9.0101   1.010    0.315    \n#> five_km_time     7.8660     0.4154  18.934   <2e-16 ***\n#> was_rainingYes   4.1673     4.5048   0.925    0.357    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 16.98 on 97 degrees of freedom\n#> Multiple R-squared:  0.7969, Adjusted R-squared:  0.7927 \n#> F-statistic: 190.3 on 2 and 97 DF,  p-value: < 2.2e-16\nsimulated_running_data |>\n  ggplot(aes(x = five_km_time, y = marathon_time, color = was_raining)) +\n  geom_point() +\n  geom_smooth(method = \"lm\",\n              color = \"black\", \n              linetype = \"dashed\",\n              formula = 'y ~ x') +\n  labs(x = \"Five-kilometer time (minutes)\",\n       y = \"Marathon time (minutes)\",\n       color = \"Was raining\") +\n  theme_classic() +\n  scale_color_brewer(palette = \"Set1\")\nsimulated_running_data <-\n  simulated_running_data |>\n  mutate(humidity = sample(\n    c(\"High\", \"Low\"),\n    size = number_of_observations,\n    replace = TRUE,\n    prob = c(0.2, 0.8)\n  ))\n\nsimulated_running_data\n#> # A tibble: 100 × 4\n#>    five_km_time marathon_time was_raining humidity\n#>           <dbl>         <dbl> <chr>       <chr>   \n#>  1         20.4          152. No          Low     \n#>  2         16.8          134. No          Low     \n#>  3         22.3          198. No          Low     \n#>  4         19.7          166. No          Low     \n#>  5         15.6          163. No          Low     \n#>  6         21.1          168. No          Low     \n#>  7         17            131. No          Low     \n#>  8         18.6          150. No          Low     \n#>  9         17.4          158. No          High    \n#> 10         17.8          147. No          Low     \n#> # … with 90 more rows\nsimulated_running_data_rain_and_humidity_model <-\n  lm(marathon_time ~ five_km_time + was_raining*humidity,\n     data = simulated_running_data)\n\nsummary(simulated_running_data_rain_and_humidity_model)\n#> \n#> Call:\n#> lm(formula = marathon_time ~ five_km_time + was_raining * humidity, \n#>     data = simulated_running_data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -48.904  -8.523   0.404  10.130  59.951 \n#> \n#> Coefficients:\n#>                            Estimate Std. Error t value\n#> (Intercept)                 15.0595    10.3144   1.460\n#> five_km_time                 7.7313     0.4167  18.552\n#> was_rainingYes              15.6008     9.6199   1.622\n#> humidityLow                 -3.7380     4.9569  -0.754\n#> was_rainingYes:humidityLow -14.5825    10.7410  -1.358\n#>                            Pr(>|t|)    \n#> (Intercept)                   0.148    \n#> five_km_time                 <2e-16 ***\n#> was_rainingYes                0.108    \n#> humidityLow                   0.453    \n#> was_rainingYes:humidityLow    0.178    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 16.79 on 95 degrees of freedom\n#> Multiple R-squared:  0.8054, Adjusted R-squared:  0.7972 \n#> F-statistic: 98.31 on 4 and 95 DF,  p-value: < 2.2e-16\nlibrary(modelsummary)\n\nmodelsummary(list(simulated_running_data_first_model,\n                  simulated_running_data_rain_model, \n                  simulated_running_data_rain_and_humidity_model),\n             fmt = 2,\n             title = \"Explaining marathon time based on five-kilometer run times and weather features\")\nlibrary(tidymodels)\n\nset.seed(853)\n\nsimulated_running_data_split <- \n  initial_split(data = simulated_running_data, \n                prop = 0.80)\n\nsimulated_running_data_split\n#> <Analysis/Assess/Total>\n#> <80/20/100>\nsimulated_running_data_train <- training(simulated_running_data_split)\n\nsimulated_running_data_train\n#> # A tibble: 80 × 4\n#>    five_km_time marathon_time was_raining humidity\n#>           <dbl>         <dbl> <chr>       <chr>   \n#>  1         17.4          158. No          High    \n#>  2         23.8          205. No          High    \n#>  3         23.4          198. Yes         Low     \n#>  4         22.3          175  No          Low     \n#>  5         19.3          158  No          Low     \n#>  6         24.4          204. No          High    \n#>  7         17            120  No          High    \n#>  8         19.1          178. No          Low     \n#>  9         22.3          198. No          Low     \n#> 10         20.6          166. No          Low     \n#> # … with 70 more rows\n\nsimulated_running_data_test <- testing(simulated_running_data_split)\n\nsimulated_running_data_test\n#> # A tibble: 20 × 4\n#>    five_km_time marathon_time was_raining humidity\n#>           <dbl>         <dbl> <chr>       <chr>   \n#>  1         17            131. No          Low     \n#>  2         16.6          118. No          Low     \n#>  3         19.6          164. No          Low     \n#>  4         21.1          164. Yes         Low     \n#>  5         21            180. No          Low     \n#>  6         27.9          246. No          Low     \n#>  7         23.7          198. No          High    \n#>  8         16            143  No          Low     \n#>  9         24.9          202. No          Low     \n#> 10         15.2          140. Yes         Low     \n#> 11         28.9          238. No          Low     \n#> 12         19.2          132  Yes         Low     \n#> 13         22            200. No          Low     \n#> 14         26.5          229  Yes         High    \n#> 15         25.3          222  Yes         High    \n#> 16         25.9          208. Yes         Low     \n#> 17         15.5          120  No          Low     \n#> 18         18            144. No          Low     \n#> 19         27.2          227. No          High    \n#> 20         20.8          201. No          Low\nsimulated_running_data_first_model_tidymodels <- \n  linear_reg() |>\n  set_engine(engine = \"lm\") |> \n  fit(marathon_time ~ five_km_time + was_raining, \n      data = simulated_running_data_train\n      )\n\nsimulated_running_data_first_model_tidymodels\n#> parsnip model object\n#> \n#> Fit time:  3ms \n#> \n#> Call:\n#> stats::lm(formula = marathon_time ~ five_km_time + was_raining, \n#>     data = data)\n#> \n#> Coefficients:\n#>    (Intercept)    five_km_time  was_rainingYes  \n#>         16.601           7.490           8.244\nlibrary(rstanarm)\n\nsimulated_running_data_first_model_rstanarm <-\n  stan_lm(\n    marathon_time ~ five_km_time + was_raining, \n    data = simulated_running_data,\n    prior = NULL,\n    seed = 853\n  )\n\n# simulated_running_data_first_model_rstanarm <-\n#   stan_lm(\n#     formula = marathon_time ~ five_km_time,\n#     data = simulated_running_data,\n#     prior = normal(0, 3),\n#     prior_intercept = normal(0, 3),\n#     prior_aux = normal(0, 3),\n#     seed = 853\n#     )\n\nsaveRDS(simulated_running_data_first_model_rstanarm,\n        file = \"simulated_running_data_first_model_rstanarm.rds\")\nsimulated_running_data_first_model_rstanarm\n#> stan_lm\n#>  family:       gaussian [identity]\n#>  formula:      marathon_time ~ five_km_time + was_raining\n#>  observations: 100\n#>  predictors:   3\n#> ------\n#>                Median MAD_SD\n#> (Intercept)    9.2    8.7   \n#> five_km_time   7.9    0.4   \n#> was_rainingYes 4.6    4.5   \n#> \n#> Auxiliary parameter(s):\n#>               Median MAD_SD\n#> R2             0.8    0.0  \n#> log-fit_ratio  0.0    0.0  \n#> sigma         17.1    1.3  \n#> \n#> ------\n#> * For help interpreting the printed output see ?print.stanreg\n#> * For info on the priors used see ?prior_summary.stanreg"},{"path":"ijalm.html","id":"logistic-regression","chapter":"14 It’s Just A Linear Model","heading":"14.4 Logistic regression","text":"Linear regression nice way come understand better data. assumes continuous outcome variable can take number real line. like way use machinery satisfy condition. turn logistic Poisson regression binary count outcome variables, respectively.Logistic regression close variants useful variety settings, elections (W. Wang et al. 2015) horse racing (Chellel 2018; Bolton Chapman 1986). use logistic regression dependent variable binary outcome, 0 1. Although presence binary outcome variable may sound limiting, lot circumstances outcome either naturally falls situation, can adjusted .reason use logistic regression modelling probability bounded 0 1. Whereas linear regression may end values outside . said, logistic regression, Daniella Witten teaches us, just linear model. foundation logistic regression logit function:\\[\n\\mbox{logit}(x) = \\log\\left(\\frac{x}{1-x}\\right),\n\\]\ntranspose values 0 1, onto real line. instance, logit(0.1) = -2.2, logit(0.5) = 0, logit(0.9) = 2.2.simulate data whether day night, based number cars can see.linear regression, logistic regression can use glm() base put together quick model summary() look . case try work whether day night, based number cars can see. interested estimating Equation (14.1):\n\\[\n\\mbox{Pr}(y_i=1) = \\mbox{logit}^{-1}\\left(\\beta_0+\\beta_1 x_i\\right). \\tag{14.1}\n\\]One reason logistic regression can bit tricky initially, coefficients take bit work interpret. particular, estimate likelihood night 0.91 odds. , odds night, increase 0.91 number cars saw increases. can translate result probabilities using augment() broom (D. Robinson, Hayes, Couch 2021) allows us graph results (Figure 14.7).\nFigure 14.7: Logistic regression probability results simulated data whether day night based number cars around\ncan use tidymodels run wanted. order , first need change class dependent variable factor., can make graph actual results compared estimates. one nice aspect use test dataset thoroughly evaluate model’s forecasting ability, instance confusion matrix. find model well held-dataset.Finally, might interested inference, want build Bayesian model using rstanarm. , fully specify model:Finally, can build Bayesian model estimate rstanarm.\\[\n\\begin{aligned}\n\\mbox{Pr}(y_i=1) & = \\mbox{logit}^{-1}\\left(\\beta_0+\\beta_1 x_i\\right)\\\\\n\\beta_0 & \\sim \\mbox{Normal}(0, 3)\\\\\n\\beta_1 & \\sim \\mbox{Normal}(0, 3)\n\\end{aligned}\n\\]","code":"\nlibrary(tidyverse)\n\nset.seed(853)\n\nday_or_night <- \n  tibble(\n    number_of_cars = runif(n = 1000, min = 0, 100),\n    noise = rnorm(n = 1000, mean = 0, sd = 2),\n    is_night = if_else(number_of_cars + noise > 50, 1, 0)\n  ) |> \n  mutate(number_of_cars = round(number_of_cars)) |> \n  select(-noise)\n  \nday_or_night\n#> # A tibble: 1,000 × 2\n#>    number_of_cars is_night\n#>             <dbl>    <dbl>\n#>  1             36        0\n#>  2             12        0\n#>  3             48        0\n#>  4             32        0\n#>  5              4        0\n#>  6             40        0\n#>  7             13        0\n#>  8             24        0\n#>  9             16        0\n#> 10             19        0\n#> # … with 990 more rows\nday_or_night_model <- \n  glm(is_night ~ number_of_cars,\n      data = day_or_night,\n      family = 'binomial')\n#> Warning: glm.fit: fitted probabilities numerically 0 or 1\n#> occurred\n\nsummary(day_or_night_model)\n#> \n#> Call:\n#> glm(formula = is_night ~ number_of_cars, family = \"binomial\", \n#>     data = day_or_night)\n#> \n#> Deviance Residuals: \n#>      Min        1Q    Median        3Q       Max  \n#> -2.39419  -0.00002   0.00000   0.00002   2.33776  \n#> \n#> Coefficients:\n#>                Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)    -45.5353     7.3389  -6.205 5.48e-10 ***\n#> number_of_cars   0.9121     0.1470   6.205 5.47e-10 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 1386.194  on 999  degrees of freedom\n#> Residual deviance:   77.243  on 998  degrees of freedom\n#> AIC: 81.243\n#> \n#> Number of Fisher Scoring iterations: 11\nlibrary(broom)\n\nday_or_night <-\n  augment(day_or_night_model,\n          data = day_or_night,\n          type.predict = \"response\")\n\nday_or_night\n#> # A tibble: 1,000 × 8\n#>    number_of_cars is_night  .fitted        .resid .std.resid\n#>             <dbl>    <dbl>    <dbl>         <dbl>      <dbl>\n#>  1             36        0 3.06e- 6 -0.00247        -2.47e-3\n#>  2             12        0 2.22e-16 -0.0000000211   -2.11e-8\n#>  3             48        0 1.48e- 1 -0.565          -5.71e-1\n#>  4             32        0 7.95e- 8 -0.000399       -3.99e-4\n#>  5              4        0 2.22e-16 -0.0000000211   -2.11e-8\n#>  6             40        0 1.17e- 4 -0.0153         -1.53e-2\n#>  7             13        0 2.22e-16 -0.0000000211   -2.11e-8\n#>  8             24        0 5.39e-11 -0.0000104      -1.04e-5\n#>  9             16        0 2.22e-16 -0.0000000211   -2.11e-8\n#> 10             19        0 5.63e-13 -0.00000106     -1.06e-6\n#> # … with 990 more rows, and 3 more variables: .hat <dbl>,\n#> #   .sigma <dbl>, .cooksd <dbl>\nday_or_night |>\n  mutate(is_night = factor(is_night)) |>\n  ggplot(aes(x = number_of_cars,\n             y = .fitted,\n             color = is_night)) +\n  geom_jitter(width = 0.01, height = 0.01) +\n  labs(x = \"Number of cars that were seen\",\n       y = \"Estimated probability it is night\",\n       color = \"Was actually night\") +\n  theme_classic() +\n  scale_color_brewer(palette = \"Set1\")\nset.seed(853)\n\nday_or_night <-\n  day_or_night |>\n  mutate(is_night = as_factor(is_night))\n\nday_or_night_split <- initial_split(day_or_night, prop = 0.80)\nday_or_night_train <- training(day_or_night_split)\nday_or_night_test <- testing(day_or_night_split)\n\nday_or_night_tidymodels <-\n  logistic_reg(mode = \"classification\") |>\n  set_engine(\"glm\") |>\n  fit(is_night ~ number_of_cars,\n      data = day_or_night_train)\n#> Warning: glm.fit: fitted probabilities numerically 0 or 1\n#> occurred\n\nday_or_night_tidymodels\n#> parsnip model object\n#> \n#> Fit time:  6ms \n#> \n#> Call:  stats::glm(formula = is_night ~ number_of_cars, family = stats::binomial, \n#>     data = data)\n#> \n#> Coefficients:\n#>    (Intercept)  number_of_cars  \n#>       -44.4817          0.8937  \n#> \n#> Degrees of Freedom: 799 Total (i.e. Null);  798 Residual\n#> Null Deviance:       1109 \n#> Residual Deviance: 62.5  AIC: 66.5\nday_or_night_tidymodels |>\n  predict(new_data = day_or_night_test) |>\n  cbind(day_or_night_test) |>\n  conf_mat(truth = is_night, estimate = .pred_class)\n#>           Truth\n#> Prediction   0   1\n#>          0  95   0\n#>          1   3 102\nday_or_night_rstanarm <-\n  stan_glm(\n    is_night ~ number_of_cars,\n    data = day_or_night,\n    family = binomial(link = \"logit\"),\n    prior = normal(0, 3),\n    prior_intercept = normal(0, 3),\n    seed = 853\n  )\n\nsaveRDS(day_or_night_rstanarm,\n        file = \"day_or_night_rstanarm.rds\")\nday_or_night_rstanarm\n#> stan_glm\n#>  family:       binomial [logit]\n#>  formula:      is_night ~ number_of_cars\n#>  observations: 1000\n#>  predictors:   2\n#> ------\n#>                Median MAD_SD\n#> (Intercept)    -47.3    8.0 \n#> number_of_cars   0.9    0.2 \n#> \n#> ------\n#> * For help interpreting the printed output see ?print.stanreg\n#> * For info on the priors used see ?prior_summary.stanreg"},{"path":"ijalm.html","id":"poisson-regression","chapter":"14 It’s Just A Linear Model","heading":"14.5 Poisson regression","text":"count data, initially think use Poisson distribution. Poisson distribution interesting feature mean also variance, mean increases, variance. , Poisson distribution governed parameter, \\(\\lambda\\) distributes probabilities non-negative integers. Poisson distribution (Pitman 1993, 121):\\[P_{\\lambda}(k) = e^{-\\lambda}\\mu^k/k!\\mbox{, }k=0,1,2,...\\]\ncan simulate \\(n=20\\) draws Poisson distribution rpois(), \\(\\lambda\\) mean variance. \\(\\lambda\\) parameter governs shape distribution (Figure 14.8).\nFigure 14.8: Poisson distribution governed value mean, variance\ninstance, look number + grades awarded university course given term course count.simulated dataset number + grades awarded courses, structured within departments. can use glm() summary() base quickly get sense data.interpretation coefficient ‘department2’ log expected difference departments. expect \\(\\exp(1.85345) \\approx 6.3\\) \\(\\exp(1.00663) \\approx 2.7\\) additional + grades departments 2 3, compared department 1.can use tidymodels estimate Poisson regression models poissonreg (Kuhn 2021).finally, can build Bayesian model estimate rstanarm. put tight prior coefficients propensity Poisson distribution expand substantially.\\[\n\\begin{aligned}\ny_i &\\sim \\mbox{Poisson}(\\lambda_i)\\\\\n\\log(\\lambda_i) & = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\\\\n\\beta_0 & \\sim \\mbox{Normal}(0, 0.5)\\\\\n\\beta_1 & \\sim \\mbox{Normal}(0, 0.5)\\\\\n\\beta_2 & \\sim \\mbox{Normal}(0, 0.5)\n\\end{aligned}\n\\]","code":"\nrpois(n = 20, lambda = 3)\n#>  [1] 1 5 5 4 5 2 1 4 5 4 6 2 3 4 4 6 5 1 3 5\nset.seed(853)\n\nnumber_of_each <- 1000\n\ntibble(\n  lambda = c(\n    rep(0, number_of_each),\n    rep(1, number_of_each),\n    rep(2, number_of_each),\n    rep(4, number_of_each),\n    rep(7, number_of_each),\n    rep(10, number_of_each),\n    rep(15, number_of_each),\n    rep(50, number_of_each),\n    rep(100, number_of_each)\n  ),\n  draw = c(\n    rpois(n = number_of_each, lambda = 0),\n    rpois(n = number_of_each, lambda = 1),\n    rpois(n = number_of_each, lambda = 2),\n    rpois(n = number_of_each, lambda = 4),\n    rpois(n = number_of_each, lambda = 7),\n    rpois(n = number_of_each, lambda = 10),\n    rpois(n = number_of_each, lambda = 15),\n    rpois(n = number_of_each, lambda = 50),\n    rpois(n = number_of_each, lambda = 100)\n  )\n) |>\n  ggplot(aes(x = draw)) +\n  geom_density() +\n  facet_wrap(vars(lambda),\n             scales = \"free_y\") +\n  theme_minimal() +\n  labs(x = 'Integer',\n       y = 'Density')\nset.seed(853)\n\ncount_of_A_plus <-\n  tibble(\n    # Thanks to Chris DuBois: https://stackoverflow.com/a/1439843\n    department = c(rep.int(\"1\", 26), rep.int(\"2\", 26), rep.int(\"3\", 26)),\n    course = c(paste0(\"DEP_1_\", letters), paste0(\"DEP_2_\", letters), paste0(\"DEP_3_\", letters)),\n    number_of_A_plus = c(\n      sample(c(1:10),\n             size = 26,\n             replace = TRUE),\n      sample(c(1:50),\n             size = 26,\n             replace = TRUE),\n      sample(c(1:25),\n             size = 26,\n             replace = TRUE)\n    )\n  )\n\ncount_of_A_plus\n#> # A tibble: 78 × 3\n#>    department course  number_of_A_plus\n#>    <chr>      <chr>              <int>\n#>  1 1          DEP_1_a                9\n#>  2 1          DEP_1_b               10\n#>  3 1          DEP_1_c                1\n#>  4 1          DEP_1_d                5\n#>  5 1          DEP_1_e                2\n#>  6 1          DEP_1_f                4\n#>  7 1          DEP_1_g                3\n#>  8 1          DEP_1_h                3\n#>  9 1          DEP_1_i                1\n#> 10 1          DEP_1_j                3\n#> # … with 68 more rows\ngrades_model_base <- \n  glm(number_of_A_plus ~ department, \n    data = count_of_A_plus, \n    family = 'poisson')\n\nsummary(grades_model_base)\n#> \n#> Call:\n#> glm(formula = number_of_A_plus ~ department, family = \"poisson\", \n#>     data = count_of_A_plus)\n#> \n#> Deviance Residuals: \n#>    Min      1Q  Median      3Q     Max  \n#> -6.739  -1.210  -0.171   1.424   3.952  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  1.44238    0.09535  15.128   <2e-16 ***\n#> department2  1.85345    0.10254  18.075   <2e-16 ***\n#> department3  1.00663    0.11141   9.035   <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for poisson family taken to be 1)\n#> \n#>     Null deviance: 952.12  on 77  degrees of freedom\n#> Residual deviance: 450.08  on 75  degrees of freedom\n#> AIC: 768.21\n#> \n#> Number of Fisher Scoring iterations: 5\nlibrary(poissonreg)\n\nset.seed(853)\n\ncount_of_A_plus_split <-\n  rsample::initial_split(count_of_A_plus, prop = 0.80)\ncount_of_A_plus_train <- rsample::training(count_of_A_plus_split)\ncount_of_A_plus_test <- rsample::testing(count_of_A_plus_split)\n\na_plus_model_tidymodels <-\n  poisson_reg(mode = \"regression\") |>\n  set_engine(\"glm\") |>\n  fit(number_of_A_plus ~ department,\n      data = count_of_A_plus_train)\n\na_plus_model_tidymodels\n#> parsnip model object\n#> \n#> Fit time:  2ms \n#> \n#> Call:  stats::glm(formula = number_of_A_plus ~ department, family = stats::poisson, \n#>     data = data)\n#> \n#> Coefficients:\n#> (Intercept)  department2  department3  \n#>       1.470        1.925        1.011  \n#> \n#> Degrees of Freedom: 61 Total (i.e. Null);  59 Residual\n#> Null Deviance:       758 \n#> Residual Deviance: 276.8     AIC: 534.8\ncount_of_A_plus_rstanarm <-\n  stan_glm(\n    number_of_A_plus ~ department,\n    data = count_of_A_plus,\n    family = poisson(link = \"log\"),\n    prior = normal(0, 0.5),\n    prior_intercept = normal(0, 0.5),\n    seed = 853\n  )\n\nsaveRDS(count_of_A_plus_rstanarm,\n        file = \"count_of_A_plus_rstanarm.rds\")\ncount_of_A_plus_rstanarm\n#> stan_glm\n#>  family:       poisson [log]\n#>  formula:      number_of_A_plus ~ department\n#>  observations: 78\n#>  predictors:   3\n#> ------\n#>             Median MAD_SD\n#> (Intercept) 1.5    0.1   \n#> department2 1.8    0.1   \n#> department3 0.9    0.1   \n#> \n#> ------\n#> * For help interpreting the printed output see ?print.stanreg\n#> * For info on the priors used see ?prior_summary.stanreg"},{"path":"ijalm.html","id":"exercises-and-tutorial-13","chapter":"14 It’s Just A Linear Model","heading":"14.6 Exercises and tutorial","text":"","code":""},{"path":"ijalm.html","id":"exercises-13","chapter":"14 It’s Just A Linear Model","heading":"14.6.1 Exercises","text":"Please write linear relationship response variable, Y, predictor, X. intercept term? slope term? adding hat indicate?least squares criterion? Similarly, RSS trying run least squares regression?statistical bias?three variables: Snow, Temperature, Wind, please write R code fit simple linear regression explain Snow function Temperature Wind. think another explanatory variable - daily stock market returns - model?According Greenland et al. (2016), p-values test (pick one)?\nassumptions data generated (entire model), just targeted hypothesis supposed test (null hypothesis).\nWhether hypothesis targeted testing true .\ndichotomy whereby results can declared ‘statistically significant’.\nassumptions data generated (entire model), just targeted hypothesis supposed test (null hypothesis).Whether hypothesis targeted testing true .dichotomy whereby results can declared ‘statistically significant’.According Greenland et al. (2016), p-value may small (select )?\ntargeted hypothesis false.\nstudy protocols violated.\nselected presentation based small size.\ntargeted hypothesis false.study protocols violated.selected presentation based small size.According Obermeyer et al. (2019), racial bias occur algorithm used guide health decisions US (pick one)?\nalgorithm uses health costs proxy health needs.\nalgorithm trained Reddit data.\nalgorithm uses health costs proxy health needs.algorithm trained Reddit data.use logistic regression (pick one)?\nContinuous dependent variable.\nBinary dependent variable.\nCount dependent variable.\nContinuous dependent variable.Binary dependent variable.Count dependent variable.interested studying voting intentions recent US presidential election vary individual’s income. set logistic regression model study relationship. study, one possible dependent variable (pick one)?\nWhether respondent US citizen (yes/)\nrespondent’s personal income (high/low)\nWhether respondent going vote Trump (yes/)\nrespondent voted 2016 (Trump/Clinton)\nWhether respondent US citizen (yes/)respondent’s personal income (high/low)Whether respondent going vote Trump (yes/)respondent voted 2016 (Trump/Clinton)interested studying voting intentions recent US presidential election vary individual’s income. set logistic regression model study relationship. study, one possible dependent variable (pick one)?\nrace respondent (white/white)\nrespondent’s marital status (married/)\nWhether respondent registered vote (yes/)\nWhether respondent going vote Biden (yes/)\nrace respondent (white/white)respondent’s marital status (married/)Whether respondent registered vote (yes/)Whether respondent going vote Biden (yes/)Please explain p-value , using term (.e. ‘p-value’) words amongst 1,000 common English language according XKCD Simple Writer - https://xkcd.com/simplewriter/. (Please write one two paragraphs.)mean Poisson distribution equal ?\nMedian.\nStandard deviation.\nVariance.\nMedian.Standard deviation.Variance.power (statistical context)?According McElreath (2020, 162) ‘Regression sort . Regression indeed oracle, cruel one. speaks riddles delights punishing us …’ (please select one answer)?\novercomplicating models.\nasking bad questions.\nusing bad data.\novercomplicating models.asking bad questions.using bad data.model fits small large world important , ?","code":""},{"path":"ijalm.html","id":"tutorial-13","chapter":"14 It’s Just A Linear Model","heading":"14.6.2 Tutorial","text":"Simulate data similar discussed Gould (2013). build regression model. Discuss results","code":""},{"path":"ijalm.html","id":"paper-4","chapter":"14 It’s Just A Linear Model","heading":"14.6.3 Paper","text":"point, Paper Four (Appendix B.4) appropriate.","code":""},{"path":"causality.html","id":"causality","chapter":"15 Causality from observational data","heading":"15 Causality from observational data","text":"Required materialRead BNT162b2 mRNA Covid-19 Vaccine Nationwide Mass Vaccination Setting, (Dagan et al. 2021)Read Effect: Introduction Research Design Causality, Chapters 18 ‘Difference--Differences’, 19 ‘Instrumental Variables’, 20 ‘Regression Discontinuity’, (Huntington-Klein 2021)Read Understanding regression discontinuity designs observational studies, (Sekhon Titiunik 2017)Key concepts skillsBeing able put together DAGs.Essential matching methods weaknesses matching.Implementing difference differences.Identifying opportunities instrumental variables implementing .Challenges validity instrumental variables.Reading foreign data.Understanding regression discontinuity implementing manually using packages.Appreciating threats validity regression discontinuity.Key librariesbroom (D. Robinson, Hayes, Couch 2021)DiagrammeR (Iannone 2020)estimatr [estimatr]haven [citehaven]MatchIt (Ho et al. 2011)modelsummary (Arel-Bundock 2021a)rdrobust (Calonico et al. 2021)scales (Wickham Seidel 2020)tidyverse (Wickham 2017)Key functionsDiagrammeR::grViz()estimatr::iv_robust()haven::read_dta()MatchIt::matchit()modelsummary::datasummary_skim()modelsummary::modelsummary()poly()rdrobust::rdrobust()scales::dollar_format()","code":""},{"path":"causality.html","id":"introduction-12","chapter":"15 Causality from observational data","heading":"15.1 Introduction","text":"Life grand can conduct experiments able speak causality. circumstances run experiment, nonetheless want able make causal claims. data outside experiments value experiments . chapter discuss circumstances methods allow us speak causality using observational data. use relatively simple methods, sophisticated ways, drawing statistics, also variety social sciences, including economics, political science, well epidemiology.instance, Dagan et al. (2021) use observational data confirm effectiveness Pfizer-BioNTech vaccine. discuss one concern using observational data way confounding, concerned variable affects explanatory dependent variables can lead spurious relationships. Dagan et al. (2021) adjust first making list potential confounders, age, sex, geographic location, healthcare usage adjusting , matching, one--one people vaccinated . experimental data guided use observational data, larger size later enabled focus specific age-groups extent disease.Using observational data sophisticated ways chapter . can nonetheless comfortable making causal statements, even run /B tests RCTs. Indeed, circumstances may prefer run run observational-based approaches addition . cover three major methods: difference differences; regression discontinuity; instrumental variables.","code":""},{"path":"causality.html","id":"directed-acyclic-graphs","chapter":"15 Causality from observational data","heading":"15.2 Directed acyclic graphs","text":"discussing causality, can help specific mean. easy get caught observational data trick . important think hard, use tools available us. instance, earlier example, Dagan et al. (2021) able use experimental data guide. time, lucky experimental data observational data available us. one framework can help thinking hard data use directed acyclic graph (DAG). DAGs fancy name flow diagram involves drawing arrows lines variables indicate relationship . Following Igelström (2020) use DiagrammeR (Iannone 2020) build , can use skills outside just DAGs DiagrammeR provides quite lot control (Figure 15.1). ggdag also useful (Barrett 2021b).\nFigure 15.1: Using DAG illustrate perceived relationships\nFigure 15.1, think x causes y. build another DAG situation less clear. make examples little easier follow, switch fruits (Figure 15.2).\nFigure 15.2: DAG showing Carrot confounder\nFigure 15.2, think Apple causes Banana. also think Carrot causes Banana, Carrot also causes Apple. relationship ‘backdoor path’, create spurious correlation analysis. may think changes Apple causing changes Banana, Carrot changing . variable, case, Carrot, called ‘confounder’.Hernan Robins (2020, 83) discuss interesting case researcher interested whether one person looking sky makes others look sky also. clear relationship responses people. also case noise sky. , unclear whether second person looked first person looked , looked noise. using experimental data, randomization allows us avoid concern, observational data rely . also case bigger data necessarily get around problem us. Instead, important think carefully situation.confounders, still interested causal effects, need adjust . One way include regression. validity requires several assumptions. particular, Gelman Hill (2007, 169) warn estimate correspond average causal effect sample include confounders right model. Putting second requirement one side, focusing first, think observe confounder, can difficult adjust . area domain expertise theory can bring lot analysis.Figure 15.3 similar situation , may think Apple causes Banana. Figure 15.3 Apple also causes Carrot, causes Banana.\nFigure 15.3: DAG showing Carrot mediator\nFigure 15.3, Carrot called ‘mediator’ adjust interested effect Apple Banana. adjust , attributing Apple, due Carrot.Finally, Figure 15.4 yet another similar situation, , think Apple causes Banana. time Apple Banana also cause Carrot.\nFigure 15.4: DAG showing Carrot collider\ncase, Carrot called ‘collider’ condition , create misleading relationship.important clear : must create DAG , way must put together model . nothing create us. means need think carefully situation. one thing see something DAG something . another even know . McElreath (2020, 180) describes haunted DAGs. DAGs helpful, just tool help us think deeply situation.two situations data can trick us common explicitly go . : 1) Simpson’s paradox, 2) Berkson’s paradox. important keep situations mind, use DAGs can help identify .Simpson’s paradox occurs estimate relationship subsets data, different relationship consider entire dataset (Simpson 1951). instance, may positive relationship undergraduate grades performance graduate school two departments considering department individually. undergraduate grades tended higher one department another graduate school performance tended opposite, may find negative relationship undergraduate grades performance graduate school. can simulate data show clearly (Figure 15.5).\nFigure 15.5: Illustration simulated data shows Simpson’s paradox\nBerkson’s paradox occurs estimate relationship based dataset . dataset selected, relationship different general dataset (Berkson 1946). instance, dataset professional cyclists might find relationship VO2 max chance winning bike race. dataset general population might find relationship two variables. professional dataset just selected relationship disappears; one become professional cyclist unless one good-enough VO2 max. , can simulate data show clearly (Figure 15.6).\nFigure 15.6: Illustration simulated data shows Berkson’s paradox\n","code":"\nlibrary(DiagrammeR)\n\nDiagrammeR::grViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape = plaintext, fontsize = 10, fontname = Helvetica]\n    x\n    y\n  edge [minlen = 2, arrowhead = vee]\n    x->y\n  { rank = same; x; y }\n}\n\", height = 200)\nDiagrammeR::grViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape = plaintext, fontsize = 10, fontname = Helvetica]\n    Apple\n    Banana\n    Carrot\n  edge [minlen = 2, arrowhead = vee]\n    Apple->Banana\n    Carrot->Apple\n    Carrot->Banana\n  { rank = same; Apple; Banana }\n}\n\", height = 300)\nDiagrammeR::grViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape = plaintext, fontsize = 10, fontname = Helvetica]\n    Apple\n    Banana\n    Carrot\n  edge [minlen = 2, arrowhead = vee]\n    Apple->Banana\n    Apple->Carrot\n    Carrot->Banana\n  { rank = same; Apple; Banana }\n}\n\", height = 300)\nDiagrammeR::grViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape = plaintext, fontsize = 10, fontname = Helvetica]\n    Apple\n    Banana\n    Carrot\n  edge [minlen = 2, arrowhead = vee]\n    Apple->Banana\n    Apple->Carrot\n    Banana->Carrot\n  { rank = same; Apple; Banana }\n}\n\", height = 300)\nlibrary(tidyverse)\n\nset.seed(853)\n\nnumber_in_each <- 1000\n\ndepartment_one <-\n  tibble(\n    undergrad = runif(n = number_in_each, min = 0.7, max = 0.9),\n    noise = rnorm(n = number_in_each, 0, sd = 0.1),\n    grad = undergrad + noise,\n    type = \"Department 1\"\n  )\n\ndepartment_two <-\n  tibble(\n    undergrad = runif(n = number_in_each, min = 0.6, max = 0.8),\n    noise = rnorm(n = number_in_each, 0, sd = 0.1),\n    grad = undergrad + noise + 0.3,\n    type = \"Department 2\"\n  )\n\nboth_departments <- rbind(department_one, department_two)\n\nboth_departments\n#> # A tibble: 2,000 × 4\n#>    undergrad   noise  grad type        \n#>        <dbl>   <dbl> <dbl> <chr>       \n#>  1     0.772 -0.0566 0.715 Department 1\n#>  2     0.724 -0.0312 0.693 Department 1\n#>  3     0.797  0.0770 0.874 Department 1\n#>  4     0.763 -0.0664 0.697 Department 1\n#>  5     0.707  0.0717 0.779 Department 1\n#>  6     0.781 -0.0165 0.764 Department 1\n#>  7     0.726 -0.104  0.623 Department 1\n#>  8     0.749  0.0527 0.801 Department 1\n#>  9     0.732 -0.0471 0.684 Department 1\n#> 10     0.738  0.0552 0.793 Department 1\n#> # … with 1,990 more rows\nboth_departments |>\n  ggplot(aes(x = undergrad, y = grad)) +\n  geom_point(aes(color = type), alpha = 0.1) +\n  geom_smooth(aes(color = type), method = 'lm', formula = 'y ~ x') +\n  geom_smooth(method = 'lm',\n              formula = 'y ~ x',\n              color = 'black') +\n  labs(x = \"Undergraduate results\",\n       y = \"Graduate results\",\n       color = \"Department\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\nset.seed(853)\n\nnumber_of_pros <- 100\n\nnumber_of_public <- 1000\n\nprofessionals <-\n  tibble(\n    VO2 = runif(n = number_of_pros, min = 0.7, max = 0.9),\n    chance_of_winning = runif(n = number_of_pros, min = 0.7, max = 0.9),\n    type = \"Professionals\"\n  )\n\ngeneral_public <-\n  tibble(\n    VO2 = runif(n = number_of_public, min = 0.6, max = 0.8),\n    noise = rnorm(n = number_of_public, 0, sd = 0.03),\n    chance_of_winning = VO2 + noise + 0.1,\n    type = \"Public\"\n  ) |>\n  select(-noise)\n\nprofessionals_and_public = rbind(professionals, general_public)\n\nprofessionals_and_public\n#> # A tibble: 1,100 × 3\n#>      VO2 chance_of_winning type         \n#>    <dbl>             <dbl> <chr>        \n#>  1 0.772             0.734 Professionals\n#>  2 0.724             0.773 Professionals\n#>  3 0.797             0.772 Professionals\n#>  4 0.763             0.754 Professionals\n#>  5 0.707             0.843 Professionals\n#>  6 0.781             0.740 Professionals\n#>  7 0.726             0.803 Professionals\n#>  8 0.749             0.750 Professionals\n#>  9 0.732             0.890 Professionals\n#> 10 0.738             0.821 Professionals\n#> # … with 1,090 more rows\nprofessionals_and_public |> \n  ggplot(aes(x = VO2, y = chance_of_winning)) +\n  geom_point(aes(color = type), alpha = 0.1) +\n  geom_smooth(aes(color = type), method = 'lm', formula = 'y ~ x') +\n  geom_smooth(method = 'lm', formula = 'y ~ x', color = 'black') +\n  labs(x = \"VO2 max\",\n       y = \"Chance of winning a bike race\",\n       color = \"Type\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")"},{"path":"causality.html","id":"difference-in-differences","chapter":"15 Causality from observational data","heading":"15.3 Difference in differences","text":"","code":""},{"path":"causality.html","id":"overview","chapter":"15 Causality from observational data","heading":"15.3.1 Overview","text":"ideal situation able conduct experiment rarely possible. Can reasonably expect Netflix allow us change prices? even , let us , , ? , rarely can explicitly create treatment control groups. Finally, experiments can expensive unethical. Instead, need make . Rather counterfactual coming us randomization, hence us knowing two treatment, try identify groups similar treatment, hence differences can attributed treatment.observational data, sometimes differences two groups treat. Provided pre-treatment differences satisfy assumptions essentially amount differences consistent, expect consistency continue absence treatment—‘parallel trends’ assumption—can look difference differences effect treatment. One aspects difference differences analysis can using relatively straight-forward methods. Linear regression binary variable enough get started convincing job.Consider wanting know effect new tennis racket serve speed. One way test measure difference , say, Roger Federer’s serve speed without tennis racket serve speed enthusiastic amateur, let us call Ville, tennis racket. Yes, find difference, know much attribute tennis racket? Another way consider difference Ville’s serve speed without new tennis racket Ville’s serve speed new tennis racket. serves just getting faster naturally time? Instead, combine two approaches look difference differences.begin measuring Federer’s serve speed compare Ville’s serve speed, without new racket. measure Federer’s serve speed , measure Ville’s serve speed new racket. difference differences estimate effect new racket. key assumptions need make analysis appropriate:something else may affected Ville, Federer affect Ville’s serve speed?likely Federer Ville trajectory serve speed improvement? ‘parallel trends’ assumption, dominates many discussions difference differences analysis.Finally, likely variance serve speeds Federer Ville ?Despite requirements, difference differences powerful approach need treatment control group treatment. just need good idea differed.","code":""},{"path":"causality.html","id":"simulated-example","chapter":"15 Causality from observational data","heading":"15.3.2 Simulated example","text":"specific situation, simulate data. simulate situation initially difference one serve speeds different people, new tennis racket, difference six. can use graph illustrate situation (Figure 15.7).\nFigure 15.7: Illustration simulated data shows difference getting new tennis racket\nstraight-forward example, can obtain estimate manually, looking average difference differences. , find estimate effect new tennis racket 5.06, similar simulated.can use linear regression get result. equation interested :\n\\[Y_{,t} = \\beta_0 + \\beta_1\\mbox{Treatment binary}_i + \\beta_2\\mbox{Time binary}_t + \\beta_3(\\mbox{Treatment binary} \\times\\mbox{Time binary})_{,t} + \\epsilon_{,t}\\]include separate aspects well, estimate interaction interested . case \\(\\beta_3\\). find estimated effect 5.06.","code":"\nlibrary(tidyverse)\n\nset.seed(853)\n\nsimulated_difference_in_differences <-\n  tibble(\n    person = rep(c(1:1000), times = 2),\n    time = c(rep(0, times = 1000), rep(1, times = 1000)),\n    treatment_group = rep(\n      sample(\n        x = 0:1,\n        size  = 1000,\n        replace = TRUE\n        ), \n      times = 2)\n    ) |>\n  mutate(treatment_group = as.factor(treatment_group),\n         time = as.factor(time)\n  )\n\n\nsimulated_difference_in_differences <-\n  simulated_difference_in_differences |>\n  rowwise() |>\n  mutate(\n    serve_speed = case_when(\n      time == 0 & treatment_group == 0 ~ rnorm(n = 1, mean = 5, sd = 1),\n      time == 1 & treatment_group == 0 ~ rnorm(n = 1, mean = 6, sd = 1),\n      time == 0 & treatment_group == 1 ~ rnorm(n = 1, mean = 8, sd = 1),\n      time == 1 & treatment_group == 1 ~ rnorm(n = 1, mean = 14, sd = 1)\n    )\n  )\n\nsimulated_difference_in_differences\n#> # A tibble: 2,000 × 4\n#> # Rowwise: \n#>    person time  treatment_group serve_speed\n#>     <int> <fct> <fct>                 <dbl>\n#>  1      1 0     0                      4.43\n#>  2      2 0     1                      6.96\n#>  3      3 0     1                      7.77\n#>  4      4 0     0                      5.31\n#>  5      5 0     0                      4.09\n#>  6      6 0     0                      4.85\n#>  7      7 0     0                      6.43\n#>  8      8 0     0                      5.77\n#>  9      9 0     1                      6.13\n#> 10     10 0     1                      7.32\n#> # … with 1,990 more rows\nsimulated_difference_in_differences |>\n  ggplot(aes(x = time,\n             y = serve_speed,\n             color = treatment_group)) +\n  geom_point(alpha = 0.2) +\n  geom_line(aes(group = person), alpha = 0.1) +\n  theme_minimal() +\n  labs(x = \"Time period\",\n       y = \"Serve speed\",\n       color = \"Person got a new racket\") +\n  scale_color_brewer(palette = \"Set1\")\naverage_differences <-\n  simulated_difference_in_differences |>\n  pivot_wider(names_from = time,\n              values_from = serve_speed,\n              names_prefix = \"time_\") |>\n  mutate(difference = time_1 - time_0) |>\n  group_by(treatment_group) |>\n  summarize(average_difference = mean(difference))\n\naverage_differences$average_difference[2] - average_differences$average_difference[1]\n#> [1] 5.058414\ndiff_in_diff_example_regression <- \n  lm(serve_speed ~ treatment_group*time, \n     data = simulated_difference_in_differences)\n\nsummary(diff_in_diff_example_regression)\n#> \n#> Call:\n#> lm(formula = serve_speed ~ treatment_group * time, data = simulated_difference_in_differences)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -4.1415 -0.6638 -0.0039  0.6708  3.2664 \n#> \n#> Coefficients:\n#>                        Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)             4.97131    0.04281  116.12   <2e-16\n#> treatment_group1        3.03350    0.06225   48.73   <2e-16\n#> time1                   1.00680    0.06055   16.63   <2e-16\n#> treatment_group1:time1  5.05841    0.08803   57.46   <2e-16\n#>                           \n#> (Intercept)            ***\n#> treatment_group1       ***\n#> time1                  ***\n#> treatment_group1:time1 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.9828 on 1996 degrees of freedom\n#> Multiple R-squared:  0.9268, Adjusted R-squared:  0.9266 \n#> F-statistic:  8418 on 3 and 1996 DF,  p-value: < 2.2e-16"},{"path":"causality.html","id":"assumptions","chapter":"15 Causality from observational data","heading":"15.3.3 Assumptions","text":"want use difference differences, need satisfy assumptions. three touched earlier, focus one: ‘parallel trends’ assumption. parallel trends assumption haunts everything difference differences analysis can never prove , can just convinced , try convince others.see can never prove , consider example want know effect new stadium professional sports team’s wins/loses. consider two teams: Golden State Warriors Toronto Raptors. Warriors changed stadiums start 2019-20 season, Raptors , consider four time periods: 2016-17 season, 2017-18 season, 2018-19 season, finally compare performance one moved, 2019-20 season. Raptors act counterfactual. means assume relationship Warriors Raptors, absence new stadium, continued change consistent way. fundamental problem causal inference means can never know certain. must present sufficient evidence assuage concerns reader may .four main threats validity use difference differences, need address (Cunningham 2021, 272–77):Non-parallel trends. treatment control groups may based differences. can difficult convincingly argue parallel trends. case, maybe try find another factor consider model may adjust . may require difference difference differences (earlier example, perhaps add San Francisco 49ers broad geographic area Warriors). maybe re-think analysis see can make different control group. Adding additional earlier time periods may help may introduce issues, touch third point.Compositional differences. concern working repeated cross-sections. composition cross-sections change? instance, working app rapidly growing, want look effect change. initial cross-section, may mostly young people, subsequent cross-section, may older people demographics app usage change. Hence results may just age-effect, effect change interested .Long-term effects compared reliability. discussed Chapter 10, trade-length analysis run. run analysis longer opportunity factors affect results. also increased chance someone treated treated. , hand, can difficult convincingly argue short-term results continue long-term.Functional form dependence. less issue outcomes similar, different functional form may responsible aspects results.","code":""},{"path":"causality.html","id":"case-study-french-newspaper-prices-between-1960-and-1974","chapter":"15 Causality from observational data","heading":"15.3.4 Case study: French newspaper prices between 1960 and 1974","text":"case study introduce Angelucci Cagé (2019), replicate main findings.business model newspapers challenged internet many local newspapers closed. issue new. television introduced, similar concerns. Angelucci Cagé (2019) use introduction television advertising France, announced 1967, examine effect decreased advertising revenue newspapers. create dataset French newspapers 1960 1974 use difference differences examine effect reduction advertising revenues newspapers’ content prices. change focus introduction television advertising, argue affected national newspapers local newspapers. find change results less journalism-content newspapers lower newspaper prices. Focusing change, analyzing using difference differences, important allows us disentangle competing effects. instance, newspapers become redundant longer charge high prices advertisements, consumers preferred get news television?can get free access data underpins Angelucci Cagé (2019) registration. dataset Stata data format, ‘dta’, can read read_dta() haven (Wickham Miller 2020). file interested ‘Angelucci_Cage_AEJMicro_dataset.dta’, ‘dta’ folder.1,196 observations dataset 52 variables. Angelucci Cagé (2019) interested 1960-1974 time-period around 100 newspapers. 14 national newspapers beginning period 12 end. key period 1967, French government announced allow advertising television. Angelucci Cagé (2019) argue national newspapers affected chance, local newspapers . , national newspapers treatment group local newspapers control group.focus just headline difference differences result construct summary statistics (Table 15.1).\nTable 15.1: Summary statistics French newspapers dataset (1960-1974)\ninterested happened 1967 onward, especially terms advertising revenue, whether different national, compared local newspapers (Figure 15.8). use scales adjust y axis (Wickham Seidel 2020).\nFigure 15.8: Revenue French newspapers (1960-1974), whether local national\nmodel interested estimating :\n\\[\\mbox{ln}(y_{n,t}) = \\beta_0 + \\beta_1(\\mbox{National binary}\\times\\mbox{1967 onward binary}) + \\lambda_n + \\gamma_y + \\epsilon.\\]\n\\(\\beta_1\\) coefficient especially interested . use \\(\\lambda_n\\) fixed effect newspaper, \\(\\gamma_y\\) fixed effect year. estimate models using lm().Looking advertising-side variables (Table 15.2) find consistently negative coefficients everything apart advertising space.\nTable 15.2: Effect changed television advertising laws revenue French newspapers (1960-1974)\nlooking advertising-side variables (Table 15.3) , find consistently negative coefficients everything apart share subscriptions unit price.\nTable 15.3: Effect changed television advertising laws consumers French newspapers (1960-1974)\ngeneral, able replicate main results Angelucci Cagé (2019) find many cases appears difference 1967 onward. results similar Angelucci Cagé (2019).","code":"\nlibrary(haven)\n\nnewspapers <- read_dta(\"Angelucci_Cage_AEJMicro_dataset.dta\")\n\nnewspapers#> # A tibble: 1,196 × 52\n#>     year id_news local national after_national   Had po_cst\n#>    <dbl>   <dbl> <dbl>    <dbl>          <dbl> <dbl>  <dbl>\n#>  1  1960       1     1        0              0     0   2.60\n#>  2  1961       1     1        0              0     0   2.51\n#>  3  1962       1     1        0              0     0   2.39\n#>  4  1963       1     1        0              0     0   2.74\n#>  5  1964       1     1        0              0     0   2.65\n#>  6  1965       1     1        0              0     0   2.59\n#>  7  1966       1     1        0              0     0   2.52\n#>  8  1967       1     1        0              0     0   3.27\n#>  9  1968       1     1        0              0     0   3.91\n#> 10  1969       1     1        0              0     0   3.67\n#> # … with 1,186 more rows, and 45 more variables:\n#> #   ps_cst <dbl>, etotal_cst <dbl>, ra_cst <dbl>,\n#> #   ra_s <dbl>, rs_cst <dbl>, rtotal_cst <dbl>,\n#> #   profit_cst <dbl>, nb_journ <dbl>, qs_s <dbl>,\n#> #   qtotal <dbl>, pages <dbl>, ads_q <dbl>, ads_s <dbl>,\n#> #   news_hole <dbl>, share_Hard <dbl>, ads_p4_cst <dbl>,\n#> #   R_sh_edu_primaire_ipo <dbl>, …\nnewspapers <- \n  newspapers |> \n  select(year, id_news, after_national, local, national, # Diff in diff variables\n         ra_cst, ads_p4_cst, ads_s, # Advertising side dependents\n         ps_cst, po_cst, qtotal, qs_s, rs_cst) |> # Reader side dependents\n  mutate(ra_cst_div_qtotal = ra_cst / qtotal) |> # An advertising side dependent needs to be built\n  mutate(across(c(id_news, after_national, local, national), as.factor)) |> \n  mutate(year = as.integer(year))\n\nnewspapers\n#> # A tibble: 1,196 × 14\n#>     year id_news after_national local national    ra_cst\n#>    <int> <fct>   <fct>          <fct> <fct>        <dbl>\n#>  1  1960 1       0              1     0         52890272\n#>  2  1961 1       0              1     0         56601060\n#>  3  1962 1       0              1     0         64840752\n#>  4  1963 1       0              1     0         70582944\n#>  5  1964 1       0              1     0         74977888\n#>  6  1965 1       0              1     0         74438248\n#>  7  1966 1       0              1     0         81383000\n#>  8  1967 1       0              1     0         80263152\n#>  9  1968 1       0              1     0         87165704\n#> 10  1969 1       0              1     0        102596384\n#> # … with 1,186 more rows, and 8 more variables:\n#> #   ads_p4_cst <dbl>, ads_s <dbl>, ps_cst <dbl>,\n#> #   po_cst <dbl>, qtotal <dbl>, qs_s <dbl>, rs_cst <dbl>,\n#> #   ra_cst_div_qtotal <dbl>\nlibrary(modelsummary)\n\ndatasummary_skim(newspapers,\n                 title = \"Summary statistics for French newspapers dataset (1960-1974)\")\nlibrary(scales)\nnewspapers |> \n  mutate(type = if_else(local == 1, \"Local\", \"National\")) |> \n  ggplot(aes(x = year, y = ra_cst)) +\n  geom_point(alpha = 0.5) +\n  scale_y_continuous(labels = dollar_format(prefix=\"$\", suffix = \"M\", scale = 0.000001)) +\n  labs(x = \"Year\",\n       y = \"Advertising revenue\") +\n  facet_wrap(vars(type),\n               nrow = 2) +\n  theme_minimal() +\n  geom_vline(xintercept = 1966.5, linetype = \"dashed\")\n# Advertising side\nad_revenue <-\n  lm(log(ra_cst) ~ after_national + id_news + year, data = newspapers)\nad_revenue_div_circulation <-\n  lm(log(ra_cst_div_qtotal) ~ after_national + id_news + year, data = newspapers)\nad_price <-\n  lm(log(ads_p4_cst) ~ after_national + id_news + year, data = newspapers)\nad_space <-\n  lm(log(ads_s) ~ after_national + id_news + year, data = newspapers)\n\n# Consumer side\nsubscription_price <-\n  lm(log(ps_cst) ~ after_national + id_news + year, data = newspapers)\nunit_price <-\n  lm(log(po_cst) ~ after_national + id_news + year, data = newspapers)\ncirculation <-\n  lm(log(qtotal) ~ after_national + id_news + year, data = newspapers)\nshare_of_sub <-\n  lm(log(qs_s) ~ after_national + id_news + year, data = newspapers)\nrevenue_from_sales <-\n  lm(log(rs_cst) ~ after_national + id_news + year, data = newspapers)\nselected_variables <- \n  c(\"year\" = \"Year\",\n  \"after_national1\" = \"Is after advertising change\")\n\nadvertising_models <- list(\n  \"Ad revenue\" = ad_revenue,\n  \"Ad revenue over circulation\" = ad_revenue_div_circulation,\n  \"Ad prices\" = ad_price,\n  \"Ad space\" = ad_space\n)\n\nmodelsummary(\n  advertising_models,\n  fmt = 2,\n  coef_map = selected_variables,\n  title = \"Effect of changed television advertising laws on revenue of French newspapers (1960-1974)\"\n)\nconsumer_models <- list(\n  \"Subscription price\" = subscription_price,\n  \"Circulation\" = circulation,\n  \"Share of subscriptions\" = share_of_sub,\n  \"Revenue from sales\" = revenue_from_sales,\n  \"Unit price\" = unit_price\n)\n\nmodelsummary(\n  consumer_models,\n  fmt = 2,\n  coef_map = selected_variables,\n  title = \"Effect of changed television advertising laws on consumers of French newspapers (1960-1974)\"\n)"},{"path":"causality.html","id":"propensity-score-matching","chapter":"15 Causality from observational data","heading":"15.4 Propensity score matching","text":"Difference differences powerful analysis framework. can tough identify appropriate treatment control groups. R. Alexander Ward (2018) compare migrant brothers, one brother education different country, brother education US. Given data available, match provides reasonable treatment control group. matches given different results, instance friends cousins.can match based observable variables. instance, age-group education. two different times compare smoking rates 18-year-olds one city smoking rates 18-year-olds another city. coarse match know many differences 18-year-olds, even terms variables commonly observe, say gender education. One way deal create sub-groups: 18-year-old males high school education, etc. sample sizes quickly become small. also issue deal continuous variables. , 18-year-old 19-year-old really different? also compare ?One way proceed consider nearest neighbor approach. can limited concern uncertainty approach. can also issue many variables end high-dimension graph. leads propensity score matching. explain process propensity score matching, something widely used (G. King Nielsen 2019), go case.Propensity score matching involves assigning probability observation. construct probability based observation’s values independent variables, values treatment. probability best guess probability observation treated, regardless whether treated . instance, 18-year-old males treated 19-year-old males , much difference 18-year-old males 19-year-old males assigned probability similar. can compare outcomes observations similar propensity scores.One advantage propensity score matching allows us easily consider many independent variables , can constructed using logistic regression.specific can simulate data. pretend work large online retailer. going treat individuals free shipping see happens average purchase.need add probability treated free shipping. say depends variables younger, higher-income, male Toronto-based individuals make treatment slightly likely.Finally, need measure person’s average spend. want free shipping slightly higher without.use matchit() MatchIt (Ho et al. 2011) implement logistic regression create matched groups. use match.data() get data matches containing 371 people actually treated free shipping untreated person considered similar , based propensity score, possible. result dataset 742 observations.Finally, can estimate effect treated average spend using linear regression. particularly interested coefficient associated treatment variable, case free shipping.cover propensity score matching widely used. many issues propensity score matching mean propensity scores used matching (G. King Nielsen 2019). include:Matching. Propensity score matching match unobserved variables. may fine classroom setting, realistic settings likely cause issues.Modelling. results tend specific model used.Statistically. using data twice.","code":"\nset.seed(853)\n\nsample_size <- 10000\n\npurchase_data <-\n  tibble(\n    unique_person_id = c(1:sample_size),\n    age = runif(n = sample_size,\n                min = 18,\n                max = 100),\n    city = sample(\n      x = c(\"Toronto\", \"Montreal\", \"Calgary\"),\n      size = sample_size,\n      replace = TRUE\n    ),\n    gender = sample(\n      x = c(\"Female\", \"Male\", \"Other/decline\"),\n      size = sample_size,\n      replace = TRUE,\n      prob = c(0.49, 0.47, 0.02)\n    ),\n    income = rlnorm(n = sample_size,\n                    meanlog = 0.5,\n                    sdlog = 1)\n  )\npurchase_data <-\n  purchase_data |>\n  mutate(\n    age_num = case_when(age < 30 ~ 3,\n                        age < 50 ~ 2,\n                        age < 70 ~ 1,\n                        TRUE ~ 0),\n    city_num = case_when(\n      city == \"Toronto\" ~ 3,\n      city == \"Montreal\" ~ 2,\n      city == \"Calgary\" ~ 1,\n      TRUE ~ 0\n    ),\n    gender_num = case_when(\n      gender == \"Male\" ~ 3,\n      gender == \"Female\" ~ 2,\n      gender == \"Other/decline\" ~ 1,\n      TRUE ~ 0\n    ),\n    income_num = case_when(income > 3 ~ 3,\n                           income > 2 ~ 2,\n                           income > 1 ~ 1,\n                           TRUE ~ 0)\n  ) |>\n  rowwise() |>\n  mutate(\n    sum_num = sum(age_num, city_num, gender_num, income_num),\n    softmax_prob = exp(sum_num) / exp(12),\n    free_shipping = sample(\n      x = c(0:1),\n      size = 1,\n      replace = TRUE,\n      prob = c(1 - softmax_prob, softmax_prob)\n    )\n  ) |>\n  ungroup()\n\npurchase_data <-\n  purchase_data |>\n  select(-age_num,-city_num,-gender_num,-income_num,-sum_num,-softmax_prob)\npurchase_data <-\n  purchase_data |>\n  mutate(mean_spend = if_else(free_shipping == 1, 60, 50)) |>\n  rowwise() |>\n  mutate(average_spend = rnorm(1, mean_spend, sd = 5)) |>\n  ungroup() |>\n  select(-mean_spend) |>\n  mutate(across(c(city, gender, free_shipping), as.factor))\n\npurchase_data\n#> # A tibble: 10,000 × 7\n#>    unique_person_id   age city   gender income free_shipping\n#>               <int> <dbl> <fct>  <fct>   <dbl> <fct>        \n#>  1                1  47.5 Calga… Female  1.72  0            \n#>  2                2  27.8 Montr… Male    1.54  0            \n#>  3                3  57.7 Toron… Female  3.16  0            \n#>  4                4  43.9 Toron… Male    0.636 0            \n#>  5                5  21.1 Toron… Female  1.43  0            \n#>  6                6  51.1 Calga… Male    1.18  0            \n#>  7                7  28.7 Toron… Female  1.49  0            \n#>  8                8  37.9 Toron… Female  0.414 0            \n#>  9                9  31.0 Calga… Male    0.384 0            \n#> 10               10  33.5 Montr… Female  1.11  0            \n#> # … with 9,990 more rows, and 1 more variable:\n#> #   average_spend <dbl>\nlibrary(MatchIt)\n\nmatched_groups <- matchit(free_shipping ~ age + city + gender + income, \n                  data = purchase_data,\n                  method = \"nearest\", distance = \"glm\")\n\nmatched_groups\n#> A matchit object\n#>  - method: 1:1 nearest neighbor matching without replacement\n#>  - distance: Propensity score\n#>              - estimated with logistic regression\n#>  - number of obs.: 10000 (original), 742 (matched)\n#>  - target estimand: ATT\n#>  - covariates: age, city, gender, income\n\nmatched_dataset <- match.data(matched_groups)\n\nmatched_dataset\n#> # A tibble: 742 × 10\n#>    unique_person_id   age city   gender income free_shipping\n#>               <int> <dbl> <fct>  <fct>   <dbl> <fct>        \n#>  1                5  21.1 Toron… Female  1.43  0            \n#>  2               20  30.0 Montr… Male    8.65  0            \n#>  3               22  22.8 Toron… Male    0.898 0            \n#>  4               38  41.3 Toron… Female  6.01  1            \n#>  5               43  24.7 Toron… Male    1.59  1            \n#>  6               76  56.4 Toron… Male   15.0   0            \n#>  7              102  48.1 Toron… Male    3.48  1            \n#>  8              105  76.7 Toron… Male    2.84  0            \n#>  9              118  26.7 Toron… Female  0.315 0            \n#> 10              143  36.3 Toron… Male   10.6   0            \n#> # … with 732 more rows, and 4 more variables:\n#> #   average_spend <dbl>, distance <dbl>, weights <dbl>,\n#> #   subclass <fct>\npropensity_score_regression <- lm(average_spend ~ age + city + gender + income + free_shipping, \n                                  data = matched_dataset)\n\npropensity_score_regression\n#> \n#> Call:\n#> lm(formula = average_spend ~ age + city + gender + income + free_shipping, \n#>     data = matched_dataset)\n#> \n#> Coefficients:\n#>         (Intercept)                  age  \n#>            49.56747              0.00735  \n#>        cityMontreal          cityToronto  \n#>             0.12787              0.58628  \n#>          genderMale  genderOther/decline  \n#>            -1.09978             -1.99861  \n#>              income       free_shipping1  \n#>             0.01903             10.60550"},{"path":"causality.html","id":"regression-discontinuity-design","chapter":"15 Causality from observational data","heading":"15.5 Regression discontinuity design","text":"","code":""},{"path":"causality.html","id":"overview-1","chapter":"15 Causality from observational data","heading":"15.5.1 Overview","text":"Regression discontinuity design (RDD) established Thistlethwaite Campbell (1960) popular way get causality continuous variable cut-offs determine treatment. difference student gets 79 per cent student gets 80 per cent? Probably much, one may get -, may get B+, seeing transcript affect gets job affect income. case percentage ‘forcing variable’ cut-- ‘threshold’. treatment determined forcing variable need control variable. , seemingly arbitrary cut-offs can seen time. Hence, great deal work using RDD.sometimes slightly different terminology used comes RDD. instance, Cunningham (2021) refers forcing function running variable. exact terminology used matter provided use consistently.","code":""},{"path":"causality.html","id":"simulated-example-1","chapter":"15 Causality from observational data","heading":"15.5.2 Simulated example","text":"specific situation, simulate data. consider relationship income grades, simulate change student gets least 80 (Figure 15.9).\nFigure 15.9: Illustration simulated data shows effect income getting mark 80, compared 79\ncan use binary variable linear regression estimate effect. expect coefficient around two, simulated.various caveats estimate discuss, essentials RDD . Given appropriate set-, model, RDD can compare favorably randomized trials (Bloom, Bell, Reiman 2020).also implement RDD using rdrobust (Calonico et al. 2021). advantage approach many extensions easily available.","code":"\nlibrary(tidyverse)\n\nset.seed(853)\n\nnumber_of_observation <- 1000\n\nrdd_example_data <- tibble(\n  person = c(1:number_of_observation),\n  mark = runif(number_of_observation, min = 78, max = 82),\n  income = rnorm(number_of_observation, 10, 1)\n)\n\n## Make income more likely to be higher if they have a mark at least 80\nrdd_example_data <-\n  rdd_example_data |>\n  mutate(\n    noise = rnorm(n = number_of_observation, mean = 2, sd = 1),\n    income = if_else(mark >= 80, income + noise, income)\n  )\n\nrdd_example_data\n#> # A tibble: 1,000 × 4\n#>    person  mark income noise\n#>     <int> <dbl>  <dbl> <dbl>\n#>  1      1  79.4   9.43 1.87 \n#>  2      2  78.5   9.69 2.26 \n#>  3      3  79.9  10.8  1.14 \n#>  4      4  79.3   9.34 2.50 \n#>  5      5  78.1  10.7  2.21 \n#>  6      6  79.6   9.83 2.47 \n#>  7      7  78.5   8.96 4.22 \n#>  8      8  79.0  10.5  3.11 \n#>  9      9  78.6   9.53 0.671\n#> 10     10  78.8  10.6  2.46 \n#> # … with 990 more rows\nrdd_example_data |> \n  ggplot(aes(x = mark,\n             y = income)) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(data = rdd_example_data |> filter(mark < 80), \n              method='lm',\n              color = \"black\",\n              formula = 'y ~ x') +\n  geom_smooth(data = rdd_example_data |> filter(mark >= 80), \n              method='lm',\n              color = \"black\",\n              formula = 'y ~ x') +\n  theme_minimal() +\n  labs(x = \"Mark\",\n       y = \"Income ($)\")\nrdd_example_data <- \n  rdd_example_data |> \n  mutate(mark_80_and_over = if_else(mark < 80, 0, 1)) \n\nlm(income ~ mark + mark_80_and_over, data = rdd_example_data) |> \n  summary()\n#> \n#> Call:\n#> lm(formula = income ~ mark + mark_80_and_over, data = rdd_example_data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -4.3418 -0.8218 -0.0043  0.7740  6.1209 \n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)       5.30130    5.16331   1.027    0.305    \n#> mark              0.06025    0.06535   0.922    0.357    \n#> mark_80_and_over  1.89221    0.14921  12.682   <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.189 on 997 degrees of freedom\n#> Multiple R-squared:  0.4178, Adjusted R-squared:  0.4166 \n#> F-statistic: 357.7 on 2 and 997 DF,  p-value: < 2.2e-16\nlibrary(rdrobust)\nrdrobust(y = rdd_example_data$income, \n         x = rdd_example_data$mark, \n         c = 80, h = 2, all = TRUE) |> \n  summary()\n#> Call: rdrobust\n#> \n#> Number of Obs.                 1000\n#> BW type                      Manual\n#> Kernel                   Triangular\n#> VCE method                       NN\n#> \n#> Number of Obs.                  497          503\n#> Eff. Number of Obs.             497          503\n#> Order est. (p)                    1            1\n#> Order bias  (q)                   2            2\n#> BW est. (h)                   2.000        2.000\n#> BW bias (b)                   2.000        2.000\n#> rho (h/b)                     1.000        1.000\n#> Unique Obs.                     497          503\n#> \n#> =============================================================================\n#>         Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]       \n#> =============================================================================\n#>   Conventional     1.913     0.161    11.876     0.000     [1.597 , 2.229]     \n#> Bias-Corrected     1.966     0.161    12.207     0.000     [1.650 , 2.282]     \n#>         Robust     1.966     0.232     8.461     0.000     [1.511 , 2.422]     \n#> ============================================================================="},{"path":"causality.html","id":"assumptions-1","chapter":"15 Causality from observational data","heading":"15.5.3 Assumptions","text":"key assumptions RDD (Cunningham 2021, 163):cut-specific, fixed, known .forcing function continuous.first assumption largely unable manipulate cut-, ensures cut-meaning. second assumption enables us confident folks either side threshold similar, apart just happening just fall either side threshold.discussed randomized control trials /B testing Chapter 10 randomized assignment treatment meant control treatment groups , treatment. moved difference differences, assumed common trend treated control groups. allowed groups different, ‘difference ’ differences. Finally, considered matching, said even control treatment groups seemed quite different, able match, extent, treated group similar ways, apart fact treated.regression discontinuity consider slightly different setting. two groups completely different terms forcing variable. either side threshold. overlap . know threshold believe either side essentially matched. Let us consider 2019 NBA Eastern Conference Semifinals - Toronto Philadelphia. Game 1: Raptors win 108-95; Game 2: 76ers win 94-89; Game 3: 76ers win 116-95; Game 4: Raptors win 101-96; Game 5: Raptors win 125-89; Game 6: 76ers win 112-101; finally, Game 7: Raptors win 92-90, ball win bouncing rim four times. really much difference teams?continuity assumption important, test based counterfactual. Instead, need convince people . Ways include:Using test/train set-.Trying different specifications. especially concerned results broadly persist just linear quadratic functions.Considering different subsets data.Considering different windows.clear uncertainty intervals, especially graphs.Discussing assuage concerns possibility omitted variables.threshold also important. instance, actual shift non-linear relationship?variety weaknesses RDD including:External validity may difficult. instance, think -/B+ example, hard see generalizing also B-/C+ students.important responses close cut-. means even many B students, help much. Hence, need lot data may concerns ability support claims (D. P. Green et al. 2009).researcher, lot freedom implement different options. means open science best practice becomes vital.point considered ‘sharp’ RDD. , threshold strict. , reality, often boundary little less strict. instance, consider drinking age. legal drinking age, say 18. looked number people drunk, likely increase years leading age.sharp RDD setting, know value forcing function know outcome. instance, student gets mark 80 know got -, got mark 79 know got B+. fuzzy RDD known probability. can say Canadian 19-year-old likely drunk alcohol Canadian 18-year-old, number Canadian 18-year-olds drunk alcohol zero.may possible deal fuzzy RDD settings appropriate choice model data. may also possible deal using instrumental variables.want ‘sharp’ effect possible, thresholds known, gamed. instance, lot evidence people run certain marathon times, know people aim certain grades. Similarly, side, lot easier instructor just give justify Bs. One way look consider ‘balanced’ sample either side threshold. can using histograms appropriate bins. instance, think age-heaping found cleaned Kenyan census data Chapter 9Another key factor RDD possible effect decision around choice model. see , consider difference linear polynomial.result estimate dependent choice model. see issue occur often RDD (Gelman 2019).","code":"\nsome_data <- \n  tibble(outcome = rnorm(n = 100, mean = 1, sd = 1),\n         running_variable = c(1:100),\n         location = \"before\")\n\nsome_more_data <- \n  tibble(outcome = rnorm(n = 100, mean = 2, sd = 1),\n         running_variable = c(101:200),\n         location = \"after\")\n\nboth <- \n  rbind(some_data, some_more_data)\n\nboth |> \n  ggplot(aes(x = running_variable, y = outcome, color = location)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(formula = y~x, method = 'lm')\n  \nboth |> \n  ggplot(aes(x = running_variable, y = outcome, color = location)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(formula = y ~ poly(x, 3), method = 'lm')"},{"path":"causality.html","id":"instrumental-variables","chapter":"15 Causality from observational data","heading":"15.6 Instrumental variables","text":"","code":""},{"path":"causality.html","id":"overview-2","chapter":"15 Causality from observational data","heading":"15.6.1 Overview","text":"Instrumental variables (IV) approach can handy type treatment control going , lot correlation variables possibly variable actually measures interested . adjusting observables enough create good estimate. Instead find variable—eponymous instrumental variable—:correlated treatment variable, butnot correlated outcome.solves problem way instrumental variable can effect treatment variable, can adjust understanding effect treatment variable appropriately. trade-instrumental variables must satisfy bunch different assumptions, , frankly, difficult identify ex ante. Nonetheless, able use , powerful tool speaking causality.canonical instrumental variables example smoking. days know smoking causes cancer. smoking correlated lot variables, instance, education, actually education causes cancer. RCTs may possible, likely troublesome terms speed ethics, instead look variable correlated smoking, , , lung cancer. case, look tax rates, policy responses, cigarettes. tax rates cigarettes correlated number cigarettes smoked, correlated lung cancer, impact cigarette smoking, can assess effect cigarettes smoked lung cancer.implement instrumental variables first regress tax rates cigarette smoking get coefficient instrumental variable, (separate regression) regress tax rates lung cancer , get coefficient instrumental variable. estimate ratio coefficients, described ‘Wald estimate’ (Gelman Hill 2007, 219).Following language (Gelman Hill 2007, 216) use instrumental variables make variety assumptions including:Ignorability instrument.Correlation instrumental variable treatment variable.Monotonicity.Exclusion restriction.history instrumental variables intriguing, Stock Trebbi (2003) provide brief overview. method first published Wright (1928). book effect tariffs animal vegetable oil. might instrumental variables important book tariffs animal vegetable oil? fundamental problem effect tariffs depends supply demand. know prices quantities, know driving effect. can use instrumental variables pin causality. intriguing aspect instrumental variables discussion Appendix B. seem odd relegate major statistical break-appendix. , Philip G. Wright, book’s author, son Sewall Wright, considerable expertise statistics specific method used Appendix B. Hence mystery Appendix B: Philip Sewall write ? Cunningham (2021) Stock Trebbi (2003) go detail, balance feel likely Philip actually author work.","code":""},{"path":"causality.html","id":"simulated-example-2","chapter":"15 Causality from observational data","heading":"15.6.2 Simulated example","text":"Let us generate data. explore simulation related canonical example health status, smoking, tax rates. looking explain healthy someone based amount smoke, via tax rate smoking. going generate different tax rates provinces. understanding tax rate cigarettes now pretty much provinces, fairly recent. pretend Alberta low tax, Nova Scotia high tax.reminder, simulating data illustrative purposes, need impose answer want. actually use instrumental variables reversing process.Now need relate number cigarettes someone smoked health. model health status draw normal distribution, either high low mean depending whether person smokes.Now need relationship cigarettes province (illustration, provinces different tax rates).Now can look data.Finally, can use tax rate instrumental variable estimate effect smoking health.find, luckily, smoke health likely worse smoke.Equivalently, can think instrumental variables two-stage regression context.can use iv_robust() estimatr (G. Blair et al. 2021) estimate IV. One nice reason can help keep everything organised adjust standard errors.","code":"\nlibrary(broom)\nlibrary(tidyverse)\n\nset.seed(853)\n\nnumber_of_observation <- 10000\n\niv_example_data <- tibble(person = c(1:number_of_observation),\n                          smoker = sample(x = c(0:1),\n                                          size = number_of_observation, \n                                          replace = TRUE)\n                          )\niv_example_data <- \n  iv_example_data |> \n  mutate(health = if_else(smoker == 0,\n                          rnorm(n = n(), mean = 1, sd = 1),\n                          rnorm(n = n(), mean = 0, sd = 1)\n                          )\n         )\n## So health will be one standard deviation higher for people who do not or barely smoke.\niv_example_data <- \n  iv_example_data |> \n  rowwise() |> \n  mutate(province = \n           case_when(smoker == 0 ~ sample(x = c(\"Nova Scotia\", \"Alberta\"),\n                                          size = 1, \n                                          replace = FALSE, \n                                          prob = c(1/2, 1/2)),\n                     smoker == 1 ~ sample(x = c(\"Nova Scotia\", \"Alberta\"),\n                                          size = 1, \n                                          replace = FALSE, \n                                          prob = c(1/4, 3/4)))) |> \n  ungroup()\n\niv_example_data <- \n  iv_example_data |> \n  mutate(tax = case_when(province == \"Alberta\" ~ 0.3,\n                         province == \"Nova Scotia\" ~ 0.5,\n                         TRUE ~ 9999999\n  )\n  )\n\niv_example_data$tax |> table()\n#> \n#>  0.3  0.5 \n#> 6206 3794\n\nhead(iv_example_data)\n#> # A tibble: 6 × 5\n#>   person smoker  health province      tax\n#>    <int>  <int>   <dbl> <chr>       <dbl>\n#> 1      1      0  1.11   Alberta       0.3\n#> 2      2      1 -0.0831 Alberta       0.3\n#> 3      3      1 -0.0363 Alberta       0.3\n#> 4      4      0  2.48   Alberta       0.3\n#> 5      5      0  0.617  Alberta       0.3\n#> 6      6      0  0.748  Nova Scotia   0.5\niv_example_data |> \n  mutate(smoker = as_factor(smoker)) |> \n  ggplot(aes(x = health, fill = smoker)) +\n  geom_histogram(position = \"dodge\", binwidth = 0.2) +\n  theme_minimal() +\n  labs(x = \"Health rating\",\n       y = \"Number of people\",\n       fill = \"Smoker\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  facet_wrap(vars(province))\nhealth_on_tax <- lm(health ~ tax, data = iv_example_data)\nsmoker_on_tax <- lm(smoker ~ tax, data = iv_example_data)\n\ncoef(health_on_tax)[\"tax\"] / coef(smoker_on_tax)[\"tax\"]\n#>        tax \n#> -0.8554502\nfirst_stage <- lm(smoker ~ tax, data = iv_example_data)\nhealth_hat <- first_stage$fitted.values\nsecond_stage <- lm(health ~ health_hat, data = iv_example_data)\n\nsummary(second_stage)\n#> \n#> Call:\n#> lm(formula = health ~ health_hat, data = iv_example_data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.9867 -0.7600  0.0068  0.7709  4.3293 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  0.91632    0.04479   20.46   <2e-16 ***\n#> health_hat  -0.85545    0.08911   -9.60   <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.112 on 9998 degrees of freedom\n#> Multiple R-squared:  0.009134,   Adjusted R-squared:  0.009034 \n#> F-statistic: 92.16 on 1 and 9998 DF,  p-value: < 2.2e-16\nlibrary(estimatr)\niv_robust(health ~ smoker | tax, data = iv_example_data) |> \n  summary()\n#> \n#> Call:\n#> iv_robust(formula = health ~ smoker | tax, data = iv_example_data)\n#> \n#> Standard error type:  HC2 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value   Pr(>|t|) CI Lower\n#> (Intercept)   0.9163    0.04057   22.59 3.163e-110   0.8368\n#> smoker       -0.8555    0.08047  -10.63  2.981e-26  -1.0132\n#>             CI Upper   DF\n#> (Intercept)   0.9958 9998\n#> smoker       -0.6977 9998\n#> \n#> Multiple R-squared:  0.1971 ,    Adjusted R-squared:  0.197 \n#> F-statistic:   113 on 1 and 9998 DF,  p-value: < 2.2e-16"},{"path":"causality.html","id":"assumptions-2","chapter":"15 Causality from observational data","heading":"15.6.3 Assumptions","text":"discussed earlier, variety assumptions made using instrumental variables. two important :Exclusion Restriction. assumption instrumental variable affects dependent variable independent variable interest.Relevance. must actually relationship instrumental variable independent variable.typically trade-two. plenty variables satisfy one, precisely satisfy . Cunningham (2021, 211) describes one test good instrument people initially confused explain , think obvious hindsight.Relevance can tested using regression tests correlation. exclusion restriction tested. need present evidence convincing arguments. difficult aspect instrument needs seem irrelevant implication exclusion restriction (Cunningham 2021, 225).Instrumental variables useful approach one can obtain causal estimates even without explicit randomization. Finding instrumental variables used bit white whale, especially academia. increased use IV approaches downstream /B tests (Taddy 2019, 162).","code":""},{"path":"causality.html","id":"exercises-and-tutorial-14","chapter":"15 Causality from observational data","heading":"15.7 Exercises and tutorial","text":"","code":""},{"path":"causality.html","id":"exercises-14","chapter":"15 Causality from observational data","heading":"15.7.1 Exercises","text":"three months Sharla Gelfand shared two functions day: one new another already knew love. Please go ‘Two Functions Days’ GitHub repo, find package mention never used. Find relevant website package, paragraph two, describe package context useful .Please , go Sharla’s ‘Two Functions Days’ GitHub repo, find function mention never used. Please look help file function, detail arguments function, context useful .propensity score matching? matching people, features like match ? sort ethical questions collecting storing information raise ?Putting ethical issues one side, following G. King Nielsen (2019), least two paragraphs, please describe statistical concerns propensity score matching.key assumption using difference differences?Please read fascinating article Markup car insurance algorithms: https://themarkup.org/allstates-algorithm/2020/02/25/car-insurance-suckers-list. Please read article tell think. may wish focus ethical, legal, social, statistical, , aspects.Please go GitHub page related fascinating article Markup car insurance algorithms: https://github.com/-markup/investigation-allstates-algorithm. great work? improved?fundamental features regression discontinuity design?conditions needed order regression discontinuity design able used?Can think situation life regression discontinuity design may useful?threats validity regression discontinuity design estimates?Please read reproduce main findings Eggers, Fowler, Hainmueller, Hall, Snyder, 2015.instrumental variable?circumstances instrumental variables might useful?conditions must instrumental variables satisfy?early instrumental variable authors?Can please think explain application instrumental variables life?key assumption difference differences\nParallel trends.\nHeteroscedasticity.\nParallel trends.Heteroscedasticity.using regression discontinuity, whare aspects aware think hard (select apply)?\ncut-free manipulation?\nforcing function continuous?\nextent functional form driving estimate?\ndifferent fitted lines affect results?\ncut-free manipulation?forcing function continuous?extent functional form driving estimate?different fitted lines affect results?main reason Oostrom (2021) finds outcome RCT can depend funding (pick one)?\nPublication bias\nExplicit manipulation\nSpecialisation\nLarger number arms\nPublication biasExplicit manipulationSpecialisationLarger number armsWhat key coefficient interest Angelucci Cagé, 2019 (pick one)?\n\\(\\beta_0\\)\n\\(\\beta_1\\)\n\\(\\lambda\\)\n\\(\\gamma\\)\n\\(\\beta_0\\)\\(\\beta_1\\)\\(\\lambda\\)\\(\\gamma\\)instrumental variable (please pick apply):\nCorrelated treatment variable.\ncorrelated outcome.\nHeteroskedastic.\nCorrelated treatment variable.correlated outcome.Heteroskedastic.two candidates invented instrumental variables?\nSewall Wright\nPhilip G. Wright\nSewall Cunningham\nPhilip G. Cunningham\nSewall WrightPhilip G. WrightSewall CunninghamPhilip G. CunninghamWhat two main assumptions instrumental variables?\nExclusion Restriction.\nRelevance.\nIgnorability.\nRandomization.\nExclusion Restriction.Relevance.Ignorability.Randomization.According Meng (2021) ‘Data science can persuade via…’ (pick apply):\ncareful establishment evidence fair-minded high-quality data collection\nprocessing analysis\nhonest interpretation communication findings\nlarge sample sizes\ncareful establishment evidence fair-minded high-quality data collectionprocessing analysisthe honest interpretation communication findingslarge sample sizesAccording Reiderer, 2021, ‘disjoint treated untreated groups partitioned sharp cut-’ method use measure local treatment effect juncture groups (pick one)?\nregression discontinuity\nmatching\ndifference differences\nevent study methods\nregression discontinuitymatchingdifference differencesevent study methodsAccording Reiderer, 2021, ‘Causal inference requires investment ’ (pick apply):\ndata management\ndomain knowledge\nprobabilistic reasoning\ndata science\ndata managementdomain knowledgeprobabilistic reasoningdata scienceI Australian 30-39 year old male living Toronto one child PhD. following think match closely (please explain paragraph two)?\nAustralian 30-39 year old male living Toronto one child bachelors degree\nCanadian 30-39 year old male living Toronto one child PhD\nAustralian 30-39 year old male living Ottawa one child PhD\nCanadian 18-29 year old male living Toronto one child PhD\nAustralian 30-39 year old male living Toronto one child bachelors degreeA Canadian 30-39 year old male living Toronto one child PhDAn Australian 30-39 year old male living Ottawa one child PhDA Canadian 18-29 year old male living Toronto one child PhDIn disdainful tone (jokes, love DAGs), DAG (words please)?confounder (please select one answer)?\nvariable, z, causes x y, x also causes y.\nvariable, z, caused x y, x also causes y.\nvariable, z, causes y caused x, x also causes y.\nvariable, z, causes x y, x also causes y.variable, z, caused x y, x also causes y.variable, z, causes y caused x, x also causes y.mediator (please select one answer)?\nvariable, z, causes y caused x, x also causes y.\nvariable, z, causes x y, x also causes y.\nvariable, z, caused x y, x also causes y.\nvariable, z, causes y caused x, x also causes y.variable, z, causes x y, x also causes y.variable, z, caused x y, x also causes y.collider (please select one answer)?\nvariable, z, causes x y, x also causes y.\nvariable, z, causes y caused x, x also causes y.\nvariable, z, caused x y, x also causes y.\nvariable, z, causes x y, x also causes y.variable, z, causes y caused x, x also causes y.variable, z, caused x y, x also causes y.Please talk brief example may want careful checking Simpson’s paradox.Please talk brief example may want careful checking Berkson’s paradox.Kahneman, Sibony, Sunstein (2021) authors, including Nobel Prize winner Daniel Kahneman, say ‘… correlation imply causation, causation imply correlation. causal link, find correlation’. reference Cunningham (2021, chap. 1), right wrong, ?","code":""},{"path":"causality.html","id":"tutorial-14","chapter":"15 Causality from observational data","heading":"15.7.2 Tutorial","text":"","code":""},{"path":"causality.html","id":"paper-5","chapter":"15 Causality from observational data","heading":"15.7.3 Paper","text":"point, Paper Five (Appendix B.5) appropriate.","code":""},{"path":"mrp.html","id":"mrp","chapter":"16 Multilevel regression with post-stratification","heading":"16 Multilevel regression with post-stratification","text":"Required materialRead Analyzing name changes marriage using non-representative survey, (M. Alexander 2019c)Read Forecasting elections non-representative polls, (W. Wang et al. 2015).Watch Statistical Models Election Outcomes, (Gelman 2020).Read Mister P helps us understand vaccine hesitancy, (E. Green 2020).Key librarieshaven (Wickham Miller 2020)labelled (Larmarange 2021)modelsummary (Arel-Bundock 2021a)rstanarm (Goodrich et al. 2020)tidybayes (Kay 2020)tidyverse (Wickham 2017)","code":""},{"path":"mrp.html","id":"introduction-13","chapter":"16 Multilevel regression with post-stratification","heading":"16.1 Introduction","text":"[Presidential election ] 2016 largest analytics failure US political history.David Shor, 13 August 2020Multilevel regression post-stratification (MRP) popular way adjust non-representative samples better analyse opinion survey responses. uses regression model relate individual-level survey responses various characteristics rebuilds sample better match population. way MRP can allow better understanding responses, also allow us analyse data may otherwise unusable. However, can challenge get started MRP terminology may unfamiliar, data requirements can onerous.Let us say biased survey. Maybe conducted survey computers academic seminar, folks post-graduate degrees likely -represented. nonetheless interested making claims population. Let us say found 37.5 per cent respondents prefer Macs. One way forward just ignore bias say ‘37.5 per cent people prefer Macs’. Another way say, well 50 per cent respondents post-graduate degree prefer Macs, without post-graduate degree, 25 per cent prefer Macs. knew proportion broader population post-graduate degree, Let us assume 10 per cent, conduct re-weighting, post-stratification, follows: \\(0.5 \\times 0.1 + 0.25 \\times 0.9 = 0.275\\), estimate 27.5 per cent people prefer Macs. MRP third approach, uses model help re-weighting. use logistic regression estimate relationship preferring Macs highest educational attainment survey. apply relationship population dataset.MRP handy approach dealing survey data. Hanretty (2020) describes use MRP alternatives either poor expensive. Essentially, trains model based survey, applies trained model another dataset. two main, related, advantages:can allow us ‘re-weight’ way includes uncertainty front--mind isn’t hamstrung small samples. alternative way deal small sample either go gather data throw away.can allow us use broad surveys speak subsets. Hanretty (2020) says ‘poor alternative [using MRP] simply splitting large sample (much) smaller geographic subsamples. poor alternative guarantee sample representative national level representative broken smaller groups.’.practical perspective, tends less expensive collect non-probability samples benefits able use types data. said, magic-bullet laws statistics still apply. larger uncertainty around estimates still subject usual biases. Dr Lauren Kennedy, Lecturer, Monash University, describes used MRP time probability surveys, shows great potential comes non-probability surveys, limitations MRP known moment. exciting area research academia industry.workflow need MRP straight-forward, details tiny decisions made step can become overwhelming. point keep mind trying create relationship two datasets using statistical model, need establish similarity two datasets terms variables levels. steps :gather prepare survey dataset, thinking needed coherence post-stratification dataset;gather prepare post-stratification dataset thinking needed coherence survey dataset;model variable interest survey using independent variables levels available survey post-stratification datasets;apply model post-stratification data.general, MRP good way accomplish specific aims, without trade-offs. good quality survey, may way speak disaggregated aspects . concerned uncertainty good way think . biased survey, great place start, panacea. plenty scope exciting work variety approaches:statistical perspective, lot work terms thinking survey design modelling approaches interact extent underestimating uncertainty. also also interested thinking implications small samples uncertainty post-stratification dataset. awful lot terms thinking appropriate model use, even evaluate ‘appropriate’ means ? Si (2020) appropriate starting point.lot done sociology perspective terms survey responses can better design surveys, knowing going used MRP putting respect respondents first.political science perspective, just little idea conditions stable preferences relationships required MRP accurate, understanding relates uncertainty survey design. political science interests, natural next step go Lauderdale et al. (2020) Ghitza Gelman (2020).Economists might interested think use MRP better understand inflation unemployment rates local levels.statistical software side things, really need develop better packages around .finally, information perspective exciting think store protect datasets, yet retain ability correspond purposes MRP. put levels together way meaningful? extent people appreciate uncertainty estimates can better communicate estimates?generally, pretty much use MRP anywhere samples. Determining conditions actually , work whole generations.chapter, begin simulating situation pretend know features population. move famous example MRP used survey data Xbox platform exit poll data forecast 2012 US election. move examples Australian political situation. discuss features aware conducting MRP.","code":""},{"path":"mrp.html","id":"simulation---toddler-bedtimes","chapter":"16 Multilevel regression with post-stratification","heading":"16.2 Simulation - Toddler bedtimes","text":"","code":""},{"path":"mrp.html","id":"construct-a-population","chapter":"16 Multilevel regression with post-stratification","heading":"16.2.1 Construct a population","text":"get started simulate data population various properties, take biased sample, conduct MRP demonstrate can get properties back. going two ‘explanatory variables’ - age-group toilet-trained - one dependent variable - bedtime. Bed-time increase age-group increases, later children toilet-trained, compared . clear, example ‘know’ ‘true’ features population, isn’t something occurs use real data - just help explain happening MRP. going rely heavily tidyverse (Wickham et al. 2019a)., always, dataset, first try graph better understand going . million points, can just grab first 1,000 plots nicely.can also work ‘truth’ information interested (remembering ’d never actually know move away simulated examples).","code":"\nlibrary(tidyverse)\n\nset.seed(853)\n\nsize_of_population <- 1000000\n\npopulation_for_mrp_example <- \n  tibble(age_group = sample(x = c(1:3),\n                            size = size_of_population,\n                            replace = TRUE\n                            ),\n         toilet_trained = sample(x = c(0, 1),\n                                 size = size_of_population,\n                                 replace = TRUE\n                                 ),\n         noise = rnorm(size_of_population, mean = 0, sd = 1), \n         # No special reason for this intercept to be five; it could be anything.\n         bed_time = 5 + 0.5 * age_group + 1 * toilet_trained + noise, \n         ) |> \n  select(-noise) |> \n  mutate(age_group = as_factor(age_group),\n         toilet_trained = as_factor(toilet_trained)\n         )\n\npopulation_for_mrp_example |> \n  head()\n#> # A tibble: 6 × 3\n#>   age_group toilet_trained bed_time\n#>   <fct>     <fct>             <dbl>\n#> 1 1         0                  5.74\n#> 2 2         1                  6.48\n#> 3 1         0                  6.53\n#> 4 1         1                  5.39\n#> 5 1         1                  8.40\n#> 6 3         0                  6.54\npopulation_for_mrp_example |> \n  slice(1:1000) |> \n  ggplot(aes(x = age_group, y = bed_time)) +\n  geom_jitter(aes(color = toilet_trained), \n              alpha = 0.4, \n              width = 0.1, \n              height = 0) +\n  labs(x = \"Age-group\",\n       y = \"Bed time\",\n       color = \"Toilet trained\") +\n  theme_classic() +\n  scale_color_brewer(palette = \"Set1\")\npopulation_for_mrp_example_summarised <- \n  population_for_mrp_example |> \n  group_by(age_group, toilet_trained) |> \n  summarise(median_bed_time = median(bed_time)) \n#> `summarise()` has grouped output by 'age_group'. You can\n#> override using the `.groups` argument.\n\npopulation_for_mrp_example_summarised |> \n  knitr::kable(digits = 2,\n               col.names = c(\"Age-group\", \"Is toilet trained\", \"Average bed time\"))"},{"path":"mrp.html","id":"get-a-biased-sample-from-it","chapter":"16 Multilevel regression with post-stratification","heading":"16.2.2 Get a biased sample from it","text":"Now want pretend survey biased sample. allow -samples children younger toilet-trained. instance, perhaps gathered sample based records paediatrician, likely see biased sample children. interested knowing proportion children toilet-trained various age-groups.can plot also.pretty clear sample different bedtime overall population, Let us just exercise look median, age toilet-trained status.","code":"\n# This code based on that of Monica Alexander\nset.seed(853)\n\n# Add a weight for each 'type' (has to sum to one)\npopulation_for_mrp_example <- \n  population_for_mrp_example |> \n  mutate(weight = \n           case_when(toilet_trained == 0 & age_group == 1 ~ 0.7,\n                     toilet_trained == 0 ~ 0.1,\n                     age_group %in% c(1, 2, 3) ~ 0.2\n                     ),\n         id = 1:n()\n         )\n\nget_these <- \n  sample(\n    x = population_for_mrp_example$id,\n    size = 1000,\n    prob = population_for_mrp_example$weight\n    )\n\nsample_for_mrp_example <- \n  population_for_mrp_example |> \n  filter(id %in% get_these) |> \n  select(-weight, -id)\n\n# Clean up\npoststratification_dataset <- \n  population_for_mrp_example |> \n  select(-weight, -id)\nsample_for_mrp_example |> \n  mutate(toilet_trained = as_factor(toilet_trained)) |> \n  ggplot(aes(x = age_group, y = bed_time)) +\n  geom_jitter(aes(color = toilet_trained), alpha = 0.4, width = 0.1, height = 0) +\n  labs(x = \"Age-group\",\n       y = \"Bed time\",\n       color = \"Toilet trained\") +\n  theme_classic() +\n  scale_color_brewer(palette = \"Set1\")\nsample_for_mrp_example_summarized <- \n  sample_for_mrp_example |> \n  group_by(age_group, toilet_trained) |> \n  summarise(median_bed_time = median(bed_time))\n#> `summarise()` has grouped output by 'age_group'. You can\n#> override using the `.groups` argument.\n\nsample_for_mrp_example_summarized |> \n  knitr::kable(digits = 2,\n               col.names = c(\"Age-group\", \"Is toilet trained\", \"Average bed time\"))"},{"path":"mrp.html","id":"model-the-sample","chapter":"16 Multilevel regression with post-stratification","heading":"16.2.3 Model the sample","text":"quickly train model based (biased) survey. use modelsummary (Arel-Bundock 2021a) format estimates.‘multilevel regression’ part MRP (although isn’t really multilevel model just keep things simple now).","code":"\nlibrary(modelsummary)\n\nmrp_example_model <- \n  lm(bed_time ~ age_group + toilet_trained, data = sample_for_mrp_example)\n\nmrp_example_model |> \n  modelsummary::modelsummary(fmt = 2)"},{"path":"mrp.html","id":"get-a-post-stratification-dataset","chapter":"16 Multilevel regression with post-stratification","heading":"16.2.4 Get a post-stratification dataset","text":"Now use post-stratification dataset get estimates number cell. typically use larger dataset may closely reflection population. US popular choice ACS, countries typically use census.simulation example, just take 10 per cent sample population use post-stratification dataset.ideal world individual-level data post-stratification dataset (case ). world can apply model individual. likely situation, reality, just counts groups, going try construct estimate group.","code":"\nset.seed(853)\n\npoststratification_dataset <- \n  population_for_mrp_example |> \n  slice(1:100000) |> \n  select(-bed_time)\n\npoststratification_dataset |> \n  head()\n#> # A tibble: 6 × 4\n#>   age_group toilet_trained weight    id\n#>   <fct>     <fct>           <dbl> <int>\n#> 1 1         0                 0.7     1\n#> 2 2         1                 0.2     2\n#> 3 1         0                 0.7     3\n#> 4 1         1                 0.2     4\n#> 5 1         1                 0.2     5\n#> 6 3         0                 0.1     6\npoststratification_dataset_grouped <- \n  poststratification_dataset |> \n  group_by(age_group, toilet_trained) |> \n  count()\n\npoststratification_dataset_grouped |> \n  head()\n#> # A tibble: 6 × 3\n#> # Groups:   age_group, toilet_trained [6]\n#>   age_group toilet_trained     n\n#>   <fct>     <fct>          <int>\n#> 1 1         0              16766\n#> 2 1         1              16649\n#> 3 2         0              16801\n#> 4 2         1              16617\n#> 5 3         0              16625\n#> 6 3         1              16542"},{"path":"mrp.html","id":"post-stratify-our-model-estimates","chapter":"16 Multilevel regression with post-stratification","heading":"16.2.5 Post-stratify our model estimates","text":"Now create estimate group, add confidence intervals.point can look MRP estimates (circles) along confidence intervals, compare raw estimates data (squares). case, know truth, can also compare known truth (triangles) (something can normally).","code":"\npoststratification_dataset_grouped <- \n  mrp_example_model |> \n  predict(newdata = poststratification_dataset_grouped, interval = \"confidence\") |> \n  as_tibble() |> \n  cbind(poststratification_dataset_grouped) \npoststratification_dataset_grouped |> \n  ggplot(aes(x = age_group, y = fit)) +\n  geom_point(data = population_for_mrp_example_summarised,\n             aes(x = age_group, y = median_bed_time, color = toilet_trained), \n             shape = 17) +\n  geom_point(data = sample_for_mrp_example_summarized,\n             aes(x = age_group, y = median_bed_time, color = toilet_trained), \n             shape = 15) +\n  geom_pointrange(aes(ymin=lwr, ymax=upr, color = toilet_trained)) +\n  labs(x = \"Age-group\",\n       y = \"Bed time\",\n       color = \"Toilet trained\") +\n  theme_classic() +\n  scale_color_brewer(palette = \"Set1\")"},{"path":"mrp.html","id":"case-study---xbox-paper","chapter":"16 Multilevel regression with post-stratification","heading":"16.3 Case study - Xbox paper","text":"","code":""},{"path":"mrp.html","id":"overview-3","chapter":"16 Multilevel regression with post-stratification","heading":"16.3.1 Overview","text":"One famous MRP example W. Wang et al. (2015). used data Xbox gaming platform forecast 2012 US Presidential Election.Key facts set-:Data opt-poll available Xbox gaming platform 45 days leading 2012 US presidential election (Obama Romney).day three five questions, including voter intention: ‘election held today, vote ?’.Respondents allowed answer per day.First-time respondents asked provide information , including sex, race, age, education, state, party ID, political ideology, voted 2008 presidential election.total, 750,148 interviews conducted, 345,858 unique respondents - 30,000 completed five polls.Young men dominate Xbox population: 18--29-year-olds comprise 65 per cent Xbox dataset, compared 19 per cent exit poll; men make 93 per cent Xbox sample 47 per cent electorate.","code":""},{"path":"mrp.html","id":"model-1","chapter":"16 Multilevel regression with post-stratification","heading":"16.3.2 Model","text":"Given structure US electorate, use two-stage modelling approach. details don’t really matter much, essentially model likely respondent vote Obama, given various information state, education, sex, etc:\\[\nPr\\left(Y_i = \\mbox{Obama} | Y_i\\\\{\\mbox{Obama, Romney}\\}\\right) = \\mbox{logit}^{-1}(\\alpha_0 + \\alpha_1(\\mbox{state last vote share}) + \\alpha_{j[]}^{\\mbox{state}} + \\alpha_{j[]}^{\\mbox{edu}} + \\alpha_{j[]}^{\\mbox{sex}} + ...)\n\\]run R using glmer() ‘lme4’ (Bates et al. 2015).","code":""},{"path":"mrp.html","id":"post-stratify","chapter":"16 Multilevel regression with post-stratification","heading":"16.3.3 Post-stratify","text":"trained model considers effect various independent variables support candidates, now post-stratify, ‘cell-level estimates weighted proportion electorate cell aggregated appropriate level (.e., state national).’means need cross-tabulated population data. general, census worked, one large surveys available US, difficulty variables need available cross-tab basis. , use exit polls (viable option countries).make state-specific estimates post-stratifying features state (Figure 16.1).\nFigure 16.1: Post-stratified estimates state based Xbox survey MRP\nSimilarly, can examine demographic-differences (Figure 16.2).\nFigure 16.2: Post-stratified estimates demographic basis based Xbox survey MRP\nFinally, convert estimates electoral college estimates (Figure 16.3).\nFigure 16.3: Post-stratified estimates electoral college outcomes based Xbox survey MRP\n","code":""},{"path":"mrp.html","id":"simulation---australian-voting","chapter":"16 Multilevel regression with post-stratification","heading":"16.4 Simulation - Australian voting","text":"","code":""},{"path":"mrp.html","id":"overview-4","chapter":"16 Multilevel regression with post-stratification","heading":"16.4.1 Overview","text":"reminder, workflow use :read poll;model poll;read post-stratification data; andapply model post-stratification data.earlier example, really much modelling step, despite name ‘multilevel modelling post-stratification’, actually use multilevel model. nothing says use multilevel model, lot situations circumstances likely worse. clear, means although individual-level data, grouping individuals ’ll take advantage . instance, case trying model elections, usually districts/divisions/electorates/ridings/etc exist within provinces/states likely make sense , least, include coefficient adjusts intercept province.section simulate another dataset fit different models . going draw Australian elections set-. Australia parliamentary system, 151 seats parliament, one electorate. electorates grouped within six states two territories. two major parties - Australian Labor Party (ALP) Liberal Party (LP). Somewhat confusingly, Liberal party actually conservative, right-wing party, Labor party progressive, left-wing, party.","code":""},{"path":"mrp.html","id":"construct-a-survey","chapter":"16 Multilevel regression with post-stratification","heading":"16.4.2 Construct a survey","text":"move us slightly closer reality, going simulate survey (rather sample population earlier) post-stratify using real data. dependent variable ‘supports_ALP’, binary variable - either 0 1. just start three independent variables :‘gender’, either ‘female’ ‘male’ (available Australian Bureau Statistics);‘age_group’, one four groups: ‘ages 18 29’, ‘ages 30 44’, ‘ages 45 59’, ‘ages 60 plus’;‘state’, one eight integers: 1 - 8 (inclusive).Finally, want survey -sample females, ’ll just get rid 300 males.","code":"\nlibrary(tidyverse)\nset.seed(853)\n\nsize_of_sample_for_australian_polling <- 2000\n\nsample_for_australian_polling <- \n  tibble(age_group = \n           sample(x = c(0:3), \n                  size = size_of_sample_for_australian_polling,\n                  replace = TRUE\n                  ),\n         gender = \n           sample(x = c(0:1),\n                  size = size_of_sample_for_australian_polling,\n                  replace = TRUE\n                  ),\n         state = \n           sample(x = c(1:8),\n                  size = size_of_sample_for_australian_polling,\n                  replace = TRUE\n                  ),\n         noise = rnorm(size_of_sample_for_australian_polling, mean = 0, sd = 1), \n         support_alp = 1 + 0.5 * age_group + 0.5 * gender + 0.01 * state + noise\n         ) \n\n# Normalize the outcome variable\nsample_for_australian_polling <- \n  sample_for_australian_polling |> \n  mutate(support_alp = \n           if_else(support_alp > median(support_alp, na.rm = TRUE), \n                   'Supports ALP', \n                   'Does not')\n         )\n\n# Clean up the simulated data\nsample_for_australian_polling <- \n  sample_for_australian_polling |> \n  mutate(\n    age_group = case_when(\n      age_group == 0 ~ 'Ages 18 to 29',\n      age_group == 1 ~ 'Ages 30 to 44',\n      age_group == 2 ~ 'Ages 45 to 59',\n      age_group == 3 ~ 'Ages 60 plus',\n      TRUE ~ 'Problem'\n      ),\n    gender = case_when(\n      gender == 0 ~ 'Male',\n      gender == 1 ~ 'Female',\n      TRUE ~ 'Problem'\n      ),\n    state = case_when(\n      state == 1 ~ 'Queensland',\n      state == 2 ~ 'New South Wales',\n      state == 3 ~ 'Australian Capital Territory',\n      state == 4 ~ 'Victoria',\n      state == 5 ~ 'Tasmania',\n      state == 6 ~ 'Northern Territory',\n      state == 7 ~ 'South Australia',\n      state == 8 ~ 'Western Australia',\n      TRUE ~ 'Problem'\n      ),\n    \n    ) |> \n  select(-noise)\n\n# Tidy the class\nsample_for_australian_polling <- \n  sample_for_australian_polling |> \n  mutate(across(c(age_group, gender, state, support_alp), as_factor))\n\nsample_for_australian_polling |>   \n  head()\n#> # A tibble: 6 × 4\n#>   age_group     gender state           support_alp \n#>   <fct>         <fct>  <fct>           <fct>       \n#> 1 Ages 18 to 29 Female South Australia Supports ALP\n#> 2 Ages 60 plus  Male   South Australia Supports ALP\n#> 3 Ages 30 to 44 Male   Victoria        Does not    \n#> 4 Ages 18 to 29 Male   Tasmania        Does not    \n#> 5 Ages 18 to 29 Female Victoria        Does not    \n#> 6 Ages 18 to 29 Male   Queensland      Supports ALP\nsample_for_australian_polling <- \n  sample_for_australian_polling |> \n  arrange(gender) |> \n  slice(1:1700)"},{"path":"mrp.html","id":"model-the-survey","chapter":"16 Multilevel regression with post-stratification","heading":"16.4.3 Model the survey","text":"polling data generated make males older people less likely vote ALP; females younger people likely vote Labor Party. Females -sampled. , ALP skew dataset. going use gtsummary package quickly make summary table (Arel-Bundock 2021a).Now ’d like see can get results back (find females less likely males vote Australian Labor Party people less likely vote Australian Labor Party get older). model :ADD MODEL.model says probability person, \\(j\\), vote Australian Labor Party depends gender age-group. Based simulated data, like older age-groups less likely vote Australian Labor Party males less likely vote Australian Labor Party.Essentially got inputs back. dependent variable binary, used logistic regression results little difficult interpret.","code":"\nlibrary(modelsummary)\n\nsample_for_australian_polling |> \n    datasummary_skim(type = \"categorical\")\nalp_support <- \n  glm(support_alp ~ gender + age_group + state, \n      data = sample_for_australian_polling,\n      family = \"binomial\"\n      )\n\nalp_support |> \n  modelsummary(fmt = 2, exponentiate = TRUE)"},{"path":"mrp.html","id":"post-stratify-1","chapter":"16 Multilevel regression with post-stratification","heading":"16.4.4 Post-stratify","text":"Now ’d like see can use found poll get estimate state based demographic features.First read real demographic data, state basis, ABS.point, got decision make need variables survey post-stratification dataset, state abbreviations used, survey, full names used. change post-stratification dataset survey data already modelled.just going rough forecasts. gender age-group want relevant coefficient example data can construct estimates.now post-stratified estimates state model fair weaknesses. instance, small cell counts going problematic. approach ignores uncertainty, now something working can complicate .","code":"\npost_strat_census_data <- \n  read_csv(\"outputs/data/census_data.csv\")\n\nhead(post_strat_census_data)\n#> # A tibble: 6 × 5\n#>   state gender age_group  number cell_prop_of_division_total\n#>   <chr> <chr>  <chr>       <dbl>                       <dbl>\n#> 1 ACT   Female ages18to29  34683                       0.125\n#> 2 ACT   Female ages30to44  42980                       0.155\n#> 3 ACT   Female ages45to59  33769                       0.122\n#> 4 ACT   Female ages60plus  30322                       0.109\n#> 5 ACT   Male   ages18to29  34163                       0.123\n#> 6 ACT   Male   ages30to44  41288                       0.149\npost_strat_census_data <- \n  post_strat_census_data |> \n  mutate(\n    state = \n      case_when(\n        state == 'ACT' ~ 'Australian Capital Territory',\n        state == 'NSW' ~ 'New South Wales',\n        state == 'NT' ~ 'Northern Territory',\n        state == 'QLD' ~ 'Queensland',\n        state == 'SA' ~ 'South Australia',\n        state == 'TAS' ~ 'Tasmania',\n        state == 'VIC' ~ 'Victoria',\n        state == 'WA' ~ 'Western Australia',\n        TRUE ~ \"Problem\"\n      ),\n    age_group = \n      case_when(\n        age_group == 'ages18to29' ~ 'Ages 18 to 29',\n        age_group == 'ages30to44' ~ 'Ages 30 to 44',\n        age_group == 'ages45to59' ~ 'Ages 45 to 59',\n        age_group == 'ages60plus' ~ 'Ages 60 plus',\n        TRUE ~ \"Problem\"\n      )\n  )\npost_strat_census_data <- \n  alp_support |> \n  predict(newdata = post_strat_census_data, type = 'response', se.fit = TRUE) |> \n  as_tibble() |> \n  cbind(post_strat_census_data)\n\npost_strat_census_data |> \n  mutate(alp_predict_prop = fit*cell_prop_of_division_total) |> \n  group_by(state) |> \n  summarise(alp_predict = sum(alp_predict_prop))\n#> # A tibble: 8 × 2\n#>   state                        alp_predict\n#>   <chr>                              <dbl>\n#> 1 Australian Capital Territory       0.551\n#> 2 New South Wales                    0.487\n#> 3 Northern Territory                 0.546\n#> 4 Queensland                         0.491\n#> 5 South Australia                    0.403\n#> 6 Tasmania                           0.429\n#> 7 Victoria                           0.521\n#> 8 Western Australia                  0.460"},{"path":"mrp.html","id":"improving-the-model","chapter":"16 Multilevel regression with post-stratification","heading":"16.4.5 Improving the model","text":"’d like address major issues approach, specifically able deal small cell counts, also taking better account uncertainty. dealing survey data, prediction intervals something similar critical, appropriate report central estimates. ’ll use broad approach , just improve model. going change Bayesian model use rstanarm package (Goodrich et al. 2020).Now, using basic model , Bayesian setting., ’d like estimate state based demographic features. just going rough forecasts. gender age-group want relevant coefficient example data can construct estimates (code Monica Alexander). going use tidybayes (Kay 2020).now post-stratified estimates division. new Bayesian approach enable us think deeply uncertainty. complicate variety ways including adding coefficients (remember ’d need get new cell counts), adding layers.One interesting aspect multilevel approach allow us deal small cell counts borrowing information cells. Even remove , say, 18--29-year-old, male respondents Tasmania model still provide estimates. pooling, effect young, male, Tasmanians partially determined cells respondents.many interesting aspects may like communicate others. instance, may like show model affecting results. can make graph compares raw estimate model estimate.Similarly, may like plot distribution coefficients. can work coefficients pass gather_draws tidybayes::get_variables(model; example passed ‘b_.’, always case.","code":"\nlibrary(rstanarm)\n#> Loading required package: Rcpp\n#> This is rstanarm version 2.21.1\n#> - See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n#> - Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n#> - For execution on a local, multicore CPU with excess RAM we recommend calling\n#>   options(mc.cores = parallel::detectCores())\n\nimproved_alp_support <- \n  stan_glm(support_alp ~ gender + age_group + state,\n                     data = sample_for_australian_polling,\n                     family = binomial(link = \"logit\"),\n                     prior = normal(0, 1), \n                     prior_intercept = normal(0, 1),\n                     cores = 2, \n                     seed = 12345)\nlibrary(tidybayes)\n\npost_stratified_estimates <- \n  improved_alp_support |> \n  add_fitted_draws(newdata = post_strat_census_data) |> \n  rename(alp_predict = .value) |> \n  mutate(alp_predict_prop = alp_predict*cell_prop_of_division_total) |> \n  group_by(state, .draw) |> \n  summarise(alp_predict = sum(alp_predict_prop)) |> \n  group_by(state) |> \n  summarise(mean = mean(alp_predict), \n            lower = quantile(alp_predict, 0.025), \n            upper = quantile(alp_predict, 0.975))\n#> Warning: `fitted_draws` and `add_fitted_draws` are deprecated as their names were confusing.\n#> Use [add_]epred_draws() to get the expectation of the posterior predictive.\n#> Use [add_]linpred_draws() to get the distribution of the linear predictor.\n#> For example, you used [add_]fitted_draws(..., scale = \"response\"), which\n#> means you most likely want [add_]epred_draws(...).\n#> `summarise()` has grouped output by 'state'. You can\n#> override using the `.groups` argument.\n\npost_stratified_estimates\n#> # A tibble: 8 × 4\n#>   state                         mean lower upper\n#>   <chr>                        <dbl> <dbl> <dbl>\n#> 1 Australian Capital Territory 0.550 0.494 0.604\n#> 2 New South Wales              0.486 0.429 0.544\n#> 3 Northern Territory           0.544 0.483 0.607\n#> 4 Queensland                   0.491 0.432 0.548\n#> 5 South Australia              0.412 0.361 0.464\n#> 6 Tasmania                     0.429 0.372 0.487\n#> 7 Victoria                     0.519 0.453 0.583\n#> 8 Western Australia            0.460 0.401 0.520\npost_stratified_estimates |> \n  ggplot(aes(y = mean, x = forcats::fct_inorder(state), color = \"MRP estimate\")) + \n  geom_point() +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0) + \n  labs(y = \"Proportion ALP support\",\n       x = \"State\") + \n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\") +\n  theme(legend.title = element_blank()) +\n  coord_flip()\n# tidybayes::get_variables(improved_alp_support)\n# improved_alp_support |>\n#   gather_draws(genderMale) |>\n#   ungroup() |>\n#   # mutate(coefficient = stringr::str_replace_all(.variable, c(\"b_\" = \"\"))) |>\n#   mutate(coefficient = forcats::fct_recode(coefficient,\n#                                            Intercept = \"Intercept\",\n#                                            `Is male` = \"genderMale\",\n#                                            `Age 30-44` = \"age_groupages30to44\",\n#                                            `Age 45-59` = \"age_groupages45to59\",\n#                                            `Age 60+` = \"age_groupages60plus\"\n#                                            )) |> \n# \n# # both |> \n#   ggplot(aes(y=fct_rev(coefficient), x = .value)) + \n#   ggridges::geom_density_ridges2(aes(height = ..density..),\n#                                  rel_min_height = 0.01, \n#                                  stat = \"density\",\n#                                  scale=1.5) +\n#   xlab(\"Distribution of estimate\") +\n#   ylab(\"Coefficient\") +\n#   scale_fill_brewer(name = \"Dataset: \", palette = \"Set1\") +\n#   theme_minimal() +\n#   theme(panel.grid.major = element_blank(),\n#         panel.grid.minor = element_blank()) +\n#   theme(legend.position = \"bottom\")"},{"path":"mrp.html","id":"forecasting-the-2020-us-election","chapter":"16 Multilevel regression with post-stratification","heading":"16.5 Forecasting the 2020 US election","text":"US election lot features unique US, model going build going fairly generic , largely generalization earlier model Australian election. One good thing forecasting US election lot data around. case can use survey data Democracy Fund Voter Study Group.2 conducted polling lead-US election make publicly available registration. use Integrated Public Use Microdata Series (IPUMS), access 2018 American Community Survey (ACS) post-stratification dataset. use state, age-group, gender, education explanatory variables.","code":""},{"path":"mrp.html","id":"survey-data","chapter":"16 Multilevel regression with post-stratification","heading":"16.5.1 Survey data","text":"first step need actually get survey data. Go website: https://www.voterstudygroup.org looking ‘Nationscape’ button along lines ‘Get latest Nationscape data’. get dataset, need fill form, process email . real person side form, request take days.get access want download .dta files. Nationscape conducted many surveys many files. filename reference date, ‘ns20200625’ refers 25 June 2020, one use .can read ‘.dta’ files using haven package (Wickham Miller 2020). code based Alen Mitrovski, Xiaoyan Yang, Matthew Wankiewicz: https://github.com/matthewwankiewicz/US_election_forecast.seen, one difficult aspects MRP ensuring consistency datasets. case, need work make variables consistent.","code":"\nlibrary(haven)\nlibrary(tidyverse)\n\nraw_nationscape_data <- \n  read_dta(here::here(\"dont_push/ns20200625.dta\"))\n\n# The Stata format separates labels so reunite those\nraw_nationscape_data <- \n  labelled::to_factor(raw_nationscape_data)\n\n# Just keep relevant variables\nnationscape_data <- \n  raw_nationscape_data |> \n  select(vote_2020,\n         gender,\n         education,\n         state,\n         age)\n\n# For simplicity, remove anyone undecided or planning to vote for someone other than Biden/Trump and make vote a binary variable: 1 for Biden, 0 for Trump.\nnationscape_data <- \n  nationscape_data |> \n  filter(vote_2020 == \"Joe Biden\" | vote_2020 == \"Donald Trump\") |> \n  mutate(vote_biden = if_else(vote_2020 == \"Joe Biden\", 1, 0)) |> \n  select(-vote_2020)\n\n# Create the dependent variables by grouping the existing variables\nnationscape_data <- \n  nationscape_data |> \n  mutate(\n    age_group = case_when( # case_when works in order and exits when there's a match\n      age <= 29 ~ 'age_18-29',\n      age <= 44 ~ 'age_30-44',\n      age <= 59 ~ 'age_45-59',\n      age >= 60 ~ 'age_60_or_more',\n      TRUE ~ 'Trouble'\n      ),\n    gender = case_when(\n      gender == \"Female\" ~ 'female',\n      gender == \"Male\" ~ 'male',\n      TRUE ~ 'Trouble'\n      ),\n    education_level = case_when(\n      education == \"3rd Grade or less\" ~ \"High school or less\",\n      education == \"Middle School - Grades 4 - 8\" ~ \"High school or less\",\n      education == \"Completed some high school\" ~ \"High school or less\",\n      education == \"High school graduate\" ~ \"High school or less\",\n      education == \"Other post high school vocational training\" ~ \"Some post secondary\",\n      education == \"Completed some college, but no degree\" ~ \"Some post secondary\",\n      education == \"Associate Degree\" ~ \"Post secondary or higher\",\n      education == \"College Degree (such as B.A., B.S.)\" ~ \"Post secondary or higher\",\n      education == \"Completed some graduate, but no degree\" ~ \"Post secondary or higher\",\n      education == \"Masters degree\" ~ \"Graduate degree\",\n      education == \"Doctorate degree\" ~ \"Graduate degree\",\n      TRUE ~ 'Trouble'\n      )\n    ) |> \n  select(-education, -age)\n\ntests <- \n  nationscape_data |> \n  mutate(test = stringr::str_detect(age_group, 'Trouble'),\n         test = if_else(test == TRUE, TRUE, \n                        stringr::str_detect(education_level, 'Trouble')),\n         test = if_else(test == TRUE, TRUE, \n                        stringr::str_detect(gender, 'Trouble'))\n         ) |> \n  filter(test == TRUE)\n\nif(nrow(tests) != 0) {\n  print(\"Check nationscape_data\")\n  } else {\n    rm(tests)\n    }\n\nnationscape_data |> \n  head()\n#> # A tibble: 6 × 5\n#>   gender state vote_biden age_group      education_level    \n#>   <chr>  <chr>      <dbl> <chr>          <chr>              \n#> 1 female WI             0 age_45-59      Post secondary or …\n#> 2 female VA             0 age_45-59      Post secondary or …\n#> 3 female TX             0 age_60_or_more High school or less\n#> 4 female WA             0 age_45-59      High school or less\n#> 5 female MA             1 age_18-29      Some post secondary\n#> 6 female TX             1 age_30-44      Some post secondary\n# This code is very directly from Alen Mitrovski, Xiaoyan Yang, and Matthew Wankiewicz.\n# Format state names so the whole state name is written out, to match IPUMS data\nstates_names_and_abbrevs <- \n  tibble(stateicp = state.name, state = state.abb)\n\nnationscape_data <-\n  nationscape_data |>\n  left_join(states_names_and_abbrevs)\n#> Joining, by = \"state\"\n\nrm(states_names_and_abbrevs)\n\n# Make lowercase to match IPUMS data\nnationscape_data <- \n  nationscape_data |> \n  mutate(stateicp = tolower(stateicp))\n\n# Replace NAs with DC\nnationscape_data$stateicp <- \n  replace_na(nationscape_data$stateicp, \"district of columbia\")\n\n# Tidy the class\nnationscape_data <- \n  nationscape_data |> \n  mutate(across(c(gender, stateicp, education_level, age_group), as_factor))\n\n# Save data\nwrite_csv(nationscape_data, \"outputs/data/polling_data.csv\")\n\nnationscape_data |> \n  head()\n#> # A tibble: 6 × 6\n#>   gender state vote_biden age_group education_level stateicp\n#>   <fct>  <chr>      <dbl> <fct>     <fct>           <fct>   \n#> 1 female WI             0 age_45-59 Post secondary… wiscons…\n#> 2 female VA             0 age_45-59 Post secondary… virginia\n#> 3 female TX             0 age_60_o… High school or… texas   \n#> 4 female WA             0 age_45-59 High school or… washing…\n#> 5 female MA             1 age_18-29 Some post seco… massach…\n#> 6 female TX             1 age_30-44 Some post seco… texas"},{"path":"mrp.html","id":"post-stratification-data","chapter":"16 Multilevel regression with post-stratification","heading":"16.5.2 Post-stratification data","text":"lot options dataset post-stratify various considerations. dataset better quality (however defined), likely larger. strictly data perspective, best choice probably something like Cooperative Congressional Election Study (CCES), however whatever reason released election reasonable choice. W. Wang et al. (2015) use exit poll data, available election.countries ’d stuck using census, course quite large, likely --date. Luckily US opportunity use American Community Survey (ACS) asks analogous questions census, conducted every month, course year, end million responses. case going access ACS IPUMS.go IPUMS website - https://ipums.org - looking something like IPUMS USA ‘get data’. Create account, need . ’ll take process. account, go ‘Select Samples’ de-select everything apart 2019 ACS. need get variables interested . household want ‘STATEICP’, person want ‘SEX’, ‘AGE’, ‘EDUC’. everything selected, ‘view cart’, want careful change ‘data format’ ‘.dta’ (’s nothing wrong formats, just already got code earlier deal type). Briefly just check many rows columns requesting. around million rows, around ten twenty columns. much 300MB maybe just see willve accidently selected something don’t need. Submit request within day, get email saying data can downloaded. take 30 minutes , don’t get email within day check size dataset, customize sample size reduce size initially.case Let us tidy data.dataset individual level. ’ll create counts sub-cell, proportions state.Now ’d like add proportions state.","code":"\n# Again, closely following code from Alen Mitrovski, Xiaoyan Yang, and Matthew Wankiewicz.\nlibrary(haven)\nlibrary(tidyverse)\n\nraw_poststrat_data <- \n  read_dta(here::here(\"dont_push/usa_00004.dta\"))\n\n# The Stata format separates labels so reunite those\nraw_poststrat_data <- \n  labelled::to_factor(raw_poststrat_data)\nhead(raw_poststrat_data)\n#> # A tibble: 6 × 28\n#>   year  sample serial cbserial  hhwt cluster region stateicp\n#>   <fct> <fct>   <dbl>    <dbl> <dbl>   <dbl> <fct>  <fct>   \n#> 1 2018  2018 …      2  2.02e12 392.  2.02e12 east … alabama \n#> 2 2018  2018 …      7  2.02e12  94.1 2.02e12 east … alabama \n#> 3 2018  2018 …     13  2.02e12  83.7 2.02e12 east … alabama \n#> 4 2018  2018 …     18  2.02e12  57.5 2.02e12 east … alabama \n#> 5 2018  2018 …     23  2.02e12 157.  2.02e12 east … alabama \n#> 6 2018  2018 …     28  2.02e12 157.  2.02e12 east … alabama \n#> # … with 20 more variables: strata <dbl>, gq <fct>,\n#> #   pernum <dbl>, perwt <dbl>, sex <fct>, age <fct>,\n#> #   marst <fct>, race <fct>, raced <fct>, hispan <fct>,\n#> #   hispand <fct>, bpl <fct>, bpld <fct>, citizen <fct>,\n#> #   educ <fct>, educd <fct>, empstat <fct>, empstatd <fct>,\n#> #   labforce <fct>, inctot <dbl>\n\nraw_poststrat_data$age <- as.numeric(raw_poststrat_data$age)\n\npoststrat_data <- \n  raw_poststrat_data |> \n  filter(inctot < 9999999) |> \n  filter(age >= 18) |> \n  mutate(gender = sex) |> \n  mutate(\n    age_group = case_when( # case_when works in order and exits when there's a match\n      age <= 29 ~ 'age_18-29',\n      age <= 44 ~ 'age_30-44',\n      age <= 59 ~ 'age_45-59',\n      age >= 60 ~ 'age_60_or_more',\n      TRUE ~ 'Trouble'\n      ),\n    education_level = case_when(\n      educd == \"nursery school, preschool\" ~ \"High school or less\",\n      educd == \"kindergarten\" ~ \"High school or less\",\n      educd == \"grade 1\" ~ \"High school or less\",\n      educd == \"grade 2\" ~ \"High school or less\",\n      educd == \"grade 3\" ~ \"High school or less\",\n      educd == \"grade 4\" ~ \"High school or less\",\n      educd == \"grade 5\" ~ \"High school or less\",\n      educd == \"grade 6\" ~ \"High school or less\",\n      educd == \"grade 7\" ~ \"High school or less\",\n      educd == \"grade 8\" ~ \"High school or less\",\n      educd == \"grade 9\" ~ \"High school or less\",\n      educd == \"grade 10\" ~ \"High school or less\",\n      educd == \"grade 11\" ~ \"High school or less\",\n      educd == \"12th grade, no diploma\" ~ \"High school or less\",\n      educd == \"regular high school diploma\" ~ \"High school or less\",\n      educd == \"ged or alternative credential\" ~ \"High school or less\",\n      educd == \"some college, but less than 1 year\" ~ \"Some post secondary\",\n      educd == \"1 or more years of college credit, no degree\" ~ \"Some post secondary\",\n      educd == \"associate's degree, type not specified\" ~ \"Post secondary or higher\",\n      educd == \"bachelor's degree\" ~ \"Post secondary or higher\",\n      educd == \"master's degree\" ~ \"Graduate degree\",\n      educd == \"professional degree beyond a bachelor's degree\" ~ \"Graduate degree\",\n      educd == \"doctoral degree\" ~ \"Graduate degree\",\n      educd == \"no schooling completed\" ~ \"High school or less\",\n      TRUE ~ 'Trouble'\n      )\n    )\n\n# Just keep relevant variables\npoststrat_data <- \n  poststrat_data |> \n  select(gender,\n         age_group,\n         education_level,\n         stateicp)\n\n# Tidy the class\npoststrat_data <- \n  poststrat_data |> \n  mutate(across(c(gender, stateicp, education_level, age_group), as_factor))\n\n# Save data\nwrite_csv(poststrat_data, \"outputs/data/us_poststrat.csv\")\n\npoststrat_data |> \n  head()\n#> # A tibble: 6 × 4\n#>   gender age_group      education_level     stateicp\n#>   <fct>  <fct>          <fct>               <fct>   \n#> 1 female age_18-29      Some post secondary alabama \n#> 2 female age_60_or_more Some post secondary alabama \n#> 3 male   age_45-59      Some post secondary alabama \n#> 4 male   age_30-44      High school or less alabama \n#> 5 female age_60_or_more High school or less alabama \n#> 6 male   age_30-44      High school or less alabama\npoststrat_data_cells <- \n  poststrat_data |> \n  group_by(stateicp, gender, age_group, education_level) |> \n  count()\npoststrat_data_cells <- \n  poststrat_data_cells |> \n  group_by(stateicp) |> \n  mutate(prop = n/sum(n)) |> \n  ungroup()\n\npoststrat_data_cells |> head()\n#> # A tibble: 6 × 6\n#>   stateicp    gender age_group education_level     n    prop\n#>   <fct>       <fct>  <fct>     <fct>           <int>   <dbl>\n#> 1 connecticut male   age_18-29 Some post seco…   149 0.0260 \n#> 2 connecticut male   age_18-29 High school or…   232 0.0404 \n#> 3 connecticut male   age_18-29 Post secondary…    96 0.0167 \n#> 4 connecticut male   age_18-29 Graduate degree    25 0.00436\n#> 5 connecticut male   age_60_o… Some post seco…   142 0.0248 \n#> 6 connecticut male   age_60_o… High school or…   371 0.0647"},{"path":"mrp.html","id":"model-2","chapter":"16 Multilevel regression with post-stratification","heading":"16.5.3 Model","text":"going use logistic regression estimate model binary support Biden explained gender, age-group, education-level, state. going Bayesian framework using rstanarm (Goodrich et al. 2020). variety reasons using rstanarm , main one Stan pre-compiled eases computer set-issues may otherwise . great resource implementing MRP rstanarm Kennedy Gabry (2020).variety options largely unthinkingly set, exploring effect good idea, now can just quick look model.","code":"\nlibrary(rstanarm)\n\nus_election_model <- \n  rstanarm::stan_glmer(vote_biden ~ gender + age_group + (1 | stateicp) + education_level,\n                       data = nationscape_data,\n                       family = binomial(link = \"logit\"),\n                       prior = normal(0, 1), \n                       prior_intercept = normal(0, 1),\n                       cores = 2, \n                       seed = 853)\nmodelsummary::get_estimates(us_election_model)\n#>                                      term effect\n#> 1                             (Intercept)  fixed\n#> 2                              gendermale  fixed\n#> 3                 age_groupage_60_or_more  fixed\n#> 4                      age_groupage_18-29  fixed\n#> 5                      age_groupage_30-44  fixed\n#> 6      education_levelHigh school or less  fixed\n#> 7      education_levelSome post secondary  fixed\n#> 8          education_levelGraduate degree  fixed\n#> 9 Sigma[stateicp:(Intercept),(Intercept)] random\n#>      estimate conf.level    conf.low    conf.high      pd\n#> 1  0.27591651       0.95  0.10322566  0.447878764 0.99850\n#> 2 -0.54094841       0.95 -0.65227570 -0.430121742 1.00000\n#> 3  0.04886803       0.95 -0.10847716  0.200831286 0.72600\n#> 4  0.87817445       0.95  0.69830955  1.061899702 1.00000\n#> 5  0.12455081       0.95 -0.02530517  0.284994549 0.94125\n#> 6 -0.35080948       0.95 -0.51262507 -0.204222132 1.00000\n#> 7 -0.14970941       0.95 -0.29927232  0.006135831 0.96625\n#> 8 -0.21988079       0.95 -0.38172115 -0.036764485 0.99100\n#> 9  0.08002654       0.95  0.02912381  0.160211514 1.00000\n#>   rope.percentage      rhat      ess prior.distribution\n#> 1       0.1281242 1.0002399 2262.704             normal\n#> 2       0.0000000 0.9996776 5264.121             normal\n#> 3       0.9765851 0.9998468 3726.472             normal\n#> 4       0.0000000 0.9997188 3931.709             normal\n#> 5       0.7729545 0.9995061 3567.152             normal\n#> 6       0.0000000 0.9997706 3700.321             normal\n#> 7       0.6771902 1.0005471 4090.628             normal\n#> 8       0.3249145 0.9999286 4589.040             normal\n#> 9       1.0000000 1.0031942 1284.436               <NA>\n#>   prior.location prior.scale\n#> 1              0           1\n#> 2              0           1\n#> 3              0           1\n#> 4              0           1\n#> 5              0           1\n#> 6              0           1\n#> 7              0           1\n#> 8              0           1\n#> 9             NA          NA\n# The default usage of modelsummary requires statistics that we don't have.\n# Uncomment the following line if you want to look at what is available and specify your own:\n# modelsummary::get_estimates(us_election_model)\nmodelsummary::modelsummary(us_election_model,\n                            statistic = c('conf.low', 'conf.high')\n                            )"},{"path":"mrp.html","id":"post-stratify-2","chapter":"16 Multilevel regression with post-stratification","heading":"16.5.4 Post-stratify","text":"can look estimates, like.","code":"\nbiden_support_by_state <- \n  us_election_model |>\n  tidybayes::add_fitted_draws(newdata=poststrat_data_cells) |>\n  rename(support_biden_predict = .value) |> \n  mutate(support_biden_predict_prop = support_biden_predict*prop) |> \n  group_by(stateicp, .draw) |> \n  summarise(support_biden_predict = sum(support_biden_predict_prop)) |> \n  group_by(stateicp) |> \n  summarise(mean = mean(support_biden_predict), \n            lower = quantile(support_biden_predict, 0.025), \n            upper = quantile(support_biden_predict, 0.975))\n#> Warning: `fitted_draws` and `add_fitted_draws` are deprecated as their names were confusing.\n#> Use [add_]epred_draws() to get the expectation of the posterior predictive.\n#> Use [add_]linpred_draws() to get the distribution of the linear predictor.\n#> For example, you used [add_]fitted_draws(..., scale = \"response\"), which\n#> means you most likely want [add_]epred_draws(...).\n#> `summarise()` has grouped output by 'stateicp'. You can\n#> override using the `.groups` argument.\nbiden_support_by_state |> \n  ggplot(aes(y = mean, x = stateicp, color = \"MRP estimate\")) + \n  geom_point() +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0) + \n  geom_point(data = \n               nationscape_data |> \n               group_by(stateicp, vote_biden) |>\n               summarise(n = n()) |> \n               group_by(stateicp) |> \n               mutate(prop = n/sum(n)) |> \n               filter(vote_biden==1), \n             aes(y = prop, x = stateicp, color = 'Nationscape raw data')) +\n  geom_hline(yintercept = 0.5, linetype = 'dashed') +\n  labs(x = 'State',\n       y = 'Estimated proportion support for Biden',\n       color = 'Source') +\n  theme_classic() +\n  scale_color_brewer(palette = 'Set1') +\n  coord_flip()\n#> `summarise()` has grouped output by 'stateicp'. You can\n#> override using the `.groups` argument."},{"path":"mrp.html","id":"exercises-and-tutorial-15","chapter":"16 Multilevel regression with post-stratification","heading":"16.6 Exercises and tutorial","text":"","code":""},{"path":"mrp.html","id":"exercises-15","chapter":"16 Multilevel regression with post-stratification","heading":"16.6.1 Exercises","text":"Please explain MRP someone university-education, necessarily taken statistics, need explain technical terms use, clear strengths weaknesses (write least three paragraphs).respect W. Wang et al. (2015): paper interesting? like paper? wish better? extent can reproduce paper? (Write least four paragraphs).respect W. Wang et al. (2015), feature mention election forecasts need?\nExplainable.\nAccurate.\nCost-effective.\nRelevant.\nTimely.\nExplainable.Accurate.Cost-effective.Relevant.Timely.respect W. Wang et al. (2015), weakness MRP?\nDetailed data requirement.\nAllows use biased data.\nExpensive conduct.\nDetailed data requirement.Allows use biased data.Expensive conduct.respect W. Wang et al. (2015), concerning Xbox sample?\nNon-representative.\nSmall sample size.\nMultiple responses respondent.\nNon-representative.Small sample size.Multiple responses respondent.interested studying voting intentions 2020 US presidential election vary individual’s income. set logistic regression model study relationship. study, possible independent variables (select )?\nWhether respondent registered vote (yes/).\nWhether respondent going vote Biden (yes/).\nrace respondent (white/white).\nrespondent’s marital status (married/).\nWhether respondent registered vote (yes/).Whether respondent going vote Biden (yes/).race respondent (white/white).respondent’s marital status (married/).Please think Cohn (2016) type exercise carried ? think different groups, even background level quantitative sophistication, different estimates even use data? (Write least four paragraphs).think multilevel regression post-stratification, key assumptions making? (Write least four paragraphs).train model based survey, post-stratify using 2020 ACS dataset. practical considerations may contend ? (Write least four paragraphs).model output lm() called ‘my_model_output’ can use modelsummary display output (assume package loaded) (select )?\nmodelsummary::modelsummary(my_model_output)\nmodelsummary(my_model_output)\nmy_model_output |> modelsummary()\nmy_model_output |> modelsummary(statistic = NULL)\nmodelsummary::modelsummary(my_model_output)modelsummary(my_model_output)my_model_output |> modelsummary()my_model_output |> modelsummary(statistic = NULL)following examples linear models (select )?\nlm(y ~ x_1 + x_2 + x_3, data = my_data)\nlm(y ~ x_1 + x_2^2 + x_3, data = my_data)\nlm(y ~ x_1 * x_2 + x_3, data = my_data)\nlm(y ~ x_1 + x_1^2 + x_2 + x_3, data = my_data)\nlm(y ~ x_1 + x_2 + x_3, data = my_data)lm(y ~ x_1 + x_2^2 + x_3, data = my_data)lm(y ~ x_1 * x_2 + x_3, data = my_data)lm(y ~ x_1 + x_1^2 + x_2 + x_3, data = my_data)Consider situation survey dataset age-groups: 18-29; 30-44; 45- 60; 60+. post-stratification dataset age-groups: 18-24; 25-29; 30-34; 35-39; 40-44; 45-49; 50-54; 55-59; 60+. approach take bringing together? [Please write paragraph.]Consider situation survey dataset age-groups: 18-29; 30-44; 45- 60; 60+. time post-stratification dataset age-groups: 18-34; 35-49; 50-64; 65+. approach take bringing together? (Write least one paragraph).Please consider Kennedy et al. (2020). statistical facets considering survey focused gender, post-stratification survey (select )?\nImpute non-male female\nEstimate gender using auxiliary information\nImpute population\nImpute sample values\nModel population distribution using auxiliary data\nRemove non-binary respondents\nRemove respondents\nAssume population distribution\nImpute non-male femaleEstimate gender using auxiliary informationImpute populationImpute sample valuesModel population distribution using auxiliary dataRemove non-binary respondentsRemove respondentsAssume population distributionPlease consider Kennedy et al. (2020). ethical facets considering survey focused gender, post-stratification survey (select )?\nImpute non-male female\nEstimate gender using auxiliary information\nImpute population\nImpute sample values\nModel population distribution using auxiliary data\nRemove non-binary respondents\nRemove respondents\nAssume population distribution\nImpute non-male femaleEstimate gender using auxiliary informationImpute populationImpute sample valuesModel population distribution using auxiliary dataRemove non-binary respondentsRemove respondentsAssume population distributionPlease consider Kennedy et al. (2020). define ethics?\nRespecting perspectives dignity individual survey respondents.\nGenerating estimates general population subpopulations interest.\nUsing complicated procedures serve useful function.\nRespecting perspectives dignity individual survey respondents.Generating estimates general population subpopulations interest.Using complicated procedures serve useful function.","code":""},{"path":"mrp.html","id":"tutorial-15","chapter":"16 Multilevel regression with post-stratification","heading":"16.6.2 Tutorial","text":"similar manner Ghitza Gelman (2020) pretend willve got access US voter file record private company. train model 2020 US CCES, post-stratify , individual-basis, based voter file.\n. please put-together datasheet voter file dataset following Gebru et al. (2021)? reminder, datasheets accompany datasets document ‘motivation, composition, collection process, recommended uses,’ among aspects.\nb. also please put together model card model, following Mitchell et al. (2019)? reminder, model cards deliberately straight-forward one- two-page documents report aspects : model details; intended use; metrics; training data; ethical considerations; well caveats recommendations (Mitchell et al. 2019).\nc. please discuss three ethical aspects around features using model? [Please write paragraph two point.]\nd. please detail protections put place terms dataset, model, predictions?","code":""},{"path":"mrp.html","id":"paper-6","chapter":"16 Multilevel regression with post-stratification","heading":"16.6.3 Paper","text":"point, Paper Five (Appendix B.5) appropriate.","code":""},{"path":"text-as-data.html","id":"text-as-data","chapter":"17 Text as data","heading":"17 Text as data","text":"Required materialRead Naked Truth: names 6,816 complexion products can reveal bias beauty, (Amaka Thomas 2021).Read Supervised Machine Learning Text Analysis R, Chapters 2 ‘Tokenization’, 3 ‘Stop words’, 6 ‘Regression’, 7 ‘Classification’, (Hvitfeldt Silge 2021).Key concepts skillsUnderstanding text dataset can use.Key librariesgutenbergr (D. Robinson 2021)janitor (Firke 2020).tidytext (Silge Robinson 2016).Key functionsglmnet::glmnet()gutenbergr::gutenberg_download()tidytext::bind_tf_idf()tidytext::cast_dtm()","code":""},{"path":"text-as-data.html","id":"introduction-14","chapter":"17 Text as data","heading":"17.1 Introduction","text":"Text can considered unwieldy, general similar, version datasets used throughout book. main difference typically begin wide data, insofar often column word, token generally. entry often count. typically transform rather long data, column word another column count.larger size text datasets means especially important simulate, start small, comes analysis. Using text data exciting quantity variety text available us. general, dealing text datasets messy. lot cleaning preparation typically required. Often text datasets large. , workflow place, work reproducible way, simulating data first, clearly communicating findings becomes critical, keep everything organized mind. Nonetheless, exciting area.terms next steps two, related, concerns: data analysis.terms data many places get large amounts text data relatively easily, sources already used, including:Accessing Twitter API using rtweet (Kearney 2019).Using Inside Airbnb, provides text reviews.Getting text --copyright books using gutenbergr (D. Robinson 2021).finally, scraping Wikipedia.chapter first consider preparing text datasets. consider logistic lasso regression. finally consider topic models.","code":""},{"path":"text-as-data.html","id":"tf-idf","chapter":"17 Text as data","heading":"17.2 TF-IDF","text":"Inspired Gelfand (2019) following Amaka Thomas (2021), draw dataset put together makeup names descriptions Sephora Ulta. interested counts word. can read data using read_csv().focus ‘product’, provides name item, ‘lightness’ value 0 1. interested whether products lightness values less 0.5, typically use different words lightness values least 0.5.example going split everything separate words. , just searching space. , case just words considered ‘words’, instance, numbers. use unnest_tokens() tidytext (Silge Robinson 2016) .now want count number times word used lightness classifications.can see popular words appear similar two categories. point, use data variety ways. might interested know words characterize group—say, words commonly used group. can first looking word’s term frequency (tf), many times word used product name. issue lot words commonly used regardless context. , may also like look inverse document frequency (idf) ‘penalize’ words occur groups. instance, seen ‘foundation’ occurs products high low lightness values. , idf lower another word occurred products lightness value half. term frequency–inverse document frequency (tf-idf) product .can create value using bind_tf_idf() tidytext. create bunch new columns, one word star combination.","code":"\nlibrary(tidyverse)\n\nmakeup <-\n  read_csv(file = \n             \"https://raw.githubusercontent.com/the-pudding/data/master/foundation-names/allNumbers.csv\")\n\nmakeup\n#> # A tibble: 3,117 × 9\n#>    brand  product name  specific lightness hex   lightToDark\n#>    <chr>  <chr>   <chr> <chr>        <dbl> <chr> <lgl>      \n#>  1 Makeu… Concea… <NA>  F0           0.949 #F2F… TRUE       \n#>  2 HOURG… Veil F… Porc… No. 0        0.818 #F6D… TRUE       \n#>  3 TOM F… Tracel… Pearl 0.0          0.851 #F0D… TRUE       \n#>  4 Arman… Neo Nu… <NA>  0            0.912 #F0E… TRUE       \n#>  5 TOM F… Tracel… Pearl 0.0          0.912 #FDE… TRUE       \n#>  6 Charl… Magic … <NA>  0            0.731 #D9A… TRUE       \n#>  7 Bobbi… Skin W… Porc… 0            0.822 #F3C… TRUE       \n#>  8 Given… Matiss… <NA>  N00          0.831 #F5D… TRUE       \n#>  9 Smash… Studio… <NA>  0.1          0.814 #F8C… TRUE       \n#> 10 Smash… Studio… <NA>  0.1          0.910 #F9E… TRUE       \n#> # … with 3,107 more rows, and 2 more variables:\n#> #   numbers <dbl>, id <dbl>\nmakeup <-\n  makeup |>\n  select(product, lightness) |>\n  mutate(lightness_above_half = if_else(lightness >= 0.5, \"Yes\", \"No\")\n         )\n\ntable(makeup$lightness_above_half)\n#> \n#>   No  Yes \n#>  702 2415\nlibrary(tidytext)\n\nmakeup_by_words <-\n  makeup |>\n  unnest_tokens(output = word, \n                input = product, \n                token = \"words\")\n\nhead(makeup_by_words)\n#> # A tibble: 6 × 3\n#>   lightness lightness_above_half word      \n#>       <dbl> <chr>                <chr>     \n#> 1     0.949 Yes                  conceal   \n#> 2     0.949 Yes                  define    \n#> 3     0.949 Yes                  full      \n#> 4     0.949 Yes                  coverage  \n#> 5     0.949 Yes                  foundation\n#> 6     0.818 Yes                  veil\nmakeup_by_words <-\n  makeup_by_words |>\n  count(lightness_above_half, word, sort = TRUE)\n\nmakeup_by_words |>\n  filter(lightness_above_half == \"Yes\") |>\n  slice(1:5)\n#> # A tibble: 5 × 3\n#>   lightness_above_half word           n\n#>   <chr>                <chr>      <int>\n#> 1 Yes                  foundation  2214\n#> 2 Yes                  skin         452\n#> 3 Yes                  spf          443\n#> 4 Yes                  matte        422\n#> 5 Yes                  powder       327\n\nmakeup_by_words |>\n  filter(lightness_above_half == \"No\") |>\n  slice(1:5)\n#> # A tibble: 5 × 3\n#>   lightness_above_half word           n\n#>   <chr>                <chr>      <int>\n#> 1 No                   foundation   674\n#> 2 No                   matte        106\n#> 3 No                   spf          103\n#> 4 No                   skin          98\n#> 5 No                   liquid        89\nmakeup_by_words_tf_idf <-\n  makeup_by_words |>\n  bind_tf_idf(term = word, \n              document = lightness_above_half, \n              n = n) |>\n  arrange(-tf_idf)\n\nmakeup_by_words_tf_idf\n#> # A tibble: 505 × 6\n#>    lightness_above_half word        n       tf   idf  tf_idf\n#>    <chr>                <chr>   <int>    <dbl> <dbl>   <dbl>\n#>  1 Yes                  cushion    25 0.00187  0.693 1.30e-3\n#>  2 Yes                  combo      16 0.00120  0.693 8.31e-4\n#>  3 Yes                  custom     16 0.00120  0.693 8.31e-4\n#>  4 Yes                  oily       16 0.00120  0.693 8.31e-4\n#>  5 Yes                  perfect    16 0.00120  0.693 8.31e-4\n#>  6 Yes                  refill     16 0.00120  0.693 8.31e-4\n#>  7 Yes                  50         14 0.00105  0.693 7.27e-4\n#>  8 Yes                  compact    13 0.000974 0.693 6.75e-4\n#>  9 Yes                  lifting    12 0.000899 0.693 6.23e-4\n#> 10 Yes                  satte      12 0.000899 0.693 6.23e-4\n#> # … with 495 more rows\nmakeup_by_words_tf_idf |>\n  group_by(lightness_above_half) |>\n  slice(1:5)\n#> # A tibble: 10 × 6\n#> # Groups:   lightness_above_half [2]\n#>    lightness_above_half word         n      tf   idf  tf_idf\n#>    <chr>                <chr>    <int>   <dbl> <dbl>   <dbl>\n#>  1 No                   able         2 5.25e-4 0.693 3.64e-4\n#>  2 No                   concent…     2 5.25e-4 0.693 3.64e-4\n#>  3 No                   marc         2 5.25e-4 0.693 3.64e-4\n#>  4 No                   re           2 5.25e-4 0.693 3.64e-4\n#>  5 No                   look         1 2.63e-4 0.693 1.82e-4\n#>  6 Yes                  cushion     25 1.87e-3 0.693 1.30e-3\n#>  7 Yes                  combo       16 1.20e-3 0.693 8.31e-4\n#>  8 Yes                  custom      16 1.20e-3 0.693 8.31e-4\n#>  9 Yes                  oily        16 1.20e-3 0.693 8.31e-4\n#> 10 Yes                  perfect     16 1.20e-3 0.693 8.31e-4"},{"path":"text-as-data.html","id":"lasso-regression","chapter":"17 Text as data","heading":"17.3 Lasso regression","text":"One nice aspects text can adapt existing methods use input. going use logistic regression, along text inputs, forecast. Inspired Silge (2018) going two different text inputs, train model sample text , try use model forecast text training set. Although arbitrary example, imagine many real-world applications. instance, may interested whether text likely written bot humanFirst need get data. use books Project Gutenberg using gutenberg_download() gutenbergr (D. Robinson 2021). consider Jane Eyre (Bronte 1847) Alice’s Adventures Wonderland (Carroll 1865).One great things dataset tibble. can work familiar skills. line book read different row dataset. Notice downloaded two books , added title. two books one .looking number lines , looks like Jane Eyre much longer Alice Wonderland. start getting rid blank lines using remove_empty() janitor (Firke 2020).still overwhelming amount Jane Eyre, compared Alice Wonderland, sample Jane Eyre make equal.variety issues , instance, whole Alice, random bits Jane, nonetheless continue add counter line number book.now want unnest tokes. use unnest_tokens() tidytext (Silge Robinson 2016).remove word used 10 times. Nonetheless still 500 unique words. (require word used author least 10 times end 6,000 words.)reason relevant independent variables. may used something less 10 explanatory variables, case going 585 , need model can handle .However, mentioned , going rows essentially just one word. filter also, ensures model least words work .’ll create test/training split, load tidymodels.can use cast_dtm() create document-term matrix. provides count many times word used.independent variables sorted, now need binary dependent variable, whether book Alice Wonderland Jane Eyre.Now can run model.Perhaps unsurprisingly, line mentions Alice likely Alice Wonderland mention Jane likely Jane Eyre.","code":"\nlibrary(gutenbergr)\nlibrary(tidyverse)\n\n# The books that we are interested in have the keys of 1260 and 11, respectively.\nalice_and_jane <- \n  gutenberg_download(\n    gutenberg_id = c(1260, 11), \n    meta_fields = \"title\")\n\nwrite_csv(alice_and_jane, \"alice_and_jane.csv\")\n\nhead(alice_and_jane)\nlibrary(janitor)\n#> \n#> Attaching package: 'janitor'\n#> The following objects are masked from 'package:stats':\n#> \n#>     chisq.test, fisher.test\n\nalice_and_jane <- \n  alice_and_jane %>% \n  mutate(blank_line = if_else(text == \"\", 1, 0)) %>% \n  filter(blank_line == 0) %>% \n  select(-blank_line)\n\ntable(alice_and_jane$title)\n#> \n#> Alice's Adventures in Wonderland \n#>                             2481 \n#>      Jane Eyre: An Autobiography \n#>                            16395\nset.seed(853)\n\nalice_and_jane$rows <- c(1:nrow(alice_and_jane))\nsample_from_me <- alice_and_jane %>% filter(title == \"Jane Eyre: An Autobiography\")\nkeep_me <- sample(x = sample_from_me$rows, size = 2481, replace = FALSE)\n\nalice_and_jane <- \n  alice_and_jane %>% \n  filter(title == \"Alice's Adventures in Wonderland\" | rows %in% keep_me) %>% \n  select(-rows)\n\ntable(alice_and_jane$title)\n#> \n#> Alice's Adventures in Wonderland \n#>                             2481 \n#>      Jane Eyre: An Autobiography \n#>                             2481\nalice_and_jane <- \n  alice_and_jane %>% \n  group_by(title) %>% \n  mutate(line_number = paste(gutenberg_id, row_number(), sep = \"_\")) %>% \n  ungroup()\nlibrary(tidytext)\n\nalice_and_jane_by_word <- \n  alice_and_jane %>% \n  unnest_tokens(word, text) %>%\n  group_by(word) %>%\n  filter(n() > 10) %>%\n  ungroup()\nalice_and_jane_by_word <- \n  alice_and_jane_by_word %>% \n  group_by(title, line_number) %>% \n  mutate(number_of_words_in_line = n()) %>% \n  ungroup() %>% \n  filter(number_of_words_in_line > 2) %>% \n  select(-number_of_words_in_line)\nlibrary(tidymodels)\n\nset.seed(853)\n\nalice_and_jane_by_word_split <- \n  alice_and_jane_by_word %>%\n  select(title, line_number) %>% \n  distinct() %>% \n  initial_split(prop = 3/4, strata = title)\nalice_and_jane_dtm_training <- \n  alice_and_jane_by_word %>% \n  count(line_number, word) %>% \n  inner_join(training(alice_and_jane_by_word_split) %>% select(line_number)) %>% \n  cast_dtm(term = word, document = line_number, value = n)\n#> Joining, by = \"line_number\"\n\ndim(alice_and_jane_dtm_training)\n#> [1] 3413  585\nresponse <- \n  data.frame(id = dimnames(alice_and_jane_dtm_training)[[1]]) %>% \n  separate(id, into = c(\"book\", \"line\", sep = \"_\")) %>% \n  mutate(is_alice = if_else(book == 11, 1, 0)) \n#> Warning: Expected 3 pieces. Missing pieces filled with `NA`\n#> in 3413 rows [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,\n#> 15, 16, 17, 18, 19, 20, ...].\n  \npredictor <- alice_and_jane_dtm_training[] %>% as.matrix()\nlibrary(glmnet)\n\nmodel <- cv.glmnet(x = predictor,\n                   y = response$is_alice,\n                   family = \"binomial\",\n                   keep = TRUE\n                   )\n\nsave(model, file = \"alice_vs_jane.rda\")\nlibrary(glmnet)\n#> Loading required package: Matrix\n#> \n#> Attaching package: 'Matrix'\n#> The following objects are masked from 'package:tidyr':\n#> \n#>     expand, pack, unpack\n#> Loaded glmnet 4.1-3\nlibrary(broom)\n\ncoefs <- model$glmnet.fit %>%\n  tidy() %>%\n  filter(lambda == model$lambda.1se)\n\ncoefs %>% head()\n#> # A tibble: 6 × 5\n#>   term         step estimate  lambda dev.ratio\n#>   <chr>       <dbl>    <dbl>   <dbl>     <dbl>\n#> 1 (Intercept)    36 -0.335   0.00597     0.562\n#> 2 in             36 -0.144   0.00597     0.562\n#> 3 she            36  0.390   0.00597     0.562\n#> 4 so             36  0.00249 0.00597     0.562\n#> 5 a              36 -0.117   0.00597     0.562\n#> 6 about          36  0.279   0.00597     0.562\ncoefs %>%\n  group_by(estimate > 0) %>%\n  top_n(10, abs(estimate)) %>%\n  ungroup() %>%\n  ggplot(aes(fct_reorder(term, estimate), estimate, fill = estimate > 0)) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  coord_flip() +\n  theme_minimal() +\n  labs(x = \"Coefficient\",\n       y = \"Word\") +\n  scale_fill_brewer(palette = \"Set1\")"},{"path":"text-as-data.html","id":"topic-models","chapter":"17 Text as data","heading":"17.4 Topic models","text":"Sometimes statement want know . Sometimes easy, always titles statements, even , sometimes titles define topics well-defined consistent way. One way get consistent estimates topics statement use topic models. many variants, one way use latent Dirichlet allocation (LDA) method Blei, Ng, Jordan (2003), implemented R package ‘topicmodels’ Grün Hornik (2011).key assumption behind LDA method statement, ‘document’, made person decides topics like talk document, chooses words, ‘terms’, appropriate topics. topic thought collection terms, document collection topics. topics specified ex ante; outcome method. Terms necessarily unique particular topic, document one topic. provides flexibility approaches strict word count method. goal words found documents group define topics.LDA considers statement result process person first chooses topics want speak . choosing topics, person chooses appropriate words use topics. generally, LDA topic model works considering document generated probability distribution topics. instance, five topics two documents, first document may comprised mostly first topics; document may mostly final topics (Figure 17.1).\nFigure 17.1: Probability distributions topics\nSimilarly, topic considered probability distribution terms. choose terms used document speaker picks terms topic appropriate proportion. instance, ten terms, one topic defined giving weight terms related immigration; topic may give weight terms related economy (Figure 17.2).\nFigure 17.2: Probability distributions terms\nFollowing Blei Lafferty (2009), Blei (2012) Griffiths Steyvers (2004), process document generated formally considered :\\(1, 2, \\dots, k, \\dots, K\\) topics vocabulary consists \\(1, 2, \\dots, V\\) terms. topic, decide terms topic uses randomly drawing distributions terms. distribution terms \\(k\\)th topic \\(\\beta_k\\). Typically topic small number terms Dirichlet distribution hyperparameter \\(0<\\eta<1\\) used: \\(\\beta_k \\sim \\mbox{Dirichlet}(\\eta)\\).[^Dirichletfootnote] Strictly, \\(\\eta\\) actually vector hyperparameters, one \\(K\\), practice tend value.Decide topics document cover randomly drawing distributions \\(K\\) topics \\(1, 2, \\dots, d, \\dots, D\\) documents. topic distributions \\(d\\)th document \\(\\theta_d\\), \\(\\theta_{d,k}\\) topic distribution topic \\(k\\) document \\(d\\). , Dirichlet distribution hyperparameter \\(0<\\alpha<1\\) used usually document cover handful topics: \\(\\theta_d \\sim \\mbox{Dirichlet}(\\alpha)\\). , strictly \\(\\alpha\\) vector length \\(K\\) hyperparameters, practice usually value.\\(1, 2, \\dots, n, \\dots, N\\) terms \\(d\\)th document, choose \\(n\\)th term, \\(w_{d, n}\\):\nRandomly choose topic term \\(n\\), document \\(d\\), \\(z_{d,n}\\), multinomial distribution topics document, \\(z_{d,n} \\sim \\mbox{Multinomial}(\\theta_d)\\).\nRandomly choose term relevant multinomial distribution terms topic, \\(w_{d,n} \\sim \\mbox{Multinomial}(\\beta_{z_{d,n}})\\).\nRandomly choose topic term \\(n\\), document \\(d\\), \\(z_{d,n}\\), multinomial distribution topics document, \\(z_{d,n} \\sim \\mbox{Multinomial}(\\theta_d)\\).Randomly choose term relevant multinomial distribution terms topic, \\(w_{d,n} \\sim \\mbox{Multinomial}(\\beta_{z_{d,n}})\\).Dirichlet distribution variation beta distribution commonly used prior categorical multinomial variables. just two categories, Dirichlet beta distributions . special case symmetric Dirichlet distribution, \\(\\eta=1\\), equivalent uniform distribution. \\(\\eta<1\\), distribution sparse concentrated smaller number values, number decreases \\(\\eta\\) decreases. hyperparameter parameter prior distribution.Given set-, joint distribution variables (Blei (2012), p.6):\n\\[p(\\beta_{1:K}, \\theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}) = \\prod^{K}_{=1}p(\\beta_i) \\prod^{D}_{d=1}p(\\theta_d) \\left(\\prod^N_{n=1}p(z_{d,n}|\\theta_d)p\\left(w_{d,n}|\\beta_{1:K},z_{d,n}\\right) \\right).\\]Based document generation process analysis problem, discussed next section, compute posterior \\(\\beta_{1:K}\\) \\(\\theta_{1:D}\\), given \\(w_{1:D, 1:N}\\). intractable directly, can approximated (Griffiths Steyvers (2004) Blei (2012)).documents created, analyze. term usage document, \\(w_{1:D, 1:N}\\), observed, topics hidden, ‘latent’. know topics document, terms defined topics. , know probability distributions Figures 17.1 17.2. sense trying reverse document generation process – terms like discover topics.earlier process around documents generated assumed observe terms document, can obtain estimates topics (Steyvers Griffiths (2006)). outcomes LDA process probability distributions define topics. term given probability member particular topic, document given probability particular topic. , trying calculate posterior distribution topics given terms observed document (Blei (2012), p.7):\n\\[p(\\beta_{1:K}, \\theta_{1:D}, z_{1:D, 1:N} | w_{1:D, 1:N}) = \\frac{p\\left(\\beta_{1:K}, \\theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}\\right)}{p(w_{1:D, 1:N})}.\\]initial practical step implementing LDA given corpus documents remove ‘stop words’. words common, typically help define topics. general list stop words : “”; “’s”; “able”; “”; “”… also remove punctuation capitalization. documents need transformed document-term-matrix. essentially table column number times term appears document.dataset ready, R package ‘topicmodels’ Grün Hornik (2011) can used implement LDA approximate posterior. using Gibbs sampling variational expectation-maximization algorithm. Following Steyvers Griffiths (2006) Darling (2011), Gibbs sampling process attempts find topic particular term particular document, given topics terms documents. Broadly, first assigning every term every document random topic, specified Dirichlet priors \\(\\alpha = \\frac{50}{K}\\) \\(\\eta = 0.1\\) (Steyvers Griffiths (2006) recommends \\(\\eta = 0.01\\)), \\(\\alpha\\) refers distribution topics \\(\\eta\\) refers distribution terms (Grün Hornik (2011), p.7). selects particular term particular document assigns new topic based conditional distribution topics terms documents taken given (Grün Hornik (2011), p.6):\n\\[p(z_{d, n}=k | w_{1:D, 1:N}, z'_{d, n}) \\propto \\frac{\\lambda'_{n\\rightarrow k}+\\eta}{\\lambda'_{.\\rightarrow k}+V\\eta} \\frac{\\lambda'^{(d)}_{n\\rightarrow k}+\\alpha}{\\lambda'^{(d)}_{-}+K\\alpha} \\]\n\\(z'_{d, n}\\) refers topic assignments; \\(\\lambda'_{n\\rightarrow k}\\) count many times term assigned topic \\(k\\); \\(\\lambda'_{.\\rightarrow k}\\) count many times term assigned topic \\(k\\); \\(\\lambda'^{(d)}_{n\\rightarrow k}\\) count many times term assigned topic \\(k\\) particular document; \\(\\lambda'^{(d)}_{-}\\) count many times term assigned document. \\(z_{d,n}\\) estimated, estimates distribution words topics topics documents can backed .conditional distribution assigns topics depending often term assigned topic previously, common topic document (Steyvers Griffiths (2006)). initial random allocation topics means results early passes corpus document poor, given enough time algorithm converges appropriate estimate.choice number topics, k, affects results, must specified priori. strong reason particular number, can used. Otherwise, one way choose appropriate number use test training set process. Essentially, means running process variety possible values k picking appropriate value performs well.One weakness LDA method considers ‘bag words’ order words matter (Blei (2012)). possible extend model reduce impact bag--words assumption add conditionality word order. Additionally, alternatives Dirichlet distribution can used extend model allow correlation. instance, Hansard topics related army may expected commonly found topics related navy, less commonly topics related banking.","code":""},{"path":"text-as-data.html","id":"exercises-and-tutorial-16","chapter":"17 Text as data","heading":"17.5 Exercises and tutorial","text":"","code":""},{"path":"text-as-data.html","id":"exercises-16","chapter":"17 Text as data","heading":"17.5.1 Exercises","text":"","code":""},{"path":"text-as-data.html","id":"tutorial-16","chapter":"17 Text as data","heading":"17.5.2 Tutorial","text":"","code":""},{"path":"deploying-models.html","id":"deploying-models","chapter":"18 Deploying models","heading":"18 Deploying models","text":"Required readingRead Machine learning going real-time, (Huyen 2020).Read Real-time machine learning: challenges solutions, (Huyen 2022).Watch Democratizing R Plumber APIs, (J. Blair 2019).Key concepts skillsBenefits/costs cloud.Getting started cloud.Starting virtual machines R Studio.Stopping virtual machines.Putting models production requires different set skills building model. need familiarity cloud provider, APIs, course modelling. biggest difficulty, , getting things set-.Key librariesplumber (Schloerke Allen 2021)shiny (Chang et al. 2021)","code":""},{"path":"deploying-models.html","id":"introduction-15","chapter":"18 Deploying models","heading":"18.1 Introduction","text":"done work develop dataset explore model confident can used, may wish enable used widely thank just computer. variety ways , including:using cloud,creating R packages,making shiny applications, andusing plumber create API.general idea need know, allow others come trust, whole workflow. approach point brings. , may like use model broadly. Say able scrape data website, bring order chaos, make charts, appropriately model , write . academic settings enough. many industry settings like use model something. instance, setting website allows model used generate insurance quote given several inputs.chapter, begin moving compute local computer cloud. describe use R packages Shiny sharing models. works well, settings users may like interact model ways focused . One way allow make results available computers, want make APIs. Hence, introduce plumber (Schloerke Allen 2021), way creating APIs.","code":""},{"path":"deploying-models.html","id":"amazon-web-services","chapter":"18 Deploying models","heading":"18.2 Amazon Web Services","text":"apocryphal quote cloud another name someone else’s computer. true various degrees, purposes enough. Learning use someone else’s computer can great number reasons including:Scalability: can quite expensive buy new computer, especially need run something every now , using someone else’s computer, can just rent hours days. allows use amortize cost work actually need committing purchase. also allows us easily increase decrease compute scale suddenly substantial increase demand.Portability: can shift analysis workflow local machine cloud, suggests likely good things terms reproducibility portability. least, code can run locally cloud, big step terms reproducibility.Set--forget: something take , can great worry computer needing run overnight, , say, able watch Netflix computer. Additionally, many cloud options, open-source statistical software, R Python, either already available, relatively easy set-.said, downsides, including:Cost: cloud options cheap, rarely free. provide idea cost, using well-featured AWS instance days, may end dollars. also easy accidentally forget something, generate unexpectedly large bills, especially initially.Public: can easy make mistakes accidentally make everything public.Time: takes time get set-comfortable cloud.use cloud, typically running code ‘virtual machine’ (VM). allocation part larger collection computers designed act like computer specific features. instance, may specify virtual machine , say, 8 GB RAM, 128 storage, 4 CPUs. VM act like computer specifications. cost use cloud options increases based specifications virtual machine.sense, started cloud option, initial recommendation, Chapter 2 using R Studio Cloud, moved local machine Chapter 3. cloud option specifically designed beginners. now introduce\n\ngeneral cloud option: Amazon Web Services (AWS). Often particular business use particular cloud option, Google, AWS, Azure, developing familiarity one make use others easier.Amazon Web Services cloud service Amazon. get started need create AWS Developer account (Figure 18.1).\nFigure 18.1: AWS Developer website\ncreated account, need select region computer access located. , want “Launch virtual machine” EC2 (Figure 18.2).\nFigure 18.2: AWS Developer console\nfirst step choose Amazon Machine Image (AMI). provides details computer using. instance, local computer may MacBook running Monterey. Louis Aslett provides AMIs already set-R Studio much else . can either search AMI region registered , click relevant link Aslett’s website. instance, use AMI set-Canadian central region search ‘ami-0bdd24fd36f07b638’. benefit using AMIs set-specifically R Studio, trade-little outdated, compiled August 2020.next step can choose powerful computer . free tier basic computer, can choose better ones need . point can pretty much just launch instance (Figure 18.3). start using AWS seriously go back select different options, especially around security account. AWS relies key pairs. need create PEM save locally (Figure 18.4). can launch instance.\nFigure 18.3: AWS Developer launch instance\n\nFigure 18.4: AWS Developer establishing key-pair\nminutes, instance running. can use pasting ‘public DNS’ browser. username ‘rstudio’ password instance ID.R Studio running, exciting. first thing probably change default password using instructions instance.need install, say, tidyverse, instead can just call library keep going. AMI comes many packages already installed. can see list packages installed installed.packages(). instance, rstan already installed, set-instance GPUs needed.Perhaps important able start AWS instance able stop (get billed). free tier pretty great, need turn . stop instance, AWS instances page, select , ‘Actions -> Instance State -> Terminate’.","code":""},{"path":"deploying-models.html","id":"plumber-and-model-apis","chapter":"18 Deploying models","heading":"18.3 Plumber and model APIs","text":"general idea behind plumber package (Schloerke Allen 2021) can train model make available via API can call want forecast. pretty great.Just get something working, let us make function returns ‘Hello Toronto’ regardless output. Open new R file, add following, save ‘plumber.R’ (may need install plumber package ’ve done yet).saved, top right editor get button ‘Run API’. Click , API load. ‘Swagger’ application, provides GUI around API. Expand GET method, click ‘Try ’ ‘Execute’. response body, get ‘Toronto’.closely reflect fact API designed computers, can copy/paste ‘request HTML’ browser return ‘Hello Toronto’.","code":"\nlibrary(plumber)\n\n#* @get /print_toronto\nprint_toronto <- function() {\n  result <- \"Hello Toronto\"\n  return(result)\n}"},{"path":"deploying-models.html","id":"local-model","chapter":"18 Deploying models","heading":"18.3.1 Local model","text":"Now, going update API serves model output, given input. going follow Buhr (2017) fairly closely.point, start new R Project. get started, let us simulate data train model . case interested forecasting long baby may sleep overnight, given know long slept afternoon nap.Let us now use tidymodels quickly make dodgy model.point, model. One difference might used saved model ‘.rds’ file. going read .Now model want put file use API access, called ‘plumber.R’. also want file sets API, called ‘server.R’. make R script called ‘server.R’ add following content:‘plumber.R’ add following content:, save ‘plumber.R’ file option ‘Run API’. Click can try API locally way .","code":"\nlibrary(tidyverse)\nset.seed(853)\n\nnumber_of_observations <- 1000\n\nbaby_sleep <- \n  tibble(afternoon_nap_length = rnorm(number_of_observations, 120, 5) %>% abs(),\n         noise = rnorm(number_of_observations, 0, 120),\n         night_sleep_length = afternoon_nap_length * 4 + noise,\n         )\n\nbaby_sleep %>% \n  ggplot(aes(x = afternoon_nap_length, y = night_sleep_length)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Baby's afternoon nap length (minutes)\",\n       y = \"Baby's overnight sleep length (minutes)\") +\n  theme_classic()\nset.seed(853)\nlibrary(tidymodels)\n\nbaby_sleep_split <- rsample::initial_split(baby_sleep, prop = 0.80)\nbaby_sleep_train <- rsample::training(baby_sleep_split)\nbaby_sleep_test <- rsample::testing(baby_sleep_split)\n\nmodel <- \n  parsnip::linear_reg() %>%\n  parsnip::set_engine(engine = \"lm\") %>% \n  parsnip::fit(night_sleep_length ~ afternoon_nap_length, \n               data = baby_sleep_train\n               )\n\nwrite_rds(x = model, file = \"baby_sleep.rds\")\nlibrary(plumber)\n\nserve_model <- plumb(\"plumber.R\")\nserve_model$run(port = 8000)\nlibrary(plumber)\nlibrary(tidyverse)\n\nmodel <- readRDS(\"baby_sleep.rds\")\n\nversion_number <- \"0.0.1\"\n\nvariables <- \n  list(\n    afternoon_nap_length = \"A value in minutes, likely between 0 and 240.\",\n    night_sleep_length = \"A forecast, in minutes, likely between 0 and 1000.\"\n  )\n\n#* @param afternoon_nap_length\n#* @get /survival\npredict_sleep <- function(afternoon_nap_length=0) {\n  afternoon_nap_length = as.integer(afternoon_nap_length)\n  \n  payload <- data.frame(afternoon_nap_length=afternoon_nap_length)\n  \n  prediction <- predict(model, payload)\n\n  result <- list(\n    input = list(payload),\n    response = list(\"estimated_night_sleep\" = prediction),\n    status = 200,\n    model_version = version_number)\n\n  return(result)\n}"},{"path":"deploying-models.html","id":"cloud-model","chapter":"18 Deploying models","heading":"18.3.2 Cloud model","text":"point, got API working machine, really want get working computer API can accessed anyone. going use DigitalOcean. charged service, create account, come $100 credit, enough get started.set-process take time, need . Install two additional packages assist plumberDeploy (Allen 2021) analogsea (Chamberlain, Wickham, Chang 2021).Now need connect local computer DigitalOcean account.Now need authenticate connection, done using SSH public key.want ‘.pub’ file computer. copy public key aspect file, add SSH keys section account security settings. key local computer, can check ., take validate. DigitalOcean calls every computer start ‘droplet’. start three computers, started three droplets. can check droplets running.everything set-properly, print information droplets associated account (point, probably none). first create droplet.get asked SSH passphrase just set-bunch things. going need install whole bunch things onto droplet.finally set-(take 30 minutes ) can deploy API.","code":"\ninstall.packages(\"plumberDeploy\")\nremotes::install_github(\"sckott/analogsea\")\nanalogsea::account()\nanalogsea::key_create()\nssh::ssh_key_info()\nanalogsea::droplets()\nid <- plumberDeploy::do_provision(example = FALSE)\nanalogsea::install_r_package(droplet = id, c(\"plumber\", \n                                             \"remotes\", \n                                             \"here\"))\nanalogsea::debian_apt_get_install(id, \"libssl-dev\", \n                                  \"libsodium-dev\", \n                                  \"libcurl4-openssl-dev\")\nanalogsea::debian_apt_get_install(id, \n                                  \"libxml2-dev\")\n\nanalogsea::install_r_package(id, c(\"config\",\n                                   \"httr\",\n                                   \"urltools\",\n                                   \"plumber\"))\n\nanalogsea::install_r_package(id, c(\"xml2\"))\nanalogsea::install_r_package(id, c(\"tidyverse\"))\n\nanalogsea::install_r_package(id, c(\"tidymodels\"))\nplumberDeploy::do_deploy_api(droplet = id, \n                             path = \"example\", \n                             localPath = getwd(), \n                             port = 8000, \n                             docs = TRUE, \n                             overwrite=TRUE)"},{"path":"deploying-models.html","id":"exercises-and-tutorial-17","chapter":"18 Deploying models","heading":"18.4 Exercises and tutorial","text":"","code":""},{"path":"deploying-models.html","id":"exercises-17","chapter":"18 Deploying models","heading":"18.4.1 Exercises","text":"","code":""},{"path":"deploying-models.html","id":"tutorial-17","chapter":"18 Deploying models","heading":"18.4.2 Tutorial","text":"","code":""},{"path":"efficiency.html","id":"efficiency","chapter":"19 Efficiency","heading":"19 Efficiency","text":"Required materialWatch Code smells feels, (Jenny Bryan 2018).Read Notes data witch: Getting started Apache Arrow, (Navarro 2021).Key concepts skillsRefactoring codeDeveloping appreciation SQLKey librariesarrow (Richardson et al. 2022)tictoc (Izrailev 2014)Key functionsarrow::read_parquet()arrow::write_parquet()tictoc::tic()tictoc::toc()","code":""},{"path":"efficiency.html","id":"introduction-16","chapter":"19 Efficiency","heading":"19.1 Introduction","text":"much book largely concerned just getting something done. necessarily getting done best efficient way. large extent, worried getting something done best efficient way almost always waste time. . Eventually inefficient ways storing data, ugly slow code, insistence using R effect. point need open new approaches ensure efficiency.chapter briefly cover ways efficient data, using SQL, feather parquet. discuss code efficiency, particularly, need measure refactor code. discuss experimental efficiency, particular, multi-armed bandit enables us quickly test different effects. Finally, briefly introduce languages, Python Julia, important role play data science.","code":""},{"path":"efficiency.html","id":"code-efficiency","chapter":"19 Efficiency","heading":"19.2 Code efficiency","text":"large, worrying performance waste time. part better just pushing things cloud, letting run reasonable time, using time worry aspects pipeline. , eventually becomes unfeasible. instance, something takes day, becomes pain need completely switch tasks return . rarely common area obvious performance gains. Instead important develop ability measure refactor code.fast valuable mostly able iterate fast necessarily code runs fast. first thing find speed code running becoming bottle neck shard. throw machines . eventually go back refactor code.refactor code means re-write new code achieves outcome old code, just new code better. can use tic() toc() tictoc (Izrailev 2014) time various aspects code find largest delays .know something slowing code; artificial case Sys.sleep() causing delay 3 seconds.start refactor code, want make sure re-written code achieves outcomes original code. means important tests written. generally want reduce size functions, breaking smaller ones.","code":"\nlibrary(tictoc)\ntic(\"First bit of code\")\nprint(\"Fast code\")\n#> [1] \"Fast code\"\ntoc()\n#> First bit of code: 0.05 sec elapsed\n\ntic(\"Second bit of code\")\nSys.sleep(3)\nprint(\"Slow code\")\n#> [1] \"Slow code\"\ntoc()\n#> Second bit of code: 3.004 sec elapsed"},{"path":"efficiency.html","id":"data-efficiency","chapter":"19 Efficiency","heading":"19.3 Data efficiency","text":"","code":""},{"path":"efficiency.html","id":"sql","chapter":"19 Efficiency","heading":"19.3.1 SQL","text":"may true SQL never good original, SQL popular way working data. Advanced users probably lot alone, even just working knowledge SQL increases number datasets can access. can use SQL within RStudioSQL straightforward variant dplyr verbs used throughout book. used mutate(), filter() left_join() tidyverse means much core commands familiar. means main difficulty getting top order operations SQL can pedantic.SQL (“see-quell” “S.Q.L.”) used relational databases. relational database just collection least one table, table just data organized rows columns. one table database, column links . Using feels bit like HTML/CSS terms halfway markup programming. One fun aspect line spaces mean nothing: include , always end SQL command semicolon;can create empty table three columns type: int, text, int:Add row data:Add column:can view particular aspects data, using SELECT similar way select().See two columns:See columns:See unique rows column (similar R’s distinct):See rows match criteria (similar idea R’s filter):usual operators fine : =, !=, >, <, >=, <=. Just make sure condition evaluates true/false.See rows pretty close criteria:_ wildcard matches character e.g. ‘Cough Menzies’ matched , ‘Gough Menzies’. LIKE case-sensitive: ‘Gough Menzies’ ‘gough menzies’ match .Use % anchor matches pieces:matches anything ending ‘Menzies’, ‘Cough Menzies’, ‘Gough Menzies’, ‘Sir Menzies’ etc, matched . Use surrounding percentages match within, e.g. %Menzies% also match ‘Sir Menzies Jr’ whereas %Menzies .wild: NULL values (!) (True/False/NULL) possible, just True/False, need explicitly matched :wild: ’s underlying ordering build number, date text fields allows use , just numeric! following looks text starts letter M (including M) match ‘Gough Menzies’, ‘Sir Gough Menzies’!look numeric (opposed text) inclusive.Combine conditions (must true returned) (least one must true):can order result:Ascending default, add DESC alternative:Restrict return certain number values adding LIMIT end:(doesn’t work time - certain SQL databases.)can modify data use logic. instance can edit value.Implement /else logic:returns column called ‘Party’ looks name person return party.Delete rows:Add alias column name (just shows output):can use COUNT, SUM, MAX, MIN, AVG ROUND place summarize(). COUNT counts number rows empty column passing column name, using *.Similarly, can pass column SUM, MAX, MIN, AVG.ROUND takes column integer specify many decimal places.SELECT GROUP similar group_by R.can GROUP column number instead name e.g. 1 instead column3 GROUP line 2 instead COUNT(*) interest.aggregates, similar filter R rows earlier. Use GROUP ORDER LIMIT.can combine two tables using JOIN LEFT JOIN.careful specify matching columns using dot notation. Primary key columns uniquely identify rows : 1) never NULL; 2) unique; 3) one column per table. primary key can primary one table foreign another. Unique columns different value every row can many one table.UNION equivalent cbind tables already fairly similar.","code":"CREATE TABLE table_name (\n  column1 INTEGER,\n  column2 TEXT,\n  column3 INTEGER\n);INSERT INTO table_name (column1, column2, column3)\n  VALUES (1234, 'Gough Menzies', 32);ALTER TABLE table_name\n  ADD COLUMN column4 TEXT;SELECT column2\n  FROM table_name;SELECT column1, column2\n  FROM table_name;SELECT *\n  FROM table_name;SELECT DISTINCT column2\n  FROM table_name;SELECT *\n  FROM table_name\n    WHERE column3 > 30;SELECT *\n  FROM table_name\n    WHERE column2 LIKE  '_ough Menzies';SELECT *\n  FROM table_name\n    WHERE column2 LIKE  '%Menzies';SELECT *\n  FROM table_name\n    WHERE column2 IS NOT NULL;SELECT *\n  FROM table_name\n    WHERE column2 BETWEEN 'A' AND 'M';SELECT *\n  FROM table_name\n    WHERE column2 BETWEEN 'A' AND 'M'\n    AND column3 = 32;SELECT *\n  FROM table_name\n    ORDER BY column3;SELECT *\n  FROM table_name\n    ORDER BY column3 DESC;SELECT *\n  FROM table_name\n    ORDER BY column3 DESC\n    LIMIT 1;UPDATE table_name\n  SET column3 = 33\n    WHERE column1 = 1234;SELECT *,\n  CASE\n    WHEN column2 = 'Gough Whitlam' THEN 'Labor'\n    WHEN column2 = 'Robert Menzies' THEN 'Liberal'\n    ELSE 'Who knows'\n  END AS 'Party'\n  FROM table_name;DELETE FROM table_name\n  WHERE column3 IS NULL;SELECT column2 AS 'Names'\n  FROM table_name;SELECT COUNT(*)\n  FROM table_name;SELECT SUM(column1)\n  FROM table_name;SELECT ROUND(column1, 0)\n  FROM table_name;SELECT column3, COUNT(*)\n  FROM table_name\n    GROUP BY column3;SELECT *\n  FROM table1_name\n  JOIN table2_name\n    ON table1_name.colum1 = table2_name.column1;"},{"path":"efficiency.html","id":"parquet","chapter":"19 Efficiency","heading":"19.3.2 Parquet","text":"use CSVs great widely used little overhead, also minimal. can lead issues, especially terms class. various modern alternatives, including arrow (Richardson et al. 2022). use write_csv() read_csv() can use write_parquet() read_parquet(). One advantage retain class R Python. also faster CSV.","code":"\nlibrary(arrow)\nlibrary(tictoc)\nlibrary(tidyverse)\n\nnumber_of_draws <- 1000000\n\nsome_data <- \n  tibble(\n    first = runif(n = number_of_draws),\n    second = sample(x = LETTERS, size = number_of_draws, replace = TRUE)\n  )\n\ntic(\"CSV\")\nwrite_csv(x = some_data,\n          file = \"some_data.csv\")\nread_csv(file = \"some_data.csv\")\n#> # A tibble: 1,000,000 × 2\n#>     first second\n#>     <dbl> <chr> \n#>  1 0.104  E     \n#>  2 0.222  G     \n#>  3 0.728  Z     \n#>  4 0.974  J     \n#>  5 0.155  U     \n#>  6 0.733  O     \n#>  7 0.0172 M     \n#>  8 0.273  F     \n#>  9 0.677  B     \n#> 10 0.939  Q     \n#> # … with 999,990 more rows\ntoc()\n#> CSV: 0.444 sec elapsed\n\ntic(\"parquet\")\nwrite_parquet(x = some_data,\n              sink = \"some_data.parquet\")\nread_parquet(file = \"some_data.parquet\")\n#> # A tibble: 1,000,000 × 2\n#>     first second\n#>     <dbl> <chr> \n#>  1 0.104  E     \n#>  2 0.222  G     \n#>  3 0.728  Z     \n#>  4 0.974  J     \n#>  5 0.155  U     \n#>  6 0.733  O     \n#>  7 0.0172 M     \n#>  8 0.273  F     \n#>  9 0.677  B     \n#> 10 0.939  Q     \n#> # … with 999,990 more rows\ntoc()\n#> parquet: 0.357 sec elapsed"},{"path":"efficiency.html","id":"exercises-and-tutorial-18","chapter":"19 Efficiency","heading":"19.4 Exercises and tutorial","text":"","code":""},{"path":"efficiency.html","id":"exercises-18","chapter":"19 Efficiency","heading":"19.4.1 Exercises","text":"","code":""},{"path":"efficiency.html","id":"tutorial-18","chapter":"19 Efficiency","heading":"19.4.2 Tutorial","text":"","code":""},{"path":"efficiency.html","id":"paper-7","chapter":"19 Efficiency","heading":"19.4.3 Paper","text":"point, Final Paper (Appendix B.7) appropriate.","code":""},{"path":"concludingremarks.html","id":"concludingremarks","chapter":"20 Concluding remarks","heading":"20 Concluding remarks","text":"Required materialWatch Wrong ! 30+ Years Statistical Mistakes, (Gelman 2021).","code":""},{"path":"concludingremarks.html","id":"concluding-remarks","chapter":"20 Concluding remarks","heading":"20.1 Concluding remarks","text":"old saying, something along lines ‘may live interesting times’. Possibly every generation feels way, sure live interesting times. book, covered broad range essential skills allow tell stories data. just getting started.little decade data science gone something barely existed, defining part academia industry. extent pace change many implications learning data science. instance, may imply one just make decisions optimize data science looks like right now, also happen. little difficult, also one things makes data science exciting. might mean choices like:taking courses fundamentals, just fashionable applications;reading books, just whatever trending; andtrying intersection least different areas, rather hyper-specialized.One exciting times learn data science realizing just love playing data. decade ago, fit particular department, days fits almost . Data science needs diversity, terms approaches applications. increasingly important work world hegemonic approaches place. just exciting time enthusiastic data able build.","code":""},{"path":"concludingremarks.html","id":"some-issues","chapter":"20 Concluding remarks","heading":"20.2 Some issues","text":"Data science draws variety disciplines. variety concerns common across . detail .1. write unit tests data science?One thing computer scientists know importance unit tests. Basically just means writing small checks heads time. Like column purports year, ’s unlikely ’s character, ’s unlikely ’s integer larger 2500, ’s unlikely ’s negative integer. know , writing unit tests us write .case ’s obvious unit test looks like. generally, often little idea results look like ’re running well. approach taken add simulation—simulate reasonable results, write unit tests based , bring real data bear adjust necessary. really think need extensive work area current state---art lacking.2. happened machine learning revolution?don’t understand happened promised machine learning revolution social sciences. Specifically, yet see convincing application machine learning methods designed prediction social sciences problem care understanding. like either see evidence definitive thesis can’t happen. current situation untenable folks, especially fields historically female, made feel inferior even though results worse.3. p-values power?someone learnt statistics economists, now partly statistics department, think everyone learn statistics statisticians. isn’t anything economists, conversations statistics department statistical methods used different departments.think problem people outside statistics, treat statistics recipe follow various steps comes cake. regard ‘power’—turns bunch instructions one bothered check—turned oven temperature without checking 180C, ’s fine whatever mess came accepted people evaluating cake didn’t know needed check temperature appropriately set. (ditching analogy right now).know, issue power related broader discussion p-values, basically one taught properly, require changing awful lot teach statistics .e. moving away recipe approach., specific issue people think statistics recipe followed. think ’s trained especially social sciences like political science economics, ’s rewarded. ’s methods . Instead, statistics collection different instruments let us look data certain way. think need revolution , metaphorical tucking one’s shirt.4. teach data science?beginning start agreement foundations data science . involves computational thinking, concern sampling, statistics, graphics, Git/GitHub, SQL, command line, comfort messy data, comfort across languages including R Python. little agreement best teach . Partly data science instructors often come different fields, also also partly difference resources priorities.5. happening data cleaning preparation stage?basically good understanding much matters. Huntington-Klein et al. (2021) showed hidden research decisions big effect subsequent estimates, sometimes greater standard errors. findings invalidate claims. need much investigation early stages data science workflow affect conclusions.","code":""},{"path":"concludingremarks.html","id":"next-steps","chapter":"20 Concluding remarks","heading":"20.3 Next steps","text":"book covered much ground, toward end , butler Stevens told novel Remains Day (Ishiguro 1989):evening’s best part day. ’ve done day’s work. Now can put feet enjoy .Chances aspects want explore , building foundation established. , accomplished set .new data science start book, next step backfill skipped , recommend T.-. Timbers, Campbell, Lee (2022). , learn R terms data science going Wickham Grolemund (2017). deepen understanding R , go next Wickham (2019a).interested learning causality start Cunningham (2021) Huntington-Klein (2021).interested learn statistics begin McElreath (2020), backfill . . Johnson, Ott, Dogucu (2022) solidify foundation Gelman et al. (2014). probably also backfill fundamentals around probability, starting Wasserman (2005).one next natural step interested learning statistical (come called machine) learning ’s James et al. (2017) followed Friedman, Tibshirani, Hastie (2009).interested sampling next book turn Lohr (2019). deepen understanding surveys experiments, go next Gerber Green (2012) combination Kohavi, Tang, Xu (2020).developing better data visualization skills, begin turning Healy (2018), , develop strong foundations, L. Wilkinson (2005). writing, best turn inward. Force write publish everyday month. . get better. said, useful books, including Caro (2019) S. King (2000).often hear phrase let data speak. Hopefully point understand never happens. can acknowledge ones using data tell stories, strive seek make worthy.voice made\nsky acutest vanishing.\nmeasured hour solitude.\nsingle artificer world\nsang. sang, sea,\nWhatever self , became self\nsong, maker.‘Idea Order Key West’, (Stevens 1934)","code":""},{"path":"oh-you-think-and-shoulders-and-datasets.html","id":"oh-you-think-and-shoulders-and-datasets","chapter":"A Oh you think and shoulders and datasets","heading":"A Oh you think and shoulders and datasets","text":"just holding place content developed.","code":""},{"path":"oh-you-think-and-shoulders-and-datasets.html","id":"oh-you-think-we-have-good-data-on-that","chapter":"A Oh you think and shoulders and datasets","heading":"A.1 Oh, you think we have good data on that!","text":"Chapter 8:Oh, think good data ! City boundaries. constitues ‘Atlanta’? Different definitions - metro, X, Y. (also issue countries boundaries changing time)Chapter 10:Oh, think good data ! One representation reality commonplace, chess. chess board (see Figure X - add photo chess board) 8 x 8 board alternating black white squares. squares denonated unique combination letter (-G) number (1-8). piece unique abbreviation, instance pawns X, knights Y. game recorded player noting move. way entire game can recreated. 2021 World Championship contested Magnus Carlsen Ian Nepomniachtchi. Figure X shows score sheet Game 6. variety reasons game particularly noteworthy, one uncharactertic mistakes Carlsen Nepomniachtchi made. instance, Move 32 Carlsen exploit opportunity; Move 36 different move provided Nepomniachtchi promising endgame (CITATION). One reason may players point game little time remaining—decide moves quickly. sense representation provided game sheet. ‘correct’ representation happened game, necessarily happened.Oh, think good data ! Migration.Oh, think good data ! Weather stationsOh, think good data ! Olympics events. decides scoring?. timing?Oh, think good data ! Personality scores. Myers Briggs Big 5 generally.Oh, think good data ! Cause deathOh, think good data ! Timing","code":""},{"path":"oh-you-think-and-shoulders-and-datasets.html","id":"shoulders-of-giants","chapter":"A Oh you think and shoulders and datasets","heading":"A.2 Shoulders of giants","text":"Chapter 1:Shoulders giants Dr Michael Jordan Pehong Chen Distinguished Professor University California, Berkeley. taking PhD Cognitive Science University California, San Diego, 1985, appointed assistant professor MIT, promoted full professor 1997, 1998 moved Berkeley. One area research statistical machine learning. One particularly important paper Blei, Ng, Jordan (2003), enables text grouped together define topics, cover Chapter 17.Chapter 2:Shoulders giants Robert Gentleman Ross IhakaChapter 3:Shoulders giants Hadley WickhamChapter 5:Shoulders giants Xiao-Li MengChapter 8:Shoulders giants Barbara BailarChapter 8:Shoulders giants Leo GoodmanChapter 9:Shoulders giants Donald B. RubinShoulders giants Marcella AlsanChapter 10:Shoulders giants Susan AtheyChapter 12:Shoulders giants Timnit GebruChapter 12:Shoulders giants Katherine WallmanChapter 13Shoulders giants John TukeyChapter 14:Shoulders giants Dr Daniela WittenChapter 14:Shoulders giants Dr Nancy Reid University Professor Statistical Sciences, University Toronto. taking PhD Statistics Stanford University 1979, appointed assistant professor UBC, moved University Toronto 1986, promoted full professor 1988 serving chair 1997 2002. One area research conditional inference higher order asymptotics. One particularly important paper :Chapter 14:Shoulders giants Rob TibshiraniChapter 15:Shoulders giants Evelyn KitagawaChapter 16:Shoulders giants Andrew GelmanElizabeth ScottGertrude Mary CoxSimon KuznetsStella Cunliffe","code":""},{"path":"oh-you-think-and-shoulders-and-datasets.html","id":"possible-datasets","chapter":"A Oh you think and shoulders and datasets","heading":"A.3 Possible datasets","text":"https://som.yale.edu/faculty-research/-centers/international-center-finance/dataAlex cooksonDavid Andrew’s bookTidycensushttps://stat.ethz.ch/R-manual/R-patched/library/datasets/html/00Index.htmlhttps://www.historicalstatistics.org/https://data.cityofberkeley.info/browse?limitTo=datasets&utf8https://data.gov.hk/en-datasets/category/educationhttps://data.rijksmuseum.nl/object-metadata/download/World bank https://data.worldbank.org/ eg development indicatorsSouth sea bubbleOECDAer R package paper?CESrPaspaleyCanlangFred - api?538The Economisthttps://pds.nasa.gov/datasearch/subscription-service/SS-Release.shtmlhttps://github.com/BuzzFeedNews/nics-firearm-background-checksThe markupTom CardosoList APIs: https://bookdown.org/paul/apis_for_social_scientists/","code":""},{"path":"papers.html","id":"papers","chapter":"B Papers","heading":"B Papers","text":"","code":""},{"path":"papers.html","id":"paper-one","chapter":"B Papers","heading":"B.1 Paper One","text":"","code":""},{"path":"papers.html","id":"task","chapter":"B Papers","heading":"B.1.1 Task","text":"Working individually entirely reproducible way, please find dataset interest Open Data Toronto write short paper telling story data.\nCreate well-organized folder appropriate sub-folders, add GitHub. welcome use starter folder.\nFind dataset interest Open Data Toronto.\nPut together R script, ‘scripts/00-simulation.R’, simulates dataset interest. Push GitHub include informative commit message\nWrite R script, ‘scripts/00-download_data.R’ download actual data reproducible way using opendatatoronto (Gelfand 2020). Save data: ‘inputs/data/raw_data.csv’. Push GitHub include informative commit message\n\nPrepare PDF using R Markdown ‘outputs/paper/paper.Rmd’ sections: title, author, date, abstract, introduction, data, references.\ntitle descriptive, informative, specific.\ndate unambiguous format. Add link GitHub repo acknowledgements.\nabstract three four sentences. abstract must tell reader top-level finding. one thing learn world paper?\nintroduction two three paragraphs content. additional final paragraph sets remainder paper.\ndata section thoroughly precisely discuss source data bias brings (ethical, statistical, otherwise). Comprehensively describe summarize data using text, graphs, tables. Graphs must made ggplot2 (Wickham 2016) tables must made knitr (Xie 2021) gt (Iannone, Cheng, Schloerke 2020). Graphs must show actual data, close possible, summary statistics. Graphs tables shoudl cross-referenced text e.g. ‘Table 1 shows…’).\nReferences added using BibTeX. sure reference R R packages use, well dataset. Strong submissions draw related literature also reference .\npaper well-written, draw relevant literature, explain technical concepts. Pitch educated, non-specialist, audience.\nUse appendices supporting, critical, material.\nPush GitHub include informative commit message\n\nCreate well-organized folder appropriate sub-folders, add GitHub. welcome use starter folder.Find dataset interest Open Data Toronto.\nPut together R script, ‘scripts/00-simulation.R’, simulates dataset interest. Push GitHub include informative commit message\nWrite R script, ‘scripts/00-download_data.R’ download actual data reproducible way using opendatatoronto (Gelfand 2020). Save data: ‘inputs/data/raw_data.csv’. Push GitHub include informative commit message\nPut together R script, ‘scripts/00-simulation.R’, simulates dataset interest. Push GitHub include informative commit messageWrite R script, ‘scripts/00-download_data.R’ download actual data reproducible way using opendatatoronto (Gelfand 2020). Save data: ‘inputs/data/raw_data.csv’. Push GitHub include informative commit messagePrepare PDF using R Markdown ‘outputs/paper/paper.Rmd’ sections: title, author, date, abstract, introduction, data, references.\ntitle descriptive, informative, specific.\ndate unambiguous format. Add link GitHub repo acknowledgements.\nabstract three four sentences. abstract must tell reader top-level finding. one thing learn world paper?\nintroduction two three paragraphs content. additional final paragraph sets remainder paper.\ndata section thoroughly precisely discuss source data bias brings (ethical, statistical, otherwise). Comprehensively describe summarize data using text, graphs, tables. Graphs must made ggplot2 (Wickham 2016) tables must made knitr (Xie 2021) gt (Iannone, Cheng, Schloerke 2020). Graphs must show actual data, close possible, summary statistics. Graphs tables shoudl cross-referenced text e.g. ‘Table 1 shows…’).\nReferences added using BibTeX. sure reference R R packages use, well dataset. Strong submissions draw related literature also reference .\npaper well-written, draw relevant literature, explain technical concepts. Pitch educated, non-specialist, audience.\nUse appendices supporting, critical, material.\nPush GitHub include informative commit message\ntitle descriptive, informative, specific.date unambiguous format. Add link GitHub repo acknowledgements.abstract three four sentences. abstract must tell reader top-level finding. one thing learn world paper?introduction two three paragraphs content. additional final paragraph sets remainder paper.data section thoroughly precisely discuss source data bias brings (ethical, statistical, otherwise). Comprehensively describe summarize data using text, graphs, tables. Graphs must made ggplot2 (Wickham 2016) tables must made knitr (Xie 2021) gt (Iannone, Cheng, Schloerke 2020). Graphs must show actual data, close possible, summary statistics. Graphs tables shoudl cross-referenced text e.g. ‘Table 1 shows…’).References added using BibTeX. sure reference R R packages use, well dataset. Strong submissions draw related literature also reference .paper well-written, draw relevant literature, explain technical concepts. Pitch educated, non-specialist, audience.Use appendices supporting, critical, material.Push GitHub include informative commit messageSubmit PDF paper.evidence class assignment.","code":""},{"path":"papers.html","id":"checks","chapter":"B Papers","heading":"B.1.2 Checks","text":"R code raw R output final PDF.Code entirely reproducible, well-documented, commented, readable.paper knit directly PDF .e. use ‘Knit PDF’.\nuse ‘Knit html’ save PDF.\nuse ‘Knit Word’ save PDF\nuse ‘Knit html’ save PDF.use ‘Knit Word’ save PDFGraphs, tables, text clear, comparable quality FiveThirtyEight.date --date unambiguous (e.g. 2/3/2022 ambiguous, 2 March 2022 ).entire workflow entirely reproducible.typos.sign school paper.must link paper’s GitHub repo using footnote.GitHub repo well-organized, contain informative README.paper well-written able understood average reader , say, FiveThirtyEight. means allowed use mathematical notation, must explain plain language. statistical concepts terminology must explained. reader someone university education, necessarily someone understands p-value .","code":""},{"path":"papers.html","id":"faq","chapter":"B Papers","heading":"B.1.3 FAQ","text":"Can use dataset Kaggle instead? , done hard work .use code download dataset, can just manually download ? , entire workflow needs reproducible. Please fix download problem pick different dataset.much write? students submit something two--six-page range, . precise thorough.data apartment blocks/NBA/League Legends ’s ethical bias aspect, ? Please re-read readings better understand bias ethics. really think something, might worth picking different dataset.Can use Python? . already know Python doesn’t hurt learn another language.need cite R, don’t need cite Word? R free statistical programming language academic origins, appropriate acknowledge work others. also important reproducibility.reference style use? major reference style fine (APA, Harvard, Chicago, etc); just pick one used .paper starter folder model section, need put together model? . starter folder designed applicable papers; just delete aspects need.‘graph actual data’ mean? , say 5,000 observations dataset three variables, every variable graph 5,000 points case dots, adds 5,000 case bar charts histograms.","code":""},{"path":"papers.html","id":"rubric","chapter":"B Papers","heading":"B.1.4 Rubric","text":"","code":""},{"path":"papers.html","id":"previous-examples","chapter":"B Papers","heading":"B.1.5 Previous examples","text":"Adam Labas, Alicia Yang, Alyssa Schleifer, Amy Farrow, Ethan Sansom, Hudson Yuen, Jack McKay, Morgaine Westin, Rachael Lam, Roy Chan, Thomas D’Onofrio, William Gerecke.","code":""},{"path":"papers.html","id":"paper-two","chapter":"B Papers","heading":"B.2 Paper Two","text":"","code":""},{"path":"papers.html","id":"task-1","chapter":"B Papers","heading":"B.2.1 Task","text":"Working part team three people, please pick paper interest , code data available, published anytime since 2019, American Economic Association journal. journals : ‘American Economic Review’, ‘AER: Insights’, ‘AEJ: Applied Economics’, ‘AEJ: Economic Policy’, ‘AEJ: Macroeconomics’, ‘AEJ: Microeconomics’, ‘Journal Economic Literature’, ‘Journal Economic Perspectives’, ‘AEA Papers & Proceedings’.Following Guide Accelerating Computational Reproducibility Social Sciences, please complete replication3 least three graphs, tables, combination, paper, using Social Science Reproduction Platform. Note DOI replication.Working entirely reproducible way conduct reproduction based two three aspects paper, write short paper .\nCreate well-organized folder appropriate sub-folders, add GitHub, prepare PDF using R Markdown sections (welcome use starter folder): title, author, date, abstract, introduction, data, results, discussion, references.\naspects focus paper aspects replicated, need . Follow direction paper, make . means ask slightly different question, answer question slightly different way, still use dataset.\nInclude DOI replication paper link GitHub repo underpins paper.\nresults section convey findings.\ndiscussion include three four sub-sections focus interesting point, another sub-section weaknesses paper, another potential next steps .\ndiscussion section, relevant section, please sure discuss ethics bias, reference relevant literature.\npaper well-written, draw relevant literature, explain technical concepts. Pitch educated, non-specialist, audience.\nUse appendices supporting, critical, material.\nCode entirely reproducible, well-documented, readable.\nCreate well-organized folder appropriate sub-folders, add GitHub, prepare PDF using R Markdown sections (welcome use starter folder): title, author, date, abstract, introduction, data, results, discussion, references.aspects focus paper aspects replicated, need . Follow direction paper, make . means ask slightly different question, answer question slightly different way, still use dataset.Include DOI replication paper link GitHub repo underpins paper.results section convey findings.discussion include three four sub-sections focus interesting point, another sub-section weaknesses paper, another potential next steps .discussion section, relevant section, please sure discuss ethics bias, reference relevant literature.paper well-written, draw relevant literature, explain technical concepts. Pitch educated, non-specialist, audience.Use appendices supporting, critical, material.Code entirely reproducible, well-documented, readable.Submit PDF paper.evidence class assignment.","code":""},{"path":"papers.html","id":"checks-1","chapter":"B Papers","heading":"B.2.2 Checks","text":"paper just copy/paste code original paper, instead used foundation work .paper link associated GitHub repository DOI Social Science Reproduction Platform replication conducted.Make sure referenced everything, including R. Strong submissions draw related literature discussion (sections) sure also reference . style references matter, provided consistent.","code":""},{"path":"papers.html","id":"faq-1","chapter":"B Papers","heading":"B.2.3 FAQ","text":"much write? students submit something 10--15-page range, . precise thorough.focus model result? , likely best stay away point, instead focus tables graphs summary explanatory statistics.paper choose language R? replication reproduction code R. need translate code R replication. reproduction work, also R. One common language Stata, Huntington-Klein (2022) might useful ‘Rosetta Stone’ sorts, R, Python, Stata.Can work ? Yes.graphs/tables look identical original? , welcome , , make look better part reproduction. even part replication, identical, just similar enough.","code":""},{"path":"papers.html","id":"rubric-1","chapter":"B Papers","heading":"B.2.4 Rubric","text":"","code":""},{"path":"papers.html","id":"previous-examples-1","chapter":"B Papers","heading":"B.2.5 Previous examples","text":"Alyssa Schleifer, Hudson Yuen, Tamsen Yau, Olaedo Okpareke, Arsh Lakhanpal, Swarnadeep Chattopadhyay, Kimlin Chin.","code":""},{"path":"papers.html","id":"paper-three","chapter":"B Papers","heading":"B.3 Paper Three","text":"","code":""},{"path":"papers.html","id":"task-2","chapter":"B Papers","heading":"B.3.1 Task","text":"Working teams one three people, entirely reproducible way, please obtain data US General Social Survey4. (welcome use different government-run survey, please obtain permission starting.)Obtain data, focus one aspect survey, use tell story.\nCreate well-organized folder appropriate sub-folders, add GitHub, use R Markdown prepare PDF sections (welcome use starter folder): title, author, date, abstract, introduction, data, results, discussion, appendix , least, contain survey, references.\naddition conveying sense dataset interest, data section include, limited :\ndiscussion survey’s methodology, key features, strengths, weaknesses. instance: population, frame, sample; sample recruited; sampling approach taken, trade-offs ; non-response handled.\ndiscussion questionnaire: good bad ?\nbecomes detailed, use appendices supporting essential aspects.\n\nappendix, please put together supplementary survey used augment general social survey paper focuses . purpose supplementary survey gain additional information topic focus paper, beyond gathered general social survey. survey distributed manner general social survey needs stand independently. supplementary survey put together using survey platform. link included appendix. Additionally, copy survey included appendix.\nPlease sure discuss ethics bias, reference relevant literature.\nCode entirely reproducible, well-documented, readable.\nCreate well-organized folder appropriate sub-folders, add GitHub, use R Markdown prepare PDF sections (welcome use starter folder): title, author, date, abstract, introduction, data, results, discussion, appendix , least, contain survey, references.addition conveying sense dataset interest, data section include, limited :\ndiscussion survey’s methodology, key features, strengths, weaknesses. instance: population, frame, sample; sample recruited; sampling approach taken, trade-offs ; non-response handled.\ndiscussion questionnaire: good bad ?\nbecomes detailed, use appendices supporting essential aspects.\ndiscussion survey’s methodology, key features, strengths, weaknesses. instance: population, frame, sample; sample recruited; sampling approach taken, trade-offs ; non-response handled.discussion questionnaire: good bad ?becomes detailed, use appendices supporting essential aspects.appendix, please put together supplementary survey used augment general social survey paper focuses . purpose supplementary survey gain additional information topic focus paper, beyond gathered general social survey. survey distributed manner general social survey needs stand independently. supplementary survey put together using survey platform. link included appendix. Additionally, copy survey included appendix.Please sure discuss ethics bias, reference relevant literature.Code entirely reproducible, well-documented, readable.Submit PDF paper.paper well-written, draw relevant literature, explain technical concepts. Pitch university-educated, non-specialist, audience. Use survey, sampling, statistical terminology, sure explain . paper flow, easy follow understand.evidence class paper.","code":""},{"path":"papers.html","id":"checks-2","chapter":"B Papers","heading":"B.3.2 Checks","text":"appendix contain link supplementary survey details , including questions (case link fails, make paper self-contained).","code":""},{"path":"papers.html","id":"faq-2","chapter":"B Papers","heading":"B.3.3 FAQ","text":"focus ? may focus year, aspect, geography reasonable given focus constraints general social survey interested . Please consider year topics interested together, surveys focus particular topics years.need include raw GSS data repo? general social surveys permission share GSS data. case, add clear details README explaining data obtained.many graphs need? general, need least many graphs variables, need show observations variables. may able combine ; , vice versa, may interested looking different aspects relationships.","code":""},{"path":"papers.html","id":"rubric-2","chapter":"B Papers","heading":"B.3.4 Rubric","text":"","code":""},{"path":"papers.html","id":"previous-examples-2","chapter":"B Papers","heading":"B.3.5 Previous examples","text":"Chyna Hui Marco Chau.","code":""},{"path":"papers.html","id":"paper-four","chapter":"B Papers","heading":"B.4 Paper Four","text":"","code":""},{"path":"papers.html","id":"task-3","chapter":"B Papers","heading":"B.4.1 Task","text":"Working teams one three people, entirely reproducible way, please convert\none full-page table \none DHS Program ‘Final Report’ written English 1980s 1990s available ,\nusable dataset, write short paper telling story data.Create well-organized folder appropriate sub-folders, add GitHub. welcome use starter folder.Create document dataset:\nSave PDF ‘inputs’.\nPut together R script, ‘scripts/00-simulation.R’, simulates dataset intend put together.\nWrite R script, ‘scripts/00-gather_data.R’ either OCR parse PDF, appropriate, save data ‘outputs/data/raw_data.csv’.\nWrite R script, ‘scripts/00-clean_and_prepare_data.R’, draws ‘raw_data.csv’ clean prepare dataset, sure use pointblank put together tests dataset must pass (minimum, every variable test class another content). Save dataset ‘outputs/data/cleaned_data.csv’.\nPut together data sheet dataset (go appendix paper).\nSave PDF ‘inputs’.Put together R script, ‘scripts/00-simulation.R’, simulates dataset intend put together.Write R script, ‘scripts/00-gather_data.R’ either OCR parse PDF, appropriate, save data ‘outputs/data/raw_data.csv’.Write R script, ‘scripts/00-clean_and_prepare_data.R’, draws ‘raw_data.csv’ clean prepare dataset, sure use pointblank put together tests dataset must pass (minimum, every variable test class another content). Save dataset ‘outputs/data/cleaned_data.csv’.Put together data sheet dataset (go appendix paper).Use dataset tell story using R Markdown prepare PDF sections: title, author, date, abstract, introduction, data, results, discussion, appendix , least, contain datasheet dataset, references.\naddition conveying sense dataset interest, data section include details survey’s methodology, key features, strengths, weaknesses.\naddition conveying sense dataset interest, data section include details survey’s methodology, key features, strengths, weaknesses.Submit PDF paper.evidence class paper.","code":""},{"path":"papers.html","id":"checks-3","chapter":"B Papers","heading":"B.4.2 Checks","text":"Code entirely reproducible, well-documented, readable.paper well-written able understood average reader , say, FiveThirtyEight. means allowed use mathematical notation, must explain plain language. statistical concepts terminology must explained. reader someone university education, necessarily someone understands p-value .","code":""},{"path":"papers.html","id":"faq-3","chapter":"B Papers","heading":"B.4.3 FAQ","text":"","code":""},{"path":"papers.html","id":"rubric-3","chapter":"B Papers","heading":"B.4.4 Rubric","text":"","code":""},{"path":"papers.html","id":"paper-five","chapter":"B Papers","heading":"B.5 Paper Five","text":"","code":""},{"path":"papers.html","id":"task-4","chapter":"B Papers","heading":"B.5.1 Task","text":"Paper causal inference.Paper causal inference.must include DAG (probably model section).must include DAG (probably model section).","code":""},{"path":"papers.html","id":"paper-six","chapter":"B Papers","heading":"B.6 Paper Six","text":"","code":""},{"path":"papers.html","id":"task-5","chapter":"B Papers","heading":"B.6.1 Task","text":"Working part team three people, please forecast popular vote 2020 US election using multilevel regression post-stratification. requires individual-level survey data, post-stratification data, model brings together. Given expense collecting data, privilege access , please sure properly cite datasets use.Individual-level survey data:\nBegin simulating data survey key features one using.\nRequest access Democracy Fund + UCLA Nationscape ‘Full Data Set’: https://www.voterstudygroup.org/data/nationscape. take day two. Please start early.\naccess pick survey interest. use “ns20200102.dta” example (number may different).\nlarge file share. push GitHub (use .gitignore file accomplish , instead document get raw data README).\nUse example R code get started preparing dataset, go cleaning preparing based need.\nBegin simulating data survey key features one using.Request access Democracy Fund + UCLA Nationscape ‘Full Data Set’: https://www.voterstudygroup.org/data/nationscape. take day two. Please start early.access pick survey interest. use “ns20200102.dta” example (number may different).large file share. push GitHub (use .gitignore file accomplish , instead document get raw data README).Use example R code get started preparing dataset, go cleaning preparing based need.Post-stratification data:\nBegin simulating dataset like use post-stratification.\nUse American Community Surveys (ACS).\nPlease create account IPUMS: https://usa.ipums.org/usa/index.shtml\nwant 2018 1-year ACS. need select variables. depend want model survey data, options include: REGION, STATEICP, AGE, SEX, MARST, RACE, HISPAN, BPL, CITIZEN, EDUC, LABFORCE, INCTOT. look around see interested , remembering need establish correspondence survey.\nDownload relevant post-stratification data (probably easiest change data format .dta). , can take time. Please start early.\n, large file, push GitHub (use .gitignore file accomplish , instead document get raw data README).\nClean prepare post-stratification dataset. Remember need cell counts sub-populations model.\nBegin simulating dataset like use post-stratification.Use American Community Surveys (ACS).Please create account IPUMS: https://usa.ipums.org/usa/index.shtmlYou want 2018 1-year ACS. need select variables. depend want model survey data, options include: REGION, STATEICP, AGE, SEX, MARST, RACE, HISPAN, BPL, CITIZEN, EDUC, LABFORCE, INCTOT. look around see interested , remembering need establish correspondence survey.Download relevant post-stratification data (probably easiest change data format .dta). , can take time. Please start early., large file, push GitHub (use .gitignore file accomplish , instead document get raw data README).Clean prepare post-stratification dataset. Remember need cell counts sub-populations model.Modelling:\nwant explain vote intention based variety explanatory variables. Construct vote intention variable binary (either ‘supports Trump’ ‘supports Biden’).\ndecision , probably use logistic regression. increasing levels complexity: glm(), lme4::glmer(), rstanarm:: stan_glm, brms::brm().\nThink model fit, diagnostics, similar things need order convince someone model appropriate.\nflexibility model use, (hence cells need create next). general, cells better, may want fewer cells simplicity writing process ensure decent sample cell.\nApply trained model post-stratification dataset make best estimate election result can. specifics depend modelling approach likely involve predict(), add_predicted_draws(), similar. See examples readings. primarily interested distribution forecast overall Presidential popular vote, explanatory variables affect . Strong submissions go beyond .\nwant explain vote intention based variety explanatory variables. Construct vote intention variable binary (either ‘supports Trump’ ‘supports Biden’).decision , probably use logistic regression. increasing levels complexity: glm(), lme4::glmer(), rstanarm:: stan_glm, brms::brm().Think model fit, diagnostics, similar things need order convince someone model appropriate.flexibility model use, (hence cells need create next). general, cells better, may want fewer cells simplicity writing process ensure decent sample cell.Apply trained model post-stratification dataset make best estimate election result can. specifics depend modelling approach likely involve predict(), add_predicted_draws(), similar. See examples readings. primarily interested distribution forecast overall Presidential popular vote, explanatory variables affect . Strong submissions go beyond .Write-:\nCreate well-organized folder appropriate sub-folders, add GitHub, prepare PDF using R Markdown sections (welcome use starter folder): title, author, date, abstract, introduction, data, model, results, discussion, references.\npaper may use appendices supporting, critical, material.\ndiscussion include three four sub-sections focus point find interesting, another sub-section weaknesses paper next steps paper.\npaper well-written, draw relevant literature, explain technical concepts. Pitch educated, non-specialist, audience.\nUse appendices supporting, critical, material.\nCode entirely reproducible, well-documented, readable.\nCreate well-organized folder appropriate sub-folders, add GitHub, prepare PDF using R Markdown sections (welcome use starter folder): title, author, date, abstract, introduction, data, model, results, discussion, references.paper may use appendices supporting, critical, material.discussion include three four sub-sections focus point find interesting, another sub-section weaknesses paper next steps paper.paper well-written, draw relevant literature, explain technical concepts. Pitch educated, non-specialist, audience.Use appendices supporting, critical, material.Code entirely reproducible, well-documented, readable.Submit PDF paper.evidence class assignment.","code":""},{"path":"papers.html","id":"checks-4","chapter":"B Papers","heading":"B.6.2 Checks","text":"expected submission well written able understood average reader say 538. means allowed use mathematical notation, must able explain plain English. Similarly, can (hint: ) use survey, sampling, observational, statistical terminology, need explain . average person doesn’t know p-value confidence interval . need explain plain language first time use . work flow easy follow understand. communicate well, anyone university level able read report relay back methodology, overall results, findings, weaknesses, next steps without confusion.Make sure use GitHub appropriately, meaning push time time (just end) meaningful commit messages.recommended (informally) proofread one another’s work - exchange papers another group?Everyone team receives mark.evidence class assignment.","code":""},{"path":"papers.html","id":"faq-4","chapter":"B Papers","heading":"B.6.3 FAQ","text":"much write? students submit something 10--15-page range, . precise thorough.","code":""},{"path":"papers.html","id":"rubric-4","chapter":"B Papers","heading":"B.6.4 Rubric","text":"","code":""},{"path":"papers.html","id":"previous-examples-3","chapter":"B Papers","heading":"B.6.5 Previous examples","text":"Alen Mitrovski, Xiaoyan Yang, Matthew Wankiewicz","code":""},{"path":"papers.html","id":"final-paper","chapter":"B Papers","heading":"B.7 Final paper","text":"","code":""},{"path":"papers.html","id":"task-6","chapter":"B Papers","heading":"B.7.1 Task","text":"Working individually entirely reproducible way please write paper involves original research tell story data.Options include (pick one):\nDevelop research question interest obtain create relevant dataset. option involves developing research question based interests, background, expertise.\nreproduction, sure use paper foundation rather end--(must different paper Paper Three).\nDevelop research question interest obtain create relevant dataset. option involves developing research question based interests, background, expertise.reproduction, sure use paper foundation rather end--(must different paper Paper Three).Create well-organized folder appropriate sub-folders, add GitHub, prepare PDF using R Markdown sections (welcome use starter folder):\nTitle, date, author, keywords, abstract, introduction, data, model, results, discussion, appendix (optional, supporting, critical, material), reference list.\nmust also include enhancement, either contained linked appendix.\nTitle, date, author, keywords, abstract, introduction, data, model, results, discussion, appendix (optional, supporting, critical, material), reference list.must also include enhancement, either contained linked appendix.","code":""},{"path":"papers.html","id":"peer-review-submission","chapter":"B Papers","heading":"B.7.2 Peer review submission","text":"initial ‘submission’ get comments feedback draft.Submit PDF draft.paper finished point, must include following sections:\nTitle, author, date, keywords, abstract, introduction.\nsections must present paper, filled (e.g. must ‘Data’ heading, need content section).\nTitle, author, date, keywords, abstract, introduction.sections must present paper, filled (e.g. must ‘Data’ heading, need content section).clear, fine later change aspect submit checkpoint.awarded one percentage point just submitting draft meets minimum.extensions possible submission following submission dependent date.","code":""},{"path":"papers.html","id":"conduct-peer-review","chapter":"B Papers","heading":"B.7.3 Conduct peer-review","text":"individual, randomly assigned handful rough drafts provide feedback. three days provide feedback peers.provide feedback one peer receive one percentage point, provide feedback two peers receive two percentage points, provide feedback three () peers receive full three percentage points.feedback must include least five comments (meaningful useful bullet points). must well-written thoughtful.extensions granted submission since following submission dependent date.Please remember providing feedback help colleagues. comments professional kind. challenging receive criticism. Please remember goal help peers advance writing/analysis. feedback inappropriate standard receive 0.","code":""},{"path":"papers.html","id":"checks-5","chapter":"B Papers","heading":"B.7.4 Checks","text":"TBD","code":""},{"path":"papers.html","id":"faq-5","chapter":"B Papers","heading":"B.7.5 FAQ","text":"Can work part team? . important work entirely . really need work show job applications etc.much write? students submit something 10--16-pages main content, additional pages devoted appendices, . precise thorough.submit initial paper order peer-review? Yes.","code":""},{"path":"papers.html","id":"rubric-5","chapter":"B Papers","heading":"B.7.6 Rubric","text":"","code":""},{"path":"papers.html","id":"previous-examples-4","chapter":"B Papers","heading":"B.7.7 Previous examples","text":"Rachael Lam, Amy Farrow, Laura Cline, Hong Shi, Jia Jia Ji.","code":""},{"path":"cocktails.html","id":"cocktails","chapter":"C Cocktails","heading":"C Cocktails","text":"chapters inspired cocktail.Chapter 1Chapter 2\\(2\\) oz Hennessy\\(1\\) oz lemon juice\\(1\\) oz strawberry syrup\nTop lemonadeChapter 3\\(1\\frac{1}{2}\\) oz bourbon\\(\\frac{1}{2}\\) oz Benedictine\\(1\\) oz cherry syrup\\(\\frac{1}{2}\\) oz raspberry syrup\\(1\\) oz lemon juice\nCherry garnishChapter 4\\(1\\) oz cognac\\(\\frac{1}{2}\\) oz Grand Marnier\\(\\frac{1}{2}\\) oz Amaro Nonino\\(\\frac{1}{4}\\) oz lemon juice\\(\\frac{1}{4}\\) oz honey syrup\nCherry garnishChapter 5\\(1\\frac{1}{2}\\) oz vodka\\(\\frac{3}{4}\\) oz lime juice\\(\\frac{1}{2}\\) oz ginger syrup\nOne dash Angostura bitters\nTop lemonadeChapter 6\\(1\\frac{1}{2}\\) oz cherry-infused vodka\\(1\\) oz sweet vermouth\\(1\\) oz raspberry syrup\\(1\\) oz lime juice\nOne dash Angostura bittersChapter 7\\(1\\) oz cherry-infused vodka\\(1\\) oz lemon juice\\(\\frac{1}{2}\\) oz St Germain\\(\\frac{1}{2}\\) oz limoncello\\(\\frac{1}{2}\\) oz strawberry syrup\nOne dash elderflower bittersChapter 8\\(1\\frac{1}{4}\\) oz rye\\(\\frac{1}{2}\\) oz Grand Marnier\\(\\frac{3}{4}\\) oz lemon juice\\(\\frac{3}{4}\\) oz orange juice\\(\\frac{1}{2}\\) oz grenadine\\(\\frac{1}{4}\\) oz raspberry syrupChapter 9\\(1\\frac{1}{4}\\) oz Four Roses bourbon\\(1\\) oz apple cider\\(\\frac{1}{2}\\) oz Grand Marnier\\(\\frac{1}{4}\\) oz ameretto\\(\\frac{1}{2}\\) oz lemon juice\\(\\frac{1}{2}\\) oz ginger syrup\nOne dash Angostura bittersChapter 10January April showers 2+2 always makes 5 (measurement error)Chapter 11You can try best can, best can good enough (models useful)Chapter 12Just cause feel doesn’t mean ’s (p hacking)Chapter 13We accidents waiting happen (model overfit)\\(1\\frac{1}{2}\\) oz bourbon\\(\\frac{1}{2}\\) oz Amaro Averna\\(\\frac{1}{2}\\) oz lemon juice\\(\\frac{1}{2}\\) oz ginger syrupChapter 14Chapter 15Chapter 16\\(1\\frac{1}{2}\\) oz Canadian whiskey\\(\\frac{3}{4}\\) oz Sweet vermouth\\(\\frac{3}{4}\\) oz Benedictine\nOne dash absinthe\nTwo dashes Peychaud’s bitters\nTwo dashes Angostura bittersChapter 17Chapter 18Chapter 19Chapter 20","code":""},{"path":"references-1.html","id":"references-1","chapter":"References","heading":"References","text":"","code":""}]
