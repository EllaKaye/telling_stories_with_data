[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"\nFigure 0.1: Telling stories data\nbook help tell stories data. establishes foundation can build share knowledge aspect world interest based data observe. Telling stories small groups around fire played critical role development humans society (Wiessner 2014). Today stories, based data, can influence millions.book explore, prod, push, manipulate, knead, ultimately, try understand implications data. variety features drive choices book.motto university took PhD naturam primum cognoscere rerum roughly ‘learn first nature things.’ original quote continues temporis aeterni quoniam, roughly ‘eternal time.’ things. focus tools, approaches, workflows enable establish lasting reproducible knowledge.talk data book, typically related humans. Humans centre stories, tell social, cultural, economic stories. particular, throughout book draw attention inequity social phenomena data. data analysis reflects world . Many least well-face double burden regard: disadvantaged, extent difficult measure. Respecting whose data dataset primary concern, thinking systematically dataset.data often specific various contexts disciplines, approaches used understand tend similar. Data also increasingly global resources opportunities available variety sources. Hence, draw examples many disciplines geographies.become knowledge, findings must communicated , understood, trusted people. Scientific economic progress can made building work others. possible can understand . Similarly, create knowledge world, must enable others understand precisely , found, went tasks. , book particularly prescriptive communication reproducibility.Improving quality quantitative work enormous challenge, yet challenge time. Data around us, little enduring knowledge created. book hopes contribute, small way, changing .","code":""},{"path":"index.html","id":"audience-and-assumed-background","chapter":"Preface","heading":"Audience and assumed background","text":"typical person reading book familiarity first-year statistics, instance run regression. targeted particular level, instead providing aspects relevant almost quantitative course. taught book high school, undergraduate, graduate levels. Everyone unique needs, hopefully aspect book speaks .book especially complements Statistical Rethinking (McElreath 2020), R Data Science (Wickham Grolemund 2017), Introduction Statistical Learning (James et al. 2017), Causal Inference: Mixtape (Cunningham 2021), Building Software Together (G. Wilson 2021). instance, book may interested learning Bayesian statistics, data science, statistical learning, causal inference, building software.said, successful students quantitative coding background. Enthusiasm interest taken folks far. , don’t worry much else.","code":""},{"path":"index.html","id":"structure-and-content","chapter":"Preface","heading":"Structure and content","text":"book structured around six parts: ) Foundations, II) Communication, III) Acquisition, IV) Preparation, V) Modelling, VI) Enrichment.Part – Foundations – begins Chapter 1, provides overview trying achieve book read . Chapter 2 provides worked examples. intention can experience full workflow recommended book without worrying much specifics happening. workflow : plan, simulate, acquire, model, communicate. normal follow everything chapter, go , typing executing code . time read one chapter book, recommend one. Chapter 3 goes essential tasks R, statistical programming language used book. reference chapter, may find returning time time. Chapter 4 introduces key tools used workflow advocate. things like using command line, R Markdown, R Projects, Git GitHub, using R practice, developing research questions.Part II – Communication – considers three types communication: written, static, interactive. Chapter 5 details features quantitative writing go writing crisp, technical, paper. Static communication Chapter 6 introduces features like graphs, tables, maps. Interactive communication Chapter 7 covers aspects websites, web applications, maps can manipulated.Part III – Acquisition – focuses three aspects: gathering data, hunting data, farming data. Gathering data Chapter 8 covers things like using Application Programming Interface (APIs), scraping data, getting data PDFs, Optical Character Recognition (OCR). idea data available, necessarily designed datasets, must go get . Hunting data Chapter 9 covers aspects expected us. instance, may need conduct experiment, run /B test, surveys. Finally, farming data Chapter 10 covers datasets explicitly provided us use data, instance censuses government statistics. typically clean, pre-packaged datasets.Part IV – Preparation – covers respectfully transform raw data something can explored shared. Chapter 11 begins detailing principles follow approaching task cleaning preparing data, goes specific steps take checks implement. Chapter 12 focuses methods storing retrieving datasets, including use R packages. Chapter 13 discusses considerations steps take wanting disseminate datasets broadly possible, time respecting whose data based .Part V – Modelling – begins exploratory data analysis Chapter 14. critical process coming understand nature dataset, something typically finds final product. Chapter 15 use statistical models explore data introduced. Chapter 16 first three applications modelling. focuses attempts make causal claims observational data covers approaches difference--differences, regression discontinuity, instrumental variables. Chapter 17 second modelling applications chapters focuses multilevel regression post-stratification use statistical model adjust sample known biases. Chapter 18 third final modelling application focused text--data.Part VI – Enrichment – introduces various next steps improve aspects workflow approaches introduced previous chapters. Chapter 19 goes moving away computer toward using cloud. Chapter 20 discusses deploying models use packages, web applications, APIs. Chapter 21 discusses various alternatives storage data including feather SQL; also covers ways improve performance code. Finally, Chapter 22 offers concluding remarks, details open problems, suggests next steps.","code":""},{"path":"index.html","id":"pedagogy-and-key-features","chapter":"Preface","heading":"Pedagogy and key features","text":"work. actively go material code . King (2000) says ‘[]mateurs sit wait inspiration, rest us just get go work.’ passively read book. role best described Hamming (1996, 2–3):, , coach. run mile ; best can discuss styles criticize . know must run mile athletics course benefit —hence must think carefully hear read book effective changing —must obviously purpose…book structured around dense 12-week course. provides enough material advanced readers challenged, establishing core readers master. Typically courses cover material Chapter 15, pick another couple chapters particular interest.early Chapter 2 workflow—plan, simulate, acquire, model, communicate—allowing tell convincing story data. subsequent chapter add aspects depth workflow allow speak increasing sophistication credibility. workflow expands addresses skills typically sought industry. instance, features : communication, ethics, reproducibility, research question development, data collection, data cleaning, data protection dissemination, exploratory data analysis, statistical modelling, scaling.One defining aspects book ethics inequity concerns integrated throughout, rather clustered one, easily ignorable, chapter. aspects critical, yet can difficult immediately see value, hence tight integration.book also designed enable build portfolio work show potential employer. want industry job, arguably important thing . E. Robinson Nolis (2020, 55) describe portfolio collection projects show can something can help successful job search.DeWitt (2000, 326), character says:[] scholar able look word passage instantly think another passage occurred; … [] text like pack icebergs word snowy peak huge frozen mass cross-references beneath surface.analogous way, book provides text instruction self-contained, also helps develop critical masses knowledge expertise built. chapter positions last word, instead written relation work.chapter following features:list required materials go read chapter. clear, first read material return book. chapter also contains recommended materials particularly interested topic want starting place exploration.summary key concepts skills developed chapter. Technical chapters additionally contain list main packages functions used chapter. combination features acts checklist learning, return completing chapter.section called ‘Oh, think good data !’ focuses particular setting, cause death, often assumed unimpeachable unambiguous data reality tends quite far .section called ‘Shoulders giants,’ focuses created intellectual foundation build.series short exercises complete going required materials, going chapter, test knowledge. completing chapter, go back exercises make sure understand aspect.One two tutorial questions included end chapter encourage actively engage material. consider forming small groups discuss answers questions.Finally, set six papers included appendix. write , conducting original research topic interest . Although open-ended research may new , extent able : develop questions, use quantitative methods explore , communicates findings, measure success book.","code":""},{"path":"index.html","id":"software-information-and-conventions","chapter":"Preface","heading":"Software information and conventions","text":"software use book R (R Core Team 2021). language chosen open source, widely used, general enough cover entire workflow, yet specific enough plenty well-developed features. assume used R , another reason selecting R book community R users. community especially welcoming new-comers lot complementary beginner-friendly material available. R package, DoSSToolkit (R. Alexander et al. 2021), contains learnr modules (Schloerke et al. 2021). may useful newer R especially complementary book.don’t programming language, R great one start . preferred programming language already, wouldn’t hurt pick R well. said, good reason prefer another open-source programming language (instance use Python daily work) may wish stick . However, examples book R.Please download R R Studio onto computer. can download R free : http://cran.utstat.utoronto.ca/, can download R Studio Desktop free : https://rstudio.com/products/rstudio/download/#download.\nPlease also create account R Studio Cloud: https://rstudio.cloud/. allow run R cloud.Packages typewriter text, instance, tidyverse, functions also typewriter text, include brackets, instance dplyr::filter().","code":""},{"path":"index.html","id":"acknowledgments","chapter":"Preface","heading":"Acknowledgments","text":"Many people generously gave code, data, examples, guidance, opportunities, thoughts, time, helped develop book.Thank David Grubbs team CRC Press taking chance providing invaluable support.Thank Michael Chong Sharla Gelfand greatly helping shape approaches advocate. However, much contribute enormous way spirit generosity characterises R community.Thank Kelly Lyons support, guidance, mentorship, friendship. Every day demonstrates academic , broadly, aspire person.Thank Greg Wilson providing structure think teaching, catalyst book, helpful comments drafts. Every day provides example contribute intellectual community.Thank anonymous reviewer Isabella Ghement, thoroughly went early draft book provided detailed feedback improved book.Thank Hareem Naveed helpful feedback encouragement. industry experience invaluable resource grappled questions coverage focus.Thank PhD supervisory panel John Tang, Martine Mariotti, Tim Hatton, Zach Ward gave freedom explore intellectual space interest , support follow interests, guidance ensure resulted something tangible.Thank Elle Côtè enabling book written.book greatly benefited notes teaching materials others freely available online, especially: Chris Bail, Scott Cunningham, Andrew Heiss, Lisa Lendway, Grant McDermott, Nathan Matias, David Mimno, Ed Rubin. Thank folks. changed norm academics making materials freely available online great one one hope free online version book helps contribute .Thank students contributed substantially development book, including: Mahfouz, Faria Khandaker, Keli Chiu, Paul Hodgetts, Thomas William Rosenthal. discussed aspects book , made specific contributions, also changed sharpened way thought almost everything covered . Paul additionally made art book.Thank students identified specific improvements, including: Aaron Miller, Amy Farrow, Cesar Villarreal Guzman, Flavia López, Hong Shi, Laura Cline, Lorena Almaraz De La Garza, Mounica Thanam, Reem Alasadi, Wijdan Tariq, Yang Wu.Finally, thank Monica Alexander. Without written book; even thought possible. Thank inestimable help writing book, providing base builds (remember library showing many times get certain rows R!), giving time needed write, encouragement turned writing book just meant endlessly re-writing perfect day , reading everything book many times, providing perfect hydration form coffee cocktails appropriate, much .can contact : rohan.alexander@utoronto.ca.\nRohan Alexander\nToronto, Canada\n","code":""},{"path":"about-the-author.html","id":"about-the-author","chapter":"About the author","heading":"About the author","text":"Rohan Alexander assistant professor University Toronto Information Statistical Sciences. also assistant director CANSSI Ontario, senior fellow Massey College, faculty affiliate Schwartz Reisman Institute Technology Society, co-lead Data Sciences Institute Thematic Program Reproducibility. holds PhD Economics Australian National University supervised John Tang (chair), Martine Mariotti, Tim Hatton, Zach Ward.interested using statistical models try understand world. particularly get data go models; whose data systematically missing; clean, prepare, tidy data modelled; effects implications models; can reproducibly share totality process. tries develop students skilled using statistical methods across various disciplines, also appreciate limitations, think deeply broader contexts work.enjoys teaching aims help students wide range backgrounds learn use data tell convincing stories. teaches Faculty Information Department Statistical Sciences undergraduate graduate levels. RStudio Certified Tidyverse Trainer.married Monica Alexander two children. probably spends much money books, certainly much time libraries. book recommendations , ’d love hear .","code":""},{"path":"telling-stories-with-data.html","id":"telling-stories-with-data","chapter":"1 Telling Stories with Data","heading":"1 Telling Stories with Data","text":"Required materialRead Counting Countless, (Keyes 2019).Watch Data Science Ethics 6 Minutes, (Register 2020).","code":""},{"path":"telling-stories-with-data.html","id":"on-telling-stories","chapter":"1 Telling Stories with Data","heading":"1.1 On telling stories","text":"Like many parents, children born, one first things wife regularly read stories . carried tradition occurred millennia. Myths, fables, fairy tales can seen heard around us. entertaining enable us learn something world. Hungry Caterpillar (Carle 1969) may seem quite far world dealing data, similarities. trying tell story teaching us something world.using data, try tell convincing story. may exciting predicting elections, banal increasing internet advertising click rates, serious finding cause disease, fun forecasting basketball games. case key elements . English author, E. M. Forster, described aspects common novels : story, people, plot, fantasy, prophecy, pattern, rhythm (Forster 1927). Similarly, tell stories data, common concerns, regardless setting:dataset? generated dataset ?process underpins dataset? Given process, missing dataset poorly measured? datasets generated, , different one ?dataset trying say, can let say ? else say? decide ?hoping others see dataset, can convince ? much work must convince ? extent can share came believe ?affected processes outcomes related dataset? extent represented dataset, part conducting analysis?past, certain elements telling stories data easier. instance, experimental design long robust tradition within agricultural medical sciences, physics, chemistry. Student’s t-distribution identified early 1900s chemist, William Sealy Gosset, working Guinness, beer manufacturer, needed assess quality beer (Boland 1984)! possible randomly sample beer change one aspect time.Many fundamentals statistical methods use today developed settings. , typically possible establish control groups randomize; settings fewer ethical concerns. story told resulting data likely fairly convincing.Unfortunately, little applies days, given diversity settings statistical methods applied. hand, many advantages. instance, well-developed statistical techniques, easier access large datasets, open-source statistical languages R (R Core Team 2021). difficulty conducting traditional experiments means must also turn aspects tell convincing story.","code":""},{"path":"telling-stories-with-data.html","id":"workflow-components","chapter":"1 Telling Stories with Data","heading":"1.2 Workflow components","text":"five core components workflow needed tell stories data:Plan sketch endpoint.Simulate reasonable data consider .Acquire prepare real data.Explore understand dataset.Share found.begin planning sketching endpoint ensures think carefully want go. forces us deeply consider situation, acts keep us focused efficient, helps reduce scope creep. Alice’s Adventures Wonderland (Carroll 1865), Alice asks Cheshire Cat way go. Cheshire Cat replies asking Alice like go. Alice replies mind, long gets somewhere, Cheshire Cat says direction matter always get somewhere ‘walk long enough.’ issue, case, typically afford walk aimlessly long. may endpoint needs change, important deliberate, reasoned, decision. possible given initial target. need spend much time get lot value . Often five minutes paper pen, enough.next step simulate data forces us details. helps cleaning preparing dataset focuses us classes dataset distribution values expect. instance, interested effect age-groups political preferences, may expect age-group column factor, four possible values: ‘18-29,’ ‘30-44,’ ‘45-59,’ ‘60+.’ process simulation provides us clear features real dataset satisfy. use features define tests guide data cleaning preparation. instance, check real dataset age-groups one four values. tests pass, confident age-group column contains values expect.Simulating data also important turn statistical modelling stage. stage, concerned whether model reflects dataset. issue go straight modelling real dataset, know whether problem model. simulate data precisely know underlying data generation process. apply model simulated dataset. get put , know model performing appropriately, can turn real dataset. Without initial application simulated data, difficult confidence model.Simulation often cheap—almost free given modern computing resources statistical programming languages—fast. provides ‘intimate feeling situation’ (Hamming 1996, 239). way proceed start simulation just contains essentials, get working, complicate .Acquiring preparing data interested often-overlooked stage workflow. surprising can one difficult stages requires many decisions made. increasingly subject research. instance, found decisions made stage greatly affect statistical results (Huntington-Klein et al. 2021).stage workflow, common feel little overwhelmed. Typically, data can acquire leave us little scared. may little , case worry going able make statistical machinery work. Alternatively, may problem worried can even begin deal large amount data.Perhaps dragons lives princesses waiting see us act, just , beauty courage. Perhaps everything frightens us , deepest essence, something helpless wants love.Rilke (1929)Developing comfort stage workflow unlocks rest . dataset needed tell convincing story , need iteratively remove everything data need, shape .dataset, want explore understand  certain relationships dataset. use statistical models understand implications data free bias, ‘truth’; tell . Within workflow tell stories data, statistical models tools approaches use explore dataset, way may use graphs tables. something provide us definitive result enable us understand dataset clearly particular way.time get step workflow, large extent, model reflect decisions made earlier stages, especially acquisition cleaning, much reflects type underlying process. Sophisticated modellers know statistical models like bit iceberg surface; build , possible due , majority underneath, case, data. expert whole workflow uses modelling, recognise results obtained additionally due choices whose data matters, decisions measure record data, aspects reflect world , well data available specific workflow.Finally, must share found, high fidelity possible. Talking knowledge , make knowledgeable, includes knowledge ‘past ’ . communicating, need clear decisions made, made , findings, weaknesses approach. aiming uncover something important (otherwise, bother) write everything, first instance, although written communication may supplemented forms communication later. many decisions must make workflow want sure open entire thing—start finish. means much just statistical modelling creation graphs tables, everything. Without , stories based data credibility.world rational meritocracy everything carefully judiciously evaluated. Instead, use shortcuts, hacks, heuristics, based experience. Unclear communication render even best work moot, thoroughly engaged . minimum comes communication, upper limit impressive can . culmination thought-workflow, best, obtains certain sprezzatura, studied carelessness. Achieving mastery, work years.","code":""},{"path":"telling-stories-with-data.html","id":"telling-stories-with-data-1","chapter":"1 Telling Stories with Data","heading":"1.3 Telling stories with data","text":"compelling story based data can likely told around ten--twenty pages. Much less , likely light details. easy write much , often reflection enables succinctness multiple stories separated. best stories typically based research independent learning.possible tell convincing stories even possible conduct traditional experiments. approaches rely ‘big data’—panacea (Meng 2018)—instead better using data available. blend theory application, combined practical skills, sophisticated workflow, appreciation one know, often enough create lasting knowledge.best stories based data tend multi-disciplinary. take whatever field need , almost always draw : statistics, data visualisation, computer science, experimental design, economics, information science (name ). , end--end workflow requires blend skills areas. best way learn skills use real-world data conduct research projects :obtain clean relevant datasets;develop research questions;use statistical techniques explore questions; andcommunicate meaningful way.key elements telling convincing stories data :Communication.Ethics.Reproducibility.Questions.Measurement.Data collection.Data cleaning.Exploratory data analysis.Modelling.Scaling.elements foundation workflow built (Figure 1.1).\nFigure 1.1: workflow builds various elements\nlot master, communication important. Simple analysis, communicated well, valuable complicated analysis communicated poorly. latter understood trusted others. lack clear communication sometimes reflects failure researcher understand going , even . , level analysis match dataset, instruments, task, skillset, trade-required clarity complication, can sensible err side clarity.Clear communication means writing plain language, help tables, graphs, technical terms, way brings audience along . means setting done , well found. minimum hurdle way enables another person independently find found. One challenge immerse data, can difficult remember like first came . audience coming . Learning provide appropriate level nuance detail especially difficult made easier trying write audience’s benefit.Active consideration ethics needed dataset likely concerns humans. means considering things like: dataset, missing, ? extent story perpetuate past? something happen? Even dataset concern humans, story likely put together humans, affect almost everything else. means moral responsibility use data ethically, concern environmental impact, inequity.many definitions ethics, comes telling stories data, minimum means considering full context dataset (D’Ignazio Klein 2020). jurisprudence, textual approach law means literally considering words law printed, purposive approach means laws interpreted within broader context. ethical approach telling stories data means adopting latter approach, considering social, cultural, historical, political forces shape world, hence data (Crawford 2021).Reproducibility required create lasting knowledge world. means everything done—, end--end—can independently redone. Ideally, autonomous end--end reproducibility possible; anyone can get code, data, environment, verify everything done. Unfettered access code almost always possible. default data also, always reasonable. instance, studies psychology may small, personally identifying, samples. One way forward openly share simulated data similar properties, along defining process real data accessed, given appropriate bona fides.Curiosity provides internal motivation explore dataset, associated process, proper extent. Questions tend beget questions, usually improve refine process coming understand dataset carries . contrast stock Popperian approach hypothesis testing often taught, questions typically developed continuous evolving process (Franklin 2005). can difficult find initial question. Selecting area interest can help, can sketching broad claim intent evolving specific question, finally, bringing together two different areas.Developing comfort ease messiness real-world data means getting ask new questions time data update. knowing dataset detail tends surface unexpected groupings values can work subject-area experts understand. Becoming bit ‘mongrel’ developing base knowledge across variety areas especially valuable, becoming comfortable possibility initially asking dumb questions.Measurement data collection deciding world become data. challenging. world vibrant difficult reduce something possible consistently measure collect. Take, instance, someone’s height. can, probably, agree take shoes measure height. height changes course day. measuring someone’s height tape measure give different results using laser. comparing heights people time, therefore becomes important measure time day, using method. quickly becomes infeasible.questions interested use data complicated height. measure sad someone ? measure pain? decides measure measure ? certain arrogance required think can reduce world value compare . Ultimately, must, difficult consistently define measured. process value-free. way reasonably come terms brutal reduction deeply understand, respect measuring collecting. central essence, can stripped away?Pablo Picasso, twentieth century Spanish painter, series drawings depicts outline animal using one line (Figure 1.2). Despite simplicity, recognise animal depicted—drawing sufficient tell animal dog, cat. used determine whether dog sick? Probably . likely want detailed drawing. decision features measured collected, ignore, turns context purpose.\nFigure 1.2: drawing clearly dog, even though just one line\nData cleaning preparation critical part using data. need massage data available us dataset can use. requires making lot decisions. data cleaning preparation stage critical, worthy much attention care .Consider survey collected information gender using four options: ‘man,’ ‘woman,’ ‘prefer say,’ ‘,’ ‘’ dissolved open textbox. come dataset, likely find responses either ‘man’ ‘woman.’ need decide ‘prefer say.’ drop dataset, actively ignoring respondents. drop , makes analysis complicated. Similarly, need decide deal open text responses. , drop responses, ignores experiences respondents. Another option merge ‘prefer say,’ shows disregard respondents, specifically choose option.easy, always-correct, choice many data cleaning preparation situations. depends context purpose. Data cleaning preparation involves making many choices like , vital record every step others can understand done . Data never speak ; dummies ventriloquists cleaned prepared .process coming understand look feel dataset termed exploratory data analysis (EDA). open-ended process. need understand shape dataset can formally model . process EDA iterative one involves producing summary statistics, graphs, tables, sometimes even modelling. process never formally finishes requires variety skills.difficult delineate EDA ends formal statistical modelling begins, especially considering beliefs understanding develop (Hullman Gelman 2021). core, ‘EDA starts data,’ involves immersing (Cook, Reid, Tanaka 2021). EDA something typically explicitly part final story. central role come understand story telling. , critical steps taken EDA recorded shared.Statistical modelling long robust history. knowledge statistics built hundreds years. Statistics series dry theorems proofs instead way exploring world. analogous ‘knowledge foreign languages algebra: may prove use time circumstances’ (Bowley 1901, 4). statistical model recipe blindly followed ---way instead way understanding data (James et al. 2017). Modelling usually required infer statistical patterns data. formally, ‘statistical inference, “learning” called computer science, process using data infer distribution generated data’ (Wasserman 2005, 87).Statistical significance scientific significance, realising cost dominant paradigm. rarely appropriate put data arbitrary pass/fail statistical test. Instead, proper use statistical modelling kind echolocation. listen comes back us model, help learn shape world, recognising one representation world.use statistical programming languages, R, enables us rapidly scale work. refers inputs outputs. basically just easy consider 10 observations 1,000, even 1,000,000. enables us quickly see extent stories apply. also case outputs can consumed easily one person 10, 100. Using Application Programming Interface (API) even possible stories considered many thousands times second.","code":""},{"path":"telling-stories-with-data.html","id":"how-do-our-worlds-become-data","chapter":"1 Telling Stories with Data","heading":"1.4 How do our worlds become data?","text":"famous story Eddington people went fishing sea net. Upon examining size fish caught, decided minimum size fish sea! conclusion arose tool used reality.Hamming (1996, 177)certain extent wasting time. perfect model world—world! complicated. knew perfectly everything affected uncountable factors influence , perfectly forecast coin toss, dice roll, every seemingly random process time. . Instead, must simplify things plausibly measurable, define data. data simplification messy, complex, world generated.different approximations ‘plausibly measurable.’ Hence, datasets always result choices. must decide whether nonetheless reasonable task hand. use statistical models help us think deeply , explore, hopefully come better understand, data.Much statistics focused considering, thoroughly, data . appropriate data predominately agricultural, astronomical, physical sciences. rise data science, mostly value application datasets generated humans, must also actively consider dataset. systematically missing dataset? Whose data fit nicely approach using hence inappropriately simplified? process world becoming data necessarily involves abstraction simplification, need clear points can reasonably simplify, inappropriate, recognising application specific.process world becoming data necessarily involves measurement. Paradoxically, often measurement deeply immersed details less trust data removed . Even seemingly clear tasks, measuring distance, defining boundaries, counting populations, surprisingly difficult practice. Turning world data requires many decisions imposes much error. Among many considerations, need decide measured, accurately , measurement.Oh, think good data ! important example something seemingly simple quickly becomes difficult maternal mortality. refers number women die pregnant, soon termination, cause related pregnancy management (2019). difficult critical turn tragedy death cause-specific data helps mitigate future deaths. countries well-developed civil registration vital statistics (CRVS). collect data every death. many countries CRVS every death recorded. Even death recorded, defining cause death may difficult, especially lack qualified medical personal equipment. Maternal mortality especially difficult typically many causes. CRVS checkbox form specify whether death counted maternal mortality. even developed countries recently adopted . instance, introduced US 2003, even 2015 Alabama, California, West Virginia adopted standard question (MacDorman Declercq 2018).typically use various instruments turn world data. astronomy, development better telescopes, eventually satellites probes, enabled new understanding worlds. Similarly, new instruments turning world data developed day. census generational-defining event, now regular surveys, transactions data available second, almost interactions internet become data kind. development instruments enabled exciting new stories.world imperfectly becomes data. nonetheless use data learn world, need actively seek understand ways imperfect implications imperfections.","code":""},{"path":"telling-stories-with-data.html","id":"what-is-data-science-and-how-should-we-use-it-to-learn-about-the-world","chapter":"1 Telling Stories with Data","heading":"1.5 What is data science and how should we use it to learn about the world","text":"agreed definition data science, lot people tried. instance, Wickham Grolemund (2017) say ‘…exciting discipline allows turn raw data understanding, insight, knowledge.’ Similarly, Leek Peng (2020) say ‘…process formulating quantitative question can answered data, collecting cleaning data, analyzing data, communicating answer question relevant audience.’ Baumer, Kaplan, Horton (2021) say ‘…science extracting meaningful information data.’ Craiu (2019) argues lack certainty data science might matter ‘…can really say makes someone poet scientist?’ goes broadly say data scientist ‘…someone data driven research agenda, adheres aspires using principled implementation statistical methods uses efficient computation skills.’case, alongside specific, technical, definitions, value simple definition, even lose bit specificity. Probability often informally defined ‘counting things’ (McElreath 2020, 10). similar informal sense, data science can defined something like: humans measuring stuff, typically related humans, using sophisticated averaging explain predict.may sound touch cute, Francis Edgeworth, nineteenth century statistician economist, considered statistics science ‘Means presented social phenomena,’ good company (Edgeworth 1885). case, one feature definition treat data terra nullius, nobody’s land. Statisticians tend see data result process can never know, try use data come understand. Many statisticians care deeply data measurement, many cases statistics data kind just appear; belong one. never actually case.Data generated, must gathered, cleaned, prepared, decisions matter. Every dataset sui generis, class , come know one dataset well, just know one dataset, datasets.Much data science focuses ‘science,’ important also focus ‘data.’ another feature cutesy definition data science. lot data scientists generalists, interested broad range problems. Often, thing unites need gather, clean, prepare messy data. often specifics data requires time, updates often, worthy full attention.Jordan (2019) describes medical office given probability, based prenatal screening, child, fetus, syndrome. way background, one can test know sure, test comes risk fetus surviving, initial screening probability matters. Jordan (2019) found probabilities determined based study done decade earlier UK. issue ensuing 10 years, imaging technology improved test expecting high-resolution images subsequent (false) increase syndrome diagnoses images improved. problem science, data.just ‘science’ bit hard, ‘data’ bit well. instance, researchers went back examined one popular text datasets computer science, found around 30 per cent data inappropriately duplicated (Bandy Vincent 2021). entire field—linguistics—specialises types datasets, inappropriate use data one dangers one field hegemonic. strength data science brings together folks variety backgrounds training task learning dataset. constrained done past. means must go way show respect come tradition, nonetheless similarly interested dataset . Data science multi-disciplinary increasingly critical; hence must reflect world. pressing need diversity backgrounds, approaches, disciplines data science.world messy, data. successfully tell stories data need become comfortable fact process difficult. Hannah Fry, British mathematician, describes spending six months rewriting code solved problem (Thornhill 2021). need learn stick . also need countenance failure, developing resilience intrinsic motivation. world data considering possibilities probabilities, learning make trade-offs . almost never anything know certain, perfect analysis.Ultimately, just telling stories data, stories increasingly among important world.","code":""},{"path":"telling-stories-with-data.html","id":"exercises-and-tutorial","chapter":"1 Telling Stories with Data","heading":"1.6 Exercises and tutorial","text":"","code":""},{"path":"telling-stories-with-data.html","id":"exercises","chapter":"1 Telling Stories with Data","heading":"1.6.1 Exercises","text":"According Register (2020) data decisions affect (pick one)?\nReal people.\none.\ntraining set.\ntest set.\nReal people.one.training set.test set.words, data science?According Keyes (2019) perhaps accurate definition data science (pick one)?\ninhumane reduction humanity can counted.\nquantitative analysis large amounts data purpose decision-making.\nData science inter-disciplinary field uses scientific methods, processes, algorithms, systems extract knowledge insights many structural unstructured data.\ninhumane reduction humanity can counted.quantitative analysis large amounts data purpose decision-making.Data science inter-disciplinary field uses scientific methods, processes, algorithms, systems extract knowledge insights many structural unstructured data.Imagine job including ‘race’ explanatory variable improves performance model. types issues consider deciding whether include variable production? variable sexuality?Re-order following steps workflow correct:\nSimulate.\nAcquire.\nShare.\nPlan.\nExplore.\nSimulate.Acquire.Share.Plan.Explore.According Crawford (2021), following forces shape world, hence data (select apply)?\nPolitical.\nHistorical.\nCultural.\nSocial.\nPolitical.Historical.Cultural.Social.required tell convincing stories (select apply)?\nSophisticated workflow.\nPractical skills.\nBig data.\nHumility one’s knowledge.\nTheory application.\nSophisticated workflow.Practical skills.Big data.Humility one’s knowledge.Theory application.ethics key element telling convincing stories?","code":""},{"path":"telling-stories-with-data.html","id":"tutorial","chapter":"1 Telling Stories with Data","heading":"1.6.2 Tutorial","text":"purpose tutorial clarify mind difficulty measurement, even seemingly simple things, hence likelihood measurement issues complicated areas.Please obtain seeds fast-growing plant radishes, mustard greens, arugula. Plant seeds measure much soil used. Water measure water used. day take note changes. generally, measure record much can. Note thoughts difficulty measurement. Eventually seeds sprout, measure big . return use data put together.waiting seeds sprout, one week , please measure length hair daily. Write one--two-page paper found learned difficulty measurement.","code":""},{"path":"drinking-from-a-fire-hose.html","id":"drinking-from-a-fire-hose","chapter":"2 Drinking from a fire hose","heading":"2 Drinking from a fire hose","text":"Required materialRead Data science atomic habit, (Barrett 2021a)Read AI bias really happens, (Hao 2019)Read mundanity excellence: ethnographic report stratification Olympic swimmers, (Chambliss 1989)Key concepts skillsThe statistical programming language R enables us tell interesting stories using data.language like , slow path mastery.way learn start small project break required achieve tiny steps. look people’s code work might deal steps. Copy, paste, modify code achieve step. Don’t worry perfection, just worry achieving step. Complete project move onto next project. Rinse repeat. project ’ll get little better.key just start.Key librariesggplot2 (Wickham 2016)janitor (Firke 2020)opendatatoronto (Gelfand 2020)tidyr (Wickham 2021)tidyverse (Wickham 2017)Key functions<- ‘assign’|> ‘pipe’+ ‘add’c()citation()class()dplyr::arrange()dplyr::filter()dplyr::mutate()dplyr::recode()dplyr::rename()dplyr::select()dplyr::summarise()ggplot2::geom_histogram()ggplot2::geom_point()ggplot2::ggplot()head()janitor::clean_names()library()names()readr::read_csv()readr::write_csv()rep()sample()set.seed()stats::rpois()stats::runif()stringr::str_remove()sum()tail()tidyr::separate()","code":""},{"path":"drinking-from-a-fire-hose.html","id":"hello-world","chapter":"2 Drinking from a fire hose","heading":"2.1 Hello, World!","text":"way start, start. chapter go three complete examples workflow advocated book. means : plan, simulate, acquire, explore, share. new R, code may bit unfamiliar . new statistical modelling, concepts may unfamiliar. worry. soon familiar.way learn tell stories, start telling stories . means try get examples working. sketches , type everything (using R Studio Cloud new R installed computer), execute . important, normal, realise going bad .Whenever ’re learning new tool, long time, ’re going suck… good news typical; ’s something happens everyone, ’s temporary.Hadley Wickham quoted Barrett (2021a).Although guided thoroughly achieve , hopefully seeing power telling stories data , feel empowered stick run difficulties later.get started, go https://rstudio.cloud/ create account. just getting started free version fine now. account log , look something like Figure 2.1.\nFigure 2.1: Opening R Studio Cloud first time\n(’ll ‘Workspace,’ won’t ‘Example Workspace.’) start ‘New Project.’ can give project name clicking ‘Untitled Project’ replacing .","code":""},{"path":"drinking-from-a-fire-hose.html","id":"canadian-elections","chapter":"2 Drinking from a fire hose","heading":"2.2 Canadian elections","text":"Canada parliamentary democracy 338 seats House Commons, lower house, government formed. two major parties—‘Liberal’ ‘Conservative’—three minor parties—‘Bloc Québécois,’ ‘New Democratic,’ ‘Green’—well many smaller parties. example create graph number seats party won 2019 Federal Election.","code":""},{"path":"drinking-from-a-fire-hose.html","id":"plan","chapter":"2 Drinking from a fire hose","heading":"2.2.1 Plan","text":"example, need plan two aspects. first dataset need look like, second final graph look like.basic requirement dataset name seat (typically called ‘riding’ Canada) party person represents . , quick sketch dataset need look something like Figure 2.2.\nFigure 2.2: Quick sketch dataset useful analysing Canadian elections\nalso need make graph interested . Given want display number seats party won, quick sketch probably aiming something like Figure 2.3.\nFigure 2.3: Quick sketch graph number ridings won party\n","code":""},{"path":"drinking-from-a-fire-hose.html","id":"simulate","chapter":"2 Drinking from a fire hose","heading":"2.2.2 Simulate","text":"now simulate data, bring specificity sketches.get started, within R Studio Cloud, make new R Markdown file (File -> New File -> R Markdown). first example, just put everything one R Markdown document.R Markdown document create new R code chunk (Code -> Insert Chunk) add preamble documentation explains:purpose document;author contact details;file written last updated; andpre-requisites file relies .R, lines start ‘#’ comments, means run code, instead designed read humans. line preamble start ‘#.’ Also make clear preamble section surrounding ‘####.’example follows., set workspace. involves installing /loading packages needed, possibly updating . package needs installed computer, needs loaded time used. case going use tidyverse (Wickham 2017), janitor (Firke 2020), tidyr (Wickham 2021). need installed first time used, need loaded.example installing packages follows (excessive comments added clear going ; general, level commenting unnecessary). run code clicking small green arrow associated R code chunk.Now packages installed, need loaded. installation step needs done per computer, code can commented accidentally run. , example become follows.packages contain help file provides information functions. can accessed appending question mark package name running code. instance ?tidyverse.simulate data, need create dataset two columns: ‘Riding’ ‘Party,’ values . case ‘Riding’ reasonable values name one 338 Canadian ridings. case ‘Party’ reasonable values one following six: ‘Liberal,’ ‘Conservative,’ ‘Bloc Québécois,’ ‘New Democratic,’ ‘Green,’ ‘.’example simulated dataset follows.","code":"\n#### Preamble ####\n# Purpose: Read in data from the 2019 Canadian Election and make a graph\n# of the number of ridings each party won.\n# Author: Rohan Alexander\n# Email: rohan.alexander@utoronto.ca\n# Date: 1 January 2022\n# Prerequisites: Need to know where to get Canadian elections data from.\n#### Workspace set-up ####\ninstall.packages(\"tidyverse\") # Only need to do this once per computer\ninstall.packages(\"janitor\") # Only need to do this once per computer\ninstall.packages(\"tidyr\") # Only need to do this once per computer\n#### Workspace set-up ####\n# install.packages(\"tidyverse\") # Only need to do this once per computer\n# install.packages(\"janitor\") # Only need to do this once per computer\n# install.packages(\"tidyr\") # Only need to do this once per computer\n\nlibrary(tidyverse) # A collection of data-related packages\nlibrary(janitor) # Helps clean datasets\nlibrary(tidyr) # Helps make tidy datasets\nsimulated_data <-\n  tibble(\n    # We use the number 1 through to 338 to represent each riding\n    'Riding' = 1:338,\n    # We randomly choose one of six options, with replacement, 338 times\n    'Party' = sample(\n      x = c(\n        'Liberal',\n        'Conservative',\n        'Bloc Québécois',\n        'New Democratic',\n        'Green',\n        'Other'\n        ),\n      size = 338,\n      replace = TRUE\n      )\n    )\n\nsimulated_data\n#> # A tibble: 338 × 2\n#>    Riding Party         \n#>     <int> <chr>         \n#>  1      1 Other         \n#>  2      2 Liberal       \n#>  3      3 Green         \n#>  4      4 New Democratic\n#>  5      5 Other         \n#>  6      6 Bloc Québécois\n#>  7      7 Liberal       \n#>  8      8 Other         \n#>  9      9 Conservative  \n#> 10     10 New Democratic\n#> # … with 328 more rows"},{"path":"drinking-from-a-fire-hose.html","id":"acquire","chapter":"2 Drinking from a fire hose","heading":"2.2.3 Acquire","text":"Now want get actual data. read dataset Elections Canada, non-partisan agency organizes Canadian federal elections. can pass website read_csv() function readr package (Wickham, Hester, Bryan 2021), saves lot time. (need explicitly load readr package part tidyverse.)can take quick look using function head() show first six rows, tail() show last six rows.need clean data can use . particular, trying make similar dataset thought wanted planning stage. fine move away plan, needs deliberate, reasoned, decision. reading dataset saved, first thing adjust names make easier type. using function clean_names() janitor (Firke 2020).Even though names still quite long, reason faster type R Studio auto complete . , begin typing name column use tab auto complete .many columns dataset, primarily interested two: ‘electoral_district_name_nom_de_circonscription,’ ‘elected_candidate_candidat_elu.’ can choose certain columns interest using select() dplyr (Wickham et al. 2020) (loaded part tidyverse). Additionally, pipe operator, |>, pushes output one line first input function next line.names columns still quite long English French . can look names columns names(). can change names using rename() dplyr (Wickham et al. 2020).Looking dataset, ‘elected_candidate’ column particular, can see surname elected candidate, followed comma, followed first name, followed space, followed name party English French, separated slash. can break-column pieces using separate() tidyr (Wickham 2021).Finally want change party names French English match simulated using recode() dplyr (Wickham et al. 2020).data now matches plan (Figure 2.2) pretty well. every electoral district party person won .now nicely cleaned dataset, save , can start cleaned dataset next stage.","code":"\n#### Read in the data ####\n# The '<-' or 'assignment operator' is allocating the output of read_csv() to a\n# object called raw_elections_data. \nraw_elections_data <- \n  read_csv(\n    file =\n      \"https://www.elections.ca/res/rep/off/ovr2019app/51/data_donnees/table_tableau11.csv\",\n    show_col_types = FALSE\n    ) \n\n# We have read the data from the Elections Canada website. We may like to save \n# it just in case something happens and they move it. \nwrite_csv(\n  x = raw_elections_data, \n  file = \"canadian_voting.csv\"\n  )\nhead(raw_elections_data)\n#> # A tibble: 6 × 13\n#>   Province   `Electoral Distri… `Electoral Distr… Population\n#>   <chr>      <chr>                          <dbl>      <dbl>\n#> 1 Newfoundl… Avalon                         10001      86494\n#> 2 Newfoundl… Bonavista--Burin-…             10002      74116\n#> 3 Newfoundl… Coast of Bays--Ce…             10003      77680\n#> 4 Newfoundl… Labrador                       10004      27197\n#> 5 Newfoundl… Long Range Mounta…             10005      86553\n#> 6 Newfoundl… St. John's East/S…             10006      85697\n#> # … with 9 more variables: Electors/Électeurs <dbl>,\n#> #   Polling Stations/Bureaux de scrutin <dbl>,\n#> #   Valid Ballots/Bulletins valides <dbl>,\n#> #   Percentage of Valid Ballots /Pourcentage des bulletins valides <dbl>,\n#> #   Rejected Ballots/Bulletins rejetés <dbl>,\n#> #   Percentage of Rejected Ballots /Pourcentage des bulletins rejetés <dbl>,\n#> #   Total Ballots Cast/Total des bulletins déposés <dbl>, …\ntail(raw_elections_data)\n#> # A tibble: 6 × 13\n#>   Province   `Electoral Distri… `Electoral Distr… Population\n#>   <chr>      <chr>                          <dbl>      <dbl>\n#> 1 British C… Vancouver South/V…             59040     102927\n#> 2 British C… Victoria                       59041     117133\n#> 3 British C… West Vancouver--S…             59042     119113\n#> 4 Yukon      Yukon                          60001      35874\n#> 5 Northwest… Northwest Territo…             61001      41786\n#> 6 Nunavut    Nunavut                        62001      35944\n#> # … with 9 more variables: Electors/Électeurs <dbl>,\n#> #   Polling Stations/Bureaux de scrutin <dbl>,\n#> #   Valid Ballots/Bulletins valides <dbl>,\n#> #   Percentage of Valid Ballots /Pourcentage des bulletins valides <dbl>,\n#> #   Rejected Ballots/Bulletins rejetés <dbl>,\n#> #   Percentage of Rejected Ballots /Pourcentage des bulletins rejetés <dbl>,\n#> #   Total Ballots Cast/Total des bulletins déposés <dbl>, …\n#### Basic cleaning ####\nraw_elections_data <- \n  read_csv(file = \"canadian_voting.csv\",\n           show_col_types = FALSE\n           )\n# Make the names easier to type\ncleaned_elections_data <- \n  clean_names(raw_elections_data)\n\n# Have a look at the first six rows\nhead(cleaned_elections_data)\n#> # A tibble: 6 × 13\n#>   province   electoral_distric… electoral_distri… population\n#>   <chr>      <chr>                          <dbl>      <dbl>\n#> 1 Newfoundl… Avalon                         10001      86494\n#> 2 Newfoundl… Bonavista--Burin-…             10002      74116\n#> 3 Newfoundl… Coast of Bays--Ce…             10003      77680\n#> 4 Newfoundl… Labrador                       10004      27197\n#> 5 Newfoundl… Long Range Mounta…             10005      86553\n#> 6 Newfoundl… St. John's East/S…             10006      85697\n#> # … with 9 more variables: electors_electeurs <dbl>,\n#> #   polling_stations_bureaux_de_scrutin <dbl>,\n#> #   valid_ballots_bulletins_valides <dbl>,\n#> #   percentage_of_valid_ballots_pourcentage_des_bulletins_valides <dbl>,\n#> #   rejected_ballots_bulletins_rejetes <dbl>,\n#> #   percentage_of_rejected_ballots_pourcentage_des_bulletins_rejetes <dbl>,\n#> #   total_ballots_cast_total_des_bulletins_deposes <dbl>, …\ncleaned_elections_data <- \n  cleaned_elections_data |> \n  # Select only certain columns\n  select(electoral_district_name_nom_de_circonscription,\n         elected_candidate_candidat_elu\n         )\n\n# Have a look at the first six rows\nhead(cleaned_elections_data)\n#> # A tibble: 6 × 2\n#>   electoral_district_name_no… elected_candidate_candidat_elu\n#>   <chr>                       <chr>                         \n#> 1 Avalon                      McDonald, Kenneth Liberal/Lib…\n#> 2 Bonavista--Burin--Trinity   Rogers, Churence Liberal/Libé…\n#> 3 Coast of Bays--Central--No… Simms, Scott Liberal/Libéral  \n#> 4 Labrador                    Jones, Yvonne Liberal/Libéral \n#> 5 Long Range Mountains        Hutchings, Gudie Liberal/Libé…\n#> 6 St. John's East/St. John's… Harris, Jack NDP-New Democrat…\nnames(cleaned_elections_data)\n#> [1] \"electoral_district_name_nom_de_circonscription\"\n#> [2] \"elected_candidate_candidat_elu\"\ncleaned_elections_data <- \n  cleaned_elections_data |> \n  rename(\n    electoral_district = electoral_district_name_nom_de_circonscription,\n    elected_candidate = elected_candidate_candidat_elu\n    )\n\nhead(cleaned_elections_data)\n#> # A tibble: 6 × 2\n#>   electoral_district                 elected_candidate      \n#>   <chr>                              <chr>                  \n#> 1 Avalon                             McDonald, Kenneth Libe…\n#> 2 Bonavista--Burin--Trinity          Rogers, Churence Liber…\n#> 3 Coast of Bays--Central--Notre Dame Simms, Scott Liberal/L…\n#> 4 Labrador                           Jones, Yvonne Liberal/…\n#> 5 Long Range Mountains               Hutchings, Gudie Liber…\n#> 6 St. John's East/St. John's-Est     Harris, Jack NDP-New D…\ncleaned_elections_data <- \n  cleaned_elections_data |> \n  # Separate the column into two based on the slash\n  separate(col = elected_candidate,\n           into = c('other', 'party'),\n           sep = '/') |> \n  # Remove the 'other' column\n  select(-other)\n\nhead(cleaned_elections_data)\n#> # A tibble: 6 × 2\n#>   electoral_district                 party                  \n#>   <chr>                              <chr>                  \n#> 1 Avalon                             Libéral                \n#> 2 Bonavista--Burin--Trinity          Libéral                \n#> 3 Coast of Bays--Central--Notre Dame Libéral                \n#> 4 Labrador                           Libéral                \n#> 5 Long Range Mountains               Libéral                \n#> 6 St. John's East/St. John's-Est     NPD-Nouveau Parti démo…\ncleaned_elections_data$party <- \n  recode(cleaned_elections_data$party,\n         'Conservateur' = 'Conservative',\n         'Indépendant(e)' = 'Other',\n         'Libéral' = 'Liberal',\n         'NPD-Nouveau Parti démocratique' = 'New Democratic',\n         'Parti Vert' = 'Green')\n\nhead(cleaned_elections_data)\n#> # A tibble: 6 × 2\n#>   electoral_district                 party         \n#>   <chr>                              <chr>         \n#> 1 Avalon                             Liberal       \n#> 2 Bonavista--Burin--Trinity          Liberal       \n#> 3 Coast of Bays--Central--Notre Dame Liberal       \n#> 4 Labrador                           Liberal       \n#> 5 Long Range Mountains               Liberal       \n#> 6 St. John's East/St. John's-Est     New Democratic\nwrite_csv(\n  x = cleaned_elections_data,\n  file = \"cleaned_elections_data.csv\"\n  )"},{"path":"drinking-from-a-fire-hose.html","id":"explore","chapter":"2 Drinking from a fire hose","heading":"2.2.4 Explore","text":"point like explore dataset created. One way better understand dataset make graph. particular, like build graph planned Figure 2.3.First, need read dataset just created.build graph interested , rely ggplot2 package (Wickham 2016). key aspect package involves building graphs adding layers using ‘+’ operator. particular create histogram use function geom_histogram() ggplot2 (Wickham 2016).accomplishes set . can make look bit nicer modifying default options (Figure 2.4).\nFigure 2.4: Number seats won, political party, 2019 Canadian Federal Election.\n","code":"\n#### Read in the data ####\ncleaned_elections_data <- \n  read_csv(\n    \"cleaned_elections_data.csv\",\n    show_col_types = FALSE\n    )\ncleaned_elections_data |> \n  ggplot(aes(x = party)) + # aes abbreviates aesthetics and enables us \n  # to specify the x axis variable\n  geom_histogram(stat = \"count\")\ncleaned_elections_data |> \n  ggplot(aes(x = party)) +\n  geom_histogram(stat = \"count\") +\n  theme_minimal() + # Make the theme neater\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + # Rotate label\n  labs(x = \"Party\",\n       y = \"Number of seats\") # Make the labels more meaningful"},{"path":"drinking-from-a-fire-hose.html","id":"communicate","chapter":"2 Drinking from a fire hose","heading":"2.2.5 Communicate","text":"point downloaded data, cleaned , made graph. typically need communicate done length. case, can write paragraphs , , found. example follows.Canada parliamentary democracy 338 seats House Commons, house forms government. two major parties—‘Liberal’ ‘Conservative’—three minor parties—‘Bloc Québécois,’ ‘New Democratic,’ ‘Green’—many smaller parties. 2019 Federal Election occurred 21 October, 17 million votes cast.downloaded results, seat-specific basis, Elections Canada website. interested number seats won party. cleaned tidied dataset using statistical programming language R (R Core Team 2021) well packages tidyverse (Wickham et al. 2019a) janitor (Firke 2020). created graph number seats political party won (Figure 2.4).found Liberal Party won 157 seats, followed Conservative Party 121 seats. minor parties won following number seats: Bloc Québécois, 32 seats, New Democratic Party, 24 seats, Green Party, 3 seats. Finally, one independent candidate won seat.distribution seats skewed toward two major parties reflect relatively stable preferences part Canadian voters, possibly inertia due benefits already major party national network funding, reason. better understanding reasons distribution interest future work. dataset consists everyone voted, worth noting systematically excluded voting; much difficult vote others.","code":""},{"path":"drinking-from-a-fire-hose.html","id":"toronto-homelessness","chapter":"2 Drinking from a fire hose","heading":"2.3 Toronto homelessness","text":"Toronto large homeless population. Freezing winters mean critical enough places shelters. example make table shelter usage second half 2021 compares average use month. expectation greater usage colder months, instance, December, compared warmer months, instance, July.","code":""},{"path":"drinking-from-a-fire-hose.html","id":"plan-1","chapter":"2 Drinking from a fire hose","heading":"2.3.1 Plan","text":"dataset interested need date, shelter, number beds occupied night. quick sketch dataset work Figure 2.5.\nFigure 2.5: Quick sketch dataset useful understanding shelter usage Toronto\ninterested creating table average number beds occupied month. table probably look something like Figure 2.6.\nFigure 2.6: Quick sketch table average number beds occupied month\n","code":""},{"path":"drinking-from-a-fire-hose.html","id":"simulate-1","chapter":"2 Drinking from a fire hose","heading":"2.3.2 Simulate","text":"next step simulate data resemble dataset.within R Studio Cloud make new R Markdown file, make new R code chunk add preamble documentation. install /load libraries needed. use tidyverse (Wickham 2017), janitor (Firke 2020), tidyr (Wickham 2021). installed earlier, need installed . example additionally use opendatatoronto (Gelfand 2020), lubridate (Grolemund Wickham 2011), knitr (Xie 2021) need installed.example follows.add bit detail earlier example, libraries contain code people written. common ones see regularly, especially tidyverse. use package, must first install need load . package needs installed per computer, must loaded every time. , packages installed earlier need reinstalled .Given lot people freely gave time make R packages use, important cite . get information needed, can use function citation(). run without arguments, provides citation information R , run argument name package, provides citation information package.Turning simulation, need three columns: ‘date,’ ‘shelter,’ ‘occupancy.’ example build earlier one adding ‘seed.’ enables us always generate random data. integer can used seed. case seed 853. use seed, get random numbers example. use different seed, get different random numbers.simulation first create list dates 2021. repeat list three times. assume data three shelters every day year. simulate number beds occupied night, draw Poisson distribution.","code":"\n#### Preamble ####\n# Purpose: Read in data about 2021 houseless shelter usage and make a table\n# Author: Rohan Alexander\n# Email: rohan.alexander@utoronto.ca\n# Date: 1 January 2022\n# Prerequisites: - \n\n#### Workspace set-up ####\ninstall.packages(\"opendatatoronto\")\ninstall.packages(\"lubridate\")\ninstall.packages(\"knitr\")\n\nlibrary(knitr)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(opendatatoronto)\nlibrary(tidyverse)\nlibrary(tidyr)\ncitation() # Get the citation information for R\n#> \n#> To cite R in publications use:\n#> \n#>   R Core Team (2021). R: A language and environment\n#>   for statistical computing. R Foundation for\n#>   Statistical Computing, Vienna, Austria. URL\n#>   https://www.R-project.org/.\n#> \n#> A BibTeX entry for LaTeX users is\n#> \n#>   @Manual{,\n#>     title = {R: A Language and Environment for Statistical Computing},\n#>     author = {{R Core Team}},\n#>     organization = {R Foundation for Statistical Computing},\n#>     address = {Vienna, Austria},\n#>     year = {2021},\n#>     url = {https://www.R-project.org/},\n#>   }\n#> \n#> We have invested a lot of time and effort in creating\n#> R, please cite it when using it for data analysis.\n#> See also 'citation(\"pkgname\")' for citing R packages.\ncitation('tidyverse') # Get the citation information for a particular package\n#> \n#>   Wickham et al., (2019). Welcome to the tidyverse.\n#>   Journal of Open Source Software, 4(43), 1686,\n#>   https://doi.org/10.21105/joss.01686\n#> \n#> A BibTeX entry for LaTeX users is\n#> \n#>   @Article{,\n#>     title = {Welcome to the {tidyverse}},\n#>     author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},\n#>     year = {2019},\n#>     journal = {Journal of Open Source Software},\n#>     volume = {4},\n#>     number = {43},\n#>     pages = {1686},\n#>     doi = {10.21105/joss.01686},\n#>   }\n#### Simulate data ####\nset.seed(853)   \n\nsimulated_occupancy_data <- \n  tibble(\n    date = rep(as.Date(\"2021-07-01\") + c(0:183), 3), # Based on Dirk\n    # Eddelbuettel: https://stackoverflow.com/a/21502386\n    shelter = c(rep(\"Shelter 1\", 184), \n                rep(\"Shelter 2\", 184),\n                rep(\"Shelter 3\", 184)),\n    number_occupied = \n      rpois(n = 184*3,\n            lambda = 150) # Draw 552 times from the Poisson distribution\n    )\n\nhead(simulated_occupancy_data)\n#> # A tibble: 6 × 3\n#>   date       shelter   number_occupied\n#>   <date>     <chr>               <int>\n#> 1 2021-07-01 Shelter 1             145\n#> 2 2021-07-02 Shelter 1             144\n#> 3 2021-07-03 Shelter 1             136\n#> 4 2021-07-04 Shelter 1             139\n#> 5 2021-07-05 Shelter 1             132\n#> 6 2021-07-06 Shelter 1             127"},{"path":"drinking-from-a-fire-hose.html","id":"acquire-1","chapter":"2 Drinking from a fire hose","heading":"2.3.3 Acquire","text":"use data made available Toronto homeless shelters City Toronto. premise data night 4am count made occupied beds. access data, use R package opendatatoronto (Gelfand 2020).much needs done make similar dataset interested . need change names make easier type, reduce columns relevant, add month column.remains save nicely cleaned dataset.","code":"\n#### Acquire data ####\n# Based on code from: \n# https://open.toronto.ca/dataset/daily-shelter-overnight-service-occupancy-capacity/\ntoronto_shelters <- \n  list_package_resources(\"21c83b32-d5a8-4106-a54f-010dbe49f6f2\") |> \n  filter(row_number()==1) |> \n  get_resource()\n\nwrite_csv(\n  x = toronto_shelters, \n  file = \"toronto_shelters.csv\"\n  )\n\nhead(toronto_shelters)\ntoronto_shelters_clean <- \n  clean_names(toronto_shelters) |> \n  select(occupancy_date, id, occupied_beds) |> \n  mutate(occupancy_date = as_date(occupancy_date),\n         occupancy_month = month(occupancy_date, \n                                 label = TRUE, \n                                 abbr = FALSE)\n         ) |> \n  filter(occupancy_date >= as_date(\"2021-07-01\"))\n\nhead(toronto_shelters_clean)\n#> # A tibble: 6 × 4\n#>   occupancy_date      id occupied_beds occupancy_month\n#>   <date>           <dbl>         <dbl> <ord>          \n#> 1 2021-12-27     7323151            50 December       \n#> 2 2021-12-27     7323152            18 December       \n#> 3 2021-12-27     7323153            28 December       \n#> 4 2021-12-27     7323154            50 December       \n#> 5 2021-12-27     7323155            NA December       \n#> 6 2021-12-27     7323156            NA December\nwrite_csv(\n  x = toronto_shelters_clean, \n  file = \"cleaned_toronto_shelters.csv\"\n  )"},{"path":"drinking-from-a-fire-hose.html","id":"explore-1","chapter":"2 Drinking from a fire hose","heading":"2.3.4 Explore","text":"First, need read dataset just created.dataset daily basis shelter. interested understanding average monthly usage, need create summary statistic basis monthly groups., looks fine, achieves set . can make tweaks defaults make look even better (Table 2.1).Table 2.1: Homeless shelter usage Toronto 2021","code":"\n#### Explore ####\ntoronto_shelters_clean <- \n  read_csv(\n    \"cleaned_toronto_shelters.csv\",\n    show_col_types = FALSE\n    )\n# Code based on that of Florence Vallée-Dubois and Lisa Lendway\ntoronto_shelters_clean |>\n  drop_na(occupied_beds) |> # We only want rows that have data\n  group_by(occupancy_month) |> # We want to know the occupancy by month\n  summarise(number_occupied = mean(occupied_beds)) |> \n  kable()\n# Code based on that of Florence Vallée-Dubois and Lisa Lendway\ntoronto_shelters_clean |>\n  drop_na(occupied_beds) |> # We only want rows that have data\n  group_by(occupancy_month) |> # We want to know the occupancy by month\n  summarise(number_occupied = mean(occupied_beds)) |> \n  kable(caption = \"Homeless shelter usage in Toronto in 2021\", \n        col.names = c(\"Month\", \"Average daily number of occupied beds\"),\n        digits = 1,\n        booktabs = TRUE,\n        linesep = \"\"\n        )"},{"path":"drinking-from-a-fire-hose.html","id":"communicate-1","chapter":"2 Drinking from a fire hose","heading":"2.3.5 Communicate","text":", write brief paragraphs , , found. example follows.Toronto large homeless population. Freezing winters mean critical enough places shelters. interested understand usage shelters changes colder months, compared warmer months.use data provided City Toronto Toronto homeless shelter bed occupancy. Specifically, 4am night count made occupied beds. interested averaging month. cleaned, tidied, analyzed dataset using statistical programming language R (R Core Team 2021) well packages tidyverse (Wickham 2017), janitor (Firke 2020), tidyr (Wickham 2021), opendatatoronto (Gelfand 2020), lubridate (Grolemund Wickham 2011), knitr (Xie 2021). made table average number occupied beds night month (Table 2.1).found daily average number occupied beds higher December 2021 July 2021, 34 occupied beds December, compared 30 July. generally, steady increase daily average number occupied beds July December, slight increase month.dataset basis shelters, results may skewed changes specific especially large especially small shelters. may particular shelters especially attractive colder months, leading substitution, policy implications reflected Table 2.1.Although example paragraphs, reduced form abstract, increased form full report. first paragraph general motivational; second focuses data; third results; fourth general discussion. expanded form sections short report.","code":""},{"path":"drinking-from-a-fire-hose.html","id":"neonatal-mortality","chapter":"2 Drinking from a fire hose","heading":"2.4 Neonatal mortality","text":"Neonatal mortality refers death occurs within first month life, particular, neonatal mortality rate (NMR) number neonatal deaths per 1,000 live births (M. Alexander Alkema 2018). example create graph NMR past fifty years : Argentina, Australia, Canada, China, Kenya.","code":""},{"path":"drinking-from-a-fire-hose.html","id":"plan-2","chapter":"2 Drinking from a fire hose","heading":"2.4.1 Plan","text":"example, need think dataset look like, also graph look like.dataset needs column specifies country, another specifies year. also needs column NMR estimate year country. Roughly, look like Figure 2.7.\nFigure 2.7: Quick sketch potentially useful NMR dataset\ninterested make graph year x-axis NMR y-axis. country line. Roughly similar Figure 2.8.\nFigure 2.8: Quick sketch graph NMR country time\n","code":""},{"path":"drinking-from-a-fire-hose.html","id":"simulate-2","chapter":"2 Drinking from a fire hose","heading":"2.4.2 Simulate","text":"like simulate data aligns plan. case need three columns: country, year, NMR.within R Studio Cloud make new R Markdown file. Add preamble documentation load workspace. use tidyverse (Wickham 2017), janitor (Firke 2020), lubridate (Grolemund Wickham 2011), .example follows.Libraries contain code people written, code can update time time. can see version package using packageVersion(). instance, using version 1.3.1 tidyverse version 2.1.0 janitor.update version package, use update.packages().need run, say, every day, time--time worth updating packages. many packages take care ensure backward compatibility, certain point become reasonable, important aware updating packages can result old code needing updated.Returning simulation, repeat name country 50 times, use numbers 1 50 stand passing years adding 1970 . Finally, draw uniform distribution using runif() simulation NMR value year country.simulation works, time-consuming error-prone decided instead fifty years, interested simulating, say, sixty years. One way make easier replace instances 50 variable. example follows.result , now change fifty sixty years change one place.can confidence simulated dataset relatively straight-forward, wrote code . turn real dataset, difficult sure claims . Even trust data, important able share confidence others. One way forward establish checks data . instance, expect:‘country’ , , one five: ‘Argentina,’ ‘Australia,’ ‘Canada,’ ‘China,’ ‘Kenya.’Conversely, ‘country’ contains five countries.‘year’ smaller 1971 larger 2020 integer.‘nmr’ value somewhere 0 1,000 number.can write series tests based features, expect dataset pass.passed tests, can confidence simulated dataset. importantly, can apply tests real dataset. enables us greater confidence dataset share confidence others.","code":"\n#### Preamble ####\n# Purpose: Obtain and prepare data about neonatal mortality for five countries\n# for the past fifty years and create a graph.\n# Author: Rohan Alexander\n# Email: rohan.alexander@utoronto.ca\n# Date: 1 January 2022\n# Prerequisites: - \n\n#### Workspace set-up ####\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(tidyverse)\npackageVersion('tidyverse')\n#> [1] '1.3.1'\npackageVersion('janitor')\n#> [1] '2.1.0'\nupdate.packages()\n#### Simulate data ####\nset.seed(853)\n\nsimulated_nmr_data <- \n  tibble(\n    country = \n      c(\n        rep('Argentina', 50),\n        rep('Australia', 50),\n        rep('Canada', 50),\n        rep('China', 50),\n        rep('Kenya', 50)\n        ),\n    year = \n      rep(c(1:50 + 1970), 5),\n    nmr = \n      runif(n = 250,\n            min = 0, \n            max = 100)\n  )\n\nhead(simulated_nmr_data)\n#> # A tibble: 6 × 3\n#>   country    year   nmr\n#>   <chr>     <dbl> <dbl>\n#> 1 Argentina  1971 35.9 \n#> 2 Argentina  1972 12.0 \n#> 3 Argentina  1973 48.4 \n#> 4 Argentina  1974 31.6 \n#> 5 Argentina  1975  3.74\n#> 6 Argentina  1976 40.4\n#### Simulate data ####\nset.seed(853)\n\nnumber_of_years <- 50\n\nsimulated_nmr_data <- \n  tibble(\n    country = \n      c(\n        rep('Argentina', number_of_years),\n        rep('Australia', number_of_years),\n        rep('Canada', number_of_years),\n        rep('China', number_of_years),\n        rep('Kenya', number_of_years)\n        ),\n    year = \n      rep(c(1:number_of_years + 1970), 5),\n    nmr = \n      runif(n = number_of_years * 5,\n            min = 0, \n            max = 100)\n  )\n\nhead(simulated_nmr_data)\n#> # A tibble: 6 × 3\n#>   country    year   nmr\n#>   <chr>     <dbl> <dbl>\n#> 1 Argentina  1971 35.9 \n#> 2 Argentina  1972 12.0 \n#> 3 Argentina  1973 48.4 \n#> 4 Argentina  1974 31.6 \n#> 5 Argentina  1975  3.74\n#> 6 Argentina  1976 40.4\n# Tests for simulated data\nsimulated_nmr_data$country |> unique() == c(\"Argentina\", \"Australia\", \"Canada\", \"China\", \"Kenya\")\n#> [1] TRUE TRUE TRUE TRUE TRUE\n\nsimulated_nmr_data$country |> unique() |> length() == 5\n#> [1] TRUE\n\nsimulated_nmr_data$year |> min() == 1971\n#> [1] TRUE\n\nsimulated_nmr_data$year |> max() == 2020\n#> [1] TRUE\n\nsimulated_nmr_data$nmr |> min() >= 0\n#> [1] TRUE\n\nsimulated_nmr_data$nmr |> max() <= 1000\n#> [1] TRUE\n\nsimulated_nmr_data$nmr |> class() == \"numeric\"\n#> [1] TRUE"},{"path":"drinking-from-a-fire-hose.html","id":"acquire-2","chapter":"2 Drinking from a fire hose","heading":"2.4.3 Acquire","text":"UN Inter-agency Group Child Mortality Estimation (IGME) provides estimates NMR website: https://childmortality.org/.can take quick look , especially interested dataset seems look like (using head() tail()), names columns (using names()), type column (using class()).like clean names keep rows columns interested . Based planning, interested rows ‘Sex’ ‘Total,’ ‘Series Name’ ‘UN IGME estimate,’ ‘Geographic area’ one ‘Argentina,’ ‘Australia,’ ‘Canada,’ ‘China,’ ‘Kenya,’ ‘Indicator’ ‘Neonatal mortality rate.’ interested just columns: ‘geographic_area,’ ‘time_period,’ ‘obs_value.’looking good, just need fix two final things: class ‘time_period’ character need date (actually year) name ‘obs_value’ ‘nmr’ informative.Finally, can check dataset passes tests developed based simulated dataset.remains save nicely cleaned dataset.","code":"\n#### Acquire data ####\nraw_igme_data <- \n  read_csv(\n    file =\n      \"https://childmortality.org/wp-content/uploads/2021/09/UNIGME-2021.csv\",\n    show_col_types = FALSE) \n\nwrite_csv(\n  raw_igme_data, \n  here(\"igme.csv\")\n  )\nhead(raw_igme_data)\n#> # A tibble: 6 × 29\n#>   `Geographic area` Indicator         Sex   `Wealth Quintil…\n#>   <chr>             <chr>             <chr> <chr>           \n#> 1 Afghanistan       Neonatal mortali… Total Total           \n#> 2 Afghanistan       Neonatal mortali… Total Total           \n#> 3 Afghanistan       Neonatal mortali… Total Total           \n#> 4 Afghanistan       Neonatal mortali… Total Total           \n#> 5 Afghanistan       Neonatal mortali… Total Total           \n#> 6 Afghanistan       Neonatal mortali… Total Total           \n#> # … with 25 more variables: Series Name <chr>,\n#> #   Series Year <chr>, Regional group <chr>,\n#> #   TIME_PERIOD <chr>, OBS_VALUE <dbl>,\n#> #   COUNTRY_NOTES <chr>, CONNECTION <lgl>,\n#> #   DEATH_CATEGORY <lgl>, CATEGORY <chr>,\n#> #   Observation Status <chr>, Unit of measure <chr>,\n#> #   Series Category <chr>, Series Type <chr>, …\nnames(raw_igme_data)\n#>  [1] \"Geographic area\"        \"Indicator\"             \n#>  [3] \"Sex\"                    \"Wealth Quintile\"       \n#>  [5] \"Series Name\"            \"Series Year\"           \n#>  [7] \"Regional group\"         \"TIME_PERIOD\"           \n#>  [9] \"OBS_VALUE\"              \"COUNTRY_NOTES\"         \n#> [11] \"CONNECTION\"             \"DEATH_CATEGORY\"        \n#> [13] \"CATEGORY\"               \"Observation Status\"    \n#> [15] \"Unit of measure\"        \"Series Category\"       \n#> [17] \"Series Type\"            \"STD_ERR\"               \n#> [19] \"REF_DATE\"               \"Age Group of Women\"    \n#> [21] \"Time Since First Birth\" \"DEFINITION\"            \n#> [23] \"INTERVAL\"               \"Series Method\"         \n#> [25] \"LOWER_BOUND\"            \"UPPER_BOUND\"           \n#> [27] \"STATUS\"                 \"YEAR_TO_ACHIEVE\"       \n#> [29] \"Model Used\"\nsapply(raw_igme_data, class)\n#>        Geographic area              Indicator \n#>            \"character\"            \"character\" \n#>                    Sex        Wealth Quintile \n#>            \"character\"            \"character\" \n#>            Series Name            Series Year \n#>            \"character\"            \"character\" \n#>         Regional group            TIME_PERIOD \n#>            \"character\"            \"character\" \n#>              OBS_VALUE          COUNTRY_NOTES \n#>              \"numeric\"            \"character\" \n#>             CONNECTION         DEATH_CATEGORY \n#>              \"logical\"              \"logical\" \n#>               CATEGORY     Observation Status \n#>            \"character\"            \"character\" \n#>        Unit of measure        Series Category \n#>            \"character\"            \"character\" \n#>            Series Type                STD_ERR \n#>            \"character\"              \"numeric\" \n#>               REF_DATE     Age Group of Women \n#>              \"numeric\"            \"character\" \n#> Time Since First Birth             DEFINITION \n#>            \"character\"            \"character\" \n#>               INTERVAL          Series Method \n#>              \"numeric\"            \"character\" \n#>            LOWER_BOUND            UPPER_BOUND \n#>              \"numeric\"              \"numeric\" \n#>                 STATUS        YEAR_TO_ACHIEVE \n#>            \"character\"            \"character\" \n#>             Model Used \n#>            \"character\"\ncleaned_igme_data <- \n  clean_names(raw_igme_data) |> \n  filter(sex == 'Total',\n         series_name == 'UN IGME estimate',\n         geographic_area %in% \n           c('Argentina', 'Australia', 'Canada', 'China', 'Kenya'),\n         indicator == 'Neonatal mortality rate') |> \n  select(geographic_area,\n         time_period,\n         obs_value)\n\nhead(cleaned_igme_data)\n#> # A tibble: 6 × 3\n#>   geographic_area time_period obs_value\n#>   <chr>           <chr>           <dbl>\n#> 1 Argentina       1970-06          24.9\n#> 2 Argentina       1971-06          24.7\n#> 3 Argentina       1972-06          24.6\n#> 4 Argentina       1973-06          24.6\n#> 5 Argentina       1974-06          24.5\n#> 6 Argentina       1975-06          24.1\ncleaned_igme_data <- \n  cleaned_igme_data |> \n  mutate(time_period = str_remove(time_period, \"-06\"),\n         time_period = as.integer(time_period)) |> \n  filter(time_period >= 1971) |> \n  rename(nmr = obs_value,\n         year = time_period,\n         country = geographic_area)\n\nhead(cleaned_igme_data)\n#> # A tibble: 6 × 3\n#>   country    year   nmr\n#>   <chr>     <int> <dbl>\n#> 1 Argentina  1971  24.7\n#> 2 Argentina  1972  24.6\n#> 3 Argentina  1973  24.6\n#> 4 Argentina  1974  24.5\n#> 5 Argentina  1975  24.1\n#> 6 Argentina  1976  23.3\n# Test the cleaned dataset\ncleaned_igme_data$country |> unique() == c(\"Argentina\", \"Australia\", \"Canada\", \"China\", \"Kenya\")\n#> [1] TRUE TRUE TRUE TRUE TRUE\n\ncleaned_igme_data$country |> unique() |> length() == 5\n#> [1] TRUE\n\ncleaned_igme_data$year |> min() == 1971\n#> [1] TRUE\n\ncleaned_igme_data$year |> max() == 2020\n#> [1] TRUE\n\ncleaned_igme_data$nmr |> min() >= 0\n#> [1] TRUE\n\ncleaned_igme_data$nmr |> max() <= 1000\n#> [1] TRUE\n\ncleaned_igme_data$nmr |> class() == \"numeric\"\n#> [1] TRUE\nwrite_csv(\n  x = cleaned_igme_data, \n  file = \"cleaned_igme_data.csv\"\n  )"},{"path":"drinking-from-a-fire-hose.html","id":"explore-2","chapter":"2 Drinking from a fire hose","heading":"2.4.4 Explore","text":"First, need read dataset just created.can now make graph interested (Figure 2.9). interested showing NMR changed time difference countries.\nFigure 2.9: Neonatal Mortality Rate (NMR), Argentina, Australia, Canada, China, Kenya, (1971-2020)\n","code":"\n#### Explore ####\ncleaned_igme_data <- \n  read_csv(\n    \"cleaned_igme_data.csv\",\n    show_col_types = FALSE\n    )\ncleaned_igme_data |> \n  ggplot(aes(x = year, y = nmr, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Year\",\n       y = \"Neonatal Mortality Rate (NMR)\",\n       color = \"Country\") +\n  scale_color_brewer(palette = \"Set1\")"},{"path":"drinking-from-a-fire-hose.html","id":"communicate-2","chapter":"2 Drinking from a fire hose","heading":"2.4.5 Communicate","text":"point downloaded data, cleaned , wrote tests, made graph. typically need communicate done length. case, write paragraphs , , found.Neonatal mortality refers death occurs within first month life. particular, neonatal mortality rate (NMR) number neonatal deaths per 1,000 live births (M. Alexander Alkema 2018). obtain estimates NMR five countries—Argentina, Australia, Canada, China, Kenya—past fifty years. find substantial reductions time NMR across countries.UN Inter-agency Group Child Mortality Estimation (IGME) provides estimates NMR website: https://childmortality.org/. downloaded estimates cleaned tidied dataset using statistical programming language R (R Core Team 2021).found considerable change estimated NMR time five countries interest (Figure 2.9). particular, found 1970s tended associated reductions estimated NMR. Australia Canada estimated low NMR point remained 2020, slight improvements. estimates Argentina Kenya continued substantial reductions 2020. Data available 1990 China estimates show substantial reduction NMR, especially 1990s 2000s.results suggest considerable improvements estimated NMR time. worth emphasising estimates NMR based statistical model underlying data. paradox data availability often high-quality data less easily available countries worse outcomes. instance, M. Alexander Alkema (2018) say ‘[t]large variability availability data neonatal mortality.’ conclusions subject model underpins estimates, quality underlying data independently verify either . Additionally, often particular concern validity Chinese government data (Lyu et al. 2018).","code":""},{"path":"drinking-from-a-fire-hose.html","id":"exercises-and-tutorial-1","chapter":"2 Drinking from a fire hose","heading":"2.5 Exercises and tutorial","text":"","code":""},{"path":"drinking-from-a-fire-hose.html","id":"exercises-1","chapter":"2 Drinking from a fire hose","heading":"2.5.1 Exercises","text":"dataset underpins Chambliss (1989) collected (pick one)?\nAugust 1983 August 1984\nJanuary 1983 August 1984\nJanuary 1983 January 1984\nAugust 1983 January 1984\nAugust 1983 August 1984January 1983 August 1984January 1983 January 1984August 1983 January 1984When Chambliss (1989) talks stratification, talking ?Chambliss (1989) define ‘excellence’ (pick one)?\nProlonged performance world-class level.\nOlympic medal winners.\nConsistent superiority performance.\nnational-level athletes.\nProlonged performance world-class level.Olympic medal winners.Consistent superiority performance.national-level athletes.Think following quote Chambliss (1989, 81) list three small skills activities help achieve excellence data science.Excellence mundane. Superlative performance really confluence dozens small skills activities, one learned stumbled upon, carefully drilled habit fitted together synthesized whole. nothing extraordinary super-human one actions; fact done consistently correctly, together, produce excellence.following arguments readr::read_csv() (select apply)? (Hint: can access help function ?readr::read_csv().)\n‘all_cols’\n‘file’\n‘show_col_types’\n‘number’\n‘all_cols’‘file’‘show_col_types’‘number’used rpois() runif() draw Poisson Uniform distributions, respectively. following can used draw Normal Binomial distributions (select apply)?\nrnormal() rbinom()\nrnorm() rbinomial()\nrnormal() rbinomial()\nrnorm() rbinom()\nrnormal() rbinom()rnorm() rbinomial()rnormal() rbinomial()rnorm() rbinom()result sample(x = letters, size = 2) seed set ‘853?’ seed set ‘1234’ (pick one)?\n‘“” “q”’ ‘“p” “v”’\n‘“e” “l”’ ‘“e” “r”’\n‘“” “q”’ ‘“e” “r”’\n‘“e” “l”’ ‘“p” “v”’\n‘“” “q”’ ‘“p” “v”’‘“e” “l”’ ‘“e” “r”’‘“” “q”’ ‘“e” “r”’‘“e” “l”’ ‘“p” “v”’want cite R find recommended citation (pick one)?\ncite('R').\ncite().\ncitation('R').\ncitation().\ncite('R').cite().citation('R').citation().get citation information opendatatoronto (pick one)?\ncite()\ncitation()\ncite(‘opendatatoronto’)\ncitation(‘opendatatoronto’)\ncite()citation()cite(‘opendatatoronto’)citation(‘opendatatoronto’)argument needs changed order change column labels kable (pick one)?\n‘booktabs’\n‘col.names’\n‘digits’\n‘linesep’\n‘caption’\n‘booktabs’‘col.names’‘digits’‘linesep’‘caption’function used update packages (pick one)?\nupdate.packages()\nupgrade.packages()\nrevise.packages()\nrenovate.packages()\nupdate.packages()upgrade.packages()revise.packages()renovate.packages()features might typically expect column claimed year (select apply)?\nclass ‘character.’\nnegative numbers.\nletters column.\nentry four digits.\nclass ‘character.’negative numbers.letters column.entry four digits.","code":""},{"path":"drinking-from-a-fire-hose.html","id":"tutorial-1","chapter":"2 Drinking from a fire hose","heading":"2.5.2 Tutorial","text":"purpose tutorial encourage greater comfort data simulation communication, engender broader understanding practice data science.Please pick either seed-growing, hair-length, example Chapter 1. already started record data, probably much . Please use sample() create tibble twelve weeks’ worth data.Pretend dataset just generated actual data end . Please write three paragraphs (.e. page ) communicate , , found.Reflecting Chambliss (1989), please write one-page paper stratification excellence relates using programming languages, R Python, data science.","code":""},{"path":"r-essentials.html","id":"r-essentials","chapter":"3 R essentials","heading":"3 R essentials","text":"Required materialRead Kitchen Counter Observatory, (Healy 2020)Read AI bias really happens—’s hard fix, (Hao 2019)Read R Data Science, Chapters 4 5, (Wickham Grolemund 2017)Key concepts skillsUnderstanding foundational aspects R R Studio.able use key dplyr verbs.Knowing fundamentals class manipulate .Ability simulate data.Can make graphs.Comfort aspects tidyverse including importing data, dataset manipulation, string manipulation, factors.Develop strategies things work.Key librariesggplot2tidyverseKey functions| ‘’& ‘’|> ‘pipe’$ ‘extract’.character().integer()c()citation()class()dplyr::arrange()dplyr::case_when()dplyr::count()dplyr::filter()dplyr::group_by()dplyr::if_else()dplyr::left_join()dplyr::mutate()dplyr::pull()dplyr::rename()dplyr::select()dplyr::slice()dplyr::summarise()forcats::as_factor()forcats::fct_relevel()function()ggplot2::facet_wrap()ggplot2::geom_density()ggplot2::geom_histogram()ggplot2::geom_point()ggplot2::ggplot()head()janitor::clean_names()library()lubridate::ymd()max()mean()print()readr::read_csv()round()sample()set.seed()stats::rnorm()stats::runif()stringr::str_detect()stringr::str_replace()stringr::str_squish()sum()tibble::tibble()tidyr::pivot_longer()tidyr::pivot_wider()","code":""},{"path":"r-essentials.html","id":"background","chapter":"3 R essentials","heading":"3.1 Background","text":"chapter focus foundational skills needed use statistical programming language R (R Core Team 2021) tell stories data. may make sense first, skills approaches come back throughout notes. initially just go chapter quickly, noting aspects understand. come back chapter time time continue rest book. way see various bits fit context.R open-source language useful statistical programming. can download R free Comprehensive R Archive Network (CRAN): https://cran.r-project.org. R Studio Integrated Development Environment (IDE) R makes language easier use can downloaded free: https://www.rstudio.com/products/rstudio/.past ten years , characterised increased use tidyverse. ‘…opinionated collection R packages designed data science. packages share underlying design philosophy, grammar, data structures’ (Wickham 2020b). three distinctions : original R language, typically referred ‘base’; ‘tidyverse’ collection packages build top R language; packages.Essentially everything can tidyverse, can also base. , tidyverse built especially modern data science often easier use tidyverse, especially learning. Additionally, often everything can tidyverse, can also packages. , tidyverse coherent collection packages, often easier use tidyverse, , especially learning. Eventually one begins see cases makes sense trade-convenience coherence tidyverse features base packages. Indeed, see various points later book. instance, tidyverse can slow, one needs import thousands CSVs can make sense switch away readr::read_csv(). appropriate use base non-tidyverse packages, rather dogmatic insistence particular solution, sign one’s development.Central use statistical programming language R data, data use humans heart . Sometimes easy forget , times opposite effect.practice, find far distancing questions meaning, quantitative data forces confront . numbers draw . Working data like unending exercise humility, constant compulsion think can see, standing invitation understand measures really capture—mean, .Healy (2020)","code":""},{"path":"r-essentials.html","id":"broader-impact","chapter":"3 R essentials","heading":"3.2 Broader impact","text":"“shouldn’t think societal impact work ’s hard people can us” really bad argument. stopped CV research saw impact work . loved work military applications privacy concerns eventually became impossible ignore. basically facial recognition work get published took Broader Impacts sections seriously. almost upside enormous downside risk. fair though lot humility . grad school bought myth science apolitical research objectively moral good matter subject .Joe Redmon, 20 February 2020.Although term ‘data science’ ubiquitous academia, industry, even generally, difficult define. One deliberately antagonistic definition data science ‘[t]inhumane reduction humanity can counted’ (Keyes 2019). purposefully controversial, definition highlights one reason increased demand data science quantitative methods past decade—individuals behaviour now heart . Many techniques around many decades, makes popular now human focus.Unfortunately, even though much work may focused individuals, issues privacy consent, ethical concerns broadly, rarely seem front mind. exceptions, general, even time claiming AI, machine learning, data science going revolutionise society, consideration types issues appears largely treated something nice , rather something may like think embrace revolution.part, new issues. sciences, considerable recent ethical consideration around CRISPR technology gene editing, earlier time similar conversations , instance, Wernher von Braun allowed building rockets US. medicine, course, concerns front--mind time. Data science seems determined Tuskegee-moment rather think , deal appropriately , issues, based experiences fields occur data science.said, evidence data scientists beginning concerned ethics surrounding practice. instance, NeurIPS, prestigious machine learning conference, now requires statement ethics accompany submissions.order provide balanced perspective, authors required include statement potential broader impact work, including ethical aspects future societal consequences. Authors take care discuss positive negative outcomes.NeurIPS 2020 Conference Call PapersThe purpose ethical consideration concern broader impact work prescriptively rule things , provide opportunity raise issues front mind. variety data science applications, relative youth field, speed change, mean considerations sometimes knowingly set aside, acceptable rest field. contrasts fields science, medicine, engineering, accounting. Possibly fields self-aware (Figure 3.1).\nFigure 3.1: Probability, https://xkcd.com/881/.\n","code":""},{"path":"r-essentials.html","id":"r-r-studio-and-r-studio-cloud","chapter":"3 R essentials","heading":"3.3 R, R Studio, and R Studio Cloud","text":"R R Studio complementary, thing. Liza Bolton, Assistant Professor, Teaching Stream, Department Statistical Sciences University Toronto explains relationship analogy R like engine R Studio like car. Although us can use car engine directly, us use car interact engine.","code":""},{"path":"r-essentials.html","id":"r","chapter":"3 R essentials","heading":"3.3.1 R","text":"R—https://www.r-project.org/---open-source free programming language focused general statistics. Free context refer price zero, instead ‘freedom,’ also price zero. contrast open-source programming language designed general purpose, Python, open-source programming language focused probability, Stan. created Ross Ihaka Robert Gentleman University Auckland. maintained R Core Team changes ‘base’ code occur methodically concern given variety different priorities.Many people build stable base, extend capabilities R better quickly suit needs. creating packages. Typically, although always, package collection R code, mostly functions, allows us easily things want . packages managed CRAN, repositories Bioconductor. CRAN built download R just got, can use straight away.want use package need firstly install computer, need load want use .Di Cook, Professor Business Analytics Monash University, describes analogous lightbulb. want light house, first need fit lightbulb, need turn switch . Installing package, say, install.packages(\"tidyverse\"), akin fitting lightbulb socket—need lightbulb. time want light need turn switch lightbulb, R packages case, means calling library, say, library(tidyverse).install package computer (, need per computer) use function install.packages().want use package, need use function library().downloaded , can open R use directly. primarily designed interacted command line. functional, can useful richer environment command line provides. particular, can useful install Integrated Development Environment (IDE), application brings together various bits pieces used often. One common IDE R R Studio.","code":"\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)"},{"path":"r-essentials.html","id":"r-studio","chapter":"3 R essentials","heading":"3.3.2 R Studio","text":"R Studio distinct R, different entities. R Studio builds top R make easier use R. way one use internet command line, folks use browser Chrome, Firefox, Safari.R Studio free sense pay . also free sense able take code, modify , distribute code. important recognise R Studio company possible current situation change. can downloaded: https://www.rstudio.com/products/rstudio/.open R Studio look like Figure 3.2.\nFigure 3.2: Opening R Studio first time\nleft pane console can type execute R code line line. Try 2+2 clicking next prompt ‘>’ typing pressing enter. code type :hopefully get answer printed console.pane top right information environment. instance, create variables list names properties appear . Try type following code, replacing name name, next prompt, press enter:notice new value environment pane variable name value.pane bottom right file manager. moment just two files: R History file R Project file. get later, now create save file.Type following code (worry much details now):see new ‘.rds’ file list files.","code":"\n2 + 2\n#> [1] 4\nmy_name <- \"Rohan\"\nsaveRDS(object = my_name, file = \"my_first_file.rds\")"},{"path":"r-essentials.html","id":"r-studio-cloud","chapter":"3 R essentials","heading":"3.3.3 R Studio Cloud","text":"can download R Studio computer, initially use R Studio Cloud: https://rstudio.cloud/. online version provided R Studio. use can focus getting comfortable R R Studio environment consistent. way worry computer installation permissions, amongst things.free version R Studio Cloud free ‘financial cost.’ trade-powerful, sometimes slow, purposes getting started enough.","code":""},{"path":"r-essentials.html","id":"getting-started","chapter":"3 R essentials","heading":"3.4 Getting started","text":"now start going code. important actively write .working line--line console fine, easier write whole script can executed. making R Script. go : File -> New File -> R Script. console pane fall bottom left R Script open top left. write code get Australian federal politicians construct small table genders prime ministers.(code make sense stage, just type get habit run . run whole script, can click ‘Run’ can highlight certain lines click ‘Run’ just run lines.can see , end 2021, one female prime minister (Julia Gillard), 29 prime ministers maleOne critical operator programming ‘pipe’: |>. read ‘.’ takes output line code uses first input next line code. makes code easier read.idea pipe take dataset, , something . used earlier example. Another example follows look first six lines dataset piping head(). Notice head() explicitly take arguments example. knows data display pipe implicitly.can save R Script ‘my_first_r_script.R’ using File -> Save . point, workspace look something like Figure 3.3.\nFigure 3.3: running R Script\nOne thing aware R Studio Cloud workspace essentially new computer. , need install package want use workspace. instance, can use tidyverse, need install function install.packages(\"tidyverse\"). contrasts use computer.final notes R Studio Cloud:Australian politician’s example, got data website GitHub using R package, can get data workspace local computer variety ways. One way use ‘upload’ button ‘Files’ panel.R Studio Cloud allows degree collaboration. instance, can give someone else access workspace create. useful collaborating assignment, although quite full featured yet workspace time (contrast , say, Google Docs).variety weaknesses R Studio Cloud, particular RAM limits. Additionally, things break time time. R Studio Community page focused R Studio Cloud sometimes helpful: https://community.rstudio.com/c/rstudio-cloud.","code":"\n# Install the packages that we need\ninstall.packages(\"tidyverse\")\ninstall.packages(\"AustralianPoliticians\")\n# Load the packages that we need to use this time\nlibrary(tidyverse)\nlibrary(AustralianPoliticians)\n\n# Make a table of the counts of genders of the prime ministers\nAustralianPoliticians::get_auspol('all') |> \n  as_tibble() |> \n  filter(wasPrimeMinister == 1) |> \n  count(gender)\n#> # A tibble: 2 × 2\n#>   gender     n\n#>   <chr>  <int>\n#> 1 female     1\n#> 2 male      29\nAustralianPoliticians::get_auspol('all') |> \n  head()\n#> # A tibble: 6 × 20\n#>   uniqueID   surname allOtherNames      firstName commonName\n#>   <chr>      <chr>   <chr>              <chr>     <chr>     \n#> 1 Abbott1859 Abbott  Richard Hartley S… Richard   <NA>      \n#> 2 Abbott1869 Abbott  Percy Phipps       Percy     <NA>      \n#> 3 Abbott1877 Abbott  Macartney          Macartney Mac       \n#> 4 Abbott1886 Abbott  Charles Lydiard A… Charles   Aubrey    \n#> 5 Abbott1891 Abbott  Joseph Palmer      Joseph    <NA>      \n#> 6 Abbott1957 Abbott  Anthony John       Anthony   Tony      \n#> # … with 15 more variables: displayName <chr>,\n#> #   earlierOrLaterNames <chr>, title <chr>, gender <chr>,\n#> #   birthDate <date>, birthYear <dbl>, birthPlace <chr>,\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>, wikidataID <chr>,\n#> #   wikipedia <chr>, adb <chr>, comments <chr>"},{"path":"r-essentials.html","id":"the-dplyr-verbs","chapter":"3 R essentials","heading":"3.5 The dplyr verbs","text":"One key packages use tidyverse (Wickham et al. 2019b). tidyverse actually package packages, means install tidyverse, actually install whole bunch different packages. key package tidyverse terms manipulating data dplyr (Wickham et al. 2020).five dplyr functions regularly used, now go . commonly referred dplyr verbs.select()filter()arrange()mutate()summarise() equally summarize()Two additional functions cover closely related group_by(), count().already installed tidyverse need load .begin using data Australian politicians AustralianPoliticians package (R. Alexander Hodgetts 2021).","code":"\nlibrary(tidyverse)\nlibrary(AustralianPoliticians)\n\naustralian_politicians <- \n  get_auspol('all')\n\nhead(australian_politicians)\n#> # A tibble: 6 × 20\n#>   uniqueID   surname allOtherNames      firstName commonName\n#>   <chr>      <chr>   <chr>              <chr>     <chr>     \n#> 1 Abbott1859 Abbott  Richard Hartley S… Richard   <NA>      \n#> 2 Abbott1869 Abbott  Percy Phipps       Percy     <NA>      \n#> 3 Abbott1877 Abbott  Macartney          Macartney Mac       \n#> 4 Abbott1886 Abbott  Charles Lydiard A… Charles   Aubrey    \n#> 5 Abbott1891 Abbott  Joseph Palmer      Joseph    <NA>      \n#> 6 Abbott1957 Abbott  Anthony John       Anthony   Tony      \n#> # … with 15 more variables: displayName <chr>,\n#> #   earlierOrLaterNames <chr>, title <chr>, gender <chr>,\n#> #   birthDate <date>, birthYear <dbl>, birthPlace <chr>,\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>, wikidataID <chr>,\n#> #   wikipedia <chr>, adb <chr>, comments <chr>"},{"path":"r-essentials.html","id":"select","chapter":"3 R essentials","heading":"3.5.1 select()","text":"use select() pick particular columns dataset. instance, might like select ‘firstName’ column.R, many ways things. Sometimes different ways thing, times different ways almost thing. instance, another way pick particular column dataset use extract operator ‘$.’ base, opposed select() tidyverse.two appear similar—pick ‘firstName’ column—different class return. sake completeness, combine select() pull() get class output used extract operator.can also use select() remove columns. , select() negative sense.Finally, can use select() based conditions. instance, can select() columns start , say, ‘birth.’variety similar ‘selection helpers’ including ‘starts_with(),’ ‘ends_with(),’ ‘contains().’ information available help page select() can accessed ?select().point, use select() reduce width dataset.","code":"\naustralian_politicians |> \n  select(firstName) |> \n  head()\n#> # A tibble: 6 × 1\n#>   firstName\n#>   <chr>    \n#> 1 Richard  \n#> 2 Percy    \n#> 3 Macartney\n#> 4 Charles  \n#> 5 Joseph   \n#> 6 Anthony\naustralian_politicians$firstName |> \n  head()\n#> [1] \"Richard\"   \"Percy\"     \"Macartney\" \"Charles\"  \n#> [5] \"Joseph\"    \"Anthony\"\naustralian_politicians |> \n  select(firstName) |> \n  pull() |> \n  head()\n#> [1] \"Richard\"   \"Percy\"     \"Macartney\" \"Charles\"  \n#> [5] \"Joseph\"    \"Anthony\"\naustralian_politicians |> \n  select(-firstName) |> \n  head()\n#> # A tibble: 6 × 19\n#>   uniqueID   surname allOtherNames    commonName displayName\n#>   <chr>      <chr>   <chr>            <chr>      <chr>      \n#> 1 Abbott1859 Abbott  Richard Hartley… <NA>       Abbott, Ri…\n#> 2 Abbott1869 Abbott  Percy Phipps     <NA>       Abbott, Pe…\n#> 3 Abbott1877 Abbott  Macartney        Mac        Abbott, Mac\n#> 4 Abbott1886 Abbott  Charles Lydiard… Aubrey     Abbott, Au…\n#> 5 Abbott1891 Abbott  Joseph Palmer    <NA>       Abbott, Jo…\n#> 6 Abbott1957 Abbott  Anthony John     Tony       Abbott, To…\n#> # … with 14 more variables: earlierOrLaterNames <chr>,\n#> #   title <chr>, gender <chr>, birthDate <date>,\n#> #   birthYear <dbl>, birthPlace <chr>, deathDate <date>,\n#> #   member <dbl>, senator <dbl>, wasPrimeMinister <dbl>,\n#> #   wikidataID <chr>, wikipedia <chr>, adb <chr>,\n#> #   comments <chr>\naustralian_politicians |> \n  select(starts_with(\"birth\")) |> \n  head()\n#> # A tibble: 6 × 3\n#>   birthDate  birthYear birthPlace  \n#>   <date>         <dbl> <chr>       \n#> 1 NA              1859 Bendigo     \n#> 2 1869-05-14        NA Hobart      \n#> 3 1877-07-03        NA Murrurundi  \n#> 4 1886-01-04        NA St Leonards \n#> 5 1891-10-18        NA North Sydney\n#> 6 1957-11-04        NA London\naustralian_politicians <-\n  australian_politicians |>\n  select(uniqueID,\n         surname,\n         firstName,\n         gender,\n         birthDate,\n         birthYear,\n         deathDate,\n         member,\n         senator,\n         wasPrimeMinister)\n\naustralian_politicians\n#> # A tibble: 1,783 × 10\n#>    uniqueID   surname firstName gender birthDate  birthYear\n#>    <chr>      <chr>   <chr>     <chr>  <date>         <dbl>\n#>  1 Abbott1859 Abbott  Richard   male   NA              1859\n#>  2 Abbott1869 Abbott  Percy     male   1869-05-14        NA\n#>  3 Abbott1877 Abbott  Macartney male   1877-07-03        NA\n#>  4 Abbott1886 Abbott  Charles   male   1886-01-04        NA\n#>  5 Abbott1891 Abbott  Joseph    male   1891-10-18        NA\n#>  6 Abbott1957 Abbott  Anthony   male   1957-11-04        NA\n#>  7 Abel1939   Abel    John      male   1939-06-25        NA\n#>  8 Abetz1958  Abetz   Eric      male   1958-01-25        NA\n#>  9 Adams1943  Adams   Judith    female 1943-04-11        NA\n#> 10 Adams1951  Adams   Dick      male   1951-04-29        NA\n#> # … with 1,773 more rows, and 4 more variables:\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>"},{"path":"r-essentials.html","id":"filter","chapter":"3 R essentials","heading":"3.5.2 filter()","text":"use filter() pick particular rows dataset. instance, might like filter politicians became prime minister.also pass filter() two conditions. instance, look politicians become prime minister named Joseph, using ‘’ operator ‘&.’get result use comma instead ampersand.Similarly, look politicians named Myles Ruth using ‘’ operator ‘|’also pipe result. instance pipe filter() select().happen know particular row number interest filter() particular row. instance, say row 853 interest.also dedicated function , slice()may seem somewhat esoteric, especially useful like remove particular row using negation, duplicate specific rows. instance, remove first row.also , say, keep first three rows.Finally, duplicate first two rows.","code":"\naustralian_politicians |> \n  filter(wasPrimeMinister == 1)\n#> # A tibble: 30 × 10\n#>    uniqueID    surname firstName gender birthDate  birthYear\n#>    <chr>       <chr>   <chr>     <chr>  <date>         <dbl>\n#>  1 Abbott1957  Abbott  Anthony   male   1957-11-04        NA\n#>  2 Barton1849  Barton  Edmund    male   1849-01-18        NA\n#>  3 Bruce1883   Bruce   Stanley   male   1883-04-15        NA\n#>  4 Chifley1885 Chifley Joseph    male   1885-09-22        NA\n#>  5 Cook1860    Cook    Joseph    male   1860-12-07        NA\n#>  6 Curtin1885  Curtin  John      male   1885-01-08        NA\n#>  7 Deakin1856  Deakin  Alfred    male   1856-08-03        NA\n#>  8 Fadden1894  Fadden  Arthur    male   1894-04-13        NA\n#>  9 Fisher1862  Fisher  Andrew    male   1862-08-29        NA\n#> 10 Forde1890   Forde   Francis   male   1890-07-18        NA\n#> # … with 20 more rows, and 4 more variables:\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>\naustralian_politicians |> \n  filter(wasPrimeMinister == 1 & firstName == \"Joseph\")\n#> # A tibble: 3 × 10\n#>   uniqueID    surname firstName gender birthDate  birthYear\n#>   <chr>       <chr>   <chr>     <chr>  <date>         <dbl>\n#> 1 Chifley1885 Chifley Joseph    male   1885-09-22        NA\n#> 2 Cook1860    Cook    Joseph    male   1860-12-07        NA\n#> 3 Lyons1879   Lyons   Joseph    male   1879-09-15        NA\n#> # … with 4 more variables: deathDate <date>, member <dbl>,\n#> #   senator <dbl>, wasPrimeMinister <dbl>\naustralian_politicians |> \n  filter(wasPrimeMinister == 1, firstName == \"Joseph\")\n#> # A tibble: 3 × 10\n#>   uniqueID    surname firstName gender birthDate  birthYear\n#>   <chr>       <chr>   <chr>     <chr>  <date>         <dbl>\n#> 1 Chifley1885 Chifley Joseph    male   1885-09-22        NA\n#> 2 Cook1860    Cook    Joseph    male   1860-12-07        NA\n#> 3 Lyons1879   Lyons   Joseph    male   1879-09-15        NA\n#> # … with 4 more variables: deathDate <date>, member <dbl>,\n#> #   senator <dbl>, wasPrimeMinister <dbl>\naustralian_politicians |> \n  filter(firstName == \"Myles\" | firstName == \"Ruth\")\n#> # A tibble: 3 × 10\n#>   uniqueID     surname firstName gender birthDate  birthYear\n#>   <chr>        <chr>   <chr>     <chr>  <date>         <dbl>\n#> 1 Coleman1931  Coleman Ruth      female 1931-09-27        NA\n#> 2 Ferricks1875 Ferric… Myles     male   1875-11-12        NA\n#> 3 Webber1965   Webber  Ruth      female 1965-03-24        NA\n#> # … with 4 more variables: deathDate <date>, member <dbl>,\n#> #   senator <dbl>, wasPrimeMinister <dbl>\naustralian_politicians |> \n  filter(firstName == \"Ruth\" | firstName == \"Myles\") |> \n  select(firstName, surname)\n#> # A tibble: 3 × 2\n#>   firstName surname \n#>   <chr>     <chr>   \n#> 1 Ruth      Coleman \n#> 2 Myles     Ferricks\n#> 3 Ruth      Webber\naustralian_politicians |> \n  filter(row_number() == 853)\n#> # A tibble: 1 × 10\n#>   uniqueID     surname firstName gender birthDate  birthYear\n#>   <chr>        <chr>   <chr>     <chr>  <date>         <dbl>\n#> 1 Jakobsen1947 Jakobs… Carolyn   female 1947-09-11        NA\n#> # … with 4 more variables: deathDate <date>, member <dbl>,\n#> #   senator <dbl>, wasPrimeMinister <dbl>\naustralian_politicians |> \n  slice(853)\n#> # A tibble: 1 × 10\n#>   uniqueID     surname firstName gender birthDate  birthYear\n#>   <chr>        <chr>   <chr>     <chr>  <date>         <dbl>\n#> 1 Jakobsen1947 Jakobs… Carolyn   female 1947-09-11        NA\n#> # … with 4 more variables: deathDate <date>, member <dbl>,\n#> #   senator <dbl>, wasPrimeMinister <dbl>\naustralian_politicians |> \n  slice(-1)\n#> # A tibble: 1,782 × 10\n#>    uniqueID    surname firstName gender birthDate  birthYear\n#>    <chr>       <chr>   <chr>     <chr>  <date>         <dbl>\n#>  1 Abbott1869  Abbott  Percy     male   1869-05-14        NA\n#>  2 Abbott1877  Abbott  Macartney male   1877-07-03        NA\n#>  3 Abbott1886  Abbott  Charles   male   1886-01-04        NA\n#>  4 Abbott1891  Abbott  Joseph    male   1891-10-18        NA\n#>  5 Abbott1957  Abbott  Anthony   male   1957-11-04        NA\n#>  6 Abel1939    Abel    John      male   1939-06-25        NA\n#>  7 Abetz1958   Abetz   Eric      male   1958-01-25        NA\n#>  8 Adams1943   Adams   Judith    female 1943-04-11        NA\n#>  9 Adams1951   Adams   Dick      male   1951-04-29        NA\n#> 10 Adamson1857 Adamson John      male   1857-02-18        NA\n#> # … with 1,772 more rows, and 4 more variables:\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>\naustralian_politicians |> \n  slice(1:3)\n#> # A tibble: 3 × 10\n#>   uniqueID   surname firstName gender birthDate  birthYear\n#>   <chr>      <chr>   <chr>     <chr>  <date>         <dbl>\n#> 1 Abbott1859 Abbott  Richard   male   NA              1859\n#> 2 Abbott1869 Abbott  Percy     male   1869-05-14        NA\n#> 3 Abbott1877 Abbott  Macartney male   1877-07-03        NA\n#> # … with 4 more variables: deathDate <date>, member <dbl>,\n#> #   senator <dbl>, wasPrimeMinister <dbl>\naustralian_politicians |> \n  slice(1:2, 1:n())\n#> # A tibble: 1,785 × 10\n#>    uniqueID   surname firstName gender birthDate  birthYear\n#>    <chr>      <chr>   <chr>     <chr>  <date>         <dbl>\n#>  1 Abbott1859 Abbott  Richard   male   NA              1859\n#>  2 Abbott1869 Abbott  Percy     male   1869-05-14        NA\n#>  3 Abbott1859 Abbott  Richard   male   NA              1859\n#>  4 Abbott1869 Abbott  Percy     male   1869-05-14        NA\n#>  5 Abbott1877 Abbott  Macartney male   1877-07-03        NA\n#>  6 Abbott1886 Abbott  Charles   male   1886-01-04        NA\n#>  7 Abbott1891 Abbott  Joseph    male   1891-10-18        NA\n#>  8 Abbott1957 Abbott  Anthony   male   1957-11-04        NA\n#>  9 Abel1939   Abel    John      male   1939-06-25        NA\n#> 10 Abetz1958  Abetz   Eric      male   1958-01-25        NA\n#> # … with 1,775 more rows, and 4 more variables:\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>"},{"path":"r-essentials.html","id":"arrange","chapter":"3 R essentials","heading":"3.5.3 arrange()","text":"use arrange() change order dataset based values particular columns. instance, arrange ‘australian_politicians’ birthday.modify arrange() desc() change ascending descending order.arrange based one column. instance, two politicians first name, arrange based birthday.achieve result using pipe two instances arrange().use arrange() important clear precedence. instance, changing birthday first name give different arrangement.nice way arrange variety columns use across(). means can specify variety columns. also enables us use ‘selection helpers’ ‘starts_with()’ mentioned association select().","code":"\naustralian_politicians |> \n  arrange(birthDate)\n#> # A tibble: 1,783 × 10\n#>    uniqueID    surname firstName gender birthDate  birthYear\n#>    <chr>       <chr>   <chr>     <chr>  <date>         <dbl>\n#>  1 Braddon1829 Braddon Edward    male   1829-06-11        NA\n#>  2 Ferguson18… Fergus… John      male   1830-03-15        NA\n#>  3 Zeal1830    Zeal    William   male   1830-12-05        NA\n#>  4 Fraser1832  Fraser  Simon     male   1832-08-21        NA\n#>  5 Groom1833   Groom   William   male   1833-03-09        NA\n#>  6 Sargood1834 Sargood Frederick male   1834-05-30        NA\n#>  7 Fysh1835    Fysh    Philip    male   1835-03-01        NA\n#>  8 Playford18… Playfo… Thomas    male   1837-11-26        NA\n#>  9 Solomon1839 Solomon Elias     male   1839-09-02        NA\n#> 10 McLean1840  McLean  Allan     male   1840-02-03        NA\n#> # … with 1,773 more rows, and 4 more variables:\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>\naustralian_politicians |> \n  arrange(desc(birthDate))\n#> # A tibble: 1,783 × 10\n#>    uniqueID   surname  firstName gender birthDate  birthYear\n#>    <chr>      <chr>    <chr>     <chr>  <date>         <dbl>\n#>  1 SteeleJoh… Steele-… Jordon    male   1994-10-14        NA\n#>  2 Chandler1… Chandler Claire    female 1990-06-01        NA\n#>  3 Roy1990    Roy      Wyatt     male   1990-05-22        NA\n#>  4 Thompson1… Thompson Phillip   male   1988-05-07        NA\n#>  5 Paterson1… Paterson James     male   1987-11-21        NA\n#>  6 Burns1987  Burns    Joshua    male   1987-02-06        NA\n#>  7 Smith1986  Smith    Marielle  female 1986-12-30        NA\n#>  8 Kakoschke… Kakosch… Skye      female 1985-12-19        NA\n#>  9 Simmonds1… Simmonds Julian    male   1985-08-29        NA\n#> 10 Gorman1984 Gorman   Patrick   male   1984-12-12        NA\n#> # … with 1,773 more rows, and 4 more variables:\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>\naustralian_politicians |> \n  arrange(firstName, birthDate)\n#> # A tibble: 1,783 × 10\n#>    uniqueID    surname firstName gender birthDate  birthYear\n#>    <chr>       <chr>   <chr>     <chr>  <date>         <dbl>\n#>  1 Blain1894   Blain   Adair     male   1894-11-21        NA\n#>  2 Dein1889    Dein    Adam      male   1889-03-04        NA\n#>  3 Armstrong1… Armstr… Adam      male   1909-07-01        NA\n#>  4 Bandt1972   Bandt   Adam      male   1972-03-11        NA\n#>  5 Ridgeway19… Ridgew… Aden      male   1962-09-18        NA\n#>  6 Bennett1933 Bennett Adrian    male   1933-01-21        NA\n#>  7 Gibson1935  Gibson  Adrian    male   1935-11-03        NA\n#>  8 Wynne1850   Wynne   Agar      male   1850-01-15        NA\n#>  9 Robertson1… Robert… Agnes     female 1882-07-31        NA\n#> 10 Pittard1902 Pittard Alan      male   1902-11-15        NA\n#> # … with 1,773 more rows, and 4 more variables:\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>\naustralian_politicians |> \n  arrange(birthDate) |> \n  arrange(firstName)\n#> # A tibble: 1,783 × 10\n#>    uniqueID    surname firstName gender birthDate  birthYear\n#>    <chr>       <chr>   <chr>     <chr>  <date>         <dbl>\n#>  1 Blain1894   Blain   Adair     male   1894-11-21        NA\n#>  2 Dein1889    Dein    Adam      male   1889-03-04        NA\n#>  3 Armstrong1… Armstr… Adam      male   1909-07-01        NA\n#>  4 Bandt1972   Bandt   Adam      male   1972-03-11        NA\n#>  5 Ridgeway19… Ridgew… Aden      male   1962-09-18        NA\n#>  6 Bennett1933 Bennett Adrian    male   1933-01-21        NA\n#>  7 Gibson1935  Gibson  Adrian    male   1935-11-03        NA\n#>  8 Wynne1850   Wynne   Agar      male   1850-01-15        NA\n#>  9 Robertson1… Robert… Agnes     female 1882-07-31        NA\n#> 10 Pittard1902 Pittard Alan      male   1902-11-15        NA\n#> # … with 1,773 more rows, and 4 more variables:\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>\naustralian_politicians |> \n  arrange(birthYear, firstName)\n#> # A tibble: 1,783 × 10\n#>    uniqueID    surname firstName gender birthDate birthYear\n#>    <chr>       <chr>   <chr>     <chr>  <date>        <dbl>\n#>  1 Edwards1842 Edwards Richard   male   NA             1842\n#>  2 Sawers1844  Sawers  William   male   NA             1844\n#>  3 Barker1846  Barker  Stephen   male   NA             1846\n#>  4 Corser1852  Corser  Edward    male   NA             1852\n#>  5 Lee1856     Lee     Henry     male   NA             1856\n#>  6 Grant1857   Grant   John      male   NA             1857\n#>  7 Palmer1859  Palmer  Albert    male   NA             1859\n#>  8 Riley1859   Riley   Edward    male   NA             1859\n#>  9 Abbott1859  Abbott  Richard   male   NA             1859\n#> 10 Kennedy1860 Kennedy Thomas    male   NA             1860\n#> # … with 1,773 more rows, and 4 more variables:\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>\naustralian_politicians |> \n  arrange(across(c(firstName, birthYear))) |> \n  head()\n#> # A tibble: 6 × 10\n#>   uniqueID    surname  firstName gender birthDate  birthYear\n#>   <chr>       <chr>    <chr>     <chr>  <date>         <dbl>\n#> 1 Blain1894   Blain    Adair     male   1894-11-21        NA\n#> 2 Armstrong1… Armstro… Adam      male   1909-07-01        NA\n#> 3 Bandt1972   Bandt    Adam      male   1972-03-11        NA\n#> 4 Dein1889    Dein     Adam      male   1889-03-04        NA\n#> 5 Ridgeway19… Ridgeway Aden      male   1962-09-18        NA\n#> 6 Bennett1933 Bennett  Adrian    male   1933-01-21        NA\n#> # … with 4 more variables: deathDate <date>, member <dbl>,\n#> #   senator <dbl>, wasPrimeMinister <dbl>\n\naustralian_politicians |> \n  arrange(across(starts_with('birth'))) |> \n  head()\n#> # A tibble: 6 × 10\n#>   uniqueID     surname firstName gender birthDate  birthYear\n#>   <chr>        <chr>   <chr>     <chr>  <date>         <dbl>\n#> 1 Braddon1829  Braddon Edward    male   1829-06-11        NA\n#> 2 Ferguson1830 Fergus… John      male   1830-03-15        NA\n#> 3 Zeal1830     Zeal    William   male   1830-12-05        NA\n#> 4 Fraser1832   Fraser  Simon     male   1832-08-21        NA\n#> 5 Groom1833    Groom   William   male   1833-03-09        NA\n#> 6 Sargood1834  Sargood Frederick male   1834-05-30        NA\n#> # … with 4 more variables: deathDate <date>, member <dbl>,\n#> #   senator <dbl>, wasPrimeMinister <dbl>"},{"path":"r-essentials.html","id":"mutate","chapter":"3 R essentials","heading":"3.5.4 mutate()","text":"use mutate() want make new column. instance, perhaps want make new column 1 person member senator 0 otherwise. say new column denote politicians served upper lower house.can use mutate() math, addition subtraction. instance, calculate age politicians () 2022.variety functions especially useful constructing new columns. include log() compute natural logarithm, lead() bring values one row, lag() push values one row, cumsum() creates cumulative sum column.earlier examples, can also use mutate() combination across(). inlcudes potential use ‘selection helpers.’ instance, count number characters first last names time.Finally, use case_when() need make new column basis two conditional statements. instance, may years want group decades.accomplish series if_else() statements, case_when() clear. cases evaluated order soon match case_when() continue remainder cases. can useful catch-end signal potential issue might like know .","code":"\naustralian_politicians <- \n  australian_politicians |> \n  mutate(was_both = if_else(member == 1 & senator == 1, 1, 0))\n\naustralian_politicians |> \n  select(member, senator, was_both)\n#> # A tibble: 1,783 × 3\n#>    member senator was_both\n#>     <dbl>   <dbl>    <dbl>\n#>  1      0       1        0\n#>  2      1       1        1\n#>  3      0       1        0\n#>  4      1       0        0\n#>  5      1       0        0\n#>  6      1       0        0\n#>  7      1       0        0\n#>  8      0       1        0\n#>  9      0       1        0\n#> 10      1       0        0\n#> # … with 1,773 more rows\naustralian_politicians <- \n  australian_politicians |> \n  mutate(age = 2022 - lubridate::year(birthDate))\n\naustralian_politicians |> \n  select(uniqueID, age)\n#> # A tibble: 1,783 × 2\n#>    uniqueID     age\n#>    <chr>      <dbl>\n#>  1 Abbott1859    NA\n#>  2 Abbott1869   153\n#>  3 Abbott1877   145\n#>  4 Abbott1886   136\n#>  5 Abbott1891   131\n#>  6 Abbott1957    65\n#>  7 Abel1939      83\n#>  8 Abetz1958     64\n#>  9 Adams1943     79\n#> 10 Adams1951     71\n#> # … with 1,773 more rows\naustralian_politicians |> \n  select(uniqueID, age) |> \n  mutate(log_age = log(age)) |> \n  head()\n#> # A tibble: 6 × 3\n#>   uniqueID     age log_age\n#>   <chr>      <dbl>   <dbl>\n#> 1 Abbott1859    NA   NA   \n#> 2 Abbott1869   153    5.03\n#> 3 Abbott1877   145    4.98\n#> 4 Abbott1886   136    4.91\n#> 5 Abbott1891   131    4.88\n#> 6 Abbott1957    65    4.17\n\naustralian_politicians |> \n  select(uniqueID, age) |> \n  mutate(lead_age = lead(age)) |> \n  head()\n#> # A tibble: 6 × 3\n#>   uniqueID     age lead_age\n#>   <chr>      <dbl>    <dbl>\n#> 1 Abbott1859    NA      153\n#> 2 Abbott1869   153      145\n#> 3 Abbott1877   145      136\n#> 4 Abbott1886   136      131\n#> 5 Abbott1891   131       65\n#> 6 Abbott1957    65       83\n\naustralian_politicians |> \n  select(uniqueID, age) |> \n  mutate(lag_age = lag(age)) |> \n  head()\n#> # A tibble: 6 × 3\n#>   uniqueID     age lag_age\n#>   <chr>      <dbl>   <dbl>\n#> 1 Abbott1859    NA      NA\n#> 2 Abbott1869   153      NA\n#> 3 Abbott1877   145     153\n#> 4 Abbott1886   136     145\n#> 5 Abbott1891   131     136\n#> 6 Abbott1957    65     131\n\naustralian_politicians |> \n  select(uniqueID, age) |> \n  filter(!is.na(age)) |> \n  mutate(cumulative_age = cumsum(age)) |> \n  head()\n#> # A tibble: 6 × 3\n#>   uniqueID     age cumulative_age\n#>   <chr>      <dbl>          <dbl>\n#> 1 Abbott1869   153            153\n#> 2 Abbott1877   145            298\n#> 3 Abbott1886   136            434\n#> 4 Abbott1891   131            565\n#> 5 Abbott1957    65            630\n#> 6 Abel1939      83            713\naustralian_politicians |> \n  mutate(across(c(firstName, surname), str_count)) |> \n  select(uniqueID, firstName, surname)\n#> # A tibble: 1,783 × 3\n#>    uniqueID   firstName surname\n#>    <chr>          <int>   <int>\n#>  1 Abbott1859         7       6\n#>  2 Abbott1869         5       6\n#>  3 Abbott1877         9       6\n#>  4 Abbott1886         7       6\n#>  5 Abbott1891         6       6\n#>  6 Abbott1957         7       6\n#>  7 Abel1939           4       4\n#>  8 Abetz1958          4       5\n#>  9 Adams1943          6       5\n#> 10 Adams1951          4       5\n#> # … with 1,773 more rows\naustralian_politicians |> \n  mutate(year_of_birth = lubridate::year(birthDate),\n         decade_of_birth = \n           case_when(\n             year_of_birth <= 1929 ~ \"pre-1930\",\n             year_of_birth <= 1939 ~ \"1930s\",\n             year_of_birth <= 1949 ~ \"1940s\",\n             year_of_birth <= 1959 ~ \"1950s\",\n             year_of_birth <= 1969 ~ \"1960s\",\n             year_of_birth <= 1979 ~ \"1970s\",\n             year_of_birth <= 1989 ~ \"1980s\",\n             TRUE ~ \"Unknown or error\"\n             )\n  ) |> \n  select(uniqueID, year_of_birth, decade_of_birth)\n#> # A tibble: 1,783 × 3\n#>    uniqueID   year_of_birth decade_of_birth \n#>    <chr>              <dbl> <chr>           \n#>  1 Abbott1859            NA Unknown or error\n#>  2 Abbott1869          1869 pre-1930        \n#>  3 Abbott1877          1877 pre-1930        \n#>  4 Abbott1886          1886 pre-1930        \n#>  5 Abbott1891          1891 pre-1930        \n#>  6 Abbott1957          1957 1950s           \n#>  7 Abel1939            1939 1930s           \n#>  8 Abetz1958           1958 1950s           \n#>  9 Adams1943           1943 1940s           \n#> 10 Adams1951           1951 1950s           \n#> # … with 1,773 more rows"},{"path":"r-essentials.html","id":"summarise","chapter":"3 R essentials","heading":"3.5.5 summarise()","text":"use summarise() like make new, condensed, summary variables. instance, perhaps like know minimum, average, maximum column.aside, summarise() summarize() equivalent can use either get result.default, summarise() provide one row output whole dataset. instance, earlier example found youngest, oldest, average across politicians. However, can create groups dataset using function group_by(). can apply another function within context groups. use many functions basis groups, summarise() function particularly powerful conjunction group_by(). instance, summarise() gender, get age-based summary statistics.Similarly, look youngest, oldest, mean age death gender.learn female members parliament average lived slightly longer male members parliament.can use group_by() basis one group. instance, looking average number days lived gender, also house.can use function count() create counts groups. instance, number politicians gender.addition count(), make proportion wanted .function count() essentially using group_by() summarise(). instance, can get result.similarly helpful function mutate(), add_count(). difference number added column.","code":"\naustralian_politicians |> \n  summarise(youngest = min(age, na.rm = TRUE),\n            oldest = max(age, na.rm = TRUE),\n            average = mean(age, na.rm = TRUE))\n#> # A tibble: 1 × 3\n#>   youngest oldest average\n#>      <dbl>  <dbl>   <dbl>\n#> 1       28    193    101.\naustralian_politicians |> \n  summarize(youngest = min(age, na.rm = TRUE),\n            oldest = max(age, na.rm = TRUE),\n            average = mean(age, na.rm = TRUE))\n#> # A tibble: 1 × 3\n#>   youngest oldest average\n#>      <dbl>  <dbl>   <dbl>\n#> 1       28    193    101.\naustralian_politicians |> \n  group_by(gender) |> \n  summarise(youngest = min(age, na.rm = TRUE),\n            oldest = max(age, na.rm = TRUE),\n            average = mean(age, na.rm = TRUE))\n#> # A tibble: 2 × 4\n#>   gender youngest oldest average\n#>   <chr>     <dbl>  <dbl>   <dbl>\n#> 1 female       32    140    66.0\n#> 2 male         28    193   106.\naustralian_politicians |>\n  mutate(days_lived = deathDate - birthDate) |> \n  filter(!is.na(days_lived)) |> \n  group_by(gender) |> \n  summarise(\n    min_days = min(days_lived),\n    mean_days = mean(days_lived) |> round(),\n    max_days = max(days_lived)\n    )\n#> # A tibble: 2 × 4\n#>   gender min_days   mean_days  max_days  \n#>   <chr>  <drtn>     <drtn>     <drtn>    \n#> 1 female 14856 days 28857 days 35560 days\n#> 2 male   12380 days 27376 days 36416 days\naustralian_politicians |>\n  mutate(days_lived = deathDate - birthDate) |> \n  filter(!is.na(days_lived)) |> \n  group_by(gender, member) |> \n  summarise(\n    min_days = min(days_lived),\n    mean_days = mean(days_lived) |> round(),\n    max_days = max(days_lived)\n    )\n#> # A tibble: 4 × 5\n#> # Groups:   gender [2]\n#>   gender member min_days   mean_days  max_days  \n#>   <chr>   <dbl> <drtn>     <drtn>     <drtn>    \n#> 1 female      0 21746 days 29517 days 35560 days\n#> 2 female      1 14856 days 27538 days 33442 days\n#> 3 male        0 13619 days 27133 days 36416 days\n#> 4 male        1 12380 days 27496 days 36328 days\naustralian_politicians |> \n  group_by(gender) |> \n  count()\n#> # A tibble: 2 × 2\n#> # Groups:   gender [2]\n#>   gender     n\n#>   <chr>  <int>\n#> 1 female   240\n#> 2 male    1543\naustralian_politicians |> \n  group_by(gender) |> \n  count() |> \n  ungroup() |> \n  mutate(proportion = n/(sum(n)))\n#> # A tibble: 2 × 3\n#>   gender     n proportion\n#>   <chr>  <int>      <dbl>\n#> 1 female   240      0.135\n#> 2 male    1543      0.865\naustralian_politicians |> \n  group_by(gender) |> \n  summarise(n = n())\n#> # A tibble: 2 × 2\n#>   gender     n\n#>   <chr>  <int>\n#> 1 female   240\n#> 2 male    1543\naustralian_politicians |> \n  group_by(gender) |> \n  add_count() |> \n  select(uniqueID, gender, n)\n#> # A tibble: 1,783 × 3\n#> # Groups:   gender [2]\n#>    uniqueID   gender     n\n#>    <chr>      <chr>  <int>\n#>  1 Abbott1859 male    1543\n#>  2 Abbott1869 male    1543\n#>  3 Abbott1877 male    1543\n#>  4 Abbott1886 male    1543\n#>  5 Abbott1891 male    1543\n#>  6 Abbott1957 male    1543\n#>  7 Abel1939   male    1543\n#>  8 Abetz1958  male    1543\n#>  9 Adams1943  female   240\n#> 10 Adams1951  male    1543\n#> # … with 1,773 more rows"},{"path":"r-essentials.html","id":"base","chapter":"3 R essentials","heading":"3.6 Base","text":"tidyverse put together recently help data science, R existed long . host functionality built especially around core needs programmers statisticians.particular, cover:class()data simulationfunction()need install additional packages, functionality comes R.","code":""},{"path":"r-essentials.html","id":"class","chapter":"3 R essentials","heading":"3.6.1 Class","text":"way ‘, b, c, …’ letters ‘1, 2, 3,…’ numbers, R needs way distinguishing different classes content. use letters numbers differently, instance add letters. Similarly, R needs formalise define properties class , ‘behaves, relates types objects’ (Wickham 2019a).Classes hierarchy. instance, ‘human,’ ‘animal.’ ‘humans’ ‘animals,’ ‘animals’ ‘humans.’ Similarly, integers numbers, numbers integers. find R use function class().classes cover ‘numeric,’ ‘character,’ ‘factor,’ ‘date,’ ‘data.frame.’first thing know can sometimes change class object. instance, start ‘numeric,’ change ‘character’ .character(), ‘factor’ .factor(). tried make date .Date() get error number used properties needed date.Compared ‘numeric’ ‘character’ classes, ‘factor’ class might less familiar. ‘factor’ used categorical data, means ranking, can take certain values (Wickham 2019a). instance, typical usage factor variable binary, ‘day’ ‘night.’ also often used age-groups, ‘18-29,’ ‘30-44,’ ‘45-60,’ ‘60+’ (opposed age, often ‘numeric’); sometimes level education: ‘less high school,’ ‘high school,’ ‘college,’ ‘undergraduate degree,’ ‘postgraduate degree.’ can find allowed levels ‘factor’ using levels().Dates especially tricky class quickly become complicated. Nonetheless, foundational level, can use .Date() convert character looks like date actual date. enables us , say, calculate perform addition subtraction, able character representation.final class discuss ‘data.frame.’ looks like spreadsheet commonly used store data analyse. Formally, ‘data frame list equal-length vectors’ (Wickham 2019a). column row names can see using colnames() rownames(), although often names rows just numbers.illustrate use ‘ResumeNames’ dataset AER package (Kleiber Zeileis 2008). package can installed way package CRAN: install.packages(\"AER\"). dataset comprises cross-sectional data resume content, especially name used resume, associated information whether candidate received call-back 4,870 fictitious resumes. dataset created Bertrand Mullainathan (2004) sent fictitious resumes response job advertisements Boston Chicago differed whether resume assigned ‘African American sounding name White sounding name.’ found considerable discrimination whereby ‘White names receive 50 percent callbacks interviews.’can examine class vectors make-data frame specifying column name.Sometimes helpful able change classes many columns . can using mutate() across().many ways code run issue class always among first things check. Common issues variables think ‘character’ ‘numeric,’ actually ‘factor.’ variables think ‘numeric’ actually ‘character.’","code":"\na_number <- 8\nclass(a_number)\n#> [1] \"numeric\"\n\na_letter <- \"a\"\nclass(a_letter)\n#> [1] \"character\"\na_number <- 8\na_number\n#> [1] 8\nclass(a_number)\n#> [1] \"numeric\"\n\na_number <- as.character(a_number)\na_number\n#> [1] \"8\"\nclass(a_number)\n#> [1] \"character\"\n\na_number <- as.factor(a_number)\na_number\n#> [1] 8\n#> Levels: 8\nclass(a_number)\n#> [1] \"factor\"\nage_groups <- factor(\n  c('18-29', '30-44', '45-60', '60+')\n)\nage_groups\n#> [1] 18-29 30-44 45-60 60+  \n#> Levels: 18-29 30-44 45-60 60+\nclass(age_groups)\n#> [1] \"factor\"\nlevels(age_groups)\n#> [1] \"18-29\" \"30-44\" \"45-60\" \"60+\"\nlooks_like_a_date_but_is_not <- \"2022-01-01\"\nlooks_like_a_date_but_is_not\n#> [1] \"2022-01-01\"\nclass(looks_like_a_date_but_is_not)\n#> [1] \"character\"\nis_a_date <- as.Date(looks_like_a_date_but_is_not)\nis_a_date\n#> [1] \"2022-01-01\"\nclass(is_a_date)\n#> [1] \"Date\"\nis_a_date + 3\n#> [1] \"2022-01-04\"\nlibrary(AER)\ndata(\"ResumeNames\", package = \"AER\")\nResumeNames |> \n  head()\n#>      name gender ethnicity quality call    city jobs\n#> 1 Allison female      cauc     low   no chicago    2\n#> 2 Kristen female      cauc    high   no chicago    3\n#> 3 Lakisha female      afam     low   no chicago    1\n#> 4 Latonya female      afam    high   no chicago    4\n#> 5  Carrie female      cauc    high   no chicago    3\n#> 6     Jay   male      cauc     low   no chicago    2\n#>   experience honors volunteer military holes school email\n#> 1          6     no        no       no   yes     no    no\n#> 2          6     no       yes      yes    no    yes   yes\n#> 3          6     no        no       no    no    yes    no\n#> 4          6     no       yes       no   yes     no   yes\n#> 5         22     no        no       no    no    yes   yes\n#> 6          6    yes        no       no    no     no    no\n#>   computer special college minimum equal     wanted\n#> 1      yes      no     yes       5   yes supervisor\n#> 2      yes      no      no       5   yes supervisor\n#> 3      yes      no     yes       5   yes supervisor\n#> 4      yes     yes      no       5   yes supervisor\n#> 5      yes      no      no    some   yes  secretary\n#> 6       no     yes     yes    none   yes      other\n#>   requirements reqexp reqcomm reqeduc reqcomp reqorg\n#> 1          yes    yes      no      no     yes     no\n#> 2          yes    yes      no      no     yes     no\n#> 3          yes    yes      no      no     yes     no\n#> 4          yes    yes      no      no     yes     no\n#> 5          yes    yes      no      no     yes    yes\n#> 6           no     no      no      no      no     no\n#>                           industry\n#> 1                    manufacturing\n#> 2                    manufacturing\n#> 3                    manufacturing\n#> 4                    manufacturing\n#> 5 health/education/social services\n#> 6                            trade\nclass(ResumeNames)\n#> [1] \"data.frame\"\ncolnames(ResumeNames)\n#>  [1] \"name\"         \"gender\"       \"ethnicity\"   \n#>  [4] \"quality\"      \"call\"         \"city\"        \n#>  [7] \"jobs\"         \"experience\"   \"honors\"      \n#> [10] \"volunteer\"    \"military\"     \"holes\"       \n#> [13] \"school\"       \"email\"        \"computer\"    \n#> [16] \"special\"      \"college\"      \"minimum\"     \n#> [19] \"equal\"        \"wanted\"       \"requirements\"\n#> [22] \"reqexp\"       \"reqcomm\"      \"reqeduc\"     \n#> [25] \"reqcomp\"      \"reqorg\"       \"industry\"\nclass(ResumeNames$name)\n#> [1] \"factor\"\nclass(ResumeNames$jobs)\n#> [1] \"integer\"\nResumeNames |>\n  mutate(across(c(name, gender, ethnicity), as.character)) |>\n  head()\n#>      name gender ethnicity quality call    city jobs\n#> 1 Allison female      cauc     low   no chicago    2\n#> 2 Kristen female      cauc    high   no chicago    3\n#> 3 Lakisha female      afam     low   no chicago    1\n#> 4 Latonya female      afam    high   no chicago    4\n#> 5  Carrie female      cauc    high   no chicago    3\n#> 6     Jay   male      cauc     low   no chicago    2\n#>   experience honors volunteer military holes school email\n#> 1          6     no        no       no   yes     no    no\n#> 2          6     no       yes      yes    no    yes   yes\n#> 3          6     no        no       no    no    yes    no\n#> 4          6     no       yes       no   yes     no   yes\n#> 5         22     no        no       no    no    yes   yes\n#> 6          6    yes        no       no    no     no    no\n#>   computer special college minimum equal     wanted\n#> 1      yes      no     yes       5   yes supervisor\n#> 2      yes      no      no       5   yes supervisor\n#> 3      yes      no     yes       5   yes supervisor\n#> 4      yes     yes      no       5   yes supervisor\n#> 5      yes      no      no    some   yes  secretary\n#> 6       no     yes     yes    none   yes      other\n#>   requirements reqexp reqcomm reqeduc reqcomp reqorg\n#> 1          yes    yes      no      no     yes     no\n#> 2          yes    yes      no      no     yes     no\n#> 3          yes    yes      no      no     yes     no\n#> 4          yes    yes      no      no     yes     no\n#> 5          yes    yes      no      no     yes    yes\n#> 6           no     no      no      no      no     no\n#>                           industry\n#> 1                    manufacturing\n#> 2                    manufacturing\n#> 3                    manufacturing\n#> 4                    manufacturing\n#> 5 health/education/social services\n#> 6                            trade"},{"path":"r-essentials.html","id":"simulating-data","chapter":"3 R essentials","heading":"3.6.2 Simulating data","text":"Simulating data key skill telling believable stories data. order simulate data, need able randomly draw statistical distributions collections. R variety functions make easier, including: normal distribution, rnorm(), uniform distribution, runif(), Poisson distribution, rpois, binomial distribution, rbinom, many others. randomly sample vector, can use sample().dealing randomness, need reproducibility makes important, paradoxically, randomness repeatable. say another person needs able draw random numbers draw. setting seed random draws using set.seed().get observations standard normal distribution put data frame.add draws uniform, Poisson, binomial distributions, using cbind() bring columns original dataset new one together.Finally, sample() enables can draw vector items. instance, add favourite colour observation.set option ‘replace’ ‘TRUE’ choosing two items, time choose want option pick colours. Depending simulation may need think whether ‘replace’ ‘TRUE’ ‘FALSE.’ Another useful optional argument sample() adjust probability item drawn. default options equally likely, specify particular probabilities wanted ‘prob.’ always functions, can find help file, instance ?sample.","code":"\nset.seed(853)\n\nnumber_of_observations <- 5\n\nsimulated_data <- \n  data.frame(\n    person = c(1:number_of_observations),\n    std_normal_observations = rnorm(n = number_of_observations,\n                                    mean = 0,\n                                    sd = 1)\n    )\n\nsimulated_data\n#>   person std_normal_observations\n#> 1      1             -0.35980342\n#> 2      2             -0.04064753\n#> 3      3             -1.78216227\n#> 4      4             -1.12242282\n#> 5      5             -1.00278400\nsimulated_data <- \n  data.frame(\n    uniform_observations = runif(n = number_of_observations, min = 0, max = 10),\n    poisson_observations = rpois(n = number_of_observations, lambda = 100),\n    binomial_observations = rbinom(n = number_of_observations, size = 2, prob = 0.5)\n    ) |>\n  cbind(simulated_data)\n\nsimulated_data\n#>   uniform_observations poisson_observations\n#> 1            9.6219155                   81\n#> 2            7.2269016                   91\n#> 3            0.8252921                   84\n#> 4            1.0379810                  100\n#> 5            3.0942004                   97\n#>   binomial_observations person std_normal_observations\n#> 1                     2      1             -0.35980342\n#> 2                     1      2             -0.04064753\n#> 3                     1      3             -1.78216227\n#> 4                     1      4             -1.12242282\n#> 5                     1      5             -1.00278400\nsimulated_data <- \n  data.frame(\n    favourite_color = sample(x = c(\"blue\", \" white \"), \n                             size = number_of_observations,\n                             replace = TRUE)\n    ) |>\n  cbind(simulated_data)\n\nsimulated_data\n#>   favourite_color uniform_observations poisson_observations\n#> 1            blue            9.6219155                   81\n#> 2            blue            7.2269016                   91\n#> 3            blue            0.8252921                   84\n#> 4          white             1.0379810                  100\n#> 5            blue            3.0942004                   97\n#>   binomial_observations person std_normal_observations\n#> 1                     2      1             -0.35980342\n#> 2                     1      2             -0.04064753\n#> 3                     1      3             -1.78216227\n#> 4                     1      4             -1.12242282\n#> 5                     1      5             -1.00278400"},{"path":"r-essentials.html","id":"functions","chapter":"3 R essentials","heading":"3.6.3 Functions","text":"R ‘functional programming language’ (Wickham 2019a). means foundationally write, use, compose functions, collections code accomplish something specific.lot functions R people written, can use. Almost common statistical data science task might need accomplish likely already function written someone else made available us, either part base R installation package. need write functions time time, especially -specific tasks.can define function using function() assign name. likely need include inputs outputs function. Inputs specified round brackets. specific task function accomplish goes braces.can specify defaults inputs case person using function supply .","code":"\nprint_names <- function(some_names) {\n  print(some_names)\n}\n\nprint_names(c(\"rohan\", \"monica\"))\n#> [1] \"rohan\"  \"monica\"\nprint_names <- function(some_names = c(\"edward\", \"hugo\")) {\n  print(some_names)\n}\n\nprint_names()\n#> [1] \"edward\" \"hugo\""},{"path":"r-essentials.html","id":"making-graphs-with-ggplot2","chapter":"3 R essentials","heading":"3.7 Making graphs with ggplot2","text":"key package tidyverse terms manipulating data dplyr (Wickham et al. 2020), key package tidyverse terms creating graphs ggplot2 (Wickham 2016). part tidyverse collection packages need explicitly installed loaded tidyverse loaded.similar way pipe operator, ggplot2 works passing outputs one line inputs another line. formally, defining layers graph, based around ‘grammar graphics’ (hence, ‘gg’). Instead pipe operator (|>) ggplot uses add operator +.three key aspects need specified build graph using ggplot2:data;aesthetics / mapping; andtype.get started obtain data GDP OECD countries (OECD 2022).interested, firstly, building histogram GDP change third quarter 2021 twelve countries: Australia, Canada, Chile, Germany, Great Britain, Indonesia, India, Japan, New Zealand, South Africa, Spain, US.start ggplot function, specify mapping, case means specifying x-axis y-axis.Now need specify type graph interested . case want bar chart adding geom_bar().Now can color bars whether country European adding another aesthetic, ‘fill.’Finally, make look nicer, adding labels labs(), change color scale_fill_brewer(), background theme_classic.Facets mean create subplots focus specific aspects data. invaluable allow us add another variable graph without make 3D graph. use facet_wrap() add facet specify variable like facet .","code":"\nlibrary(tidyverse)\n\noecd_gdp <- \n  read_csv(\"https://stats.oecd.org/sdmx-json/data/DP_LIVE/.QGDP.../OECD?contentType=csv&detail=code&separator=comma&csv-lang=en\")\n\nwrite_csv(oecd_gdp, 'inputs/data/oecd_gdp.csv')#> # A tibble: 6 × 8\n#>   LOCATION INDICATOR SUBJECT MEASURE  FREQUENCY TIME  Value\n#>   <chr>    <chr>     <chr>   <chr>    <chr>     <chr> <dbl>\n#> 1 OECD     QGDP      TOT     PC_CHGPP A         1962   5.70\n#> 2 OECD     QGDP      TOT     PC_CHGPP A         1963   5.20\n#> 3 OECD     QGDP      TOT     PC_CHGPP A         1964   6.38\n#> 4 OECD     QGDP      TOT     PC_CHGPP A         1965   5.35\n#> 5 OECD     QGDP      TOT     PC_CHGPP A         1966   5.75\n#> 6 OECD     QGDP      TOT     PC_CHGPP A         1967   3.96\n#> # … with 1 more variable: Flag Codes <chr>\noecd_gdp_most_recent <- \n  oecd_gdp |> \n  filter(TIME == \"2021-Q3\",\n         SUBJECT == \"TOT\",\n         LOCATION %in% c(\"AUS\", \"CAN\", \"CHL\", \"DEU\",\n                         \"GBR\", \"IDN\", \"IND\", \"ESP\", \n                         \"JPN\", \"NZL\", \"USA\", \"ZAF\"),\n         MEASURE == \"PC_CHGPY\") |> \n  mutate(european = if_else(LOCATION %in% c(\"DEU\", \"GBR\", \"ESP\"),\n                             \"European\",\n                             \"Not european\"),\n         hemisphere = if_else(LOCATION %in% c(\"CAN\", \"DEU\", \"GBR\", \"ESP\", \"JPN\", \"USA\"),\n                             \"Northern Hemisphere\",\n                             \"Southern Hemisphere\"),\n         )\noecd_gdp_most_recent |> \n  ggplot(mapping = aes(x = LOCATION, y = Value))\noecd_gdp_most_recent |> \n  ggplot(mapping = aes(x = LOCATION, y = Value)) +\n  geom_bar(stat=\"identity\")\noecd_gdp_most_recent |> \n  ggplot(mapping = aes(x = LOCATION, y = Value, fill = european)) +\n  geom_bar(stat=\"identity\")\noecd_gdp_most_recent |> \n  ggplot(mapping = aes(x = LOCATION, y = Value, fill = european)) +\n  geom_bar(stat=\"identity\") + \n  labs(title = \"Quarterly change in GDP for twelve OECD countries in 2021Q3\", \n       x = \"Countries\", \n       y = \"Change (%)\",\n       fill = \"Is European?\") +\n  theme_classic() +\n  scale_fill_brewer(palette = \"Set1\")\noecd_gdp_most_recent |> \n  ggplot(mapping = aes(x = LOCATION, y = Value, fill = european)) +\n  geom_bar(stat=\"identity\") + \n  labs(title = \"Quarterly change in GDP for six OECD countries in 2021Q3\", \n       x = \"Countries\", \n       y = \"Change (%)\",\n       fill = \"Is European?\") +\n  theme_classic() +\n  scale_fill_brewer(palette = \"Set1\") +\n  facet_wrap(~hemisphere, \n              scales = \"free_x\")"},{"path":"r-essentials.html","id":"exploring-the-tidyverse","chapter":"3 R essentials","heading":"3.8 Exploring the tidyverse","text":"focused two aspects tidyverse: dplyr, ggplot2. However, tidyverse comprises variety different packages functions. now go four common aspects:Importing data tibble()Joining pivoting datasetsString manipulation stringrFactor variables forcats","code":""},{"path":"r-essentials.html","id":"importing-data-and-tibble","chapter":"3 R essentials","heading":"3.8.1 Importing data and tibble()","text":"variety ways get data R can use . CSV files, read_csv() readr (Wickham, Hester, Bryan 2021), dta files, read_dta() haven (Wickham Miller 2020).CSVs common format many advantages including fact typically modify data. field separated comma, row record. can provide read_csv() URL local file read. variety different options can passed function including ability specify whether dataset column names, types columns, many lines skip. specify types columns read_csv() make guess looking dataset.use read_dta() read .dta files, commonly produced statistical program Stata. means common fields sociology, political science, economics. format separates data labels typically need reunite using labelled::to_factor(). haven part tidyverse, automatically loaded default, contrast package ggplot2, need run library(haven).Typically dataset enters R ‘data.frame.’ can useful, another helpful class dataset ‘tibble.’ can created using tibble() tibble package part tidyverse. tibble data frame, particular changes make easier work , including converting strings factors default, showing class columns, printing nicely.can make tibble manually, need , instance, simulate data. typically import data directly tibble, instance, use read_csv().","code":"\npeople_as_dataframe <- \n  data.frame(names = c(\"rohan\", \"monica\"),\n             website = c(\"rohanalexander.com\", \"monicaalexander.com\"),\n             fav_colour = c(\"blue\", \" white \")\n             )\nclass(people_as_dataframe)\n#> [1] \"data.frame\"\npeople_as_dataframe\n#>    names             website fav_colour\n#> 1  rohan  rohanalexander.com       blue\n#> 2 monica monicaalexander.com     white\n\npeople_as_tibble <- \n  tibble(names = c(\"rohan\", \"monica\"),\n         website = c(\"rohanalexander.com\", \"monicaalexander.com\"),\n         fav_colour = c(\"blue\", \" white \")\n         )\npeople_as_tibble\n#> # A tibble: 2 × 3\n#>   names  website             fav_colour\n#>   <chr>  <chr>               <chr>     \n#> 1 rohan  rohanalexander.com  \"blue\"    \n#> 2 monica monicaalexander.com \" white \"\nclass(people_as_tibble)\n#> [1] \"tbl_df\"     \"tbl\"        \"data.frame\""},{"path":"r-essentials.html","id":"dataset-manipulation-with-joins-and-pivots","chapter":"3 R essentials","heading":"3.8.2 Dataset manipulation with joins and pivots","text":"two common types dataset manipulations often needed: joins pivots.often situation two, , datasets interested combining . can join datasets together variety ways. common way use left_join() dplyr. useful one main dataset using another dataset useful variables want add . critical aspect common column, potentially columns, can use link two datasets.create two tibbles join .want join another based common column names. ’ll join two datasets based favourite colour.variety options join datasets, including inner_join(), right_join() full_join().Another common dataset manipulation task need pivot . Datasets tend either long wide. Generally, tidyverse, certainly ggplot2, need long data. go one pivot_longer() pivot_wider() tidyr (Wickham 2021).create wide data whether red team blue team won competition year.dataset wide format moment. get long format, need column specifies team, another specifies result. use pivot_longer() achieve .Occasionally, need go long data wide data. can use pivot_wider() .","code":"\nmain_dataset <- \n  tibble(\n    names = c('rohan', 'monica', 'edward', 'hugo'),\n    status = c('adult', 'adult', 'child', 'infant')\n  )\nmain_dataset\n#> # A tibble: 4 × 2\n#>   names  status\n#>   <chr>  <chr> \n#> 1 rohan  adult \n#> 2 monica adult \n#> 3 edward child \n#> 4 hugo   infant\n\nsupplementary_dataset <- \n  tibble(\n    names = c('rohan', 'monica', 'edward', 'hugo'),\n    favourite_food = c('pasta', 'salmon', 'pizza', 'milk')\n  )\nsupplementary_dataset\n#> # A tibble: 4 × 2\n#>   names  favourite_food\n#>   <chr>  <chr>         \n#> 1 rohan  pasta         \n#> 2 monica salmon        \n#> 3 edward pizza         \n#> 4 hugo   milk\n\nmain_dataset <- \n  main_dataset |> \n  left_join(supplementary_dataset, by = \"names\")\n\nmain_dataset\n#> # A tibble: 4 × 3\n#>   names  status favourite_food\n#>   <chr>  <chr>  <chr>         \n#> 1 rohan  adult  pasta         \n#> 2 monica adult  salmon        \n#> 3 edward child  pizza         \n#> 4 hugo   infant milk\npivot_example_data <- \n  tibble(year = c(2019, 2020, 2021),\n         blue_team = c(\"first\", \"second\", \"first\"),\n         red_team = c(\"second\", \"first\", \"second\"))\n\nhead(pivot_example_data)\n#> # A tibble: 3 × 3\n#>    year blue_team red_team\n#>   <dbl> <chr>     <chr>   \n#> 1  2019 first     second  \n#> 2  2020 second    first   \n#> 3  2021 first     second\ndata_pivoted_longer <- \n  pivot_example_data |> \n  tidyr::pivot_longer(cols = c(\"blue_team\", \"red_team\"),\n              names_to = \"team\",\n               values_to = \"position\")\n\nhead(data_pivoted_longer)\n#> # A tibble: 6 × 3\n#>    year team      position\n#>   <dbl> <chr>     <chr>   \n#> 1  2019 blue_team first   \n#> 2  2019 red_team  second  \n#> 3  2020 blue_team second  \n#> 4  2020 red_team  first   \n#> 5  2021 blue_team first   \n#> 6  2021 red_team  second\ndata_pivoted_wider <- \n  data_pivoted_longer |> \n  tidyr::pivot_wider(id_cols = c(\"year\", \"team\"),\n                     names_from = \"team\",\n                     values_from = \"position\")\n\nhead(data_pivoted_wider)\n#> # A tibble: 3 × 3\n#>    year blue_team red_team\n#>   <dbl> <chr>     <chr>   \n#> 1  2019 first     second  \n#> 2  2020 second    first   \n#> 3  2021 first     second"},{"path":"r-essentials.html","id":"string-manipulation-and-stringr","chapter":"3 R essentials","heading":"3.8.3 String manipulation and stringr","text":"R often create string double quotes, although using single quotes fine . instance c(\"\", \"b\") consists two strings ‘’ ‘b,’ contained character vector. variety ways manipulate strings R focus stringr package (Wickham 2019e). automatically loaded load tidyverse.want look whether string contains certain content, can use str_detect(). want remove change particular content can use str_remove() str_replace().variety functions often especially useful data cleaning. instance, can use str_length() find long string , str_c() bring strings together.Finally, separate() tidyr, although part stringr, indespensible function string manipulation. turns one character column many.","code":"\ndataset_of_strings <- \n  tibble(\n    names = c(\"rohan alexander\", \n              \"monica alexander\", \n              \"edward alexander\", \n              \"hugo alexander\")\n  )\n\ndataset_of_strings |> \n  mutate(is_rohan = str_detect(names, \"rohan\"),\n         make_howlett = str_replace(names, \"alexander\", \"howlett\"),\n         remove_rohan = str_remove(names, \"rohan\")\n         )\n#> # A tibble: 4 × 4\n#>   names            is_rohan make_howlett   remove_rohan     \n#>   <chr>            <lgl>    <chr>          <chr>            \n#> 1 rohan alexander  TRUE     rohan howlett  \" alexander\"     \n#> 2 monica alexander FALSE    monica howlett \"monica alexande…\n#> 3 edward alexander FALSE    edward howlett \"edward alexande…\n#> 4 hugo alexander   FALSE    hugo howlett   \"hugo alexander\"\ndataset_of_strings |> \n  mutate(length_is = str_length(string = names),\n         remove_rohan = str_c(names, length_is, sep = \" - \")\n         )\n#> # A tibble: 4 × 3\n#>   names            length_is remove_rohan         \n#>   <chr>                <int> <chr>                \n#> 1 rohan alexander         15 rohan alexander - 15 \n#> 2 monica alexander        16 monica alexander - 16\n#> 3 edward alexander        16 edward alexander - 16\n#> 4 hugo alexander          14 hugo alexander - 14\ndataset_of_strings |> \n  separate(col = names,\n           into = c(\"first\", \"last\"),\n           sep = \" \",\n           remove = FALSE)\n#> # A tibble: 4 × 3\n#>   names            first  last     \n#>   <chr>            <chr>  <chr>    \n#> 1 rohan alexander  rohan  alexander\n#> 2 monica alexander monica alexander\n#> 3 edward alexander edward alexander\n#> 4 hugo alexander   hugo   alexander"},{"path":"r-essentials.html","id":"factor-variables-and-forcats","chapter":"3 R essentials","heading":"3.8.4 Factor variables and forcats","text":"factor collection strings categories. Sometimes inherent ordering. instance, days week order—Monday, Tuesday, Wednesday, …—alphabetical. inherent order gender—female, male, . Factors feature prominently base R. can useful ensure appropriate strings allowed. instance, ‘days_of_the_week’ factor variable ‘January’ allowed . can add great deal complication, less prominent role tidyverse. Nonetheless taking advantage factors useful certain circumstances. instance, plotting days week probably want usual ordering alphabetical ordering result character variable. factors built base R, one tidyverse package especially useful using factors forcats (Wickham 2020a).Sometimes character vector, want ordered particular way. default character vector ordered alphabetically, may want . instance, days week look strange graph alphabetically ordered: Friday, Monday, Saturday, Sunday, Thursday, Tuesday, Wednesday!way change ordering change variable character factor. can use fct_relevel() forcats specify ordering.can compare results graphing first original character vector x-axis, another graph factor vector x-axis.","code":"\nset.seed(853)\n\ndays_data <-\n  tibble(\n    days =\n      c(\n        \"Monday\",\n        \"Tuesday\",\n        \"Wednesday\",\n        \"Thursday\",\n        \"Friday\",\n        \"Saturday\",\n        \"Sunday\"\n      ),\n    some_value = c(sample.int(100, 7))\n  )\n\ndays_data <-\n  days_data |>\n  mutate(\n    days_as_factor = factor(days),\n    days_as_factor = fct_relevel(\n      days,\n      \"Monday\",\n      \"Tuesday\",\n      \"Wednesday\",\n      \"Thursday\",\n      \"Friday\",\n      \"Saturday\",\n      \"Sunday\"\n    )\n  )\ndays_data |> \n  ggplot(aes(x = days, y = some_value)) +\n  geom_point()\n\ndays_data |> \n  ggplot(aes(x = days_as_factor, y = some_value)) +\n  geom_point()"},{"path":"r-essentials.html","id":"exercises-and-tutorial-2","chapter":"3 R essentials","heading":"3.9 Exercises and tutorial","text":"","code":""},{"path":"r-essentials.html","id":"exercises-2","chapter":"3 R essentials","heading":"3.9.1 Exercises","text":"one four challenges mitigating bias mentioned Hao (2019) (pick one)?\nUnknown unknowns.\nImperfect processes.\ndefinitions fairness.\nLack social context.\nDisinterest given profit considerations.\nUnknown unknowns.Imperfect processes.definitions fairness.Lack social context.Disinterest given profit considerations.R?\nopen-source statistical programming language\nprogramming language created Guido van Rossum\nclosed source statistical programming language\nintegrated development environment (IDE)\nopen-source statistical programming languageA programming language created Guido van RossumA closed source statistical programming languageAn integrated development environment (IDE)three advantages R? three disadvantages?R Studio?\nintegrated development environment (IDE).\nclosed source paid program.\nprogramming language created Guido van Rossum\nstatistical programming language.\nintegrated development environment (IDE).closed source paid program.programming language created Guido van RossumA statistical programming language.class output ‘2 + 2’ (pick one)?\ncharacter\nfactor\nnumeric\ndate\ncharacterfactornumericdateSay run: my_name <- 'Rohan'. result running print(my_name) (pick one)?\n‘Edward’\n‘Monica’\n‘Hugo’\n‘Rohan’\n‘Edward’‘Monica’‘Hugo’‘Rohan’Say dataset two columns: ‘name,’ ‘age.’ verb use pick just ‘name’ (pick one)?\ntidyverse::select()\ntidyverse::mutate()\ntidyverse::filter()\ntidyverse::rename()\ntidyverse::select()tidyverse::mutate()tidyverse::filter()tidyverse::rename()Say loaded AustralianPoliticians tidyverse run following code: australian_politicians <- AustralianPoliticians::get_auspol(''). select columns end ‘Name’ (pick one)?\naustralian_politicians |> select(contains(\"Name\"))\naustralian_politicians |> select(starts_with(\"Name\"))\naustralian_politicians |> select(matches(\"Name\"))\naustralian_politicians |> select(ends_with(\"Name\"))\naustralian_politicians |> select(contains(\"Name\"))australian_politicians |> select(starts_with(\"Name\"))australian_politicians |> select(matches(\"Name\"))australian_politicians |> select(ends_with(\"Name\"))circumstances, terms names columns, use ‘contains()’ potentially give different answers using ‘ends_with()’ question?following tidyverse verbs (pick one)?\nselect()\nfilter()\narrange()\nmutate()\nvisualize()\nselect()filter()arrange()mutate()visualize()wanted make new column verb use (pick one)?\nselect()\nfilter()\narrange()\nmutate()\nvisualize()\nselect()filter()arrange()mutate()visualize()wanted focus particular rows verb use (pick one)?\nselect()\nfilter()\narrange()\nmutate()\nsummarise()\nselect()filter()arrange()mutate()summarise()wanted summary data gave mean sex, two verbs use (pick one)?\nsummarise()\nfilter()\narrange()\nmutate()\ngroup_by()\nsummarise()filter()arrange()mutate()group_by()Assume variable called ‘age’ integer. line code create column exponential (pick one)?\nmutate(exp_age = exponential(age))\nmutate(exp_age = exponent(age))\nmutate(exp_age = exp(age))\nmutate(exp_age = expon(age))\nmutate(exp_age = exponential(age))mutate(exp_age = exponent(age))mutate(exp_age = exp(age))mutate(exp_age = expon(age))Assume column called ‘age.’ line code create column contains value five rows ?\nmutate(five_before = lag(age))\nmutate(five_before = lead(age))\nmutate(five_before = lag(age, n = 5))\nmutate(five_before = lead(age, n = 5))\nmutate(five_before = lag(age))mutate(five_before = lead(age))mutate(five_before = lag(age, n = 5))mutate(five_before = lead(age, n = 5))output class('edward') (pick one)?\n‘numeric’\n‘character’\n‘data.frame’\n‘vector’\n‘numeric’‘character’‘data.frame’‘vector’function enable us draw three options ‘blue, white, red,’ 10 per cent probability ‘blue’ ‘white,’ remainder ‘red?’\nsample(c('blue', 'white', 'red'), prob = c(0.1, 0.1, 0.8))\nsample(c('blue', 'white', 'red'), size = 1)\nsample(c('blue', 'white', 'red'), size = 1, prob = c(0.8, 0.1, 0.1))\nsample(c('blue', 'white', 'red'), size = 1, prob = c(0.1, 0.1, 0.8))\nsample(c('blue', 'white', 'red'), prob = c(0.1, 0.1, 0.8))sample(c('blue', 'white', 'red'), size = 1)sample(c('blue', 'white', 'red'), size = 1, prob = c(0.8, 0.1, 0.1))sample(c('blue', 'white', 'red'), size = 1, prob = c(0.1, 0.1, 0.8))can simulate 10,000 draws normal distribution mean 27 standard deviation 3 (pick one)?\nrnorm(10000, mean = 27, sd = 3)\nrnorm(27, mean = 10000, sd = 3)\nrnorm(3, mean = 10000, sd = 27)\nrnorm(27, mean = 3, sd = 1000)\nrnorm(10000, mean = 27, sd = 3)rnorm(27, mean = 10000, sd = 3)rnorm(3, mean = 10000, sd = 27)rnorm(27, mean = 3, sd = 1000)three key aspects grammar graphics (select )?\ndata\naesthetics\ntype\ngeom_histogram()\ndataaestheticstypegeom_histogram()","code":""},{"path":"r-essentials.html","id":"tutorial-2","chapter":"3 R essentials","heading":"3.9.2 Tutorial","text":"Listen Gladwell rank sports teams departments something else interest based least three variables one peer thoughts.","code":""},{"path":"workflow.html","id":"workflow","chapter":"4 Workflow","heading":"4 Workflow","text":"Required materialRead Good enough practices scientific computing, (J. . C. Wilson Greg Bryan 2017)Read happened winds changed, (Gelman 2016)Watch Overcoming barriers sharing code, (M. Alexander 2021)Read Six steps better relationship future self, (Bowers 2011)Read Happy Git GitHub useR, Chapters 4, 6, 7, 9, (Jenny Bryan 2020a)Read Forgot Teach R, Chapters 1-5, 11, 12, 13 (Jennifer Bryan Hester 2020)Watch Make reprex… Please, (Gelfand 2021)Watch Object type ‘closure’ subsettable, (Jenny Bryan 2020b)Key concepts/skills/etcRestart R often (Session -> Restart R Clear Output).Debugging skill, get better time practice.Start reading error message.Check class.may get frustrated times, normal.various tools can help. Google friend.Make small example try get code running .Cultivating tenacious mentality may help.Writing code future-can understand.Developing important questions.Reproducibility replicabilityThe importance data code accessThe importance version control modern scientific workflow.basics Git GitHub, solo data scientist.Key GitHub workflow commandsGet latest changes: git pull.Add updates: git add -.Check everything: git status.Commit changes: git commit -m \"Short description changes\".Push changes GitHub: git push.","code":""},{"path":"workflow.html","id":"introduction","chapter":"4 Workflow","heading":"4.1 Introduction","text":"Suppose cancer choose black box AI surgeon explain works 90% cure rate human surgeon 80% cure rate. want AI surgeon illegal?Geoffrey Hinton, 20 February 2020.number one thing keep mind machine learning performance evaluated samples one dataset, model used production samples may necessarily follow characteristics… finance industry saying : “past performance guarantee future results.” model scoring X test dataset doesn’t mean perform level X next N situations encounters real world. future may like past.asking question, “rather use model evaluated 90% accurate, human evaluated 80% accurate,” answer depends whether data typical per evaluation process. Humans adaptable, models . significant uncertainty involved, go human. may inferior pattern recognition capabilities (versus models trained enormous amounts data), understand , can reason , can improvise faced noveltyIf every possible situation known want prioritize scalability cost-reduction, go model. Models exist encode operationalize human cognition well-understood situations. (“well understood” meaning either can explicitly described programmer, can amass dataset densely samples distribution possible situations – must static)François Chollet, 20 February 2020.science systematically building organising knowledge terms testable explanations predictions, data science takes focuses data. means building, organising, sharing knowledge critical aspect. Creating knowledge, , way can , meet standard. Hence, focus reproducible workflows data science.M. Alexander (2019a) says ‘research reproducible can reproduced exactly, given materials used study… [hence] materials need provided!… [M]aterials usually means data, code software.’ minimum requirement another person able ‘reproduce data, methods results (including figures, tables).’ Similarly, Gelman (2016) identifies large issue various sciences, psychology. problem work reproducible, contribute stock knowledge world. around time Gelman (2016) written, got point longer knew facts psychology. Since time, great deal work done psychology, situation improved. remains issue many fields, especially social sciences.examples Gelman (2016) talks turned dodgy really matter. instance, extrasensory perception (ESP), power pose. time, saw similar approaches used areas big impacts. instance, many governments created ‘nudge’ units implement public policy. increasingly methods applied areas matter, instance, ‘nudge’ units. big problem -data sciences.minimum, work must release code dataset, potentially various restrictions dataset governed external party authors. Without data, know finding speaks understand representative sample . dataset biased, undermine claims. reason initial medical trials often done mice animals, eventually human trials required.specific, consider Y. Wang Kosinski (2018), authors used deep neural networks train model distinguish gay heterosexual men (Murphy (2017) provides summary paper associated issues, along comments authors). , Y. Wang Kosinski (2018, 248) needed photos dataset ‘adult, Caucasian, fully visible, gender matched one reported user’s profile’ verified using Amazon Mechanical Turk, online platform pays workers small amount money complete specific tasks. Figure 4.1 Y. Wang Kosinski (2018) supplemental material, shows instructions provided Mechanical Turk workers task.\nFigure 4.1: Instructions given Mechanical Turk workers removing incomplete, non-Caucasian, nonadult, nonhuman male faces, Y. Wang Kosinski (2018) supplemental material\nissues instructions include Obama white mother black father, classified ‘Black’; Latino ethnicity, rather race (Mattson 2017).discussing one specific concern one part workflow Y. Wang Kosinski (2018). Broader concerns raised others including Gelman, Mattson, Simpson (2018). main issue statistical models specific data trained. reason able identify likely issues model trained Y. Wang Kosinski (2018) , despite releasing specific dataset used, nonetheless open procedure. fundamental issue order work credible, needs reproducible others.steps can take make work reproducible include:Ensure entire workflow documented. raw dataset obtained access likely persistent available others? specific steps taken transform raw data data analysed, can made available others? analysis done, clearly can shared? final paper report built extent can others follow process ?worrying perfect reproducibility initially, instead focusing trying improve successive project. instance, following requirements increasingly onerous need concerned able last, can first:\nCan run entire workflow ?\nCan ‘another person’ run entire workflow ?\nCan future run entire workflow ?\nCan future ‘another person’ run entire workflow ?\nCan run entire workflow ?Can ‘another person’ run entire workflow ?Can future run entire workflow ?Can future ‘another person’ run entire workflow ?Including detailed discussion limitations dataset approach final paper report.","code":""},{"path":"workflow.html","id":"r-markdown","chapter":"4 Workflow","heading":"4.2 R Markdown","text":"","code":""},{"path":"workflow.html","id":"getting-started-1","chapter":"4 Workflow","heading":"4.2.1 Getting started","text":"R Markdown mark-language similar HyperText Markup Language (HTML) LaTeX, comparison ‘See Get’ (WYSIWYG) language, Microsoft Word. means aspects consistent, instance, top-level heading look . , means must use symbols designate like certain aspects appear, build compile mark-get see looks like.R Markdown variant Markdown specifically designed allow R code chunks included. One advantage can get ‘live’ document code executes printed document. Another advantage R Markdown similar code can compile variety documents, including html pages PDFs. R Markdown also default options set including title, author, date sections. One disadvantage can take document compile code needs run.can create new R Markdown document within R Studio (File -> New File -> R Markdown Document).","code":""},{"path":"workflow.html","id":"basic-commands","chapter":"4 Workflow","heading":"4.2.2 Basic commands","text":"Basic command including emphasis, headers, lists, links, images. reminder included R Studio (Help -> Markdown Quick Reference).Emphasis: *italic*, **bold**Headers (need go line line ): # First level header, ## Second level header, ### Third level headerUnordered list, sub-lists:Ordered list, sub-lists:URLs can added just including address auto-link: https://www.tellingstorieswithdata.com, linking text [address book](https://www.tellingstorieswithdata.com).added aspects, may want see actual document. build document click ‘Knit.’","code":"* Item 1\n* Item 2\n    + Item 2a\n    + Item 2b1. Item 1\n2. Item 2\n3. Item 3\n    + Item 3a\n    + Item 3b"},{"path":"workflow.html","id":"r-chunks","chapter":"4 Workflow","heading":"4.2.3 R chunks","text":"can include code R many languages code chunks within R Markdown document. knit document, code run included document.create R chunk, start three backticks within curly braces tell R Markdown R chunk. Anything inside chunk considered R code run . example chunk loads tidyverse AER makes graph number times survey respondent visited doctor past two weeks follows.output code Figure 4.2.\nFigure 4.2: Number doctor visits past two weeks, based 1977–1978 Australian Health Survey\nvarious evaluation options available chunks. include putting comma r specifying options closing curly brace. Helpful options include:echo = FALSE: run code include output, print code document.include = FALSE: run code output anything print code document.eval = FALSE: run code, hence include outputs, print code document.warning = FALSE: display warnings.message = FALSE: display messages.example include output, code, also suppress warnings follows.","code":"```{r}\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits %>%\n  ggplot(aes(x = visits)) +\n  geom_histogram(stat = \"count\")\n``````{r, echo = FALSE, warning = FALSE}\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits %>%\n  ggplot(aes(x = visits)) +\n  geom_histogram(stat = \"count\")\n```"},{"path":"workflow.html","id":"abstracts-and-pdf-outputs","chapter":"4 Workflow","heading":"4.2.4 Abstracts and PDF outputs","text":"abstract short summary paper. default preamble, can add section abstract. Similarly, can change output html_document pdf_document produce PDF. uses LaTeX background may require installation supporting packages.","code":"---\ntitle: My document\nauthor: Rohan Alexander\ndate: 1 January 2022\noutput: pdf_document\nabstract: \"This is my abstract.\"\n---"},{"path":"workflow.html","id":"references","chapter":"4 Workflow","heading":"4.2.5 References","text":"can include references specifying bib file preamble calling within text, needed.need make separate file called ‘bibliography.bib.’ need entry item referenced. citation R can obtained citation() can added ‘bibliography.bib’ file, similarly citation package can found including package name, instance citation('tidyverse').need create unique key use refer item text. can anything, provided unique, meaningful ones can easier remember, instance ‘citeR.’cite R R Markdown document include @citeR, put brackets around year, like : R Core Team (2021) [@citeR], put brackets around whole thing, like : (Wickham et al. 2019a).","code":"---\ntitle: My document\nauthor: Rohan Alexander\ndate: 1 January 2022\noutput: pdf_document\nabstract: \"This is my abstract.\"\nbibliography: bibliography.bib\n---@Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2021},\n    url = {https://www.R-project.org/},\n  }\n@Article{,\n    title = {Welcome to the {tidyverse}},\n    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},\n    year = {2019},\n    journal = {Journal of Open Source Software},\n    volume = {4},\n    number = {43},\n    pages = {1686},\n    doi = {10.21105/joss.01686},\n  }@Manual{citeR,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2021},\n    url = {https://www.R-project.org/},\n  }\n@Article{citetidyverse,\n    title = {Welcome to the {tidyverse}},\n    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},\n    year = {2019},\n    journal = {Journal of Open Source Software},\n    volume = {4},\n    number = {43},\n    pages = {1686},\n    doi = {10.21105/joss.01686},\n  }"},{"path":"workflow.html","id":"cross-references","chapter":"4 Workflow","heading":"4.2.6 Cross-references","text":"can useful cross-reference figures, tables, equations. makes easier refer text. figure refer name R chunk creates contains figure. instance, (Figure \\@ref(fig:uniquename)) produce: (Figure 4.3) name R chunk uniquename. also need add ‘fig’ front chunk name R Markdown knows figure. include ‘fig.cap’ R chunk specifies caption.\nFigure 4.3: Number illnesses past two weeks, based 1977–1978 Australian Health Survey\ncan similar tables. instance, (Table \\@ref(tab:docvisittable)) produce: (Table 4.1). case specify ‘tab’ unique reference table, R Markdown knows table. tables need include caption main content, ‘caption,’ rather ‘fig.cap’ chunk option case figures.Table 4.1: Number visits doctor past two weeks, based 1977–1978 Australian Health SurveyFinally, can also cross-reference equations. need add tag (\\#eq:slope) reference. instance, use Equation \\@ref(eq:slope). produce Equation (4.1).\\[\\begin{equation}\nY = + b X \\tag{4.1}\n\\end{equation}\\]using cross-references, important R chunks simple labels. general, try keep names simple unique, possible, avoid punctuation just stick letters. use underbars names cause error.","code":"```{r uniquename, fig.cap = \"Number of illnesses in the past two weeks, based on the 1977--1978 Australian Health Survey\", echo = TRUE}\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits %>%\n  ggplot(aes(x = illness)) +\n  geom_histogram(stat = \"count\")\nDoctorVisits %>% \n  count(visits) %>% \n  knitr::kable(caption = \"Number of visits to the doctor in the past two weeks, based on the 1977--1978 Australian Health Survey\")\\begin{equation}\nY = a + b X (\\#eq:slope)\n\\end{equation}"},{"path":"workflow.html","id":"r-projects-and-file-structure","chapter":"4 Workflow","heading":"4.3 R projects and file structure","text":"can use R Studio create R project. means can keep files (data, analysis, report, etc) associated particular project together. create project, select ‘Click File’ -> ‘New Project,’ select ‘empty project,’ name project decide save . instance, project focused maternal mortality, may called ‘maternalmortality,’ might saved within folder projects.project created, new file extension ‘.RProj’ appear folder. example, folder R Project, example R Markdown document, appropriate file structure available: https://github.com/RohanAlexander/starter_folder. can downloaded : ‘Code’ -> ‘Download ZIP.’main advantage using R Project easily able reference files self-contained way. means others want reproduce work, know file references structure need changed. means files referenced relation ‘.Rproj’ file . instance, instead reading csv , say, \"~/Documents/projects/book/data/\" can just read book/data/. may someone else ‘projects’ folder, former work , latter .use R projects required meet minimal level reproducibility. use functions setwd(), computer-specific file paths, bind work computer way appropriate.variety ways set-folder. variant J. . C. Wilson Greg Bryan (2017) often useful shown example: https://github.com/RohanAlexander/starter_folder. ‘inputs’ folder contains raw data (never modified) literature related project (modified). ‘outputs’ folder contains data create using R, well paper writing. ‘scripts’ folder modifies raw data saves ‘outputs.’ Useful aspects include ‘README.md’ specify overview details project, LICENSE.","code":""},{"path":"workflow.html","id":"git-and-github","chapter":"4 Workflow","heading":"4.4 Git and GitHub","text":"use combination Git GitHub :enhance reproducibility work making easier share code data;make easier share work;improve workflow encouraging systematic approaches; andmake easier work teams.Git version control system. One way one often starts version control various versions one file: ‘first_go.R,’ ‘first_go-fixed.R,’ ‘first_go-fixed--mons-edits.R.’ soon becomes cumbersome. One often soon turns dates, instance: ‘2022-01-01-analysis.R,’ ‘2022-01-02-analysis.R,’ ‘2022-01-03-analysis.R,’ etc. keeps record can difficult search need go back, can difficult remember date change made. case, quickly gets unwieldy project regularly worked .Instead , use Git can just one version file, say, ‘analysis.R’ use Git keep record changes file, snapshot file given point time.determine Git takes snapshot, take snapshot, additionally include message saying changed snapshot last. way, ever one version file, history can easily searched.issue Git designed software developers. , works, can little ungainly non-developers (Figure 4.4).\nFigure 4.4: infamous response launch Dropbox 2007, trivialising use-case Dropbox, user’s approach probably work , probably folks.\nHence, GitHub, GitLab, various companies offer easier--use services build Git. introduce GitHub ‘far dominant code-hosting platform’ (Eghbal 2020, 21) built R Studio, options advantages.One challenging aspects Git terminology. Folders called ‘repos.’ Saving called ‘commit.’ One gets used eventually, initially feeling confused entirely normal.","code":""},{"path":"workflow.html","id":"git","chapter":"4 Workflow","heading":"4.4.1 Git","text":"need git check whether Git installed. Open R Studio, go Terminal, type following, enter/return.get version number, done (Figure 4.5).\nFigure 4.5: access Terminal within R Studio\nMac Git come pre-installed, Windows chance, Linux probably need guide. get version number, need install . follow instructions specific operating system Chapter 5 Jenny Bryan (2020a).Git, need tell username email. need Git adds information whenever take ‘snapshot,’ use Git’s language, whenever make commit., within Terminal, type following, replacing details , enter/return line.details enter public. various ways hide email address need GitHub provides instructions . , issues, need detailed instructions step, please see Chapter 7 Jenny Bryan (2020a).","code":"git --versiongit config --global user.name 'Rohan Alexander'\ngit config --global user.email 'rohan.alexander@utoronto.ca'\ngit config --global --list"},{"path":"workflow.html","id":"github","chapter":"4 Workflow","heading":"4.4.2 GitHub","text":"Now Git set-need set-GitHub. first step create account GitHub: https://github.com (Figure 4.6).\nFigure 4.6: GitHub sign-screen\nGitHub intuitive user experience world, now going make new folder (called ‘repo’ Git). Look ‘+’ top right, select ‘New Repository’ (Figure 4.7).\nFigure 4.7: Start process creating new repository\npoint can add sensible name repo. Leave public now, can always deleted later. check box ‘Initialize repository README.’ Leave ‘Add .gitignore’ set ‘None.’ , click ‘Create repository’ (Figure 4.8).\nFigure 4.8: Finish creating new repository\ntake us screen fairly empty, details need green ‘Clone Download’ button, can copy clicking clipboard (Figure 4.9).\nFigure 4.9: Get details new repository\nNow returning R Studio, open Terminal, use cd navigate directory want create folder. type following, replacing repo details , enter/return.point, new folder created can now use .use GitHub project actively working follow procedure:first thing almost always pull latest changes git pull. , open Terminal, navigate folder using cd. type git pull enter/return.can make change folder, instance, update README, save normal.done, need ‘add,’ ‘commit,’ ‘push.’\n, use cd navigate folder, type git status enter/return see changes (see reference change made).\ntype git add -enter/return. adds changes staging area.\ntype git status enter/return verify happy added.\ntype git commit -m \"Minor update README\" enter/return. message informative change made.\n, type git status enter/return check everything.\nFinally, type git push enter/return push everything GitHub.\n, use cd navigate folder, type git status enter/return see changes (see reference change made).type git add -enter/return. adds changes staging area.type git status enter/return verify happy added.type git commit -m \"Minor update README\" enter/return. message informative change made., type git status enter/return check everything.Finally, type git push enter/return push everything GitHub.summarise workflow (assuming already relevant folder):","code":"git clone https://github.com/RohanAlexander/test.gitgit pull\ngit status\ngit add -A\ngit status\ngit commit -m \"Short commit message\"\ngit status\ngit push"},{"path":"workflow.html","id":"using-github-within-r-studio-with-the-git-pane","chapter":"4 Workflow","heading":"4.4.3 Using GitHub within R Studio with the Git pane","text":"procedure just went useful better understand happening use Git GitHub can bit cumbersome. Usefully, GitHub built R Studio can use Git pane move away Terminal.Get started creating new repo GitHub, , copy repo information, . point, open R Studio, create new project using version control (‘Files’ -> ‘New Project’ -> ‘Version Control’ -> ‘Git,’ paste information repo). Follow rest set-naming project something sensible, saving somewhere sensible, clicking ‘Open new session,’ creating project. create new folder R Project Git-initialised linked GitHub repo.open R Project, ‘Git’ tab (Figure 4.10).\nFigure 4.10: Git pane R Studio\ncan use Git tab. , first want ‘pull,’ can clicking blue arrow. , want commit files changes, selecting ‘staged’ checkbox files like commit. ‘Commit.’ , want include message commit, typing message ‘Commit message’ box ‘Commit.’ Finally, can ‘Push.’ details workflow available Chapter 12 Jenny Bryan (2020a).","code":""},{"path":"workflow.html","id":"using-github-with-usethis","chapter":"4 Workflow","heading":"4.4.4 Using GitHub with usethis","text":"used Git pane R Studio reduce need use Terminal, remove need go GitHub set-new project. set Git , can improve workflow using usethis much work us (Wickham Bryan 2020).First, need check set-Git usethis::git_sitrep(). print information user. can use usethis::use_git_config() update username email address. instance,can create new R Project (‘File’ -> ‘New Project’ -> ‘New Directory’ -> ‘New Project’ -> Add name save sensible location click ‘Open new session.’)use usethis::use_git() initiate things. ask happy commit files.committed, can use usethis::use_github() push GitHub.","code":"\nlibrary(usethis)\n\nuse_git_config(user.name = \"Rohan Alexander\", user.email = \"rohan.alexander@utoronto.ca\")"},{"path":"workflow.html","id":"using-r-in-practice","chapter":"4 Workflow","heading":"4.5 Using R in practice","text":"","code":""},{"path":"workflow.html","id":"dealing-with-errors","chapter":"4 Workflow","heading":"4.5.1 Dealing with errors","text":"programming, eventually code break, say eventually, mean like probably 10 20 times day.Gelfand (2021)Everyone uses R, programming language matter, trouble find point. normal. Programming hard everyone struggles sometimes. point code run throw error. normal, happens everyone. Everyone gets frustrated.move forward develop strategies work issues. get errors steps worthwhile systematically going .getting error message, sometimes useful. Try read carefully see anything use .Try search, say Google, error message. can useful include ‘tidyverse’ ‘R’ search help make results appropriate. Sometimes Stack Overflow results can useful.Look help file function, putting ‘?’ function, instance, ?pivot_wider(). instance, common use slightly incorrect argument name format accidentally including string instead name vice versa.Check class object ensuring expected. using class(), instance, class(data_set$data_column).Look typos code.Restart R (‘Session’ -> ‘Restart R Clear Output’) load everything .Restart computer.Search trying , rather error, sure include ‘tidyverse’ ‘R’ search help make results appropriate. instance, ‘save PDF graph R made using ggplot.’ Sometimes relevant blog posts Stack Overflow answers help.Making small, self-contained, reproducible example ‘reprex’ see issue can isolated enable others help.generally, rarely possible , almost always helpful take break come back next day.","code":""},{"path":"workflow.html","id":"reproducible-examples","chapter":"4 Workflow","heading":"4.5.2 Reproducible examples","text":"one can advise help —one. one thing . Go .Rilke (1929)Asking help skill like . get better practice. important try say ‘doesn’t work,’ ‘tried everything,’ ‘code work,’ ‘error message, ?’ , general, possible help based comments, many possibilities. Instead, need make easy others help . involves steps.Provide small, self-contained, example data, code, detail going wrong.Document tried far—Stack Overflow pages looked quite ? R Studio Community pages tried?clear outcome like.Begin creating minimal REPRoducible EXample, ‘reprex.’ code contains needed reproduce error, needed. means code likely smaller, simpler, version nonetheless reproduces error.Sometimes process enables one solve problem. , gives someone else fighting chance able help. important recognise almost chance got problem someone addressed . likely main difficulty trying communicate trying happening, way allows others recognise . Developing tenacity important.develop reproducible examples, reprex package (Jennifer Bryan et al. 2019) especially useful. use :Load reprex package library(reprex).Highlight copy code giving issues.Run reprex() Console.code self-contained, preview Viewer. , error.need data reproduce error, use data built R. large number datasets built R can seen using library(help = \"datasets\"). possible, use common option ‘mtcars’ ‘faithful.’","code":""},{"path":"workflow.html","id":"mentality","chapter":"4 Workflow","heading":"4.5.3 Mentality","text":"(Y)ou real, valid, competent user programmer matter IDE develop tools use make work work (L)et’s break gates, ’s enough room everyoneSharla Gelfand, 10 March 2020.write code, programmer regardless , using , . traits one tends notice great programmers common.Focused: Often aim ‘learn R’ something similar tends problematic, real end point . tends efficient smaller, specific goals, ‘make histogram 2019 Canadian Election ggplot.’ something can focused achieved hours. issue goals nebulous, ‘want learn R,’ becomes easy get lost tangents, much difficult get help. can demoralizing lead folks quitting early.Curious: almost always useful just go. general, worst happens waste time. can rarely break something irreparably code. want know happens pass ‘vector’ instead ‘dataframe’ ggplot() just try .Pragmatic: time, can useful stick within reasonable bounds, just make one small change time. instance, say want run regressions, curious possibility using tidymodels package (Kuhn Wickham 2020) instead lm(). pragmatic way proceed just use one aspect tidymodels package initially make another change next time.Tenacious: , balancing act. always unexpected problems issues every project. one hand, persevering despite good tendency. hand, sometimes one need prepared give something seem like break-possible. mentors can useful tend better judge reasonable. also appropriate planning useful.Planned: almost always useful excessively plan going . instance, may want make histogram 2019 Canadian Election. plan steps needed even sketch step might implemented. instance, first step get data. packages might useful? might data ? back-plan data exist ?Done better perfect: various perfectionist tendencies certain extent, can useful initially try turn certain extent. first instance, just try write code works, especially early days. can always come back improve aspects . important actually ship. Ugly code gets job done, better beautiful code never finished.","code":""},{"path":"workflow.html","id":"code-comments","chapter":"4 Workflow","heading":"4.5.4 Code comments","text":"Comment code (Lee 2018).one way write code, especially R. However, general guidelines make easier even just working .Comment code.Comments R can added including # symbol. put comment start line, can midway . general, need comment every aspect code comment parts obvious. instance, read value may like comment coming .Comment code.comment something. trying achieve?Comment code.must comment explain weird things. Like removing specific row, say row 27, removing row? may seem obvious moment, future-six months won’t remember.Comment code.break code sections. instance, setting workspace, reading datasets, manipulating cleaning dataset, analysing datasets, finally producing tables figures. separated comments explaining going , sometimes separate files, depending length.Comment code.Additionally, top file important basic information, purpose file, pre-requisites dependencies, date, author contact information, finally red-flags todos.Comment code.least every R script needs preamble.","code":"#### Preamble ####\n# Purpose: Brief sentence about what this script does\n# Author: Your name\n# Data: The date it was written\n# Contact: Add your email\n# License: Think about how your code may be used\n# Pre-requisites: \n# - Maybe you need some data or some other script to have been run?\n\n\n#### Workspace setup ####\n# do not keep the install.packages line - just comment out if need be\n# Load libraries\nlibrary(tidyverse)\n\n# Read in the raw data. \nraw_data <- readr::read_csv(\"inputs/data/raw_data.csv\")\n\n\n#### Next section ####\n...\n"},{"path":"workflow.html","id":"exercises-and-tutorial-3","chapter":"4 Workflow","heading":"4.6 Exercises and tutorial","text":"","code":""},{"path":"workflow.html","id":"exercises-3","chapter":"4 Workflow","heading":"4.6.1 Exercises","text":"three features good research question (write paragraph two)?counterfactual (pick one)?\n-statements didn’t happen.\n-statements happen.\nStatements either true false.\nStatements neither true false.\n-statements didn’t happen.-statements happen.Statements either true false.Statements neither true false.hide warnings R Markdown R chunk (pick one)?\necho = FALSE\ninclude = FALSE\neval = FALSE\nwarning = FALSE\nmessage = FALSE\necho = FALSEinclude = FALSEeval = FALSEwarning = FALSEmessage = FALSEWhat reprex important able make one (select )?\nreproducible example enables error reproduced.\nreproducible example helps others help .\nreproducible example construction may solve problem.\nreproducible example demonstrates ’ve actually tried help .\nreproducible example enables error reproduced.reproducible example helps others help .reproducible example construction may solve problem.reproducible example demonstrates ’ve actually tried help .R Projects important (select )?\nhelp reproducibility.\nmake easier share code.\nmake workspace organized.\nneeds done.\nhelp reproducibility.make easier share code.make workspace organized.needs done.Consider sequence: ‘git pull, git status, ________, git status, git commit -m \"message\", git push.’ missing step (pick one)?\ngit add -.\ngit status.\ngit pull.\ngit push.\ngit add -.git status.git pull.git push.","code":""},{"path":"workflow.html","id":"tutorial-3","chapter":"4 Workflow","heading":"4.6.2 Tutorial","text":"Make first contribution: ‘First Contributions’ https://github.com/forwards/first-contributions.Write code exchange someone else. Another great way learn exchanging code others. Initially, just read give feedback . get bit confident run ’s code. efficiently ’ve ever improved R journey Monica try run code.","code":""},{"path":"on-writing.html","id":"on-writing","chapter":"5 On writing","heading":"5 On writing","text":"Required readingRandom stuff:https://psyarxiv.com/n32qw/https://en.wikipedia.org/wiki/Einstein–Szilárd_letter#/media/File:Einstein-Roosevelt-letter.pnghttps://www.w3.org/History/1989/proposal.htmlRequired viewingRecommended readingExamples well-written papersKey concepts/skills/etcKey librariesKey functions/etcQuiz","code":""},{"path":"on-writing.html","id":"introduction-1","chapter":"5 On writing","heading":"5.1 Introduction","text":"","code":""},{"path":"on-writing.html","id":"writing","chapter":"5 On writing","heading":"5.2 Writing","text":"indeed published anything commenced “Professor,” many crude effort, destroyed almost soon composed, got taste might ornamented redundant composition, come prefer plain homely.Currer Bell (aka Charlotte Brontë), Professor.’s say, ’s hear.Frank Luntz","code":""},{"path":"on-writing.html","id":"developing-research-questions","chapter":"5 On writing","heading":"5.3 Developing research questions","text":"qualitative quantitative approaches place, focus quantitative approaches. Qualitative research important well, often interesting work little —‘mixed methods.’ means subject issues surrounding data quality, scales, measures, sources, etc. especially interested trying tease causality.Broadly two ways go research:data-first,question-first.get job somewhere typically initially data-first. means need work questions can reasonably answer data available . show promise, may given latitude explore specific questions, possibly even gathering data specifically purpose. Contrast example Behavioural Insights Team, (Gertler et al. 2016, 23) got design carry experiments given remit entire British government (spun prime minister’s office).deciding questions can reasonably answer data available, need think :Theory: reasonable expectation something causal determined? Charting stock market - maybe, might better haruspex least way something eat. need reasonable theory \\(x\\) may affecting \\(y\\).Importance: plenty trivial questions ask, ’s important waste time. importance question also helps motivation fourth straight week cleaning data de-bugging code. also (becomes important) makes easier get talented people work , similarly convince people fund allow work project.Availability: Can reasonably expect get data research question future extent data gathered?Iteration: something can come back run often -analysis?‘FINER framework’ mnemonic device used medicine. framework reminds us ask questions (Hulley 2007):Feasible: Adequate number subjects; adequate technical expertise; affordable time money; manageable scope.Interesting: Getting answer intrigues investigator, peers community.Novel: Confirms, refutes extends previous findingsEthical: Amenable study institutional review board approve.Relevant: scientific knowledge; clinical health policy; future research.Farrugia et al. (2010) build terms developing research questions recommend ‘PICOT’:Population: specific population interested ?Intervention: investigational intervention?Comparison group: main alternative compare intervention?Outcome interest: intend accomplish, measure, improve affect?Time: appropriate follow-time assess outcomeOften time constrained, possibly interesting ways can guide research. interested effect Trump’s tweets stock market, can done just looking minutes (milliseconds?) tweets. interested effect cancer drug long term outcomes? effect takes 20 years, either wait , need look people treated 2000, selection effects different circumstances give drug today. Often reasonable thing build statistical model, need adequate sample sizes, etc.Usually, creation counterfactual crucial. ’ll discuss counterfactuals lot later, briefly, counterfactual -statement ‘’ false. Consider example Humpty Dumpty Lewis Carroll’s Looking-Glass:\nFigure 5.1: Humpty Dumpty example\nHumpty satisfied happen fall , even though similarly satisfied never happen. (won’t ruin story .) comparison group often determines results e.g. relationship VO2 athletic outcomes, compared elite athletic outcomes.Finally, can often dodge ethics boards data science, especially leave university. Typically, ethics guides medicine fields focused ethics boards. often data science applications. Even intentions unimpeachable, want suggest one additional aspect think , Bayes theorem:\n\\[P(|B) = \\frac{P(B|)\\times P()}{P(B)}\\]\n(probability given B depends probability B given , probability , probability B.)see may relevant, let’s go canonical Bayes example: test disease 99 per cent accurate ways (, person actually disease 99 per cent chance test says positive, person disease 99 per cent chance test says negative). Let’s just say 0.005 population disease. randomly pick someone general population chance disease outstandingly low. even test positive:\n\\[\\frac{0.99\\times0.005}{0.99\\times0.005 + 0.01\\times0.995} \\approx 33.2\\]see may relevant, consider example Google’s AI cancer testing (Shetty Tse 2020). Basically, done train model can identify breast cancer. claim ‘greater accuracy, fewer false positives, fewer false negatives experts.’, many others (Aschwanden 2020), argue probably want resources directed point. Even perfectly healthy people go get screened, tend find various things ‘wrong’ . issue ’re perfectly healthy ’ve rarely got good idea whether aspect flagged test big deal .Given low prevalence community, probably want wide-spread use particular testing regime looks one aspect (.e. mammogram case). Bayes rule guides us danger caused unnecessary ‘treatment’ probably outweigh benefits. authors Google blog post likely unimpeachable ethics, may understand Bayes rule.","code":""},{"path":"on-writing.html","id":"title-abstract-and-introduction","chapter":"5 On writing","heading":"5.3.1 Title, abstract, and introduction","text":"title first opportunity tell reader story. Ideally tell reader exactly found. effective title critical order get work read competing priorities. title doesn’t ‘cute’ great.Good: ‘2019 Canadian Federal Election.’ (least reader knows paper .)Better: ‘Liberal Party performance 2019 Canadian Federal Election.’ (least reader knows paper specifically.)Even better: ‘Liberal Party poorly rural areas 2019 Canadian Federal Election.’ (reader knows paper .)put name date paper provides important context paper.six-page paper, good abstract three five sentence paragraph. longer paper abstract can slightly longer. abstract say: , found, reader care. just sentence two, keep high level.introduction tells reader everything need know. writing mystery story - tell reader important points introduction. six-page paper, introduction may two three paragraphs. Four likely much, depends context.introduction set scene give reader background. instance, may like start little broader, provide context paper. describe paper fits context. give high-level results - provide detail provided abstract, don’t get weeds - finally broadly discuss next steps glaring weaknesses. regard high-level result: need pick one. bunch interesting findings, good , pick one write introduction around . ’s compelling enough reader end reading interesting findings discussion/results sections. Finally, highlight remainder paper.example:Canadian Liberal Party always struggled rural ridings. past 100 years never won 25 per cent . even standards 2019 Federal Election disappointment Liberal Party winning 2 40 rural ridings.paper look performance Liberal Party recent election poor. construct model whether Liberal Party won riding explained number farms riding, average internet connectivity, median age. find median age riding increases, likelihood riding won Liberal Party decreases 14 percentage points. Future work expand time horizon considered allow nuanced understanding effects.remainder paper structured follows: Section 2 discusses data, Section 3 discusses model, Section 4 presents results, finally Section 5 discusses findings weaknesses.recommended readings provide lovely examples titles, abstracts, introductions. Please take time briefly read papers.","code":""},{"path":"on-writing.html","id":"figures-tables-equations-and-technical-terms","chapter":"5 On writing","heading":"5.3.2 Figures, tables, equations, and technical terms","text":"Figure tables critical aspect convincing people story. graph can show data let people decide . table, can easily summarise data.Figures, tables, equations, etc, numbered referenced text e.g. “Figure 1 shows…” Figure 1.make sure aspects graph legible. Always label axes. graphs titles, point want communicate clear.use technical term, briefly explained plain language readers might familiar . great example post Monica Alexander explains Gini coefficient:look concentration baby names, let’s calculate Gini coefficient country, sex year. Gini coefficient measures dispersion inequality among values frequency distribution. can take value 0 1. case income distributions, Gini coefficient 1 mean one person income. case, Gini coefficient 1 mean babies name. contrast, Gini coefficient 0 mean names evenly distributed across babies.","code":""},{"path":"on-writing.html","id":"on-brevity","chapter":"5 On writing","heading":"5.3.3 On brevity","text":"\nFigure 5.2: ‘four pages, ’s never going read . Two pages preferable.’\nSource: Shipman, Tim, 2020, \"prime minister’s vanishing briefs’, Sunday Times, 23 February, available : https://www.thetimes.co.uk/article/-prime-ministers-vanishing-briefs-67mt0bg95 via Sarah Nickson.Insisting two page briefs sensible - ‘government ADHD.’ PM across lots issues - across () depth secretaries state. Danger lies PM trying take much getting bogged detail.might irk officials lack sense issue sits within PM’s list priorities - writing skills draft succinct brief. ’d occasions brief PM warrants two pages.something peculiar current PM - ministers raised interviews @instituteforgov Oliver Letwin complained ‘huge amount terrible guff, huge, colossal, humungous length coming departments’\nhttps://www.instituteforgovernment.org.uk/ministers-reflect/person/oliver-letwin/Letwin sent briefs back asked re-drafted one quarter length. ‘Somewhere along line Civil Service got used splurge meaningless kind’ Similarly, Theresa Villiers talked civil service’s ‘frustrating tendency produce six pages obscure rather impenetrable text’ wishes ’d firmer sending documents back re-drafting:\nhttps://www.instituteforgovernment.org.uk/ministers-reflect/person/theresa-villiers/Sarah Nickson, 23 Feb 2020.Brevity important. Partly writing reader, , reader priorities. also writer focuses consider important points , can best support , arguments weakest.don’t think examples government persuasive, please consider Amazon’s 2017 Letter Shareholders, statements Bezos memo writing, instance:Well structured, narrative text ’re rather just text… reason writing 4 page memo harder “writing” 20 page powerpoint narrative structure good memo forces better thought better understanding ’s important , things related.Jeff Bezos, 9 June 2004.","code":""},{"path":"on-writing.html","id":"other","chapter":"5 On writing","heading":"5.3.4 Other","text":"Typos grammatical mistakes affect credibility claims. reader can’t trust use spell-checker, trust use logistic regression? Microsoft Word fantastic spell-checker much better available R Markdown: copy/paste work , look red lines fix R Markdown. look green lines think need fix R Markdown. don’t Word, Google Docs pretty good Apple’s Pages.general tips stolen various people including Reserve Bank Australia’s style guide:Think writing. Aim write everything though front page newspaper, one day .concise. Remove many words possible.direct. Think structure story identify key pieces information arrange paper flows logically one next. use sub-headings need.precise. instance, stock-market didn’t improve worsen, rose fell. Distinguish levels rates change.clear.Write simply.Use short sentence possible.Avoid jargon.break rules need . way know whether need break rule know rules first instance.","code":""},{"path":"on-writing.html","id":"exercises-and-tutorial-4","chapter":"5 On writing","heading":"5.4 Exercises and tutorial","text":"","code":""},{"path":"on-writing.html","id":"exercises-4","chapter":"5 On writing","heading":"5.4.1 Exercises","text":"following best title (pick one)?\n“Problem Set 1”\n“Unemployment”\n“Examining Canada’s Unemployment (2010-2020)”\n“Canada’s Unemployment Increased 2010 2020”\n“Problem Set 1”“Unemployment”“Examining Canada’s Unemployment (2010-2020)”“Canada’s Unemployment Increased 2010 2020”word/s can removed sentence without affecting meaning ‘ADD SENTENCE’ (pick apply)?\n\nword\nanother\nword\nawordanotherword","code":""},{"path":"on-writing.html","id":"tutorial-4","chapter":"5 On writing","heading":"5.4.2 Tutorial","text":"","code":""},{"path":"static-communication.html","id":"static-communication","chapter":"6 Static communication","heading":"6 Static communication","text":"STATUS: construction.Required reading() Economist, 2013, ‘Johnson: six little rules,’ Prospero, 29 July 2013, available : https://www.economist.com/prospero/2013/07/29/johnson--six-little-rules.Alexander, Monica, 2019, ‘concentration uniqueness baby names Australia US,’ https://www.monicaalexander.com/posts/2019-20-01-babynames/. (Look Monica explains concepts, especially Gini coefficient, way can understand even ’ve never heard .)Bronner, Laura, 2020, ‘Quant Editing,’ http://www.laurabronner.com/quant-editing. (Read points evaluate writing . ’s fine comply good reason, need know ’re complying ).Girouard, Dave, 2020, ‘Founder’s Guide Writing Well,’ First Round Review, 4 August, https://firstround.com/review/-founders-guide--writing-well/.Graham, Paul, 2020, ‘Write Usefully,’ http://paulgraham.com/useful.html. (Graham good writing programmer, similar background may like .)Healy, Kieran, 2019, Data Visualization: Practical Introduction, Princeton University Press, Chapters 3, 4, 7, https://socviz.co/.Hodgetts, Paul, 2020, ‘ggfortify Package,’ 31 December, https://www.hodgettsp.com/posts/r-ggfortify/.Wickham, Hadley, Garrett Grolemund, 2017, R Data Science, Chapter 28, https://r4ds..co.nz/.Zinsser, William, 1976 [2016], Writing Well. (edition fine. book included ’re serious improving writing start book. takes hours read. ’ll go onto books, start one.)Zinsser, William, 2009, ‘Writing English Second Language,’ Lecture, Columbia Graduate School Journalism, 11 August, https://theamericanscholar.org/writing-english---second-language/. (’m realistic enough realise requiring book, even though ’ve said ’s great ’s short, bit stretch. really don’t want commit reading Zinsser, please least read ‘crib notes’ version .)Required viewingKuriwaki, Shiro, 2020, ‘Making maps R sf,’ 1 March, freely available : https://vimeo.com/394800836.Recommended reading() Economist, 1991 [2014], ‘Economist Style Guide,’ Twelfth edition. (edition fine. Pick point two day think related writing.)Cochrane, John H., 2005, ‘Writing Tips Ph. D. Students,’ https://faculty.chicagobooth.edu/john.cochrane/research/papers/phd_paper_writing.pdf. (aimed academic research papers, parts still broadly relevant. ’re going academia relevant.)Codrey, Laura, 2013, ‘Churchill’s call brevity,’ 17 October, https://blog.nationalarchives.gov.uk/churchills-call--brevity/.Engel, Claudia , 2019, Using Spatial Data R, 11 February, Chapter 3 Making Maps R, freely available : https://cengel.github.io/R-spatial/mapping.html.Five Thirty Eight, 2020, Pick almost article sports (https://fivethirtyeight.com/sports/) politics (https://fivethirtyeight.com/politics/) sections. (people 538 write beautifully. Look titles tell exactly going , found. Look nicely first paragraphs motivates read rest article. reading BYU basketball ’m indifferent BYU college basketball? title first paragraph hooked .)Graham, Paul, 2005, ‘Writing, Briefly,’ http://paulgraham.com/writing44.html.Lovelace, Robin, Jakub Nowosad, Jannes Muenchow, 2020, Geocomputation R, 29 March, Chapter 8, Making maps R, freely available : https://geocompr.robinlovelace.net/adv-map.html.Patrick, Cameron, 2019, ‘Plotting multiple variables using ggplot2 tidyr,’ 26 November, https://cameronpatrick.com/post/2019/11/plotting-multiple-variables-ggplot2-tidyr/.Patrick, Cameron, 2020, ‘Making beautiful bar charts ggplot,’ 15 March, https://cameronpatrick.com/post/2020/03/beautiful-bar-charts-ggplot/.Shapiro, Jesse M., ‘Four Steps Applied Micro Paper,’ https://www.brown.edu/Research/Shapiro/pdfs/foursteps.pdf. (mostly recommended part ‘robot’ regard data section.)Shapiro, Julian, ‘Writing Well,’ https://www.julian.com/guide/write/intro.Strunk, William Jr., 1959 [2009] ‘Elements Style.’ (edition fine. Eventually ’ll move beyond , ’s important know rules break ).Vanderplas, Susan, Dianne Cook, Heike Hofmann, 2020, ‘Testing Statistical Charts: Makes Good Graph?’ Annual Review Statistics Application, https://www.annualreviews.org/doi/abs/10.1146/annurev-statistics-031219-041252Examples well-written papersBarron, Alexander TJ, Jenny Huang, Rebecca L. Spang, Simon DeDeo. “Individuals, institutions, innovation debates French Revolution.” Proceedings National Academy Sciences 115, . 18 (2018): 4607-4612. rChambliss, Daniel F. “Mundanity Excellence: Ethnographic Report Stratification Olympic Swimmers.” Sociological Theory 7, . 1 (1989): 70-86. doi:10.2307/202063.Joyner, Michael J. “Modeling: optimal marathon performance basis physiological factors.” Journal Applied Physiology, 70, . 2 (1991): 683-687.Kharecha, Pushker ., James E. Hansen, 2013, ‘Prevented mortality greenhouse gas emissions historical projected nuclear power,’ Environmental science & technology, 47, . 9, pp. 4889-4895.Samuel, Arthur L., 1959, ‘studies machine learning using game checkers,’ IBM Journal research development, 3, . 3, pp. 210-229.Wardrop, Robert L., 1995, ‘Simpson’s paradox hot hand basketball,’ American Statistician, 49, . 1, 24-28.Key concepts/skills/etcShow reader raw data, close can come .Use either geom_point geom_bar initially.Writing efficiently effectively requirement want work convincing.Don’t waste reader’s time.good title says paper , great title says paper found.six-page paper, good abstract three five sentence paragraph. longer paper abstract can slightly longer.Thinking maps (often fiddly, strangely enjoyable) variant usual ggplot.Broadening data make available via interactive maps, still telling clear story.Becoming comfortable (excited ) creating static maps.Key librariesggplotpatchworkggmapmapsKey functions/etcggplot::geom_point()ggplot::geom_bar()canada.citiesgeom_polygon()ggmap()map()map_data()","code":""},{"path":"static-communication.html","id":"introduction-2","chapter":"6 Static communication","heading":"6.1 Introduction","text":"[T]duty scientist find new things, bu communicate successfully least three forms: 1) Writing papers books. 2) Prepared public talks. 3) Impromptu talks.Hamming (1996, 65)order convince someone story, paper must well-written, well-organized, easy follow. flow easily one point next. proper sentence structure, spelling, vocabulary, grammar. point articulated clearly completely without overly verbose. Papers demonstrate understanding topics writing confidence discussing terms, techniques issues relevant. References must included properly cited enhances credibility.People need write: founders, VCs, lawyers, software engineers, designers, painters, data scientists, musicians, filmmakers, creative directors, physical trainers, teachers, writers.\nLearn write.Sahil Lavingia.great advice. Writing well done just much knowing code. ’d add ’re intimidated writing, start blog write often something ’re interested . ’ll get better. least ’s ’ve done past 10 years. :)Vicki Boykis.chapter writing. end better idea write short, detailed, quantitative papers communicate exactly want don’t waste time reader.One critical part telling stories data, ’s ultimately data convince . ’re medium, data message. end, easiest way try convince someone story show data allowed come story. Plot raw data, close possible.ggplot fantastic tool , lot package can difficult know start. recommendation start either scatter plot bar chart. critical show reader raw data. notes run . discusses advanced options, important thing show reader raw data (close can). Students seem get confused ‘raw’ means; ’m using refer close original dataset possible, sums, averages, etc, possible. Sometimes data disperse ’ve got constraints, needs element manipulation. main point , least, need plot data ’re going modelling. dealing larger datasets just take 10/1/0.1/etc per cent sample.\nFigure 5.2: Show data!\nSource: YouTube screenshot.","code":""},{"path":"static-communication.html","id":"graphs","chapter":"6 Static communication","heading":"6.2 Graphs","text":"Graphs critical tell compelling story. important thing graphs plot raw data. : Plot. . Raw. Data.Figure 6.1 provides invaluable advice (thank Thomas William Rosenthal).\nFigure 6.1: get started data?\nLet’s look somewhat fun example datasauRus package (Locke D’Agostino McGowan 2018).despite similarities summary statistic level, ’re actually different, well, beasts, plot raw data.","code":"\nlibrary(datasauRus)\n\n# Code from: https://juliasilge.com/blog/datasaurus-multiclass/\ndatasaurus_dozen %>%\n  filter(dataset %in% c(\"dino\", \"star\", \"away\", \"bullseye\")) %>%\n  group_by(dataset) %>%\n  summarise(across(c(x, y), list(mean = mean, sd = sd)),\n    x_y_cor = cor(x, y)\n  ) %>% \n  ungroup()\n#> # A tibble: 4 × 6\n#>   dataset  x_mean  x_sd y_mean  y_sd x_y_cor\n#>   <chr>     <dbl> <dbl>  <dbl> <dbl>   <dbl>\n#> 1 away       54.3  16.8   47.8  26.9 -0.0641\n#> 2 bullseye   54.3  16.8   47.8  26.9 -0.0686\n#> 3 dino       54.3  16.8   47.8  26.9 -0.0645\n#> 4 star       54.3  16.8   47.8  26.9 -0.0630\ndatasaurus_dozen %>% \n  filter(dataset %in% c(\"dino\", \"star\", \"away\", \"bullseye\")) %>%\n  ggplot(aes(x=x, y=y, colour=dataset)) +\n  geom_point() +\n  theme_minimal() +\n  facet_wrap(vars(dataset), nrow = 2, ncol = 2) +\n  labs(colour = \"Dataset\")"},{"path":"static-communication.html","id":"bar-chart","chapter":"6 Static communication","heading":"6.2.1 Bar chart","text":"Bar charts useful one variable want focus . Hint: almost always one variable want focus . Hence, almost always include least one (likely many) bar charts. Bar charts go variety names, depending specifics. recommend R Studio Data Viz Cheat Sheet.get started, let’s simulate data.First, let’s look data.Now let’s plot age distribution. Based simulated data, ’re expecting fairly uniform plot.Now let’s make look little better. themes built ggplot, can install themes packages, can edit aspects . ’d recommend starting ggthemes package fun ones, tend just use classic minimal. Remember must always refer graphs text (Figure 6.2).\nFigure 6.2: Number people died age\nmay want facet variable, case whether person smoker (Figure 6.3).\nFigure 6.3: Number people died age, whether smoke\nAlternatively, may wish colour instead (Figure 6.4). ’ll filter just handful age-groups keep tractable.\nFigure 6.4: Number people died age, whether smoke\n’s important recognise boxplot hides full distribution variable. Unless need communicate general distribution many variables use . box plot can apply different distributions.","code":"\nset.seed(853)\n\nnumber_of_observation <- 10000\n\nexample_data <- tibble(person = c(1:number_of_observation),\n                       smoker = sample(x = c(\"Smoker\", \"Non-smoker\"),\n                                       size = number_of_observation, \n                                       replace = TRUE),\n                       age_died = runif(number_of_observation,\n                                        min = 0,\n                                        max = 100) %>% round(digits = 0),\n                       height = sample(x = c(50:220), \n                                       size =  number_of_observation, \n                                       replace = TRUE),\n                       num_children = sample(x = c(0:5),\n                                             size = number_of_observation, \n                                             replace = TRUE,\n                                             prob = c(0.1, 0.2, 0.40, 0.15, 0.1, 0.05))\n                       )\nhead(example_data)\n#> # A tibble: 6 × 5\n#>   person smoker     age_died height num_children\n#>    <int> <chr>         <dbl>  <int>        <int>\n#> 1      1 Smoker           55     80            3\n#> 2      2 Non-smoker       54     78            2\n#> 3      3 Non-smoker       84    109            1\n#> 4      4 Smoker           75    114            4\n#> 5      5 Smoker           32    135            1\n#> 6      6 Smoker           37    220            0\nexample_data %>% \n  ggplot(mapping = aes(x = age_died)) +\n  geom_bar()\nexample_data %>% \n  ggplot(mapping = aes(x = age_died)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age died\",\n       y = \"Number\",\n       title = \"Number of people who died at each age\",\n       caption = \"Source: Simulated data.\")\nexample_data %>% \n  ggplot(mapping = aes(x = age_died)) +\n  geom_bar() +\n  theme_minimal() +\n  facet_wrap(vars(smoker)) +\n  labs(x = \"Age died\",\n       y = \"Number\",\n       title = \"Number of people who died at each age, by whether they smoke\",\n       caption = \"Source: Simulated data.\")\nexample_data %>% \n  filter(age_died < 25) %>% \n  ggplot(mapping = aes(x = age_died, fill = smoker)) +\n  geom_bar(position = \"dodge\") +\n  theme_minimal() +\n  labs(x = \"Age died\",\n       y = \"Number\",\n       fill = \"Smoker\",\n       title = \"Number of people who died at each age, by whether they smoke\",\n       caption = \"Source: Simulated data.\")"},{"path":"static-communication.html","id":"scatter-plot","chapter":"6 Static communication","heading":"6.2.2 Scatter plot","text":"Often, also interested relationship two series. ’ll scatter plot. scatter plot almost always best choice (Weissgerber et al. 2015). case, let’s simulate data, say years education income.Now let’s look income function years education (Figure 6.5).\nFigure 6.5: Relationship income years education\n","code":"\nset.seed(853)\n\nnumber_of_observation <- 500\n\nscatter_data <- \n  tibble(years_of_education = runif(n = number_of_observation, min = 10, max = 25),\n         error = rnorm(n= number_of_observation, mean = 0, sd = 10000),\n         ) %>% \n  mutate(income = years_of_education * 5000 + error,\n         income = if_else(income < 0, 0, income))\n\nhead(scatter_data)\n#> # A tibble: 6 × 3\n#>   years_of_education   error income\n#>                <dbl>   <dbl>  <dbl>\n#> 1               15.4 -13782. 63180.\n#> 2               11.8   7977. 66985.\n#> 3               17.3  -9787. 76498.\n#> 4               14.7  12999. 86689.\n#> 5               10.6  -1500. 51302.\n#> 6               16.1   1911. 82202.\nscatter_data %>% \n  ggplot(mapping = aes(x = years_of_education, y = income)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Years of education\",\n       y = \"Income\",\n       title = \"Relationship between income and years of education\",\n       caption = \"Source: Simulated data.\")"},{"path":"static-communication.html","id":"never-use-box-plots","chapter":"6 Static communication","heading":"6.2.3 Never use box plots","text":"Box plots almost never appropriate hide distribution data. see , consider data beta distribution.compare box plots.","code":"\nleft <- rbeta(10000,5,2)\nright <- rbeta(10000,2,5)\nmiddle <- rbeta(10000,5,5)\n\ntricky_data <- \n  tibble(left_and_right = \n           c(\n             rbeta(10000,5,2),\n             rbeta(10000,2,5)\n           ),\n         middle = \n           rbeta(20000,1,1))\nboxplot(tricky_data$left_and_right)\nboxplot(tricky_data$middle)\n\nhist(tricky_data$left_and_right)\nhist(tricky_data$middle)"},{"path":"static-communication.html","id":"other-1","chapter":"6 Static communication","heading":"6.2.4 Other","text":"","code":""},{"path":"static-communication.html","id":"best-fit","chapter":"6 Static communication","heading":"6.2.4.1 Best fit","text":"’re interested quickly adding line best fit , continuing earlier income example, can geom_smooth() (Figure 6.6).\nFigure 6.6: Relationship income years education\n","code":"\nscatter_data %>% \n  ggplot(mapping = aes(x = years_of_education, y = income)) +\n  geom_point() +\n  geom_smooth(method = lm, color = \"black\") +\n  theme_minimal() +\n  labs(x = \"Years of education\",\n       y = \"Income\",\n       title = \"Relationship between income and years of education\",\n       caption = \"Source: Simulated data.\")\n#> `geom_smooth()` using formula 'y ~ x'"},{"path":"static-communication.html","id":"histogram","chapter":"6 Static communication","heading":"6.2.4.2 Histogram","text":"want get counts groups, may want use histogram. Figure 6.7 shows counts simulated incomes.\nFigure 6.7: Distribution income\n","code":"\nscatter_data %>% \n  ggplot(mapping = aes(x = income)) +\n  geom_histogram() +\n  theme_minimal() +\n  labs(x = \"Income\",\n       y = \"Number\",\n       title = \"Distribution of income\",\n       caption = \"Source: Simulated data.\")\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`."},{"path":"static-communication.html","id":"multiple-plots","chapter":"6 Static communication","heading":"6.2.4.3 Multiple plots","text":"Finally, let’s try putting together. ’re going use patchwork package (Pedersen 2020) penguins package data. Don’t forget install.packages(\"palmerpenguins\") probably first time ’ve used package.can make things fairly involved fairly quickly.","code":"\nlibrary(patchwork)\nlibrary(palmerpenguins)\n\np1 <- \n  ggplot(palmerpenguins::penguins) + \n  geom_point(aes(bill_length_mm, bill_depth_mm)) +\n  labs(x = \"Bill length (mm)\",\n       y = \"Bill depth (mm)\")\np2 <- \n  ggplot(palmerpenguins::penguins) + \n  geom_bar(aes(species)) +\n  labs(x = \"Species\",\n       y = \"Number\")\n\np1 + p2\n\n(p1 | p2) /\n  p2"},{"path":"static-communication.html","id":"tables","chapter":"6 Static communication","heading":"6.3 Tables","text":"Tables also critical tell compelling story. may prefer table graph features want focus . ’ll use knitr::kable() alongside ‘kableExtra’ package also gt package.Let’s start kable package summary dinosaur data earlier.Even defaults pretty good, can add tweaks make table better. first many significant digits inappropriate, may also like add caption, make column names consistent, change alignment.Table 6.1: first table.‘’kableExtra’ package builds extra functionality (Zhu 2020).gt package (Iannone, Cheng, Schloerke 2020) newer package brings lot exciting features. However, newer sometimes issues PDF output.add sub-titles easily.One common reason needing table report regression results. consider gtsummary, stargazer, modelsummary. moment, favourite modelsummary (Arel-Bundock 2021).","code":"\nexample_data <- \n  datasaurus_dozen %>% \n  filter(dataset %in% c(\"dino\", \"star\", \"away\")) %>% \n  group_by(dataset) %>% \n  summarize(\n    Mean    = mean(x),\n    Std_dev = sd(x),\n    ) \n\nexample_data %>% \n  knitr::kable()\nexample_data %>% \n  knitr::kable(digits = 2, \n               caption = \"My first table.\", \n               col.names = c(\"Dataset\", \"Mean\", \"Standard deviation\"),\n               align = c('l', 'l', 'l')\n               )\nlibrary(gt)\n\nexample_data %>% \n  gt()\nexample_data %>% \n  gt() %>%\n  tab_header(\n    title = \"Summary stats can be misleading\",\n    subtitle = \"With an example from a dinosaur!\"\n  )\nlibrary(modelsummary)\n#> \n#> Attaching package: 'modelsummary'\n#> The following object is masked from 'package:gt':\n#> \n#>     escape_latex\n\nmod <- lm(y ~ x, datasaurus_dozen)\nmodelsummary(mod)"},{"path":"static-communication.html","id":"maps","chapter":"6 Static communication","heading":"6.4 Maps","text":"many ways maps can thought fancy graph, x-axis latitude, y-axis longitude, outline background image. used type set-, instance, ggplot setting quite familiar. Static maps useful printed output, PDF Word report, something particular want illustrate.small complications, part straight-forward . first step get data. helpfully, geographic data built ggplot, information built package called maps.information hand can create map Canada shows cities population 1,000. (geom_polygon() function within ggplot draws shapes, connecting points within groups. coord_map() function adjusts fact making something 2D map represent something 3D.)often case R, many different ways get started creating static maps. ’ve already seen can built using simply ggplot, ’ll explore one package bunch functionalities built make things easier: ggmap.two essential components map: 1) border background image (also known tile); 2) something interest within border top tile. ggmap, use open-source option tile, Stamen Maps (maps.stamen.com), use plot points based latitude longitude.","code":"\nggplot() +\n  geom_polygon( # First draw an outline\n    data = some_data, \n    aes(x = latitude, \n        y = longitude,\n        group = group\n        )) +\n  geom_point( # Then add points of interest\n    data = some_other_data, \n    aes(x = latitude, \n        y = longitude)\n    )\nlibrary(maps)\nlibrary(tidyverse)\n\ncanada <- map_data(database = \"world\", regions = \"canada\")\ncanadian_cities <- maps::canada.cities\n\nhead(canada)\n#>        long      lat group order region    subregion\n#> 1 -59.78760 43.93960     1     1 Canada Sable Island\n#> 2 -59.92227 43.90391     1     2 Canada Sable Island\n#> 3 -60.03775 43.90664     1     3 Canada Sable Island\n#> 4 -60.11426 43.93911     1     4 Canada Sable Island\n#> 5 -60.11748 43.95337     1     5 Canada Sable Island\n#> 6 -59.93604 43.93960     1     6 Canada Sable Island\n\nhead(canadian_cities)\n#>            name country.etc    pop   lat    long capital\n#> 1 Abbotsford BC          BC 157795 49.06 -122.30       0\n#> 2      Acton ON          ON   8308 43.63  -80.03       0\n#> 3 Acton Vale QC          QC   5153 45.63  -72.57       0\n#> 4    Airdrie AB          AB  25863 51.30 -114.02       0\n#> 5    Aklavik NT          NT    643 68.22 -135.00       0\n#> 6    Albanel QC          QC   1090 48.87  -72.42       0\nggplot() +\n  geom_polygon(data = canada,\n               aes(x = long,\n                   y = lat,\n                   group = group),\n               fill = \"white\", \n               colour = \"grey\") +\n  coord_map(ylim = c(40, 70)) +\n  geom_point(aes(x = canadian_cities$long, \n                 y = canadian_cities$lat),\n             alpha = 0.3,\n             color = \"black\") +\n  theme_classic() +\n  labs(x = \"Longitude\",\n       y = \"Latitude\")\n# If I'm being honest, this 'simple example' took me six hours to work out. Firstly \n# to find Canada and then to find Canadian cities."},{"path":"static-communication.html","id":"australian-polling-places","chapter":"6 Static communication","heading":"6.4.1 Australian polling places","text":"Like Canada, Australia people go specific locations, called booths, vote. booths latitudes longitudes can plot . One reason may like notice patterns geographies.get started need get tile. going use ggmap get tile Stamen Maps, builds OpenStreetMap (openstreetmap.org). main argument function specify bounding box. requires two latitudes - one top box one bottom box - two longitudes - one left box one right box. (can useful use Google Maps, alternative, find values need.) bounding box provides coordinates edges interested . case provided coordinates centered around Canberra, Australia (equivalent Ottawa - small city created purposes capital).defined bounding box, function get_stamenmap() get tiles area. number tiles needs get depends zoom, type tiles gets depends maptype. ’ve chosen maptype like - black white option - helpfile specifies others may like. point can pass maps ggmap plot tile! actively downloading tiles, need internet connection.map can use ggmap() plot . (circle middle map Australian Parliament House … yes, parliament surrounded circular roads (call ‘roundabouts’), actually ’s surrounded two .)Now want get data plot top tiles. just plot location polling places, based ‘division’ (Australian equivalent ‘ridings’ Canada) . available : https://results.aec.gov.au/20499/Website/Downloads/HouseTppByPollingPlaceDownload-20499.csv. (Australian Electoral Commission (AEC) official government agency responsible elections Australia.)dataset whole Australia, just going plot area around Canberra filter booths geographic (AEC various options people hospital, able get booth, etc, still ‘booths’ dataset).Now can use ggmap way plot underlying tiles, build using geom_point() add points interest.may like save map don’t draw every time, can way graph, using ggsave().Finally, reason used Stamen Maps OpenStreetMap open source, however can also use Google Maps want. requires first register credit card Google, specify key, low usage free. get_googlemap() function ggmap, brings nice features get_stamenmap() . instance, can enter placename ’ll ’s best find rather needing specify bounding box.","code":"\nlibrary(ggmap)\n\nbbox <- c(left = 148.95, bottom = -35.5, right = 149.3, top = -35.1)\ncanberra_stamen_map <- get_stamenmap(bbox, zoom = 11, maptype = \"toner-lite\")\n\nggmap(canberra_stamen_map)\n# Read in the booths data for each year\nbooths <- readr::read_csv(\"https://results.aec.gov.au/24310/Website/Downloads/GeneralPollingPlacesDownload-24310.csv\", \n                          skip = 1, \n                          guess_max = 10000)\n\nhead(booths)\n#> # A tibble: 6 × 15\n#>   State DivisionID DivisionNm PollingPlaceID\n#>   <chr>      <dbl> <chr>               <dbl>\n#> 1 ACT          318 Bean                93925\n#> 2 ACT          318 Bean                93927\n#> 3 ACT          318 Bean                11877\n#> 4 ACT          318 Bean                11452\n#> 5 ACT          318 Bean                 8761\n#> 6 ACT          318 Bean                 8763\n#> # … with 11 more variables: PollingPlaceTypeID <dbl>,\n#> #   PollingPlaceNm <chr>, PremisesNm <chr>,\n#> #   PremisesAddress1 <chr>, PremisesAddress2 <chr>,\n#> #   PremisesAddress3 <chr>, PremisesSuburb <chr>,\n#> #   PremisesStateAb <chr>, PremisesPostCode <chr>,\n#> #   Latitude <dbl>, Longitude <dbl>\n# Reduce the booths data to only rows with that have latitude and longitude\nbooths_reduced <-\n  booths %>%\n  filter(State == \"ACT\") %>% \n  select(PollingPlaceID, DivisionNm, Latitude, Longitude) %>% \n  filter(!is.na(Longitude)) %>% # Remove rows that don't have a geography\n  filter(Longitude < 165) # Remove Norfolk Island\nggmap(canberra_stamen_map, \n      extent = \"normal\", \n      maprange = FALSE) +\n  geom_point(data = booths_reduced,\n             aes(x = Longitude, \n                 y = Latitude, \n                 colour = DivisionNm),\n             ) +\n  scale_color_brewer(name = \"2019 Division\", palette = \"Set1\") +\n  coord_map(projection=\"mercator\",\n            xlim=c(attr(map, \"bb\")$ll.lon, attr(map, \"bb\")$ur.lon),\n            ylim=c(attr(map, \"bb\")$ll.lat, attr(map, \"bb\")$ur.lat)) +\n  labs(x = \"Longitude\",\n       y = \"Latitude\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank())\nggsave(\"outputs/figures/map.pdf\", width = 20, height = 10, units = \"cm\")"},{"path":"static-communication.html","id":"toronto-bike-parking","chapter":"6 Static communication","heading":"6.4.2 Toronto bike parking","text":"Let’s see another example static map, time using Toronto data accessed via opendatatoronto package. dataset going plot available : https://open.toronto.ca/dataset/street-furniture-bicycle-parking/.Now ’ve saved copy data, can use one. First, need clean bit. clear errors ADDRESSNUMBERTEXT field, many, ’ll just ignore .bike racks temporary remove also let’s just look area around university, Ward 11If look dataset point, ’ll notice row every bike parking spot. don’t really need know , sometimes lots right next . Instead, ’d just like one point (’ll take advantage interactive graph moment). , want create count address, just get one instance per address.Now can grab tile add bike rack data onto .","code":"\n# This code is based on code from: https://open.toronto.ca/dataset/street-furniture-bicycle-parking/.\nlibrary(opendatatoronto)\n# (The string identifies the package.)\nresources <- list_package_resources(\"71e6c206-96e1-48f1-8f6f-0e804687e3be\")\n# In this case there is only one dataset within this resource so just need the first one    \nraw_data <- filter(resources, row_number()==1) %>% get_resource()\nwrite_csv(raw_data, \"inputs/data/bike_racks.csv\")\nhead(raw_data)\nraw_data <- read_csv(\"inputs/data/bike_racks.csv\")\n# We'll just focus on the data that we want\nbike_data <- tibble(ward = raw_data$WARD,\n                    id = raw_data$ID,\n                    status = raw_data$STATUS,\n                    street_address = paste(raw_data$ADDRESSNUMBERTEXT, raw_data$ADDRESSSTREET),\n                    latitude = raw_data$LATITUDE,\n                    longitude = raw_data$LONGITUDE)\nrm(raw_data)\n# Only keep ones that still exist\nbike_data <- \n  bike_data %>%\n  filter(status == \"Existing\") %>% \n  select(-status)\n\nbike_data <- bike_data %>% \n  filter(ward == 11) %>% \n  select(-ward)\nbike_data <- \n  bike_data %>%\n  group_by(street_address) %>% \n  mutate(number_of_spots = n(),\n         running_total = row_number()\n         ) %>% \n  ungroup() %>% \n  filter(running_total == 1) %>% \n  select(-id, -running_total)\n\nhead(bike_data)\n#> # A tibble: 6 × 4\n#>   street_address   latitude longitude number_of_spots\n#>   <chr>               <dbl>     <dbl>           <int>\n#> 1 8 Kensington Ave     43.7     -79.4               1\n#> 2 87 Avenue Rd         43.7     -79.4               4\n#> 3 162 Mc Caul St       43.7     -79.4               1\n#> 4 147 Baldwin St       43.7     -79.4               2\n#> 5 888 Yonge St         43.7     -79.4               1\n#> 6 180 Elizabeth St     43.7     -79.4              10\n\nwrite_csv(bike_data, \"outputs/data/bikes.csv\")\nbbox <- c(left = -79.420390, bottom = 43.642658, right = -79.383354, top = 43.672557)\n\ntoronto_stamen_map <- get_stamenmap(bbox, zoom = 14, maptype = \"toner-lite\")\n\nggmap(toronto_stamen_map,  maprange = FALSE) +\n  geom_point(data = bike_data,\n             aes(x = longitude, \n                 y = latitude),\n             alpha = 0.3\n             ) +\n  labs(x = \"Longitude\",\n       y = \"Latitude\") +\n  theme_minimal() "},{"path":"static-communication.html","id":"geocoding","chapter":"6 Static communication","heading":"6.4.3 Geocoding","text":"point just assumed already geocoded data. places ‘Canberra, Australia,’ ‘Ottawa, Canada,’ just names, don’t actually inherently location. order plot need get latitude longitude . process going names coordinates called geocoding.range options geocode data R, one good package tidygeocoder (Cambon Belanger 2021). get started using package need dataframe locations. ’ll just quickly make one .","code":"\nsome_locations <- \n  tibble(city = c('Canberra', 'Ottawa'),\n         country = c('Australia', 'Canada'))\ntidygeocoder::geo(city = some_locations$city, \n                  country = some_locations$country, \n                  method = 'osm')\n#> Passing 2 addresses to the Nominatim single address geocoder\n#> Query completed in: 2 seconds\n#> # A tibble: 2 × 4\n#>   city     country     lat  long\n#>   <chr>    <chr>     <dbl> <dbl>\n#> 1 Canberra Australia -35.3 149. \n#> 2 Ottawa   Canada     45.4 -75.7"},{"path":"static-communication.html","id":"exercises-and-tutorial-5","chapter":"6 Static communication","heading":"6.5 Exercises and tutorial","text":"","code":""},{"path":"static-communication.html","id":"exercises-5","chapter":"6 Static communication","heading":"6.5.1 Exercises","text":"dataset contains measurements height (cm) sample 300 penguins, either Adeline Emperor species. interested visualizing distribution heights species graphical way. Please discuss whether pie chart appropriate type graph use. box whisker plot? Finally, considerations made histogram? [Please write paragraph two aspect.]Assume dataset columns exist. code work? data %>% ggplot(aes(x = col_one)) %>% geom_point() (pick one)?\nYes\n\nYesNoIf categorical data, geom use plot (pick one)?\ngeom_bar()\ngeom_point()\ngeom_abline()\ngeom_boxplot()\ngeom_bar()geom_point()geom_abline()geom_boxplot()box plots often inappropriate (pick one)?\nhide full distribution data.\nhard make.\nugly.\nmode clearly displayed.\nhide full distribution data.hard make.ugly.mode clearly displayed.following, , elements layered grammar graphics (Wickham 2010) (select apply)?\ndefault dataset set mappings variables aesthetics.\nOne layers, layer one geometric object, one statistical transformation, one position adjustment, optionally, one dataset set aesthetic mappings.\nColours enable reader understand main point.\ncoordinate system.\nfacet specification.\nOne scale aesthetic mapping used.\ndefault dataset set mappings variables aesthetics.One layers, layer one geometric object, one statistical transformation, one position adjustment, optionally, one dataset set aesthetic mappings.Colours enable reader understand main point.coordinate system.facet specification.One scale aesthetic mapping used.","code":""},{"path":"static-communication.html","id":"tutorial-5","chapter":"6 Static communication","heading":"6.5.2 Tutorial","text":"Discuss, page two, layered grammar Wickham (2010) relates telling stories data.","code":""},{"path":"interactive-communication.html","id":"interactive-communication","chapter":"7 Interactive communication","heading":"7 Interactive communication","text":"STATUS: construction.Required readingMcQuire (2019)Presmanes Hill (2021a)Presmanes Hill (2021b)Mock (2020)Required viewingKuriwaki, Shiro, 2020, ‘Making maps R sf,’ 1 March, https://vimeo.com/394800836.Recommended readingCooley, David, 2020, ‘mapdeck,’ https://symbolixau.github.io/mapdeck/index.html.Gabrielle, 2019, ‘Visualising spatial data using sf mapdeck - part one,’ 4 December, https://resources.symbolix.com.au/2019/12/04/mapdeck-1/.Kolb, Jan-Philipp, 2019, ‘Using Web Services Work Geodata R,’ R Journal, 11:2, pages 6-23, https://journal.r-project.org/archive/2019/RJ-2019-041/index.html.‘Leaflet R,’ https://rstudio.github.io/leaflet/.Xie, Yihui, Amber Thomas, Alison Presmanes Hill, 2020, blogdown: Creating Websites R Markdown, https://bookdown.org/yihui/blogdown/.Key concepts/skills/etcBuilding website using within R environment using (order ease): postcards, distill, blogdown.Thinking can take advantage interaction maps, broadening data make available via interactive maps, still telling clear story.Key librariesblogdowndistillleafletmapdeckpostcardstidyverseusethisKey functions/etcblogdown:::serve_site()distill::create_article()postcards::create_postcard()usethis::use_git()usethis::use_github()","code":""},{"path":"interactive-communication.html","id":"making-a-website","chapter":"7 Interactive communication","heading":"7.1 Making a website","text":"","code":""},{"path":"interactive-communication.html","id":"introduction-3","chapter":"7 Interactive communication","heading":"7.1.1 Introduction","text":"website critical part communication. instance, place bring together everything ’ve done, allows control online presence. need website.One way make website use blogdown package (Xie, Dervieux, Hill 2021). blogdown package allows make websites (just blogs, notwithstanding name) largely within R Studio. builds Hugo, popular tool making websites. blogdown lets freely quickly get website --running. easy add content time--time. integrates R Markdown lets easily share work. separation content styling allows relatively quickly change website’s design.However, blogdown brittle. dependent Hugo, features work today may work tomorrow. Also, owners Hugo templates can update time, without thought existing users. blogdown great know ’re specific use-case, style, mind. However, recently two alternatives better starting points.first distill (Allaire et al. 2021). , R package wraps around another framework, case Distill. However, contrast Hugo, Distill focused common needs data science, also maintained one group, can stable choice. said, default distill site fairly unremarkable. , recommend third option.third option, one ’ll start , postcards (Kross 2021). tailored solution creates simple biographical websites look great. followed earlier chapter set-GitHub, literally able get postcards website online five minutes.","code":""},{"path":"interactive-communication.html","id":"postcards","chapter":"7 Interactive communication","heading":"7.1.2 Postcards","text":"get started postcards, first need install packages.want create new project website, ‘File -> New Project -> New Directory -> Postcards Website.’ ’ll get pick name location project, can select postcards theme. case ’ll choose ‘trestles,’ probably want tick ‘Open new session.’open new file now click ‘Knit’ build site. result fairly great one-page website (Figure 7.1)!\nFigure 7.1: Example default Trestles website made postcards package\npoint, update basic content match . instance, website, details (Figure 7.2).\nFigure 7.2: Example Trestles website details\n’ve got site ’d like , add GitHub. GitHub try build site, don’t want need first add hidden file running console:easiest way (assuming set everything earlier chapters) use usethis package (Wickham Bryan 2020).project GitHub repo can use GitHub pages host : ‘Settings -> Pages’ change source ‘main’ ‘master,’ depending settings.","code":"\ninstall.packages('postcards')\nfile.create('.nojekyll')\nusethis::use_git()\nusethis::use_github()"},{"path":"interactive-communication.html","id":"distill","chapter":"7 Interactive communication","heading":"7.1.3 Distill","text":"get started distill (Allaire et al. 2021), going build framework around postcards site, following Presmanes Hill (2021a) fairly closely (please go Alison’s blogpost details). ’ll explore aspects distill make nice choice, mention trade-offs make choose option. First, need install distill., create new project website, ‘File -> New Project -> New Directory -> Distill Blog’ (’s really much difference website blog options).’ll get pick name location project, can set title. Select ‘Configure GitHub Pages’ also ‘Open new session’ (forget change mind ’s big deal - can always changed ex post can just delete directory start ). look something like Figure 7.3.\nFigure 7.3: Example settings setting Distill.\npoint can click ‘Build Website’ Build tab, ’ll see default website, look something like Figure 7.4.\nFigure 7.4: Example default Distill website.\n, now need work update things. default ‘Distill Blog’ setting blog homepage. can change . really liked bio page earlier, use approach.First change name ‘index.Rmd’ file ‘blog.Rmd.’ create new ‘trestles’ page:trestles page just created open, need add following line yaml.Figure 7.5 added line 16 rebuilt website.\nFigure 7.5: Updating yaml change homepage.\ncan make changes default content earlier, updating links, image, bio. advantage using Distill now additional pages, just one-page website, also blog. default, ‘’ page, pages may useful, depending particular use-case, include: ‘research,’ ‘teaching,’ ‘talks,’ ‘projects,’ ‘software,’ ‘datasets.’ now, ’ll talk adding editing page called ‘software.’can use following function:create open R Markdown document. add website, open ’_site.yml’ add line ‘navbar’ (Figure 7.6(. done re-building site result software page added.\nFigure 7.6: Adding another page website.\nContinue process ’re happy site. instance, may want add blog back. follow pattern , ‘blog’ instance ‘software.’’re ready, can get website online way postcards site (.e. push GitHub use GitHub Pages).Using distill great option want multi-page website, still want fairly controlled environment. lot options can change best place start see Alison Hill’s blog post (Presmanes Hill 2021a), distill package homepage also useful.said, distill opinionated. recently didn’t even allow different citation style! great option (use website), want something flexible, blogdown might better option.","code":"\ninstall.packages('distill')\npostcards::create_postcard(file = \"index.Rmd\", template = \"trestles\")\nsite: distill::distill_website\ndistill::create_article(file = 'software')"},{"path":"interactive-communication.html","id":"blogdown","chapter":"7 Interactive communication","heading":"7.1.4 Blogdown","text":"Using blogdown (Xie, Dervieux, Hill 2021) work Google sites Squarespace. requires little knowledge using basic Wordpress site. want customise absolutely every aspect website, need everything ‘just ’ blogdown may . , blogdown still active development various aspects may break future releases. However, blogdown allows variety level expression possible distill.post simplified version Presmanes Hill (2021b) Xie, Thomas, Presmanes Hill (2021). sticks basics doesn’t require much decision-making. purpose allow someone without much experience use blogdown get website --running. Head two resources ’ve got website working want dive bit deeper.’ll need install blogdown., create new project website, ‘File -> New Project -> New Directory -> Website using blogdown.’ point can set name location, also select ‘Open new session.’ look something like Figure 7.7.\nFigure 7.7: Example settings setting blogdown\ncan click ‘Build Website’ ‘Build’ pane, extra step needed, serving site:site show ‘Viewer’ pane (Figure 7.8).\nFigure 7.8: Serving default blogdown site.\npoint, default website ‘served’ locally. means changes make reflected website see Viewer pane. see website web browser, click ‘Show new window’ button top left Viewer. open website using address R Studio also tells .probably want update ‘’ section. go ‘content -> .md’ add content. One nice aspect blogdown automatically re-load content save, see changes immediately show .may also like change logo. adding square image ‘public/images/’ changing call ‘logo.png’ ‘config.yaml.’’re happy , can make website public way described postcards.said, biggest advantage using blogdown allows us use Hugo templates. provides large number beautifully crafted websites. pick theme can go Hugo themes page: https://themes.gohugo.io. hundreds different themes. general, can made work blogdown, sometimes can bit hassle get working.One particularly like Apéro: https://hugo-apero-docs.netlify.app. like , use theme calling create new site. reminder, ‘File -> New Project -> New Directory -> Website using blogdown.’ point, addition setting name location, can specify theme. Specifically, ‘Hugo theme’ field, specify GitHub username repository, case ‘hugo-apero/apero’ (Figure 7.9).\nFigure 7.9: Using Apéro theme.\n","code":"\ninstall.packages(\"blogdown\")\nblogdown:::serve_site()"},{"path":"interactive-communication.html","id":"interactive-maps","chapter":"7 Interactive communication","heading":"7.2 Interactive maps","text":"nice thing interactive maps can let users decide interested . Additionally, lot information may like leave users selectively focus interested . instance, case Canadian politics, people interested Toronto ridings, others interested Manitoba, etc. difficult present map focuses , interactive map great option allowing users zoom want.said, important cognizant build maps, broadly, done scale enable us able build maps. instance, regard Google, McQuire (2019) says:Google began life 1998 company famously dedicated organising vast amounts data Internet. last two decades ambitions changed crucial way. Extracting data words numbers physical world now merely stepping-stone towards apprehending organizing physical world data. Perhaps shift surprising moment become possible comprehend human identity form (genetic) ‘code.’ However, apprehending organizing world data current settings likely take us well beyond Heidegger’s ‘standing reserve’ modern technology enframed ‘nature’ productive resource. 21st century, stuff human life —genetics bodily appearances, mobility, gestures, speech, behaviour —progressively rendered productive resource can harvested continuously subject modulation time.mean use build interactive maps? course . ’s important aware fact frontier, boundaries appropriate use still determined. Indeed, literal boundaries maps consistently determined updated. move digital maps, compared physical printed maps, means actually possible different users presented different realities. instance, ‘…Google routinely takes sides border disputes. Take, instance, representation border Ukraine Russia. Russia, Crimean Peninsula represented hard-line border Russian-controlled, whereas Ukrainians others see dotted-line border. strategically important peninsula claimed nations violently seized Russia 2014, one many skirmishes control’ Bensinger (2020).","code":""},{"path":"interactive-communication.html","id":"leaflet","chapter":"7 Interactive communication","heading":"7.2.1 Leaflet","text":"leaflet package (Cheng, Karambelkar, Xie 2021) originally JavaScript library name brought R. makes easy make interactive maps. basics similar ggmap (Kahle Wickham 2013) set-, course , many, many, options.Let’s redo bike map earlier, possibly interaction allow us see issue data.way graph ggplot begins ggplot() function, map leaflet package begins call leaflet() function. allows specify data, bunch options width height. , add ‘layers,’ way added ggplot. first layer ’ll add tile function addTiles(). case, default OpenStreeMap. ’ll add markers show location bike parking spot addMarkers().two options may familiar. first ‘popup,’ happens click marker. example giving address. second ‘label,’ happens hover marker. example given number spots.Let’s another go, time making map fire stations Toronto. can use data Open Data Toronto, via opendatatoronto R package (Gelfand 2020). ensure book works, save use dataset 13 May 2021, able get --date dataset using link code.lot information , ’ll just plot location fire station along name address.introduce different type marker , circles. allow us use different colours outcomes type. three possible outcomes: “Fire/Ambulance Stations” “Fire Station,” “Restaurant,” “Unknown.”","code":"\nlibrary(leaflet)\nlibrary(tidyverse)\n#> ── Attaching packages ─────────────────── tidyverse 1.3.1 ──\n#> ✓ ggplot2 3.3.5     ✓ purrr   0.3.4\n#> ✓ tibble  3.1.6     ✓ dplyr   1.0.7\n#> ✓ tidyr   1.1.4     ✓ stringr 1.4.0\n#> ✓ readr   2.1.1     ✓ forcats 0.5.1\n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> x dplyr::filter() masks stats::filter()\n#> x dplyr::lag()    masks stats::lag()\n\nbike_data <- read_csv(\"outputs/data/bikes.csv\")\n#> Rows: 1400 Columns: 4\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (1): street_address\n#> dbl (3): latitude, longitude, number_of_spots\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nleaflet(data = bike_data) %>%\n  addTiles() %>%  # Add default OpenStreetMap map tiles\n  addMarkers(lng = bike_data$longitude, \n             lat = bike_data$latitude, \n             popup = bike_data$street_address,\n             label = ~as.character(bike_data$number_of_spots))\nlibrary(opendatatoronto)\n# Get starter code from: https://open.toronto.ca/dataset/fire-station-locations/\nfire_stations_locations <- get_resource('9d1b7352-32ce-4af2-8681-595ce9e47b6e')\n# Grab the lat and long - thanks https://stackoverflow.com/questions/47661354/converting-geometry-to-longitude-latitude-coordinates-in-r\nfire_stations_locations <- \n  fire_stations_locations %>% \n  tidyr::extract(geometry, c('lon', 'lat'), '\\\\((.*), (.*)\\\\)', convert = TRUE)\n\nwrite_csv(fire_stations_locations, \"inputs/data/fire_stations_locations.csv\")\nfire_stations_locations <- read_csv(\"inputs/data/fire_stations_locations.csv\")\n\nhead(fire_stations_locations)\n#> # A tibble: 6 × 26\n#>   `_id`    ID NAME     ADDRESS   ADDRESS_POINT_ID ADDRESS_ID\n#>   <dbl> <dbl> <chr>    <chr>                <dbl>      <dbl>\n#> 1     1    21 FIRE ST… 900 TAPS…          4236992     363382\n#> 2     2    60 FIRE ST… 106 ASCO…           764237      70190\n#> 3     3    61 FIRE ST… 65 HENDR…           819425     127148\n#> 4     4    55 FIRE ST… 260 ADEL…         12763904     484214\n#> 5     5    24 FIRE ST… 745 MEAD…          6349868     357277\n#> 6     6    74 FIRE ST… 140 LANS…         10757599     157562\n#> # … with 20 more variables: CENTRELINE_ID <dbl>,\n#> #   MAINT_STAGE <chr>, ADDRESS_NUMBER <dbl>,\n#> #   LINEAR_NAME_FULL <chr>, POSTAL_CODE <chr>,\n#> #   GENERAL_USE <chr>, CLASS_FAMILY_DESC <chr>,\n#> #   ADDRESS_ID_LINK <dbl>, PLACE_NAME <chr>, X <lgl>,\n#> #   Y <lgl>, LATITUDE <lgl>, LONGITUDE <lgl>,\n#> #   WARD_NAME <chr>, MUNICIPALITY_NAME <chr>, …\nlibrary(leaflet)\n\npal <- colorFactor(\"Dark2\", domain = fire_stations_locations$GENERAL_USE %>% unique())\n\nleaflet() %>%\n  addTiles() %>%  # Add default OpenStreetMap map tiles\n  addCircleMarkers(\n    data = fire_stations_locations,\n    lng = fire_stations_locations$lon, \n    lat = fire_stations_locations$lat, \n    color = pal(fire_stations_locations$GENERAL_USE),\n    popup = paste(\"<b>Name:<\/b>\", as.character(fire_stations_locations$NAME), \"<br>\",\n                  \"<b>Address:<\/b>\", as.character(fire_stations_locations$ADDRESS), \"<br>\")\n    ) %>% \n  addLegend(\"bottomright\", \n            pal = pal, \n            values = fire_stations_locations$GENERAL_USE %>% unique(),\n    title = \"Type\",\n    opacity = 1\n  )"},{"path":"interactive-communication.html","id":"mapdeck","chapter":"7 Interactive communication","heading":"7.2.2 Mapdeck","text":"package Mapdeck (Cooley 2020) R package built top Mapbox (https://www.mapbox.com).1 based WebGL, means web browser lot work . nice thing , can bunch things leaflet struggles , especially dealing larger datasets. Mapbox full-featured application many businesses may heard use: https://www.mapbox.com/showcase. close discussion interactive mapping, want briefly touch mapdeck, newer, exciting, package.point used ‘stamen maps’ underlying tile, mapdeck uses ‘Mapbox’ - https://www.mapbox.com/ - need register get token . ’s free need . token add R. (cover happening detail later chapter.) Run function:run function open file. can add Mapbox secret token.Save ‘.Renviron’ file, restart R (Session -> Restart R).can call map. ’ll just plot firefighters data earlier.pretty nice!","code":"\nusethis::edit_r_environ() \nMAPBOX_TOKEN = 'PUT_YOUR_MAPBOX_SECRET_HERE'\nlibrary(mapdeck)\n#> \n#> Attaching package: 'mapdeck'\n#> The following object is masked from 'package:tibble':\n#> \n#>     add_column\n\nmapdeck(style = mapdeck_style('dark')\n        ) %>%\n  add_scatterplot(\n    data = fire_stations_locations, \n    lat = \"lat\", \n    lon = \"lon\", \n    layer_id = 'scatter_layer',\n    radius = 10,\n    radius_min_pixels = 5,\n    radius_max_pixels = 100,\n    tooltip = \"ADDRESS\"\n  )\n#> Registered S3 method overwritten by 'jsonify':\n#>   method     from    \n#>   print.json jsonlite"},{"path":"interactive-communication.html","id":"shiny","chapter":"7 Interactive communication","heading":"7.3 Shiny","text":"Shiny (Chang et al. 2021) way making interactive web applications (just maps) using R. ’s fun, fiddly. ’re going step one way take advantage Shiny, ’s quickly add interactivity graphs. ’ll return Shiny later chapters also, much just first pass.’re going make quick interactive graph based ‘babynames’ dataset package babynames (Wickham 2019b). First, ’ll build static version.can see possibly popular boys names tend clustered, compared -popular girls names, may spread . However, one thing might interested effect ‘bins’ parameter shapes see. might like use interactivity explore different values.get started, create new Shiny app menu: ‘File -> New File -> Shiny Web App.’ Give name, ‘not_my_first_shiny’ leave options default. new file ‘app.R’ open can click ‘Run app’ see looks like.Now replace content file - ‘app.R’ content , click ‘Run app’find served interactive graph can change number bins look something like Figure 7.10.\nFigure 7.10: Example Shiny app user controls number bins.\n","code":"\nlibrary(babynames)\nlibrary(tidyverse)\n\ntop_five_names_by_year <- \n  babynames %>% \n  group_by(year, sex) %>% \n  arrange(desc(n)) %>% \n  slice_head(n = 5)\n\ntop_five_names_by_year %>% \n  ggplot(aes(x = n, fill = sex)) +\n  geom_histogram(position = \"dodge\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(x = \"Babies with that name\",\n       y = \"Occurances\",\n       fill = \"Sex\"\n       )\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\nlibrary(shiny)\n\n# Define UI for application that draws a histogram\nui <- fluidPage(\n\n    # Application title\n    titlePanel(\"Count of names for five most popular names each year.\"),\n\n    # Sidebar with a slider input for number of bins \n    sidebarLayout(\n        sidebarPanel(\n            sliderInput(inputId = \"number_of_bins\",\n                        label = \"Number of bins:\",\n                        min = 1,\n                        max = 50,\n                        value = 30)\n        ),\n\n        # Show a plot of the generated distribution\n        mainPanel(\n           plotOutput(\"distPlot\")\n        )\n    )\n)\n\n# Define server logic required to draw a histogram\nserver <- function(input, output) {\n\n    output$distPlot <- renderPlot({\n\n        # Draw the histogram with the specified number of bins\n        top_five_names_by_year %>% \n            ggplot(aes(x = n, fill = sex)) +\n            geom_histogram(position = \"dodge\", bins = input$number_of_bins) +\n            theme_minimal() +\n            scale_fill_brewer(palette = \"Set1\") +\n            labs(x = \"Babies with that name\",\n                 y = \"Occurances\",\n                 fill = \"Sex\"\n                 )\n    })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"},{"path":"interactive-communication.html","id":"exercises-and-tutorial-6","chapter":"7 Interactive communication","heading":"7.4 Exercises and tutorial","text":"","code":""},{"path":"interactive-communication.html","id":"exercises-6","chapter":"7 Interactive communication","heading":"7.4.1 Exercises","text":"","code":""},{"path":"interactive-communication.html","id":"tutorial-6","chapter":"7 Interactive communication","heading":"7.4.2 Tutorial","text":"","code":""},{"path":"gather-data.html","id":"gather-data","chapter":"8 Gather data","heading":"8 Gather data","text":"STATUS: construction.Recommended readingBenoit, Kenneth, 2019, ‘Text data: overview,’ 17 July, https://kenbenoit.net/pdfs/28%20Benoit%20Text%20as%20Data%20draft%202.pdf.Bolton, Liza, 2019, ‘quick look museums per capita,’ 26 March, http://blog.dataembassy.co.nz/museums-per-capita/.Bryan, Jennifer, Jim Hester, 2020, Forgot Teach R, Chapter 7, https://rstats.wtf/index.html.Cardoso, Tom, 2019, ‘Introduction scraping,’ https://github.com/tomcardoso/intro--scraping.Clavelle, Tyler, 2017, ‘Using R extract data web APIs,’ 5 June, https://www.tylerclavelle.com/code/2017/randapis/.Cooksey, Brian, 2014, ‘Introduction APIs,’ Zapier, 22 April, https://zapier.com/learn/apis/.Dogucu, Mine, Mine Çetinkaya-Runde, 2020 ,‘Web Scraping Statistics Data Science Curriculum: Challenges Opportunities,’ 6 May.Gelfand, Sharla, 2019, ‘Crying @ Sephora,’ 8 November, https://sharla.party/post/crying-sephora/.Goldman, Shayna, 2019, ‘Much NHL Players Really Make? Part 2: Taxes,’ https://hockey-graphs.com/2019/01/08/-much--nhl-players-really-make-part-2-taxes/.Graham, Shawn, 2019, ‘Scraping rvest,’ 7 November, https://electricarchaeology.ca/2019/11/07/scraping--rvest/.Henze, Martin, 2020, ‘Web Scraping rvest + Astro Throwback,’ 23 January, https://heads0rtai1s.github.io/2020/01/23/rvest-intro-astro/.Hudon, Caitlin, 2017, ‘’Blue Christmas: data-driven search depressing Christmas song,’ 22 December, https://caitlinhudon.com/2017/12/22/blue-christmas/.Luscombe, Alex, 2020, ‘Gentle Introduction Tesseract OCR,’ 3 June, https://alexluscombe.ca/post/ocr-tutorial/.Luscombe, Alex, 2020, ‘Getting .pdfs R,’ 5 August, https://alexluscombe.ca/post/r-pdftools/.Luscombe, Alex, 2020, ‘Parsing .pdfs R,’ 10 August, https://alexluscombe.ca/post/parsing-pdfs/.Marshall, James, ‘HTML Made Really Easy,’ https://www.jmarshall.com/easy/html/.Marshall, James, ‘HTTP Made Really Easy,’ https://www.jmarshall.com/easy/http/.Nakagawara, Ryo, 2020, ‘Intro {polite} Web Scraping Soccer Data R!’ 14 May, https://ryo-n7.github.io/2020-05-14-webscrape-soccer-data--R/.Pavlik, Kaylin, 2020, ‘fiber types appear together yarn blends?’ 17 February, https://www.kaylinpavlik.com/ravelry-yarn-fibers/.Silge, Julia David Robinson, 2020, Text Mining R, Chapters 1, 3, 6, https://www.tidytextmining.com/.Silge, Julia, 2017, ‘Scraping CRAN rvest,’ 5 March, https://juliasilge.com/blog/scraping-cran/.Smale, David, 2020, ‘Daniel Johnston,’ https://davidsmale.netlify.com/portfolio/daniel-johnston/.Taddy, Matt, 2019, Business Data Science, Chapter 8, pp. 231-259.Wickham, Hadley, ‘Managing Secrets,’ https://cran.r-project.org/web/packages/httr/vignettes/secrets.html.Wickham, Hadley, 2014, ‘rvest: easy web scraping R,’ 24 November, https://blog.rstudio.com/2014/11/24/rvest-easy-web-scraping--r/.Wickham, Hadley, nd, ‘Getting started httr,’ https://cran.r-project.org/web/packages/httr/vignettes/quickstart.html.Recommended viewingD’Agostino McGowan, Lucy, 2020 ‘Harnessing Power Web via R Clients Web APIs,’ talk ASA Joint Statistical Meeting 2018, https://www.lucymcgowan.com/talk/asa_joint_statistical_meeting_2018/.Tatman, Rachel, 2018, ‘Character Encoding ,’ 21 February, https://youtu./2U9EHYqc59Y.Key concepts/skills/etcUse APIs possible data provider specified data like make available , conditions making available.Often R packages written make easier use APIs.Use R environments manage keys.Using verb GET (‘GET request’) means providing URL server return something, using verb POST (POST request’) means providing data server deal data.Cleaning dataGraphing data tell storyRespectfully scraping dataApproaching extracting text PDFs workflow.Planning needed start.Starting small iterating.Putting place checks.Gathering text data.Preparing text datasets.Key librariesbabynamesbroomdplyrggplot2gutenbergrjanitorjsonlitepdftoolspurrrrtweetrvestspotifyrstringitidymodelstidytexttidyverseusethisKey functions/etcas_factor()as_tibble()bind_tf_idf()c()case_when()cat()edit_r_environ()file()fromJSON()function()GET()get_artist_audio_features()get_favorites()get_my_top_artists_or_tracks()html_node()html_nodes()html_text()pdf_data()pdf_text()pmap_dfr()read_html()readRDS()safely()search_tweets()sleep()tesseract()unnest_tokens()walk2()write_html()write_lines()","code":""},{"path":"gather-data.html","id":"apis","chapter":"8 Gather data","heading":"8.1 APIs","text":"everyday language, purposes, Application Programming Interface (API) simply situation someone set specific files computer can follow instructions get . instance, use gif Slack, Slack asks Giphy’s server appropriate gif, Giphy’s server gives gif Slack Slack inserts chat. way Slack Giphy interact determined Giphy’s API. strictly, API just application runs server access using HTTP protocol.case, going focus using APIs gathering data. ’ll tailor language use toward :[]n API tool makes website’s data digestible computer. , computer can view edit data, just like person can loading pages submitting forms.Cooksey (2014), Chapter 1.instance, go Google Maps scroll click drag center map Canberra, Australia, just paste browser: https://www.google.ca/maps/@-35.2812958,149.1248113,16z. just used Google Maps API.2 result map looks something like Figure 8.1 .\nFigure 8.1: Example Google Maps, 25 January 2021.\nadvantage using API data provider specifies exactly data willing provide, terms provide . terms may include things like rate limits (.e. often can ask data), can data (e.g. maybe ’re allowed use commercial purposes, republish , whatever). Additionally, API provided specifically use , less likely subject unexpected changes. ethically legally clear API available try use .’re going run case studies interacting APIs R. first deal directly API. works handy skill , lot R packages wrap around APIs making easier use API within ‘familiar surroundings.’ ’ll also run two fun APIs R packages built around .","code":""},{"path":"gather-data.html","id":"case-study---arxiv","chapter":"8 Gather data","heading":"8.2 Case study - arXiv","text":"section introduce GET requests use API directly. use httr package (Wickham 2019c). GET request tries obtain specific data main argument url. Exactly Google Maps example! case, specific information map information .example ’ll look arXiv, repository academic articles go peer-review. ’ll ask arXiv return information paper recently uploaded former student. content returned series information paper.get variety information paper including title, abstract, authors.","code":"\n# install.packages('httr')\nlibrary(httr)\narxiv <- httr::GET('http://export.arxiv.org/api/query?id_list=2101.05225')\nclass(arxiv)\n#> [1] \"response\"\ncontent(arxiv, \"text\") %>% \n  cat(\"\\n\")\n#> <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n#> <feed xmlns=\"http://www.w3.org/2005/Atom\">\n#>   <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D2101.05225%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\n#>   <title type=\"html\">ArXiv Query: search_query=&amp;id_list=2101.05225&amp;start=0&amp;max_results=10<\/title>\n#>   <id>http://arxiv.org/api/p9UZyl2Vt0cHwPSKinDSThE23qI<\/id>\n#>   <updated>2022-01-06T00:00:00-05:00<\/updated>\n#>   <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1<\/opensearch:totalResults>\n#>   <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0<\/opensearch:startIndex>\n#>   <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10<\/opensearch:itemsPerPage>\n#>   <entry>\n#>     <id>http://arxiv.org/abs/2101.05225v1<\/id>\n#>     <updated>2021-01-13T17:37:07Z<\/updated>\n#>     <published>2021-01-13T17:37:07Z<\/published>\n#>     <title>On consistency scores in text data with an implementation in R<\/title>\n#>     <summary>  In this paper, we introduce a reproducible cleaning process for the text\n#> extracted from PDFs using n-gram models. Our approach compares the originally\n#> extracted text with the text generated from, or expected by, these models using\n#> earlier text as stimulus. To guide this process, we introduce the notion of a\n#> consistency score, which refers to the proportion of text that is expected by\n#> the model. This is used to monitor changes during the cleaning process, and\n#> across different corpuses. We illustrate our process on text from the book Jane\n#> Eyre and introduce both a Shiny application and an R package to make our\n#> process easier for others to adopt.\n#> <\/summary>\n#>     <author>\n#>       <name>Ke-Li Chiu<\/name>\n#>     <\/author>\n#>     <author>\n#>       <name>Rohan Alexander<\/name>\n#>     <\/author>\n#>     <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">13 pages, 0 figures<\/arxiv:comment>\n#>     <link href=\"http://arxiv.org/abs/2101.05225v1\" rel=\"alternate\" type=\"text/html\"/>\n#>     <link title=\"pdf\" href=\"http://arxiv.org/pdf/2101.05225v1\" rel=\"related\" type=\"application/pdf\"/>\n#>     <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n#>     <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n#>   <\/entry>\n#> <\/feed>\n#> "},{"path":"gather-data.html","id":"case-study---rtweet","chapter":"8 Gather data","heading":"8.3 Case study - rtweet","text":"Twitter rich source text data. Twitter API way Twitter ask interact Twitter order gather data. rtweet package (Kearney 2019) built around API allows us interact ways similar using R package. Initially need regular Twitter account.Get started install library need calling .get started need authorise rtweet. start process calling function package.open browser computer, log regular Twitter account shown Figure 8.2.\nFigure 8.2: rtweet authorisation page\ndone can actually get favourites save .looking recent favourite, can see Professor Bolton tweeted one stellar students ISSC.Let’s look tweeting R, using one common R hashtags: #rstats. ’ve removed retweets hopefully get actual interesting projects.look .bunch things can just using regular user account, ’re interested try examples rtweet package documentation: https://rtweet.info/index.html. available register developer (https://developer.twitter.com/en/apply--access). Twitter API document surprisingly readable, may enjoy : https://developer.twitter.com/en/docs.introduced APIs said ‘data provider specifies exactly data willing provide…’ certainly able take advantage provide continued ‘…terms provide ’ haven’t done part. particular, took tweets saved . pushed GitHub, ’s possible may accidently stored sensitive information happened tweets. taken enough tweets start reasonable statistical analysis even wasn’t sensitive information, may violated terms pushed saved tweets GitHub. Finally, linked Twitter username, case @Liza_Bolton Professor Bolton. happened ask okay, hadn’t done violating Twitter terms service.use Twitter data, please take moment look terms: https://developer.twitter.com/en/developer-terms/--restricted-use-cases.","code":"\n# install.packages('rtweet')\nlibrary(rtweet)\nlibrary(tidyverse)\nget_favorites(user = \"RohanAlexander\")\nrohans_favs <- get_favorites(\"RohanAlexander\")\n\nsaveRDS(rohans_favs, \"dont_push/rohans_favs.rds\")\nrohans_favs %>% \n  arrange(desc(created_at)) %>% \n  slice(1) %>% \n  select(screen_name, text)\n#> # A tibble: 1 × 2\n#>   screen_name text                                                              \n#>   <chr>       <chr>                                                             \n#> 1 les_ja      I've signed an offer letter, so I think I can formally announce: …\nrstats_tweets <- search_tweets(\n  q = \"#rstats\",\n  include_rts = FALSE\n)\n\nsaveRDS(rstats_tweets, \"dont_push/rstats_tweets.rds\")\nnames(rstats_tweets)\n#>  [1] \"user_id\"                 \"status_id\"              \n#>  [3] \"created_at\"              \"screen_name\"            \n#>  [5] \"text\"                    \"source\"                 \n#>  [7] \"display_text_width\"      \"reply_to_status_id\"     \n#>  [9] \"reply_to_user_id\"        \"reply_to_screen_name\"   \n#> [11] \"is_quote\"                \"is_retweet\"             \n#> [13] \"favorite_count\"          \"retweet_count\"          \n#> [15] \"quote_count\"             \"reply_count\"            \n#> [17] \"hashtags\"                \"symbols\"                \n#> [19] \"urls_url\"                \"urls_t.co\"              \n#> [21] \"urls_expanded_url\"       \"media_url\"              \n#> [23] \"media_t.co\"              \"media_expanded_url\"     \n#> [25] \"media_type\"              \"ext_media_url\"          \n#> [27] \"ext_media_t.co\"          \"ext_media_expanded_url\" \n#> [29] \"ext_media_type\"          \"mentions_user_id\"       \n#> [31] \"mentions_screen_name\"    \"lang\"                   \n#> [33] \"quoted_status_id\"        \"quoted_text\"            \n#> [35] \"quoted_created_at\"       \"quoted_source\"          \n#> [37] \"quoted_favorite_count\"   \"quoted_retweet_count\"   \n#> [39] \"quoted_user_id\"          \"quoted_screen_name\"     \n#> [41] \"quoted_name\"             \"quoted_followers_count\" \n#> [43] \"quoted_friends_count\"    \"quoted_statuses_count\"  \n#> [45] \"quoted_location\"         \"quoted_description\"     \n#> [47] \"quoted_verified\"         \"retweet_status_id\"      \n#> [49] \"retweet_text\"            \"retweet_created_at\"     \n#> [51] \"retweet_source\"          \"retweet_favorite_count\" \n#> [53] \"retweet_retweet_count\"   \"retweet_user_id\"        \n#> [55] \"retweet_screen_name\"     \"retweet_name\"           \n#> [57] \"retweet_followers_count\" \"retweet_friends_count\"  \n#> [59] \"retweet_statuses_count\"  \"retweet_location\"       \n#> [61] \"retweet_description\"     \"retweet_verified\"       \n#> [63] \"place_url\"               \"place_name\"             \n#> [65] \"place_full_name\"         \"place_type\"             \n#> [67] \"country\"                 \"country_code\"           \n#> [69] \"geo_coords\"              \"coords_coords\"          \n#> [71] \"bbox_coords\"             \"status_url\"             \n#> [73] \"name\"                    \"location\"               \n#> [75] \"description\"             \"url\"                    \n#> [77] \"protected\"               \"followers_count\"        \n#> [79] \"friends_count\"           \"listed_count\"           \n#> [81] \"statuses_count\"          \"favourites_count\"       \n#> [83] \"account_created_at\"      \"verified\"               \n#> [85] \"profile_url\"             \"profile_expanded_url\"   \n#> [87] \"account_lang\"            \"profile_banner_url\"     \n#> [89] \"profile_background_url\"  \"profile_image_url\"\n\nrstats_tweets %>% \n  select(screen_name, text) %>% \n  head()\n#> # A tibble: 6 × 2\n#>   screen_name    text                                                           \n#>   <chr>          <chr>                                                          \n#> 1 RahaPhD        \"#WFH multitasking woes:  I was just sitting here, working on …\n#> 2 AmandaKMontoya \"Teaching with the #PublishingPaidMe data this week in my intr…\n#> 3 digitalke1     \"130 #MachineLearning ProjectsSolved and Explained\\n@ruben_arc…\n#> 4 digitalke1     \"#Infographic: 6 simple steps to effectively analyse data.\\nVi…\n#> 5 dataclaudius   \"When Did the US Senate Best Reflect the US Population? via #r…\n#> 6 alexpghayes    \"has anyone written an #rstats package to interface with SNAP …"},{"path":"gather-data.html","id":"case-study---spotifyr","chapter":"8 Gather data","heading":"8.4 Case study - spotifyr","text":"next example introduce spotifyr package (Thompson et al. 2020). , wrapper developed around API, case Spotify API. install package developer’s GitHub repo using devtools (Wickham, Hester, Chang 2020).order use account, need Spotify Developer Account, can set-: https://developer.spotify.com/dashboard/. ’ll log Spotify details accept terms (’s worth looking ’ll follow ) Figure 8.3.\nFigure 8.3: rtweet authorisation page\nneed ‘Client ID’ can just fill basic details. case probably ‘don’t know’ ’re building, means Spotify requires us use non-commercial agreement, fine. order use Spotify API need Client ID Client Secret.things want keep . variety ways keeping secret, (understanding helpful package way) ’ll keep System Environment. way, push GitHub won’t included. need careful naming, spotifyr look environment specifically named keys.going use usethis package (Wickham Bryan 2020). don’t please install . file called ‘.Renviron’ open add secrets . file also controls things like default library location information available Lopp (2017) Jennifer Bryan Hester (2020).run function open file. can add Spotify secrets.Save ‘.Renviron’ file, restart R (Session -> Restart R). can now draw variable need.functions require secrets arguments now just work. instance, get information Radiohead using get_artist_audio_features(). One arguments authorization, set default look R Environment, don’t need anything .Let’s just make quick graph looking track length time.Just can, let’s settle argument. ’ve always said Radiohead quite depressing, ’re wife’s favourite band. Let’s see depressing . Spotify provides various information track, including ‘valence,’ Spotify define ‘() measure 0.0 1.0 describing musical positiveness conveyed track. Tracks high valence sound positive (e.g. happy, cheerful, euphoric), tracks low valence sound negative (e.g. sad, depressed, angry).’ Higher values happier. Let’s compare someone know likely happy - Taylor Swift - Radiohead.Finally, sake embarrassment, let’s look played artists.pretty much wife like everyone else likes, exception Ainslie Wills, Australian suspect used listen homesick.amazing live world information available little effort cost., lot package’s website: https://www.rcharlie.com/spotifyr/. nice little application Spotify API using statistical analysis Pavlik (2019).","code":"\n# devtools::install_github('charlie86/spotifyr')\nlibrary(spotifyr)\nusethis::edit_r_environ() \nSPOTIFY_CLIENT_ID = 'PUT_YOUR_CLIENT_ID_HERE'\nSPOTIFY_CLIENT_SECRET = 'PUT_YOUR_SECRET_HERE'\nradiohead <- get_artist_audio_features('radiohead')\nsaveRDS(radiohead, \"inputs/radiohead.rds\")\nradiohead <- readRDS(\"inputs/radiohead.rds\")\n\nnames(radiohead)\n#>  [1] \"artist_name\"                  \"artist_id\"                   \n#>  [3] \"album_id\"                     \"album_type\"                  \n#>  [5] \"album_images\"                 \"album_release_date\"          \n#>  [7] \"album_release_year\"           \"album_release_date_precision\"\n#>  [9] \"danceability\"                 \"energy\"                      \n#> [11] \"key\"                          \"loudness\"                    \n#> [13] \"mode\"                         \"speechiness\"                 \n#> [15] \"acousticness\"                 \"instrumentalness\"            \n#> [17] \"liveness\"                     \"valence\"                     \n#> [19] \"tempo\"                        \"track_id\"                    \n#> [21] \"analysis_url\"                 \"time_signature\"              \n#> [23] \"artists\"                      \"available_markets\"           \n#> [25] \"disc_number\"                  \"duration_ms\"                 \n#> [27] \"explicit\"                     \"track_href\"                  \n#> [29] \"is_local\"                     \"track_name\"                  \n#> [31] \"track_preview_url\"            \"track_number\"                \n#> [33] \"type\"                         \"track_uri\"                   \n#> [35] \"external_urls.spotify\"        \"album_name\"                  \n#> [37] \"key_name\"                     \"mode_name\"                   \n#> [39] \"key_mode\"\n\nradiohead %>% \n  select(artist_name, track_name, album_name) %>% \n  head()\n#>   artist_name                               track_name\n#> 1   Radiohead                      Airbag - Remastered\n#> 2   Radiohead            Paranoid Android - Remastered\n#> 3   Radiohead Subterranean Homesick Alien - Remastered\n#> 4   Radiohead     Exit Music (For a Film) - Remastered\n#> 5   Radiohead                    Let Down - Remastered\n#> 6   Radiohead                Karma Police - Remastered\n#>                      album_name\n#> 1 OK Computer OKNOTOK 1997 2017\n#> 2 OK Computer OKNOTOK 1997 2017\n#> 3 OK Computer OKNOTOK 1997 2017\n#> 4 OK Computer OKNOTOK 1997 2017\n#> 5 OK Computer OKNOTOK 1997 2017\n#> 6 OK Computer OKNOTOK 1997 2017\nradiohead %>% \n  ggplot(aes(x = album_release_year, y = duration_ms)) +\n  geom_point()\nswifty <- get_artist_audio_features('taylor swift')\nsaveRDS(swifty, \"inputs/swifty.rds\")\nswifty <- readRDS(\"inputs/swifty.rds\")\n\ntibble(name = c(swifty$artist_name, radiohead$artist_name),\n       year = c(swifty$album_release_year, radiohead$album_release_year),\n       valence = c(swifty$valence, radiohead$valence)\n               ) %>% \n  ggplot(aes(x = year, y = valence, color = name)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Year\",\n       y = \"Valence\",\n       color = \"Name\") +\n  scale_color_brewer(palette = \"Set1\")\ntop_artists <- get_my_top_artists_or_tracks(type = 'artists', time_range = 'long_term', limit = 20)\n\nsaveRDS(top_artists, \"inputs/top_artists.rds\")\ntop_artists <- readRDS(\"inputs/top_artists.rds\")\n\ntop_artists %>% \n  select(name, popularity)\n#>                   name popularity\n#> 1            Radiohead         81\n#> 2  Bombay Bicycle Club         66\n#> 3                Drake        100\n#> 4        Glass Animals         74\n#> 5                JAY-Z         85\n#> 6        Laura Marling         65\n#> 7       Sufjan Stevens         75\n#> 8      Vampire Weekend         73\n#> 9     Sturgill Simpson         65\n#> 10          Nick Drake         66\n#> 11        Dire Straits         78\n#> 12               Lorde         80\n#> 13         Marian Hill         65\n#> 14       José González         68\n#> 15       Stevie Wonder         79\n#> 16          Disclosure         82\n#> 17      Ben Folds Five         52\n#> 18       Ainslie Wills         40\n#> 19            Coldplay         89\n#> 20               alt-J         75"},{"path":"gather-data.html","id":"scraping","chapter":"8 Gather data","heading":"8.5 Scraping","text":"","code":""},{"path":"gather-data.html","id":"introduction-4","chapter":"8 Gather data","heading":"8.5.1 Introduction","text":"Web-scraping way get data websites R. Rather going website browser, write code us. opens lot data us, hand, typically data made available purposes important respectful . generally illegal, specifics regard legality web-scraping depends jurisdictions specifics ’re , also important mindful . finally, web-scraping imposes cost website host, important reduce extent ’s possible.said, web-scraping invaluable source data. typically datasets can created -product someone trying achieve another aim. instance, retailer may website products prices. created deliberately source data, can scrape create dataset. , following principles guide web-scraping.Avoid . Try use API wherever possible.Abide desires. websites file ‘robots.txt’ contains information comfortable scrapers , instance ‘https://www.google.com/robots.txt.’ one read abide .Reduce impact.\nFirstly, slow scraper, instance, rather visit website every second, slow (using sys.sleep()). ’re hundred files just visit minute, running background overnight?\nSecondly, consider timing run scraper. instance, ’s retailer set script run 10pm morning, fewer customers likely need site? ’s government website big monthly release avoid day?\nFirstly, slow scraper, instance, rather visit website every second, slow (using sys.sleep()). ’re hundred files just visit minute, running background overnight?Secondly, consider timing run scraper. instance, ’s retailer set script run 10pm morning, fewer customers likely need site? ’s government website big monthly release avoid day?Take need. instance, don’t scrape entire Wikipedia need know names 10 largest cities Canada. reduces impact website allows easily justify .scrape . Save everything go don’t re-collect data. Similarly, data, keep separate modify . course, need data time need go back, different needlessly re-scraping page.Don’t republish pages scraped. (contrast datasets create .)Take ownership ask permission possible. minimum level scripts contact details . Depending circumstances, may worthwhile asking permission scrape.","code":""},{"path":"gather-data.html","id":"getting-started-2","chapter":"8 Gather data","heading":"8.5.2 Getting started","text":"Web-scraping possible taking advantage underlying structure webpage. use patterns HTML/CSS get data want. look underlying HTML/CSS can either: 1) open browser, right-click, choose something like ‘Inspect’; 2) save website open text editor rather browser.HTML/CSS markup language comprised matching tags. want text bold use something like:Similarly, want list start end list well item.scraping search tags.get started, HTML/CSS website. Let’s say want grab name . can see name bold, want probably focus feature extract .use rvest package Wickham (2019d).language used rvest look tags ‘node,’ focus bold nodes. default html_nodes() returns tags well. can focus text contain, using html_text().result learn first name.","code":"<b>My bold text<\/b><ul>\n  <li>Learn webscraping<\/li>\n  <li>Do data science<\/li>\n  <li>Proft<\/li>\n<\/ul>\nwebsite_extract <- \"<p>Hi, I’m <b>Rohan<\/b> Alexander.<\/p>\"\n# install.packages(\"rvest\")\nlibrary(rvest)\n\nrohans_data <- read_html(website_extract)\n\nrohans_data\n#> {html_document}\n#> <html>\n#> [1] <body><p>Hi, I’m <b>Rohan<\/b> Alexander.<\/p><\/body>\nrohans_data %>% \n  html_nodes(\"b\")\n#> {xml_nodeset (1)}\n#> [1] <b>Rohan<\/b>\n\nfirst_name <- \n  rohans_data %>% \n  html_nodes(\"b\") %>%\n  html_text()\n\nfirst_name\n#> [1] \"Rohan\""},{"path":"gather-data.html","id":"case-study---rohans-books","chapter":"8 Gather data","heading":"8.6 Case study - Rohan’s books","text":"","code":""},{"path":"gather-data.html","id":"introduction-5","chapter":"8 Gather data","heading":"8.6.1 Introduction","text":"case study going scrape list books , clean , look distribution first letters author surnames. slightly complicated example , underlying approach - download website, look nodes interest, extract information, clean .","code":""},{"path":"gather-data.html","id":"gather","chapter":"8 Gather data","heading":"8.6.2 Gather","text":", key library using rvest library. makes easier download website, navigate html find aspects interested . create new project new folder (File -> New Project). Within new folder make three new folders: inputs, outputs, scripts.scripts folder write save script along lines. script loads libraries need, visits website, saves local copy.","code":"\n#### Contact details ####\n# Title: Get data from rohanalexander.com\n# Purpose: This script gets data from Rohan's website about the books that he \n# owns. It calls his website and then saves the dataset to inputs.\n# Author: Rohan Alexander\n# Contact: rohan.alexander@utoronto.ca\n# Last updated: 20 May 2020\n\n\n#### Set up workspace ####\nlibrary(rvest)\nlibrary(tidyverse)\n\n\n#### Get html ####\nrohans_data <- read_html(\"https://rohanalexander.com/bookshelf.html\")\n# This takes a website as an input and will read it into R, in the same way that we \n# can read a, say, CSV into R.\n\nwrite_html(rohans_data, \"inputs/my_website/raw_data.html\") \n# Always save your raw dataset as soon as you get it so that you have a record \n# of it. This is the equivalent of, say, write_csv() that we have used earlier."},{"path":"gather-data.html","id":"clean","chapter":"8 Gather data","heading":"8.6.3 Clean","text":"Now need navigate HTML get aspects want, put sensible structure. always try get data tibble early possible. ’s possible work nested data, move tibble usual verbs ’m used can used.scripts folder write save new R script along lines. First, need add top matter, read libraries data scraped.Now need identify data interested using html tags convert tibble. look website, notice likely trying focus list items (Figure 8.4).\nFigure 8.4: Rohan’s books\nLet’s look source (Figure 8.5).\nFigure 8.5: Source code top page\n’s lot debris, scrolling eventually get list (Figure 8.6).\nFigure 8.6: Source code list\ntag list item ‘li,’ modify earlier code focus get text.now need clean data. First want separate title authorFinally, specific cleaning needed.looks end best . ’ll just get rid manually ’s focus.","code":"\n#### Contact details ####\n# Title: Clean data from rohanaledander.com\n# Purpose: This script cleans data that was downloaded in 01-get_data.R.\n# Author: Rohan Alexander\n# Contact: rohan.alexander@utoronto.ca\n# Pre-requisites: Need to have run 01_get_data.R and have saved the data.\n# Last updated: 20 May 2020\n\n\n#### Set up workspace ####\nlibrary(tidyverse)\nlibrary(rvest)\n\nrohans_data <- read_html(\"inputs/my_website/raw_data.html\")\n\nrohans_data\n#> {html_document}\n#> <html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"\" xml:lang=\"\">\n#> [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n#> [2] <body>\\n\\n<!--radix_placeholder_front_matter-->\\n\\n<script id=\"distill-fr ...\n#### Clean data ####\n# Identify the lines that have books on them based on the list html tag\ntext_data <- rohans_data %>%\n  html_nodes(\"li\") %>%\n  html_text()\n\nall_books <- tibble(books = text_data)\n\nhead(all_books)\n#> # A tibble: 6 × 1\n#>   books                                                                         \n#>   <chr>                                                                         \n#> 1 \"-“A Little Life”, Hanya Yanighara. Recommended by Lauren.\"                   \n#> 2 \"“The Andromeda Strain”, Michael Crichton.\"                                   \n#> 3 \"“Is There Life After Housework”, Don Aslett.\\nGot given this at the Museum o…\n#> 4 \"“The Chosen”, Chaim Potok.\"                                                  \n#> 5 \"“The Forsyth Saga”, John Galsworthy.\"                                        \n#> 6 \"“Freakonomics”, Steven Levitt and Stephen Dubner.\"\n# All content is just one string, so need to separate title and author\nall_books <-\n  all_books %>%\n  separate(books, into = c(\"title\", \"author\"), sep = \"”\")\n\n# Remove leading comma and clean up the titles a little\nall_books <-\n  all_books %>%\n  mutate(author = str_remove_all(author, \"^, \"),\n         author = str_squish(author),\n         title = str_remove(title, \"“\"),\n         title = str_remove(title, \"^-\")\n         )\n\nhead(all_books)\n#> # A tibble: 6 × 2\n#>   title                         author                                          \n#>   <chr>                         <chr>                                           \n#> 1 A Little Life                 Hanya Yanighara. Recommended by Lauren.         \n#> 2 The Andromeda Strain          Michael Crichton.                               \n#> 3 Is There Life After Housework Don Aslett. Got given this at the Museum of Cle…\n#> 4 The Chosen                    Chaim Potok.                                    \n#> 5 The Forsyth Saga              John Galsworthy.                                \n#> 6 Freakonomics                  Steven Levitt and Stephen Dubner.\n# Some authors have comments after their name, so need to get rid of them, although there are some exceptions that will not work\n# J. K. Rowling.\n# M. Mitchell Waldrop.\n# David A. Price\nall_books <-\n  all_books %>%\n  mutate(author = str_replace_all(author,\n                              c(\"J. K. Rowling.\" = \"J K Rowling.\",\n                                \"M. Mitchell Waldrop.\" = \"M Mitchell Waldrop.\",\n                                \"David A. Price\" = \"David A Price\")\n                              )\n         ) %>%\n  separate(author, into = c(\"author_correct\", \"throw_away\"), sep = \"\\\\.\", extra = \"drop\") %>%\n  select(-throw_away) %>%\n  rename(author = author_correct)\n\n# Some books have multiple authors, so need to separate them\n# One has multiple authors:\n# \"Daniela Witten, Gareth James, Robert Tibshirani, and Trevor Hastie\"\nall_books <-\n  all_books %>%\n  mutate(author = str_replace(author,\n                              \"Daniela Witten, Gareth James, Robert Tibshirani, and Trevor Hastie\",\n                              \"Daniela Witten and Gareth James and Robert Tibshirani and Trevor Hastie\")) %>%\n  separate(author, into = c(\"author_first\", \"author_second\", \"author_third\", \"author_fourth\"), sep = \" and \", fill = \"right\") %>%\n  pivot_longer(cols = starts_with(\"author_\"),\n               names_to = \"author_position\",\n               values_to = \"author\") %>%\n  select(-author_position) %>%\n  filter(!is.na(author))\n\nhead(all_books)\n#> # A tibble: 6 × 2\n#>   title                         author          \n#>   <chr>                         <chr>           \n#> 1 A Little Life                 Hanya Yanighara \n#> 2 The Andromeda Strain          Michael Crichton\n#> 3 Is There Life After Housework Don Aslett      \n#> 4 The Chosen                    Chaim Potok     \n#> 5 The Forsyth Saga              John Galsworthy \n#> 6 Freakonomics                  Steven Levitt\nall_books <- \n  all_books %>% \n  slice(1:118)"},{"path":"gather-data.html","id":"explore-3","chapter":"8 Gather data","heading":"8.6.4 Explore","text":"Finally, just data now, may well try something , let’s look distribution first letter author names.","code":"\nall_books %>% \n  mutate(\n    first_letter = str_sub(author, 1, 1)\n    ) %>% \n  group_by(first_letter) %>% \n  count()\n#> # A tibble: 21 × 2\n#> # Groups:   first_letter [21]\n#>    first_letter     n\n#>    <chr>        <int>\n#>  1 \"\"               1\n#>  2 \"A\"              8\n#>  3 \"B\"              5\n#>  4 \"C\"              4\n#>  5 \"D\"             10\n#>  6 \"E\"              3\n#>  7 \"F\"              1\n#>  8 \"G\"             10\n#>  9 \"H\"              6\n#> 10 \"I\"              1\n#> # … with 11 more rows"},{"path":"gather-data.html","id":"case-study---canadian-prime-ministers","chapter":"8 Gather data","heading":"8.7 Case study - Canadian Prime Ministers","text":"","code":""},{"path":"gather-data.html","id":"introduction-6","chapter":"8 Gather data","heading":"8.7.1 Introduction","text":"case study interested long Canadian prime ministers lived, based year born. scrape data Wikipedia, clean , make graph.key library use scraping rvest. adds lot functions make life easier. said, every time scrape website things change. scrape largely bespoke, even can borrow code earlier projects completed. completely normal feel frustrated times. helps begin end mind.end, let’s generate simulated data. Ideally, want table row prime minister, column name, column birth death years. still alive, death year can empty. know birth death years somewhere 1700 1990, death year larger birth year. Finally, also know years integers, names characters. , want something looks roughly like :One advantages generating simulated dataset working groups one person can start making graph, using simulated dataset, person gathers data. terms graph, want something like Figure 8.7.\nFigure 8.7: Sketch planned graph.\n","code":"\nlibrary(babynames)\nlibrary(tidyverse)\n\nsimulated_dataset <- \n  tibble(prime_minister = sample(x = babynames %>% filter(prop > 0.01) %>% \n                                   select(name) %>% unique() %>% unlist(), \n                                 size = 10, replace = FALSE),\n         birth_year = sample(x = c(1700:1990), size = 10, replace = TRUE),\n         years_lived = sample(x = c(50:100), size = 10, replace = TRUE),\n         death_year = birth_year + years_lived) %>% \n  select(prime_minister, birth_year, death_year, years_lived) %>% \n  arrange(birth_year)\n\nhead(simulated_dataset)\n#> # A tibble: 6 × 4\n#>   prime_minister birth_year death_year years_lived\n#>   <chr>               <int>      <int>       <int>\n#> 1 Raymond              1711       1802          91\n#> 2 Eric                 1833       1893          60\n#> 3 Ryan                 1845       1929          84\n#> 4 Dolores              1893       1981          88\n#> 5 Sharon               1900       1975          75\n#> 6 Joshua               1926       2001          75"},{"path":"gather-data.html","id":"gather-1","chapter":"8 Gather data","heading":"8.7.2 Gather","text":"starting question interest, long Canadian prime minister lived. , need identify source data likely plenty data sources births deaths prime minister, want one can trust, going scraping, want one structure . Wikipedia page (https://en.wikipedia.org/wiki/List_of_prime_ministers_of_Canada) fits criteria. popular page information likely correct, data available table.load library read data relevant page. key function read_html(), can use way , say, read_csv(), except takes html page input. call read_html() page downloaded computer, usually good idea save , using write_html() raw data. Saving also means don’t keep visiting website want start cleaning, part polite. However, likely property (case Wikipedia, might okay), probably share .","code":"\nlibrary(rvest)\nraw_data <- read_html(\"https://en.wikipedia.org/wiki/List_of_prime_ministers_of_Canada\")\nwrite_html(raw_data, \"inputs/wiki/pms.html\") # Note that we save the file as a html file."},{"path":"gather-data.html","id":"clean-1","chapter":"8 Gather data","heading":"8.7.3 Clean","text":"Websites made html, markup language. looking patterns mark-can use help us get closer data want. iterative process requires lot trial error. Even simple examples take time. can look html using browser, right clicking, selecting view page source. Similarly, open html file using text editor.","code":""},{"path":"gather-data.html","id":"by-inspection","chapter":"8 Gather data","heading":"8.7.3.1 By inspection","text":"looking patterns can use select information interest - names, birth year, death year. look html looks like something going <tr>, <td> (thanks Thomas Rosenthal identifying ). select nodes using html_nodes(), takes tags input. want first one singular version, html_node().point data character vector, want convert table, reduce data just information want. key going allow us fact seems blank line (html denoted \\n) key information need. , identify line can filter just line !","code":"\n# Read in our saved data\nraw_data <- read_html(\"inputs/wiki/pms.html\")\n\n# We can parse tags in order\nparse_data_inspection <- \n  raw_data %>% \n  html_nodes(\"tr\") %>% \n  html_nodes(\"td\") %>% \n  html_text() # html_text removes any remaining html tags\n\n# But this code does exactly the same thing - the nodes are just pushed into \n# the one function call\nparse_data_inspection <- \n  raw_data %>% \n  html_nodes(\"tr td\") %>% \n  html_text()\n\nhead(parse_data_inspection)\n#> [1] \"Abbreviation key:\"                                                                                                                                                                                                                              \n#> [2] \"No.: Incumbent number, Min.: Ministry, Refs: References\\n\"                                                                                                                                                                                      \n#> [3] \"Colour key:\"                                                                                                                                                                                                                                    \n#> [4] \"\\n\\n  Liberal Party of Canada\\n \\n  Historical Conservative parties (including Liberal-Conservative, Conservative (Historical),     Unionist, National Liberal and Conservative, Progressive Conservative) \\n  Conservative Party of Canada\\n\\n\"\n#> [5] \"Provinces key:\"                                                                                                                                                                                                                                 \n#> [6] \"AB: Alberta, BC: British Columbia, MB: Manitoba, NS: Nova Scotia,ON: Ontario, QC: Quebec, SK: Saskatchewan\\n\"\nparsed_data <- \n  tibble(raw_text = parse_data_inspection) %>% # Convert the character vector to a table\n  mutate(is_PM = if_else(raw_text == \"\\n\\n\", 1, 0), # Look for the blank line that is \n         # above the row that we want\n         is_PM = lag(is_PM, n = 1)) %>% # Identify the actual row that we want\n  filter(is_PM == 1) # Just get the rows that we want\n\nhead(parsed_data)\n#> # A tibble: 6 × 2\n#>   raw_text                                                                 is_PM\n#>   <chr>                                                                    <dbl>\n#> 1 \"\\nSir John A. MacDonald(1815–1891)MP for Kingston, ON\\n\"                    1\n#> 2 \"\\nAlexander Mackenzie(1822–1892)MP for Lambton, ON\\n\"                       1\n#> 3 \"\\nSir John A. MacDonald(1815–1891)MP for Victoria, BC until 1882MP for…     1\n#> 4 \"\\nSir John Abbott(1821–1893)Senator for Quebec\\n\"                           1\n#> 5 \"\\nSir John Thompson(1845–1894)MP for Antigonish, NS\\n\"                      1\n#> 6 \"\\nSir Mackenzie Bowell(1823–1917)Senator for Ontario\\n\"                     1"},{"path":"gather-data.html","id":"using-the-selector-gadget","chapter":"8 Gather data","heading":"8.7.3.2 Using the selector gadget","text":"comfortable html might able see patterns, one tool may help SelectorGadget: https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html. allows pick choose elements want, gives input give html_nodes() (Figure 8.8)\nFigure 8.8: Using Selector Gadget identify tag, 13 March 2020.\ncase one prime minister - Robert Borden - changed party need filter away row: \\nUnionist Party\\n\".","code":"\n# Read in our saved data\nraw_data <- read_html(\"inputs/wiki/pms.html\")\n\n# We can parse tags in order\nparse_data_selector_gadget <- \n  raw_data %>% \n  html_nodes(\"td:nth-child(3)\") %>% \n  html_text() # html_text removes any remaining html tags\n\nhead(parse_data_selector_gadget)\n#> [1] \"\\nSir John A. MacDonald(1815–1891)MP for Kingston, ON\\n\"                                                            \n#> [2] \"\\nAlexander Mackenzie(1822–1892)MP for Lambton, ON\\n\"                                                               \n#> [3] \"\\nSir John A. MacDonald(1815–1891)MP for Victoria, BC until 1882MP for Carleton, ON until 1887MP for Kingston, ON\\n\"\n#> [4] \"\\nSir John Abbott(1821–1893)Senator for Quebec\\n\"                                                                   \n#> [5] \"\\nSir John Thompson(1845–1894)MP for Antigonish, NS\\n\"                                                              \n#> [6] \"\\nSir Mackenzie Bowell(1823–1917)Senator for Ontario\\n\""},{"path":"gather-data.html","id":"clean-data","chapter":"8 Gather data","heading":"8.7.3.3 Clean data","text":"Now parsed data, need clean match wanted. particular want names column, well columns birth year death year. use separate() take advantage fact looks like dates distinguished brackets.Finally, need clean columns.","code":"\ninitial_clean <- \n  parsed_data %>% \n  separate(raw_text, \n            into = c(\"Name\", \"not_name\"), \n            sep = \"\\\\(\",\n            remove = FALSE) %>% # The remove = FALSE option here means that we \n  # keep the original column that we are separating.\n  separate(not_name, \n            into = c(\"Date\", \"all_the_rest\"), \n            sep = \"\\\\)\",\n            remove = FALSE)\n\nhead(initial_clean)\n#> # A tibble: 6 × 6\n#>   raw_text           Name     not_name          Date  all_the_rest         is_PM\n#>   <chr>              <chr>    <chr>             <chr> <chr>                <dbl>\n#> 1 \"\\nSir John A. Ma… \"\\nSir … \"1815–1891)MP fo… 1815… \"MP for Kingston, O…     1\n#> 2 \"\\nAlexander Mack… \"\\nAlex… \"1822–1892)MP fo… 1822… \"MP for Lambton, ON…     1\n#> 3 \"\\nSir John A. Ma… \"\\nSir … \"1815–1891)MP fo… 1815… \"MP for Victoria, B…     1\n#> 4 \"\\nSir John Abbot… \"\\nSir … \"1821–1893)Senat… 1821… \"Senator for Quebec…     1\n#> 5 \"\\nSir John Thomp… \"\\nSir … \"1845–1894)MP fo… 1845… \"MP for Antigonish,…     1\n#> 6 \"\\nSir Mackenzie … \"\\nSir … \"1823–1917)Senat… 1823… \"Senator for Ontari…     1\ncleaned_data <- \n  initial_clean %>% \n  select(Name, Date) %>% \n  separate(Date, into = c(\"Birth\", \"Died\"), sep = \"–\", remove = FALSE) %>% # The \n  # PMs who have died have their birth and death years separated by a hyphen, but \n  # you need to be careful with the hyphen as it seems to be a slightly odd type of \n  # hyphen and you need to copy/paste it.\n  mutate(Birth = str_remove(Birth, \"b. \")) %>% # Alive PMs have slightly different format\n  select(-Date) %>% \n  mutate(Name = str_remove(Name, \"\\n\")) %>% # Remove some html tags that remain\n  mutate_at(vars(Birth, Died), ~as.integer(.)) %>% # Change birth and death to integers\n  mutate(Age_at_Death = Died - Birth) %>%  # Add column of the number of years they lived\n  distinct() # Some of the PMs had two goes at it.\n\nhead(cleaned_data)\n#> # A tibble: 6 × 4\n#>   Name                  Birth  Died Age_at_Death\n#>   <chr>                 <int> <int>        <int>\n#> 1 Sir John A. MacDonald  1815  1891           76\n#> 2 Alexander Mackenzie    1822  1892           70\n#> 3 Sir John Abbott        1821  1893           72\n#> 4 Sir John Thompson      1845  1894           49\n#> 5 Sir Mackenzie Bowell   1823  1917           94\n#> 6 Sir Charles Tupper     1821  1915           94"},{"path":"gather-data.html","id":"explore-4","chapter":"8 Gather data","heading":"8.7.4 Explore","text":"point ’d like make graph illustrates long prime minister lived. still alive like run end, like colour differently.","code":"\ncleaned_data %>% \n  mutate(still_alive = if_else(is.na(Died), \"Yes\", \"No\"),\n         Died = if_else(is.na(Died), as.integer(2020), Died)) %>% \n  mutate(Name = as_factor(Name)) %>% \n  ggplot(aes(x = Birth, \n             xend = Died,\n             y = Name,\n             yend = Name, \n             color = still_alive)) +\n  geom_segment() +\n  labs(x = \"Year of birth\",\n       y = \"Prime minister\",\n       color = \"PM is alive\",\n       title = \"Canadian Prime Minister, by year of birth\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")"},{"path":"gather-data.html","id":"pdfs","chapter":"8 Gather data","heading":"8.8 PDFs","text":"","code":""},{"path":"gather-data.html","id":"introduction-7","chapter":"8 Gather data","heading":"8.8.1 Introduction","text":"contrast API, PDF usually produced human (computer) consumption. nice thing PDFs static constant. nice make data available . trade-:overly useful larger-scale statistical analysis.don’t know PDF put together don’t know whether can trust .can’t manipulate data get results interested .Indeed, sometimes governments publish data PDFs don’t actually want able analyse ! able get data PDFs opens large number datasets , ’ll see chapter.two important aspects keep mind approaching PDF mind extracting data :Begin end mind. Planning literally sketching want final dataset/graph/paper stops wasting time keeps focused.Start simple, iterate. quickest way make complicated model often first build simple model complicate . Start just trying get one page PDF working even just one line. iterate .chapter start walking several examples go three case studies varying difficulty.","code":""},{"path":"gather-data.html","id":"getting-started-3","chapter":"8 Gather data","heading":"8.8.2 Getting started","text":"Figure 8.9 PDF consists just first sentence Jane Eyre taken Project Gutenberg Bronte (1847).\nFigure 8.9: First sentence Jane Eyre\nuse package pdftools Ooms (2019a) get text one page PDF R.can see PDF correctly read , character vector.now try slightly complicated example consists first paragraphs Jane Eyre (Figure 8.10). Also notice now chapter heading well.\nFigure 8.10: First paragraphs Jane Eyre\nuse function ., character vector. end line signalled ‘\\n,’ looks pretty good.Finally, consider first two pages.use function .Now, notice first page first element character vector second page second element.’re familiar rectangular data ’ll try get format quickly possible. can use regular tools deal .First want convert character vector tibble. point may like add page numbers well.probably now want separate lines line observation. can looking ‘\\n’ remembering need escape backslash ’s special character.","code":"\n# install.packages(\"pdftools\")\nlibrary(pdftools)\nlibrary(tidyverse)\n\nfirst_example <- pdftools::pdf_text(\"inputs/pdfs/first_example.pdf\")\n\nfirst_example\n#> [1] \"There was no possibility of taking a walk that day.\\n\"\n\nclass(first_example)\n#> [1] \"character\"\nsecond_example <- pdftools::pdf_text(\"inputs/pdfs/second_example.pdf\")\n\nsecond_example\n#> [1] \"CHAPTER I\\nThere was no possibility of taking a walk that day. We had been wandering, indeed, in the\\nleafless shrubbery an hour in the morning; but since dinner (Mrs. Reed, when there was no\\ncompany, dined early) the cold winter wind had brought with it clouds so sombre, and a rain so\\npenetrating, that further out-door exercise was now out of the question.\\n\\nI was glad of it: I never liked long walks, especially on chilly afternoons: dreadful to me was the\\ncoming home in the raw twilight, with nipped fingers and toes, and a heart saddened by the\\nchidings of Bessie, the nurse, and humbled by the consciousness of my physical inferiority to\\nEliza, John, and Georgiana Reed.\\n\\nThe said Eliza, John, and Georgiana were now clustered round their mama in the drawing-room:\\nshe lay reclined on a sofa by the fireside, and with her darlings about her (for the time neither\\nquarrelling nor crying) looked perfectly happy. Me, she had dispensed from joining the group;\\nsaying, “She regretted to be under the necessity of keeping me at a distance; but that until she\\nheard from Bessie, and could discover by her own observation, that I was endeavouring in good\\nearnest to acquire a more sociable and childlike disposition, a more attractive and sprightly\\nmanner—something lighter, franker, more natural, as it were—she really must exclude me from\\nprivileges intended only for contented, happy, little children.”\\n\\n“What does Bessie say I have done?” I asked.\\n\\n“Jane, I don’t like cavillers or questioners; besides, there is something truly forbidding in a child\\ntaking up her elders in that manner. Be seated somewhere; and until you can speak pleasantly,\\nremain silent.”\\n\\nA breakfast-room adjoined the drawing-room, I slipped in there. It contained a bookcase: I soon\\npossessed myself of a volume, taking care that it should be one stored with pictures. I mounted\\ninto the window-seat: gathering up my feet, I sat cross-legged, like a Turk; and, having drawn the\\nred moreen curtain nearly close, I was shrined in double retirement.\\n\\nFolds of scarlet drapery shut in my view to the right hand; to the left were the clear panes of\\nglass, protecting, but not separating me from the drear November day. At intervals, while\\nturning over the leaves of my book, I studied the aspect of that winter afternoon. Afar, it offered\\na pale blank of mist and cloud; near a scene of wet lawn and storm-beat shrub, with ceaseless\\nrain sweeping away wildly before a long and lamentable blast.\\n\"\n\nclass(second_example)\n#> [1] \"character\"\nthird_example <- pdftools::pdf_text(\"inputs/pdfs/third_example.pdf\")\n\nthird_example\n#> [1] \"CHAPTER I\\nThere was no possibility of taking a walk that day. We had been wandering, indeed, in the\\nleafless shrubbery an hour in the morning; but since dinner (Mrs. Reed, when there was no\\ncompany, dined early) the cold winter wind had brought with it clouds so sombre, and a rain so\\npenetrating, that further out-door exercise was now out of the question.\\n\\nI was glad of it: I never liked long walks, especially on chilly afternoons: dreadful to me was the\\ncoming home in the raw twilight, with nipped fingers and toes, and a heart saddened by the\\nchidings of Bessie, the nurse, and humbled by the consciousness of my physical inferiority to\\nEliza, John, and Georgiana Reed.\\n\\nThe said Eliza, John, and Georgiana were now clustered round their mama in the drawing-room:\\nshe lay reclined on a sofa by the fireside, and with her darlings about her (for the time neither\\nquarrelling nor crying) looked perfectly happy. Me, she had dispensed from joining the group;\\nsaying, “She regretted to be under the necessity of keeping me at a distance; but that until she\\nheard from Bessie, and could discover by her own observation, that I was endeavouring in good\\nearnest to acquire a more sociable and childlike disposition, a more attractive and sprightly\\nmanner—something lighter, franker, more natural, as it were—she really must exclude me from\\nprivileges intended only for contented, happy, little children.”\\n\\n“What does Bessie say I have done?” I asked.\\n\\n“Jane, I don’t like cavillers or questioners; besides, there is something truly forbidding in a child\\ntaking up her elders in that manner. Be seated somewhere; and until you can speak pleasantly,\\nremain silent.”\\n\\nA breakfast-room adjoined the drawing-room, I slipped in there. It contained a bookcase: I soon\\npossessed myself of a volume, taking care that it should be one stored with pictures. I mounted\\ninto the window-seat: gathering up my feet, I sat cross-legged, like a Turk; and, having drawn the\\nred moreen curtain nearly close, I was shrined in double retirement.\\n\\nFolds of scarlet drapery shut in my view to the right hand; to the left were the clear panes of\\nglass, protecting, but not separating me from the drear November day. At intervals, while\\nturning over the leaves of my book, I studied the aspect of that winter afternoon. Afar, it offered\\na pale blank of mist and cloud; near a scene of wet lawn and storm-beat shrub, with ceaseless\\nrain sweeping away wildly before a long and lamentable blast.\\n\\nI returned to my book—Bewick’s History of British Birds: the letterpress thereof I cared little\\nfor, generally speaking; and yet there were certain introductory pages that, child as I was, I could\\nnot pass quite as a blank. They were those which treat of the haunts of sea-fowl; of “the solitary\\nrocks and promontories” by them only inhabited; of the coast of Norway, studded with isles from\\nits southern extremity, the Lindeness, or Naze, to the North Cape—\\n\\n“Where the Northern Ocean, in vast whirls,\\nBoils round the naked, melancholy isles\\n\"\n#> [2] \"Of farthest Thule; and the Atlantic surge\\nPours in among the stormy Hebrides.”\\n\\nNor could I pass unnoticed the suggestion of the bleak shores of Lapland, Siberia, Spitzbergen,\\nNova Zembla, Iceland, Greenland, with “the vast sweep of the Arctic Zone, and those forlorn\\nregions of dreary space,—that reservoir of frost and snow, where firm fields of ice, the\\naccumulation of centuries of winters, glazed in Alpine heights above heights, surround the pole,\\nand concentre the multiplied rigours of extreme cold.” Of these death-white realms I formed an\\nidea of my own: shadowy, like all the half-comprehended notions that float dim through\\nchildren’s brains, but strangely impressive. The words in these introductory pages connected\\nthemselves with the succeeding vignettes, and gave significance to the rock standing up alone in\\na sea of billow and spray; to the broken boat stranded on a desolate coast; to the cold and ghastly\\nmoon glancing through bars of cloud at a wreck just sinking.\\n\\nI cannot tell what sentiment haunted the quite solitary churchyard, with its inscribed headstone;\\nits gate, its two trees, its low horizon, girdled by a broken wall, and its newly-risen crescent,\\nattesting the hour of eventide.\\n\\nThe two ships becalmed on a torpid sea, I believed to be marine phantoms.\\n\\nThe fiend pinning down the thief’s pack behind him, I passed over quickly: it was an object of\\nterror.\\n\\nSo was the black horned thing seated aloof on a rock, surveying a distant crowd surrounding a\\ngallows.\\n\\nEach picture told a story; mysterious often to my undeveloped understanding and imperfect\\nfeelings, yet ever profoundly interesting: as interesting as the tales Bessie sometimes narrated on\\nwinter evenings, when she chanced to be in good humour; and when, having brought her ironing-\\ntable to the nursery hearth, she allowed us to sit about it, and while she got up Mrs. Reed’s lace\\nfrills, and crimped her nightcap borders, fed our eager attention with passages of love and\\nadventure taken from old fairy tales and other ballads; or (as at a later period I discovered) from\\nthe pages of Pamela, and Henry, Earl of Moreland.\\n\\nWith Bewick on my knee, I was then happy: happy at least in my way. I feared nothing but\\ninterruption, and that came too soon. The breakfast-room door opened.\\n\\n“Boh! Madam Mope!” cried the voice of John Reed; then he paused: he found the room\\napparently empty.\\n\\n“Where the dickens is she!” he continued. “Lizzy! Georgy! (calling to his sisters) Joan is not\\nhere: tell mama she is run out into the rain—bad animal!”\\n\\n“It is well I drew the curtain,” thought I; and I wished fervently he might not discover my hiding-\\nplace: nor would John Reed have found it out himself; he was not quick either of vision or\\nconception; but Eliza just put her head in at the door, and said at once—\\n\"\n\nclass(third_example)\n#> [1] \"character\"\njane_eyre <- tibble(raw_text = third_example,\n                    page_number = c(1:2))\njane_eyre <- separate_rows(jane_eyre, raw_text, sep = \"\\\\n\", convert = FALSE)\nhead(jane_eyre)\n#> # A tibble: 6 × 2\n#>   raw_text                                                           page_number\n#>   <chr>                                                                    <int>\n#> 1 \"CHAPTER I\"                                                                  1\n#> 2 \"There was no possibility of taking a walk that day. We had been …           1\n#> 3 \"leafless shrubbery an hour in the morning; but since dinner (Mrs…           1\n#> 4 \"company, dined early) the cold winter wind had brought with it c…           1\n#> 5 \"penetrating, that further out-door exercise was now out of the q…           1\n#> 6 \"\"                                                                           1"},{"path":"gather-data.html","id":"case-study-us-total-fertility-rate-by-state-and-year-2000-2018","chapter":"8 Gather data","heading":"8.9 Case-study: US Total Fertility Rate, by state and year (2000-2018)","text":"","code":""},{"path":"gather-data.html","id":"introduction-8","chapter":"8 Gather data","heading":"8.9.1 Introduction","text":"’re married demographer long asked look US Department Health Human Services Vital Statistics Report. case interested trying get total fertility rate (average number births per woman assuming woman experience current age-specific fertility rates throughout reproductive years)3 state nineteen years. Annoyingly, US persists making data available PDFs, makes nice case study.case year 2000 table interested page 40 PDF available https://www.cdc.gov/nchs/data/nvsr/nvsr50/nvsr50_05.pdf column labelled: “Total fertility rate” (Figure 8.11).\nFigure 8.11: Example Vital Statistics Report, 2000\n","code":""},{"path":"gather-data.html","id":"begin-with-an-end-in-mind","chapter":"8 Gather data","heading":"8.9.2 Begin with an end in mind","text":"first step getting data PDF sketch eventually want. PDF typically contains lot information, handy clear need. helps keep focused, prevents scope creep, also helpful thinking data checks. Literally write paper mind.case, needed table column state, year TFR (Figure 8.12).\nFigure 8.12: Desired output PDF\n","code":""},{"path":"gather-data.html","id":"start-simple-then-iterate.","chapter":"8 Gather data","heading":"8.9.3 Start simple, then iterate.","text":"19 different PDFs, interested particular column particular table . Unfortunately, nothing magical coming. first step requires working link , page column name interest. end, looks like .first step get code works one . ’ll step code lot detail normal ’re going use pieces lot.choose year 2000. first download data save .now want read PDF character vector.Convert tibble, can use familiar verbs .Grab page interest (remembering page element character vector, hence row tibble).Now want separate rows.Now searching patterns can use. (lot tables interested grabbing PDFs may also worthwhile considering tabulizer package specifically designed (Leeper 2018). issue depends Java always seem run trouble need use Java avoid can.)Let’s look first ten lines content.doesn’t get much better :dots separating states data.space columns.can now separate separate columns. First want match least two dots (remembering dot special character needs escaped).get expected warnings top bottom don’t multiple dots.(Another option use pdf_data() function allow us use location rather delimiters.)can now separate data based spaces. inconsistent number spaces, first squish example one space just one.looking fairly great. thing left clean .’re done year. Now want take pieces, put function run function 19 years.","code":"\nmonicas_data <- read_csv(\"inputs/tfr_tables_info.csv\")\n\nmonicas_data %>% \n  select(year, page, table, column_name, url) %>% \n  gt()\ndownload.file(url = monicas_data$url[1], \n              destfile = \"inputs/pdfs/dhs/year_2000.pdf\")\ndhs_2000 <- pdftools::pdf_text(\"inputs/pdfs/dhs/year_2000.pdf\")\ndhs_2000 <- tibble(raw_data = dhs_2000)\n\nhead(dhs_2000)\n#> # A tibble: 6 × 1\n#>   raw_data                                                                      \n#>   <chr>                                                                         \n#> 1 \"Volume 50, Number 5                                                         …\n#> 2 \"2   National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\\n\\n\\…\n#> 3 \"                                                                            …\n#> 4 \"4   National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\\n\\n\\…\n#> 5 \"                                                                            …\n#> 6 \"6   National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\\n\\n …\ndhs_2000 <- \n  dhs_2000 %>% \n  slice(monicas_data$page[1])\n\nhead(dhs_2000)\n#> # A tibble: 1 × 1\n#>   raw_data                                                                      \n#>   <chr>                                                                         \n#> 1 \"40 National Vital Statistics Report, Vol. 50, No. 5, Revised May 15, 20022\\n…\ndhs_2000 <- \n  dhs_2000 %>% \n  separate_rows(raw_data, sep = \"\\\\n\", convert = FALSE)\n\nhead(dhs_2000)\n#> # A tibble: 6 × 1\n#>   raw_data                                                                      \n#>   <chr>                                                                         \n#> 1 \"40 National Vital Statistics Report, Vol. 50, No. 5, Revised May 15, 20022\"  \n#> 2 \"\"                                                                            \n#> 3 \"Table 10. Number of births, birth rates, fertility rates, total fertility ra…\n#> 4 \"United States, each State and territory, 2000\"                               \n#> 5 \"[By place of residence. Birth rates are live births per 1,000 estimated popu…\n#> 6 \"estimated in each area; total fertility rates are sums of birth rates for 5-…\ndhs_2000[13:22,]\n#> # A tibble: 10 × 1\n#>    raw_data                                                                     \n#>    <chr>                                                                        \n#>  1 \"                                  State                                    …\n#>  2 \"                                                                           …\n#>  3 \"                                                                           …\n#>  4 \"\"                                                                           \n#>  5 \"\"                                                                           \n#>  6 \"United States 1 ......................................................     …\n#>  7 \"\"                                                                           \n#>  8 \"Alabama ...............................................................    …\n#>  9 \"Alaska ................................................................... …\n#> 10 \"Arizona .................................................................  …\ndhs_2000 <- \n  dhs_2000 %>% \n  separate(col = raw_data, \n           into = c(\"state\", \"data\"), \n           sep = \"\\\\.{2,}\", \n           remove = FALSE,\n           fill = \"right\"\n           )\n\nhead(dhs_2000)\n#> # A tibble: 6 × 3\n#>   raw_data                              state                              data \n#>   <chr>                                 <chr>                              <chr>\n#> 1 \"40 National Vital Statistics Report… \"40 National Vital Statistics Rep… <NA> \n#> 2 \"\"                                    \"\"                                 <NA> \n#> 3 \"Table 10. Number of births, birth r… \"Table 10. Number of births, birt… <NA> \n#> 4 \"United States, each State and terri… \"United States, each State and te… <NA> \n#> 5 \"[By place of residence. Birth rates… \"[By place of residence. Birth ra… <NA> \n#> 6 \"estimated in each area; total ferti… \"estimated in each area; total fe… <NA>\ndhs_2000 <- \n  dhs_2000 %>%\n  mutate(data = str_squish(data)) %>% \n  tidyr::separate(col = data, \n           into = c(\"number_of_births\", \n                    \"birth_rate\", \n                    \"fertility_rate\", \n                    \"TFR\", \n                    \"teen_births_all\", \n                    \"teen_births_15_17\", \n                    \"teen_births_18_19\"), \n           sep = \"\\\\s\", \n           remove = FALSE\n           )\n\nhead(dhs_2000)\n#> # A tibble: 6 × 10\n#>   raw_data      state     data  number_of_births birth_rate fertility_rate TFR  \n#>   <chr>         <chr>     <chr> <chr>            <chr>      <chr>          <chr>\n#> 1 \"40 National… \"40 Nati… <NA>  <NA>             <NA>       <NA>           <NA> \n#> 2 \"\"            \"\"        <NA>  <NA>             <NA>       <NA>           <NA> \n#> 3 \"Table 10. N… \"Table 1… <NA>  <NA>             <NA>       <NA>           <NA> \n#> 4 \"United Stat… \"United … <NA>  <NA>             <NA>       <NA>           <NA> \n#> 5 \"[By place o… \"[By pla… <NA>  <NA>             <NA>       <NA>           <NA> \n#> 6 \"estimated i… \"estimat… <NA>  <NA>             <NA>       <NA>           <NA> \n#> # … with 3 more variables: teen_births_all <chr>, teen_births_15_17 <chr>,\n#> #   teen_births_18_19 <chr>\ndhs_2000 <- \n  dhs_2000 %>% \n  select(state, TFR) %>% \n  slice(13:69) %>% \n  mutate(year = 2000)\n\ndhs_2000\n#> # A tibble: 57 × 3\n#>    state                                                            TFR     year\n#>    <chr>                                                            <chr>  <dbl>\n#>  1 \"                                  State                       … <NA>    2000\n#>  2 \"                                                              … <NA>    2000\n#>  3 \"                                                              … <NA>    2000\n#>  4 \"\"                                                               <NA>    2000\n#>  5 \"\"                                                               <NA>    2000\n#>  6 \"United States 1 \"                                               2,130…  2000\n#>  7 \"\"                                                               <NA>    2000\n#>  8 \"Alabama \"                                                       2,021…  2000\n#>  9 \"Alaska \"                                                        2,437…  2000\n#> 10 \"Arizona \"                                                       2,652…  2000\n#> # … with 47 more rows"},{"path":"gather-data.html","id":"iterating","chapter":"8 Gather data","heading":"8.9.4 Iterating","text":"","code":""},{"path":"gather-data.html","id":"get-the-pdfs","chapter":"8 Gather data","heading":"8.9.4.1 Get the PDFs","text":"first part downloading 19 PDFs need. ’re going build code used . code :modify need:iterate lines dataset contains CSVs (.e. says 1, want 1, 2, 3, etc.).filename, need iterate desired filenames (.e. year_2000, year_2001, year_2002, etc).’d like way little robust errors. instance, one URLs wrong internet drops ’d like just move onto next PDF, warn us end missed one, stop. (doesn’t really matter ’s 19 files, ’s pretty easy find thousands files).draw purrr package Henry Wickham (2020).code take function download.file() give two arguments: .x .y. function walk2() applies function inputs give , case URLs columns .x pdf_names column .y. Finally, safely() function means failures just moves onto next file instead throwing error.now PDFs saved can move onto getting data .","code":"\ndownload.file(url = monicas_data$url[1], destfile = \"inputs/pdfs/dhs/year_2000.pdf\")\nlibrary(purrr)\nmonicas_data <- \n  monicas_data %>% \n  mutate(pdf_name = paste0(\"inputs/pdfs/dhs/year_\", year, \".pdf\"))\npurrr::walk2(monicas_data$url, monicas_data$pdf_name, purrr::safely(~download.file(.x , .y)))"},{"path":"gather-data.html","id":"get-data-from-the-pdfs","chapter":"8 Gather data","heading":"8.9.4.2 Get data from the PDFs","text":"Now need get data PDFs. , ’re going build code used . code (overly condensed) :bunch aspects hardcoded, first thing want iterate argument pdf_text(), number slice() also need change (work get page interested ).Two aspects hardcoded, may need updated. particular: 1) separate works tables columns order; 2) slice (restricts data just states) works particular case. Finally, add year end, whereas ’d need bring earlier process.’ll start writing function go files, grab data, get page interest, expand rows. ’ll use function purrr apply function PDFs output tibble.Now need clean state names filter .next step separate data get correct column . ’re going separate based spaces cleaned .can now grab correct column.Finally, need convert case.run checks.particular want 51 states 19 years.’re done.","code":"\ndhs_2000 <- pdftools::pdf_text(\"inputs/pdfs/dhs/year_2000.pdf\")\n\ndhs_2000 <- \n  tibble(raw_data = dhs_2000) %>% \n  slice(monicas_data$page[1]) %>% \n  separate_rows(raw_data, sep = \"\\\\n\", convert = FALSE) %>% \n  separate(col = raw_data, into = c(\"state\", \"data\"), sep = \"\\\\.{2,}\", remove = FALSE) %>% \n  mutate(data = str_squish(data)) %>% \n  separate(col = data, \n           into = c(\"number_of_births\", \"birth_rate\", \"fertility_rate\", \"TFR\", \"teen_births_all\", \"teen_births_15_17\", \"teen_births_18_19\"), \n           sep = \"\\\\s\", \n           remove = FALSE) %>% \n  select(state, TFR) %>% \n  slice(13:69) %>% \n  mutate(year = 2000)\n\ndhs_2000\nget_pdf_convert_to_tibble <- function(pdf_name, page, year){\n  \n  dhs_table_of_interest <- \n    tibble(raw_data = pdftools::pdf_text(pdf_name)) %>% \n    slice(page) %>% \n    separate_rows(raw_data, sep = \"\\\\n\", convert = FALSE) %>% \n    separate(col = raw_data, \n             into = c(\"state\", \"data\"), \n             sep = \"[�|\\\\.]\\\\s+(?=[[:digit:]])\", \n             remove = FALSE) %>% \n    mutate(\n      data = str_squish(data),\n      year_of_data = year)\n\n  print(paste(\"Done with\", year))\n  \n  return(dhs_table_of_interest)\n}\n\nraw_dhs_data <- purrr::pmap_dfr(monicas_data %>% select(pdf_name, page, year),\n                                get_pdf_convert_to_tibble)\n#> [1] \"Done with 2000\"\n#> [1] \"Done with 2001\"\n#> [1] \"Done with 2002\"\n#> [1] \"Done with 2003\"\n#> [1] \"Done with 2004\"\n#> [1] \"Done with 2005\"\n#> [1] \"Done with 2006\"\n#> [1] \"Done with 2007\"\n#> [1] \"Done with 2008\"\n#> [1] \"Done with 2009\"\n#> [1] \"Done with 2010\"\n#> [1] \"Done with 2011\"\n#> [1] \"Done with 2012\"\n#> [1] \"Done with 2013\"\n#> [1] \"Done with 2014\"\n#> [1] \"Done with 2015\"\n#> [1] \"Done with 2016\"\n#> [1] \"Done with 2016\"\n#> [1] \"Done with 2017\"\n#> [1] \"Done with 2017\"\n#> [1] \"Done with 2018\"\n\nhead(raw_dhs_data)\n#> # A tibble: 6 × 4\n#>   raw_data                       state                        data  year_of_data\n#>   <chr>                          <chr>                        <chr>        <dbl>\n#> 1 \"40 National Vital Statistics… \"40 National Vital Statisti… 50, …         2000\n#> 2 \"\"                             \"\"                           <NA>          2000\n#> 3 \"Table 10. Number of births, … \"Table 10. Number of births… <NA>          2000\n#> 4 \"United States, each State an… \"United States, each State … <NA>          2000\n#> 5 \"[By place of residence. Birt… \"[By place of residence. Bi… <NA>          2000\n#> 6 \"estimated in each area; tota… \"estimated in each area; to… <NA>          2000\nstates <- c(\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \n            \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \n            \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \n            \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \n            \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \n            \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \n            \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \n            \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \n            \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \n            \"Wyoming\", \"District of Columbia\")\n\nraw_dhs_data <- \n  raw_dhs_data %>% \n  mutate(state = str_remove_all(state, \"\\\\.\"),\n         state = str_remove_all(state, \"�\"),\n         state = str_remove_all(state, \"\\u0008\"),\n         state = str_replace_all(state, \"United States 1\", \"United States\"),\n         state = str_replace_all(state, \"United States1\", \"United States\"),\n         state = str_replace_all(state, \"United States 2\", \"United States\"),\n         state = str_replace_all(state, \"United States2\", \"United States\"),\n         state = str_replace_all(state, \"United States²\", \"United States\"),\n         ) %>% \n  mutate(state = str_squish(state)) %>% \n  filter(state %in% states)\n\nhead(raw_dhs_data)\n#> # A tibble: 6 × 4\n#>   raw_data                              state   data                year_of_data\n#>   <chr>                                 <chr>   <chr>                      <dbl>\n#> 1 Alabama ............................… Alabama 63,299 14.4 65.0 2…         2000\n#> 2 Alaska .............................… Alaska  9,974 16.0 74.6 2,…         2000\n#> 3 Arizona ............................… Arizona 85,273 17.5 84.4 2…         2000\n#> 4 Arkansas ...........................… Arkans… 37,783 14.7 69.1 2…         2000\n#> 5 California .........................… Califo… 531,959 15.8 70.7 …         2000\n#> 6 Colorado ...........................… Colora… 65,438 15.8 73.1 2…         2000\nraw_dhs_data <- \n  raw_dhs_data %>% \n  mutate(data = str_remove_all(data, \"\\\\*\")) %>% \n  separate(data, into = c(\"col_1\", \"col_2\", \"col_3\", \"col_4\", \"col_5\", \n                          \"col_6\", \"col_7\", \"col_8\", \"col_9\", \"col_10\"), \n           sep = \" \",\n           remove = FALSE)\nhead(raw_dhs_data)\n#> # A tibble: 6 × 14\n#>   raw_data    state data   col_1 col_2 col_3 col_4 col_5 col_6 col_7 col_8 col_9\n#>   <chr>       <chr> <chr>  <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n#> 1 Alabama ..… Alab… 63,29… 63,2… 14.4  65.0  2,02… 62.9  37.9  97.3  <NA>  <NA> \n#> 2 Alaska ...… Alas… 9,974… 9,974 16.0  74.6  2,43… 42.4  23.6  69.4  <NA>  <NA> \n#> 3 Arizona ..… Ariz… 85,27… 85,2… 17.5  84.4  2,65… 69.1  41.1  111.3 <NA>  <NA> \n#> 4 Arkansas .… Arka… 37,78… 37,7… 14.7  69.1  2,14… 68.5  36.7  114.1 <NA>  <NA> \n#> 5 California… Cali… 531,9… 531,… 15.8  70.7  2,18… 48.5  28.6  75.6  <NA>  <NA> \n#> 6 Colorado .… Colo… 65,43… 65,4… 15.8  73.1  2,35… 49.2  28.6  79.8  <NA>  <NA> \n#> # … with 2 more variables: col_10 <chr>, year_of_data <dbl>\ntfr_data <- \n  raw_dhs_data %>% \n  mutate(TFR = if_else(year_of_data < 2008, col_4, col_3)) %>% \n  select(state, year_of_data, TFR) %>% \n  rename(year = year_of_data)\nhead(tfr_data)\n#> # A tibble: 6 × 3\n#>   state       year TFR    \n#>   <chr>      <dbl> <chr>  \n#> 1 Alabama     2000 2,021.0\n#> 2 Alaska      2000 2,437.0\n#> 3 Arizona     2000 2,652.5\n#> 4 Arkansas    2000 2,140.0\n#> 5 California  2000 2,186.0\n#> 6 Colorado    2000 2,356.5\nhead(tfr_data)\n#> # A tibble: 6 × 3\n#>   state       year TFR    \n#>   <chr>      <dbl> <chr>  \n#> 1 Alabama     2000 2,021.0\n#> 2 Alaska      2000 2,437.0\n#> 3 Arizona     2000 2,652.5\n#> 4 Arkansas    2000 2,140.0\n#> 5 California  2000 2,186.0\n#> 6 Colorado    2000 2,356.5\n\ntfr_data <- \n  tfr_data %>% \n  mutate(TFR = str_remove_all(TFR, \",\"),\n         TFR = as.numeric(TFR))\n\nhead(tfr_data)\n#> # A tibble: 6 × 3\n#>   state       year   TFR\n#>   <chr>      <dbl> <dbl>\n#> 1 Alabama     2000 2021 \n#> 2 Alaska      2000 2437 \n#> 3 Arizona     2000 2652.\n#> 4 Arkansas    2000 2140 \n#> 5 California  2000 2186 \n#> 6 Colorado    2000 2356.\n# tfr_data %>% \n#   skimr::skim()\nhead(tfr_data)\n#> # A tibble: 6 × 3\n#>   state       year   TFR\n#>   <chr>      <dbl> <dbl>\n#> 1 Alabama     2000 2021 \n#> 2 Alaska      2000 2437 \n#> 3 Arizona     2000 2652.\n#> 4 Arkansas    2000 2140 \n#> 5 California  2000 2186 \n#> 6 Colorado    2000 2356.\n\nwrite_csv(tfr_data, \"outputs/monicas_tfr.csv\")"},{"path":"gather-data.html","id":"semi-structured","chapter":"8 Gather data","heading":"8.10 Semi-structured","text":"","code":""},{"path":"gather-data.html","id":"json-and-xml","chapter":"8 Gather data","heading":"8.10.1 JSON and XML","text":"","code":""},{"path":"gather-data.html","id":"optical-character-recognition","chapter":"8 Gather data","heading":"8.11 Optical Character Recognition","text":"predicated PDF already ‘digitized.’ images? case need first use Optical Character Recognition (OCR). go-package Tesseract (Ooms 2019c). R wrapper around Tesseract open-source OCR engine.Let’s see example scan first page Jane Eyre (Figure 8.13).\nFigure 8.13: Scan first page Jane Eyre.\n","code":"\n# install.packages('tesseract')\nlibrary(tesseract)\ntext <- tesseract::ocr(here::here(\"figures/jane_scan.png\"), engine = tesseract(\"eng\"))\ncat(text)\n#> 1 THERE was no possibility of taking a walk that day. We had\n#> been wandering, indeed, in the leafless shrubbery an hour in\n#> the morning; but since dinner (Mrs Reed, when there was no com-\n#> pany, dined early) the cold winter wind had brought with it clouds\n#> so sombre, and a rain so penetrating, that further out-door exercise\n#> \n#> was now out of the question.\n#> \n#> I was glad of it: I never liked long walks, especially on chilly\n#> afternoons: dreadful to me was the coming home in the raw twi-\n#> light, with nipped fingers and toes, and a heart saddened by the\n#> chidings of Bessie, the nurse, and humbled by the consciousness of\n#> my physical inferiority to Eliza, John, and Georgiana Reed.\n#> \n#> The said Eliza, John, and Georgiana were now clustered round\n#> their mama in the drawing-room: she lay reclined on a sofa by the\n#> fireside, and with her darlings about her (for the time neither quar-\n#> relling nor crying) looked perfectly happy. Me, she had dispensed\n#> from joining the group; saying, ‘She regretted to be under the\n#> necessity of keeping me at a distance; but that until she heard from\n#> Bessie, and could discover by her own observation that I was\n#> endeavouring in good earnest to acquire a more sociable and\n#> child-like disposition, a more attractive and sprightly manner—\n#> something lighter, franker, more natural as it were—she really\n#> must exclude me from privileges intended only for contented,\n#> happy, littie children.’\n#> \n#> ‘What does Bessie say I have done?’ I asked.\n#> \n#> ‘Jane, I don’t like cavillers or questioners: besides, there is\n#> something truly forbidding in a child taking up her elders in that\n#> manner. Be seated somewhere; and until you can speak pleasantly,\n#> remain silent.’\n#> \n#> . Bs aT sae] eae\n#> \n#> i; AN TCM TAN | Beal | Sees\n#> a) } ; | i)\n#> i i 4 | | A ae | i | eee eek?\n#> \n#> a an eames yi | bee\n#> 1 nea elem | | oe pee\n#> i i ae BC i i Hale\n#> oul | ec hi\n#> pan || i re a al! |\n#> \n#> ase } Oty 2 RIES ORT Sata ariel\n#> SEEN BE — =——_\n#> 15"},{"path":"gather-data.html","id":"text","chapter":"8 Gather data","heading":"8.12 Text","text":"Aspects section previously published.","code":""},{"path":"gather-data.html","id":"introduction-9","chapter":"8 Gather data","heading":"8.12.1 Introduction","text":"Text data around us, many cases earliest types data exposed . Recent increases computational power, development new methods, enormous availability text, means great deal interest using text data. Initial methods tend focus, essentially, converting text numbers analysing using traditional methods. recent methods begun take advantage structure inherent text, draw additional meaning. difference perhaps akin child can group similar colors, compared child knows objects ; although crocodiles trees green, can something knowledge, can knowing crocodile eat , tree probably won’t.section cover variety techniques designed equip basics using text data. One great things text data typically generated purposes analysis. ’s great removes one unobservable variables typically worry . trade-typically bunch work get form can work .","code":""},{"path":"gather-data.html","id":"getting-text-data","chapter":"8 Gather data","heading":"8.12.2 Getting text data","text":"Text data exciting tool apply. many guides assume already nice dataset. ’ve focused workflow notes, know ’s likely true! section scrape text website. ’ve already seen examples scraping, general focused exploiting tables website. ’re going instead focus paragraphs text, hence ’ll focus different html/css tags.’re going us rvest package make easier scrape data. ’re also going use purrr package apply function bunch different URLs. little bit programming, alternative using loop. bit CS, package adds functional programming R.Create function visit address_to_visit save save_name files.now apply function list URLs.result bunch files saved text data.case used scraping, , course, many ways. may able use APIs, instance, case Airbnb dataset examined earlier notes. lucky may simply column contains text data dataset.","code":"\nlibrary(rvest)\nlibrary(tidyverse)\n\n# Some websites\naddress_to_visit <- c(\"https://www.rba.gov.au/monetary-policy/rba-board-minutes/2020/2020-03-03.html\",\n                    \"https://www.rba.gov.au/monetary-policy/rba-board-minutes/2020/2020-02-04.html\",\n                    \"https://www.rba.gov.au/monetary-policy/rba-board-minutes/2019/2019-12-03.html\",\n                    \"https://www.rba.gov.au/monetary-policy/rba-board-minutes/2019/2019-11-05.html\",\n                    \"https://www.rba.gov.au/monetary-policy/rba-board-minutes/2019/2019-10-01.html\",\n                    \"https://www.rba.gov.au/monetary-policy/rba-board-minutes/2019/2019-09-03.html\"\n                    )\n\n# Save names\nsave_name <- address_to_visit %>% \n  str_remove(\"https://www.rba.gov.au/monetary-policy/rba-board-minutes/\") %>% \n  str_remove(\".html\") %>%\n  str_remove(\"20[:digit:]{2}/\") %>% \n  str_c(\"inputs/rba/\", ., \".csv\")\nvisit_address_and_save_content <-\n  function(name_of_address_to_visit,\n           name_of_file_to_save_as) {\n    # The function takes two inputs\n    name_of_address_to_visit <- address_to_visit[1]\n    name_of_file_to_save_as <- save_name[1]\n    \n    read_html(name_of_address_to_visit) %>% # Go to the website and read the html\n      html_node(\"#content\") %>% # Find the content part\n      html_text() %>% # Extract the text of the content part\n      write_lines(name_of_file_to_save_as) # Save as a text file\n    print(paste(\"Done with\", name_of_address_to_visit, \"at\", Sys.time()))  \n    # Helpful so that you know progress when running it on all the records\n    Sys.sleep(sample(30:60, 1)) # Space out each request by somewhere between \n    # 30 and 60 seconds each so that we don't overwhelm their server\n  }\n\n# If there is an error then ignore it and move to the next one\nvisit_address_and_save_content <-\n  safely(visit_address_and_save_content)\n# Walk through the addresses and apply the function to each\nwalk2(address_to_visit,\n      save_name,\n      ~ visit_address_and_save_content(.x, .y))"},{"path":"gather-data.html","id":"preparing-text-datasets","chapter":"8 Gather data","heading":"8.12.3 Preparing text datasets","text":"section draws Sharla Gelfand’s blog post, linked required readings.much like stick Australian economics politics examples, realise probably limited interest . , section consider dataset Sephora reviews. Please read Sharla’s blog post (https://sharla.party/post/crying-sephora/) another take dataset.section assume text data gathered. point need change form can work . applications counts words. others may variant . dataset going use Sephora, scraped Connie originally became aware Sharla.First let’s read data.’ll focus review_body variable number stars stars reviewer gave. 5 stars, ’ll just focus whether review five stars.example going split everything separate words. just searching space, types elements going considered ‘words?’now want count number times word used star classifications.can see popular word five-star reviews ‘,’ popular word one star reviews ‘.’point, can use data whole bunch different things, one nice measure look term frequency e.g. case many times word used reviews particular star rating. issue lot words commonly used regardless context. , may also like look inverse document frequency ‘penalise’ words occur many particular star ratings. instance, ‘’ probably occurs one star five star reviews idf lower ‘hate’ probably occurs one star reviews. term frequency–inverse document frequency (tf-idf) product .can create value using bind_tf_idf() function tidytext package, create bunch new columns, one word star combination.","code":"\n# This code is taken from https://sharla.party/post/crying-sephora/\nlibrary(dplyr)\nlibrary(jsonlite)\nlibrary(tidytext)\n\ncrying <- \n  jsonlite::fromJSON(\"https://raw.githubusercontent.com/everestpipkin/datagardens/master/students/khanniie/5_newDataSet/crying_dataset.json\",\n  simplifyDataFrame = TRUE\n)\n\ncrying <- as_tibble(crying[[\"reviews\"]])\n\nhead(crying)\n#> # A tibble: 6 × 6\n#>   date        product_info$bra… $name $type $url  review_body review_title stars\n#>   <chr>       <chr>             <chr> <chr> <chr> <chr>       <chr>        <chr>\n#> 1 29 Mar 2016 Too Faced         Bett… Masc… http… \"Now I can… AWESOME      5 st…\n#> 2 29 Sep 2016 Too Faced         Bett… Masc… http… \"This hold… if you're s… 5 st…\n#> 3 23 May 2017 Too Faced         Bett… Masc… http… \"I just bo… Hate it      1 st…\n#> 4 15 Aug 2017 Too Faced         Bett… Masc… http… \"To start … Nearly perf… 5 st…\n#> 5 21 Sep 2016 Too Faced         Bett… Masc… http… \"This masc… Amazing!!    5 st…\n#> 6 30 May 2016 Too Faced         Bett… Masc… http… \"Let's tal… Tricky but … 5 st…\n#> # … with 1 more variable: userid <dbl>\nnames(crying)\n#> [1] \"date\"         \"product_info\" \"review_body\"  \"review_title\" \"stars\"       \n#> [6] \"userid\"\ncrying <- \n  crying %>% \n  select(review_body, stars) %>% \n  mutate(stars = str_remove(stars, \" stars?\"),  # The question mark at the end means it'l get rid of 'star' and 'stars'.\n         stars = as.integer(stars)\n         ) %>% \n  mutate(five_stars = if_else(stars == 5, 1, 0))\n\ntable(crying$stars)\n#> \n#>  1  2  3  4  5 \n#>  6  2  4 14 79\ncrying_by_words <- \n  crying %>%\n  tidytext::unnest_tokens(word, review_body, token = \"words\")\n\nhead(crying_by_words)\n#> # A tibble: 6 × 3\n#>   stars five_stars word \n#>   <int>      <dbl> <chr>\n#> 1     5          1 now  \n#> 2     5          1 i    \n#> 3     5          1 can  \n#> 4     5          1 cry  \n#> 5     5          1 all  \n#> 6     5          1 i\ncrying_by_words <- \n  crying_by_words %>% \n  count(stars, word, sort = TRUE)\n\nhead(crying_by_words)\n#> # A tibble: 6 × 3\n#>   stars word      n\n#>   <int> <chr> <int>\n#> 1     5 i       348\n#> 2     5 and     249\n#> 3     5 the     239\n#> 4     5 it      211\n#> 5     5 a       193\n#> 6     5 this    178\n\ncrying_by_words %>% \n  filter(stars == 1) %>% \n  head()\n#> # A tibble: 6 × 3\n#>   stars word      n\n#>   <int> <chr> <int>\n#> 1     1 the      39\n#> 2     1 i        24\n#> 3     1 and      21\n#> 4     1 it       21\n#> 5     1 to       19\n#> 6     1 my       16\n# This code, and the one in the next block, is from Julia Silge: https://juliasilge.com/blog/sherlock-holmes-stm/\ncrying_by_words_tf_idf <- \n  crying_by_words %>%\n  bind_tf_idf(word, stars, n) %>%\n  arrange(-tf_idf)\n\nhead(crying_by_words_tf_idf)\n#> # A tibble: 6 × 6\n#>   stars word              n      tf   idf tf_idf\n#>   <int> <chr>         <int>   <dbl> <dbl>  <dbl>\n#> 1     2 below             1 0.00826  1.61 0.0133\n#> 2     2 boy               1 0.00826  1.61 0.0133\n#> 3     2 choice            1 0.00826  1.61 0.0133\n#> 4     2 contrary          1 0.00826  1.61 0.0133\n#> 5     2 exceptionally     1 0.00826  1.61 0.0133\n#> 6     2 migrates          1 0.00826  1.61 0.0133\ncrying_by_words_tf_idf %>% \n  group_by(stars) %>%\n  top_n(10) %>%\n  ungroup %>% \n  mutate(word = reorder_within(word, tf_idf, stars)) %>%\n  mutate(stars = as_factor(stars)) %>%\n  filter(stars %in% c(1, 5)) %>% \n  ggplot(aes(word, tf_idf, fill = stars)) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(vars(stars), scales = \"free\") +\n    scale_x_reordered() +\n    coord_flip() +\n    labs(x = \"Word\", \n         y = \"tf-idf\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set1\")"},{"path":"gather-data.html","id":"exercises-and-tutorial-7","chapter":"8 Gather data","heading":"8.13 Exercises and tutorial","text":"","code":""},{"path":"gather-data.html","id":"exercises-7","chapter":"8 Gather data","heading":"8.13.1 Exercises","text":"words, API (write paragraph two)?Find two APIs discuss use tell interesting stories (write paragraph two ).Find two APIs R packages written around . use tell interesting stories? (Write paragraph two .)main argument httr::GET() (pick one)?\n‘url’\n‘website’\n‘domain’\n‘location’\n‘url’‘website’‘domain’‘location’Name three reasons respectful getting scraping data websites (write paragraph two).features website typically take advantage parse code (select )?\nHTML/CSS mark-.\nCookies.\nFacebook beacons.\nCode comments.\nHTML/CSS mark-.Cookies.Facebook beacons.Code comments.three advantages three disadvantages scraping compared using API (write paragraph two)?three delimiters useful trying bring order PDF read character vector (write paragraph two)?need put inside “SOMETHING_HERE” want match regular expressions full stop .e. “.” (hint: see ‘strings’ cheat sheet) (pick one)?\n.\n\\.\n\\\\.\n\\\\\\.\n.\\.\\\\.\\\\\\.Name three reasons sketching want starting try extract data PDF (write paragraph two ).interested demographic data three checks might like ? three interested economic data GDP, interest rates, exchange rates? (Write explanation .)purrr package (select )?\nEnhances R’s functional programming toolkit.\nMakes loops easier code read.\nChecks consistency datasets.\nIdentifies issues data structures proposes replacements.\nEnhances R’s functional programming toolkit.Makes loops easier code read.Checks consistency datasets.Identifies issues data structures proposes replacements.functions purrr package (select )?\nmap()\nwalk()\nrun()\nsafely()\nmap()walk()run()safely()use safely() scraping data (pick one)?\nprotect us hackers.\navoid side effects pages issues.\nslow scraping appropriate speed.\nprotect us hackers.avoid side effects pages issues.slow scraping appropriate speed.principles follow scraping (select )?\nAvoid possible\nFollow site’s guidance\nSlow \nUse scalpel axe.\nAvoid possibleFollow site’s guidanceSlow downUse scalpel axe.robots.txt file (pick one)?\ninstructions Frankenstein followed.\nNotes web scrapers follow scraping.\ninstructions Frankenstein followed.Notes web scrapers follow scraping.html tag item list (pick one)?\nli\nbody\nb\nem\nlibodybemIf following text data ‘rohan_alexander’ column called ‘names’ want split first name surname based underbar function use (pick one)?\nseparate()\nslice()\nspacing()\ntext_to_columns()\nseparate()slice()spacing()text_to_columns()","code":""},{"path":"gather-data.html","id":"tutorial-7","chapter":"8 Gather data","heading":"8.13.2 Tutorial","text":"Gather data using method introduced - APIs directly via wrapper package, web scraping, PDF parsing, OCR, text. Write paragraphs data source, gathered, went . took longer expected? become fun? differently next time ? Please include link GitHub repo can see code, won’t strictly marked - encouraging go. (Start something tiny specific, get working, increase scope - almost everything difficult time-consuming think - don’t forget plan start.)","code":""},{"path":"hunt-data.html","id":"hunt-data","chapter":"9 Hunt data","heading":"9 Hunt data","text":"STATUS: construction.Required readingBanerjee, Abhijit Vinayak, 2020, ‘Field Experiments Practice Economics,’ American Economic Review, Vol. 110, . 7, pp. 1937-1951.Berry, Donald, 1989, ‘Comment: Ethics ECMO,’ Statistical Science, Vol 4, 4, pp. 306-310.Duflo, Esther, 2020, ‘Field Experiments Practice Policy,’ American Economic Review, Vol. 110, . 7, pp. 1952-1973 (watch speech detailed ).Fisher, Ronald, 1935, Design Experiments, pp. 20-29, https://archive.org/details/.ernet.dli.2015.502684/page/n33/mode/2up.Fry, Hanna, 2020, ‘Experiments Trial,’ New Yorker, 2 March, pp. 61-65, https://www.newyorker.com/magazine/2020/03/02/big-tech--testing-.Gertler, Paul, Sebastian Martinez, Patrick Premand, Laura Rawlings, Christel Vermeersch, Impact Evaluation Practice, Chapters 3 4, https://www.worldbank.org/en/programs/sief-trust-fund/publication/impact-evaluation--practice.Hill, Austin Bradford, 1965, ‘Environment Disease: Association Causation?’ Proceedings Royal Society Medicine, 58, 5, 295-300.Kohavi, Ron Stefan Thomke, 2017, ‘Surprising Power Online Experiments,’ Harvard Business Review, September-October, https://hbr.org/2017/09/-surprising-power--online-experiments.Kohavi, Ron, Diane Tang, Ya Xu, 2020, Trustworthy Online Controlled Experiments: Practical Guide /B Testing, Cambridge University Press. (sounds like lot, ’s light book - ’s providing examples issues think .) (Freely available U T library.)Taback, Nathan, 2020, Design Experiments Observational Studies, Chapter 8 - Completely Randomized Designs: Comparing Two Treatments, https://scidesign.github.io/designbook/completely-randomized-designs-comparing---two-treatments.html.Taylor, Sean, Dean Eckles, 2017, ‘Randomized experiments detect estimate social influence networks,’ arXiv, https://arxiv.org/abs/1709.09636v1.Ware, James H., 1989, ‘Investigating Therapies Potentially Great Benefit: ECMO,’ Statistical Science, Vol 4, 4, pp. 298-306.Wu, Changbao Mary E. Thompson, 2020, Sampling Theory Practice, Springer, Chapters 1-3, 5 (freely available U T library).Required viewingGe, Kathy, 2021, ‘Experimentation product design Uber,’ Toronto Data Workshop, 4 February, https://youtu./UYzXElJTovg.Register, Yim, 2020, ‘Introduction Sampling Randomization,’ Online Causal Inference Seminar, 14 November, https://youtu./U272FFxG8LE.Xu, Ya, 2020, ‘Causal inference challenges industry, perspective experiences LinkedIn,’ Online Causal Inference Seminar, 16 July, https://youtu./OoKsLAvyIYA.Recommended readingAngrist, Joshua D., Jörn-Steffen Pischke, 2008, Mostly harmless econometrics: empiricist’s companion, Princeton University Press, Chapter 2.Banerjee, Abhijit Vinayak, Esther Duflo, Rachel Glennerster, Dhruva Kothari, 2010, ‘Improving immunisation coverage rural India: clustered randomised controlled evaluation immunisation campaigns without incentives,’ BMJ, 340, c2220.Beaumont, Jean-François, 2020, ‘probability surveys bound disappear production official statistics?’ Survey Methodology, 46 (1), Statistics Canada, Catalogue . 12-001-X.Christian, Brian, 2012, ‘/B Test: Inside Technology ’s Changing Rules Business,’ Wired, 25 April, https://www.wired.com/2012/04/ff-abtesting/.Dablander, Fabian, 2020, “Introduction Causal Inference,” PsyArXiv, 13 February, doi:10.31234/osf.io/b3fkw, https://psyarxiv.com/b3fkw.Deaton, Angus, 2010, ‘Instruments, Randomization, Learning Development,’ Journal Economic Literature, vol. 48, . 2, pp. 424-455.Duflo, Esther, Rachel Glennerster, Michael Kremer, 2007, ‘Using Randomization Development Economics Research: Toolkit,’ https://economics.mit.edu/files/806.Gordon, Brett R., Florian Zettelmeyer, Neha Bhargava, Dan Chapsky, 2019, ‘Comparison Approaches Advertising Measurement: Evidence Big Field Experiments Facebook,’ Marketing Science, Vol. 38, . 2, March–April, pp. 193–225.Groves, Robert M., 2011, ‘Three Eras Survey Research,’ Public Opinion Quarterly, 75 (5), pp. 861–871, https://doi.org/10.1093/poq/nfr057.Hillygus, D. Sunshine, 2011, ‘evolution election polling United States,’ Public Opinion Quarterly, 75 (5), pp. 962-981.Imai, Kosuke, 2017, Quantitative Social Science: Introduction, Princeton University Press, Ch 2.3, 2.4, 4.3.Jeffries, Adrianne, Leon Yin, Surya Mattu, 2020, ‘Swinging Vote?’ Markup, 26 February, https://themarkup.org/google--giant/2020/02/26/wheres--email.Kohavi, Ron, Alex Deng, Brian Frasca, Roger Longbotham, Toby Walker, Ya Xu. 2012. Trustworthy online controlled experiments: five puzzling outcomes explained. Proceedings 18th ACM SIGKDD international conference Knowledge discovery data mining (KDD ’12). Association Computing Machinery, New York, NY, USA, 786–794. DOI:https://doi.org/10.1145/2339530.2339653Landesberg, Eddie, Molly Davies, Stephanie Yee, 2019, ‘Want make good business decisions? Learn causality,’ MultiThreaded, Stitchfix blog, 19 December, https://multithreaded.stitchfix.com/blog/2019/12/19/good-marketing-decisions/.Levay, Kevin E., Jeremy Freese, James N. Druckman, 2016, ‘demographic political composition Mechanical Turk samples,’ Sage Open, 6 (1), 2158244016636433.Lewis, Randall ., David H. Reiley, 2014 ‘Online ads offline sales: Measuring effects retail advertising via controlled experiment Yahoo!’ Quantitative Marketing Economics, Vol 12, pp. 235–266.Mullinix, Kevin J., Leeper, Thomas J., Druckman, James N. Freese, Jeremy, 2015, ‘generalizability survey experiments,’ Journal Experimental Political Science, 2 (2), pp. 109-138.Novak, Greg, Sven Schmit, Dave Spiegel, 2020, Experimentation resource constraints, 18 November, StitchFix Blog, https://multithreaded.stitchfix.com/blog/2020/11/18/virtual-warehouse/.Prepared AAPOR Executive Council Task Force operating auspices AAPOR Standards Committee, members including:, Reg Baker, Stephen J. Blumberg, J. Michael Brick, Mick P. Couper, Melanie Courtright, J. Michael Dennis, Don Dillman, Martin R. Frankel, Philip Garland, Robert M. Groves, Courtney Kennedy, Jon Krosnick, Paul J. Lavrakas, Sunghee Lee, Michael Link, Linda Piekarski, Kumar Rao, Randall K. Thomas, Dan Zahs, 2010, ‘Research Synthesis: AAPOR Report Online Panels,’ Public Opinion Quarterly, 74 (4), pp. 711–781, https://doi.org/10.1093/poq/nfq048.Ryan, . C., . R. MacKenzie, S. Watkins, R. Timmis, 2012, ‘World War II contrails: case study aviation‐induced cloudiness,’ International journal climatology, 32, . 11, pp. 1745-1753.Said, Chris, 2020, ‘Optimizing sample sizes /B testing, Part : General summary,’ 10 January, https://chris-said.io/2020/01/10/optimizing-sample-sizes--ab-testing-part-/. (See also parts 2 3).Stolberg, Michael, 2006, ‘Inventing randomized double-blind trial: Nuremberg salt test 1835,’ Journal Royal Society Medicine, 99, . 12, pp. 642-643.Sveriges Riksbank Prize Economic Sciences Memory Alfred Nobel, 2019, popular science background, https://www.nobelprize.org/uploads/2019/10/popular-economicsciencesprize2019-2.pdf.Sveriges Riksbank Prize Economic Sciences Memory Alfred Nobel, 2019, scientific background, https://www.nobelprize.org/uploads/2019/10/advanced-economicsciencesprize2019.pdf.Taddy, Matt, 2019, Business Data Science, Chapter 5.Urban, Steve, Rangarajan Sreenivasan, Vineet Kannan, 2016, ‘’s /Bout Testing: Netflix Experimentation Platform,’ Netflix Technology Blog, 29 April, https://netflixtechblog.com/---bout-testing--netflix-experimentation-platform-4e1ca458c15.VWO, ‘/B Testing Guide,’ https://vwo.com/ab-testing/.Yeager, David S., Jon . Krosnick, LinChiat Chang, Harold S. Javitz, Matthew S. Levendusky, Alberto Simpser, Rui Wang, 2011, ‘Comparing Accuracy RDD Telephone Surveys Internet Surveys Conducted Probability Non-Probability Samples,’ Public Opinion Quarterly, 75 (4), pp. 709–747, https://doi.org/10.1093/poq/nfr020.Yin, Xuan Ercan Yildiz, 2020, ‘Causal Analysis Cannibalization Online Products,’ Code Craft, Etsy blog, 24 February, https://codeascraft.com/2020/02/24/-causal-analysis--cannibalization--online-products/.Recommended listeningGalef, Julia, 2020, ‘Episode 246: Deaths despair / Effective altruism (Angus Deaton),’ Rationally Speaking, 35:30 end, available : http://rationallyspeakingpodcast.org/show/episode-246-deaths--despair-effective-altruism-angus-deato.html.Recommended viewingDuflo, Esther, 2020, ‘Inteview Esther Duflo,’ 12 October, Online Causal Inference Seminar, https://youtu./WWW9q3oMYxU.Duflo, Esther, 2019, ‘Nobel Prize Lecture,’ 8 December 2019, Stockholm: https://www.nobelprize.org/prizes/economic-sciences/2019/duflo/lecture/.Tipton, Elizabeth, 2020, ‘Intervention Work Population? Designing Randomized Trials Generalization,’ Online Causal Inference Seminar, 14 April, https://youtu./HYP32wzEZMA.Key concepts/skills/etcTreatment control groups.Internal external validity.Average treatment effect.Generating simulated datasets.Defining populations, frames samples.Distinguishing probability non-probability samplingDistinguishing strata clusters.Key librariesbroomggplot2tidyverseKey functions/etcaov()rnorm()sample()t.test()","code":""},{"path":"hunt-data.html","id":"experiments-and-randomised-controlled-trials","chapter":"9 Hunt data","heading":"9.1 Experiments and randomised controlled trials","text":"","code":""},{"path":"hunt-data.html","id":"introduction-10","chapter":"9 Hunt data","heading":"9.1.1 Introduction","text":"First note Ronald Fisher Francis Galton. Fisher Galton intellectual grandfathers much work cover. cases directly work, cases work built contributions. men believed eugenics, amongst things generally reprehensible.chapter experiments. situation can explicitly control vary aspects. advantage identification clear. treatment group treated control group . randomly split. end different must treatment. Unfortunately, life rarely smooth. Arguing similar treatment control groups tends carry indefinitely, ability speak internal validity affects ability speak external validity.’s also important note statistics designed agricultural settings ‘fertilizer work?’ etc. settings can easily divide field ‘treated’ ‘non-treated,’ magnitude effect large. general, statistical approaches still used today (especially social sciences) often inappropriately. hear someone talking ‘enough power’ similar phrases, ’s necessarily ’re right, usually pays take step back really think done whether really know ’re .","code":""},{"path":"hunt-data.html","id":"motivation-and-notation","chapter":"9 Hunt data","heading":"9.1.2 Motivation and notation","text":"Never forget: sampling way non-representative, observe[d] data sufficient population estimates. must deal design, sampling issues, data quality, misclassification. Otherwise ’ll just wrong.Dan Simpson, 30 January 2020.Monica moved San Francisco, Giants immediately won baseball, Warriors began historic streak. moved Chicago Cubs won baseball first time hundred years. moved Massachusetts, Patriots won Super Bowl . Finally, moved Toronto, Raptors won basketball. city pay us live funds better spent elsewhere?One way get answer run experiment. Make list North American cities major sports teams, roll dice send us live year. enough lifetimes, work . fundamental issue live city live city. Experiments randomised controlled trials circumstances try randomly allocate treatment, belief everything else constant (least ignorable).words Hernan Robins (2020, 3) action, \\(\\), also known ‘intervention, exposure, treatment.’ ’ll typically use ‘treated/control’ language, reflecting whether action imposed . treatment random variable typically binary, 0 1, ‘treated’ ‘treated/control/comparison.’ ’ll typically outcome random variable, \\(Y\\), typically binary, able made binary, continuous, although ’ll touch options. example binary outcome vote choice - ‘Conservative’ vs ‘Conservative’ - noticing grouped parties simply ‘Conservative’ force binary outcome.following Hernan Robins (2020, 4), notation Gertler et al. (2016, 48) describe treatment ‘causal’ \\((Y|=0)\\neq (Y|=1)\\). discussed , fundamental problem causal inference treat control one individual. want know effect treatment, need compare counterfactual, happened individual treated. causal inference turns fundamentally missing data problem.4To quote Gertler et al. (2016, 48), context evaluating income response intervention program:put another way, like measure income point time unit observation (person, case), two different states world. possible , observing much income individual point time without program, possible explanation difference person’s income program. comparing individual moment, managed eliminate outside factors might also explained difference outcomes. confident relationship vocational training program change income causal… [] unit either participated program participate. unit observed simultaneously two different states (words, without program).compared treatment control one particular individual, instead compare average two groups - treated . looking estimate counterfactual. usually consider default ’s effect require evidence us change mind. ’re interested happening groups, turn expectations, notions probability express . Hence, ’ll make claims talk, average. Maybe wearing fun socks really make lucky day, average across population, ’s probably case.5It’s worth pointing don’t just interested average effect. may consider median, variance, whatever. Nonetheless, interested average effect, one way proceed divide dataset two - treated treated - effect column 0s 1s, sum column divide length column, look ratio. estimator, way putting together guess something interest. estimand thing interest, case average effect, estimate whatever guess turns . give another example, following Gelman, Hill, Vehtari (2020):estimand, quantity interest, summary parameters data somebody interested estimating. example, regression model, \\(y = + bx + \\epsilon\\), parameters \\(\\) \\(b\\) might interest…. use data construct estimates parameters quantities interest.broadly, Cunningham (2021) defines causal inference ‘…leveraging theory deep knowledge institutional details estimate impact events choices given outcome interest.’ previous chapter discussed gathering data observed world. chapter going active. Cunningham (2021) says experimental data ‘collected something akin laboratory environment. traditional experiment, researcher participates actively process recorded.’ , want use data researchers go hunt , like.","code":""},{"path":"hunt-data.html","id":"randomised-sampling","chapter":"9 Hunt data","heading":"9.1.3 Randomised sampling","text":"Correlation can enough settings, order able make forecasts things change circumstances slightly different need understand causation. key counterfactual - happened absence treatment. Ideally keep everything else constant, randomly divide world two groups, treat one . can pretty confident difference two groups due treatment. reason population randomly select two groups , two groups (long big enough) characteristics population. Randomised controlled trials (RCTs) /B testing attempts get us close ‘gold standard’ can hope. RCTs often described ‘gold standard,’ instance Athey Imbens (2017). , ’re saying RCTs perfect, just ’re generally better options. plenty wrong RCTs.Remember challenge (Gertler et al. 2016, 51–52):…identify treatment group comparison group statistically identical, average, absence program. two groups identical, sole exception one group participates program , can sure difference outcomes must due program. Finding comparison groups crux impact evaluation, regardless type program evaluated. Simply put, without comparison group yields accurate estimate counterfactual, true impact program established.might worried underlying trends (issues /comparison), selection bias (issue self-selection), either result biased estimators. solution randomisation.get started, let’s generate simulated dataset sample . general, good way approach problems:generate simulated dataset;analysis simulated dataset; andtake analysis real dataset.reason good approach know roughly outcomes step 2, whereas go directly real dataset don’t know unexpected outcomes likely due analysis errors, actual results. first time generate simulated dataset take , bit practice ’ll get good . also packages can help, including DeclareDesign (Blair et al. 2019) survey (Lumley 2020). Another good reason ’s useful take approach simulation ’re working teams analysis can get started data collection cleaning completed. simulation also help collection cleaning team think tests run data.’ll get terminology later, sampling frame subset population can actually sampled, instance listed somewhere. instance, Lauren Kennedy likes use analogy city’s population, phonebook - almost everyone (least used ), population sampling frame almost , .Now look mean two groups drawn sampling frame.probably convinced looking , formally test difference two samples, can use t-test.properly done get ‘representative’ share people favourite color blue, also get representative share people support Maple Leafs. happen haven’t randomised variables? Let’s start looking dataset.exciting. representative share ‘unobservables’ (case ‘observe’ - illustrate point - didn’t select ). get correlated. breakdown number ways discuss. also assumes large enough groups - sampled Toronto likely get ‘representative’ share people support Canadiens? F.C. Hansa Rostock? want check two groups can ? Exactly - just check can identify difference two groups based observables (looked mean, look aspects well).","code":"\nlibrary(tidyverse)\n\nset.seed(853)\n# Construct a population so that 25 per cent of people like blue and 75 per cent \n# like white.\npopulation <- \n  tibble(person = c(1:10000),\n         favourite_color = sample(x = c(\"Blue\", \"White\"), \n                                  size  = 10000, \n                                  replace = TRUE,\n                                  prob = c(0.25, 0.75)),\n         supports_the_leafs = sample(x = c(\"Yes\", \"No\"), \n                                  size  = 10000, \n                                  replace = TRUE,\n                                  prob = c(0.80, 0.20)),\n         ) %>% \n  mutate(in_frame = sample(x = c(0:1),\n                        size  = 10000, \n                        replace = TRUE)) %>% \n  mutate(group = sample(x = c(1:10),\n                        size  = 10000, \n                        replace = TRUE)) %>% \n  mutate(group = ifelse(in_frame == 1, group, NA))\npopulation %>% \n  filter(in_frame == 1) %>% \n  filter(group %in% c(1, 2)) %>% \n  group_by(group, favourite_color) %>% \n  count()\n#> # A tibble: 4 × 3\n#> # Groups:   group, favourite_color [4]\n#>   group favourite_color     n\n#>   <int> <chr>           <int>\n#> 1     1 Blue              114\n#> 2     1 White             420\n#> 3     2 Blue              105\n#> 4     2 White             369\nlibrary(broom)\n\npopulation <- \n  population %>% \n  mutate(color_as_integer = case_when(\n    favourite_color == \"White\" ~ 0,\n    favourite_color == \"Blue\" ~ 1,\n    TRUE ~ 999\n  ))\n\ngroup_1 <- \n  population %>% \n  filter(group == 1) %>% \n  select(color_as_integer) %>% \n  as.vector() %>% \n  unlist()\n\ngroup_2 <- \n  population %>% \n  filter(group == 2) %>% \n  select(color_as_integer) %>% \n  unlist()\n\nt.test(group_1, group_2)\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  group_1 and group_2\n#> t = -0.30825, df = 988.57, p-value = 0.758\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.05919338  0.04312170\n#> sample estimates:\n#> mean of x mean of y \n#> 0.2134831 0.2215190\n\n# We could also use the tidy function in the broom package.\ntidy(t.test(group_1, group_2))\n#> # A tibble: 1 × 10\n#>   estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n#>      <dbl>     <dbl>     <dbl>     <dbl>   <dbl>     <dbl>    <dbl>     <dbl>\n#> 1 -0.00804     0.213     0.222    -0.308   0.758      989.  -0.0592    0.0431\n#> # … with 2 more variables: method <chr>, alternative <chr>\npopulation %>% \n  filter(in_frame == 1) %>% \n  filter(group %in% c(1, 2)) %>% \n  group_by(group, supports_the_leafs) %>% \n  count()\n#> # A tibble: 4 × 3\n#> # Groups:   group, supports_the_leafs [4]\n#>   group supports_the_leafs     n\n#>   <int> <chr>              <int>\n#> 1     1 No                   102\n#> 2     1 Yes                  432\n#> 3     2 No                    81\n#> 4     2 Yes                  393"},{"path":"hunt-data.html","id":"anova","chapter":"9 Hunt data","heading":"9.1.4 ANOVA","text":"‘refuse teach anova.’Statistics professor prefers remain anonymous.Analysis Variation (ANOVA) introduced Fisher working statistical problems agriculture. steal Darren L Dahly’s ‘favorite joke time’ (Dahly 2020):Q: “’s difference agricultural medical research?”: “former isn’t conducted farmers.”need cover ANOVA importance historically, general probably shouldn’t actually use ANOVA day--day. ’s nothing wrong , right circumstances, ’s just hundred years old number modern use-case ’s still best-bet pretty small. case, typically, null groups distribution.can run ANOVA function built R - aov().case, fail reject null samples . said, ’s just linear regression. ’m sure got fancy name.favourite discussion ANOVA Taback (2020Chapter 8).","code":"\njust_two_groups <- population %>%\n  filter(in_frame == 1) %>%\n  filter(group %in% c(1, 2))\n\naov(group ~ favourite_color, \n    data = just_two_groups) %>% \n  tidy()\n#> # A tibble: 2 × 6\n#>   term               df    sumsq meansq statistic p.value\n#>   <chr>           <dbl>    <dbl>  <dbl>     <dbl>   <dbl>\n#> 1 favourite_color     1   0.0238 0.0238    0.0952   0.758\n#> 2 Residuals        1006 251.     0.250    NA       NA\nlm(group ~ favourite_color, \n    data = just_two_groups) %>% \n  tidy()\n#> # A tibble: 2 × 5\n#>   term                 estimate std.error statistic   p.value\n#>   <chr>                   <dbl>     <dbl>     <dbl>     <dbl>\n#> 1 (Intercept)            1.48      0.0338    43.8   1.67e-235\n#> 2 favourite_colorWhite  -0.0118    0.0382    -0.308 7.58e-  1"},{"path":"hunt-data.html","id":"treatment-and-control","chapter":"9 Hunt data","heading":"9.1.5 Treatment and control","text":"treated control groups ways remain way, internal validity, say control work counterfactual results can speak difference groups study.words Gertler et al. (2016, 71):Internal validity means estimated impact program net potential confounding factors—, words, comparison group provides accurate estimate counterfactual, estimating true impact program.group applied randomisation representative broader population, experimental set-fairly similar outside conditions, external validity. means difference find just apply experiment, also broader population., words Gertler et al. (2016, 73):External validity means evaluation sample accurately represents population eligible units. results evaluation can generalized population eligible units. use random sampling ensure evaluation sample accurately reflects population eligible units impacts identified evaluation sample can extrapolated population.means need randomisation twice. trade-happen extent matter?, interested effect ‘treated.’ may charge different prices (continuous treatment variable), compare different colours website (discrete treatment variable, staple /B testing). consider just discrete treatments (can use dummy variables) need make sure groups otherwise . can ? One way ignore treatment variable examine variables - can detect difference groups based variables? website example, similar number :PC/Mac users?Safari/Chrome/Firefox/users?Mobile/desktop users?Users certain locations?threats validity claims.done properly, treatment truly independent, can estimate ‘average treatment effect,’ binary treatment variable setting :\n\\[\\mbox{ATE} = \\mbox{E}[y|d=1] - \\mbox{E}[y|d=0].\\], difference treated group, \\(d = 1\\), control group, \\(d = 0\\), measured expected value outcome variable, \\(y\\). mean causal effect simply difference two expectations!Let’s get stuck code. First need generate data.reality happens. experiment run long otherwise people may treated many times, become inured treatment, short otherwise can’t measure longer term outcomes. ‘representative’ sample every cross-tab, treatment control different. Practical difficulties may make difficult follow certain groups.Questions ask (haven’t answered already) include:participants selected frame consideration?selected treatment? hope lottery, term applied variety situations. Additionally, early ‘success’ can lead pressure treat everyone.treatment assessed?extent random allocation ethical fair? argue shortages mean reasonable randomly allocate, may depend linear benefits . may also difficult establish boundaries. want include people Ontario may clear, ‘students’ Ontario - student, making decision?Bias issues end world. need think carefully. famous example, Abraham Wald given data planes came back Britain shot WW2. question place armour. One option place bullet holes. Wald recognised selection effect - planes made back - didn’t need armour, instead put armour bullet holes.consider example may closer home - results survey differ asked students completed course difficult dropped ? , Dan suggests, work try make dataset good possible, may possible use model control bias. variable correlated say, attrition, can add model. Either , interaction.correlation individuals? instance, ‘hidden variable’ didn’t know , province, turned people province similar? case use ‘wider’ standard errors.better way deal may change experiment. instance, discussed stratified sampling - perhaps stratify province? implement ? course, days ’d really use 100-year-old method instead use Bayes-based approaches.","code":"\nset.seed(853)\nexample_data <- tibble(person = c(1:1000),\n                       treatment = sample(x = 0:1, size  = 1000, replace = TRUE)\n                       )\n# We want to make the outcome slightly more likely if they were treated than if not.\nexample_data <- \n  example_data %>% \n  rowwise() %>% \n  mutate(outcome = if_else(treatment == 0, \n                           rnorm(n = 1, mean = 5, sd = 1),\n                           rnorm(n = 1, mean = 6, sd = 1)\n                           )\n         )\n\nexample_data$treatment <- as.factor(example_data$treatment)\n\nexample_data %>% \n  ggplot(aes(x = outcome, \n             fill = treatment)) +\n  geom_histogram(position = \"dodge\",\n                 binwidth = 0.2) +\n  theme_minimal() +\n  labs(x = \"Outcome\",\n       y = \"Number of people\",\n       fill = \"Person was treated\") +\n  scale_fill_brewer(palette = \"Set1\")\n\nexample_regression <- lm(outcome ~ treatment, data = example_data)\n\ntidy(example_regression)\n#> # A tibble: 2 × 5\n#>   term        estimate std.error statistic  p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)     5.00    0.0430     116.  0       \n#> 2 treatment1      1.01    0.0625      16.1 5.14e-52"},{"path":"hunt-data.html","id":"case-study---fishers-tea-party","chapter":"9 Hunt data","heading":"9.2 Case study - Fisher’s tea party","text":"\nFigure 9.1: Afternoon Tea Party (1890–1891), Mary Cassatt (American, 1844-1926), downloaded https://artvee.com/dl/afternoon-tea-party.\nFisher (see note ) introduced , now, famous example experiment designed see person can distinguish cup tea milk added first, last.6From Fisher (1935, 13):lady declares tasting cup tea made milk can discriminate whether milk tea infusion first added cup. consider problem designing experiment means assertion can tested.Fisher continues:experiment consists mixing eight cups tea, four one way four , presenting subject judgment random order. subject told advance test consist, namely asked taste eight cups, shall four kind, shall presented random order, order determined arbitrarily human choice, actual manipulation physical apparatus used games chance, cards, dice, roulettes, etc., , expeditiously, published collection random sampling-numbers purporting give actual results manipulation. task divide 8 cups two sets 4, agreeing, possible, treatments received.summarize, set-:Eight randomly ordered cups tea.Four tea put first.Four milk put first.person choose four .person knows ’s experiment.’ll now try experiment. brew tea, grab eight cups, pour eight cups tea friend ’re isolating with7 - four put milk first four put milk last. Make sure use amount tea milk ! Don’t forget randomise order, possibly even using following code:friend guess four put milk first four put milk last!decide person’s choices likely occurred random , need think probability happening chance. First count number successes four chosen. Fisher (1935, 14) claims : \\({8 \\choose 4} = \\frac{8!}{4!(8-4)!}=70\\) possible outcomes.chance, two ways person perfectly correct (asking grouped): correctly identify ones milk-first (one outcome 70) correctly identify ones tea-first (one outcome 70), chance \\(2/70 \\approx 0.028\\). Now, Fisher (1935, 15) says,‘[]t open experimenter less exacting respect smallness probability require willing admit observations demonstrated positive result.’need decide evidence takes convinced. ’s possible evidence dissuade view (difference milk-first tea-first) point experiment? case, null can’t distinguish, correctly separate , five-per-cent level, reject null.miss one? Similarly, chance 16 ways person ‘--one.’ Either think one milk-first tea-first - , \\({4 \\choose 1}\\), four ways happen - think one tea-first milk-first - , , \\({4 \\choose 1}\\), four ways happen. outcomes independent, probability \\(\\frac{4\\times 4}{70} \\approx 0.228\\). . , fail reject null.Finally, aside magical ‘5 per cent.’ Fisher describes merely ‘usual convenient’ (Fisher 1935, 15). Fisher (1935, 16) continues:order assert natural phenomenon experimentally demonstrable need, isolated record, reliable method procedure. relation test significance, may say phenomenon experimentally demonstrable know conduct experiment rarely fail give us statistically significant result.start notes, said Fisher held views consider reprehensible today. guess , around today, think use p-values discrediting. just go searching meaning constellations stars. Thoroughly interrogate data think precisely statistical methods applying. conclusions want hold long-run, aim use simple, understandable, statistical methods can. Ensure can explain justify statistical decisions without recourse astrology.\nSource: https://xkcd.com/882/\nFigure 9.2: ‘triumph wisdom fortune’ Otto van Veen (Flemish, 1556 - 1629), downloaded https://artvee.com/dl/-triumph--wisdom--fortune.\n","code":"\nsample(c(1:8), size = 8, replace = FALSE)\n#> [1] 3 7 6 4 1 8 2 5"},{"path":"hunt-data.html","id":"case-study---tuskegee-syphilis-study","chapter":"9 Hunt data","heading":"9.3 Case study - Tuskegee Syphilis Study","text":"Tuskegee Syphilis Study infamous medical trial Black Americans syphilis (‘control group’ without) given appropriate treatment, even told syphilis, well standard syphilis treatments established mid-1940s (Alsan Wanamaker 2018). study began 1932 poor Black Americans South identified offered compensation including ‘hot meals, guise treatment, burial payments’ (Alsan Wanamaker 2018). men treated syphilis. , almost unbelievable, men drafted, told syphilis, ordered get treatment. treatment blocked. time study stopped, ‘majority study’s victims deceased, many syphilis-related causes.’ (Alsan Wanamaker 2018).study continued 1972, stopping leaked published newspapers. response US established requirements Institutional Review Boards President Clinton made formal apology 1997. Brandt (1978) quoted Alsan Wanamaker (2018) says ‘“retrospect Tuskegee Study revealed pathology racism pathology syphilis; nature scientific inquiry nature disease process…. degree deception damages severely underestimated.”’Tuskegee Syphilis Study Professor Monica Alexander says:may illegal exact research days, doesn’t mean unethical research doesn’t still happen, see time ML health. Just can’t explicitly discriminate design experiments, doesn’t mean can’t implicitly discriminate.example , start Obermeyer et al. (2019):Health systems rely commercial prediction algorithms identify help patients complex health needs. show widely used algorithm, typical industry-wide approach affecting millions patients, exhibits significant racial bias: given risk score, Black patients considerably sicker White patients, evidenced signs uncontrolled illnesses. Remedying disparity increase percentage Black patients receiving additional help 17.7 46.5%. bias arises algorithm predicts health care costs rather illness, unequal access care means spend less money caring Black patients White patients. Thus, despite health care cost appearing effective proxy health measures predictive accuracy, large racial biases arise. suggest choice convenient, seemingly effective proxies ground truth can important source algorithmic bias many contexts.","code":""},{"path":"hunt-data.html","id":"sampling-and-survey-essentials","chapter":"9 Hunt data","heading":"9.4 Sampling and survey essentials","text":"","code":""},{"path":"hunt-data.html","id":"introduction-11","chapter":"9 Hunt data","heading":"9.4.1 Introduction","text":"Statistics cold, hard, core using data. Statisticians spent considerable time effort thinking properties various samples data enable us speak implications broader population.Let’s say data. instance, particular toddler goes sleep 6:00pm every night. might interested know whether bed-time common generally among toddlers, unusual toddler. one toddler ability use bed time speak toddlers limited. talk friends also toddlers? many friends, friends friends, ask can begin feel comfortable speaking underlying truth toddler bedtime?wonderful phrase Wu Thompson (2020, 3) ‘[s]tatistics science collect analyze data, draw statements conclusions unknown populations. term population usually refers real hypothetical set units characteristics attributes can modelled random variables respective probability distributions.’ much less wonderful phrasing, ‘statistics involves data trying say something sensible .’ mean, ’s really one want go .case surveys, population finite set \\(N\\) labels: ‘person 1,’ ‘person 2,’ ‘person 3,’ …, ‘person \\(N\\).’ important recognise difference population interest survey population sense used talk limits similar infinity concepts statistics. instance, time time, hear people work census data say don’t need worry confidence intervals whole population country. Nothing truth.Wu Thompson (2020, 4) lovely example ambiguity surrounds definition population. Let’s consider population voters. Canada means anyone 18 older. Fine. interested consumers - definition hipsters? regularly eat avocado toast, (+1), ’ve never bullet coffee (-1). population ?things formally defined may realise. instance, idea rural area precisely defined. property either rural area . come lovely example Wu Thompson (2020, 4) comes whether someone smoker. 15 year old 100 cigarettes ’s pretty clear need treat differently none. 100 year old 100 cigarettes consider none. ’s fine, age changes? , think changes time. one point, parents used worried children two hours screen time, now children (possibly even parents) regularly likely spend eight hours front screen work office job.come critical terminology:Population: ‘set units covered main objective study.’ Wu Thompson (2020, 5).Frame: ‘Lists sampling units’ Wu Thompson (2020, 9) sampling units either observational units clusters.Sample: complete return survey.little concrete , consider trying conduct survey attitudes Australians live Toronto. target population Australians live Toronto, frame might Australians live Toronto use Facebook, going use Facebook choose sample. finally, take Facebook list Australians living Canada gave one chance surveyed sampled population, just picked ones know just Dan, Monica, Liza (New Zealand ’ll claim ’s thing Australians ).example target population frame different Australians live Toronto Facebook. Similarly, everyone gave survey actually completed survey sample frame different.identified population interest frame (.e. list gets closest population) point distinguish probability non-probability sampling.probability sampling, every member frame chance sampled. Consider example Australian Election Study - get list addresses Australia, randomly choose send letters . ‘randomista’ RCT revolution discuss later, needed lack probability sampling, exists plays role . Importantly ensure clear role uncertainty (Wu Thompson 2020, 11). trade-expensive difficult. Note unit frame doesn’t probability necessarily, just needs determined probability measure.contrast, non-probability sampling focus populations ‘readily available’ convenient, satisfy certain quotas, based judgement, volunteer. difference probability non-probability sampling degree - typically force someone take survey, hence, almost also aspect volunteering.acknowledging spectrum, statistics developed based probability sampling. much modern sampling done using non-probability sampling. particular, common approach bunch Facebook ads trying recruit panel people exchange compensation. panel group sent various surveys necessary. think moment implications - type people likely respond ad? don’t know Canada’s richest person , likely panel? grandmother likely respond ad? - even use Facebook?cases possible census. Nation-states typically one every five ten years. reason nation states - expensive, time-consuming, surprisingly, sometimes accurate may hope general need . Hence, role surveys. Note, however censuses typically many concerns.consider population, typically ordering. may simple country states/provinces. consider stratified structure one can divide population mutually exclusive collectively exhaustive sub-populations, strata. Examples strata Wu Thompson (2020, 8) include provinces, federal electoral districts, health regions. strata need geographic, may possible use different majors. use stratification help efficiency sampling balance survey. instance, surveyed provinces proportion population, even survey 10,000 responses expect 10 responses Yukon.word used takes advantage ordering population clusters. , collectively exhaustive mutually exclusive. , may geographically based, need . difference stratified sampling cluster sampling, ‘stratified sampling, sample data collected every stratum, (whereas) cluster sampling, portion clusters members final sample’ Wu Thompson (2020, 8). said, difference can become less clear practice, especially ex post - stratify randomly sample within strata, one selected - terms intention difference clear.now turn first claims, perfect frame non-response, sample results match population. ’d course worried weren’t case, ’s nice stated. establish type population mean study variable, \\(\\mu_y\\), population means auxiliary variables \\(\\mu_x\\), things like age, gender, etc. Remembering real world, may many study variables, indeed, overlap. variable indicator set-work proportion order estimate , \\(P\\). finally, get rule thumb large samples whereby variance binary perfect setting becomes \\(\\sigma_y^2 = P/(1-P)\\) (Wu Thompson 2020, 11).Finally, conclude steps consider. critical. Strong reports grapple .","code":""},{"path":"hunt-data.html","id":"simple-random-sampling","chapter":"9 Hunt data","heading":"9.4.2 Simple random sampling","text":"TBD","code":""},{"path":"hunt-data.html","id":"stratified-and-cluster-sampling","chapter":"9 Hunt data","heading":"9.4.3 Stratified and cluster sampling","text":"TBD","code":""},{"path":"hunt-data.html","id":"snowball-sampling-and-confidant-methods","chapter":"9 Hunt data","heading":"9.4.4 Snowball sampling and confidant methods","text":"TBD","code":""},{"path":"hunt-data.html","id":"case-study---the-oregon-health-insurance-experiment","chapter":"9 Hunt data","heading":"9.5 Case study - The Oregon Health Insurance Experiment","text":"Oregon Health Insurance Experiment involved 74,922 adults Oregon 2008 2010. opportunity apply health insurance randomly allocated health earnings evaluated. found (Finkelstein et al. 2012):year random assignment, treatment group selected lottery 25 percentage points likely insurance control group selected. find first year, treatment group substantively statistically significantly higher health care utilization (including primary preventive care well hospitalizations), lower --pocket medical expenditures medical debt (including fewer bills sent collection), better self-reported physical mental health control group.lottery used determine 89,824 individuals signed allowed apply Medicaid. random allocation insurance allowed researchers understand effect health insurance. ’s usually possible compare without insurance type people sign get health insurance differ don’t - decision ‘confounded’ variables. use administrative data, hospital discharge data, credit reports matched 68.5 per cent lottery participants, mortality records, uncommon. Interestingly collection data actually fairly restrained included survey conducted via mail.Turning external validity, authors restrain say (Finkelstein et al. 2012):estimates impact public health insurance apply able-bodied uninsured adults 100 percent poverty express interest insurance coverage. population considerable policy interest.lottery used allocate 10,000 places state-run Medicaid. lottery judged fair ‘state (correctly) anticipated demand program among eligible individuals far exceed 10,000 available new enrollment slots’ (Finkelstein et al. 2012). People month sign enter draw. draws conducted six-month period selected opportunity sign . 35,169 individuals selected (household actually won draw given opportunity) 30 per cent completed paperwork eligible (typically earned much). insurance lasted indefinitely.model consider (Finkelstein et al. 2012):\\[\\begin{equation}\ny_{ihj} = \\beta_0 + \\beta_1\\mbox{Lottery} + X_{ih}\\beta+2 + V_{ih}\\beta_3 + \\epsilon_{ihj} \\tag{9.1}\n\\end{equation}\\]Equation (9.1) explains various \\(j\\) outcomes (health) individual \\(\\) household \\(h\\) function indicator variable whether household \\(h\\) selected lottery. Hence, ‘(t)coefficient Lottery, \\(\\beta_1\\), main coefficient interest, gives average difference (adjusted) means treatment group (lottery winners) control group (selected lottery).’complete specification Equation (9.1), \\(X_{ih}\\) set variables correlated probability treated. adjust impact certain extent. example number individuals household. finally, \\(V_{ih}\\) set variables correlated lottery. variables include demographics, hospital discharge lottery draw.wide range literature related intervention. papers available .","code":""},{"path":"hunt-data.html","id":"case-study---student-coaching-how-far-can-technology-go","chapter":"9 Hunt data","heading":"9.6 Case study - Student Coaching: How Far Can Technology Go?","text":"general concern students dropping university finish degree. work one--one student addresses issue. doesn’t scale. point experiment see technology-based options efficient. focus University Toronto, particular first-year economics courses Fall 2015.intervention administered students part economics class. Students received 2 per cent grade completing exercise. specific exercise depended group student. intervention involved three treatments well control group just given Big Five personality traits test. Additional information obtained included ‘highest level education obtained students’ parents, amount education expect obtain, whether first-year international students, work study time plans upcoming year.’ (Oreopoulos Petronijevic 2018, 6).treatments (Oreopoulos Petronijevic 2018, 4):‘[] one-time, online exercise completed first two weeks class fall.’ exercise ‘designed get thinking future envision steps take upcoming year U T help make future reality. told exercise designed benefit take time completing . online module lasted approximately 60 90 minutes led students series writing exercises wrote ideal futures, work home, like accomplish current year U T, intend following certain study strategies meet goals, whether want get involved extracurricular activities university’ (Oreopoulos Petronijevic 2018, 6).‘[T]online intervention plus text email messaging throughout full academic year.’ involved students given ‘opportunity provide phone numbers participate text email messaging campaign lasting throughout fall semester 2015 winter semester 2016’ (Oreopoulos Petronijevic 2018, 8). students group got emails, provided phone numbers got messages. able opt , ‘chose ’ (Oreopoulos Petronijevic 2018, 8). two-way interaction students ask questions. asked ‘locations certain facilities campus stay residence holiday break, others said need help English skills specific courses. students expressed relatively deep emotions, feeling anxious family pressure succeed school poorly evaluation’ (Oreopoulos Petronijevic 2018, 9). response usually given within hour.‘[T]online intervention plus one--one coaching students assigned upper-year undergraduate coaches.’ ‘Coaches available meet students answer questions via Skype, phone, person, send students regular text email messages advice, encouragement, motivation, much like @UofT program described . contrast messaging program, however, coaches instructed proactive regularly monitor students’ progress. Whereas @UofT program attempts “nudge” students right direction academic advice, coaches play greater “support” role, sensitively guiding students problems.’ (Oreopoulos Petronijevic 2018, 11). coaching program available UTM. ‘coaching treatment group established randomly drawing twenty-four students group students randomly assigned text message campaign treatment. conclusion online exercise, instead invited provide phone number purpose receiving text messages, twenty-four students given opportunity participate pilot coaching program. total seventeen students agreed participate coaching program, seven students declined.’coaching treatment group established randomly drawing twenty-four students group students randomly assigned text message campaign treatment. conclusion online exercise, instead invited provide phone number purpose receiving text messages, twenty-four students given opportunity participate pilot coaching program. total seventeen students agreed participate coaching program, seven students declined.(Oreopoulos Petronijevic 2018, 14)model consider (Oreopoulos Petronijevic 2018, 15):\\[\\begin{equation}\ny_{ij} = \\alpha + \\beta_1\\mbox{Online}_i + \\beta_2\\mbox{Text}_i + + \\beta_3\\mbox{Coach}_i + \\delta_j + \\mu \\mbox{First year}_i + \\epsilon_{ij} \\tag{9.2}\n\\end{equation}\\]Equation (9.2) explains outcome student \\(\\) campus \\(j\\) based ‘indicators three treatment exercises students given, campus fixed effects, first-year student indicator.’ main parameters interest \\(\\beta_1\\), \\(\\beta_2\\) \\(\\beta_3\\). main outcomes course grades, GPA, credits earned failed.found, one--one coaching ‘increased grades approximately 5 percentage points,’ treatments ‘detectable impact.’ One set results summarised Figure 9.3.\nFigure 9.3: Example results intervention.\nresults important teaching context, also businesses hoping retain customers. papers available .","code":""},{"path":"hunt-data.html","id":"case-study---civic-honesty-around-the-globe","chapter":"9 Hunt data","heading":"9.7 Case study - Civic Honesty Around The Globe","text":"Trust isn’t something think regularly , ’s actually fairly fundamental interactions, economic personal. instance, many us get paid work - ’re trusting employer make good; vice versa - get paid advance trusting . strictly naive, one-shot, transaction-cost-less world, doesn’t make sense. get paid advance, incentive take money run last pay period quit, backward induction everything falls apart. course, don’t live world. one thing transaction costs, another generally repeated interactions, finally, experience, world usually ends fairly small.Understanding extent honestly different countries may help us explain economic development aspects interest tax compliance, ’s fairly hard measure. can’t really ask people honest - wouldn’t liars lie, resulting lemons problem (Akerlof 1978)? get around, . Cohn et al. (2019a) conduct experiment 355 cities across 40 countries ‘turn ’ either: wallet local equivalent US$13.45 , money. interested whether ‘recipient’ attempts return wallet. find ‘[virtually countries, citizens likely return wallets contained money’ (. Cohn et al. 2019a, 1).set-experiment fascinating. ‘turn ’ 17,303 wallets various institutions including: ‘() banks; (ii) theaters, museums, cultural establishments; (iii) post offices; (iv) hotels; (v) police stations, courts law, public offices’ (. Cohn et al. 2019a, 1). institutions roughly equally sampled, although banks slightly sampled post offices slightly -sampled. importance institutions economy generally well-accepted (Acemoglu, Johnson, Robinson 2001) common across countries. Importantly experiment, ‘typically public reception area perform drop-offs’ (. Cohn et al. 2019a, 1).way experiment worked research assistant turned wallet employee counter public reception area, saying ‘Hi, found [showing wallet] street just around corner. [Place wallet counter.] Somebody must lost . ’m hurry go. Can please take care ?’ (. Cohn et al. 2019a, 2). outcome interest whether email sent unique address business card wallet within 100 days. research assistant note various features setting, including features gender, age-group, busyness ‘recipient.’wallets transparent, business card name email contact details. also key grocery list (Figure 9.4).\nFigure 9.4: Example wallet.\ngrocery list attempt convince ‘recipient’ ‘owner’ local. Language currency adapted local conditions. key useful ‘owner,’ ‘recipient’ wallet included test altruistic concerns.primary treatment experiment whether wallet contained money . key outcome whether wallet attempted returned . found ‘[t]median response time roughly 26 minutes across countries, 88% emails arrived within 24 hours’ (. Cohn et al. 2019b, 10). email received, 3 hours later response sent, saying owner left town, contents unimportant keep donate charity (. Cohn et al. 2019b, 9).Considerable differences found countries (Figure 9.5).\nFigure 9.5: Key finding wallet return rates.\nFigure 9.5 shows almost countries wallets money likely returned wallets without. authors conducted experiment equivalent US$94.15 three countries - Poland, UK, US - found reporting rates increased. three countries tests done comparing situation wallet always contained money, presence key varied. wallet slightly likely reported key.full set 40 countries chosen based enough cities populations least 100,000, well ability research assistants safely visit withdraw cash. cities chosen starting largest ones usually 400 observations country (. Cohn et al. 2019b, 5). Real-world concerns affected specifics experiment. instance, ‘…India, made last minute change replacing Chennai Coimbatore due severe flooding took place February 2015. Kenya carry data collection last city visited (Malindi) research assistant arrested interrogated military police suspicious activity’ (. Cohn et al. 2019b, 5).addition experiments, . Cohn et al. (2019a) conducted surveys allowed understand reasons findings. also allowed specific respondents. survey involved 2,525 respondents (829 UK, 809 Poland, 887 US) (. Cohn et al. 2019b, 36). ‘qualify participation, individuals pass simple attention check meet demographic quotas (based age, gender, residence) set Qualtrics construct representative samples. Participants received flat payment US $4.00 participation’ (. Cohn et al. 2019b, 36). participants given one scenarios asked answer questions.Annoyingly authors don’t explicitly specify estimating equation. However say important covariates ‘recipient’ include: gender, age-group, busyness, whether local, spoke English, understood situation, friendliness, presence : computer, co-workers, bystanders, security cameras, security guards. Important covariates country-level include: country GDP, soil fertility, latitude, distance water, temperature volatility, precipitation volatility, elevation, terrain roughness, pathogens, language features : pronouns, politeness, future time; share protestants; family ties; state history; years democracy; executive constraints; judicial independence; constitutional review; electoral rule; primary school enrollment 1920.code data paper available: . Cohn (2019).","code":""},{"path":"hunt-data.html","id":"ab-testing","chapter":"9 Hunt data","heading":"9.8 A/B testing","text":"","code":""},{"path":"hunt-data.html","id":"introduction-12","chapter":"9 Hunt data","heading":"9.8.1 Introduction","text":"Large companies, particularly tech companies, developed incredibly sophisticated infrastructure running complex experiments. tech industry, experiments often called /B tests compare effectiveness two treatments: B. experiments frequently run things like increasing click-rates ads, experimental infrastructure can also used research advances scientific understanding.Salganik (2018, 185).past decade probably seen experiments ever run several orders magnitude extensive use /B testing websites. Every time online probably subject tens, hundreds, potentially thousands, different /B tests. use apps like TikTok run tens thousands. , heart, still just surveys result data need analysed, several interesting features, discuss.opening example Kohavi, Tang, Xu (2020, 3) particularly nice illustration.2012, employee working Bing, Microsoft’s search engine, suggested changing ad headlines display (Kohavi Thomke 2017). idea lengthen title line ads combining text first line title, shown Figure 1.1.Nobody thought simple change, among hundreds suggested, best revenue-generating idea Bing’s history!feature prioritized low languished backlog six months software developer decided try change, given easy code. implemented idea began evaluating idea real users, randomly showing new title layout others old one. User interactions website recorded, including ad clicks revenue generated . example /B test, simplest type controlled experiment compares two variants: B, Control Treatment.hours starting test, revenue--high alert triggered, indicating something wrong experiment. Treatment, , new title layout, generating much money ads. “good true” alerts useful, usually indicate serious bug, cases revenue logged twice (double billing) ads displayed, rest web page broken.experiment, however, revenue increase valid. Bing’s revenue increased whopping 12%, time translated $100M annually US alone, without significantly hurting key user-experience metrics. experiment replicated multiple times long period.example typifies several key themes online controlled experiments:hard assess value idea. case, simple change worth $100M/year delayed months.Small changes can big impact. $100M/year return--investment (ROI) days’ work one engineer extreme gets.Experiments big impact rare. Bing runs 10,000 experiments year, simple features resulting big improvement happen every years.overhead running experiment must small. Bing’s engineers access ExP, Microsoft’s experimentation system, made easy scientifically evaluate idea.overall evaluation criterion (OEC, described later chapter) must clear. case, revenue key component OEC, revenue alone insufficient OEC. lead plastering web site ads, known hurt user experience. Bing uses OEC weighs revenue user-experience metrics, including Sessions per user (users abandoning increasing engagement) several components. key point user-experience metrics significantly degrade even though revenue increased dramatically.notes, ’m going use /B testing strictly refer situation ’re dealing tech firm, type change code. dealing physical world ’ll stick RCTs.’m usually fairly dismissive CS folks adopt different language concepts around long time. However, case /B testing think ’s possibly justified. something different tens thousands small experiments time, compared normal RCT set-one experiment conducted months. finally, don’t work tech firm, don’t discount difficulty shifting experimental set-. may think ’s easy go workplace say ‘hey, let’s test stuff spend thousands/millions dollars.’ ’d wrong. opinion, hardest part /B testing isn’t science, ’s politics.","code":""},{"path":"hunt-data.html","id":"delivery","chapter":"9 Hunt data","heading":"9.8.2 Delivery","text":"Drawing Kohavi, Tang, Xu (2020, 153–61), first consider delivering /B test. case RCT ’s fairly obvious deliver - instance, make person come doctor’s clinic inject drug placebo. case /B testing, ’s less obvious - run ‘server-side’ ‘client-side?’ means , just change website - ‘server side,’ change app - ‘client side.’ may seem like silly issue, affects: 1) release; 2) data transmission.case effect release, ’s easy normal update website time, small changes can easily implemented case server-side. However, case client-side, let’s say app, ’s likely much bigger deal.needs get app store (bigger lesser deal depending one).need go release cycle (bigger lesser deal depending specifics company ships).Users opportunity upgrade. likely different upgrade? (Yes.)Now, case effect data transmission, server-side less big deal - kind get data part user interacting. case client-side - ’s necessarily case user internet time ’re using application, , may limitations data uploads. phone may limit data transmission depending effect battery, CPU, general performance, etc. maybe decide cache, user may find weird minor app takes much size photos.effect need plan build expectations - don’t promise results day release ’re evaluating client-side change. Adjust fact results conditional gather data conditions e.g. battery level whatever. Adjust analysis different devices platforms, etc. lovely opportunity multilevel regression.","code":""},{"path":"hunt-data.html","id":"instrumentation","chapter":"9 Hunt data","heading":"9.8.3 Instrumentation","text":"Drawing Kohavi, Tang, Xu (2020, 162 - 165), ’ll now discuss instrumentation. Kohavi, Tang, Xu (2020) use name ‘instrumentation.’ ’d prefer something like ‘measurement methods’ don’t confuse entirely different concept instrumental variables later course, instrumentation used industry, ’ll use .Regardless ’s called, point need consider getting data first place. instance, put cookie device different types users remove different rates. Using things like beacons can great (force user ‘download’ tiny thing don’t notice know ’ve gone somewhere - see ‘email’ etc). , practical issues - force beacon main content loads - makes worse customer experience; allow beacon load main content, case may get biased sample?likely different servers databases different faces product. instance, Twitter Australia, compared Twitter Canada, compared Twitter phone’s app, compared Twitter accessed via browser. Joining different datasets can difficult requires either unique id probabilistic approach.Kohavi, Tang, Xu (2020, 165) recommend changing culture workplace ensure instrumentation normalised, mean, yeah, good luck.","code":""},{"path":"hunt-data.html","id":"randomisation-unit","chapter":"9 Hunt data","heading":"9.8.4 Randomisation unit","text":", drawing Kohavi, Tang, Xu (2020, 162 - 165), need aware actually randomising ? , something ’s kind obvious normal RCTs, gets like really interesting case /B testing. Let’s consider malaria netting experiments - either person/village/state gets net doesn’t. Easy (relatively). case server-side /B testing - randomising page, session, user?think , let’s think colour. Let’s say change logo red blue ‘home’ page. ’re randomising page level, user goes ‘’ page logo back red. ’re randomising session level, ’ll blue ’re using website time, close come back ’ll red. Finally, ’re randomising user level ’ll always red , always blue friend. last bit assumes perfect identity tracking, might generally okay ’re Google Facebook, anyone else going challenge - visit cbc.ca phone laptop? ’re likely considered different ‘user.’matter? ’s trade-consistency importance.always interested whether treatment control groups created randomly. One way test /test. Taddy (2019, 129) describes ‘AB platforms typically run “AA” tests show website groups B. see significant difference groups AA trial, something likely wrong randomization.’Kohavi, Tang, Xu (2020, 201) says similarly, ‘(w)e highly recommend running continuous /tests parallel experiments uncover problems, including distribution mismatches plat- form anomalies.’","code":""},{"path":"hunt-data.html","id":"partnerships","chapter":"9 Hunt data","heading":"9.8.5 Partnerships","text":"Unless work Facebook/Twitter type firm, may possible run /B tests scale. can randomise personal website fairly easily, us won’t many visitors. Hence can important partner firms. Salganik (2018, 187) draws attention fact may tension ‘researchers partners.’example, Salganik (2018) discusses situation one treatment (three possible) accounted 98 per cent sample Facebook wanted treat everyone. researchers able convince ‘hold back 1 per cent related treatment 1 per cent control group.’ continues:without control group, basically impossible measure effect Info + Social treatment “perturb observe” experiment, rather randomized controlled experiment. example provides valuable practical lesson working partners: sometimes create experiment convincing someone deliver treatment sometimes create experiment convincing someone deliver treatment (.e. create control group).Salganik (2018, 188).order identify opportunities, Salganik (2018, 188) advises us ‘notice real problem can solve interesting science.’ Salganik (2018) closes four pieces advice:‘(Y)ou think much possible data collected’ Salganik (2018, 189).‘(Y)ou consider designing series experiments reinforce ’ Salganik (2018, 190).‘(c)reate zero variable cost data,’ : 1) trying replace human work computer work; 2) creating fun experiments participants want participate (Salganik 2018, 191).‘(b)uild ethics design: replace, refine, reduce,’ ‘(m)ake experiment humane replacing experiments non-experimental studies, refining treatments [harmless possible], reducing number participants’ (Salganik 2018, 196).","code":""},{"path":"hunt-data.html","id":"speed-vs-quality","chapter":"9 Hunt data","heading":"9.8.6 Speed vs quality","text":"Don’t peek results early call rest experiment ’ve got significance. essentially ruin everything underpins statistics .","code":""},{"path":"hunt-data.html","id":"conflicting-priorities","chapter":"9 Hunt data","heading":"9.8.7 Conflicting priorities","text":"One interesting aspects /B testing ’re usually running desperately care specific outcome, feeds measure care . instance, care whether website quite-dark-blue slightly-darker-blue white? Probably , probably care lot company share price. picking best blue comes cost share price?Obviously, bit contrived, let’s pretend work food delivery app ’re junior data scientist charge driver satisfaction. /B tests find drivers always happier able deliver food customer faster. Faster better, always. one way achieve faster deliveries, put food hot box maintain temperature. Something like might save 30 seconds, significant 10-15 minute deliver. Unfortunately, although making decision like basis /B tests designed optimize driver-satisfaction, ultimately likely make customer experience worse. customers receive cold food, (’s meant hot) may stop using service likely bad app longer term.trade-may obvious ’re running driver-experiment ’re looking customer complaints. Maybe small team start-. work larger team, ’d likely ensuring /B tests aren’t resulting false optimization something especially interesting, typical trade-normal RCT.","code":""},{"path":"hunt-data.html","id":"case-study---upworthy","chapter":"9 Hunt data","heading":"9.9 Case study - Upworthy","text":"trouble much /B testing ’s done firms typically don’t datasets can use. However, J. Nathan Matias (Cornell), Kevin Munger (Penn State), Marianne Aubin Le Quere (Cornell) obtained dataset /B tests Upworthy provide access (Matias et al. 2019). able request access dataset : https://upworthy.natematias.com (request may take couple weeks processed). Upworthy click-bait news company used /B testing optimize content. details provided Fitts (2014).Let’s quick look data.documentation: ‘Upworthy Research Archive contains packages within tests. Upworthy, packages bundles headlines images randomly assigned people website part test. Tests can include many packages.’ row package part test ‘clickability_test_id.’variety variables. ’ll focus ‘created_at,’ ‘clickability_test_id’ can create comparison groups, ‘headline,’ ‘impressions’ number people saw package, ‘clicks’ number clicked package. within batch tests, ’re interested effect varied headlines impressions clicks.going focus text contained headlines. also want remove effect different pictures, comparing image. ’m interested whether headlines asked question got clicks didn’t.identify whether headline asks question, ’m going just search question mark. Although complicated constructions use, enough get started.Now every test, every picture, want know whether asking question affected number clicks.find general, question headline may slightly decrease number clicks headline, although effect appear large (Figure 9.6).\nFigure 9.6: Comparison average number clicks headline contains question mark .\n","code":"\nupworthy <- read_csv(here::here(\"dont_push/upworthy-archive-exploratory-packages-03.12.2020.csv\"))\n\nupworthy %>% \n  head()\n#> # A tibble: 6 × 17\n#>    ...1 created_at          updated_at          clickability_test_id     excerpt\n#>   <dbl> <dttm>              <dttm>              <chr>                    <chr>  \n#> 1     0 2014-11-20 06:43:16 2016-04-02 16:33:38 546d88fb84ad38b2ce000024 Things…\n#> 2     1 2014-11-20 06:43:44 2016-04-02 16:25:54 546d88fb84ad38b2ce000024 Things…\n#> 3     2 2014-11-20 06:44:59 2016-04-02 16:25:54 546d88fb84ad38b2ce000024 Things…\n#> 4     3 2014-11-20 06:54:36 2016-04-02 16:25:54 546d902c26714c6c44000039 Things…\n#> 5     4 2014-11-20 06:54:57 2016-04-02 16:31:45 546d902c26714c6c44000039 Things…\n#> 6     5 2014-11-20 06:55:07 2016-04-02 16:25:54 546d902c26714c6c44000039 Things…\n#> # … with 12 more variables: headline <chr>, lede <chr>, slug <chr>,\n#> #   eyecatcher_id <chr>, impressions <dbl>, clicks <dbl>, significance <dbl>,\n#> #   first_place <lgl>, winner <lgl>, share_text <chr>, square <chr>,\n#> #   test_week <dbl>\n\nupworthy %>% \n  names()\n#>  [1] \"...1\"                 \"created_at\"           \"updated_at\"          \n#>  [4] \"clickability_test_id\" \"excerpt\"              \"headline\"            \n#>  [7] \"lede\"                 \"slug\"                 \"eyecatcher_id\"       \n#> [10] \"impressions\"          \"clicks\"               \"significance\"        \n#> [13] \"first_place\"          \"winner\"               \"share_text\"          \n#> [16] \"square\"               \"test_week\"\nupworthy_restricted <- \n  upworthy %>% \n  select(created_at, clickability_test_id, headline, impressions, clicks)\n\nhead(upworthy_restricted)\n#> # A tibble: 6 × 5\n#>   created_at          clickability_test_id     headline       impressions clicks\n#>   <dttm>              <chr>                    <chr>                <dbl>  <dbl>\n#> 1 2014-11-20 06:43:16 546d88fb84ad38b2ce000024 They're Being…        3052    150\n#> 2 2014-11-20 06:43:44 546d88fb84ad38b2ce000024 They're Being…        3033    122\n#> 3 2014-11-20 06:44:59 546d88fb84ad38b2ce000024 They're Being…        3092    110\n#> 4 2014-11-20 06:54:36 546d902c26714c6c44000039 This Is What …        3526     90\n#> 5 2014-11-20 06:54:57 546d902c26714c6c44000039 This Is What …        3506    120\n#> 6 2014-11-20 06:55:07 546d902c26714c6c44000039 This Is What …        3380     98\nupworthy_restricted <- \n  upworthy_restricted %>% \n  mutate(asks_question = stringr::str_detect(string = headline, pattern = \"\\\\?\"))\n\nupworthy_restricted %>% count(asks_question)\n#> # A tibble: 2 × 2\n#>   asks_question     n\n#>   <lgl>         <int>\n#> 1 FALSE         19130\n#> 2 TRUE           3536\nto_question_or_not_to_question <- \n  upworthy_restricted %>% \n  group_by(clickability_test_id, asks_question) %>% \n  summarise(ave_clicks = mean(clicks)) %>% \n  ungroup()\n\nlook_at_differences <- \n  to_question_or_not_to_question %>% \n  pivot_wider(id_cols = clickability_test_id,\n              names_from = asks_question,\n              values_from = ave_clicks) %>% \n  rename(ave_clicks_not_question = `FALSE`,\n         ave_clicks_is_question = `TRUE`) %>% \n  filter(!is.na(ave_clicks_not_question)) %>%\n  filter(!is.na(ave_clicks_is_question)) %>% \n  mutate(difference_in_clicks = ave_clicks_is_question - ave_clicks_not_question)\n\nlook_at_differences$difference_in_clicks %>% mean()\n#> [1] -4.890435"},{"path":"hunt-data.html","id":"implementing-surveys","chapter":"9 Hunt data","heading":"9.10 Implementing surveys","text":"","code":""},{"path":"hunt-data.html","id":"google","chapter":"9 Hunt data","heading":"9.10.1 Google","text":"","code":""},{"path":"hunt-data.html","id":"facebook","chapter":"9 Hunt data","heading":"9.10.2 Facebook","text":"","code":""},{"path":"hunt-data.html","id":"survey-monkey","chapter":"9 Hunt data","heading":"9.10.3 Survey Monkey","text":"","code":""},{"path":"hunt-data.html","id":"mechanical-turk","chapter":"9 Hunt data","heading":"9.10.4 Mechanical Turk","text":"","code":""},{"path":"hunt-data.html","id":"prolific","chapter":"9 Hunt data","heading":"9.10.5 Prolific","text":"","code":""},{"path":"hunt-data.html","id":"qualtrics","chapter":"9 Hunt data","heading":"9.10.6 Qualtrics","text":"","code":""},{"path":"hunt-data.html","id":"other-2","chapter":"9 Hunt data","heading":"9.10.7 Other","text":"","code":""},{"path":"hunt-data.html","id":"sensor-data","chapter":"9 Hunt data","heading":"9.11 Sensor data","text":"","code":""},{"path":"hunt-data.html","id":"next-steps","chapter":"9 Hunt data","heading":"9.12 Next steps","text":"Large scale experiments happening around us. days feel know lot healthcare experiments perhaps ’d like know AstraZeneca/Oxford situation especially interesting, instance, Oxford-AstraZeneca (2020), see Bastian (2020) actually possibly complicated.also well-known experiments tried see big government programs effective, :RAND Health Insurance Experiment randomly gave health insurance people US 1974 1982 (Brook et al. 1984).Oregon Health Study randomly gave health insurance Oregon 2008 (Finkelstein et al. 2012).","code":""},{"path":"hunt-data.html","id":"exercises-and-tutorial-8","chapter":"9 Hunt data","heading":"9.13 Exercises and tutorial","text":"","code":""},{"path":"hunt-data.html","id":"exercises-8","chapter":"9 Hunt data","heading":"9.13.1 Exercises","text":"words, role randomisation constructing counterfactual (write two three paragraphs)?external validity (pick one)?\nFindings experiment hold setting.\nFindings experiment hold outside setting.\nFindings experiment repeated many times.\nFindings experiment code data available.\nFindings experiment hold setting.Findings experiment hold outside setting.Findings experiment repeated many times.Findings experiment code data available.internal validity (pick one)?\nFindings experiment hold setting.\nFindings experiment hold outside setting.\nFindings experiment repeated many times.\nFindings experiment code data available.\nFindings experiment hold setting.Findings experiment hold outside setting.Findings experiment repeated many times.Findings experiment code data available.dataset named ‘netflix_data,’ columns ‘person’ ‘tv_show’ ‘hours,’ (person character class uniqueID every person, tv_show character class name tv show, hours double expressing number hours person watched tv show). please write code randomly assign people one two groups? data looks like :context randomisation, stratification mean (write paragraph two)?check randomisation done appropriately (write two three paragraphs)?Identify three companies conduct /B testing commercially write short paper work trade-offs . notable Toronto-based Canadian companies? think might case?Pretend work junior analyst large consulting firm. , pretend consulting firm taken contract put together facial recognition model Canada Border Services Agency’s Inland Enforcement branch. Taking page two, please discuss thoughts matter. ?types probability sampling, circumstances might want implement (write two three pages)?substantial political polling ‘misses’ recent years (Trump Brexit come mind). extent think non-response bias cause (write page two, sure ground writing citations)?estimate (pick one)?\nrule calculating estimate given quantity based observed data.\nquantity interest.\nresult.\nUnknown numbers determine statistical model.\nrule calculating estimate given quantity based observed data.quantity interest.result.Unknown numbers determine statistical model.estimator (pick one)?\nrule calculating estimate given quantity based observed data.\nquantity interest.\nresult.\nUnknown numbers determine statistical model.\nrule calculating estimate given quantity based observed data.quantity interest.result.Unknown numbers determine statistical model.estimand (pick one)?\nrule calculating estimate given quantity based observed data.\nquantity interest.\nresult.\nUnknown numbers determine statistical model.\nrule calculating estimate given quantity based observed data.quantity interest.result.Unknown numbers determine statistical model.parameter (pick one)?\nrule calculating estimate given quantity based observed data.\nquantity interest.\nresult.\nUnknown numbers determine statistical model.\nrule calculating estimate given quantity based observed data.quantity interest.result.Unknown numbers determine statistical model.seems like lot businesses closed downtown Toronto since pandemic. investigate , decide walk along blocks downtown count number businesses closed open. decide blocks walk, open map Toronto, start lake, pick every 10th street. type sampling (select )?\nCluster sampling.\nSystematic sampling.\nStratified sampling.\nSimple random sampling.\nConvenience sampling.\nCluster sampling.Systematic sampling.Stratified sampling.Simple random sampling.Convenience sampling.Please name reasons may wish use cluster sampling (select )?\nBalance responses.\nAdministrative convenience.\nEfficiency terms money.\nUnderlying systematic concerns.\nEstimation sub-populations.\nBalance responses.Administrative convenience.Efficiency terms money.Underlying systematic concerns.Estimation sub-populations.Please consider Beaumont, 2020, ‘probability surveys bound disappear production official statistics?’ reference paper, think probability surveys disappear, (please write paragraph two)?Ware (1989, 298) mentions ‘randomized play winner design.’ ?Ware (1989, 299) mentions ‘adaptive randomization.’ , words?Ware (1989, 299) mentions ‘randomized-consent.’ continues ‘attractive setting standard approach informed consent require parents infants near death approached give informed consent invasive surgical procedure , instances, administered. familiar agonizing experience child neonatal intensive care unit can appreciate process obtaining informed consent frightening stressful parents.’ extent agree position, especially given, Ware (1989), p. 305, mentions ‘need withhold information study parents infants receiving CMT?’Ware (1989, 300) mentions ‘equipoise.’ words, please define discuss , using example experience.power (statistical context)?","code":"\nlibrary(tidyverse)\nnetflix_data <- \n  tibble(person = c(\"Rohan\", \"Rohan\", \"Monica\", \"Monica\", \"Monica\", \n                    \"Patricia\", \"Patricia\", \"Helen\"),\n         tv_show = c(\"Broadchurch\", \"Duty-Shame\", \"Broadchurch\", \"Duty-Shame\", \n                     \"Shetland\", \"Broadchurch\", \"Shetland\", \"Duty-Shame\"),\n         hours = c(6.8, 8.0, 0.8, 9.2, 3.2, 4.0, 0.2, 10.2)\n         )"},{"path":"hunt-data.html","id":"tutorial-8","chapter":"9 Hunt data","heading":"9.13.2 Tutorial","text":"purpose tutorial ensure clear mind thoroughly know dataset. builds ‘memory palace’ technique used professional memorisers, described Foer (2011).Please think childhood home, another building know intimately. Imagine standing front . Describe looks like. ‘walk’ front throughout house, describing aspect much detail can imagine. rooms used distinguishing features? smell? evoke ? Please write page two.Now think dataset ’re interested . Please exercise, dataset.","code":""},{"path":"farm-data.html","id":"farm-data","chapter":"10 Farm data","heading":"10 Farm data","text":"STATUS: construction.Required readingCrawford (2021), Chapter 3.Statistics Canada (2017), Chapter 10 - Data quality assessment.Recommended readingKey concepts/skills/etcKey librariesKey functions/etc","code":""},{"path":"farm-data.html","id":"overview","chapter":"10 Farm data","heading":"10.1 Overview","text":"variety sources data produced purposes used datasets. One thinks especially censuses. Whitby (2020, 30–31) provides enthralling overview, describing ‘(t)earliest censuses suggested writing come… China’s Yellow River valley’ used just purposes taxation conscription. continues, highlighting links census Christianity, instance Book Luke ‘days Caesar Augustus issued decree census taken entire Roman world,’ led David Mary travelling Bethlehem.Taxation substantial motivator censuses. Jones (1953) describes census records survive ‘probably engraved late third early fourth century .D., Diocletian colleagues successors known active carrying censuses serve basis new system taxation.’ detailed records sort abused. instance, Luebke Milton (1994) say ‘(t)Nazi regime gathered information two relatively conventional tools modern administration: national census police registration.’Another source data deliberately put together dataset include economic conditions unemployment, inflation, GDP. Interestingly, Rockoff (2019) describes economic statistics actually developed federal government, even though ‘eventually took role producing statistics regular basis.’ broader point types datasets censuses typically put together governments. powers state behind .Another, similarly large established source data long-running large surveys. conducted regular basis, usually directly conducted government, usually funded, one way another, government. instance, often think electoral surveys, Canadian Election Study, run association every federal election since 1965, similarly British Election Study associated every general election since 1964.Finally, large push toward open data government. term become contentious occurred practice, underlying principle - government make available data - undeniable.chapter cover datasets, term ‘farmed data.’ typically fairly nicely put together work collecting, preparing cleaning datasets typically done. also, usually, conducted set release cycle. instance, developed countries release unemployment inflation dataset monthly basis, GDP quarterly basis, census every five ten years.datasets always useful, developed time much analysis conducted without use scripts programming languages. cottage industry R package development sprung around making easier get datasets R. chapter cover especially useful.important recognise data neutral. Thinking clearly included dataset, systematically excluded, critical. Crawford (2021, 121) says:way data understood, captured, classified, named fundamentally act world-making containment. enormous ramifications way artificial intelligence works world communities affected. myth data collection benevolent practice computer science obscured operations power, protecting profit avoiding responsibility consequences.","code":""},{"path":"farm-data.html","id":"censuses","chapter":"10 Farm data","heading":"10.2 Censuses","text":"MEasuring homelessness?https://www.ncbi.nlm.nih.gov/books/NBK218229/“S-Night survey conducted US Census Bureau 1990”“Street count surveys used many cities count number homeless people streets point time gain better understanding needs homeless populations. surveys S-Night survey conducted US Census Bureau 1990, enumerators sent pre-identified sites enumerate homeless people survey personnel (“plants”) planted among homeless people indistinguishable actual homeless. ratio plants seen enumerators number plants deployed used inform detection probability homeless provide adjustment homeless undercount. practice, one know sure plants seen enumerators distinguish plants homeless people. can rely plants’ judgement whether (yes, maybe) think seen enumerator. presence “maybes” data leads unknown parameters data points, makes estimation detection probabilities difficult. propose solve problem developing Bayesian hierarchical model uses hierarchical priors detection probabilities across survey years /across cities. hierarchical modeling data challenging data available various aggregated levels population (e.g., seen = plant seen + homeless seen, maybes = plant seen interviewed maybe + plant seen maybe.) new methodology applied simulated reconstruction original S-night survey data estimates compared original analysis S-night data.\"","code":""},{"path":"farm-data.html","id":"canada","chapter":"10 Farm data","heading":"10.2.1 Canada","text":"first census Canada conducted 1666. 3,215 inhabitants counted census ‘recorded age, sex, marital status occupation’ (Statistics Canada 2017). 1867 decennial census required ‘determine representation population new Parliament.’ Regular censuses occurred since , recent 2021.general, data Canadian census easily available countries. ‘Individuals File, 2016 Census Public Use Microdata Files (PUMF)’ - https://www150.statcan.gc.ca/n1/en/catalogue/98M0001X - probably best first step, although provide much detail. 2.7 per cent sample 2016 census. free, must requested, Statistics Canada email access details.Another way access data Canadian census use cancensus, R package provides access Canadian census (von Bergmann, Shkolnik, Jacobs 2021). use package requires API key, can requested creating account : https://censusmapper.ca/users/sign_up clicking ‘edit profile.’ package helper function cancensus::set_api_key(\"ADD_YOUR_API_KEY_HERE\", install = TRUE) makes easier add key ‘.Renviron’ file. done can use package access data.main function package cancensus::get_census(), requires argument census interest, variety factors. example use 2016 censusThe package fiddly, however worthwhile ensure reproducible workflow data interested included. two helper functions— list_census_regions() list_census_vectors() — may useful identify arguments interest.","code":"library(tidyverse)\nlibrary(cancensus)\n\nontario_population <- \n  get_census(dataset = \"CA16\",\n             level = \"Regions\",\n             vectors = \"v_CA16_1\", \n             regions = list(PR=c('35')\n                            )\n             )\n#> \nDownloading: 170 B     \nDownloading: 170 B     \nDownloading: 170 B     \nDownloading: 170 B     \nDownloading: 170 B     \nDownloading: 170 B\n\nhead(ontario_population)\n#> # A tibble: 1 × 9\n#>   GeoUID Type  `Region Name` `Area (sq km)` Population Dwellings Households\n#>   <chr>  <fct> <fct>                  <dbl>      <dbl>     <dbl>      <dbl>\n#> 1 35     PR    Ontario              986722.   13448494   5598391    5169174\n#> # … with 2 more variables: C_UID <chr>, v_CA16_1: Age Stats <dbl>"},{"path":"farm-data.html","id":"usa","chapter":"10 Farm data","heading":"10.2.2 USA","text":"requirement US Census included constitution, fairly decent, although clunky, general access. However, US interesting situation may actually better options. instance, American Community Survey (ACS) survey comparable questions asked many censuses conducted monthly. end year, ends millions responses. Although ACS smaller census, advantage available timely basis.best way access ACS use Integrated Public Use Microdata Series (IPUMS). need create account, free easy .Lol, just use IPUMS.","code":""},{"path":"farm-data.html","id":"australia","chapter":"10 Farm data","heading":"10.2.3 Australia","text":"","code":""},{"path":"farm-data.html","id":"other-government-data","chapter":"10 Farm data","heading":"10.3 Other government data","text":"","code":""},{"path":"farm-data.html","id":"unemployment","chapter":"10 Farm data","heading":"10.3.1 Unemployment","text":"","code":""},{"path":"farm-data.html","id":"weather","chapter":"10 Farm data","heading":"10.3.2 Weather","text":"","code":""},{"path":"farm-data.html","id":"that-canadian-survey-that-we-used-in-sta304-from-the-library","chapter":"10 Farm data","heading":"10.3.3 That Canadian survey that we used in STA304 from the library","text":"","code":""},{"path":"farm-data.html","id":"open-government-data","chapter":"10 Farm data","heading":"10.4 Open Government Data","text":"","code":""},{"path":"farm-data.html","id":"canada-1","chapter":"10 Farm data","heading":"10.4.1 Canada","text":"Canadian Government Open Data: https://open.canada.ca/en/open-data.Canadian Government Open Data: https://open.canada.ca/en/open-data.City Toronto Open Data PortalCity Toronto Open Data Portal","code":""},{"path":"farm-data.html","id":"electoral-studies","chapter":"10 Farm data","heading":"10.5 Electoral Studies","text":"","code":""},{"path":"farm-data.html","id":"canadian-electoral-study","chapter":"10 Farm data","heading":"10.5.1 Canadian Electoral Study","text":"","code":""},{"path":"farm-data.html","id":"australian-electoral-study","chapter":"10 Farm data","heading":"10.5.2 Australian Electoral Study","text":"","code":""},{"path":"farm-data.html","id":"cces","chapter":"10 Farm data","heading":"10.5.3 CCES","text":"","code":""},{"path":"farm-data.html","id":"other-3","chapter":"10 Farm data","heading":"10.6 Other","text":"University Toronto Dataverse: https://dataverse.scholarsportal.info/dataverse/toronto.Data Plural structured archive: https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/edit#gid=0.Kaggle Datasets: https://www.kaggle.com/datasets.Figure Eight: https://www.figure-eight.com/data--everyone/.Google dataset search: https://datasetsearch.research.google.com/.Awesome Data: https://github.com/awesomedata/awesome-public-datasets.","code":""},{"path":"farm-data.html","id":"exercises-and-tutorial-9","chapter":"10 Farm data","heading":"10.7 Exercises and tutorial","text":"","code":""},{"path":"farm-data.html","id":"exercises-9","chapter":"10 Farm data","heading":"10.7.1 Exercises","text":"Please identify three sources data interested describe available (please include link code)?Please focus one sources. steps go order get dataset can analysed R?Let’s say take job RBC (Canadian bank) already quantitative data use. questions explore deciding whether data useful ?Write three points (welcome use dot points) government data may especially useful?Please pick government interest find inflation statistics. extent know data gathered?reference Chen et al. (2019) Martinez (2019) extent think can trust government statistics? Please mention least three governments answer.2021 census Canada asked, firstly, ‘person’s sex birth? Sex refers sex assigned birth. Male/Female,’ ‘person’s gender? Refers current gender may different sex assigned birth may different indicated legal documents. Male/Female/please specify person’s gender (space typed handwritten answer).’ reference Statistics Canada (2020), please discuss extent think appropriate way census proceeded. welcome discuss case different country familiar .Pretend conducted survey everyone Canada, asked age, sex, gender. friend claims need worry uncertainty ‘whole population.’ friend right wrong, ?","code":""},{"path":"farm-data.html","id":"tutorial-9","chapter":"10 Farm data","heading":"10.7.2 Tutorial","text":"","code":""},{"path":"cleaning-and-preparing-data.html","id":"cleaning-and-preparing-data","chapter":"11 Cleaning and preparing data","heading":"11 Cleaning and preparing data","text":"STATUS: construction.Required readingData Feminism, Chapter 5 (D’Ignazio Klein 2020).R Data Science, Chapter 9 (Wickham Grolemund 2017).Recommended readingWe Gave Four Good Pollsters Raw Data. Four Different Results, (N. Cohn 2016).Data Cleaning Procedures 1993 Robert Wood Johnson Foundation Employer Health Insurance Survey, (Euller, Long, Marquis 1997).Data Cleaning, (Ilyas Chu 2019).Key concepts/skills/etcPlanning end-point, simulating data ’d like, key elements cleaning preparing data.Begin small sample dataset, write code fix , iterate generalize additional tranches.Develop series tests checks final dataset pass features dataset clear.especially concerned class variables, clear names, values variable expected given .Key librariesjanitor (Firke 2020)stringr (Wickham 2019e)tidyr (Wickham 2021)Key functions/etcdplyr::count()dplyr::mutate()dplyr::select()janitor::clean_names()stringr::str_replace_all()stringr::str_trim()tidyr::pivot_longer()tidyr::separate()tidyr::separate_rows()","code":""},{"path":"cleaning-and-preparing-data.html","id":"introduction-13","chapter":"11 Cleaning and preparing data","heading":"11.1 Introduction","text":"“Well, Lyndon, may right may every bit intelligent say,” said Rayburn, “’d feel whole lot better just one run sheriff .”Sam Rayburn’s reaction Lyndon Johnson’s enthusiasm Kennedy’s incoming cabinet, quoted Halberstam (1972, 41).earlier chapters done data cleaning preparation, chapter, get weeds. trust anyone works data spent time, point career, cleaning data. end chapter think ’ll feel way. clean prepare data make lot different decisions, many important.long time, data cleaning preparation largely overlooked. now realise mistake. longer possible trust almost result disciplines apply statistics. reproducibility crisis, started psychology now extended many fields physical social sciences, brought light issues p-value ‘hacking,’ researcher degrees freedom, file-drawer issues, even data results fabrication (Gelman Loken 2013). Steps now put place address . However, relatively little focus data gathering, cleaning, preparation aspects applied statistics, despite evidence decisions made steps greatly affect statistical results (Huntington-Klein et al. 2020). chapter focus issues.statistical practices underpin data science correct robust applied ‘fake’ datasets created simulated environment, data science typically conducted types datasets. instance, data scientists interested :…kind messy, unfiltered, possibly unclean data—tainted heteroskedasticity, complex dependence missingness patterns—recently avoided polite conversations traditional statisticians…(Craiu 2019).Big data resolve issue, may even exacerbate , instance ‘without taking data quality account, population inferences Big Data subject Big Data Paradox: data, surer fool ’ (Meng 2018). important note issues found much applied statistics research necessarily associated researcher quality, biases (Silberzahn et al. 2018). Instead, result environment within data science conducted. chapter aims give tools explicitly think work.Gelman Vehtari (2020) writing important statistical ideas past 50 years say :…much method solving existing problem, opening new ways thinking statistics new ways data analysis. put another way, ideas codification, bringing inside tent approach considered matter taste philosophy statistics.see focus data cleaning preparation chapter analogous, insofar, represents codification, bringing inside tent, aspects typically (incorrectly) considered taste rather statistics.approach recommend follow :Duplicate.Plan end state.Execute plan tiny sample.Write tests documentation.Iterate plan.Generalize execution.Update tests documentation.need use skills point effective, stuff statistical sciences! dogged, sensible. best enemy good . ’s better 90 per cent data cleaned prepared, start exploring , deciding whether ’s worth effort clean prepare remaining 10 per cent remainder likely take awful lot time effort.Van den Broeck et al. (2005) say, data regardless whether obtained hunting, gathering, farming, issues critical understand ‘deal errors various sources’ understand ‘effects study results.’ clean data analyze data. Au (2020) says ‘act cleaning data act preferentially transforming data chosen analysis algorithm produces interpretable results. also act data analysis.’ attemping triangulate situation.","code":""},{"path":"cleaning-and-preparing-data.html","id":"workflow-1","chapter":"11 Cleaning and preparing data","heading":"11.2 Workflow","text":"","code":""},{"path":"cleaning-and-preparing-data.html","id":"save-a-copy-of-the-raw-data","chapter":"11 Cleaning and preparing data","heading":"11.2.1 Save a copy of the raw data","text":"first step save raw data separate folder location. critical save raw data extent possible (J. . C. Wilson Greg Bryan 2017). ideal situation, duplicate folder contains raw data, rename duplicated folder ‘cleaned’ ever modify data cleaned folder.","code":""},{"path":"cleaning-and-preparing-data.html","id":"begin-with-an-end-in-mind-1","chapter":"11 Cleaning and preparing data","heading":"11.2.2 Begin with an end in mind","text":"Planning end state, forcing begin end mind important variety reasons. scraping data, helps us pre-active scope-creep, data cleaning see bigger benefit forces us really think want final dataset look like. , recommend first sketching dataset. key features sketch aspects names columns, class, possible range values. might look something like Figure 11.1 .\nFigure 11.1: Example dataset end plan\nNotice process made clear want full names states, rather abbreviations. population millions, rather units. process sketching end-point forces us make decisions clear desired end state.implement using code simulate data. , process forces us think reasonable values look like dataset literally forced decide functions use. Thinking carefully membership column , instance column meant ‘gender’ values ‘male,’ ‘female,’ ‘,’ ‘unknown’ may expected, number ‘1,000’ likely unexpected. also forces us explicit variable names assign outputs functions variable.Building example , perhaps might look something like .purpose, data cleaning preparation bring dataset close plan.desired end-state typically ‘tidy data’ (Wickham 2014, 4):variable columnEach observation rowEach observational unit table.following example data tidy format:","code":"\nlibrary(tidyverse)\n\nsimulated_states_population <- \n  tibble(\n    state = c('Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California'),\n    population = c(5, 0.7, 7, 3, 40)\n  )\n\nsimulated_states_population\n#> # A tibble: 5 × 2\n#>   state      population\n#>   <chr>           <dbl>\n#> 1 Alabama           5  \n#> 2 Alaska            0.7\n#> 3 Arizona           7  \n#> 4 Arkansas          3  \n#> 5 California       40\ntibble(\n  name = c(\"Alfred\", \"Ben\", \"Chris\", \"Daniela\", \"Emma\"),\n  age_group = c(\"0-9\", \"10-19\", \"0-9\", \"0-9\", \"10-19\"),\n  response_time = c(0, 9, 3, 3, 8),\n  )\n#> # A tibble: 5 × 3\n#>   name    age_group response_time\n#>   <chr>   <chr>             <dbl>\n#> 1 Alfred  0-9                   0\n#> 2 Ben     10-19                 9\n#> 3 Chris   0-9                   3\n#> 4 Daniela 0-9                   3\n#> 5 Emma    10-19                 8"},{"path":"cleaning-and-preparing-data.html","id":"start-small","chapter":"11 Cleaning and preparing data","heading":"11.2.3 Start small","text":"point, first look data dealing . Regardless raw data look like want try get rectangular dataset quickly possible can use familiar verbs. Let’s assume ’re starting .txt file.first step look regularities dataset. wanting end tabular data, means need type deliminator distinguish different columns. Ideally might features comma, semicolon, tab, double space, line break.worse cases may feature dataset can take advantage . instance, possibly data look like following:case, although don’t traditional deliminator can use regularity ‘State ’ ’ population ’ get need.difficult case following:One way approach take advantage different classes values ’re looking . instance, case, know ’re US states, 50 possible options use existence deliminator. also use fact population number , split based space followed number.move forward, ’ll assume last example move rectangular dataset.","code":"\n# Alabama, 5 million.\n# Alaska, 0.7 million.\n# Arizona, 7 million.\n# Arkansas, 3 million.\n# California, 40 million.\n# State is Alabama and population is 5 million.\n# State is Alaska and population is 0.7 million.\n# State is Arizona and population is 7 million.\n# State is Arkansas and population is 3 million.\n# State is California and population is 40 million.\n# Alabama 5 Alaska 0.7 Arizona 7 Arkansas 3 California 40\nraw_data <- c('Alabama 5 Alaska 0.7 Arizona 7 Arkansas 3 California 40')\n\ndata <- \n  tibble(raw = raw_data)\n\ndata <- \n  data %>% \n  tidyr::separate(col = raw, \n                  into = letters[1:5],\n                  sep = \"(?<=[[:digit:]]) \") %>% \n  pivot_longer(cols = letters[1:5],\n               names_to = \"drop_me\",\n               values_to = \"separate_me\") %>% \n  tidyr::separate(col = separate_me, \n                  into = c('state', 'population'),\n                  sep = \" (?=[[:digit:]])\") %>% \n  select(-drop_me)\n\ndata\n#> # A tibble: 5 × 2\n#>   state      population\n#>   <chr>      <chr>     \n#> 1 Alabama    5         \n#> 2 Alaska     0.7       \n#> 3 Arizona    7         \n#> 4 Arkansas   3         \n#> 5 California 40"},{"path":"cleaning-and-preparing-data.html","id":"write-tests-and-documentation.","chapter":"11 Cleaning and preparing data","heading":"11.2.4 Write tests and documentation.","text":"established rectangular dataset, albeit messy one, begin look classes . don’t necessarily want fix classes point, can result us losing data. look class see , compare simulated dataset see needs get . note columns different.changing class going onto bespoke issues, deal common issues class. common issues :Commas punctuation, denomination signs columns numeric.Inconsistent formatting dates, ‘December’ also ‘Dec’ ‘12.’Unexpected characters, especially unicode, may display consistently.Typically, want fix anything immediately obvious. instance, remove commas used group digits currencies. However, situation typically quickly become dire. need look membership group, triage fix. probably make decision triage based likely largest impact. usually means starting counts, sorting descending order, dealing come.tests membership passed, finally change class, run tests . ’re adapting idea software development approach unit testing. Tests crucial enable us understand whether software (case data) fit purpose (G. Wilson 2021), Chapter ‘Testing.’Let’s run example collection strings, slightly wrong. type output typical OCR, often gets way , quite., first want get rectangular dataset.now need decide errors going fix. help us decide important, ’ll create count.common element correct one, great. next one - ‘ROhan’ - looks like ‘o’ incorrectly capitalized, one - ‘5Ohan’ - distinguished ‘5’ instead ‘R.’ Let’s quickly fix issues redo count.Already much better 60 per cent values correct, compared earlier 30 per cent. two obvious errors - ‘Rhan’ ‘Roham’ - first missing ‘o’ second ‘m’ ‘n’ . , can quickly update fix .achieved 80 per cent fix much effort. two remaining issues subtle. first - ‘R0han’ - occurred ‘o’ incorrectly coded ‘0.’ fonts show , others difficult see. common issue, especially OCR, something aware . second - ‘Rohan’ - similarly subtle occurring trailing space. , trailing leading spaces common issue. fix two remaining issues entries corrected.’ve tests head example - know ’re hoping ‘Rohan.’ can start document test well. One way look see values ‘Rohan’ exist dataset.","code":"\nmessy_string <- c('Rohan, Rhan, ROhan, Roham, ROhan, Rohan, Rohan, R0han, Rohan , 5Ohan')\nmessy_string\n#> [1] \"Rohan, Rhan, ROhan, Roham, ROhan, Rohan, Rohan, R0han, Rohan , 5Ohan\"\nmy_data <- \n  tibble(names = messy_string) %>% \n  tidyr::separate_rows(names, sep = \", \") \nmy_data\n#> # A tibble: 10 × 1\n#>    names   \n#>    <chr>   \n#>  1 \"Rohan\" \n#>  2 \"Rhan\"  \n#>  3 \"ROhan\" \n#>  4 \"Roham\" \n#>  5 \"ROhan\" \n#>  6 \"Rohan\" \n#>  7 \"Rohan\" \n#>  8 \"R0han\" \n#>  9 \"Rohan \"\n#> 10 \"5Ohan\"\nmy_data %>% \n  count(names, sort = TRUE)\n#> # A tibble: 7 × 2\n#>   names        n\n#>   <chr>    <int>\n#> 1 \"Rohan\"      3\n#> 2 \"ROhan\"      2\n#> 3 \"5Ohan\"      1\n#> 4 \"R0han\"      1\n#> 5 \"Rhan\"       1\n#> 6 \"Roham\"      1\n#> 7 \"Rohan \"     1\nmy_data <- \n  my_data %>% \n  mutate(names = str_replace_all(names, 'ROhan', 'Rohan'),\n         names = str_replace_all(names, '5Ohan', 'Rohan')\n         )\n\nmy_data %>% \n  count(names, sort = TRUE)\n#> # A tibble: 5 × 2\n#>   names        n\n#>   <chr>    <int>\n#> 1 \"Rohan\"      6\n#> 2 \"R0han\"      1\n#> 3 \"Rhan\"       1\n#> 4 \"Roham\"      1\n#> 5 \"Rohan \"     1\nmy_data <- \n  my_data %>% \n  mutate(names = str_replace_all(names, 'Rhan', 'Rohan'),\n         names = str_replace_all(names, 'Roham', 'Rohan')\n         )\n\nmy_data %>% \n  count(names, sort = TRUE)\n#> # A tibble: 3 × 2\n#>   names        n\n#>   <chr>    <int>\n#> 1 \"Rohan\"      8\n#> 2 \"R0han\"      1\n#> 3 \"Rohan \"     1\nmy_data <- \n  my_data %>% \n  mutate(names = str_replace_all(names, 'R0han', 'Rohan'),\n         names = stringr::str_trim(names, side = c(\"right\"))\n         )\n\nmy_data %>% \n  count(names, sort = TRUE)\n#> # A tibble: 1 × 2\n#>   names     n\n#>   <chr> <int>\n#> 1 Rohan    10\ncheck_me <- \n  my_data %>% \n  filter(names != \"Rohan\")\n\nif (nrow(check_me) > 0) {\n  print(\"Still have values that are not Rohan!\")\n}"},{"path":"cleaning-and-preparing-data.html","id":"iterate-generalize-and-update","chapter":"11 Cleaning and preparing data","heading":"11.2.5 Iterate, generalize and update","text":"now iterate plan. recent case started 10 entries. reason couldn’t increase 100 even 1,000. may need generalize cleaning proceedures tests. eventually start dataset sort order.","code":""},{"path":"cleaning-and-preparing-data.html","id":"case-study---kenya-census","chapter":"11 Cleaning and preparing data","heading":"11.3 Case study - Kenya census","text":"make clear, let’s return Kenyan census downloaded PDFs Chapter 8.distribution population age, sex, administrative unit 2019 Kenyan census can downloaded : https://www.knbs..ke/?wpdmpro=2019-kenya-population--housing-census-volume-iii-distribution--population--age-sex--administrative-units.great make easily available, easy look-particular result, overly useful larger-scale data analysis, building Bayesian hierarchical model.section convert PDF Kenyan census results counts, age sex, county sub-county, tidy dataset can analysed. draw introduce bunch handy packages including: janitor Firke (2020), pdftools Ooms (2019b), tidyverse Wickham et al. (2019b), stringi Gagolewski (2020).","code":""},{"path":"cleaning-and-preparing-data.html","id":"set-up","chapter":"11 Cleaning and preparing data","heading":"11.3.1 Set-up","text":"get started need load necessary packages.need read PDF want convert.pdf_text function pdftools useful PDF want read content R. many recently produced PDFs ’ll work pretty well, alternatives. PDF image, won’t work, ’ll need turn OCR.can see page PDF :","code":"\nlibrary(janitor)\nlibrary(pdftools)\nlibrary(tidyverse)\nlibrary(stringi)\n# Read in the PDF\nall_content <- pdftools::pdf_text(\"inputs/pdfs/2019_Kenya_census.pdf\")\nknitr::include_graphics(\"figures/2020-04-10-screenshot-of-census.png\") "},{"path":"cleaning-and-preparing-data.html","id":"extract","chapter":"11 Cleaning and preparing data","heading":"11.3.2 Extract","text":"first challenge get dataset format can easily manipulate. way going consider page PDF extract relevant parts. , first write function want apply page.point, function need page PDF. ’m going use function map_dfr purrr package apply function page, combine outputs one tibble.","code":"\n# The function is going to take an input of a page\nget_data <- function(i){\n  # Just look at the page of interest\n  # Based on https://stackoverflow.com/questions/47793326/tabulize-function-in-r\n  just_page_i <- stringi::stri_split_lines(all_content[[i]])[[1]] \n  \n  # Grab the name of the location\n  area <- just_page_i[3] %>% str_squish()\n  area <- str_to_title(area)\n  \n  # Grab the type of table\n  type_of_table <- just_page_i[2] %>% str_squish()\n  \n  # Get rid of the top matter\n  just_page_i_no_header <- just_page_i[5:length(just_page_i)] # Just manually for now, but could create some rules if needed\n  \n  # Get rid of the bottom matter\n  just_page_i_no_header_no_footer <- just_page_i_no_header[1:62] # Just manually for now, but could create some rules if needed\n  \n  # Convert into a tibble\n  demography_data <- tibble(all = just_page_i_no_header_no_footer)\n  \n  # # Split columns\n  demography_data <-\n    demography_data %>%\n    mutate(all = str_squish(all)) %>% # Any space more than two spaces is squished down to one\n    mutate(all = str_replace(all, \"10 -14\", \"10-14\")) %>% \n    mutate(all = str_replace(all, \"Not Stated\", \"NotStated\")) %>% # Any space more than two spaces is squished down to one\n    separate(col = all,\n             into = c(\"age\", \"male\", \"female\", \"total\", \"age_2\", \"male_2\", \"female_2\", \"total_2\"),\n             sep = \" \", # Just looking for a space. Seems to work fine because the tables are pretty nicely laid out\n             remove = TRUE,\n             fill = \"right\"\n    )\n  \n  # They are side by side at the moment, need to append to bottom\n  demography_data_long <-\n    rbind(demography_data %>% select(age, male, female, total),\n          demography_data %>%\n            select(age_2, male_2, female_2, total_2) %>%\n            rename(age = age_2, male = male_2, female = female_2, total = total_2)\n    )\n  \n  # There is one row of NAs, so remove it\n  demography_data_long <- \n    demography_data_long %>% \n    janitor::remove_empty(which = c(\"rows\"))\n  \n  # Add the area and the page\n  demography_data_long$area <- area\n  demography_data_long$table <- type_of_table\n  demography_data_long$page <- i\n  \n  rm(just_page_i,\n     i,\n     area,\n     type_of_table,\n     just_page_i_no_header,\n     just_page_i_no_header_no_footer,\n     demography_data)\n  \n  return(demography_data_long)\n}\n# Run through each relevant page and get the data\npages <- c(30:513)\nall_tables <- map_dfr(pages, get_data)\nrm(pages, get_data, all_content)"},{"path":"cleaning-and-preparing-data.html","id":"clean-2","chapter":"11 Cleaning and preparing data","heading":"11.3.3 Clean","text":"now need clean dataset make useful.","code":""},{"path":"cleaning-and-preparing-data.html","id":"values","chapter":"11 Cleaning and preparing data","heading":"11.3.3.1 Values","text":"first step make numbers actual numbers, rather characters. can convert type need remove anything number otherwise ’ll converted NA. first identify values numbers can remove .use janitor package , worthwhile least first looking going sometimes odd stuff janitor (packages) deal , way want. case, ’ve used Excel similar converted couple entries dates. just took numbers column ’d 23 15 , inspecting column can use Excel reverse process enter correct values 4,923 4,611, respectively.identified everything needs removed, can actual removal convert character column numbers integers.","code":"\n# Need to convert male, female, and total to integers\n# First find the characters that should not be in there\nall_tables %>% \n  select(male, female, total) %>%\n  mutate_all(~str_remove_all(., \"[:digit:]\")) %>% \n  mutate_all(~str_remove_all(., \",\")) %>%\n  mutate_all(~str_remove_all(., \"_\")) %>%\n  mutate_all(~str_remove_all(., \"-\")) %>% \n  distinct()\n#> # A tibble: 39 × 3\n#>    male    female         total  \n#>    <chr>   <chr>          <chr>  \n#>  1  <NA>    <NA>           <NA>  \n#>  2 \".:\"    \"Distribution\" \"of\"   \n#>  3 \"Male\"  \"Female\"       \"Total\"\n#>  4 \"\"      \"\"             \"\"     \n#>  5 \"by\"    \"Age\"          \"Sex*\" \n#>  6 \"LUNGA\"  <NA>           <NA>  \n#>  7 \"NORTH\"  <NA>           <NA>  \n#>  8 \"SOUTH\"  <NA>           <NA>  \n#>  9 \"RIVER\"  <NA>           <NA>  \n#> 10 \"DELTA\"  <NA>           <NA>  \n#> # … with 29 more rows\n# We clearly need to remove \",\", \"_\", and \"-\". \n# This also highlights a few issues on p. 185 that need to be manually adjusted\n# https://twitter.com/RohanAlexander/status/1244337583016022018\nall_tables$male[all_tables$male == \"23-Jun\"] <- 4923\nall_tables$male[all_tables$male == \"15-Aug\"] <- 4611\nall_tables <-\n  all_tables %>%\n  mutate_at(vars(male, female, total), ~str_remove_all(., \",\")) %>% # First get rid of commas\n  mutate_at(vars(male, female, total), ~str_replace(., \"_\", \"0\")) %>%\n  mutate_at(vars(male, female, total), ~str_replace(., \"-\", \"0\")) %>%\n  mutate_at(vars(male, female, total), ~as.integer(.))"},{"path":"cleaning-and-preparing-data.html","id":"areas","chapter":"11 Cleaning and preparing data","heading":"11.3.3.2 Areas","text":"next thing clean areas. know 47 counties Kenya, whole bunch sub-counties. give us list pages 19 22 PDF (document pages 7 10). However, list complete, minor issues ’ll deal later.case, first need fix inconsistencies.Kenya 47 counties, sub-counties. PDF arranged county data sub-counties, without designating . can use names, certain extent, handful cases, sub-county name county need first fix .PDF made-three tables.can first get names counties based first two tables reconcile get list counties.hoped, 47 . can add flag based names, need deal sub-counties share name. based page, looking deciding county page sub-county page.Now can add flag whether area county adjust ones troublesome,","code":"\n# Fix some area names\nall_tables$area[all_tables$area == \"Taita/ Taveta\"] <- \"Taita/Taveta\"\nall_tables$area[all_tables$area == \"Elgeyo/ Marakwet\"] <- \"Elgeyo/Marakwet\"\nall_tables$area[all_tables$area == \"Nairobi City\"] <- \"Nairobi\"\nall_tables$table %>% table()\n#> .\n#>       \n#> 59185\n# Get a list of the counties \nlist_counties <- \n  all_tables %>% \n  filter(table %in% c(\"Table 2.4a: Distribution of Rural Population by Age, Sex* and County\",\n                      \"Table 2.4b: Distribution of Urban Population by Age, Sex* and County\")\n         ) %>% \n  select(area) %>% \n  distinct()\n# The following have the issue of the name being used for both a county and a sub-county:\nall_tables %>% \n  filter(table == \"Table 2.3: Distribution of Population by Age, Sex*, County and Sub- County\") %>% \n  filter(area %in% c(\"Busia\",\n                     \"Garissa\",\n                     \"Homa Bay\",\n                     \"Isiolo\",\n                     \"Kiambu\",\n                     \"Machakos\",\n                     \"Makueni\",\n                     \"Samburu\",\n                     \"Siaya\",\n                     \"Tana River\",\n                     \"Vihiga\",\n                     \"West Pokot\")\n         ) %>% \n  select(area, page) %>% \n  distinct()\n#> # A tibble: 0 × 2\n#> # … with 2 variables: area <chr>, page <int>\n# Add flag for whether it is a county or a sub-county\nall_tables <- \n  all_tables %>% \n  mutate(area_type = if_else(area %in% list_counties$area, \"county\", \"sub-county\"))\n# Fix the flag for the ones that have their names used twice\nall_tables <- \n  all_tables %>% \n  mutate(area_type = case_when(\n    area == \"Samburu\" & page == 42 ~ \"sub-county\",\n    area == \"Tana River\" & page == 56 ~ \"sub-county\",\n    area == \"Garissa\" & page == 69 ~ \"sub-county\",\n    area == \"Isiolo\" & page == 100 ~ \"sub-county\",\n    area == \"Machakos\" & page == 154 ~ \"sub-county\",\n    area == \"Makueni\" & page == 164 ~ \"sub-county\",\n    area == \"Kiambu\" & page == 213 ~ \"sub-county\",\n    area == \"West Pokot\" & page == 233 ~ \"sub-county\",\n    area == \"Vihiga\" & page == 333 ~ \"sub-county\",\n    area == \"Busia\" & page == 353 ~ \"sub-county\",\n    area == \"Siaya\" & page == 360 ~ \"sub-county\",\n    area == \"Homa Bay\" & page == 375 ~ \"sub-county\",\n    TRUE ~ area_type\n    )\n  )\n\nrm(list_counties)"},{"path":"cleaning-and-preparing-data.html","id":"ages","chapter":"11 Cleaning and preparing data","heading":"11.3.3.3 Ages","text":"Now can deal ages.First need fix errors.census done work putting together age-groups us, want make easy just focus counts single-year-age. ’ll add flag type age : age group, ages 0 5, single age, 1.moment, age character variable. decision make , don’t want character variable (won’t graph properly), don’t want numeric, total also 100+ . now, ’ll just make factor, least able nicely graphed.","code":"\n# Clean up ages\ntable(all_tables$age) %>% head()\n#> \n#>           0   0-4     1    10 10-14 \n#>   392   484   484   484   484   482\nunique(all_tables$age) %>% head()\n#> [1] \"\"        \"Table\"   \"MOMBASA\" \"Age\"     \"Total\"   \"0\"\n# Looks like there should be 484, so need to follow up on some:\nall_tables$age[all_tables$age == \"NotStated\"] <- \"Not Stated\"\nall_tables$age[all_tables$age == \"43594\"] <- \"5-9\"\nall_tables$age[all_tables$age == \"43752\"] <- \"10-14\"\nall_tables$age[all_tables$age == \"9-14\"] <- \"5-9\"\nall_tables$age[all_tables$age == \"10-19\"] <- \"10-14\"\n# Add a flag as to whether it's a summary or not\nall_tables$age_type <- if_else(str_detect(all_tables$age, c(\"-\")), \"age-group\", \"single-year\")\nall_tables$age_type <- if_else(str_detect(all_tables$age, c(\"Total\")), \"age-group\", all_tables$age_type)\nall_tables$age <- as_factor(all_tables$age)"},{"path":"cleaning-and-preparing-data.html","id":"check","chapter":"11 Cleaning and preparing data","heading":"11.3.4 Check","text":"","code":""},{"path":"cleaning-and-preparing-data.html","id":"gender-sum","chapter":"11 Cleaning and preparing data","heading":"11.3.4.1 Gender sum","text":"Given format data, point easy check total sum male female.just one seems wrong.","code":"\n# Check the parts and the sums\nfollow_up <- \n  all_tables %>% \n  mutate(check_sum = male + female,\n         totals_match = if_else(total == check_sum, 1, 0)\n         ) %>% \n  filter(totals_match == 0)\n# There is just one that looks wrong\nall_tables$male[all_tables$age == \"10\" & all_tables$page == 187] <- as.integer(1)\n\nrm(follow_up)"},{"path":"cleaning-and-preparing-data.html","id":"rural-urban-split","chapter":"11 Cleaning and preparing data","heading":"11.3.4.2 Rural-urban split","text":"census provides different tables total county sub-county; within county, number urban area county, number urban area county. counties urban count, ’d like make sure sum rural urban counts equals total count. requires reshaping data long wide format.First, construct different tables three. just manually, probably nicer way.constructed constituent parts, now join based age, area, whether county.can now check sum rural urban total.just , difference 1, ’ll just move .","code":"\n# Table 2.3\ntable_2_3 <- all_tables %>% \n  filter(table == \"Table 2.3: Distribution of Population by Age, Sex*, County and Sub- County\")\ntable_2_4a <- all_tables %>% \n  filter(table == \"Table 2.4a: Distribution of Rural Population by Age, Sex* and County\")\ntable_2_4b <- all_tables %>% \n  filter(table == \"Table 2.4b: Distribution of Urban Population by Age, Sex* and County\")\nboth_2_4s <- full_join(table_2_4a, table_2_4b, by = c(\"age\", \"area\", \"area_type\"), suffix = c(\"_rural\", \"_urban\"))\n\nall <- full_join(table_2_3, both_2_4s, by = c(\"age\", \"area\", \"area_type\"), suffix = c(\"_all\", \"_\"))\n\nall <- \n  all %>% \n  mutate(page = glue::glue('Total from p. {page}, rural from p. {page_rural}, urban from p. {page_urban}')) %>% \n  select(-page, -page_rural, -page_urban,\n         -table, -table_rural, -table_urban,\n         -age_type_rural, -age_type_urban\n         )\n\nrm(both_2_4s, table_2_3, table_2_4a, table_2_4b)\n# Check that the urban + rural = total\nfollow_up <- \n  all %>% \n  mutate(total_from_bits = total_rural + total_urban,\n         check_total_is_rural_plus_urban = if_else(total == total_from_bits, 1, 0),\n         total_from_bits - total) %>% \n  filter(check_total_is_rural_plus_urban == 0)\n\nhead(follow_up)\n#> # A tibble: 0 × 16\n#> # … with 16 variables: age <fct>, male <int>, female <int>, total <int>,\n#> #   area <chr>, area_type <chr>, age_type <chr>, male_rural <int>,\n#> #   female_rural <int>, total_rural <int>, male_urban <int>,\n#> #   female_urban <int>, total_urban <int>, total_from_bits <int>,\n#> #   check_total_is_rural_plus_urban <dbl>, total_from_bits - total <int>\nrm(follow_up)"},{"path":"cleaning-and-preparing-data.html","id":"ages-sum-to-age-groups","chapter":"11 Cleaning and preparing data","heading":"11.3.4.3 Ages sum to age-groups","text":"Finally, want check single age counts sum age-groups.Mt. Kenya Forest, Aberdare Forest, Kakamega Forest slightly dodgy. can’t see documentation, looks like apportioned various countries. ’s understandable ’d ’s probably big deal, ’ll just move .","code":"\n# One last thing to check is that the ages sum to their age-groups.\nfollow_up <- \n  all %>% \n  mutate(groups = case_when(age %in% c(\"0\", \"1\", \"2\", \"3\", \"4\", \"0-4\") ~ \"0-4\",\n                            age %in% c(\"5\", \"6\", \"7\", \"8\", \"9\", \"5-9\") ~ \"5-9\",\n                            age %in% c(\"10\", \"11\", \"12\", \"13\", \"14\", \"10-14\") ~ \"10-14\",\n                            age %in% c(\"15\", \"16\", \"17\", \"18\", \"19\", \"15-19\") ~ \"15-19\",\n                            age %in% c(\"20\", \"21\", \"22\", \"23\", \"24\", \"20-24\") ~ \"20-24\",\n                            age %in% c(\"25\", \"26\", \"27\", \"28\", \"29\", \"25-29\") ~ \"25-29\",\n                            age %in% c(\"30\", \"31\", \"32\", \"33\", \"34\", \"30-34\") ~ \"30-34\",\n                            age %in% c(\"35\", \"36\", \"37\", \"38\", \"39\", \"35-39\") ~ \"35-39\",\n                            age %in% c(\"40\", \"41\", \"42\", \"43\", \"44\", \"40-44\") ~ \"40-44\",\n                            age %in% c(\"45\", \"46\", \"47\", \"48\", \"49\", \"45-49\") ~ \"45-49\",\n                            age %in% c(\"50\", \"51\", \"52\", \"53\", \"54\", \"50-54\") ~ \"50-54\",\n                            age %in% c(\"55\", \"56\", \"57\", \"58\", \"59\", \"55-59\") ~ \"55-59\",\n                            age %in% c(\"60\", \"61\", \"62\", \"63\", \"64\", \"60-64\") ~ \"60-64\",\n                            age %in% c(\"65\", \"66\", \"67\", \"68\", \"69\", \"65-69\") ~ \"65-69\",\n                            age %in% c(\"70\", \"71\", \"72\", \"73\", \"74\", \"70-74\") ~ \"70-74\",\n                            age %in% c(\"75\", \"76\", \"77\", \"78\", \"79\", \"75-79\") ~ \"75-79\",\n                            age %in% c(\"80\", \"81\", \"82\", \"83\", \"84\", \"80-84\") ~ \"80-84\",\n                            age %in% c(\"85\", \"86\", \"87\", \"88\", \"89\", \"85-89\") ~ \"85-89\",\n                            age %in% c(\"90\", \"91\", \"92\", \"93\", \"94\", \"90-94\") ~ \"90-94\",\n                            age %in% c(\"95\", \"96\", \"97\", \"98\", \"99\", \"95-99\") ~ \"95-99\",\n                            TRUE ~ \"Other\")\n         ) %>% \n  group_by(area_type, area, groups) %>% \n  mutate(group_sum = sum(total, na.rm = FALSE),\n         group_sum = group_sum / 2,\n         difference = total - group_sum) %>% \n  ungroup() %>% \n  filter(age == groups) %>% \n  filter(total != group_sum) \n\nhead(follow_up)\n#> # A tibble: 0 × 16\n#> # … with 16 variables: age <fct>, male <int>, female <int>, total <int>,\n#> #   area <chr>, area_type <chr>, age_type <chr>, male_rural <int>,\n#> #   female_rural <int>, total_rural <int>, male_urban <int>,\n#> #   female_urban <int>, total_urban <int>, groups <chr>, group_sum <dbl>,\n#> #   difference <dbl>\n\nrm(follow_up)"},{"path":"cleaning-and-preparing-data.html","id":"tidy-up","chapter":"11 Cleaning and preparing data","heading":"11.3.5 Tidy-up","text":"Now confident everything looking good, can just convert long-format easy work .","code":"\nall <- \n  all %>% \n  rename(male_total = male,\n         female_total = female,\n         total_total = total) %>% \n  pivot_longer(cols = c(male_total, female_total, total_total, male_rural, female_rural, total_rural, male_urban, female_urban, total_urban),\n               names_to = \"type\",\n               values_to = \"number\"\n               ) %>% \n  separate(col = type, into = c(\"gender\", \"part_of_area\"), sep = \"_\") %>% \n  select(area, area_type, part_of_area, age, age_type, gender, number)\n\nwrite_csv(all, path = \"outputs/data/cleaned_kenya_2019_census.csv\")\n\nhead(all)\n#> # A tibble: 0 × 7\n#> # … with 7 variables: area <chr>, area_type <chr>, part_of_area <chr>,\n#> #   age <fct>, age_type <chr>, gender <chr>, number <int>"},{"path":"cleaning-and-preparing-data.html","id":"make-monicas-dataset","chapter":"11 Cleaning and preparing data","heading":"11.3.6 Make Monica’s dataset","text":"original purpose make table Monica. needed single-year counts, gender, counties.’ll leave fancy stats Monica, ’ll just make quick graph Nairobi.","code":"\nmonicas_dataset <- \n  all %>% \n  filter(area_type == \"county\") %>% \n  filter(part_of_area == \"total\") %>%\n  filter(age_type == \"single-year\") %>% \n  select(area, age, gender, number)\n\nhead(monicas_dataset)\n#> # A tibble: 0 × 4\n#> # … with 4 variables: area <chr>, age <fct>, gender <chr>, number <int>\nwrite_csv(monicas_dataset, \"outputs/data/monicas_dataset.csv\")\n# monicas_dataset %>% \n#   filter(area == \"Nairobi\") %>%\n#   ggplot() +\n#   geom_col(aes(x = age, y = number, fill = gender), position = \"dodge\") + \n#   scale_y_continuous(labels = scales::comma) +\n#   scale_x_discrete(breaks = c(seq(from = 0, to = 99, by = 5), \"100+\")) +\n#   theme_classic()+\n#   scale_fill_brewer(palette = \"Set1\") +\n#   labs(y = \"Number\",\n#        x = \"Age\",\n#        fill = \"Gender\",\n#        title = \"Distribution of age and gender in Nairobi in 2019\",\n#        caption = \"Data source: 2019 Kenya Census\")"},{"path":"cleaning-and-preparing-data.html","id":"checks-and-tests","chapter":"11 Cleaning and preparing data","heading":"11.4 Checks and tests","text":"Robert Caro, biographer Lyndon Johnson, spent years tracking everyone connected 36th President United States. went far live Texas Hill Country X years better understand LBJ . heard story LBJ used run Senate worked Y, ran route multiple times try understand LBJ running. Caro eventually understood ran route sun rising, just LBJ done, found time sun hits Senate Rotunda looks amazing (CITE). background work enabled uncover aspects one else knew. instance, turns LBJ almost surely stole first election win Texas Senator. need understand data extent. Turn every page go every extreme.cleaning data, looking anomalies. interested values , also opposite situation—values missing . ’ve talked fairly generally checks, tests considerations. ’d like specific mean. four tools use identify situations: plots, counts, green/red conditions, targets.","code":""},{"path":"cleaning-and-preparing-data.html","id":"plots","chapter":"11 Cleaning and preparing data","heading":"11.4.1 Plots","text":"Plots invaluable tool cleaning data, show point dataset, relation points. especially useful identifying value doesn’t belong. instance, value expected numerical, still character plot warning displayed.Plots especially useful numerical data, still useful text categorical data. Let’s pretend situation interested person’s age, youth survey. following data:graph clearly shows unexpected value 150. likely explanation data incorrectly entered trailing 0, 15.can fix , document , redo graph, see everything seems reasonable now.","code":"\nraw_data <- \n  tibble(ages = c(11, 17, 22, 13, 21, 16, 16, 6, 16, 11, 150))\n\nraw_data %>% \n  ggplot(aes(y = ages, x = 0)) +\n  geom_point()"},{"path":"cleaning-and-preparing-data.html","id":"counts","chapter":"11 Cleaning and preparing data","heading":"11.4.2 Counts","text":"want focus getting data right. interested counts unique values. Hopefully majority data concentrated common counts. can also useful invert , see especially uncommon. extent want deal depends need. Ultimately, time fix one getting additional observations, potentially even just one! Counts especially useful text categorical data, can helpful numerical well.Let’s see example.use count clearly identifies spend time - changing ‘Australie’ ‘Australia’ almost double amount useable data.","code":"\nraw_data <- \n  tibble(country = c('Australie', 'Austrelia', 'Australie', 'Australie', 'Aeustralia', 'Austraia', 'Australia', 'Australia', 'Australia', 'Australia'\n                  )\n         )\n\nraw_data %>% \n  count(country, sort = TRUE)\n#> # A tibble: 5 × 2\n#>   country        n\n#>   <chr>      <int>\n#> 1 Australia      4\n#> 2 Australie      3\n#> 3 Aeustralia     1\n#> 4 Austraia       1\n#> 5 Austrelia      1"},{"path":"cleaning-and-preparing-data.html","id":"gono-go","chapter":"11 Cleaning and preparing data","heading":"11.4.3 Go/no-go","text":"things important require cleaned dataset . go/-go conditions. typically come experience, expert knowledge, planning simulation exercises. example may negative numbers age column, ages 140.specifically require condition met. examples include:cross-country analysis, list country names know dataset useful. -go conditions : 1) values list dataset, , vice versa; 2) countries expected .concrete example, let’s consider analysis five largest counties Kenya: ‘Nairobi,’ ‘Kiambu,’ ‘Nakuru,’ ‘Kakamega,’ ‘Bungoma.’ Let’s create array first:begin following dataset:Based count know fix two numbers obvious fixes:point can use go/-go conditions decide whether finished .clear still cleaning !may also find similar conditions experts experience particular field.","code":"\ncorrect_counties <- c('Nairobi', 'Kiambu', 'Nakuru', 'Kakamega', 'Bungoma')\ntop_five_kenya <- \n  tibble(county = c('Nairobi', 'Nairob1', 'Nakuru', 'Kakamega', 'Nakuru', \n                      'Kiambu', 'Kiambru', 'Kabamega', 'Bun8oma', 'Bungoma')\n  )\n\ntop_five_kenya %>% \n  count(county, sort = TRUE)\n#> # A tibble: 9 × 2\n#>   county       n\n#>   <chr>    <int>\n#> 1 Nakuru       2\n#> 2 Bun8oma      1\n#> 3 Bungoma      1\n#> 4 Kabamega     1\n#> 5 Kakamega     1\n#> 6 Kiambru      1\n#> 7 Kiambu       1\n#> 8 Nairob1      1\n#> 9 Nairobi      1\ntop_five_kenya <- \n  top_five_kenya %>% \n  mutate(county = str_replace_all(county, 'Nairob1', 'Nairobi'),\n         county = str_replace_all(county, 'Bun8oma', 'Nairobi')\n  )\n\ntop_five_kenya %>% \n  count(county, sort = TRUE)\n#> # A tibble: 7 × 2\n#>   county       n\n#>   <chr>    <int>\n#> 1 Nairobi      3\n#> 2 Nakuru       2\n#> 3 Bungoma      1\n#> 4 Kabamega     1\n#> 5 Kakamega     1\n#> 6 Kiambru      1\n#> 7 Kiambu       1\ntop_five_kenya$county %>% unique()\n#> [1] \"Nairobi\"  \"Nakuru\"   \"Kakamega\" \"Kiambu\"   \"Kiambru\"  \"Kabamega\" \"Bungoma\"\n\nif(all(top_five_kenya$county %>% unique() == top_five_kenya)) {\n  \"Oh no\"\n}\nif(all(top_five_kenya==top_five_kenya$county %>% unique()) ) {\n  \"Oh no\"\n}"},{"path":"cleaning-and-preparing-data.html","id":"class-1","chapter":"11 Cleaning and preparing data","heading":"11.4.4 Class","text":"can’t emphasize enough, vital put place explicit checks class getting wrong can large effect analysis. particular:check whether value number factor.check dates correctly formatted.understand important clear whether value number factor, consider following situation:Let’s start group integer look logistic regression.Now can try factor. intepretation variable completely different.Another critical aspect check dates. particular want try make following format: YYYY-MM-DD. course differences opinion appropriate date format broader world, reasonable people can differ whether 1 July 2010 July 1, 2020, better, YYYY-MM-DD format generally appropriate data.","code":"\nsome_data <- \n  tibble(response = c(1, 1, 0, 1, 0, 1, 1, 0, 0),\n         group = c(1, 2, 1, 1, 2, 3, 1, 2, 3))\nsome_data %>% \n  mutate(group = as.integer(group)) %>% \n  lm(response~group, data = .) %>% \n  summary()\n#> \n#> Call:\n#> lm(formula = response ~ group, data = .)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#>  -0.68  -0.52   0.32   0.32   0.64 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)   0.8400     0.4495   1.869    0.104\n#> group        -0.1600     0.2313  -0.692    0.511\n#> \n#> Residual standard error: 0.5451 on 7 degrees of freedom\n#> Multiple R-squared:  0.064,  Adjusted R-squared:  -0.06971 \n#> F-statistic: 0.4786 on 1 and 7 DF,  p-value: 0.5113\nsome_data %>% \n  mutate(group = as.factor(group)) %>% \n  lm(response~group, data = .) %>% \n  summary()\n#> \n#> Call:\n#> lm(formula = response ~ group, data = .)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -0.7500 -0.3333  0.2500  0.2500  0.6667 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)   0.7500     0.2826   2.654   0.0378 *\n#> group2       -0.4167     0.4317  -0.965   0.3717  \n#> group3       -0.2500     0.4895  -0.511   0.6278  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.5652 on 6 degrees of freedom\n#> Multiple R-squared:  0.1375, Adjusted R-squared:  -0.15 \n#> F-statistic: 0.4783 on 2 and 6 DF,  p-value: 0.6416"},{"path":"cleaning-and-preparing-data.html","id":"naming-things","chapter":"11 Cleaning and preparing data","heading":"11.5 Naming things","text":"improved scanning software developed identified gene name errors 30.9% (3,436/11,117) articles supplementary Excel gene lists; figure significantly higher previously estimated. due gene names converted just dates floating-point numbers, also internal date format (five-digit numbers).Abeysooriya (2021)https://neverworkintheory.org/2021/08/09/abbreviated-vs-full-names.htmlNames matter. wrote book land today named Toronto, within country named Canada, long time known Turtle Island. common, days people sometimes still refer Turtle Island. tells us something , use name Canada tells something us. big rock centre country ’m , Australia. long time, called Uluru, known Ayers Rock. Today dual name combines , choice name use tells someone something . Even British Royal Family recognise power names. 1917 changed House Saxe-Coburg Gotha House Windsor, due feeling former Germanic given World War ongoing. Names matter everyday life. matter data science .importance names, ignoring existing claims re-naming clear cases, see data science well. need careful name datasets, variables, functions. tendency, days, call variable ‘gender’ even though may male female, want say word ‘sex.’ (Tukey 1962) essentially defines today call data science, popularised folks computer science 2010s ignored, either deliberately ignorance, came . past ten years characteristic renaming concepts well-established fields computer science recently expanded . instance, use binary variables regression, sometimes called ‘dummy variables,’ called one-hot encoding computer science. Like fashions, one pass also. recently saw 1980s early 2010s economics. Economists described ‘queen social sciences’ self-described imperialistic (Lazear 2000). now recognising costs imperialism social sciences, future look back count cost computer science imperialism data science. key area study ever terra nullius, nobody’s land. important recognise, adopt, use existing names, practices.Names give places meaning, ignoring existing names, ignore come us. Kimmerer (2012, 34) describes ‘Tahawus Algonquin name Mount Marcy, highest peak Adirondacks. ’s called Mount March commemorate governor never set foot wild slopes.’ continues ‘[w]hen call place name transformed wilderness homeland.’ talking regard physical places, true function names, variable names dataset names. use gender instead sex don’t want say sex front others, ignore preferences provided data.addition respecting nature data, names need satisfy two additional considerations: 1) need machine readable, 2) need human readable.Machine readable names easier standard meet, usually means avoiding spaces special characters. space can replaced underbar. Usually, special characters just removed can inconsistent different computers languages. names also unique within dataset, unique within collection datasets unless particular column deliberately used key join different datasets.especially useful function use get closer machine readable names janitor::clean_names() janitor package (Firke 2020). deals issues mentioned well others. can see example.Human readable names require additional layer! need consider cultures may interpret names ’re using. also need consider different experience levels subsequent users dataset may . terms experience programming statistics, also experience similar datasets. instance, column ‘flag’ often used signal column contains data needs followed treated carefully way. experienced analyst know , beginner . Try use meaningful names wherever possible (Lin, Ali, Wilson 2020).One interesting feature R certain cases partial matching names possible. instance:behaviour possible within tidyverse (instance data.frame replaced tibble code) recommend never using feature. makes difficult understand code break, others come fresh.","code":"\nbad_names_good_names <- \n  tibble(\n    'First' = c(1),\n    'second name has spaces' = c(1),\n    'weird#symbol' = c(1),\n    'InCoNsIsTaNtCaPs' = c(1)\n  )\n\nbad_names_good_names\n#> # A tibble: 1 × 4\n#>   First `second name has spaces` `weird#symbol` InCoNsIsTaNtCaPs\n#>   <dbl>                    <dbl>          <dbl>            <dbl>\n#> 1     1                        1              1                1\n\nbad_names_good_names <- \n  bad_names_good_names %>% \n  janitor::clean_names()\n  \nbad_names_good_names\n#> # A tibble: 1 × 4\n#>   first second_name_has_spaces weird_number_symbol in_co_ns_is_ta_nt_ca_ps\n#>   <dbl>                  <dbl>               <dbl>                   <dbl>\n#> 1     1                      1                   1                       1\nnever_use_partial_matching <- \n  data.frame(\n    my_first_name = c(1, 2),\n    another_name = c(\"wow\", \"great\")\n  )\n\nnever_use_partial_matching$my_first_name\n#> [1] 1 2\nnever_use_partial_matching$my\n#> [1] 1 2"},{"path":"cleaning-and-preparing-data.html","id":"exercises-and-tutorial-10","chapter":"11 Cleaning and preparing data","heading":"11.6 Exercises and tutorial","text":"","code":""},{"path":"cleaning-and-preparing-data.html","id":"exercises-10","chapter":"11 Cleaning and preparing data","heading":"11.6.1 Exercises","text":"following example tidy data?dealing ages likely class variable? [Select apply.]\ninteger\nmatrix\nnumeric\nfactor\nintegermatrixnumericfactor","code":"\ntibble(name = c('Anne', 'Bethany', 'Stephen', 'William'),\n       age_group = c('18-29', '30-44', '45-60', '60+'),\n       )\n#> # A tibble: 4 × 2\n#>   name    age_group\n#>   <chr>   <chr>    \n#> 1 Anne    18-29    \n#> 2 Bethany 30-44    \n#> 3 Stephen 45-60    \n#> 4 William 60+"},{"path":"cleaning-and-preparing-data.html","id":"tutorial-10","chapter":"11 Cleaning and preparing data","heading":"11.6.2 Tutorial","text":"regard Jordan (2019), D’Ignazio Klein (2020), Chapter 6, Au (2020), relevant work, extent think let data speak ? [Please write page two.]","code":""},{"path":"storing-and-retrieving-data.html","id":"storing-and-retrieving-data","chapter":"12 Storing and retrieving data","heading":"12 Storing and retrieving data","text":"STATUS: construction.Required readingRecommended readingKey concepts/skills/etcKey librariesKey functions/etc","code":""},{"path":"storing-and-retrieving-data.html","id":"introduction-14","chapter":"12 Storing and retrieving data","heading":"12.1 Introduction","text":"’ve put together dataset, important part responsible storing appropriately enabling easy retrieval. certainly possible especially concerned , entire careers based storage retrieval data, certain extent, baseline onerous. can get computer half-way ! Confirming someone else can retrieve use , puts much .said, FAIR principles especially useful formal data management. (Wilkinson et al. 2016):Findable. means one, unchanging, identifier dataset dataset high-quality descriptions explanations.Accessible.Interoperable.Reusable.’s important recognise just dataset FAIR, necessarily unbiased representation world.Oh, think good data ! One representation reality commonplace chess. chess board (see Figure X - add photo chess board) 8 x 8 board alternating black white squares. squares denonated unique combination letter (-G) number (1-8). piece unique abbreviation, instance pawns X, knights Y. game recorded player noting move. way entire game can recreated. 2021 World Championship contested Magnus Carlsen Ian Nepomniachtchi. Figure X shows score sheet Game 6. variety reasons game particularly noteworthy, one uncharactertic mistakes Carlsen Nepomniachtchi made. instance, Move 32 Carlsen exploit opportunity; Move 36 different move provided Nepomniachtchi promising endgame (CITATION). One reason may players point game little time remaining—decide moves quickly. sense representation provided game sheet. ‘correct’ representation happened game, necessarily happened.","code":""},{"path":"storing-and-retrieving-data.html","id":"plan-3","chapter":"12 Storing and retrieving data","heading":"12.2 Plan","text":"Michener (2015)Information Science librariesHart et al. (2016)","code":""},{"path":"storing-and-retrieving-data.html","id":"r-packages-for-data","chapter":"12 Storing and retrieving data","heading":"12.3 R Packages for data","text":"","code":""},{"path":"storing-and-retrieving-data.html","id":"documentation","chapter":"12 Storing and retrieving data","heading":"12.4 Documentation","text":"Datasheets (Gebru et al. 2020) increasingly critical aspect data science. Datasheets basically nutrition labels datasets. process creating enables us think carefully feed model. importantly, enable others better understand fed model. Recently researchers went back wrote datasheet one popular datasets computer science, found around 30 per cent data duplicated (Bandy Vincent 2021).Instead telling unhealthy various foods , datasheet tells things like:‘created dataset behalf entity?’‘funded creation dataset?’dataset contain possible instances sample (necessarily random) instances larger set?’‘information missing individual instances?’done lot work create dataset analyze, may make sense try publish share . typically datasheet might live appendix main work.","code":""},{"path":"storing-and-retrieving-data.html","id":"exercises-and-tutorial-11","chapter":"12 Storing and retrieving data","heading":"12.5 Exercises and tutorial","text":"","code":""},{"path":"storing-and-retrieving-data.html","id":"exercises-11","chapter":"12 Storing and retrieving data","heading":"12.5.1 Exercises","text":"According Gebru et al. (2020, 2), datasheet document dataset’s (please select apply):\ncomposition.\nrecommended uses.\nmotivation.\ncollection process.\ncomposition.recommended uses.motivation.collection process.Following Wilkinson et al. (2016), following FAIR principles (please select apply)?\nFindable.\nApproachable.\nInteroperable.\nReusable.\nIntegrated.\nFungible.\nReduced.\nAccessible.\nFindable.Approachable.Interoperable.Reusable.Integrated.Fungible.Reduced.Accessible.","code":""},{"path":"storing-and-retrieving-data.html","id":"tutorial-11","chapter":"12 Storing and retrieving data","heading":"12.5.2 Tutorial","text":"Look IQ tests conducted goes . extent think measure intelligence? aspects may like think answering question include: decides intelligence? updated? missing definition? extent generalisable? write page two.","code":""},{"path":"disseminating-and-protecting-data.html","id":"disseminating-and-protecting-data","chapter":"13 Disseminating and protecting data","heading":"13 Disseminating and protecting data","text":"STATUS: construction.Hawes, M. B. (2020). Implementing Differential Privacy: Seven Lessons 2020 United States Census. Harvard Data Science Review. https://doi.org/10.1162/99608f92.353c6f99Hawes, M. B. (2020). Implementing Differential Privacy: Seven Lessons 2020 United States Census. Harvard Data Science Review. https://doi.org/10.1162/99608f92.353c6f99https://hdsr.mitpress.mit.edu/pub/g9o4z8au/release/2https://hdsr.mitpress.mit.edu/pub/g9o4z8au/release/2https://www.census.gov/newsroom/blogs/research-matters/2020/02/census_bureau_works.htmlhttps://www.census.gov/newsroom/blogs/research-matters/2020/02/census_bureau_works.htmlRequired readingRecommended readingKey concepts/skills/etcKey librariesKey functions/etc","code":""},{"path":"disseminating-and-protecting-data.html","id":"introduction-15","chapter":"13 Disseminating and protecting data","heading":"13.1 Introduction","text":"","code":""},{"path":"disseminating-and-protecting-data.html","id":"what-is-personally-identifying-information","chapter":"13 Disseminating and protecting data","heading":"13.2 What is personally identifying information","text":"Zook et al. (2017)","code":""},{"path":"disseminating-and-protecting-data.html","id":"hashing-and-salting","chapter":"13 Disseminating and protecting data","heading":"13.3 Hashing and salting","text":"","code":""},{"path":"disseminating-and-protecting-data.html","id":"gdpr-and-hipaa","chapter":"13 Disseminating and protecting data","heading":"13.4 GDPR and HIPAA","text":"","code":""},{"path":"disseminating-and-protecting-data.html","id":"making-fake-data-to-distribute-when-you-cant-share-the-real-stuff","chapter":"13 Disseminating and protecting data","heading":"13.5 Making fake data to distribute when you can’t share the real stuff","text":"","code":""},{"path":"disseminating-and-protecting-data.html","id":"differential-privacy","chapter":"13 Disseminating and protecting data","heading":"13.6 Differential privacy","text":"Kenny et al. (2021)Ruggles et al. (2019)Suriyakumar et al. (2020)","code":""},{"path":"disseminating-and-protecting-data.html","id":"exercises-and-tutorial-12","chapter":"13 Disseminating and protecting data","heading":"13.7 Exercises and tutorial","text":"","code":""},{"path":"disseminating-and-protecting-data.html","id":"exercises-12","chapter":"13 Disseminating and protecting data","heading":"13.7.1 Exercises","text":"","code":""},{"path":"disseminating-and-protecting-data.html","id":"tutorial-12","chapter":"13 Disseminating and protecting data","heading":"13.7.2 Tutorial","text":"","code":""},{"path":"exploratory-data-analysis.html","id":"exploratory-data-analysis","chapter":"14 Exploratory data analysis","heading":"14 Exploratory data analysis","text":"STATUS: construction.Required readingBarocas, Solon, Danah Boyd, 2017, ‘Engaging ethics data science practice,’ Communications ACM, 60.11 (2017): 23-25.DiCiccio, Thomas J., Mary E. Thompson, 2004, ‘Conversation Donald . S. Fraser,’ Statistical Science, 19 (2) pp. 370-386, https://utstat.toronto.edu/craiu/DonFraser_SSInterview.pdf.Jordan, Michael , 2019, ‘AI - revolution hasn’t started yet,’ Harvard Data Science Review, 1 July, https://hdsr.mitpress.mit.edu/pub/wot7mkc1.Tukey, John W., 1961, ‘Future Data Analysis,’ annals mathematical statistics, Part 1 ‘General Considerations,’ https://projecteuclid.org/journals/annals--mathematical-statistics/volume-33/issue-1/-Future--Data-Analysis/10.1214/aoms/1177704711.full.Wickham, Hadley, Garrett Grolemund, 2017, R Data Science, Chapters 3 7, https://r4ds..co.nz/.Recommended readingHall, Megan, 2019, ‘Exploratory Data Analysis Using Tidyverse,’ https://hockey-graphs.com/2019/10/08/exploratory-data-analysis-using-tidyverse/.Kommenda, Niko, Helen Pidd Libby Brooks, 2020, ‘Revealed: areas UK one Airbnb every four homes,’ Guardian, 20 February, https://www.theguardian.com/technology/2020/feb/20/revealed--areas---uk--one-airbnb--every-four-homes.Silge, Julia, 2018, ‘Understanding PCA using Stack Overflow data,’ https://juliasilge.com/blog/stack-overflow-pca/.Soetewey, Antoine, 2020, ‘Descriptive statistics R,’ https://www.statsandr.com/blog/descriptive-statistics--r/.Stodulka, Jiri, 2019, ‘Toronto Crime Folium,’ https://www.jiristodulka.com/post/toronto-crime/.Wong, Julia Carrie, 2020, ‘One year inside Trump’s monumental Facebook campaign,’ Guardian, 29 January, https://www.theguardian.com/us-news/2020/jan/28/donald-trump-facebook-ad-campaign-2020-election.Key concepts/skills/etcQuickly coming terms new dataset constructing graphs tables.Understanding issues features dataset may affect modelling decisions.Thinking missing values outliers.Key librariesbroomggrepelherejanitorlubridateopendatatorontotidymodelstidyversevisdatKey functions/etcaugment()clean_names()coord_flip()count()distinct()facet_grid()facet_wrap()geom_bar()geom_col()geom_density()geom_histogram()geom_line()geom_point()geom_smooth()geom_text_repel()get_dupes()glance()if_else()ifelse()initial_split()left_join()mutate()mutate_all()names()ncol()nrow()pivot_wider()scale_color_brewer()scale_fill_brewer()scale_x_log10()scale_y_log10()str_detect()str_extract()str_remove()str_split()str_starts()summarise()summarise_all()theme_classic()theme_minimal()vis_dat()vis_miss()QuizIn words exploratory data analysis (difficult, please write one nuanced paragraph)?Tukey’s words, exploratory data analysis (please write one paragraph)?Tukey (please write paragraph two)?Tukey’s link DoSS (hint: advisor someone’s PhD - person)?Can identify female equivalent Tukey (historians statistics) may overlooked?dataset called ‘my_data,’ two columns: ‘first_col’ ‘second_col,’ please write rough R code generate graph (type graph doesn’t matter).Consider dataset 500 rows 3 columns, 1,500 cells. 100 cells missing data least one columns, remove whole row dataset try run analysis data , procedure? dataset 10,000 rows instead, number missing cells?Please note three ways identifying unusual values.difference categorical continuous variable?difference factor integer variable?can think systematically excluded dataset?Using opendatatoronto package, download data mayoral campaign contributions 2014. (note: 2014 file get get_resource, just keep sheet relates Mayor election).\nClean data format (fixing parsing issue standardizing column names using janitor)\nSummarize variables dataset. missing values, , worried ? every variable format ? , create new variable(s) right format.\nVisually explore distribution values contributions. contributions notable outliers? share similar characteristic(s)? may useful plot distribution contributions without outliers get better sense majority data.\nList top five candidates categories: 1) total contributions; 2) mean contribution; 3) number contributions.\nRepeat process, without contributions candidates .\nmany contributors gave money one candidate?\nClean data format (fixing parsing issue standardizing column names using janitor)Summarize variables dataset. missing values, , worried ? every variable format ? , create new variable(s) right format.Visually explore distribution values contributions. contributions notable outliers? share similar characteristic(s)? may useful plot distribution contributions without outliers get better sense majority data.List top five candidates categories: 1) total contributions; 2) mean contribution; 3) number contributions.Repeat process, without contributions candidates .many contributors gave money one candidate?Name three geoms produce graphs bars ggplot().Consider dataset 10,000 observations 27 variables. observation, least one missing variable. Please discuss, paragraph two, steps take understand going .Known missing data, leave holes dataset. data never collected? Please look McClelland, Alexander, 2019, ‘“Lock Whore ”: Legal Violence Flows Information Precipitating Personal Violence People Criminalised HIV-Related Crimes Canada,’ European Journal Risk Regulation, 10 (1), pp. 132-147. look Policing Pandemic - https://www.policingthepandemic.ca/. Look gathered dataset took put together. dataset ? missing ? affect results? might similar biases enter datasets used read ?","code":""},{"path":"exploratory-data-analysis.html","id":"introduction-16","chapter":"14 Exploratory data analysis","heading":"14.1 Introduction","text":"future data analysis can involve great progress, overcoming real difficulties, provision great service fields science technology. ? remains us, willingness take rocky road real problems preference smooth road unreal assumptions, arbitrary criteria, abstract results without real attachments. challenge?Tukey (1962, 64).Exploratory data analysis never finished, just die. active process exploring becoming familiar data. Like farmer hands earth, need know every contour aspect data. need know changes, shows, hides, limits. Exploratory data analysis unstructured process .said, exploratory data analysis (EDA) something ends final paper. means end inform entire paper, especially data section, ’s typically something belongs final draft. best way proceed make separate .Rmd add code brief notes go. Don’t delete previous code, just add . run time, ’ll useful notebook captures exploration. document collaborators guide subsequent modelling .EDA draws everything know analyst. Every tool fair game considered. Look raw data, make tables, plots, summary statistics, make models. key iterate, move quickly perfectly, come understand data.chapter working real data many issues can understand main characteristics potential issues. use opendatatoronto package (Gelfand 2020), among sources. lot options EDA (Staniak Biecek 2019) focus .","code":""},{"path":"exploratory-data-analysis.html","id":"case-study---ttc-subway-delays","chapter":"14 Exploratory data analysis","heading":"14.2 Case study - TTC subway delays","text":"section written Monica Alexander.","code":""},{"path":"exploratory-data-analysis.html","id":"introduction-17","chapter":"14 Exploratory data analysis","heading":"14.2.1 Introduction","text":"opendatatoronto package (Gelfand 2020) provides interface data available Open Data Portal provided City Toronto. going use take quick look subway delays. ’re additionally going especially draw tidyverse (Wickham et al. 2019a), well ggrepel (Slowikowski 2021), janitor (Firke 2020), lubridate (Grolemund Wickham 2011), visdat (Tierney 2017) packages.","code":"\nlibrary(opendatatoronto)\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(visdat)"},{"path":"exploratory-data-analysis.html","id":"gather-the-data","chapter":"14 Exploratory data analysis","heading":"14.2.2 Gather the data","text":"begin , use opendatatoronto::list_packages() look datasets available.’ll download data TTC subway delays 2019. multiple files 2019 need get make one big dataframe.Let’s also download delay code readme, reference. ’d probably want save ‘inputs’ folder, raw data underpin analysis.dataset bunch interesting variables. can refer readme descriptions. outcome interest min_delay, give delay minutes.Next ’re going highlight tools might useful getting used new dataset. ’s one way explore, ’s important keep mind:variables look like (type, values, distribution, etc);surprising (outliers etc); andwhat end goal (, might understanding factors associated delays, e.g. stations, time year, time day, etc).data analysis project, turns data issues, surprising values, missing data etc, ’s important document anything found subsequent steps assumptions made moving onto data analysis modeling.always:Start end mind.lazy possible.","code":"\nall_data <- opendatatoronto::list_packages(limit = 500)\nall_data\n#> # A tibble: 425 × 11\n#>    title   id      topics   civic_issues  publisher excerpt \n#>    <chr>   <chr>   <chr>    <chr>         <chr>     <chr>   \n#>  1 EarlyO… 261962… Communi… Poverty redu… Children… \"EarlyO…\n#>  2 Licens… 059d37… Communi… <NA>          Children… \"Licens…\n#>  3 Short … 2ab20f… Permits… Affordable h… Municipa… \"This d…\n#>  4 Polls … 7bce9b… City go… <NA>          City Cle… \"Polls …\n#>  5 Outbre… 80ce0b… Health,… <NA>          Toronto … \"This d…\n#>  6 COVID-… cd616c… Health   <NA>          Toronto … \"This d…\n#>  7 COVID-… d3f21f… Health   <NA>          Toronto … \"This d…\n#>  8 Rain G… f29335… Locatio… Climate chan… Toronto … \"This d…\n#>  9 Street… 99b1f3… City go… <NA>          Transpor… \"Inform…\n#> 10 Automa… a15479… Transpo… Mobility      Transpor… \"This d…\n#> # … with 415 more rows, and 5 more variables:\n#> #   dataset_category <chr>, num_resources <int>,\n#> #   formats <chr>, refresh_rate <chr>,\n#> #   last_refreshed <date>\n# We know this number based on the 'id' of the interest.\nttc_resources <- \n  list_package_resources(\"996cfe8d-fb35-40ce-b569-698d51fc683b\")\n\nttc_resources <- \n  ttc_resources %>% \n  mutate(year = str_extract(name, \"201.?\"))\n\ndelay_2019_ids <- \n  ttc_resources %>% \n  filter(year==2019) %>% \n  select(id) %>% \n  pull()\n\ndelay_2019 <- c()\n\nfor(i in 1:length(delay_2019_ids)) {\n  delay_2019 <- bind_rows(delay_2019, get_resource(delay_2019_ids[i]))\n}\n\n# make the column names nicer to work with\ndelay_2019 <- clean_names(delay_2019)\ndelay_codes <- get_resource(\"fece136b-224a-412a-b191-8d31eb00491e\")\n#> New names:\n#> * `` -> ...1\n#> * `CODE DESCRIPTION` -> `CODE DESCRIPTION...3`\n#> * `` -> ...4\n#> * `` -> ...5\n#> * `CODE DESCRIPTION` -> `CODE DESCRIPTION...7`\n\ndelay_data_codebook <- get_resource(\"54247e39-5a7d-40db-a137-82b2a9ab0708\")\nhead(delay_2019)\n#> # A tibble: 6 × 10\n#>   date                time  day     station  code  min_delay\n#>   <dttm>              <chr> <chr>   <chr>    <chr>     <dbl>\n#> 1 2019-01-01 00:00:00 01:08 Tuesday YORK MI… PUSI          0\n#> 2 2019-01-01 00:00:00 02:14 Tuesday ST ANDR… PUMST         0\n#> 3 2019-01-01 00:00:00 02:16 Tuesday JANE ST… TUSC          0\n#> 4 2019-01-01 00:00:00 02:27 Tuesday BLOOR S… SUO           0\n#> 5 2019-01-01 00:00:00 03:03 Tuesday DUPONT … MUATC        11\n#> 6 2019-01-01 00:00:00 03:08 Tuesday EGLINTO… EUATC        11\n#> # … with 4 more variables: min_gap <dbl>, bound <chr>,\n#> #   line <chr>, vehicle <dbl>"},{"path":"exploratory-data-analysis.html","id":"sanity-checks","chapter":"14 Exploratory data analysis","heading":"14.2.3 Sanity Checks","text":"need check variables say . aren’t, natural next question issues. recode , even remove ? ’s important distinguish factors characters, well factors numerics.instance, look column claims days week using unique().Another function ’s useful table().Let’s now check lines.looks like many issues , obvious re-code, . drop ? ’s absolute right answer , depends ’re using data , need aware issues data.","code":"\nunique(delay_2019$day)\n#> [1] \"Tuesday\"   \"Wednesday\" \"Thursday\"  \"Friday\"   \n#> [5] \"Saturday\"  \"Sunday\"    \"Monday\"\ntable(delay_2019$day)\n#> \n#>    Friday    Monday  Saturday    Sunday  Thursday   Tuesday \n#>      2979      2970      2238      1978      3116      2939 \n#> Wednesday \n#>      3002\nunique(delay_2019$line)\n#>  [1] \"YU\"                     \"BD\"                    \n#>  [3] \"YU/BD\"                  \"SHP\"                   \n#>  [5] \"SRT\"                    NA                      \n#>  [7] \"YUS\"                    \"B/D\"                   \n#>  [9] \"BD LINE\"                \"999\"                   \n#> [11] \"YU/ BD\"                 \"YU & BD\"               \n#> [13] \"BD/YU\"                  \"YU\\\\BD\"                \n#> [15] \"46 MARTIN GROVE\"        \"RT\"                    \n#> [17] \"BLOOR-DANFORTH\"         \"YU / BD\"               \n#> [19] \"134 PROGRESS\"           \"YU - BD\"               \n#> [21] \"985 SHEPPARD EAST EXPR\" \"22 COXWELL\"            \n#> [23] \"100 FLEMINGDON PARK\"    \"YU LINE\""},{"path":"exploratory-data-analysis.html","id":"missing-values","chapter":"14 Exploratory data analysis","heading":"14.2.4 Missing values","text":"Exploring missing data course , main point presence (lack thereof) haunt analysis. Insert joke ghostbusters .get started look known-unknowns, NAs variable.visdat package (Tierney 2017) also useful , particularly see missing values distributed.known-unknowns, interested whether missing random. want , ideally, show data happened just drop . course, unlikely case, looking see systematic data missing.","code":"\ndelay_2019 %>% \n  summarise_all(list(~sum(is.na(.))))\n#> # A tibble: 1 × 10\n#>    date  time   day station  code min_delay min_gap bound\n#>   <int> <int> <int>   <int> <int>     <int>   <int> <int>\n#> 1     0     0     0       0     0         0       0  4380\n#> # … with 2 more variables: line <int>, vehicle <int>\nvis_dat(delay_2019)\nvis_miss(delay_2019)"},{"path":"exploratory-data-analysis.html","id":"duplicate-rows","chapter":"14 Exploratory data analysis","heading":"14.2.5 Duplicate rows","text":"Sometime data happen duplicated. didn’t notice analysis wrong ways ’d able consistently expect. variety ways look duplicated rows, janitor::get_dupes() function janitor package (Firke 2020) especially useful.delays dataset quite duplicates. , ’re interested whether something systematic going . Remembering ’re trying quickly come terms dataset, one way forward flag issue come back explore later, just remove now.","code":"\njanitor::get_dupes(delay_2019)\n#> No variable names specified - using all columns.\n#> # A tibble: 158 × 11\n#>    date                time  day     station code  min_delay\n#>    <dttm>              <chr> <chr>   <chr>   <chr>     <dbl>\n#>  1 2019-01-01 00:00:00 08:18 Tuesday DONLAN… MUESA         5\n#>  2 2019-01-01 00:00:00 08:18 Tuesday DONLAN… MUESA         5\n#>  3 2019-02-01 00:00:00 05:51 Friday  SCARB … MRTO         10\n#>  4 2019-02-01 00:00:00 05:51 Friday  SCARB … MRTO         10\n#>  5 2019-02-01 00:00:00 06:45 Friday  MIDLAN… MRWEA         3\n#>  6 2019-02-01 00:00:00 06:45 Friday  MIDLAN… MRWEA         3\n#>  7 2019-02-01 00:00:00 06:55 Friday  LAWREN… ERDO          0\n#>  8 2019-02-01 00:00:00 06:55 Friday  LAWREN… ERDO          0\n#>  9 2019-02-01 00:00:00 07:16 Friday  MCCOWA… MRWEA         5\n#> 10 2019-02-01 00:00:00 07:16 Friday  MCCOWA… MRWEA         5\n#> # … with 148 more rows, and 5 more variables:\n#> #   min_gap <dbl>, bound <chr>, line <chr>, vehicle <dbl>,\n#> #   dupe_count <int>\ndelay_2019 <- \n  delay_2019 %>% \n  distinct()"},{"path":"exploratory-data-analysis.html","id":"visualizing-distributions","chapter":"14 Exploratory data analysis","heading":"14.2.6 Visualizing distributions","text":"need see data raw form understand , histograms, barplots, density plots friends . ’re looking beauty , ’re looking get look data quickly possible.Let’s look one outcome interest: ‘min_delay.’ First let’s just look histogram data.Somewhat concerningly evidence outliers (given large x-axis). variety ways focus going , quick way plot logged scale (remembering ’d expect values 0 drop away).initial exploration hinting outlying delay time, let’s take look largest delays. need join dataset ‘delay_codes’ dataset see delay , requires wrangling slightly different codes.can see 455 minute delay due ‘Rail Related Problem’ seems much outlier.","code":"\n## Removing the observations that have non-standardized lines\ndelay_2019 <- \n  delay_2019 %>% \n  filter(line %in% c(\"BD\", \"YU\", \"SHP\", \"SRT\"))\n\nggplot(data = delay_2019) + \n  geom_histogram(aes(x = min_delay))\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\nggplot(data = delay_2019) + \n  geom_histogram(aes(x = min_delay)) + \n  scale_x_log10()\n#> Warning: Transformation introduced infinite values in\n#> continuous x-axis\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n#> Warning: Removed 11944 rows containing non-finite values\n#> (stat_bin).\ndelay_2019 <- \n  delay_2019 %>% \n  left_join(delay_codes %>% \n              rename(code = `SUB RMENU CODE`, code_desc = `CODE DESCRIPTION...3`) %>%\n              select(code, code_desc)\n            )\n#> Joining, by = \"code\"\n\ndelay_2019 <- \n  delay_2019 %>%\n  mutate(code_srt = ifelse(line==\"SRT\", code, \"NA\")) %>% \n  left_join(delay_codes %>% \n              rename(code_srt = `SRT RMENU CODE`, code_desc_srt = `CODE DESCRIPTION...7`) %>%\n              select(code_srt, code_desc_srt)) %>% \n  mutate(code = ifelse(code_srt==\"NA\", code, code_srt),\n         code_desc = ifelse(is.na(code_desc_srt), code_desc, code_desc_srt)) %>% \n  select(-code_srt, -code_desc_srt)\n#> Joining, by = \"code_srt\"\ndelay_2019 %>% \n  left_join(delay_codes %>% \n              rename(code = `SUB RMENU CODE`, code_desc = `CODE DESCRIPTION...3`) %>%\n              select(code, code_desc)) %>% \n  arrange(-min_delay) %>% \n  select(date, time, station, line, min_delay, code, code_desc)\n#> Joining, by = c(\"code\", \"code_desc\")\n#> # A tibble: 18,697 × 7\n#>    date                time  station   line  min_delay code \n#>    <dttm>              <chr> <chr>     <chr>     <dbl> <chr>\n#>  1 2019-06-25 00:00:00 18:48 WILSON T… YU          455 PUTR \n#>  2 2019-02-12 00:00:00 20:28 LAWRENCE… SRT         284 MRWEA\n#>  3 2019-06-05 00:00:00 12:42 UNION TO… YU          250 MUPLA\n#>  4 2019-10-22 00:00:00 14:22 LAWRENCE… YU          228 PUTS \n#>  5 2019-09-26 00:00:00 11:38 YORK MIL… YU          193 MUPR1\n#>  6 2019-06-08 00:00:00 08:51 SPADINA … BD          180 MUPLB\n#>  7 2019-12-02 00:00:00 06:59 DUNDAS W… BD          176 MUPLB\n#>  8 2019-01-29 00:00:00 05:46 VICTORIA… BD          174 MUWEA\n#>  9 2019-02-22 00:00:00 17:32 ELLESMER… SRT         168 PRW  \n#> 10 2019-02-10 00:00:00 07:53 BAYVIEW … SHP         165 PUSI \n#> # … with 18,687 more rows, and 1 more variable:\n#> #   code_desc <chr>"},{"path":"exploratory-data-analysis.html","id":"groups-and-small-counts","chapter":"14 Exploratory data analysis","heading":"14.2.7 Groups and small counts","text":"Another thing ’re looking various groupings data, especially sub-groups may end small numbers observations (analysis easily influenced ). quick way group data variable interest, instance ‘line,’ using colour.plot uses density can look distributions comparably, also aware differences frequency. case, ’ll see SHP SRT much smaller counts.group second variable can useful use facets. ’re little fiddly initially, get used , ’re quick powerful.aside, station names mess. try quickly bring little order chaos just taking just first word (, first two starts ‘ST’).can now plot top five stations mean delay.","code":"\nggplot(data = delay_2019) + \n  geom_histogram(aes(x = min_delay, y = ..density.., fill = line), \n                 position = 'dodge', \n                 bins = 10) + \n  scale_x_log10()\n#> Warning: Transformation introduced infinite values in\n#> continuous x-axis\n#> Warning: Removed 11944 rows containing non-finite values\n#> (stat_bin).\nggplot(data = delay_2019) + \n  geom_histogram(aes(x = min_delay, fill = line), \n                 position = 'dodge', \n                 bins = 10) + \n  scale_x_log10()\n#> Warning: Transformation introduced infinite values in\n#> continuous x-axis\n#> Warning: Removed 11944 rows containing non-finite values\n#> (stat_bin).\nggplot(data = delay_2019) + \n  geom_density(aes(x = min_delay, color = day), \n               bw = .08) + \n  scale_x_log10() + \n  facet_grid(~line)\n#> Warning: Transformation introduced infinite values in\n#> continuous x-axis\n#> Warning: Removed 11944 rows containing non-finite values\n#> (stat_density).\ndelay_2019 <- \n  delay_2019 %>% \n  mutate(station_clean = ifelse(str_starts(station, \"ST\"), word(station, 1,2), word(station, 1)))\ndelay_2019 %>% \n  group_by(line, station_clean) %>% \n  summarise(mean_delay = mean(min_delay), n_obs = n()) %>% \n  filter(n_obs>1) %>% \n  arrange(line, -mean_delay) %>% \n  slice(1:5) %>% \n  ggplot(aes(station_clean, mean_delay)) + \n    geom_col() + \n    coord_flip() + \n    facet_wrap(~line, scales = \"free_y\")\n#> `summarise()` has grouped output by 'line'. You can override using the `.groups` argument."},{"path":"exploratory-data-analysis.html","id":"visualizing-time-series","chapter":"14 Exploratory data analysis","heading":"14.2.8 Visualizing time series","text":"Dates pain work . always go wrong give issues, critical aspect explore EDA. daily plot data , well, messy (can check ). Instead, let’s look week see ’s seasonality. lubridate package (Grolemund Wickham 2011) lot helpful functions deal date variables. ’s essentially indispensable.get started, let’s look mean delay (delayed zero minutes).Now let’s look proportion delays greater 10 minutes., ’s important clear . charts tables analyse place final report, allowing become comfortable data. , additionally making notes plot table go, noting warnings implications aspects return .","code":"\ndelay_2019 %>% \n  filter(min_delay>0) %>% \n  mutate(week = week(date)) %>% \n  group_by(week, line) %>% \n  summarise(mean_delay = mean(min_delay)) %>% \n  ggplot(aes(week, mean_delay, color = line)) + \n    geom_point() + \n    geom_smooth() + \n    facet_grid(~line)\n#> `summarise()` has grouped output by 'week'. You can override using the `.groups` argument.\n#> `geom_smooth()` using method = 'loess' and formula 'y ~ x'\ndelay_2019 %>% \n  mutate(week = week(date)) %>% \n  group_by(week, line) %>% \n  summarise(prop_delay = sum(min_delay>10)/n()) %>% \n  ggplot(aes(week, prop_delay, color = line)) + \n    geom_point() + \n    geom_smooth() + \n    facet_grid(~line)\n#> `summarise()` has grouped output by 'week'. You can override using the `.groups` argument.\n#> `geom_smooth()` using method = 'loess' and formula 'y ~ x'"},{"path":"exploratory-data-analysis.html","id":"visualizing-relationships","chapter":"14 Exploratory data analysis","heading":"14.2.9 Visualizing relationships","text":"also interested looking relationship two variables. Scatter plots especially useful continuous variables, good precursor modeling. saw little ‘mean_delay’ ‘week.’relationship categorical variables takes work, also, instance, look top five reasons delay station. particular, may interested whether differ, difference modelled.","code":"\ndelay_2019 %>%\n  ggplot(aes(x = min_delay, y = min_gap)) +\n  geom_point() +\n  scale_x_log10() +\n  scale_y_log10()\n#> Warning: Transformation introduced infinite values in\n#> continuous x-axis\n#> Warning: Transformation introduced infinite values in\n#> continuous y-axis\ndelay_2019 %>%\n  group_by(line, code_desc) %>%\n  summarise(mean_delay = mean(min_delay)) %>%\n  arrange(-mean_delay) %>%\n  slice(1:5) %>%\n  ggplot(aes(x = code_desc,\n             y = mean_delay)) +\n  geom_col() + \n  facet_wrap(vars(line), \n             scales = \"free_y\",\n             nrow = 4) +\n  coord_flip()\n#> `summarise()` has grouped output by 'line'. You can override using the `.groups` argument."},{"path":"exploratory-data-analysis.html","id":"principal-components-analysis","chapter":"14 Exploratory data analysis","heading":"14.2.10 Principal components analysis","text":"Principal components analysis (PCA) another powerful exploratory tool. allows pick potential clusters /outliers can help inform model building. see let’s quick (imperfect) example looking types delays station.delay categories bit mess, ’s hundreds . simple start, remembering task come terms dataset quickly possible, let’s just take first word.Let’s also just restrict analysis causes happen least 50 times 2019. PCA, dataframe also needs switched wide format.Now can quickly PCA.can plot first two principal components, add labels outlying stations.also plot factor loadings. se evidence perhaps one public, compared another operator.","code":"\ndelay_2019 <- delay_2019 %>% \n  mutate(code_red = case_when(\n    str_starts(code_desc, \"No\") ~ word(code_desc, 1, 2),\n    str_starts(code_desc, \"Operator\") ~ word(code_desc, 1,2),\n    TRUE ~ word(code_desc,1))\n         )\n\ndwide <- delay_2019 %>% \n  group_by(line, station_clean) %>% \n  mutate(n_obs = n()) %>% \n  filter(n_obs>1) %>% \n  group_by(code_red) %>% \n  mutate(tot_delay = n()) %>% \n  arrange(tot_delay) %>% \n  filter(tot_delay>50) %>% \n  group_by(line, station_clean, code_red) %>% \n  summarise(n_delay = n()) %>% \n  pivot_wider(names_from = code_red, values_from = n_delay) %>% \n  mutate_all(.funs = funs(ifelse(is.na(.), 0, .)))\n#> `summarise()` has grouped output by 'line', 'station_clean'. You can override using the `.groups` argument.\n#> `mutate_all()` ignored the following grouping variables:\n#> Columns `line`, `station_clean`\n#> Use `mutate_at(df, vars(-group_cols()), myoperation)` to silence the message.\n#> Warning: `funs()` was deprecated in dplyr 0.8.0.\n#> Please use a list of either functions or lambdas: \n#> \n#>   # Simple named list: \n#>   list(mean = mean, median = median)\n#> \n#>   # Auto named with `tibble::lst()`: \n#>   tibble::lst(mean, median)\n#> \n#>   # Using lambdas\n#>   list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n#> This warning is displayed once every 8 hours.\n#> Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\ndelay_pca <- prcomp(dwide[,3:ncol(dwide)])\n\ndf_out <- as_tibble(delay_pca$x)\ndf_out <- bind_cols(dwide %>% select(line, station_clean), df_out)\nhead(df_out)\n#> # A tibble: 6 × 40\n#> # Groups:   line, station_clean [6]\n#>   line  station_clean    PC1     PC2    PC3    PC4    PC5\n#>   <chr> <chr>          <dbl>   <dbl>  <dbl>  <dbl>  <dbl>\n#> 1 BD    BATHURST        6.50   26.9   -2.71 -10.8  -8.40 \n#> 2 BD    BAY            24.8     7.63  -2.19  -7.05  0.714\n#> 3 BD    BLOOR         -62.4  -112.    57.3  -23.4  -5.09 \n#> 4 BD    BROADVIEW      -6.60   28.1   -1.06 -14.0  -6.49 \n#> 5 BD    CASTLE         23.8    11.8   -1.31  -7.93 -3.62 \n#> 6 BD    CHESTER        24.6    -1.87 -18.6    2.75  1.85 \n#> # … with 33 more variables: PC6 <dbl>, PC7 <dbl>,\n#> #   PC8 <dbl>, PC9 <dbl>, PC10 <dbl>, PC11 <dbl>,\n#> #   PC12 <dbl>, PC13 <dbl>, PC14 <dbl>, PC15 <dbl>,\n#> #   PC16 <dbl>, PC17 <dbl>, PC18 <dbl>, PC19 <dbl>,\n#> #   PC20 <dbl>, PC21 <dbl>, PC22 <dbl>, PC23 <dbl>,\n#> #   PC24 <dbl>, PC25 <dbl>, PC26 <dbl>, PC27 <dbl>,\n#> #   PC28 <dbl>, PC29 <dbl>, PC30 <dbl>, PC31 <dbl>, …\nggplot(df_out,aes(x=PC1,y=PC2,color=line )) + \n  geom_point() + \n  geom_text_repel(data = df_out %>% filter(PC2>100|PC1<100*-1), \n                  aes(label = station_clean)\n                  )\ndf_out_r <- as_tibble(delay_pca$rotation)\ndf_out_r$feature <- colnames(dwide[,3:ncol(dwide)])\n\ndf_out_r\n#> # A tibble: 38 × 39\n#>         PC1      PC2        PC3       PC4      PC5      PC6\n#>       <dbl>    <dbl>      <dbl>     <dbl>    <dbl>    <dbl>\n#>  1 -0.0412   0.0638   0.0133    -0.0467    0.0246   0.0184 \n#>  2 -0.0332  -0.00469 -0.0414    -0.00751   0.0201  -0.0122 \n#>  3 -0.135    0.207    0.0237    -0.144     0.135   -0.0381 \n#>  4 -0.0652   0.0475  -0.0443    -0.0251   -0.00139 -0.0748 \n#>  5 -0.00443  0.00878 -0.0000499 -0.000830  0.00967  0.00954\n#>  6 -0.0268  -0.00722 -0.00439    0.000534 -0.0151  -0.0125 \n#>  7 -0.0813   0.0960  -0.0462     0.0479   -0.0978  -0.0365 \n#>  8 -0.0117   0.0135   0.00548   -0.0294    0.0125   0.0377 \n#>  9 -0.516    0.655   -0.0177    -0.162    -0.221   -0.287  \n#> 10 -0.151    0.0826   0.0548     0.352    -0.397    0.281  \n#> # … with 28 more rows, and 33 more variables: PC7 <dbl>,\n#> #   PC8 <dbl>, PC9 <dbl>, PC10 <dbl>, PC11 <dbl>,\n#> #   PC12 <dbl>, PC13 <dbl>, PC14 <dbl>, PC15 <dbl>,\n#> #   PC16 <dbl>, PC17 <dbl>, PC18 <dbl>, PC19 <dbl>,\n#> #   PC20 <dbl>, PC21 <dbl>, PC22 <dbl>, PC23 <dbl>,\n#> #   PC24 <dbl>, PC25 <dbl>, PC26 <dbl>, PC27 <dbl>,\n#> #   PC28 <dbl>, PC29 <dbl>, PC30 <dbl>, PC31 <dbl>, …\n\nggplot(df_out_r,aes(x=PC1,y=PC2,label=feature )) + geom_text_repel()\n#> Warning: ggrepel: 29 unlabeled data points (too many\n#> overlaps). Consider increasing max.overlaps"},{"path":"exploratory-data-analysis.html","id":"case-study---opinions-about-a-casino-in-toronto","chapter":"14 Exploratory data analysis","heading":"14.3 Case study - Opinions about a casino in Toronto","text":"written Michael Chong.","code":""},{"path":"exploratory-data-analysis.html","id":"data-preparation","chapter":"14 Exploratory data analysis","heading":"14.3.1 Data preparation","text":"use opendatatoronto package . See previous case study deeper explanation code works.dataset ’m extracting results survey 2012 regarding establishment casino Toronto. info available following link. analysis, ’ll hoping address question: demographic (age/gender) groups likely supportive new casino Toronto?object casino_resource isn’t quite useable yet, ’s (inconveniently) stored list 2 data frames:just return object, can see second list item empty, just want keep first one:, let’s keep first item indexing list double square brackets:Let’s check first couple rows dataframe looks like. default, head() returns first 6 rows:Unfortunately column names aren’t informative. simplicity, ’ll use ‘.pdf’ questionnaire accompanies dataset Toronto Open Data website. Alternatively, get parse ‘readme’ R package. ’s link questionnaire.Question 1 indicates level support casino Toronto. ’ll use response variable.Concerning potential predictor variables, questions ask respondents opinions different aspects potential casino development, aren’t particularly useful towards cause. demographic variables Age Gender, let’s choose .’m also going rename columns resulting data frame columns ‘opinion,’ ‘age,’ ‘gender.’","code":"\n# Get the data\ncasino_resource <- \n  search_packages(\"casino survey\")%>%\n  list_package_resources() %>%\n  filter(name == \"toronto-casino-survey-results\") %>%\n  get_resource()\n#> New names:\n#> * `` -> ...93\n#> * `` -> ...94\nhead(casino_resource)\n#> $tblSurvey\n#> # A tibble: 17,766 × 94\n#>    SurveyID Q1_A   Q1_B1 Q1_B2 Q1_B3 Q2_A  Q2_B  Q3_A  Q3_B \n#>       <dbl> <chr>  <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n#>  1        1 Stron… Do n… Do n… Do n… Does… \"As … Not … Very…\n#>  2        2 Stron… Econ… Jobs  Arts… Fits… \"Cos… Very… Very…\n#>  3        3 Stron… Ther… If t… <NA>  Fits… \"Big… Very… Very…\n#>  4        4 Somew… beli… mone… evid… Does… \"My … Very… Very…\n#>  5        5 Neutr… Like… Conc… <NA>  Neut… \"Aga… Very… Very…\n#>  6        6 Stron… have… <NA>  <NA>  Does… \"Tor… Not … Not …\n#>  7        7 Stron… The … Peop… We s… Does… \"#3 … Not … Not …\n#>  8        8 Stron… It w… Mora… <NA>  Does… \"Cas… Very… Very…\n#>  9        9 Stron… It's… traf… heal… Does… \"No … Not … Very…\n#> 10       10 Stron… Toro… Avoi… Prov… Fits… \"Tor… Very… Very…\n#> # … with 17,756 more rows, and 85 more variables:\n#> #   Q3_C <chr>, Q3_D <chr>, Q3_E <chr>, Q3_F <chr>,\n#> #   Q3_G <chr>, Q3_H <chr>, Q3_I <chr>, Q3_J <chr>,\n#> #   Q3_K <chr>, Q3_L <chr>, Q3_M <chr>, Q3_N <chr>,\n#> #   Q3_O <chr>, Q3_P <chr>, Q3_Q <chr>, Q3_Q_Other <chr>,\n#> #   Q3_Comments <chr>, Q4_A <chr>, Q5 <chr>, Q6 <chr>,\n#> #   Q6_Comments <chr>, Q7_A_StandAlone <chr>, …\n#> \n#> $Sheet1\n#> # A tibble: 0 × 0\n# Check what kind of object the casino_resource object is\nclass(casino_resource)\n#> [1] \"list\"\ncasino_resource\n#> $tblSurvey\n#> # A tibble: 17,766 × 94\n#>    SurveyID Q1_A   Q1_B1 Q1_B2 Q1_B3 Q2_A  Q2_B  Q3_A  Q3_B \n#>       <dbl> <chr>  <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n#>  1        1 Stron… Do n… Do n… Do n… Does… \"As … Not … Very…\n#>  2        2 Stron… Econ… Jobs  Arts… Fits… \"Cos… Very… Very…\n#>  3        3 Stron… Ther… If t… <NA>  Fits… \"Big… Very… Very…\n#>  4        4 Somew… beli… mone… evid… Does… \"My … Very… Very…\n#>  5        5 Neutr… Like… Conc… <NA>  Neut… \"Aga… Very… Very…\n#>  6        6 Stron… have… <NA>  <NA>  Does… \"Tor… Not … Not …\n#>  7        7 Stron… The … Peop… We s… Does… \"#3 … Not … Not …\n#>  8        8 Stron… It w… Mora… <NA>  Does… \"Cas… Very… Very…\n#>  9        9 Stron… It's… traf… heal… Does… \"No … Not … Very…\n#> 10       10 Stron… Toro… Avoi… Prov… Fits… \"Tor… Very… Very…\n#> # … with 17,756 more rows, and 85 more variables:\n#> #   Q3_C <chr>, Q3_D <chr>, Q3_E <chr>, Q3_F <chr>,\n#> #   Q3_G <chr>, Q3_H <chr>, Q3_I <chr>, Q3_J <chr>,\n#> #   Q3_K <chr>, Q3_L <chr>, Q3_M <chr>, Q3_N <chr>,\n#> #   Q3_O <chr>, Q3_P <chr>, Q3_Q <chr>, Q3_Q_Other <chr>,\n#> #   Q3_Comments <chr>, Q4_A <chr>, Q5 <chr>, Q6 <chr>,\n#> #   Q6_Comments <chr>, Q7_A_StandAlone <chr>, …\n#> \n#> $Sheet1\n#> # A tibble: 0 × 0\ncasino_data <- casino_resource[[1]]\nhead(casino_data) \n#> # A tibble: 6 × 94\n#>   SurveyID Q1_A   Q1_B1  Q1_B2 Q1_B3 Q2_A  Q2_B  Q3_A  Q3_B \n#>      <dbl> <chr>  <chr>  <chr> <chr> <chr> <chr> <chr> <chr>\n#> 1        1 Stron… Do no… Do n… Do n… Does… \"As … Not … Very…\n#> 2        2 Stron… Econo… Jobs  Arts… Fits… \"Cos… Very… Very…\n#> 3        3 Stron… There… If t… <NA>  Fits… \"Big… Very… Very…\n#> 4        4 Somew… belie… mone… evid… Does… \"My … Very… Very…\n#> 5        5 Neutr… Like … Conc… <NA>  Neut… \"Aga… Very… Very…\n#> 6        6 Stron… have … <NA>  <NA>  Does… \"Tor… Not … Not …\n#> # … with 85 more variables: Q3_C <chr>, Q3_D <chr>,\n#> #   Q3_E <chr>, Q3_F <chr>, Q3_G <chr>, Q3_H <chr>,\n#> #   Q3_I <chr>, Q3_J <chr>, Q3_K <chr>, Q3_L <chr>,\n#> #   Q3_M <chr>, Q3_N <chr>, Q3_O <chr>, Q3_P <chr>,\n#> #   Q3_Q <chr>, Q3_Q_Other <chr>, Q3_Comments <chr>,\n#> #   Q4_A <chr>, Q5 <chr>, Q6 <chr>, Q6_Comments <chr>,\n#> #   Q7_A_StandAlone <chr>, Q7_A_Integrated <chr>, …\n# Narrow down the dataframe to our variables of interest\ncasino_data <- \n  casino_data %>%\n  select(Q1_A, Age, Gender) %>%\n  rename(opinion = Q1_A, age = Age, gender = Gender)\n\n# Look at first couple rows:\nhead(casino_data)\n#> # A tibble: 6 × 3\n#>   opinion                   age   gender\n#>   <chr>                     <chr> <chr> \n#> 1 Strongly Opposed          25-34 Male  \n#> 2 Strongly in Favour        35-44 Female\n#> 3 Strongly in Favour        55-64 Male  \n#> 4 Somewhat Opposed          25-34 Male  \n#> 5 Neutral or Mixed Feelings 25-34 Female\n#> 6 Strongly Opposed          45-54 Female"},{"path":"exploratory-data-analysis.html","id":"some-visual-exploration-and-more-cleanup-of-course","chapter":"14 Exploratory data analysis","heading":"14.3.2 Some visual exploration (and more cleanup, of course)","text":"Let’s first quick exploration get feel ’s going data. ’ll first calculate proportions casino support age-gender combination:notes:use geom_col() make bar chart,facet_grid() modifies plot plot panels correspond certain values discrete variables (case, “facet” age gender). helpful case interested distribution opinions changes age gender.things note:x-axis labels order sense monotone order increasing/decreasing supportthere NA values opinion, age, gender, well “Prefer disclose” responses","code":"\n# Calculate proportions\ncasino_summary <- casino_data %>%\n  group_by(age, gender, opinion) %>%\n  summarise(n = n()) %>% # Count the number in each group and response\n  group_by(age, gender) %>%\n  mutate(prop = n/sum(n)) # Calculate proportions within each group\n#> `summarise()` has grouped output by 'age', 'gender'. You can override using the `.groups` argument.\nggplot(casino_summary) +\n  geom_col(aes(x = opinion, y = prop)) + # Specify a histogram of opinion responses\n  facet_grid(age~gender) + #Facet by age and gender\n  theme(axis.text.x = element_text(angle = 90)) # Rotate the x-axis labels to be readable"},{"path":"exploratory-data-analysis.html","id":"getting-the-data-into-a-more-model-suitable-format","chapter":"14 Exploratory data analysis","heading":"14.3.3 Getting the data into a more model-suitable format","text":"First need get rid responses aren’t suitable. simplicity ’ll assume NA values “Prefer disclose” responses occur randomly, remove dataset (note reality assumption might hold might want careful). Let’s check many rows original dataset:Now let’s dplyr::filter() accordingly omit responses don’t want. case ’re unfamiliar, ’m going make use :.na(), returns TRUE argument NA,! operator, flips TRUE FALSE. instance, !.na(x) return TRUE x NA, want keep.Let’s check many rows data ’re left :Now need convert response variable binary. clean first problem (response variables order), might well take opportunity convert format suitable model. logistic regression, like response variable binary, case 5 possible categories ranging “Strongly Opposed” “Strongly Favour.” ’ll recategorize new ‘supportive_or_not’ variable follows.‘supportive = 1’ “Strongly Favour” “Somewhat Favour”‘supportive = 0’ “Neutral Mixed Feelings,” “Somewhat Opposed,” “Strongly Opposed”dplyr::mutate() function, creates new columns (possibly functions existing columns), dplyr::case_when(), provides way assign values conditional -statements. syntax little strange. LHS ~ “” condition, RHS tilde value return. example, ‘x == 0 ~ 3’ return 3 ‘x’ 0.Another commonly used operator %% operator, checks whether something element vector, instance, e.g.:1 %% c(1, 3, 4) returns TRUE2 %% c(1, 3, 4) returns FALSENow need convert age numeric variable. Age survey given age groups. Let’s instead map numeric variable can easily talk trends age. ’ll map youngest age 1, :Now let’s make plot , new processed data:can sort see difference distribution different panels. formalize , can run logistic regression.","code":"\n# nrow() returns the number of rows in a dataframe:\nnrow(casino_data)\n#> [1] 17766\ncasino_data <- casino_data %>%\n  # Only keep rows with non-NA:\n  filter(!is.na(opinion), !is.na(age), !is.na(gender)) %>%\n  # Only keep rows where age and gender are disclosed:\n  filter(age != \"Prefer not to disclose\", gender != \"Prefer not to disclose\")\nnrow(casino_data)\n#> [1] 13658\n# Store possible opinions in vectors\nyes_opinions <- c(\"Strongly in Favour\", \"Somewhat in Favour\")\nno_opinions <- c(\"Neutral or Mixed Feelings\", \"Somewhat Opposed\", \"Strongly Opposed\")\n\n# Create `supportive` column:\ncasino_data <- \n  casino_data %>%\n  mutate(supportive = case_when(\n    opinion %in% yes_opinions ~ TRUE, # Assign TRUE\n    opinion %in% no_opinions ~ FALSE  # Assign FALSE\n  ))\ncasino_data <- \n  casino_data %>%\n  mutate(age_group = case_when(\n    age == \"Under 15\" ~ 0,\n    age == \"15-24\" ~ 1,\n    age == \"25-34\" ~ 2,\n    age == \"35-44\" ~ 3,\n    age == \"45-54\" ~ 4,\n    age == \"55-64\" ~ 5,\n    age == \"65 or older\" ~ 6\n  ))\ncasino_summary2 <- \n  casino_data %>%\n  group_by(age_group, gender, supportive) %>%\n  summarise(n = n()) %>% # Count the number in each group and response\n  group_by(age_group, gender) %>%\n  mutate(prop = n/sum(n)) # Calculate proportions within each group\n#> `summarise()` has grouped output by 'age_group', 'gender'. You can override using the `.groups` argument.\n\nggplot(casino_summary2) +\n  facet_grid(age_group ~ gender) +\n  geom_col(aes(x = supportive, y = prop)) "},{"path":"exploratory-data-analysis.html","id":"logistic-regression","chapter":"14 Exploratory data analysis","heading":"14.3.4 Logistic Regression","text":"Now, ’re set feed regression. can glm(), allows us fit generalized linear models.use family = \"binomial\" specify logistic regression, formula supportive ~ age_group + gender, indicates supportive (binary) response variable since ’s LHS, age_group gender predictor variables.can take look results running GLM using summary():Interpretation can little tricky. important things note results:Firstly, numeric age group variable. Remember coded age_group numbers 1 5. ’ve used age groups instead age, careful phrase conclusion. coefficient estimate corresponds effect moving unit age group scale (e.g. 25-34 age group 35-44 age group), rather 1 year age (e.g. age 28 29).Secondly, results log-odds ratios. effect estimates log-odds scale. means effect -0.07983 age_group interpreted : ‘unit increase age_group, estimate 0.07983 decrease log-odds supportive casino.’exponentiate coefficient estimate make least little easier interpret. number get interpreted factor odds.(cleaner) interpretation : ‘odds individuals gender pro-casino predicted change factor 0.9232733 unit increase age_group’Finally, baseline category. First, note categorical variables, gender coefficients relative “baseline” category. value gender doesn’t appear table, Female, implicitly used baseline gender category. Technical note: variable stored character class, glm() choose alphabetically first value use baseline., interpretation genderMale coefficient : ‘odds male individual supporting casino 2.0144778 times higher female individual age_group.’can make estimates variety ways. First ’ll look manually.Using formula found James et al. (2017, 4.3.3), can make estimates individual certain characteristics. Suppose wanted predict probability supporting Toronto casino individual 36 identified transgender. :age_group takes value 3, since age group 35-44 coded 3,genderTransgendered takes value 1First, let’s extract coefficient estimates vector using coefficients():Since vector labeled, can index using square brackets names. instance:first let’s evaluate exponent term \\(e^{\\beta_0 + \\cdots + \\beta_p X_p}\\):Now evaluate expression gives probability casino support:works, stream-lined ways. Thankfully R comes convenient function make prediction estimates glm(). using predict() function. First, need make dataframe relevant variables values ’re interested predicting. ’ll use values :dataframe looks like :feed predict() function, along glm object. get probability, need specify type = \"response\".matches probability got manually, yay!","code":"\ncasino_glm <- glm(supportive ~ age_group + gender, data = casino_data, family = \"binomial\")\nsummary(casino_glm)\n#> \n#> Call:\n#> glm(formula = supportive ~ age_group + gender, family = \"binomial\", \n#>     data = casino_data)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -1.0107  -0.8888  -0.6804   1.4249   1.8822  \n#> \n#> Coefficients:\n#>                     Estimate Std. Error z value Pr(>|z|)\n#> (Intercept)         -1.10594    0.05863 -18.862  < 2e-16\n#> age_group           -0.07983    0.01376  -5.801 6.59e-09\n#> genderMale           0.70036    0.04027  17.390  < 2e-16\n#> genderTransgendered  0.69023    0.39276   1.757   0.0789\n#>                        \n#> (Intercept)         ***\n#> age_group           ***\n#> genderMale          ***\n#> genderTransgendered .  \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 16010  on 13657  degrees of freedom\n#> Residual deviance: 15653  on 13654  degrees of freedom\n#> AIC: 15661\n#> \n#> Number of Fisher Scoring iterations: 4\nexp(-0.07983)\n#> [1] 0.9232733\nexp(0.70036)\n#> [1] 2.014478\ncoefs <- coefficients(casino_glm)\ncoefs\n#>         (Intercept)           age_group          genderMale \n#>         -1.10593925         -0.07983372          0.70036199 \n#> genderTransgendered \n#>          0.69022910\ncoefs[\"age_group\"]\n#>   age_group \n#> -0.07983372\nexp_term <- exp(coefs[\"(Intercept)\"] + coefs[\"age_group\"]*3 + coefs[\"genderTransgendered\"]*1)\n# The unname() command just takes off the label that it \"inherited\" from the coefs vector.\n# (don't worry about it, doesn't affect any functionality)\nunname(exp_term / (1 + exp_term))\n#> [1] 0.3418161\nprediction_df <- data.frame(age_group = 3, gender = \"Transgendered\")\nprediction_df\n#>   age_group        gender\n#> 1         3 Transgendered\npredict(casino_glm, newdata = prediction_df, type = \"response\")\n#>         1 \n#> 0.3418161"},{"path":"exploratory-data-analysis.html","id":"case-study---airbnb-listing-in-toronto","chapter":"14 Exploratory data analysis","heading":"14.4 Case study - Airbnb listing in Toronto","text":"","code":""},{"path":"exploratory-data-analysis.html","id":"essentials","chapter":"14 Exploratory data analysis","heading":"14.4.1 Essentials","text":"case study look Airbnb listings Toronto.","code":""},{"path":"exploratory-data-analysis.html","id":"set-up-1","chapter":"14 Exploratory data analysis","heading":"14.4.2 Set up","text":"","code":"\nlibrary(broom) # Helps with model outputs etc\nlibrary(here) # Helps with specifying path names\nlibrary(janitor) # Helps with initial data cleaning and pretty tables\nlibrary(tidymodels) # Help with modelling\nlibrary(tidyverse) \nlibrary(visdat) # Helps check missing values"},{"path":"exploratory-data-analysis.html","id":"get-data","chapter":"14 Exploratory data analysis","heading":"14.4.3 Get data","text":"dataset Inside Airbnb (Cox 2021). package help (Müller 2017).can give read_csv() link dataset download . helps reproducibility source clear. , link change time, longer-term reproducibility, well wanting minimise effect Inside Airbnb servers, suggest also save local copy data use . (original data , make public without first getting written permission.)","code":"\n# For reproducibility\n# airbnb_data_reduced <- read_csv(\"http://data.insideairbnb.com/canada/on/toronto/2021-01-02/data/listings.csv.gz\", guess_max = 20000)\n# write_csv(airbnb_data_reduced, \"week_6/data/airbnb_toronto_2019-12-07.csv\")\n\n# For actual work\nairbnb_data <- read_csv(here::here(\"dont_push/airbnb_toronto_2021_january-listings.csv\"), guess_max = 20000)\n\n# The guess_max option in read_csv helps us avoid having to specify the column types. Usually read_csv takes a best guess at the column types based on the first few rows. But sometimes those first ones are misleading and so guess_max forces it to look at a larger number of rows to try to work out what is going on."},{"path":"exploratory-data-analysis.html","id":"clean-data-1","chapter":"14 Exploratory data analysis","heading":"14.4.4 Clean data","text":"enormous number columns, ’ll just select .might like brief look dataset see anything weird going . bunch ways .things jump :character variables probably numerics dates/times: host_response_time, price, weekly_price, monthly_price, cleaning_fee.Weekly monthly price missing overwhelming number observations.Roughly fifth observations missing review score seems like correlation review-type variables.two variants neighbourhood name.NAs host_is_superhost.reviews seem really skewed.someone 328 properties Airbnb.","code":"\nnames(airbnb_data) %>% length()\n#> [1] 74\n\nairbnb_data_selected <- \n  airbnb_data %>% \n  select(host_id, \n         host_since, \n         host_response_time, \n         host_is_superhost, \n         host_listings_count,\n         host_total_listings_count,\n         host_neighbourhood, \n         host_listings_count, \n         neighbourhood_cleansed, \n         room_type, \n         bathrooms, \n         bedrooms, \n         price, \n         number_of_reviews, \n         has_availability, \n         review_scores_rating, \n         review_scores_accuracy, \n         review_scores_cleanliness, \n         review_scores_checkin, \n         review_scores_communication, \n         review_scores_location, \n         review_scores_value\n         )"},{"path":"exploratory-data-analysis.html","id":"price","chapter":"14 Exploratory data analysis","heading":"14.4.4.1 Price","text":"First need convert numeric. common problem, need little careful doesn’t just convert NAs. case just force price data numeric go NA lot characters R doesn’t know convert, e.g. numeric ‘$?’ need remove characters first.Now can look prices.outliers. Let’s zoom prices $1,000.Let’s look detail price $4,999.Let’s look distribution prices ‘reasonable’ range, Monica full professor, defined nightly price less $1,000.Interestingly looks like bunching prices, possible around numbers ending zero nine? Let’s just zoom prices $90 $210, interest, change bins smaller.","code":"\nairbnb_data_selected$price %>% head()\n#> [1] \"$469.00\" \"$96.00\"  \"$64.00\"  \"$70.00\"  \"$45.00\" \n#> [6] \"$127.00\"\n\n# First work out what is going on\nairbnb_data_selected$price %>% str_split(\"\") %>% unlist() %>% unique()\n#>  [1] \"$\" \"4\" \"6\" \"9\" \".\" \"0\" \"7\" \"5\" \"1\" \"2\" \"3\" \"8\" \",\"\n# It's clear that '$' needs to go. The only odd thing is ',' so take a look at those:\nairbnb_data_selected %>% \n  select(price) %>% \n  filter(str_detect(price, \",\"))\n#> # A tibble: 145 × 1\n#>    price    \n#>    <chr>    \n#>  1 $1,724.00\n#>  2 $1,000.00\n#>  3 $1,100.00\n#>  4 $1,450.00\n#>  5 $1,019.00\n#>  6 $1,000.00\n#>  7 $1,300.00\n#>  8 $2,142.00\n#>  9 $2,000.00\n#> 10 $1,200.00\n#> # … with 135 more rows\n# It's clear that the data is just nicely formatted, but we need to remove the comma:\nairbnb_data_selected <- \n  airbnb_data_selected %>% \n  mutate(price = str_remove(price, \"\\\\$\"),\n         price = str_remove(price, \",\"),\n         price = as.integer(price)\n         )\n# Look at distribution of price\nairbnb_data_selected %>%\n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 10) +\n  theme_classic() +\n  labs(x = \"Price per night\",\n       y = \"Number of properties\")\n#  We use bins with a width of 10, so that's going to aggregate prices into 10s so that we don't get overwhelmed with bars.\n# Look at distribution of high prices\nairbnb_data_selected %>%\n  filter(price > 1000) %>% \n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 10) +\n  theme_classic() +\n  labs(x = \"Price per night\",\n       y = \"Number of properties\")\nairbnb_data_selected %>%\n  filter(price > 4999)\n#> # A tibble: 11 × 21\n#>      host_id host_since host_response_time host_is_superhost\n#>        <dbl> <date>     <chr>              <lgl>            \n#>  1   9310264 2013-10-08 N/A                FALSE            \n#>  2  99076885 2016-10-10 N/A                FALSE            \n#>  3 119693302 2017-03-07 N/A                FALSE            \n#>  4 147240941 2017-08-22 N/A                FALSE            \n#>  5 174625477 2018-02-21 N/A                FALSE            \n#>  6  70349386 2016-05-04 N/A                FALSE            \n#>  7 215966560 2018-09-17 N/A                FALSE            \n#>  8  12931053 2014-03-08 within a few hours TRUE             \n#>  9 116137780 2017-02-12 N/A                FALSE            \n#> 10 113826425 2017-01-29 within a few hours FALSE            \n#> 11 184607983 2018-04-16 a few days or more FALSE            \n#> # … with 17 more variables: host_listings_count <dbl>,\n#> #   host_total_listings_count <dbl>,\n#> #   host_neighbourhood <chr>, neighbourhood_cleansed <chr>,\n#> #   room_type <chr>, bathrooms <lgl>, bedrooms <dbl>,\n#> #   price <int>, number_of_reviews <dbl>,\n#> #   has_availability <lgl>, review_scores_rating <dbl>,\n#> #   review_scores_accuracy <dbl>, …\n# It's pretty clear that there is something odd going on with some of these, but some of them seem legit.\nairbnb_data_selected %>%\n  filter(price < 1000) %>% \n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 10) +\n  theme_classic() +\n  labs(x = \"Price per night\",\n       y = \"Number of properties\")\n# Look at distribution of price again\nairbnb_data_selected %>%\n  filter(price > 90) %>% \n  filter(price < 210) %>% \n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 1) +\n  theme_classic() +\n  labs(x = \"Price per night\",\n       y = \"Number of properties\")"},{"path":"exploratory-data-analysis.html","id":"superhosts","chapter":"14 Exploratory data analysis","heading":"14.4.4.2 Superhosts","text":"Airbnb says :Superhosts experienced hosts provide shining example hosts, extraordinary experiences guests.host reaches Superhost status, badge superhost badge automatically appear listing profile help identify .check Superhosts’ activity four times year, ensure program highlights people dedicated providing outstanding hospitality.First ’ll look NAs host_is_superhost.285 ’s clear something odd going - maybe host removed listing similar?’ll also want create binary variable . ’s true/false moment, fine modelling, handful situations ’ll easier 0/1.","code":"\nairbnb_data_selected %>%\n  filter(is.na(host_is_superhost))\n#> # A tibble: 11 × 21\n#>      host_id host_since host_response_time host_is_superhost\n#>        <dbl> <date>     <chr>              <lgl>            \n#>  1  23472830 NA         <NA>               NA               \n#>  2  31675651 NA         <NA>               NA               \n#>  3  75779190 NA         <NA>               NA               \n#>  4  47614482 NA         <NA>               NA               \n#>  5 201103629 NA         <NA>               NA               \n#>  6 111820893 NA         <NA>               NA               \n#>  7  23472830 NA         <NA>               NA               \n#>  8 196269219 NA         <NA>               NA               \n#>  9  23472830 NA         <NA>               NA               \n#> 10 266594170 NA         <NA>               NA               \n#> 11 118516038 NA         <NA>               NA               \n#> # … with 17 more variables: host_listings_count <dbl>,\n#> #   host_total_listings_count <dbl>,\n#> #   host_neighbourhood <chr>, neighbourhood_cleansed <chr>,\n#> #   room_type <chr>, bathrooms <lgl>, bedrooms <dbl>,\n#> #   price <int>, number_of_reviews <dbl>,\n#> #   has_availability <lgl>, review_scores_rating <dbl>,\n#> #   review_scores_accuracy <dbl>, …\nairbnb_data_selected <- \n  airbnb_data_selected %>%\n  mutate(host_is_superhost_binary = case_when(\n    host_is_superhost == TRUE ~ 1,\n    host_is_superhost == FALSE ~ 0,\n    TRUE ~ 999\n    )\n  )\nairbnb_data_selected$host_is_superhost_binary[airbnb_data_selected$host_is_superhost_binary == 999] <- NA"},{"path":"exploratory-data-analysis.html","id":"reviews","chapter":"14 Exploratory data analysis","heading":"14.4.4.3 Reviews","text":"Airbnb says :addition written reviews, guests can submit overall star rating set category star ratings stay.Hosts can view star ratings Progress page, Reviews. Hosts using professional hosting tools can find reviews quality details Performance page, Quality.Guests can give ratings :Overall experience. Overall, stay?Cleanliness. guests feel space clean tidy?Accuracy. accurately listing page represent space? example, guests able find --date info photos listing description.Value. guest feel listing provided good value price?Communication. well communicate stay? Guests often care host responds quickly, reliably, frequently messages questions.Check-. smoothly check-go?Location. guests feel neighbourhood? may mean ’s accurate description proximity access transportation, shopping centres, city centre, etc., description includes special considerations, like noise, family safety.Amenities. guests feel amenities available stay? Guests often care amenities listed available, working, good condition.category, hosts able see often get 5 stars, guests rated nearby hosts, , cases, tips help improve listing.number stars displayed top listing page aggregate primary scores guests given listing. bottom listing page, ’s aggregate category rating. host needs receive star ratings least 3 guests aggregate score appears.TODO: don’t understand review scores constructed. Airbnb says ’s star rating, converting 10 point scale, similarly, constructing overall one, seems 100? ’s lot clumping around 20, 40, 60, 80, 100 - averaging five-star scale rebasing 100?Now ’ll deal NAs review_scores_rating. one complicated lot .Now see ’s just don’t reviews.’s clear almost cases don’t review yet don’t enough reviews. However, ’s large proportion total - almost fifth properties don’t reviews (hence NA review_scores_rating).can use vis_miss visdat package (Tierney 2017) make sure components review missing. NAs driven Airbnb requirement least three reviews expect missing.looks pretty convincing almost cases, different variants reviews missing. let’s just focus main review.’s pretty clear almost reviews 80. Let’s just zoom 60 80 range check distribution looks like range.","code":"\nairbnb_data_selected %>%\n  filter(is.na(review_scores_rating))\n#> # A tibble: 4,368 × 22\n#>    host_id host_since host_response_time host_is_superhost\n#>      <dbl> <date>     <chr>              <lgl>            \n#>  1   48239 2009-10-25 N/A                FALSE            \n#>  2  187320 2010-08-01 within a few hours TRUE             \n#>  3  188183 2010-08-01 a few days or more FALSE            \n#>  4  187320 2010-08-01 within a few hours TRUE             \n#>  5  304551 2010-11-29 within an hour     TRUE             \n#>  6  545074 2011-04-29 N/A                FALSE            \n#>  7 1210571 2011-09-26 N/A                FALSE            \n#>  8 1411076 2011-11-15 N/A                FALSE            \n#>  9 1409872 2011-11-15 N/A                FALSE            \n#> 10 1664812 2012-01-28 N/A                FALSE            \n#> # … with 4,358 more rows, and 18 more variables:\n#> #   host_listings_count <dbl>,\n#> #   host_total_listings_count <dbl>,\n#> #   host_neighbourhood <chr>, neighbourhood_cleansed <chr>,\n#> #   room_type <chr>, bathrooms <lgl>, bedrooms <dbl>,\n#> #   price <int>, number_of_reviews <dbl>,\n#> #   has_availability <lgl>, review_scores_rating <dbl>, …\nairbnb_data_selected %>%\n  filter(is.na(review_scores_rating)) %>% \n  select(number_of_reviews) %>% \n  table()\n#> .\n#>    0    1    2    3    4 \n#> 4105  227   23   10    3\n# We'll just check whether this is the same for all of the different variants of reviews\nairbnb_data_selected %>% \n  select(review_scores_rating, \n         review_scores_accuracy, \n         review_scores_cleanliness, \n         review_scores_checkin, \n         review_scores_communication, \n         review_scores_location, \n         review_scores_value) %>% \n  vis_miss()\nairbnb_data_selected %>%\n  filter(!is.na(review_scores_rating)) %>% \n  ggplot(aes(x = review_scores_rating)) +\n  geom_histogram(binwidth = 1) +\n  theme_classic() +\n  labs(x = \"Average review score\",\n       y = \"Number of properties\")\nairbnb_data_selected %>%\n  filter(!is.na(review_scores_rating)) %>% \n  filter(review_scores_rating > 60) %>%\n  filter(review_scores_rating < 80) %>% \n  ggplot(aes(x = review_scores_rating)) +\n  geom_histogram(binwidth = 1) +\n  theme_classic() +\n  labs(x = \"Average review score\",\n       y = \"Number of properties\")"},{"path":"exploratory-data-analysis.html","id":"response-time","chapter":"14 Exploratory data analysis","heading":"14.4.4.4 Response time","text":"Airbnb says :Hosts 24 hours officially accept decline reservation requests. ’ll updated email status request.half reservation requests accepted within one hour received. vast majority hosts reply within 12 hours.host confirms request, payment processed collected Airbnb full. host declines request request expires, don’t process payment.TODO: don’t understand can get response time NA? must related variable.Looking now response time:Interestingly seems like looks like ‘NAs’ host_response_time variable coded proper NAs, instead treated another category. ’ll recode actual NAs.clearly issues NAs. probably want filter away example ’s just quick example, awful lot (20 per cent) ’ll quick look relation review score.seem awful lot overall review 100. also awful lot review score NA.","code":"\ntable(airbnb_data_selected$host_response_time)\n#> \n#> a few days or more                N/A       within a day \n#>                816               8469               1235 \n#> within a few hours     within an hour \n#>               2062               5672\nairbnb_data_selected$host_response_time[airbnb_data_selected$host_response_time == \"N/A\"] <- NA\nairbnb_data_selected %>% \n  filter(is.na(host_response_time)) %>% \n  ggplot(aes(x = review_scores_rating)) +\n  geom_histogram(binwidth = 1)\n#> Warning: Removed 2590 rows containing non-finite values\n#> (stat_bin).\nairbnb_data_selected %>% \n  filter(is.na(host_response_time)) %>%\n  filter(is.na(review_scores_rating))\n#> # A tibble: 2,590 × 22\n#>    host_id host_since host_response_time host_is_superhost\n#>      <dbl> <date>     <chr>              <lgl>            \n#>  1   48239 2009-10-25 <NA>               FALSE            \n#>  2  545074 2011-04-29 <NA>               FALSE            \n#>  3 1210571 2011-09-26 <NA>               FALSE            \n#>  4 1411076 2011-11-15 <NA>               FALSE            \n#>  5 1409872 2011-11-15 <NA>               FALSE            \n#>  6 1664812 2012-01-28 <NA>               FALSE            \n#>  7 1828773 2012-02-28 <NA>               FALSE            \n#>  8 1923052 2012-03-14 <NA>               FALSE            \n#>  9 2432916 2012-05-22 <NA>               FALSE            \n#> 10 2577688 2012-06-07 <NA>               FALSE            \n#> # … with 2,580 more rows, and 18 more variables:\n#> #   host_listings_count <dbl>,\n#> #   host_total_listings_count <dbl>,\n#> #   host_neighbourhood <chr>, neighbourhood_cleansed <chr>,\n#> #   room_type <chr>, bathrooms <lgl>, bedrooms <dbl>,\n#> #   price <int>, number_of_reviews <dbl>,\n#> #   has_availability <lgl>, review_scores_rating <dbl>, …"},{"path":"exploratory-data-analysis.html","id":"host-number-of-listings","chapter":"14 Exploratory data analysis","heading":"14.4.4.5 Host number of listings","text":"two versions variable telling many properties host Airbnb, start just check whether ’s difference.none dataset can just remove one column now quick look one.large number somewhere 2-10 properties range, usual long tail. number 0 listings unexpected worth following . bunch NA ’ll need deal .’s nothing immediately jumps odd people zero listings, must something going .Based dataset, ’s third way looking number properties someone ’s look number times unique ID occurs.makes clear many multiple properties listed.","code":"\nairbnb_data_selected %>% \n  mutate(listings_count_is_same = if_else(host_listings_count == host_total_listings_count, 1, 0)) %>% \n  filter(listings_count_is_same == 0)\n#> # A tibble: 0 × 23\n#> # … with 23 variables: host_id <dbl>, host_since <date>,\n#> #   host_response_time <chr>, host_is_superhost <lgl>,\n#> #   host_listings_count <dbl>,\n#> #   host_total_listings_count <dbl>,\n#> #   host_neighbourhood <chr>, neighbourhood_cleansed <chr>,\n#> #   room_type <chr>, bathrooms <lgl>, bedrooms <dbl>,\n#> #   price <int>, number_of_reviews <dbl>, …\nairbnb_data_selected <- \n  airbnb_data_selected %>% \n  select(-host_listings_count)\n\nairbnb_data_selected %>% \n  count(host_total_listings_count)\n#> # A tibble: 49 × 2\n#>    host_total_listings_count     n\n#>                        <dbl> <int>\n#>  1                         0  2128\n#>  2                         1  7662\n#>  3                         2  2476\n#>  4                         3  1384\n#>  5                         4   802\n#>  6                         5   478\n#>  7                         6   385\n#>  8                         7   270\n#>  9                         8   291\n#> 10                         9   170\n#> # … with 39 more rows\nairbnb_data_selected %>% \n  filter(host_total_listings_count == 0) %>% \n  head()\n#> # A tibble: 6 × 21\n#>   host_id host_since host_response_time host_is_superhost\n#>     <dbl> <date>     <chr>              <lgl>            \n#> 1  140602 2010-06-08 <NA>               FALSE            \n#> 2 1024550 2011-08-26 <NA>               FALSE            \n#> 3 2647656 2012-06-15 <NA>               FALSE            \n#> 4 3783106 2012-10-06 within an hour     FALSE            \n#> 5 3814089 2012-10-09 within an hour     FALSE            \n#> 6 3827668 2012-10-10 within a day       FALSE            \n#> # … with 17 more variables:\n#> #   host_total_listings_count <dbl>,\n#> #   host_neighbourhood <chr>, neighbourhood_cleansed <chr>,\n#> #   room_type <chr>, bathrooms <lgl>, bedrooms <dbl>,\n#> #   price <int>, number_of_reviews <dbl>,\n#> #   has_availability <lgl>, review_scores_rating <dbl>,\n#> #   review_scores_accuracy <dbl>, …\nairbnb_data_selected %>% \n  count(host_id) %>% \n  arrange(-n) %>% \n  head()\n#> # A tibble: 6 × 2\n#>     host_id     n\n#>       <dbl> <int>\n#> 1  10202618    74\n#> 2   1919294    63\n#> 3 152088065    59\n#> 4 293274089    56\n#> 5    785826    54\n#> 6    846505    46"},{"path":"exploratory-data-analysis.html","id":"decisions","chapter":"14 Exploratory data analysis","heading":"14.4.4.6 Decisions","text":"purpose document just give quick introduction using real-world data, ’ll just remove anything annoying, ’re using research ’d need justify decisions /possibly make different ones.Get rid prices $999.Get rid anyone NA whether super host.Get rid anyone NA main review score - removes roughly 20 per cent observations.Get rid anyone main review score less 70.Get rid anyone NA response time - removes roughly another 20 per cent observations.TODO: don’t next step ’ve already got rid point. ’s something systematic going come back look .Get rid anyone NA number properties.Get rid anyone 100 review_scores_rating.keep people one property:","code":"\nairbnb_data_filtered <- \n  airbnb_data_selected %>% \n  filter(price < 1000)\ndim(airbnb_data_filtered)\n#> [1] 18120    21\n# Just remove host_is_superhost NAs for now.\nairbnb_data_filtered <- \n  airbnb_data_filtered %>%\n  filter(!is.na(host_is_superhost))\ndim(airbnb_data_filtered)\n#> [1] 18109    21\n# We'll just get rid of them for now, but this is probably something that deserves more attention - possibly in an appendix or similar.\nairbnb_data_filtered <- \n  airbnb_data_filtered %>%\n  filter(!is.na(review_scores_rating))\n# There are still some where the rest of the reviews are missing even though there is a main review score\n# There seem to be an awful lot that have an overall review of 100. Does that make sense?\ndim(airbnb_data_filtered)\n#> [1] 13801    21\n# We'll just get rid of them for now, but this is probably something that deserves more attention - possibly in an appendix or similar.\nairbnb_data_filtered <- \n  airbnb_data_filtered %>%\n  filter(review_scores_rating > 69)\n# There are still some where the rest of the reviews are missing even though there is a main review score\n# There seem to be an awful lot that have an overall review of 100. Does that make sense?\ndim(airbnb_data_filtered)\n#> [1] 13471    21\nairbnb_data_filtered <- \n  airbnb_data_filtered %>% \n  filter(!is.na(host_response_time))\ndim(airbnb_data_filtered)\n#> [1] 7763   21\nairbnb_data_filtered <- \n  airbnb_data_filtered %>% \n  filter(!is.na(host_total_listings_count))\ndim(airbnb_data_filtered)\n#> [1] 7763   21\nairbnb_data_filtered <- \n  airbnb_data_filtered %>% \n  filter(review_scores_rating != 100)\ndim(airbnb_data_filtered)\n#> [1] 5648   21\nairbnb_data_filtered <- \n  airbnb_data_filtered %>% \n  add_count(host_id) %>% \n  filter(n == 1) %>% \n  select(-n)\ndim(airbnb_data_filtered)\n#> [1] 2304   21"},{"path":"exploratory-data-analysis.html","id":"explore-data","chapter":"14 Exploratory data analysis","heading":"14.4.5 Explore data","text":"might like make graphs see can relationships jump . aspects come mind looking prices reviews super hosts, number properties neighbourhood.","code":""},{"path":"exploratory-data-analysis.html","id":"price-and-reviews","chapter":"14 Exploratory data analysis","heading":"14.4.5.1 Price and reviews","text":"Look relationship price reviews, whether super-host.","code":"\n# Look at both price and reviews\nairbnb_data_filtered %>%\n  ggplot(aes(x = price, y = review_scores_rating, color = host_is_superhost)) +\n  geom_point(size = 1, alpha = 0.1) + # Make the points smaller and more transparent as they overlap considerably.\n  theme_classic() +\n  labs(x = \"Price per night\",\n       y = \"Average review score\",\n       color = \"Super host\") + # Probably should recode this to more meaningful than TRUE/FALSE.\n  scale_color_brewer(palette = \"Set1\")"},{"path":"exploratory-data-analysis.html","id":"superhost-and-response-time","chapter":"14 Exploratory data analysis","heading":"14.4.5.2 Superhost and response-time","text":"One aspects may make someone super host quickly respond enquiries. One imagine superhost involves quickly saying yes enquiries. Let’s look data. First, want look possible values superhost response times.Fortunately, looks like removed reviews rows removed NAs whether super host, go back look may need check . tabyl function within janitor package (Firke, 2020) list NAs , case don’t trust , another way check try filter just NAs.Now let’s look response time.vast majority respond within hour.Finally, can look cross-tab.someone doesn’t respond within hour ’s unlikely super host.","code":"\nairbnb_data_filtered %>% \n  tabyl(host_is_superhost) %>% \n  adorn_totals(\"row\") %>% \n  adorn_pct_formatting()\n#>  host_is_superhost    n percent\n#>              FALSE 1224   53.1%\n#>               TRUE 1080   46.9%\n#>              Total 2304  100.0%\nairbnb_data_filtered %>% \n  filter(is.na(host_is_superhost))\n#> # A tibble: 0 × 21\n#> # … with 21 variables: host_id <dbl>, host_since <date>,\n#> #   host_response_time <chr>, host_is_superhost <lgl>,\n#> #   host_total_listings_count <dbl>,\n#> #   host_neighbourhood <chr>, neighbourhood_cleansed <chr>,\n#> #   room_type <chr>, bathrooms <lgl>, bedrooms <dbl>,\n#> #   price <int>, number_of_reviews <dbl>,\n#> #   has_availability <lgl>, review_scores_rating <dbl>, …\nairbnb_data_filtered %>% \n  tabyl(host_response_time) %>% \n  adorn_totals(\"row\") %>% \n  adorn_pct_formatting()\n#>  host_response_time    n percent\n#>  a few days or more  167    7.2%\n#>        within a day  378   16.4%\n#>  within a few hours  519   22.5%\n#>      within an hour 1240   53.8%\n#>               Total 2304  100.0%\nairbnb_data_filtered %>% \n  tabyl(host_response_time, host_is_superhost) %>% \n  adorn_percentages(\"row\") %>%\n  adorn_pct_formatting(digits = 0) %>%\n  adorn_ns() %>% \n  adorn_title()\n#>                     host_is_superhost          \n#>  host_response_time             FALSE      TRUE\n#>  a few days or more         88% (147) 12%  (20)\n#>        within a day         67% (254) 33% (124)\n#>  within a few hours         51% (266) 49% (253)\n#>      within an hour         45% (557) 55% (683)"},{"path":"exploratory-data-analysis.html","id":"neighbourhood","chapter":"14 Exploratory data analysis","heading":"14.4.5.3 Neighbourhood","text":"Finally, let’s look neighbourhood. data provider attempted clean neighbourhood variable us, ’ll just use now. analysis properly, ’d need check whether ’d made mistakes.","code":"\n# We expect something in the order of 100 to 150 neighbourhoods, with the top ten accounting for a large majority of listings.\nairbnb_data_filtered %>% \n  tabyl(neighbourhood_cleansed) %>% \n  adorn_totals(\"row\") %>% \n  adorn_pct_formatting() %>% \n  nrow()\n#> [1] 140\nairbnb_data_filtered %>% \n  tabyl(neighbourhood_cleansed) %>% \n  adorn_pct_formatting() %>% \n  arrange(-n) %>% \n  filter(n > 100) %>% \n  adorn_totals(\"row\") %>% \n  head()\n#>             neighbourhood_cleansed   n percent\n#>  Waterfront Communities-The Island 409   17.8%\n#>                            Niagara 101    4.4%\n#>                              Total 510       -"},{"path":"exploratory-data-analysis.html","id":"model-data","chapter":"14 Exploratory data analysis","heading":"14.4.6 Model data","text":"now run models dataset. first split data test/training groups, using functions tidymodels package (Kuhn Wickham 2020) (like tidyverse package (Wickham et al. 2019a) package packages).","code":"\nset.seed(853)\n\nairbnb_data_filtered_split <- \n  airbnb_data_filtered %>%\n  initial_split(prop = 3/4)\n\nairbnb_train <- training(airbnb_data_filtered_split)\nairbnb_test <- testing(airbnb_data_filtered_split)\n\nrm(airbnb_data_filtered_split)"},{"path":"exploratory-data-analysis.html","id":"logistic-regression-1","chapter":"14 Exploratory data analysis","heading":"14.4.6.1 Logistic regression","text":"may like look whether can forecast whether someone super host, factors go explaining . dependent variable binary, good opportunity look logistic regression. expect better reviews associated faster responses higher reviews. Specifically, model estimate :\\[\\mbox{Prob(super host} = 1) = \\beta_0 + \\beta_1 \\mbox{Response time} + \\beta_2 \\mbox{Reviews} + \\epsilon\\]estimate model using glm R language (R Core Team 2021).can quick look results, instance, summary function. also use tidy glance broom package (D. Robinson, Hayes, Couch 2020). details , broom functions tibbles, means can easily deal within tidy framework.might like look model predicts, compared whether person actually super host. can variety ways, one way use augment broom package (D. Robinson, Hayes, Couch 2020). add prediction associated uncertainty data. every row probability model estimating superhost. ultimately, need binary forecast. bunch different options, one just say model estimates probability 0.5 bin superhost, .can look far model . bunch ways , one look probability model given person.can look model probabilities change based average review score, average time respond.nice thing graph illustrates nicely effect host average response time , say, ‘within hour’ compared ‘within hours.’can focus model terms raw classification using confusionMatrix caret package (Kuhn, 2020). also gives bunch diagnostics (help file explains ). general, suggest isn’t best model ’s ever existed.case, point ’ve looking model done training set. ’s also relevant test set. , bunch ways , one use augment function, include newdata argument.expect performance slightly worse test set. ’s actually fairly similar.compare test training sets terms forecasts.","code":"\nlogistic_reg_superhost_response_review <- glm(host_is_superhost ~ \n                                                host_response_time + \n                                                review_scores_rating,\n                                              data = airbnb_train,\n                                              family = binomial\n                                              )\nsummary(logistic_reg_superhost_response_review)\n#> \n#> Call:\n#> glm(formula = host_is_superhost ~ host_response_time + review_scores_rating, \n#>     family = binomial, data = airbnb_train)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -1.9297  -0.8873  -0.0416   0.8310   4.0657  \n#> \n#> Coefficients:\n#>                                      Estimate Std. Error\n#> (Intercept)                          -40.5578     2.4208\n#> host_response_timewithin a day         1.5351     0.3466\n#> host_response_timewithin a few hours   1.9520     0.3360\n#> host_response_timewithin an hour       2.2883     0.3240\n#> review_scores_rating                   0.4037     0.0248\n#>                                      z value Pr(>|z|)    \n#> (Intercept)                          -16.754  < 2e-16 ***\n#> host_response_timewithin a day         4.429 9.46e-06 ***\n#> host_response_timewithin a few hours   5.809 6.28e-09 ***\n#> host_response_timewithin an hour       7.062 1.65e-12 ***\n#> review_scores_rating                  16.276  < 2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 2392.2  on 1727  degrees of freedom\n#> Residual deviance: 1750.0  on 1723  degrees of freedom\n#> AIC: 1760\n#> \n#> Number of Fisher Scoring iterations: 6\ntidy(logistic_reg_superhost_response_review)\n#> # A tibble: 5 × 5\n#>   term                 estimate std.error statistic  p.value\n#>   <chr>                   <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)           -40.6      2.42      -16.8  5.31e-63\n#> 2 host_response_timew…    1.54     0.347       4.43 9.46e- 6\n#> 3 host_response_timew…    1.95     0.336       5.81 6.28e- 9\n#> 4 host_response_timew…    2.29     0.324       7.06 1.65e-12\n#> 5 review_scores_rating    0.404    0.0248     16.3  1.45e-59\nglance(logistic_reg_superhost_response_review)\n#> # A tibble: 1 × 8\n#>   null.deviance df.null logLik   AIC   BIC deviance\n#>           <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>\n#> 1         2392.    1727  -875. 1760. 1787.    1750.\n#> # … with 2 more variables: df.residual <int>, nobs <int>\nairbnb_data_filtered_logistic_fit_train <- \n  augment(logistic_reg_superhost_response_review, \n          data = airbnb_train %>% select(host_is_superhost, \n                                         host_is_superhost_binary,\n                                         host_response_time,\n                                         review_scores_rating\n                                         ),\n          type.predict = \"response\") %>% # We use the \"response\" option here so that the function does the work of worrying about the exponential and log odds for us. Our result will be a probability.\n  select(-.hat, -.sigma, -.cooksd, -.std.resid) %>% \n  mutate(predict_host_is_superhost = if_else(.fitted > 0.5, 1, 0), # How do things change if we change the 0.5 cutoff?\n         host_is_superhost_binary = as.factor(host_is_superhost_binary),\n         predict_host_is_superhost_binary = as.factor(predict_host_is_superhost)\n         )\nairbnb_data_filtered_logistic_fit_train %>% \n  ggplot(aes(x = .fitted, fill = host_is_superhost_binary)) +\n  geom_histogram(binwidth = 0.05, position = \"dodge\") +\n  theme_classic() +\n  labs(x = \"Estimated probability that host is super host\",\n       y = \"Number\",\n       fill = \"Host is super host\") +\n  scale_fill_brewer(palette = \"Set1\")\nggplot(airbnb_data_filtered_logistic_fit_train, \n       aes(x = review_scores_rating, \n           y = .fitted, \n           color = host_response_time)) +\n  geom_line() +\n  geom_point() +\n  labs(x = \"Average review score\",\n       y = \"Predicted probability of being a superhost\",\n       color = \"Host response time\") +\n  theme_classic() +\n  scale_color_brewer(palette = \"Set1\")\ncaret::confusionMatrix(data = airbnb_data_filtered_logistic_fit_train$predict_host_is_superhost_binary,\n                       reference = airbnb_data_filtered_logistic_fit_train$host_is_superhost_binary)\n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction   0   1\n#>          0 619 124\n#>          1 283 702\n#>                                           \n#>                Accuracy : 0.7645          \n#>                  95% CI : (0.7437, 0.7843)\n#>     No Information Rate : 0.522           \n#>     P-Value [Acc > NIR] : < 2.2e-16       \n#>                                           \n#>                   Kappa : 0.5318          \n#>                                           \n#>  Mcnemar's Test P-Value : 4.811e-15       \n#>                                           \n#>             Sensitivity : 0.6863          \n#>             Specificity : 0.8499          \n#>          Pos Pred Value : 0.8331          \n#>          Neg Pred Value : 0.7127          \n#>              Prevalence : 0.5220          \n#>          Detection Rate : 0.3582          \n#>    Detection Prevalence : 0.4300          \n#>       Balanced Accuracy : 0.7681          \n#>                                           \n#>        'Positive' Class : 0               \n#> \nairbnb_data_filtered_logistic_fit_test <- \n  augment(logistic_reg_superhost_response_review, \n          data = airbnb_train %>% select(host_is_superhost, \n                                         host_is_superhost_binary,\n                                         host_response_time,\n                                         review_scores_rating\n                                         ),\n          newdata = airbnb_test %>% select(host_is_superhost, \n                                         host_is_superhost_binary,\n                                         host_response_time,\n                                         review_scores_rating\n                                         ), # I'm selecting just because the\n          # dataset is quite wide, and so this makes it easier to look at.\n          type.predict = \"response\") %>% \n  mutate(predict_host_is_superhost = if_else(.fitted > 0.5, 1, 0), \n         host_is_superhost_binary = as.factor(host_is_superhost_binary),\n         predict_host_is_superhost_binary = as.factor(predict_host_is_superhost)\n         )\ncaret::confusionMatrix(data = airbnb_data_filtered_logistic_fit_test$predict_host_is_superhost_binary,\n                       reference = airbnb_data_filtered_logistic_fit_test$host_is_superhost_binary)\n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction   0   1\n#>          0 197  36\n#>          1 125 218\n#>                                           \n#>                Accuracy : 0.7205          \n#>                  95% CI : (0.6819, 0.7568)\n#>     No Information Rate : 0.559           \n#>     P-Value [Acc > NIR] : 1.018e-15       \n#>                                           \n#>                   Kappa : 0.4533          \n#>                                           \n#>  Mcnemar's Test P-Value : 4.052e-12       \n#>                                           \n#>             Sensitivity : 0.6118          \n#>             Specificity : 0.8583          \n#>          Pos Pred Value : 0.8455          \n#>          Neg Pred Value : 0.6356          \n#>              Prevalence : 0.5590          \n#>          Detection Rate : 0.3420          \n#>    Detection Prevalence : 0.4045          \n#>       Balanced Accuracy : 0.7350          \n#>                                           \n#>        'Positive' Class : 0               \n#> \ntraining <- airbnb_data_filtered_logistic_fit_train %>% \n  select(host_is_superhost_binary, .fitted) %>% \n  mutate(type = \"Training set\")\n\ntest <- airbnb_data_filtered_logistic_fit_test %>% \n  select(host_is_superhost_binary, .fitted) %>% \n  mutate(type = \"Test set\")\n\nboth <- rbind(training, test)\nrm(training, test)\n\nboth %>% \n  ggplot(aes(x = .fitted, \n             fill = host_is_superhost_binary)) +\n  geom_histogram(binwidth = 0.05, position = \"dodge\") +\n  theme_minimal() +\n  labs(x = \"Estimated probability that host is super host\",\n       y = \"Number\",\n       fill = \"Host is super host\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  facet_wrap(vars(type),\n             nrow = 2,\n             scales = \"free_y\")"},{"path":"exploratory-data-analysis.html","id":"exercises-and-tutorial-13","chapter":"14 Exploratory data analysis","heading":"14.5 Exercises and tutorial","text":"","code":""},{"path":"exploratory-data-analysis.html","id":"exercises-13","chapter":"14 Exploratory data analysis","heading":"14.5.1 Exercises","text":"","code":""},{"path":"exploratory-data-analysis.html","id":"tutorial-13","chapter":"14 Exploratory data analysis","heading":"14.5.2 Tutorial","text":"","code":""},{"path":"ijalm.html","id":"ijalm","chapter":"15 It’s Just A Linear Model","heading":"15 It’s Just A Linear Model","text":"STATUS: construction.Required readingGreenland, Sander, Stephen J. Senn, Kenneth J. Rothman, John B. Carlin, Charles Poole, Steven N. Goodman, Douglas G. Altman, 2016, ‘Statistical tests, P values, confidence intervals, power: guide misinterpretations,’ European journal epidemiology, 31, . 4, pp. 337-350.James, Gareth, Daniela Witten, Trevor Hastie Robert Tibshirani, 2017, Introduction Statistical Learning Applications R, 1st Edition, Chapters 3 4.1-4.3., https://www.statlearning.com.Obermeyer, Z., Powers, B., Vogeli, C., & Sendhill, M., 2019, ‘Dissecting racial bias algorithm used manage health populations,’ Science, (366): 447-453.Wickham, Hadley, Garrett Grolemund, 2017, R Data Science, Chapter 23, https://r4ds..co.nz/.Zook M, Barocas S, boyd d, Crawford K, Keller E, Gangadharan SP, et al. (2017) ‘Ten simple rules responsible big data research,’ PLoS Comput Biol 13(3): e1005399. https://doi.org/10.1371/journal.pcbi.1005399Recommended readingAngrist, Joshua D., Jörn-Steffen Pischke, 2008, Mostly harmless econometrics: empiricist’s companion, Princeton University Press, Chapter 3.4.3.Cunningham, Scott, Causal Inference: Mixtape, Chapter 2, Yale University Press, https://mixtape.scunning.com.ElHabr, Tony, 2019, ‘Bayesian Approach Ranking English Premier League Teams (using R),’ https://tonyelhabr.rbind.io/post/bayesian-statistics-english-premier-league/.Ioannidis, John PA, 2005, ‘published research findings false,’ PLoS medicine, 2, . 8, e124.Pavlik, Kaylin, 2018, ‘Exploring Relationship Dog Names Breeds,’ https://www.kaylinpavlik.com/dog-names-tfidf/.Pavlik, Kaylin, 2019, ‘Understanding + classifying genres using Spotify audio features,’ https://www.kaylinpavlik.com/classifying-songs-genres/.Silge, Julia, 2019, ‘Modeling salary gender tech industry,’ https://juliasilge.com/blog/salary-gender/.Silge, Julia, 2019, ‘Opioid prescribing habits Texas,’ https://juliasilge.com/blog/texas-opioids/.Silge, Julia, 2019, ‘Tidymodels,’ https://juliasilge.com/blog/intro-tidymodels/.Silge, Julia, 2020, ‘#TidyTuesday hotel bookings recipes,’ https://juliasilge.com/blog/hotels-recipes/.Silge, Julia, 2020, ‘Hyperparameter tuning #TidyTuesday food consumption,’ https://juliasilge.com/blog/food-hyperparameter-tune/.Taddy, Matt, 2019, Business Data Science, Chapters 2 4.Wasserstein, Ronald L. Nicole . Lazar, 2016, ‘ASA Statement p-Values: Context, Process, Purpose,’ American Statistician, 70:2, 129-133, DOI: 10.1080/00031305.2016.1154108.Fun readingChellel, Kit, 2018, ‘Gambler Cracked Horse-Racing Code,’ Bloomberg Businessweek, 3 May, https://www.bloomberg.com/news/features/2018-05-03/-gambler--cracked--horse-racing-code.Key concepts/skills/etcSimple multiple linear regression.Logistic Poisson regression.key role uncertainty.Threats validity inferencesOverfitting.Key librariesbroomhuxtablerstanarmtidymodelstidyverseKey functionsbroom::augment()broom::glance()broom::tidy()glm()huxtable::huxreg()lm()parsnip::fit()parsnip::linear_reg()parsnip::logistic_reg()parsnip::set_engine()poissonreg::poisson_reg()rnorm()rpois()rsample::initial_split()rsample::testing()rsample::training()sample()set.seed()summary()QuizPlease write linear relationship response variable, Y, predictor, X. intercept term? slope term? adding hat indicate?least squares criterion? Similarly, RSS trying run least squares regression?statistical bias?three variables: Snow, Temperature, Wind, please write R code fit simple linear regression explain Snow function Temperature Wind. think another explanatory variable - daily stock market returns - model?According Greenland et al. (2016), p-values test (pick one)?\nassumptions data generated (entire model), just targeted hypothesis supposed test (null hypothesis).\nWhether hypothesis targeted testing true .\ndichotomy whereby results can declared ‘statistically significant.’\nassumptions data generated (entire model), just targeted hypothesis supposed test (null hypothesis).Whether hypothesis targeted testing true .dichotomy whereby results can declared ‘statistically significant.’According Greenland et al. (2016), p-value may small (select )?\ntargeted hypothesis false.\nstudy protocols violated.\nselected presentation based small size.\ntargeted hypothesis false.study protocols violated.selected presentation based small size.According Obermeyer et al. (2019), racial bias occur algorithm used guide health decisions US (pick one)?\nalgorithm uses health costs proxy health needs.\nalgorithm trained Reddit data.\nalgorithm uses health costs proxy health needs.algorithm trained Reddit data.use logistic regression (pick one)?\nContinuous dependent variable.\nBinary dependent variable.\nCount dependent variable.\nContinuous dependent variable.Binary dependent variable.Count dependent variable.interested studying voting intentions recent US presidential election vary individual’s income. set logistic regression model study relationship. study, one possible dependent variable (pick one)?\nWhether respondent US citizen (yes/)\nrespondent’s personal income (high/low)\nWhether respondent going vote Trump (yes/)\nrespondent voted 2016 (Trump/Clinton)\nWhether respondent US citizen (yes/)respondent’s personal income (high/low)Whether respondent going vote Trump (yes/)respondent voted 2016 (Trump/Clinton)interested studying voting intentions recent US presidential election vary individual’s income. set logistic regression model study relationship. study, one possible dependent variable (pick one)?\nrace respondent (white/white)\nrespondent’s marital status (married/)\nWhether respondent registered vote (yes/)\nWhether respondent going vote Biden (yes/)\nrace respondent (white/white)respondent’s marital status (married/)Whether respondent registered vote (yes/)Whether respondent going vote Biden (yes/)Please explain p-value , using term (.e. ‘p-value’) words amongst 1,000 common English language according XKCD Simple Writer - https://xkcd.com/simplewriter/. (Please write one two paragraphs.)mean Poisson distribution equal ?\nMedian.\nStandard deviation.\nVariance.\nMedian.Standard deviation.Variance.","code":""},{"path":"ijalm.html","id":"overview-1","chapter":"15 It’s Just A Linear Model","heading":"15.1 Overview","text":"Words! Mere words! terrible ! clear, vivid, cruel! One escape . yet subtle magic ! seemed able give plastic form formless things, music sweet viol lute. Mere words! anything real words?Oscar Wilde, Picture Dorian Gray.Regression sort . Regression indeed oracle, cruel one. speaks riddles delights punishing us asking bad questions.McElreath (2020, 162).Linear models around long time, least since Galton many others (eugenicists) used linear regression earnest. generalized linear model framework came , formal sense, 70s seminal folks Nelder Wedderburn (Nelder Wedderburn 1972). idea generalized linear models broaden types outcomes allowed. ’re still modelling things linear function, ’re constrained outcome normally distributed. outcome can anything exponential family. , well, generalization generalized linear models generalized additive models ’re generalizing anything outcome, instead structure explanatory side, . ’re still explaining dependent variable additive function bits, bits can functions. framework, way, came 90s, Hastie Tibshirani (Hastie Tibshirani 1990) (fun fact, Tibshirani stats masters Toronto, professor 1985 1998!).’s important recognise build models discovering ‘truth.’ using model help us explore understand data . one best model, just useful models help us learn something data hence, hopefully, something world data generated. Ben Rhodes, Obama staffer, titled White House memoirs ‘World : Memoir Obama White House.’ use models, similarly trying understand world, second part title makes clear, enormous constraints perspective. way ’d expect Rhodes advocate Australian, Canadian, even US Republican, perspective world, ’s silly expect one model universal.use models understand world. poke, push, test . build rejoice beauty, seek understand limits ultimately destroy . process important, process allows us better understand world. McElreath (2020, 19) talks small large worlds, saying ‘()ll statistical modeling two frames: small world model large world hope deploy model .’ extent model trained experiences straight, cis, men, speak world ? ’s worthless, ’s also unimpeachable. extent model teach us data ? extent data reflect world like draw conclusions? Keep questions front mind.Much statistics developed vacuum. ’s reasonable developed situations X, Y Z. original statisticians literally able randomise order fields planting literally worked agricultural stations (CITE). However, almost subsequent applications properties. often teach undergraduates science proceeds (ADD POINTS NULL HYPOTHESIS POPPER). believe ’s works, bridge sell . Scientists react incentives. dabble, guess, test, follow guesses backfill. apply grant funding things last time (know ’ll work) spend money conduct things. fine. ’s world traditional null hypothesis holds, means p-values power lose meaning. need understand ‘old world,’ also need sophisticated enough understand need move away .chapter … called ‘’s Just Linear Model’ famous quote Professor Daniela Witten, identifies far can get linear models huge extent underpin statistics.","code":""},{"path":"ijalm.html","id":"simple-linear-regression","chapter":"15 It’s Just A Linear Model","heading":"15.2 Simple linear regression","text":"\nFigure 15.1: Oh .\nSource: Mijke Rhemtulla, 3 March 2020.","code":""},{"path":"ijalm.html","id":"overview-2","chapter":"15 It’s Just A Linear Model","heading":"15.2.1 Overview","text":"two continuous variables use simple linear regression. based Normal (also ‘Gaussian’) distribution. Pitman (1993, 94) ‘normal distribution mean \\(\\mu\\) standard deviation \\(\\sigma\\) distribution x-axis defined areas normal curve parameters. equation normal curve parameters \\(\\mu\\) \\(\\sigma\\), can written :\n\\[y = \\frac{1}{\\sqrt{2\\pi\\sigma}}e^{-\\frac{1}{2}z^2},\\]\n\\(z = (x - \\mu)/\\sigma\\) measures number standard deviations mean \\(\\mu\\) number \\(x\\).’R can simulate \\(n\\) data points Normal distribution rnorm().take draws get expected shape.use simple linear regression, assume relationship characterised variables parameters, difference, often denoted \\(\\epsilon\\), expectation reality normally distributed.two variables, \\(Y\\) \\(X\\), characterise relationship :\n\\[Y \\sim \\beta_0 + \\beta_1 X.\\]two coefficients/parameters: ‘intercept’ \\(\\beta_0\\), ‘slope’ \\(\\beta_1\\). saying \\(Y\\) value, \\(\\beta_0\\), even \\(X\\) 0, \\(Y\\) change \\(\\beta_1\\) units every one unit change \\(X\\). language use ‘X regressed Y.’may take relationship data relationship order estimate coefficients particular values :\n\\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x.\\]hats used indicate estimated values. saying linear regression assume \\(x\\) doubles \\(y\\) also double. Linear regressions considers average dependent variable changes based independent variables.want focus data, ’ll make example concrete, generating data discussing everything context . example looking someone’s time running five kilometers, compared time running marathon.set-may like use \\(x\\), five-kilometer time, produce estimates \\(y\\), marathon time. involve also estimating values \\(\\beta_0\\) \\(\\beta_1\\), hat .estimate coefficients? Even impose linear relationship lot options (many straight lines can fit piece paper?). clearly fits great.One way may define great impose close possible \\(x\\) \\(y\\) combinations know. lot candidates define ‘close possible,’ one minimise sum least squares. produce estimates \\(\\hat{y}\\) based estimates \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\), given \\(x\\), work ‘wrong,’ every point \\(\\), :\n\\[e_i = y_i - \\hat{y}_i.\\]residual sum squares (RSS) requires summing across points:\n\\[\\mbox{RSS} = e^2_1+ e^2_2 +\\dots + e^2_n.\\]\nresults one ‘linear best-fit’ line, worth thinking assumptions decisions took get us point.least squares criterion want values \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\) result smallest RSS.","code":"\nrnorm(n = 20, mean = 0, sd = 1)\n#>  [1] -1.23802012 -0.26311116 -0.39901585  0.30273494\n#>  [5]  0.50354054 -1.49510337 -0.08219972  0.41609692\n#>  [9]  1.48071041 -0.42138789 -1.68248911 -0.76618528\n#> [13]  0.52963422  1.83700478  1.10022996  1.01064316\n#> [17] -0.73717428 -1.30112624  1.77188159  1.53037932\nlibrary(tidyverse)\nset.seed(853)\ntibble(\n  number_of_draws = c(\n    rep.int(x = \"2 draws\", times = 2),\n    rep.int(x = \"5 draws\", times = 5),\n    rep.int(x = \"10 draws\", times = 10),\n    rep.int(x = \"50 draws\", times = 50),\n    rep.int(x = \"100 draws\", times = 100),\n    rep.int(x = \"500 draws\", times = 500),\n    rep.int(x = \"1,000 draws\", times = 1000),\n    rep.int(x = \"10,000 draws\", times = 10000),\n    rep.int(x = \"100,000 draws\", times = 100000)),\n  draws = c(\n    rnorm(n = 2, mean = 0, sd = 1),\n    rnorm(n = 5, mean = 0, sd = 1),\n    rnorm(n = 10, mean = 0, sd = 1),\n    rnorm(n = 50, mean = 0, sd = 1),\n    rnorm(n = 100, mean = 0, sd = 1),\n    rnorm(n = 500, mean = 0, sd = 1),\n    rnorm(n = 1000, mean = 0, sd = 1),\n    rnorm(n = 10000, mean = 0, sd = 1),\n    rnorm(n = 100000, mean = 0, sd = 1))\n  ) %>% \n  mutate(number_of_draws = as_factor(number_of_draws)) %>% \n  ggplot(aes(x = draws)) +\n  geom_density() +\n  theme_classic() +\n  facet_wrap(vars(number_of_draws),\n             scales = \"free_y\") +\n  labs(x = 'Draw',\n       y = 'Density')\nset.seed(853)\nnumber_of_observations <- 100\nrunning_data <- \n  tibble(five_km_time = rnorm(number_of_observations, 20, 3),\n         noise = rnorm(number_of_observations, 0, 10),\n         marathon_time = five_km_time * 8.4 + noise,\n         was_raining = sample(c(\"Yes\", \"No\"), \n                              size = number_of_observations,\n                              replace = TRUE, \n                              prob = c(0.2, 0.8)) \n         )\n\nrunning_data %>% \n  ggplot(aes(x = five_km_time, y = marathon_time)) +\n  geom_point() +\n  labs(x = \"Five-kilometer time (minutes)\",\n       y = \"Marathon time (minutes)\") +\n  theme_classic()\nrunning_data %>% \n  ggplot(aes(x = five_km_time, y = marathon_time)) +\n  geom_point() + \n  geom_smooth(method = \"lm\", \n              se = FALSE, \n              color = \"black\", \n              linetype = \"dashed\",\n              formula = 'y ~ x') +\n  labs(x = \"Five-kilometer time (minutes)\",\n       y = \"Marathon time (minutes)\") +\n  theme_classic()"},{"path":"ijalm.html","id":"implementation-in-base-r","chapter":"15 It’s Just A Linear Model","heading":"15.2.2 Implementation in base R","text":"Within R, main function linear regression lm. included base R, don’t need call packages, moment, call bunch packages surround lm within environment familiar . specify relationship dependent variable first, ~, independent variables. Finally, specify dataset (pipe usual).general, assign object:see result regression can call summary().first part result tells us regression called, information residuals, estimated coefficients. finally useful diagnostics.considering relationship \\(X\\) \\(Y\\), : \\(Y = f(X) + \\epsilon\\). going say function, \\(f()\\), linear relationship :\n\\[\\hat{Y} = \\beta_0 + \\beta_1 X + \\epsilon.\\]‘true’ relationship \\(X\\) \\(Y\\), don’t know . can use sample data try estimate . understanding depends sample, every possible sample, get slightly different relationship (measured coefficients).\\(\\epsilon\\) measure error - model know? ’s going plenty model doesn’t know, hope error depend \\(X\\), error normally distributed.intercept marathon time expect five-kilometer time 0 minutes. Hopefully example illustrates need carefully interpret intercept coefficient! coefficient five-kilometer run time shows expect marathon time change five-kilometer run time changed one unit. case ’s 8.4, makes sense seeing marathon roughly many times longer five-kilometer run.","code":"\nlm(y ~ x, data = dataset)\nrunning_data_first_model <- \n  lm(marathon_time ~ five_km_time, \n     data = running_data)\nsummary(running_data_first_model)\n#> \n#> Call:\n#> lm(formula = marathon_time ~ five_km_time, data = running_data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -24.763  -5.686   0.722   6.650  16.707 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)    0.4114     6.0610   0.068    0.946    \n#> five_km_time   8.3617     0.3058  27.343   <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 8.474 on 98 degrees of freedom\n#> Multiple R-squared:  0.8841, Adjusted R-squared:  0.8829 \n#> F-statistic: 747.6 on 1 and 98 DF,  p-value: < 2.2e-16"},{"path":"ijalm.html","id":"tidy-up-with-broom","chapter":"15 It’s Just A Linear Model","heading":"15.2.3 Tidy up with broom","text":"nothing wrong base approach, want introduce broom package provide us outputs tidy framework (D. Robinson, Hayes, Couch 2020). three key functions:broom::tidy(): Gives coefficient estimates tidy output.broom::glance(): Gives diagnostics.broom::augment(): Adds forecast values, hence, residuals, dataset.Notice results fairly similar base summary function.now make plots residuals.say estimate unbiased, trying say even though sample estimate might high, another sample estimate might low, eventually lot data estimate population. (pro hockey player may sometimes shoot right net, sometimes left net, ’d hope average ’d right middle net). words James et al. (2017), ‘unbiased estimator systematically - -estimate true parameter.’want try speak ‘true’ relationship, need try capture much think understanding depends particular sample analyse. standard error comes . tells us estimate compared actual.standard errors, can compute confidence interval. 95 per cent confidence interval means 0.95 probability interval happens contain population parameter (typically unknown).bunch different tests can use understand model performing given data. One quick way look whole bunch different aspects use performance package (Lüdecke et al. 2020).","code":"\nlibrary(broom)\ntidy(running_data_first_model)\n#> # A tibble: 2 × 5\n#>   term         estimate std.error statistic  p.value\n#>   <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)     0.411     6.06     0.0679 9.46e- 1\n#> 2 five_km_time    8.36      0.306   27.3    1.17e-47\nglance(running_data_first_model)\n#> # A tibble: 1 × 12\n#>   r.squared adj.r.squared sigma statistic  p.value    df\n#>       <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>\n#> 1     0.884         0.883  8.47      748. 1.17e-47     1\n#> # … with 6 more variables: logLik <dbl>, AIC <dbl>,\n#> #   BIC <dbl>, deviance <dbl>, df.residual <int>,\n#> #   nobs <int>\nrunning_data <- \n  augment(running_data_first_model,\n          data = running_data)\nhead(running_data)\n#> # A tibble: 6 × 10\n#>   five_km_time  noise marathon_time was_raining .fitted\n#>          <dbl>  <dbl>         <dbl> <chr>         <dbl>\n#> 1         18.9 -3.73           155. No             159.\n#> 2         19.9  8.42           175. No             167.\n#> 3         14.7  4.32           127. No             123.\n#> 4         16.6 -2.74           137. No             139.\n#> 5         17.0 -4.89           138. No             142.\n#> 6         25.3  0.648          213. No             212.\n#> # … with 5 more variables: .resid <dbl>, .hat <dbl>,\n#> #   .sigma <dbl>, .cooksd <dbl>, .std.resid <dbl>\nggplot(running_data, \n       aes(x = .resid)) + \n  geom_histogram(binwidth = 1) +\n  theme_classic() +\n  labs(y = \"Number of occurrences\",\n       x = \"Residuals\")\n\nggplot(running_data, aes(five_km_time, .resid)) + \n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dotted\", color = \"grey\") +\n  theme_classic() +\n  labs(y = \"Residuals\",\n       x = \"Five-kilometer time (minutes)\")\nrunning_data %>% \n  ggplot(aes(x = five_km_time, y = marathon_time)) +\n  geom_point() + \n  geom_smooth(method = \"lm\", \n              se = TRUE, \n              color = \"black\", \n              linetype = \"dashed\",\n              formula = 'y ~ x') +\n  labs(x = \"Five-kilometer time (minutes)\",\n       y = \"Marathon time (minutes)\") +\n  theme_classic()\nlibrary(performance)\nperformance::check_model(running_data_first_model)"},{"path":"ijalm.html","id":"testing-hypothesis","chapter":"15 It’s Just A Linear Model","heading":"15.2.4 Testing hypothesis","text":"Now interval can say 95 per cent probability contains true population parameter can test claims. instance, null hypothesis relationship \\(X\\) \\(Y\\) (.e. \\(\\beta_1 = 0\\)), compared alternative hypothesis relationship \\(X\\) \\(Y\\) (.e. \\(\\beta_1 \\neq 0\\)).need know whether estimate \\(\\beta_1\\), \\(\\hat{\\beta}_1\\), ‘far enough’ away zero us comfortable claiming \\(\\beta_1 \\neq 0\\). far ‘far enough?’ confident estimate \\(\\beta_1\\) wouldn’t far, substantial. depends bunch things, essentially standard error \\(\\hat{\\beta}_1\\).compare standard error \\(\\hat{\\beta}_1\\) get t-statistic:\n\\[t = \\frac{\\hat{\\beta}_1 - 0}{\\mbox{SE}(\\hat{\\beta}_1)}.\\]\ncompare t-statistic t-distribution compute probability getting absolute t-statistic larger one, \\(\\beta_1 = 0\\). p-value. small p-value means unlikely observe association due chance wasn’t relationship.","code":""},{"path":"ijalm.html","id":"on-p-values","chapter":"15 It’s Just A Linear Model","heading":"15.2.5 On p-values","text":"p-value specific subtle concept. easy abuse. main issue embodies, assumes correct, every assumption model. Greenland et al. (2016, 339): ‘p-value probability chosen test statistic least large observed value every model assumption correct, including test hypothesis.’ provide background language used case ’re unfamiliar, test hypothesis typically ‘null hypothesis,’ ‘test statistic’ ‘distance data model prediction’ (Greenland et al. 2016).following quote (minor edits consistency ) summarises situation:true smaller p-value, unusual data every single assumption correct; small p-value tell us assumption incorrect. example, p-value may small targeted hypothesis false; may instead (addition) small study protocols violated, selected presentation based small size. Conversely, large p-value indicates data unusual model, imply model aspect (targeted hypothesis) correct; may instead (addition) large () study protocols violated, selected presentation based large size.general definition p-value may help one understand statistical tests tell us much less many think : p-value tell us whether hypothesis targeted testing true ; says nothing specifically related hypothesis unless can completely assured every assumption used computation correct—assurance lacking far many studies.Greenland et al. (2016, 339).nothing inherently wrong using p-values, important use sophisticated thoughtful ways.Typically one application ’s easy see abuse p-values power analysis. Gelman Hill (2007, 438) say, ‘[s]ample size never large enough…. problem… [w]e just emphasizing , just never enough money, perceived needs increase resources, inferential needs increase sample size.’ Power refers probability incorrectly failing reject null hypothesis. Imai (2017, 303) says:use power analysis order formalize degree informativeness data hypothesis tests. power statistical hypothesis test defined one minus probability type II error:power = 1-P(type II error)vacuum, ’d like high power can achieve either really big effect sizes, larger number observations.","code":""},{"path":"ijalm.html","id":"multiple-linear-regression","chapter":"15 It’s Just A Linear Model","heading":"15.3 Multiple linear regression","text":"point ’ve just considered one explanatory variable. ’ll usually one. One approach run separate regressions explanatory variable. compared separate linear regressions , adding explanatory variables allows us better understanding intercept accounts interaction. Often results quite different.slightly counterintuitive result common many real life situations. Consider absurd example illustrate point. Running regression shark attacks versus ice cream sales data collected given beach community period time show positive relationship, similar seen sales newspapers. course one (yet) suggested ice creams banned beaches reduce shark attacks. reality, higher temperatures cause people visit beach, turn results ice cream sales shark attacks. multiple regression attacks versus ice cream sales temperature reveals , intuition implies, former predictor longer significant adjusting temperature.(James et al. 2017, 74).may also like consider variables inherent ordering. instance, pregnant . two options can use binary variable 0 1. two levels use combination binary variables, ‘missing’ outcome (baseline) gets pushed onto intercept.languages may need explicitly construct dummy variables, R designed language statistical programming, lot work fairly forgiving. instance, column character values two values: c(\"Monica\", \"Rohan\", \"Rohan\", \"Monica\", \"Monica\", \"Rohan\"), used independent variable usual regression set R treat dummy variable.result probably isn’t surprising look plot data.addition wanting include additional explanatory variables may think related one another. instance, wanting explain amount snowfall Toronto, may interested humidity temperature, two variables may also interact. can using * instead + specify model R. interact variables, almost always also include individual variables well (Figure 15.2).\nFigure 15.2: Don’t leave main effects interactive model\nSource: Kai Arzheimer, 16 February 2020.","code":"\nrunning_data_rain_model <- \n  lm(marathon_time ~ five_km_time + was_raining, \n     data = running_data)\nsummary(running_data_rain_model)\n#> \n#> Call:\n#> lm(formula = marathon_time ~ five_km_time + was_raining, data = running_data)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -24.6239  -5.5806   0.8377   6.7636  16.8671 \n#> \n#> Coefficients:\n#>                Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)      0.1430     6.1476   0.023    0.981    \n#> five_km_time     8.3689     0.3081  27.166   <2e-16 ***\n#> was_rainingYes   0.7043     2.2220   0.317    0.752    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 8.513 on 97 degrees of freedom\n#> Multiple R-squared:  0.8842, Adjusted R-squared:  0.8818 \n#> F-statistic: 370.4 on 2 and 97 DF,  p-value: < 2.2e-16\nrunning_data %>%\n  ggplot(aes(x = five_km_time, y = marathon_time, color = was_raining)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\", linetype = \"dashed\") +\n  labs(x = \"Five-kilometer time (minutes)\",\n       y = \"Marathon time (minutes)\",\n       color = \"Was raining\") +\n  theme_classic() +\n  scale_color_brewer(palette = \"Set1\")\n#> `geom_smooth()` using formula 'y ~ x'"},{"path":"ijalm.html","id":"threats-to-validity-and-aspects-to-think-about","chapter":"15 It’s Just A Linear Model","heading":"15.3.1 Threats to validity and aspects to think about","text":"variety weaknesses aspects discuss use linear regression. quick list includes (James et al. 2017, 92):Non-linearity response-predictor relationships.Correlation error terms.Non-constant variance error terms.Outliers.High-leverage points.CollinearityThese also aspects discuss use linear regression. Including plots tends handy illustrate points. aspects may consider discussing include (James et al. 2017, 75):least one predictors \\(X_1, X_2, \\dots, X_p\\) useful predicting response?predictors help explain \\(Y\\), subset predictors useful?well model fit data?Given set predictor values, response value predict, accurate prediction?","code":""},{"path":"ijalm.html","id":"more-credible-outputs","chapter":"15 It’s Just A Linear Model","heading":"15.3.2 More credible outputs","text":"Finally, creating beautiful graphs tables may want regression output look just nice. variety packages R automatically format regression outputs. One particularly nice huxtable (Hugh-Jones 2020).Table 15.1:  ","code":"\nlibrary(huxtable)\nhuxreg(running_data_first_model, running_data_rain_model)"},{"path":"ijalm.html","id":"implementation-in-tidymodels","chapter":"15 It’s Just A Linear Model","heading":"15.3.3 Implementation in tidymodels","text":"reason went trouble simple regression often want fit bunch models. One way copy/paste code bunch times. ’s nothing wrong . ’s way people get started, may want take approach scales easily. also need think carefully -fitting, able evaluate models.tidymodels package (Kuhn Wickham 2020) cool kids using days. ’s attempt bring order chaos different modelling packages R. (attempts past ’ve crashed burned, hopefully time different.) issue let’s say want run simple linear regression run random forest. language ’d use code models fairly different. tidymodels package latest attempt bring coherent grammar . ’s also package packages.’ll create test training datasets.81 points training set, 19 test set 100 total.can make datasets test training samples.look dataset made can see ’s got fewer rows. reached outcome something like:","code":"\nset.seed(853)\nlibrary(tidymodels)\n\nrunning_data_split <- rsample::initial_split(running_data, prop = 0.80)\nrunning_data_split\n#> <Analysis/Assess/Total>\n#> <80/20/100>\nrunning_data_train <- rsample::training(running_data_split)\nrunning_data_test  <-  rsample::testing(running_data_split)\nrunning_data <- \n  running_data %>% \n  mutate(magic_number = sample(x = c(1:nrow(running_data)), size = nrow(running_data), replace = FALSE))\n\nrunning_data_test <- \n  running_data %>% \n  filter(magic_number <= 20)\n\nrunning_data_train <- \n  running_data %>% \n  filter(magic_number > 20)\nfirst_go <- \n  parsnip::linear_reg() %>%\n  parsnip::set_engine(engine = \"lm\") %>% \n  parsnip::fit(marathon_time ~ five_km_time + was_raining, \n               data = running_data_train\n               )"},{"path":"ijalm.html","id":"implementation-in-rstanarm","chapter":"15 It’s Just A Linear Model","heading":"15.3.4 Implementation in rstanarm","text":"tidymodels package fine specific types tasks. instance machine learning chances interested forecasting. ’s kind thing tidymodels really built . want equivalent firepower explanatory modelling one option use Bayesian approaches directly. Yes, can use Bayesian models within tidymodels ecosystem, start move away ---box solutions, becomes important start understand going hood.variety ways getting started, essentially need probabilistic programming language. one specifically designed sort thing, comparison R, designed general statistical computing. use Stan notes within context familiar R environment. interface Stan using rstanarm package (Goodrich et al. 2020).","code":"\nlibrary(rstanarm)\n\nfirst_go_in_rstanarm <-\n  stan_lm(\n    marathon_time ~ five_km_time + was_raining, \n    data = running_data,\n    prior = NULL,\n    seed = 853\n  )\nfirst_go_in_rstanarm\n#> stan_lm\n#>  family:       gaussian [identity]\n#>  formula:      marathon_time ~ five_km_time + was_raining\n#>  observations: 100\n#>  predictors:   3\n#> ------\n#>                Median MAD_SD\n#> (Intercept)    0.4    6.0   \n#> five_km_time   8.4    0.3   \n#> was_rainingYes 0.7    2.2   \n#> \n#> Auxiliary parameter(s):\n#>               Median MAD_SD\n#> R2            0.9    0.0   \n#> log-fit_ratio 0.0    0.0   \n#> sigma         8.6    0.6   \n#> \n#> ------\n#> * For help interpreting the printed output see ?print.stanreg\n#> * For info on the priors used see ?prior_summary.stanreg"},{"path":"ijalm.html","id":"logistic-regression-2","chapter":"15 It’s Just A Linear Model","heading":"15.4 Logistic regression","text":"","code":""},{"path":"ijalm.html","id":"overview-3","chapter":"15 It’s Just A Linear Model","heading":"15.4.1 Overview","text":"steal joke someone, ‘’s AI ’re fundraising, machine learning ’re hiring, logistic regression ’re implementing.’dependent variable binary outcome, 0 1, instead linear regression may like use logistic regression. Although binary outcome may sound limiting, lot circumstances outcome either naturally falls situation, can adjusted (e.g. voter supports liberals liberals).reason use logistic regression ’ll modelling probability bounded 0 1. Whereas linear regression may end values outside . practice usually fine start linear regression move logistic regression build confidence.said, logistic regression, Daniella Witten teaches us, just linear model!","code":""},{"path":"ijalm.html","id":"implementation-in-base","chapter":"15 It’s Just A Linear Model","heading":"15.4.2 Implementation in base","text":"’d like consider slightly interesting example, dataset pearl jewellery, Australian retailer Paspaley.case ’ll model whether jewellery made white yellow gold, based price year (Figure 15.3).\nFigure 15.3: Examining type gold jewellery made .\ngraph suggests filter price higher $100,000.linear regression, logistic regression built R, glm function. case, ’ll try work jewellery white gold. Although strictly necessary particular function, ’ll change binary, 1 white gold 0 .One reason logistic regression can bit pain initially coefficients take bit work interpret. particular, estimate price -3.170e-06. odds. odds white gold decrease -3.170e-06 price increases. can model make forecasts terms probability, asking .Table 15.2:  ","code":"\npaspaley_dataset <- read_csv(\"https://raw.githubusercontent.com/RohanAlexander/paspaley/master/outputs/data/cleaned_dataset.csv\")\n#> Rows: 1289 Columns: 13\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (10): product, name, description, availability, sku,...\n#> dbl  (2): price, year\n#> lgl  (1): keshi\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npaspaley_dataset$metal %>% table()\n#> .\n#>       Other    Platinum   Rose gold  White gold Yellow gold \n#>         134          23          89         475         568\npaspaley_logistic_dataset <- \n  paspaley_dataset %>% \n  filter(metal %in% c('White gold', 'Yellow gold')) %>% \n  select(metal, price, year)\npaspaley_logistic_dataset <- \n  paspaley_logistic_dataset %>% \n  filter(price < 100000)\npaspaley_logistic_dataset <- \n  paspaley_logistic_dataset %>% \n  mutate(is_white_gold = if_else(metal == \"White gold\", 1, 0))\n\nwhite_gold_model <- \n  glm(is_white_gold ~ price + year, \n    data = paspaley_logistic_dataset, \n    family = 'binomial')\n\nsummary(white_gold_model)\n#> \n#> Call:\n#> glm(formula = is_white_gold ~ price + year, family = \"binomial\", \n#>     data = paspaley_logistic_dataset)\n#> \n#> Deviance Residuals: \n#>    Min      1Q  Median      3Q     Max  \n#> -1.250  -1.103  -1.015   1.247   1.353  \n#> \n#> Coefficients:\n#>               Estimate Std. Error z value Pr(>|z|)  \n#> (Intercept)  2.087e+02  8.674e+01   2.406   0.0161 *\n#> price        3.832e-06  5.405e-06   0.709   0.4783  \n#> year        -1.035e-01  4.296e-02  -2.408   0.0160 *\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 1411.6  on 1023  degrees of freedom\n#> Residual deviance: 1405.5  on 1021  degrees of freedom\n#> AIC: 1411.5\n#> \n#> Number of Fisher Scoring iterations: 4\npaspaley_logistic_dataset <- \n  broom::augment(white_gold_model,\n          data = paspaley_logistic_dataset,\n          type.predict = \"response\")\nhead(paspaley_logistic_dataset)"},{"path":"ijalm.html","id":"implementation-in-tidymodels-1","chapter":"15 It’s Just A Linear Model","heading":"15.4.3 Implementation in tidymodels","text":"can use tidymodels run wanted. case, need factor.","code":"\nset.seed(853)\n\npaspaley_logistic_dataset <- \n  paspaley_logistic_dataset %>% \n  mutate(is_white_gold = as_factor(is_white_gold))\n\npaspaley_logistic_dataset_split <- rsample::initial_split(paspaley_logistic_dataset, prop = 0.80)\npaspaley_logistic_dataset_train <- rsample::training(paspaley_logistic_dataset_split)\npaspaley_logistic_dataset_test  <-  rsample::testing(paspaley_logistic_dataset_split)\n\nwhite_gold_model_tidymodels <-\n  parsnip::logistic_reg(mode = \"classification\") %>%\n  parsnip::set_engine(\"glm\") %>%\n  fit(is_white_gold ~ price + year, \n      data = paspaley_logistic_dataset_train)\n\nwhite_gold_model_tidymodels\n#> parsnip model object\n#> \n#> Fit time:  4ms \n#> \n#> Call:  stats::glm(formula = is_white_gold ~ price + year, family = stats::binomial, \n#>     data = data)\n#> \n#> Coefficients:\n#> (Intercept)        price         year  \n#>   1.832e+02    5.245e-06   -9.082e-02  \n#> \n#> Degrees of Freedom: 818 Total (i.e. Null);  816 Residual\n#> Null Deviance:       1130 \n#> Residual Deviance: 1125  AIC: 1131"},{"path":"ijalm.html","id":"implementation-in-rstanarm-1","chapter":"15 It’s Just A Linear Model","heading":"15.4.4 Implementation in rstanarm","text":"","code":"\npaspaley_in_rstanarm <-\n  rstanarm::stan_glm(\n    is_white_gold ~ price + year,\n    data = paspaley_logistic_dataset,\n    family = binomial(link = \"logit\"),\n    prior = NULL,\n    seed = 853\n  )"},{"path":"ijalm.html","id":"poisson-regression","chapter":"15 It’s Just A Linear Model","heading":"15.5 Poisson regression","text":"","code":""},{"path":"ijalm.html","id":"overview-4","chapter":"15 It’s Just A Linear Model","heading":"15.5.1 Overview","text":"count data, use Poisson distribution. Pitman (1993, 121) ’Poisson distribution parameter \\(\\mu\\) Poisson (\\(\\mu\\)) distribution distribution probabilities \\(P_{\\mu}(k)\\) \\({0, 1, 2, ...}\\) defined :\n\\[P_{\\mu}(k) = e^{-\\mu}\\mu^k/k!\\mbox{, }k=0,1,2,...\\]\ncan simulate \\(n\\) data points Poisson distribution rpois() \\(\\lambda\\) mean variance.\\(\\lambda\\) parameter governs shape distribution.instance, look number + grades awarded university course given term course count.","code":"\nrpois(n = 20, lambda = 3)\n#>  [1] 2 2 3 4 2 2 4 2 4 1 3 2 3 3 2 2 0 1 2 1\nset.seed(853)\nnumber_of_each <- 1000\ntibble(lambda = c(rep(0, number_of_each), rep(1, number_of_each), rep(2, number_of_each), rep(5, number_of_each), rep(10, number_of_each)),\n       draw = c(rpois(n = number_of_each, lambda = 0), rpois(n = number_of_each, lambda = 1), rpois(n = number_of_each, lambda = 2), rpois(n = number_of_each, lambda = 5), rpois(n = number_of_each, lambda = 10))) %>% \n  ggplot(aes(x = draw)) +\n  geom_density() +\n  facet_wrap(vars(lambda)) +\n  theme_classic()\nset.seed(853)\ncount_of_A_plus <- \n  tibble( \n    # https://stackoverflow.com/questions/1439513/creating-a-sequential-list-of-letters-with-r\n    department = c(rep.int(\"1\", 26), rep.int(\"2\", 26)),\n    course = c(paste0(\"DEP_1_\", letters), paste0(\"DEP_2_\", letters)),\n    number_of_A_plus = c(sample(c(1:10), \n                              size = 26,\n                              replace = TRUE),\n                         sample(c(1:50), \n                              size = 26,\n                              replace = TRUE)\n    )\n  )"},{"path":"ijalm.html","id":"implementation-in-base-1","chapter":"15 It’s Just A Linear Model","heading":"15.5.2 Implementation in base","text":"","code":"\ngrades_model <- \n  glm(number_of_A_plus ~ department, \n    data = count_of_A_plus, \n    family = 'poisson')\n\nsummary(grades_model)\n#> \n#> Call:\n#> glm(formula = number_of_A_plus ~ department, family = \"poisson\", \n#>     data = count_of_A_plus)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -6.7386  -1.2102  -0.2515   1.3292   3.9520  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  1.44238    0.09535   15.13   <2e-16 ***\n#> department2  1.85345    0.10254   18.07   <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for poisson family taken to be 1)\n#> \n#>     Null deviance: 816.08  on 51  degrees of freedom\n#> Residual deviance: 334.57  on 50  degrees of freedom\n#> AIC: 545.38\n#> \n#> Number of Fisher Scoring iterations: 5"},{"path":"ijalm.html","id":"implementation-in-tidymodels-2","chapter":"15 It’s Just A Linear Model","heading":"15.5.3 Implementation in tidymodels","text":"can use tidymodels run wanted although first need install helper package poissonreg.","code":"\n# install.packages(\"poissonreg\")\n\nset.seed(853)\n\ncount_of_A_plus_split <- rsample::initial_split(count_of_A_plus, prop = 0.80)\ncount_of_A_plus_train <- rsample::training(count_of_A_plus_split)\ncount_of_A_plus_test  <-  rsample::testing(count_of_A_plus_split)\n\na_plus_model_tidymodels <-\n  poissonreg::poisson_reg(mode = \"regression\") %>%\n  parsnip::set_engine(\"glm\") %>%\n  parsnip::fit(number_of_A_plus ~ department, \n      data = count_of_A_plus_train)\n\na_plus_model_tidymodels\n#> parsnip model object\n#> \n#> Fit time:  1ms \n#> \n#> Call:  stats::glm(formula = number_of_A_plus ~ department, family = stats::poisson, \n#>     data = data)\n#> \n#> Coefficients:\n#> (Intercept)  department2  \n#>       1.488        1.867  \n#> \n#> Degrees of Freedom: 40 Total (i.e. Null);  39 Residual\n#> Null Deviance:       618.6 \n#> Residual Deviance: 210.1     AIC: 380.4"},{"path":"ijalm.html","id":"exercises-and-tutorial-14","chapter":"15 It’s Just A Linear Model","heading":"15.6 Exercises and tutorial","text":"","code":""},{"path":"ijalm.html","id":"exercises-14","chapter":"15 It’s Just A Linear Model","heading":"15.6.1 Exercises","text":"","code":""},{"path":"ijalm.html","id":"tutorial-14","chapter":"15 It’s Just A Linear Model","heading":"15.6.2 Tutorial","text":"","code":""},{"path":"causality.html","id":"causality","chapter":"16 Causality from observational data","heading":"16 Causality from observational data","text":"STATUS: construction.TODO: Replace arm matching https://kosukeimai.github.io/MatchIt/index.htmlRequired readingAngelucci, Charles, Julia Cagé, 2019, ‘Newspapers times low advertising revenues,’ American Economic Journal: Microeconomics, vol. 11, . 3, pp. 319-364, DOI: 10.1257/mic.20170306, available : https://www.aeaweb.org/articles?id=10.1257/mic.20170306.Better Evaluation, ‘Regression Discontinuity,’ https://www.betterevaluation.org/en/evaluation-options/regressiondiscontinuityDagan, Noa, Noam Barda, Eldad Kepten, Oren Miron, Shay Perchik, Mark . Katz, Miguel . Hernán, Marc Lipsitch, Ben Reis, Ran D. Balicer, 2021, ‘BNT162b2 mRNA Covid-19 vaccine nationwide mass vaccination setting,’ New England Journal Medicine, 24 February, https://www.nejm.org/doi/full/10.1056/NEJMoa2101765.Eggers, Andrew C., Anthony Fowler, Jens Hainmueller, Andrew B. Hall, James M. Snyder Jr, 2015, ‘validity regression discontinuity design estimating electoral effects: New evidence 40,000 close races,’ American Journal Political Science, 59 (1), pp. 259-274Gelman, Andrew, 2019, ‘Another Regression Discontinuity Disaster can learn ,’ 25 June, https://statmodeling.stat.columbia.edu/2019/06/25/another-regression-discontinuity-disaster---can--learn--/.Gelman, Andrew, Jennifer Hill Aki Vehtari, 2020, Regression Stories, Cambridge University Press, Chs 18 - 21.Gertler, Paul, Sebastian Martinez, Patrick Premand, Laura Rawlings, Christel Vermeersch, ‘Impact Evaluation Practice,’ Chapter 5 - 8.McElreath, Richard, 2020, Statistical Rethinking, 2nd Edition, CRC Press, Ch 14.Meng, Xiao-Li, 2021, ‘Values Data, Data Science, Data Scientists?’ Harvard Data Science Review, https://doi.org/10.1162/99608f92.ee717cf7, https://hdsr.mitpress.mit.edu/pub/bj2dfcwg/release/2.Riederer, Emily, 2021, ‘Causal design patterns data analysts,’ 30 January, https://emilyriederer.netlify.app/post/causal-design-patterns/Sekhon, Jasjeet Rocio Titiunik, 2016, ‘Understanding Regression Discontinuity Designs Observational Studies,’ Observational Studies 2 (2016) 174-182, http://sekhon.berkeley.edu/papers/SekhonTitiunik2016-OS.pdf.Wong, Jeffrey, Colin McFarland, 2020, ‘Computational Causal Inference Netflix,’ Netflix Technology Blog, 11 Aug, https://netflixtechblog.com/computational-causal-inference--netflix-293591691c62.Required viewingGelman, Andrew, 2020 ‘100 Stories Causal Inference,’ 4 August, https://www.youtube.com/watch?v=jnI5KI843Lk.King, Gary, 2020, ‘Research Designs,’ Lectures Quantitative Social Science Methods 1, https://youtu./SBwPLwVOb7s.Kuriwaki, Shiro, 2020, ‘Difference--Differences Estimation R (parts 1 2),’ 18 April, https://vimeo.com/409267138 https://vimeo.com/409267190.Kuriwaki, Shiro, 2020, ‘Instrumental variables R,’ 11 April, https://vimeo.com/406629459.Kuriwaki, Shiro, 2020, ‘Regression Discontinuity R (parts 1 2),’ 25 March, https://vimeo.com/400826628 https://vimeo.com/400826660.Oostrom, Tamar, 2021, ‘Funding Clinical Trials Reported Drug Efficacy,’ 2 March, https://youtu./DdnpWS9Km5U.Riederer, Emily, 2021, ‘Observational Causal Inference,’ Toronto Data Workshop, 15 February, https://youtu./VP3BBZ7poc0.Recommended readingAlexander, Monica, Polimis, Kivan, Zagheni, Emilio, 2019,’ impact Hurricane Maria -migration Puerto Rico: Evidence Facebook data’, Population Development Review. (Example using diff--diff measure effect Hurricane Maria.)Alexander, Rohan, Zachary Ward, 2018, ‘Age arrival assimilation age mass migration,’ Journal Economic History, 78, . 3, 904-937. (Example used differences brothers estimate effect education.)Angrist, Joshua D., Jörn-Steffen Pischke, 2008, Mostly harmless econometrics: empiricist’s companion, Princeton University Press, Chapter 4.Angrist, Joshua D., Jörn-Steffen Pischke, 2008, Mostly harmless econometrics: empiricist’s companion, Princeton University Press, Chapter 6.Angrist, Joshua D., Jörn-Steffen Pischke, 2008, Mostly harmless econometrics: empiricist’s companion, Princeton University Press, Chapters 3.3.2 5.Austin, Peter C., 2011, ‘Introduction Propensity Score Methods Reducing Effects Confounding Observational Studies,’ Multivariate Behavioral Research, vol. 46, . 3, pp.399-424. (Broad overview propensity score matching, nice discussion comparison randomised controlled trials.)Baker, Andrew, 2019, ‘Difference--Differences Methodology,’ 25 September, https://andrewcbaker.netlify.app/2019/09/25/difference--differences-methodology/.Coppock, Alenxader, Donald P. Green, 2016, ‘Voting Habit Forming? New Evidence Experiments Regression Discontinuities,’ American Journal Political Science, Volume 60, Issue 4, pp. 1044-1062, available : https://onlinelibrary.wiley.com/doi/abs/10.1111/ajps.12210. (code data.)Cunningham, Scott, ‘Causal Inference: Mixtape,’ Chapter ‘Instrumental variables,’ http://www.scunning.com/causalinference_norap.pdf.Cunningham, Scott, ‘Causal Inference: Mixtape,’ chapter ‘Regression discontinuity,’ http://www.scunning.com/causalinference_norap.pdf.Cunningham, Scott, Causal Inference: Mixtape, chapters ‘Matching subclassifications’ ‘Differences--differences,’ http://www.scunning.com/causalinference_norap.pdf. (well-written notes diff--diff.)Dell, Melissa, Pablo Querubin, 2018, ‘Nation Building Foreign Intervention: Evidence Discontinuities Military Strategies,’ Quarterly Journal Economics, Volume 133, Issue 2, pp. 701–764, https://doi.org/10.1093/qje/qjx037.Evans, David, 2013, ‘Regression Discontinuity Porn,’ World Bank Blogs, 16 November, https://blogs.worldbank.org/impactevaluations/regression-discontinuity-porn.Gelman, Andrew, 2019, ‘Another Regression Discontinuity Disaster can learn ,’ Statistical Modeling, Causal Inference, Social Science, 25 June, https://statmodeling.stat.columbia.edu/2019/06/25/another-regression-discontinuity-disaster---can--learn--/.Gelman, Andrew, Guido Imbens, 2019, “high-order polynomials used regression discontinuity designs,” Journal Business & Economic Statistics, 37, pp. 447-456.Gelman, Andrew, Jennifer Hill, 2007, Data Analysis Using Regression Muiltilevel/Hierarchical Models, Chapter 10, pp. 207-215.Grogger, Jeffrey, Andreas Steinmayr, Joachim Winter, 2020, ‘Wage Penalty Regional Accents,’ NBER Working Paper . 26719.Harris, Rich, Mlacki Migliozzi Niraj Chokshi, ‘13,000 Missing Flights: Global Consequences Coronavirus,’ New York Times, 21 February 2020. freely available (make account): https://www.nytimes.com/interactive/2020/02/21/business/coronavirus-airline-travel.html.Imai, Kosuke, 2017, Quantitative Social Science: Introduction, Princeton University Press, Ch 2.5.Imbens, Guido W., Thomas Lemieux, 2008, ‘Regression discontinuity designs: guide practice,’ Journal Econometrics, vol. 142, . 2, pp. 615-635.King, Gary, Richard Nielsen, 2019, ‘Propensity Scores Used Matching,’ Political Analysis. (Academic paper limits propensity score matching. Propensity score matching big thing 90s everyone knew weaknesses died . Lately, resurgence CS/ML folks using without thinking King Nielsen wrote nice paper flaws. mean, can’t say weren’t warned.)Myllyvirta, Lauri, 2020, ‘Analysis: Coronavirus temporarily reduced China’s CO2 emissions quarter,’ Carbon Brief, 19 February, https://www.carbonbrief.org/analysis-coronavirus--temporarily-reduced-chinas-co2-emissions---quarter.Saeed, Sahar, Erica E. M. Moodie, Erin C. Strumpf, Marina B. Klein, 2019, ‘Evaluating impact health policies: using difference--differences approach,’ International Journal Public Health, 64, pp. 637–642, https://doi.org/10.1007/s00038-018-1195-2.Taddy, Matt, 2019, Business Data Science, Chapter 5, pp. 146-162.Tang, John, 2015, ‘Pollution havens trade toxic chemicals: evidence U.S. trade flows,’ Ecological Economics, vol. 112, pp. 150-160. (Example using diff--diff estimate pollution.)Travis, D.J., Carleton, .M. Lauritsen, R.G., 2004. ‘Regional variations US diurnal temperature range 11–14 September 2001 aircraft groundings: Evidence jet contrail influence climate,’ Journal climate, 17(5), pp.1123-1134.Travis, David J., Andrew M. Carleton, Ryan G. Lauritsen. “Contrails reduce daily temperature range.” Nature, 418, . 6898 (2002): 601-601.Valencia Caicedo, Felipe. ‘mission: Human capital transmission, economic persistence, culture South America.’ Quarterly Journal Economics 134.1 (2019): 507-556. (Data available : Valencia Caicedo, Felipe, 2018, “Replication Data : ‘Mission: Human Capital Transmission, Economic Persistence, Culture South America’,” https://doi.org/10.7910/DVN/ML1155, Harvard Dataverse, V1.).Zinovyeva, Natalia Maryna Tverdostup, 2019, ‘women earn slightly husbands hard find?’ 10 June, https://blogs.lse.ac.uk/businessreview/2019/06/10/--women--earn-slightly----husbands-hard--find/.Key concepts/skills/etcEssential matching methods.Weaknesses matching.Difference--differences.Identifying opportunities instrumental variables.Implementing instrumental variables.Challenges validity instrumental variables.Reading foreign data.Difference differences.Replicating work.Displaying multiple regression results.Discussing results.Generating simulated data.Understanding regression discontinuity implementing manually using packages.Appreciating threats validity regression discontinuity.Key librariesbroomtidyverseestimatrtidyversehavenhuxtablescalestidyversebroomrdrobusttidyverseKey functions/etctidy()lm()iv_robust()dollar_format()hux_reg()lm()mutate_at()read_dta()lm()tidy()rdrobust()()QuizSharla Gelfand ‘(s)haring two #rstats functions days - one know love, one ’s new !’ Please go Sharla’s GitHub page: https://github.com/sharlagelfand/twofunctionsmostdays. Please find package mentions never used. Please find relevant website package. Please describe package context useful .Sharla Gelfand ‘(s)haring two #rstats functions days - one know love, one ’s new !’ Please go Sharla’s GitHub page: https://github.com/sharlagelfand/twofunctionsmostdays. Please find function mentions never used. Please look help file function. Please detail arguments function, context useful .propensity score matching? matching people, features like match ? sort ethical questions collecting storing information raise ?Putting one side, ethical issues, statistical weaknesses propensity score matching?key assumption using diff--diff?Please read fascinating article Markup car insurance algorithms: https://themarkup.org/allstates-algorithm/2020/02/25/car-insurance-suckers-list. Please read article tell think. may wish focus ethical, legal, social, statistical, , aspects.Please go GitHub page related fascinating article Markup car insurance algorithms: https://github.com/-markup/investigation-allstates-algorithm. great work? improved?fundamental features regression discontinuity design?conditions needed order RDD able used?Can think situation life RDD may useful?threats validity RDD estimates?Please look performance package: https://easystats.github.io/performance/index.html. features package may useful work?think using COVID-19 RDD setting? Statistically? Ethically?Please read reproduce main findings Eggers, Fowler, Hainmueller, Hall, Snyder, 2015.instrumental variable?circumstances instrumental variables might useful?conditions must instrumental variables satisfy?early instrumental variable authors?Can please think explain application instrumental variables life?key assumption difference--differences\nParallel trends.\nHeteroscedasticity.\nParallel trends.Heteroscedasticity.’re using regression discontinuity, whare aspects aware think really hard (select apply)?\ncut-free manipulation?\nforcing function continuous?\nextent functional form driving estimate?\ndifferent fitted lines affect results?\ncut-free manipulation?forcing function continuous?extent functional form driving estimate?different fitted lines affect results?main reason Oostrom (2021) finds outcome RCT can depend funding (pick one)?\nPublication bias\nExplicit manipulation\nSpecialisation\nLarger number arms\nPublication biasExplicit manipulationSpecialisationLarger number armsWhat key coefficient interest Angelucci Cagé, 2019 (pick one)?\n\\(\\beta_0\\)\n\\(\\beta_1\\)\n\\(\\lambda\\)\n\\(\\gamma\\)\n\\(\\beta_0\\)\\(\\beta_1\\)\\(\\lambda\\)\\(\\gamma\\)instrumental variable (please pick apply):\nCorrelated treatment variable.\ncorrelated outcome.\nHeteroskedastic.\nCorrelated treatment variable.correlated outcome.Heteroskedastic.two candidates invented instrumental variables?\nSewall Wright\nPhilip G. Wright\nSewall Cunningham\nPhilip G. Cunningham\nSewall WrightPhilip G. WrightSewall CunninghamPhilip G. CunninghamWhat two main assumptions instrumental variables?\nExclusion Restriction.\nRelevance.\nIgnorability.\nRandomization.\nExclusion Restriction.Relevance.Ignorability.Randomization.According Meng, 2021, ‘Data science can persuade via…’ (pick apply):\ncareful establishment evidence fair-minded high-quality data collection\nprocessing analysis\nhonest interpretation communication findings\nlarge sample sizes\ncareful establishment evidence fair-minded high-quality data collectionprocessing analysisthe honest interpretation communication findingslarge sample sizesAccording Reiderer, 2021, ‘disjoint treated untreated groups partitioned sharp cut-’ method use measure local treatment effect juncture groups (pick one)?\nregression discontinuity\nmatching\ndifference--differences\nevent study methods\nregression discontinuitymatchingdifference--differencesevent study methodsAccording Reiderer, 2021, ‘Causal inference requires investment ’ (pick apply):\ndata management\ndomain knowledge\nprobabilistic reasoning\ndata science\ndata managementdomain knowledgeprobabilistic reasoningdata scienceI Australian 30-39 year old male living Toronto one child PhD. following think match closely (please explain paragraph two)?\nAustralian 30-39 year old male living Toronto one child bachelors degree\nCanadian 30-39 year old male living Toronto one child PhD\nAustralian 30-39 year old male living Ottawa one child PhD\nCanadian 18-29 year old male living Toronto one child PhD\nAustralian 30-39 year old male living Toronto one child bachelors degreeA Canadian 30-39 year old male living Toronto one child PhDAn Australian 30-39 year old male living Ottawa one child PhDA Canadian 18-29 year old male living Toronto one child PhDIn disdainful tone (jokes, love DAGs), DAG (words please)?confounder (please select one answer)?\nvariable, z, causes x y, x also causes y.\nvariable, z, caused x y, x also causes y.\nvariable, z, causes y caused x, x also causes y.\nvariable, z, causes x y, x also causes y.variable, z, caused x y, x also causes y.variable, z, causes y caused x, x also causes y.mediator (please select one answer)?\nvariable, z, causes y caused x, x also causes y.\nvariable, z, causes x y, x also causes y.\nvariable, z, caused x y, x also causes y.\nvariable, z, causes y caused x, x also causes y.variable, z, causes x y, x also causes y.variable, z, caused x y, x also causes y.collider (please select one answer)?\nvariable, z, causes x y, x also causes y.\nvariable, z, causes y caused x, x also causes y.\nvariable, z, caused x y, x also causes y.\nvariable, z, causes x y, x also causes y.variable, z, causes y caused x, x also causes y.variable, z, caused x y, x also causes y.Please talk brief example may want careful checking Simpson’s paradox.Please talk brief example may want careful checking Berkson’s paradox.According McElreath (2020, 162) ‘Regression sort . Regression indeed oracle, cruel one. speaks riddles delights punishing us …’ (please select one answer)?\novercomplicating models.\nasking bad questions.\nusing bad data.\novercomplicating models.asking bad questions.using bad data.model fits small large world important , ?Kahneman, Sibony, Sunstein (2021) authors, including Nobel Prize winner Daniel Kahneman, say ‘… correlation imply causation, causation imply correlation. causal link, find correlation.’ reference Cunningham (2021Chapter 1), right wrong, ?","code":""},{"path":"causality.html","id":"introduction-18","chapter":"16 Causality from observational data","heading":"16.1 Introduction","text":"Life grand can conduct experiments able speak causality. can run survey - can’t run experiment? begin discussion circumstances methods allow nonetheless speak causality. use (relatively) simple methods, sophisticated, well-developed, ways (cf, much done days) applied statistics draw variety social sciences including economics, political science.Following publication Dagan et al. (2021), one authors tweeted (slight edits formatting):’ve just confirmed effectiveness Pfizer-BioNTech vaccine outside randomized trials. Yes, great news, let’s talk methodological issues arise using observational data estimate vaccine effectiveness.critical concern observational studies vaccine effectiveness confounding: Suppose people get vaccinated , average, lower risk infection/disease don’t get vaccinated. , even vaccine useless, ’d look beneficial.adjust confounding: start identifying potential confounders. example: Age (vaccination campaigns prioritize older people older people likely develop severe disease). choose valid adjustment method. paper, matched age. age adjustment, know residual confounding? one way go : know previous randomized trial vaccine effect first days. check whether matching age suffices replicate finding. , doesn’t. matching age (sex), curves infection start diverge day 0, indicates vaccinated lower risk infection unvaccinated. Conclusion: adjustment age sex insufficient.learned match COVID-19 risk factors, e.g., location, comorbidities, healthcare use… high-quality data Clalit Research Institute, part health services organization covers >50% Israeli population. example, vaccinated 76 year-old Arab male specific neighborhood received 4 influenza vaccines last 5 years 2 comorbidities matched unvaccinated Arab male neighborhood, aged 76-77, 3-4 influenza vaccines 2 comorbidities. matching risk factors, curves infection start diverge day ~12, expected vaccinated unvaccinated comparable risk infection. Using “negative control,” provide evidence large residual confounding.good illustration randomized trials observational studies complement better efficient #causalinference. First, randomized trial conducted estimate effectiveness vaccine prevent symptomatic infection, … trial’s estimates severe disease specific age groups imprecise. Second, observational analysis emulates #targettrial (order magnitude greater) confirms vaccine’s effectiveness severe disease different age groups. However… observational study needs trial’s findings benchmark guide data analysis strengthen quality causal inference.Randomized trials & Observational studies working together. best worlds. Let’s keep pandemic. luxury able think issues colleagues Noa Dagan, Noam Barda, Marc Lipsitch, Ben Reis, Ran D. Balicer. hope experience helpful researchers around world use observational data estimate vaccine effectiveness.Miguel Hernán, 24 Februrary 2021.chapter . can nonetheless comfortable making causal statements, even can’t run /B tests RCTs. Indeed, circumstances may actually prefer run run observational-based approaches addition . cover three major methods popular use days: difference--differences; regression discontinuity; instrumental variables.","code":""},{"path":"causality.html","id":"dags-and-trying-not-to-be-tricked-by-the-data","chapter":"16 Causality from observational data","heading":"16.2 DAGs and trying not to be tricked by the data","text":"","code":""},{"path":"causality.html","id":"dags-and-confounding","chapter":"16 Causality from observational data","heading":"16.2.1 DAGs and confounding","text":"discussing causality can help specific mean. ’s easy get caught data tricks . ’s important think really hard. One framework help become popular recently use directed acyclic graph (DAG), essentially just fancy name flow diagram. DAG involves drawing arrows variables indicating relationship . use DiagrammeR package draw (Iannone 2020), provides quite lot control (Figure 16.1). However can little finicky ’re just looking something really quickly ggdag package can useful (Barrett 2021b). code draw DAGs draws heavily Igelström (2020).\nFigure 16.1: Using DAG illustrate perceived relationships\nexample, claim \\(x\\) causes \\(y\\). build another situation less clear. find \\(x\\) \\(y\\) confusing, change fruits (Figure 16.2).\nFigure 16.2: Carrot confounder\ncase think \\(apple\\) causes \\(banana\\). ’s also clear \\(carrot\\) causes \\(banana\\), \\(carrot\\) also causes \\(apple\\). relationship ‘backdoor path,’ create spurious correlation analysis. may think changes \\(apple\\) causing changes \\(banana\\), ’s actually \\(carrot\\) changing hence variable called ‘confounder.’excellent discussion Hernan Robins (2020, 83):Suppose investigator conducted observational study answer causal question “one’s looking sky make pedestrians look ?” found association first pedestrian’s looking second one’s looking . However, also found pedestrians tend look hear thunderous noise . Thus unclear making second pedestrian look , first pedestrian’s looking thunderous noise? concluded effect one’s looking confounded presence thunderous noise.randomized experiments treatment assigned flip coin, observational studies treatment (e.g., person’s looking ) may determined many factors (e.g., thunderous noise). factors affect risk developing outcome (e.g., another person’s looking ), effects factors become entangled effect treatment. say confounding, just form lack exchangeability treated untreated. Confounding often viewed main shortcoming observational studies. presence confounding, old adage “association causation” holds even study population arbitrarily large.interested causal effects need adjust \\(carrot\\), \\(thunder\\) one way include regression. However, validity requires number assumptions. particular, Gelman Hill (2007, 169) warns us estimate correspond average causal effect sample : 1) include ‘confounding covariates’; 2) ‘model correct.’ Putting one side second requirement, focusing first, don’t observe confounder, can’t adjust . role domain experts, experience, theory, really add lot analysis.might similar situation, think \\(apple\\) causes \\(banana\\), time \\(apple\\) also causes \\(carrot\\), causes \\(banana\\) (Figure 16.3).\nFigure 16.3: Carrot mediator\ncase, \\(carrot\\) called ‘mediator’ ’d like adjust , affect estimate effect \\(apple\\) \\(banana\\).Finally, might yet another similar situation, think \\(apple\\) causes \\(banana\\), time \\(apple\\) \\(banana\\) cause \\(carrot\\) (Figure 16.4).\nFigure 16.4: Carrot collider\ncase, \\(carrot\\) called ‘collider’ condition create misleading relationship.’ve circling around point , ’s time address . create DAG - nothing create . means need think really carefully situation. ’s one thing see something DAG something . ’s another know ’s . McElreath (2020, 180) describes haunted DAGs.DAGs become fashionable. course helpful, just tool help think really deeply situation. McElreath (2020, 162) says ‘Regression sort . Regression indeed oracle, cruel one. speaks riddles delights punishing us asking bad questions.’ true DAGs, methods cover notes.","code":"\nlibrary(DiagrammeR)\nDiagrammeR::grViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape = plaintext]\n    x\n    y\n  edge [minlen = 2, arrowhead = vee]\n    x->y\n  { rank = same; x; y }\n}\n\")\nDiagrammeR::grViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape = plaintext]\n    Apple\n    Banana\n    Carrot\n  edge [minlen = 2, arrowhead = vee]\n    Apple->Banana\n    Carrot->Apple\n    Carrot->Banana\n  { rank = same; Apple; Banana }\n}\n\")\nDiagrammeR::grViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape = plaintext]\n    Apple\n    Banana\n    Carrot\n  edge [minlen = 2, arrowhead = vee]\n    Apple->Banana\n    Apple->Carrot\n    Carrot->Banana\n  { rank = same; Apple; Banana }\n}\n\")\nDiagrammeR::grViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape = plaintext]\n    Apple\n    Banana\n    Carrot\n  edge [minlen = 2, arrowhead = vee]\n    Apple->Banana\n    Apple->Carrot\n    Banana->Carrot\n  { rank = same; Apple; Banana }\n}\n\")"},{"path":"causality.html","id":"selection-and-measurement-bias","chapter":"16 Causality from observational data","heading":"16.2.2 Selection and measurement bias","text":"Selection bias occurs outcomes dependent ‘process individuals selected analysis’ (Hernan Robins 2020, 99). going able see DAG (mean one draw DAG shows , need know order draw DAG mean), many default diagnostics. One way go things /testing experimental settings, comparing sample general characteristics, instance age-group, gender, education. fundamental point, Dr Jill Sheppard says, ‘people respond surveys weird,’ generalises whatever method ’re using gather data. Using Facebook ads? People click Facebook ads weird. Going door knocking? People answer door weird. Call people phone? Literally answers phone anymore.pernicious aspect selection bias, pervades every aspect analysis. Even sample starts perfectly representative, may become selected time. instance, survey panels used political polling need updated time time folks don’t get anything stop responding - people may still vote need polled.Another bias aware measurement bias, ‘association treatment outcome weakened strengthened result process study data measured’ (Hernan Robins 2020, 113). implicit definition Hernan Robins (2020), important systematic. instance, ask people person income likely get different answers ask phone via online form.","code":""},{"path":"causality.html","id":"two-common-paradoxes","chapter":"16 Causality from observational data","heading":"16.2.3 Two common paradoxes","text":"two situations data can trick common ’d like explicitly go . Simpson’s paradox, Berkson’s paradox. Keep situations back mind times dealing data.Simpson’s paradox occurs estimate relationship subsets data, different relationship consider entire dataset (Simpson 1951). instance, may positive relationship undergraduate grades performance graduate school statistics economics considering department individually. undergraduate grades tended higher statistics economics graduate school performance tended opposite, may actually find negative relationship undergraduate grades performance graduate school.see let’s simulate data.Berkson’s paradox occurs estimate relationship based dataset , dataset selected relationship different general dataset (Berkson 1946). instance, dataset professional cyclists find relationship VO2 max chance winning bike race. dataset general population find enormous relationship VO2 max chance winning bike race. professional dataset just selected relationship disappears - can’t become professional unless high VO2 max.see let’s simulate data.","code":"\nset.seed(853)\nnumber_in_each <- 1000\nstatistics <- tibble(undergrad = runif(n = number_in_each, min = 0.7, max = 0.9),\n                     noise = rnorm(n = number_in_each, 0, sd = 0.1),\n                     grad = undergrad + noise,\n                     type = \"Statistics\")\neconomics <- tibble(undergrad = runif(n = number_in_each, min = 0.6, max = 0.8),\n                    noise = rnorm(n = number_in_each, 0, sd = 0.1),\n                    grad = undergrad + noise + 0.3,\n                    type = \"Economics\")\nboth = rbind(statistics, economics)\n\nboth %>% \n  ggplot(aes(x = undergrad, y = grad)) +\n  geom_point(aes(color = type), alpha = 0.1) +\n  geom_smooth(aes(color = type), method = 'lm', formula = 'y ~ x') +\n  geom_smooth(method = 'lm', formula = 'y ~ x', color = 'black') +\n  labs(x = \"Undergraduate results\",\n       y = \"Graduate results\",\n       color = \"Type\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\nset.seed(853)\nnumber_of_pros <- 100\nnumber_of_public <- 1000\nprofessionals <- \n  tibble(VO2 = runif(n = number_of_pros, min = 0.7, max = 0.9),\n         chance_of_winning = runif(n = number_of_pros, min = 0.7, max = 0.9),\n         type = \"Professionals\")\ngeneral_public <- \n  tibble(VO2 = runif(n = number_of_public, min = 0.6, max = 0.8),\n         noise = rnorm(n = number_of_public, 0, sd = 0.03),\n         chance_of_winning = VO2 + noise + 0.1,\n         type = \"Public\") %>% \n  select(-noise)\nboth = rbind(professionals, general_public)\n\nboth %>% \n  ggplot(aes(x = VO2, y = chance_of_winning)) +\n  geom_point(aes(color = type), alpha = 0.1) +\n  geom_smooth(aes(color = type), method = 'lm', formula = 'y ~ x') +\n  geom_smooth(method = 'lm', formula = 'y ~ x', color = 'black') +\n  labs(x = \"VO2 max\",\n       y = \"Chance of winning a bike race\",\n       color = \"Type\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")"},{"path":"causality.html","id":"difference-in-differences","chapter":"16 Causality from observational data","heading":"16.3 Difference in differences","text":"","code":""},{"path":"causality.html","id":"matching-and-difference-in-differences","chapter":"16 Causality from observational data","heading":"16.3.1 Matching and difference-in-differences","text":"","code":""},{"path":"causality.html","id":"introduction-19","chapter":"16 Causality from observational data","heading":"16.3.1.1 Introduction","text":"ideal situation able conduct experiment rarely possible data science setting. Can really reasonably expect Netflix allow us change prices. even , let us , , ? , rarely can explicitly create treatment control groups. Finally, experiments really expensive potentially unethical. Instead, need make . Rather counterfactual coming us randomisation, hence us knowing two treatment, try identify groups similar treatment, hence differences can attributed treatment. practice, tend even differences two groups treat. Provided pre-treatment differences satisfy assumptions (basically consistent, expect consistency continue absence treatment) – ‘parallel trends’ assumption – can look difference differences effect treatment. One lovely aspects difference differences analysis can using fairly straight-forward quantitative methods - linear regression dummy variable needed convincing job.","code":""},{"path":"causality.html","id":"motivation","chapter":"16 Causality from observational data","heading":"16.3.1.2 Motivation","text":"Consider us wanting know effect new tennis racket serve speed. One way test measure difference Roger Federer’s serve speed without tennis racket mine tennis racket. Sure, ’d find difference know much attribute tennis racket? Another way consider difference serve speed without tennis racket serve speed tennis racket. serves just getting faster naturally time? Instead, let’s combine two look difference differences!world measure Federer’s serve compare serve without new racket. measure Federer’s serve measure serve new racket. difference differences estimate effect new racket.sorts assumptions jump going make order analysis appropriate?something else may affected , Roger affect serve speed? Probably.likely Roger Federer trajectory serve speed improvement? Probably . ‘parallel trends’ assumption, dominates discussion difference differences analysis. Finally, likely variance serve speeds ? Probably .might powerful? don’t need treatment control group treatment. just need good idea differ.","code":""},{"path":"causality.html","id":"simulated-example","chapter":"16 Causality from observational data","heading":"16.3.1.3 Simulated example","text":"Let’s generate data.Let’s make graph.simple example, manually, getting average difference differences.Let’s use OLS analysis. general regression equation :\n\\[Y_{,t} = \\beta_0 + \\beta_1\\mbox{Treatment group dummy}_i + \\beta_2\\mbox{Time dummy}_t + \\beta_3(\\mbox{Treatment group dummy} \\times\\mbox{Time dummy})_{,t} + \\epsilon_{,t}\\]use * regression automatically includes separate aspects well interaction. ’s estimate \\(\\beta_3\\) interest.Fortunately, estimates !","code":"\nlibrary(broom)\nlibrary(tidyverse)\n\nset.seed(853)\n\ndiff_in_diff_example_data <- tibble(person = rep(c(1:1000), times = 2),\n                       time = c(rep(0, times = 1000), rep(1, times = 1000)),\n                       treatment_group = rep(sample(x = 0:1, size  = 1000, replace = TRUE), times = 2)\n                       )\n## We want to make the outcome slightly more likely if they were treated than if not.\ndiff_in_diff_example_data <- \n  diff_in_diff_example_data %>% \n  rowwise() %>% \n  mutate(serve_speed = case_when(\n    time == 0 & treatment_group == 0 ~ rnorm(n = 1, mean = 5, sd = 1),\n    time == 1 & treatment_group == 0 ~ rnorm(n = 1, mean = 6, sd = 1),\n    time == 0 & treatment_group == 1 ~ rnorm(n = 1, mean = 8, sd = 1),\n    time == 1 & treatment_group == 1 ~ rnorm(n = 1, mean = 14, sd = 1),\n    )\n    )\n\nhead(diff_in_diff_example_data)\n#> # A tibble: 6 × 4\n#> # Rowwise: \n#>   person  time treatment_group serve_speed\n#>    <int> <dbl>           <int>       <dbl>\n#> 1      1     0               0        4.43\n#> 2      2     0               1        6.96\n#> 3      3     0               1        7.77\n#> 4      4     0               0        5.31\n#> 5      5     0               0        4.09\n#> 6      6     0               0        4.85\ndiff_in_diff_example_data$treatment_group <- as.factor(diff_in_diff_example_data$treatment_group)\ndiff_in_diff_example_data$time <- as.factor(diff_in_diff_example_data$time)\n\ndiff_in_diff_example_data %>% \n  ggplot(aes(x = time,\n             y = serve_speed,\n             color = treatment_group)) +\n  geom_point() +\n  geom_line(aes(group = person), alpha = 0.2) +\n  theme_minimal() +\n  labs(x = \"Time period\",\n       y = \"Serve speed\",\n       color = \"Person got a new racket\") +\n  scale_color_brewer(palette = \"Set1\")\naverage_differences <- \n  diff_in_diff_example_data %>% \n  pivot_wider(names_from = time,\n              values_from = serve_speed,\n              names_prefix = \"time_\") %>% \n  mutate(difference = time_1 - time_0) %>% \n  group_by(treatment_group) %>% \n  summarise(average_difference = mean(difference))\n\naverage_differences$average_difference[2] - average_differences$average_difference[1]\n#> [1] 5.058414\ndiff_in_diff_example_regression <- lm(serve_speed ~ treatment_group*time, \n                         data = diff_in_diff_example_data)\n\ntidy(diff_in_diff_example_regression)\n#> # A tibble: 4 × 5\n#>   term                 estimate std.error statistic  p.value\n#>   <chr>                   <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)              4.97    0.0428     116.  0       \n#> 2 treatment_group1         3.03    0.0622      48.7 0       \n#> 3 time1                    1.01    0.0605      16.6 2.97e-58\n#> 4 treatment_group1:ti…     5.06    0.0880      57.5 0"},{"path":"causality.html","id":"assumptions","chapter":"16 Causality from observational data","heading":"16.3.1.4 Assumptions","text":"want use difference differences, need satisfy assumptions. three touched earlier, want focus one: ‘parallel trends’ assumption. parallel trends assumption haunts everything diff--diff analysis can never prove , can just convinced .see can never prove , consider example want know effect new stadium professional sports team’s wins/loses. consider two teams: Warriors Raptors. Warriors changed stadiums start 2019-20 season (Raptors ), consider four time periods: 2016-17 season, 2017-18 season, 2018-19 season, finally compare performance one moved, 2019-20 season. Raptors act counterfactual. means assume relationship Warriors Raptors, absence new stadium, continued change consistent way. can never know certain. present sufficient evidence assuage concerns reader may .variety reasons, worth tougher normal requirements around evidence take convince effect.four main ‘threats validity’ using difference differences address (Cunningham, 2020, pp. 272–277):Non-parallel trends. treatment control groups may based differences. can difficult convincingly argue parallel trends. case, maybe try find another factor consider model may adjust . may require difference difference differences (earlier example, perhaps add San Francisco 49ers broad geographic area Warriors). maybe re-think analysis see can make different control group. Adding additional earlier time periods may help may introduce issues (see third point).Compositional differences. concern working repeated cross-sections. composition cross-sections change? instance, work Tik Tok app rapidly growing want look effect change. initial cross-section, may mostly young people, subsequent cross-section, may older people demographics app usage change. Hence results may just age-effect, effect change interested .Long-term effects vs. reliability. discussed last chapter, trade-length analysis run. run analysis longer opportunity factors affect results. also increased chance someone treated treated. , hand, can difficult convincingly argue short-term results continue long-term.Functional form dependence. less issue outcomes similar, different functional form may responsible aspects results.","code":""},{"path":"causality.html","id":"matching","chapter":"16 Causality from observational data","heading":"16.3.1.5 Matching","text":"section draws material Gelman Hill, 2007, pp. 207-212.Difference differences powerful analysis framework. learnt began see opportunities implement everywhere. can tough identify appropriate treatment control groups. Alexander Ward, 2018, compare migrant brothers - one education different country, education US. really best match?may able match based observable variables. instance, age-group education. two different times compare smoking rates 18-year-olds one city smoking rates 18-year-olds another city. fine, fairly coarse. know differences 18-year-olds, even terms variables commonly observe, say gender education. One way deal may create sub-groups: 18-year-old males high school education, etc. sample sizes likely quickly become small. deal continuous variables? also, difference 18-year-old 19-year-old really different? Shouldn’t also compare ?One way proceed consider nearest neighbour approach. limited concern uncertainty approach. also issue large number variables end high-dimension graph. leads us propensity score matching.Propensity score matching involves assigning probability observation. construct probability based observation’s values independent variables, values treatment. probability best guess probability observation treated, regardless whether treated . instance, 18-year-old males treated 19-year-old males , much difference 18-year-old males 19-year-old males assigned probability fairly similar. can compare outcomes observations similar propensity scores.One advantage propensity score matching allows us easily consider many independent variables , can constructed using logistic regression.Let’s generate data illustrate propensity score matching. Let’s pretend work Amazon. going treat individuals free-shipping see happens average purchase.Now need add probability treated free shipping, depends variables. Younger, higher-income, male Toronto make slightly likely.Finally, need measure person’s average spend. want free shipping slightly higher without.Now construct logistic regression model ‘explains’ whether person treated function variables think explain .now add forecast dataset.Now use forecast create matches. variety ways . moment ’ll step code , worked example small number possibilities, can just manually.every person actually treated (given free shipping) want untreated person considered similar (based propensity score) possible.’re going use matching function arm package. finds closest ones treated, one treated.Now reduce dataset just matched. 371 treated, expect dataset 742 observations.Finally, can examine ‘effect’ treated average spend ‘usual’ way.Table 16.1:  cover propensity score matching widely used. Hence, need know use . People think ’s weird didn’t, way cover ANOVA people think ’s weird entire experimental design course didn’t cover even though modern ways looking differences two means. time need know flaws propensity score matching. now discuss .Matching. Propensity score matching match unobserved variables. may fine class-room setting, realistic settings likely cause issues.Modelling. results tend specific model used. King Nielsen, 2019, discuss thoroughly.Statistically. using data twice.","code":"\nlibrary(tidyverse)\nsample_size <- 10000\nset.seed(853)\n\namazon_purchase_data <-\n  tibble(\n    unique_person_id = c(1:sample_size),\n    age = runif(n = sample_size,\n                min = 18,\n                max = 100),\n    city = sample(\n      x = c(\"Toronto\", \"Montreal\", \"Calgary\"),\n      size = sample_size,\n      replace = TRUE\n      ),\n    gender = sample(\n      x = c(\"Female\", \"Male\", \"Other/decline\"),\n      size = sample_size,\n      replace = TRUE,\n      prob = c(0.49, 0.47, 0.02)\n      ),\n    income = rlnorm(n = sample_size,\n                    meanlog = 0.5, \n                    sdlog = 1)\n    )\namazon_purchase_data <-\n  amazon_purchase_data %>% \n  mutate(age_num = case_when(\n           age < 30 ~ 3,\n           age < 50 ~ 2,\n           age < 70 ~ 1,\n           TRUE ~ 0),\n         city_num = case_when(\n           city == \"Toronto\" ~ 3,\n           city == \"Montreal\" ~ 2,\n           city == \"Calgary\" ~ 1,\n           TRUE ~ 0),\n         gender_num = case_when(\n           gender == \"Male\" ~ 3,\n           gender == \"Female\" ~ 2,\n           gender == \"Other/decline\" ~ 1,\n           TRUE ~ 0),\n         income_num = case_when(\n           income > 3 ~ 3,\n           income > 2 ~ 2,\n           income > 1 ~ 1,\n           TRUE ~ 0)\n         ) %>% \n  rowwise() %>% \n  mutate(sum_num = sum(age_num, city_num, gender_num, income_num),\n         softmax_prob = exp(sum_num)/exp(12),\n         free_shipping = sample(\n           x = c(0:1),\n           size = 1,\n           replace = TRUE,\n           prob = c(1-softmax_prob, softmax_prob)\n           )\n         ) %>% \n  ungroup()\n\namazon_purchase_data <-\n  amazon_purchase_data %>% \n  dplyr::select(-age_num, -city_num, -gender_num, -income_num, -sum_num, -softmax_prob)\namazon_purchase_data <-\n  amazon_purchase_data %>% \n  mutate(mean_spend = if_else(free_shipping == 1, 60, 50)) %>% \n  rowwise() %>% \n  mutate(average_spend = rnorm(1, mean_spend, sd = 5)\n    ) %>% \n  ungroup() %>% \n  dplyr::select(-mean_spend)\n\n## Fix the class on some\namazon_purchase_data <-\n  amazon_purchase_data %>% \n  mutate_at(vars(city, gender, free_shipping), ~as.factor(.)) ## Change some to factors\ntable(amazon_purchase_data$free_shipping)\n#> \n#>    0    1 \n#> 9629  371\n\nhead(amazon_purchase_data)\n#> # A tibble: 6 × 7\n#>   unique_person_id   age city    gender income free_shipping\n#>              <int> <dbl> <fct>   <fct>   <dbl> <fct>        \n#> 1                1  47.5 Calgary Female  1.72  0            \n#> 2                2  27.8 Montre… Male    1.54  0            \n#> 3                3  57.7 Toronto Female  3.16  0            \n#> 4                4  43.9 Toronto Male    0.636 0            \n#> 5                5  21.1 Toronto Female  1.43  0            \n#> 6                6  51.1 Calgary Male    1.18  0            \n#> # … with 1 more variable: average_spend <dbl>\npropensity_score <- glm(free_shipping ~ age + city + gender + income, \n                        family = binomial,\n                        data = amazon_purchase_data)\namazon_purchase_data <- \n  augment(propensity_score, \n          data = amazon_purchase_data,\n          type.predict = \"response\") %>% \n  dplyr::select(-.resid, -.std.resid, -.hat, -.sigma, -.cooksd) \namazon_purchase_data <- \n  amazon_purchase_data %>% \n  arrange(.fitted, free_shipping)\namazon_purchase_data$treated <- if_else(amazon_purchase_data$free_shipping == 0, 0, 1)\namazon_purchase_data$treated <- as.integer(amazon_purchase_data$treated)\n\nmatches <- arm::matching(z = amazon_purchase_data$treated, score = amazon_purchase_data$.fitted)\n\namazon_purchase_data <- cbind(amazon_purchase_data, matches)\namazon_purchase_data_matched <- \n  amazon_purchase_data %>% \n  filter(match.ind != 0) %>% \n  dplyr::select(-match.ind, -pairs, -treated)\n\nhead(amazon_purchase_data_matched)\n#>   unique_person_id      age     city gender     income\n#> 1             5710 81.15636 Montreal Female 0.67505625\n#> 2             9458 97.04859 Montreal Female 9.49752179\n#> 3             6428 83.21262  Calgary   Male 0.05851482\n#> 4             2022 98.97504 Montreal   Male 1.66683768\n#> 5             9824 64.61936  Calgary Female 3.35263989\n#> 6             1272 97.09546  Toronto Female 0.71813784\n#>   free_shipping average_spend     .fitted cnts\n#> 1             0      47.36258 0.001375987    1\n#> 2             1      61.15317 0.001376161    1\n#> 3             0      49.90080 0.001560150    1\n#> 4             1      57.75673 0.001560418    1\n#> 5             1      64.69709 0.002207195    1\n#> 6             0      56.64754 0.002207514    1\npropensity_score_regression <- lm(average_spend ~ age + city + gender + income + free_shipping, \n                                  data = amazon_purchase_data_matched)\nhuxtable::huxreg(propensity_score_regression)"},{"path":"causality.html","id":"case-study---lower-advertising-revenue-reduced-french-newspaper-prices-between-1960-and-1974","chapter":"16 Causality from observational data","heading":"16.4 Case study - Lower advertising revenue reduced French newspaper prices between 1960 and 1974","text":"","code":""},{"path":"causality.html","id":"introduction-20","chapter":"16 Causality from observational data","heading":"16.4.1 Introduction","text":"case study introduce Angelucci Cagé, 2019, replicate main findings. Angelucci Cagé, 2019, paper difference differences used examine effect reduction advertising revenues newspapers’ content prices. create dataset ‘French newspapers 1960 1974.’ ‘perform difference--differences analysis’ exploit ‘introduction advertising television’ change ‘affected national newspapers severely local ones.’ ‘find robust evidence decrease amount journalistic-intensive content produced subscription price.’order conduct analysis use dataset provide alongside paper. dataset available : https://www.openicpsr.org/openicpsr/project/116438/version/V1/view. available download registration. dataset Stata data format, use haven package read (Wickham Miller, 2019).","code":"\nlibrary(here)\nlibrary(haven)\nlibrary(huxtable)\n#> \n#> Attaching package: 'huxtable'\n#> The following objects are masked from 'package:ggdag':\n#> \n#>     label, label<-\n#> The following object is masked from 'package:dplyr':\n#> \n#>     add_rownames\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     theme_grey\nlibrary(scales)\n#> \n#> Attaching package: 'scales'\n#> The following object is masked from 'package:huxtable':\n#> \n#>     number_format\n#> The following object is masked from 'package:purrr':\n#> \n#>     discard\n#> The following object is masked from 'package:readr':\n#> \n#>     col_factor\nlibrary(tidyverse)"},{"path":"causality.html","id":"background-1","chapter":"16 Causality from observational data","heading":"16.4.2 Background","text":"Newspapers trouble. can probably think local newspaper closed recently pressure brought internet. issue isn’t new. television started, similar concerns. paper, Angelucci Cagé use introduction television advertising France, announced 1967, examine effect decreased advertising revenue newspapers.reason important allows us disentangle competing effects. instance, newspapers becoming redundant can longer charge high prices ads consumers prefer get news ways? fewer journalists needed smartphones technology mean can productive? Angelucci Cagé look advertising revenue features, new advertising platform arrives, case television advertising.","code":""},{"path":"causality.html","id":"data","chapter":"16 Causality from observational data","heading":"16.4.3 Data","text":"() dataset contains annual data local national newspapers 1960 1974, well detailed information television content. 1967, French government announced relax long-standing regulations prohibited television advertising. provide evidence reform can plausibly interpreted exogenous negative shock advertising side newspaper industry… []t likely introduction television advertising constituted direct shock advertising side newspaper industry indirect shock reader side… (O)ur empirical setting constitutes unique opportunity isolate consequences decrease newspapers’ advertising revenues choices regarding size newsroom, amount information produce, prices charge sides market.authors’ argue national newspapers affected television advertising change, local newspapers . national newspapers treatment group local newspapers control group.dataset can read using read_dta(), function within haven package reading Stata dta files. equivalent read_csv().1,196 observations dataset 52 variables. authors interested 1960-1974 time period around 100 newspapers. 14 national newspapers beginning period 12 end.just want replicate main results, don’t need variables. just select() ones interested change class() needed.can now look main variables interest national (Figure 16.5) local daily newspapers (Figure 16.6).\nFigure 16.5: Angelucci Cagé, 2019, summary statistics: national daily newspapers\nSource: Angelucci Cagé, 2019, p. 333.\nFigure 16.6: Angelucci Cagé, 2019, summary statistics: local daily newspapers\nSource: Angelucci Cagé, 2019, p. 334.Please read section paper see describe dataset.interested change 1967 onward.","code":"\nnewspapers <- read_dta(here::here(\"inputs/data/116438-V1/data/dta/Angelucci_Cage_AEJMicro_dataset.dta\"))\n\ndim(newspapers)\n#> [1] 1196   52\nnewspapers <- \n  newspapers %>% \n  dplyr::select(year, id_news, after_national, local, national, ## Diff in diff variables\n         ra_cst, qtotal, ads_p4_cst, ads_s, ## Advertising side dependents\n         ps_cst, po_cst, qtotal, qs_s, rs_cst) %>% #Reader side dependents\n  mutate(ra_cst_div_qtotal = ra_cst / qtotal) %>% ## An advertising side dependents needs to be built\n  mutate_at(vars(id_news, after_national, local, national), ~as.factor(.)) %>% ## Change some to factors\n  mutate(year = as.integer(year))\nnewspapers %>% \n  mutate(type = if_else(local == 1, \"Local\", \"National\")) %>% \n  ggplot(aes(x = year, y = ra_cst)) +\n  geom_point(alpha = 0.5) +\n  scale_y_continuous(labels = dollar_format(prefix=\"$\", suffix = \"M\", scale = 0.000001)) +\n  labs(x = \"Year\",\n       y = \"Advertising revenue\") +\n  facet_wrap(vars(type),\n               nrow = 2) +\n  theme_classic() +\n  geom_vline(xintercept = 1966.5, linetype = \"dashed\")"},{"path":"causality.html","id":"model","chapter":"16 Causality from observational data","heading":"16.4.4 Model","text":"model interested estimating :\n\\[\\mbox{ln}(y_{n,t}) = \\beta_0 + \\beta_1(\\mbox{National dummy}\\times\\mbox{1967 onward dummy}) + \\lambda_n + \\gamma_y + \\epsilon.\\]\n\\(\\lambda_n\\) fixed effect newspaper, \\(\\gamma_y\\) fixed effect year. just use regular linear regression, different dependent variables. \\(\\beta_1\\) coefficient interested .","code":""},{"path":"causality.html","id":"results","chapter":"16 Causality from observational data","heading":"16.4.5 Results","text":"can run models using lm().Looking advertising-side variables.Table 16.2:  Similarly, can look reader-side variables.Table 16.3:  ","code":"\n## Advertising side\nad_revenue <- lm(log(ra_cst) ~ after_national + id_news + year, data = newspapers)\nad_revenue_div_circulation <- lm(log(ra_cst_div_qtotal) ~ after_national + id_news + year, data = newspapers)\nad_price <- lm(log(ads_p4_cst) ~ after_national + id_news + year, data = newspapers)\nad_space <- lm(log(ads_s) ~ after_national + id_news + year, data = newspapers)\n\n## Consumer side\nsubscription_price <- lm(log(ps_cst) ~ after_national + id_news + year, data = newspapers)\nunit_price <- lm(log(po_cst) ~ after_national + id_news + year, data = newspapers)\ncirculation <- lm(log(qtotal) ~ after_national + id_news + year, data = newspapers)\nshare_of_sub <- lm(log(qs_s) ~ after_national + id_news + year, data = newspapers)\nrevenue_from_sales <- lm(log(rs_cst) ~ after_national + id_news + year, data = newspapers)\nomit_me <- c(\"(Intercept)\", \"id_news3\", \"id_news6\", \"id_news7\", \"id_news13\", \n             \"id_news16\", \"id_news25\", \"id_news28\", \"id_news34\", \"id_news38\", \n             \"id_news44\", \"id_news48\", \"id_news51\", \"id_news53\", \"id_news54\", \n             \"id_news57\", \"id_news60\", \"id_news62\", \"id_news66\", \"id_news67\", \n             \"id_news70\", \"id_news71\", \"id_news72\", \"id_news80\", \"id_news82\", \n             \"id_news88\", \"id_news95\", \"id_news97\", \"id_news98\", \"id_news103\", \n             \"id_news105\", \"id_news106\", \"id_news118\", \"id_news119\", \"id_news127\", \n             \"id_news136\", \"id_news138\", \"id_news148\", \"id_news151\", \"id_news153\", \n             \"id_news154\", \"id_news157\", \"id_news158\", \"id_news161\", \"id_news163\", \n             \"id_news167\", \"id_news169\", \"id_news179\", \"id_news184\", \"id_news185\", \n             \"id_news187\", \"id_news196\", \"id_news206\", \"id_news210\", \"id_news212\", \n             \"id_news213\", \"id_news224\", \"id_news225\", \"id_news234\", \"id_news236\", \n             \"id_news245\", \"id_news247\", \"id_news310\", \"id_news452\", \"id_news467\", \n             \"id_news469\", \"id_news480\", \"id_news20040\", \"id_news20345\", \n             \"id_news20346\", \"id_news20347\", \"id_news20352\", \"id_news20354\", \n             \"id_news21006\", \"id_news21025\", \"id_news21173\", \"id_news21176\", \n             \"id_news33718\", \"id_news34689\", \"id_news73\")\n\nhuxreg(\"Ad. rev.\" = ad_revenue, \n       \"Ad rev. div. circ.\" = ad_revenue_div_circulation, \n       \"Ad price\" = ad_price, \n       \"Ad space\" = ad_space,\n        omit_coefs = omit_me, \n        number_format = 2\n        )\nomit_me <- c(\"(Intercept)\", \"id_news3\", \"id_news6\", \"id_news7\", \"id_news13\", \n             \"id_news16\", \"id_news25\", \"id_news28\", \"id_news34\", \"id_news38\", \n             \"id_news44\", \"id_news48\", \"id_news51\", \"id_news53\", \"id_news54\", \n             \"id_news57\", \"id_news60\", \"id_news62\", \"id_news66\", \"id_news67\", \n             \"id_news70\", \"id_news71\", \"id_news72\", \"id_news80\", \"id_news82\", \n             \"id_news88\", \"id_news95\", \"id_news97\", \"id_news98\", \"id_news103\", \n             \"id_news105\", \"id_news106\", \"id_news118\", \"id_news119\", \"id_news127\", \n             \"id_news136\", \"id_news138\", \"id_news148\", \"id_news151\", \"id_news153\", \n             \"id_news154\", \"id_news157\", \"id_news158\", \"id_news161\", \"id_news163\", \n             \"id_news167\", \"id_news169\", \"id_news179\", \"id_news184\", \"id_news185\", \n             \"id_news187\", \"id_news196\", \"id_news206\", \"id_news210\", \"id_news212\", \n             \"id_news213\", \"id_news224\", \"id_news225\", \"id_news234\", \"id_news236\", \n             \"id_news245\", \"id_news247\", \"id_news310\", \"id_news452\", \"id_news467\", \n             \"id_news469\", \"id_news480\", \"id_news20040\", \"id_news20345\", \n             \"id_news20346\", \"id_news20347\", \"id_news20352\", \"id_news20354\", \n             \"id_news21006\", \"id_news21025\", \"id_news21173\", \"id_news21176\", \n             \"id_news33718\", \"id_news34689\", \"id_news73\")\n\nhuxreg(\"Subscription price\" = subscription_price, \n       \"Unit price\" = unit_price, \n       \"Circulation\" = circulation, \n       \"Share of sub\" = share_of_sub,\n       \"Revenue from sales\" = revenue_from_sales,\n       omit_coefs = omit_me, \n       number_format = 2\n       )"},{"path":"causality.html","id":"other-points","chapter":"16 Causality from observational data","heading":"16.4.6 Other points","text":"certainly find many cases appears difference 1967 onward.general, able obtain results similar Angelucci Cagé, 2019. spent time, probably replicate findings perfectly. Isn’t great! else ?Parallel trends: Notice wonderful way test ‘parallel trends’ assumption pp. 350-351.Discussion: Look wonderful discussion (pp. 353-358) interpretation, external validity, robustness.–>","code":""},{"path":"causality.html","id":"case-study---funding-of-clinical-trials-and-reported-drug-efficacy","chapter":"16 Causality from observational data","heading":"16.5 Case study - Funding of Clinical Trials and Reported Drug Efficacy","text":"Oostrom (2021) looks clinical trials drugs. days, course, know lot may ever wished , clinical trials. one thing (think) know , well, clinical. mean, doesn’t matter actual trial, outcome . Oostrom (2021) says isn’t true.way background, clinical trials needed drug can approved. Oostrom (2021) finds pharmaceutical firms sponsor clinical trial, ‘drug appears 0.15 standard deviations effective trial sponsored drug’s manufacturer, compared drug trial without drug manufacturer’s involvement.’ exploiting fact often ‘exact sets drugs often compared different randomized control trials conducted parties different financial interests.’main finding (Oostrom 2021, 2):Utilizing dozens drug combinations across hundreds clinical trials, estimate drug appears 36 percent effective (0.15 standard deviations base 0.42) trial sponsored drug’s manufacturing marketing firm, compared drug, evaluated comparators, without drug manufacturer’s involvement. medical literature, measure efficacy, case antidepressants, share patients respond medication , case schizophrenia, average decline symptoms.might happen? Oostrom (2021) looks variety different options, grouped happen trial happen trial ‘publication bias.’ finds ‘publication bias can explain much half sponsorship effect. Incorporating data unpublished clinical trials, find sponsored trials less likely publish non-positive results drugs.’Oostrom (2021) focuses antidepressant antipsychotic drugs allows obtain dataset trials. ‘arm’ trial refers ‘unit randomization occurs. Arms often unique drugs occasionally refer unique drug dosage combinations.’ (Oostrom 2021, 9).Summary statistics provided summary table (Figure 16.7) (approach common economics, great idea hides distribution data - better plot raw data.)\nFigure 16.7: Summary statistics Ooostrom\nmodel :\\[y_{ij} = \\alpha + \\beta \\mbox{ Sponsor}_{ij} + X_{ij}\\gamma + G_{d(),s(j)} +\\epsilon_{ij}\\]\\(y_{ij}\\) efficacy arm \\(\\) trial \\(j\\). main coefficient interest \\(\\beta\\) based whether \\(\\mbox{Sponsor}_{ij}\\). outcome relative placebo arm trial, least effective arm.‘Table 3.3’ paper actually reason included case student. sounds odd ’ve read millions papers unclear results. ‘Table 3.3’ (republished Figure 16.8) beautiful ’ll allow speak .\nFigure 16.8: Results\npaper available : https://www.tamaroostrom.com/research ’d recommend brief read.","code":""},{"path":"causality.html","id":"regression-discontinuity-design","chapter":"16 Causality from observational data","heading":"16.6 Regression discontinuity design","text":"","code":""},{"path":"causality.html","id":"introduction-21","chapter":"16 Causality from observational data","heading":"16.6.1 Introduction","text":"Regression discontinuity design (RDD) popular way get causality continuous variable cut-offs determine treatment. difference student gets 79 per cent student gets 80 per cent? Probably much, one gets -, gets B+, seeing transcript affect gets job affect income. case percentage ‘forcing variable’ cut-- ‘threshold.’ treatment determined forcing variable need control variable. , seemingly arbitrary cut-offs can seen time. Hence, ‘explosion’ use regression discontinuity design (Figure 16.9).Please note ’ve followed terminology Taddy, 2019. Gelman Hill, 2007, others use slightly different terminology. instance, Cunningham refers forcing function running variable. doesn’t matter use long consistent. terminology familiar please feel free use , share !\nFigure 16.9: explosion regression discontinuity designs recent years.\nSource: John Holbein, 13 February 2020.key assumptions :cut-‘known, precise free manipulation’ (Cunningham, 2020, p. 163).forcing function continuous means can say people either side threshold , happening just fall either side threshold.","code":""},{"path":"causality.html","id":"simulated-example-1","chapter":"16 Causality from observational data","heading":"16.6.2 Simulated example","text":"Let’s generate data.Table 15.2:  Let’s make graph.can use dummy variable linear regression estimate effect (’re hoping ’s 2 imposed.)Table 16.4:  various caveats estimate ’ll get later, essentials .great thing regression discontinuity can almost good RCT. instance, (thank John Holbein pointer) Bloom, Bell, Reiman (2020) compare randomized trials RDDs find RCTs compare favourably.","code":"\nlibrary(broom)\nlibrary(tidyverse)\n\nset.seed(853)\n\nnumber_of_observation <- 1000\n\nrdd_example_data <- tibble(person = c(1:number_of_observation),\n                           grade = runif(number_of_observation, min = 78, max = 82),\n                           income = rnorm(number_of_observation, 10, 1)\n                           )\n\n## We want to make income more likely to be higher if they are have a grade over 80\nrdd_example_data <- \n  rdd_example_data %>% \n  mutate(income = if_else(grade > 80, income + 2, income))\n\nhead(rdd_example_data)\nrdd_example_data %>% \n  ggplot(aes(x = grade,\n             y = income)) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(data = rdd_example_data %>% filter(grade < 80), \n              method='lm',\n              color = \"black\") +\n  geom_smooth(data = rdd_example_data %>% filter(grade >= 80), \n              method='lm',\n              color = \"black\") +\n  theme_minimal() +\n  labs(x = \"Grade\",\n       y = \"Income ($)\")\n#> `geom_smooth()` using formula 'y ~ x'\n#> `geom_smooth()` using formula 'y ~ x'\nrdd_example_data <- \n  rdd_example_data %>% \n  mutate(grade_80_and_over = if_else(grade < 80, 0, 1)) \n\nlm(income ~ grade + grade_80_and_over, data = rdd_example_data) %>% \n  tidy()"},{"path":"causality.html","id":"different-slopes","chapter":"16 Causality from observational data","heading":"16.6.2.1 Different slopes","text":"Figure 16.10 shows example different slopes.\nFigure 16.10: Effect minimum unit pricing alcohol Scotland.\nSource: John Burn-Murdoch, 7 February 2020.","code":""},{"path":"causality.html","id":"overlap","chapter":"16 Causality from observational data","heading":"16.6.3 Overlap","text":"randomised control trial /B testing section, randomised assignment treatment, imposed control treatment groups treatment. moved difference--differences, assumed common trend treated control groups. allowed groups different, ‘difference ’ differences. Finally, considered matching, said even control treatment groups seemed quite different able match treated group similar ways, apart fact treated.regression discontinuity consider slightly different setting - two groups completely different terms forcing variable - either side threshold. overlap . know threshold believe either side essentially matched. Let’s consider 2019 NBA Eastern Conference Semifinals - Toronto Philadelphia. Game 1: Raptors win 108-95; Game 2: 76ers win 94-89; Game 3: 76ers win 116-95; Game 4: Raptors win 101-96; Game 5: Raptors win 125-89; Game 6: 76ers win 112-101; finally, Game 7: Raptors win 92-90, ball win bouncing rim four times. really much difference teams (Figure 16.11)?\nFigure 16.11: took four bounces go , different teams…?\nSource: Stan Behal / Postmedia Network.","code":""},{"path":"causality.html","id":"examples","chapter":"16 Causality from observational data","heading":"16.6.4 Examples","text":"difference--differences, learnt , began see opportunities implement everywhere. Frankly, find lot easier think legitimate examples using regression discontinuity difference--differences. , risk mentioning yet another movie 1990s none seen, think RDD, first thought often Sliding Doors (Figure 16.12).\nFigure 16.12: Nobody expects Spanish Inquisition.\nSource: Mlotek, Haley, 2018, ‘Almosts -ifs ’Sliding Doors’’, Ringer, 24 April, freely available : https://www.theringer.com/movies/2018/4/24/17261506/sliding-doors-20th-anniversary.movie great soundtrack help propel Gwyneth Paltrow super-stardom, features iconic moment Paltrow’s character, Helen, arrives tube station point movie splits two. one version just makes train, arrives home find boyfriend cheating ; another just misses train doesn’t find boyfriend.’d say, spoiler alert, movie released 1998, … course, ‘threshold’ turns important. world gets train leaves boyfriend, cuts hair, changes everything life. world misses train doesn’t. least initially. , can’t say better Ashley Fetters:end Sliding Doors, “bad” version Helen’s life elides right “good” version; even “bad” version, philandering !@#$%^& boyfriend eventually gets found dumped, true love eventually gets met-cute, MVP friend comes . According Sliding Doors philosophy, words, even lives take fluky, chaotic detours, ultimately good-hearted people find , bad boyfriends home-wreckers world get comeuppance. ’s freak turn events allows cheating boyfriend just keep cheating, well-meaning, morally upright soulmates just keep floating around universe unacquainted.Fetters, Ashley, 2018, ‘Think Lot: Sliding Doors Sliding Doors,’ Cut, 9 April, freely available : https://www.thecut.com/2018/04/-think----lot--sliding-doors--sliding-doors.html.’m getting -track , point , seem though ‘threshold,’ seems though ’s continuity!Let’s see legitimate implementations regression discontinuity. (thank Ryan Edwards pointing .)","code":""},{"path":"causality.html","id":"elections","chapter":"16 Causality from observational data","heading":"16.6.4.1 Elections","text":"Elections common area application regression discontinuity election close arguably ’s much difference candidates. plenty examples regression discontinuity elections setting, one recent one George, Siddharth Eapen, 2019, ‘Like Father, Like Son? Effect Political Dynasties Economic Development,’ freely available : https://www.dropbox.com/s/orhvh3n03wd9ybl/sid_JMP_dynasties_latestdraft.pdf?dl=0.paper George interested political dynasties. child politician likely elected child politician, happen also similarly skilled politics? Regression discontinuity can help close election, can look differences places someone narrowly won similar someone narrowly lost.George, 2019, case examines:descendant effects using close elections regression discontinuity (RD) design. focus close races dynastic descendants (.e. direct relatives former officeholders) non-dynasts, compare places descendant narrowly won descendant narrowly lost. elections, descendants non-dynasts similar demographic political characteristics, win similar places similar rates. Nevertheless, find negative economic effects descendant narrowly wins. Villages represented descendant lower asset ownership public good provision electoral term: households less likely live brick house basic amenities like refrigerator, mobile phone, vehicle. Moreover, voters assess descendants perform worse office. additional standard deviation exposure descendants lowers village’s wealth rank 12pp.model George, 2019, estimates (p. 19:\n\\[y_i = \\alpha_{\\mbox{district}} + \\beta \\times \\mbox{Years descendant rule}_i + f(\\mbox{Descendant margin}) + \\gamma X_i + \\epsilon_{,t}.\\]\nmodel, \\(y_i\\) various development outcomes village \\(\\); \\(\\mbox{Years descendant rule}_i\\) number years dynastic descendant represented village \\(\\) national state parliament; \\(\\mbox{Descendant margin}\\) vote share difference dynastic descendant non-dynast; \\(\\gamma X_i\\) vector village-level adjustments.George, 2019, conducts whole bunch tests validity regression discontinuity design (p. 19). critical order results believed. lot different results one shown Figure 16.13.\nFigure 16.13: George, 2019, descendant effects identified using close elections RD design (p. 41).\n","code":""},{"path":"causality.html","id":"economic-development","chapter":"16 Causality from observational data","heading":"16.6.4.2 Economic development","text":"One issues considering economic development place typically either subject treatment . However, sometimes regression discontinuity allows us compare areas just barely treated just barely .One recent paper Esteban Mendez-Chacon Diana Van Patten, 2020, ‘Multinationals, monopsony local development: Evidence United Fruit Company’ available : https://www.dianavanpatten.com/. interested effect United Fruit Company (UFCo), given land Costa Rica 1889 1984. given roughly 4 per cent national territory around 4500 acres. key land assignment redrawn 1904 based river hence re-assignment essentially random regard determinants growth point. compare areas assigned UFCo . find:find firm positive persistent effect living standards. Regions within UFCo 26 per cent less likely poor 1973 nearby counterfactual locations, 63 per cent gap closing following three decades. Company documents explain key concern time attract maintain sizable workforce, induced firm invest heavily local amenities likely account result.model :\n\\[y_{,g,t} = \\gamma\\mbox{UFCo}_g + f(\\mbox{geographic location}_g) + X_{,g,t}\\beta + X_g\\Gamma + \\alpha_t + \\epsilon_{,g,t}.\\]\nmodel, \\(y_{,g,t}\\) development outcome household \\(\\) census-block \\(g\\) year \\(t\\); \\(\\gamma\\mbox{UFCo}_g\\) indicator variable whether census-block UFCo area ; \\(f(\\mbox{geographic location}_g)\\) function latitude longitude adjust geographic area; \\(X_{,g,t}\\) covariates household \\(\\); \\(X_g\\) geographic characteristics census-block; \\(\\alpha_t\\) year fixed effect., lot different results one shown Figure 16.14.\nFigure 16.14: George, 2020, UFCo effect probability poor (p. 17).\n","code":""},{"path":"causality.html","id":"implementation","chapter":"16 Causality from observational data","heading":"16.6.5 Implementation","text":"Although fairly conceptually similar work done past, wanting use regression discontinuity work might like consider specialised package. package rdrobust one recommendation, although others available try interested. (rdd package go-, seems taken CRAN recently. use RDD, maybe just follow see comes back one pretty nice.)Let’s look example using rdrobust.","code":"\nlibrary(rdrobust)\nrdrobust(y = rdd_example_data$income, \n         x = rdd_example_data$grade, \n         c = 80, h = 2, all = TRUE) %>% \n  summary()\n#> Call: rdrobust\n#> \n#> Number of Obs.                 1000\n#> BW type                      Manual\n#> Kernel                   Triangular\n#> VCE method                       NN\n#> \n#> Number of Obs.                  497          503\n#> Eff. Number of Obs.             497          503\n#> Order est. (p)                    1            1\n#> Order bias  (q)                   2            2\n#> BW est. (h)                   2.000        2.000\n#> BW bias (b)                   2.000        2.000\n#> rho (h/b)                     1.000        1.000\n#> Unique Obs.                     497          503\n#> \n#> =============================================================================\n#>         Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]       \n#> =============================================================================\n#>   Conventional     1.974     0.143    13.783     0.000     [1.693 , 2.255]     \n#> Bias-Corrected     1.977     0.143    13.805     0.000     [1.696 , 2.258]     \n#>         Robust     1.977     0.211     9.374     0.000     [1.564 , 2.390]     \n#> ============================================================================="},{"path":"causality.html","id":"fuzzy-rdd","chapter":"16 Causality from observational data","heading":"16.6.6 Fuzzy RDD","text":"examples point ‘sharp’ RDD. , threshold strict. However, reality, often boundary little less strict. instance, consider drinking age. Although legal drinking age, say 19. looked number people drank, ’s likely increase years leading age. Perhaps went Australia drinking age 18 drank. perhaps snuck bar 17, etc.sharp RDD setting, know value forcing function know outcome. instance, get grade 80 know got -, got grade 79 know got B+. fuzzy RDD known probability. can say Canadian 19-year-old likely drunk alcohol Canadian 18 year old, number Canadian 18-year-olds drunk alcohol zero.may possible deal fuzzy RDD settings appropriate choice model data. may also possible deal using instrumental variables, cover next section.","code":""},{"path":"causality.html","id":"threats-to-validity","chapter":"16 Causality from observational data","heading":"16.6.7 Threats to validity","text":"continuity assumption fairly important, test based counterfactual. Instead need convince people . Ways include:Using test/train set-.Trying different specifications (careful results don’t broadly persist just consider linear quadratic functions).Considering different subsets data.Consider different windows.-front uncertainty intervals, especially graphs.Discuss assuage concerns possibility omitted variables.threshold also important. instance, actual shift non-linear relationship?want ‘sharp’ effect possible, thresholds known, gamed. instance, lot evidence people run certain marathon times, know people aim certain grades. Similarly, side, lot easier instructor just give justify Bs. One way look consider ‘balanced’ sample either side threshold. using histograms appropriate bins, instance Figure 16.15, E. J. Allen et al. (2017).\nFigure 16.15: Bunching around marathon times.\nneed really think possible effect decision around choice model. see consider difference linear polynomial.","code":"\nsome_data <- \n  tibble(outcome = rnorm(n = 100, mean = 1, sd = 1),\n         running_variable = c(1:100),\n         location = \"before\")\n\nsome_more_data <- \n  tibble(outcome = rnorm(n = 100, mean = 2, sd = 1),\n         running_variable = c(101:200),\n         location = \"after\")\n\nboth <- \n  rbind(some_data, some_more_data)\n\nboth %>% \n  ggplot(aes(x = running_variable, y = outcome, color = location)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(formula = y~x, method = 'lm')\n  \nboth %>% \n  ggplot(aes(x = running_variable, y = outcome, color = location)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(formula = y ~ poly(x, 3), method = 'lm')"},{"path":"causality.html","id":"weaknesses","chapter":"16 Causality from observational data","heading":"16.6.8 Weaknesses","text":"External validity may hard - think -/B+ example - think findings generalise B-/C+?important responses close cut-. even whole bunch B- + students, don’t really help much. Hence need lot data.lot freedom researcher, open science best practice becomes vital.","code":""},{"path":"causality.html","id":"case-study---stiers-hooghe-and-dassonneville-2020","chapter":"16 Causality from observational data","heading":"16.7 Case study - Stiers, Hooghe, and Dassonneville, 2020","text":"Paper: Stiers, D., Hooghe, M. Dassonneville, R., 2020. Voting 16: lowering voting age lead political engagement? Evidence quasi-experiment city Ghent (Belgium). Political Science Research Methods, pp.1-8. Available : https://www.cambridge.org/core/journals/political-science-research--methods/article/voting--16--lowering--voting-age-lead---political-engagement-evidence---quasiexperiment---city--ghent-belgium/172A2D9B75ECB66E98C9680787F302AD#fndtn-informationData: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/J1FQW9","code":""},{"path":"causality.html","id":"case-study---caughey-and-sekhon.-2011","chapter":"16 Causality from observational data","heading":"16.8 Case study - Caughey, and Sekhon., 2011","text":"Paper: Caughey, Devin, Jasjeet S. Sekhon. “Elections regression discontinuity design: Lessons close US house races, 1942–2008.” Political Analysis 19.4 (2011): 385-408. Available : https://www.cambridge.org/core/journals/political-analysis/article/elections---regression-discontinuity-design-lessons--close-us-house-races-19422008/E5A69927D29BE682E012CAE9BFD8AEB7Data: https://dataverse.harvard.edu/dataset.xhtml?persistentId=hdl:1902.1/16357&version=1.0","code":""},{"path":"causality.html","id":"instrumental-variables","chapter":"16 Causality from observational data","heading":"16.9 Instrumental variables","text":"","code":""},{"path":"causality.html","id":"introduction-22","chapter":"16 Causality from observational data","heading":"16.9.1 Introduction","text":"Instrumental variables (IV) approach can handy type treatment control going , lot correlation variables possibly don’t variable actually measures interested . adjusting observables enough create good estimate. Instead find variable - eponymous instrumental variable - :correlated treatment variable, butnot correlated outcome.solves problem way instrumental variable can effect treatment variable, able adjust understanding effect treatment variable appropriately. trade-instrumental variables must satisfy bunch different assumptions, , frankly, difficult identify ex ante. Nonetheless, able use powerful tool speaking causality.canonical instrumental variables example smoking. days know smoking causes cancer. smoking correlated lot variables, instance, education, actually education causes cancer. RCTs may possible, likely troublesome terms speed ethics, instead look variable correlated smoking, , , lung cancer. case, look tax rates, policy responses, cigarettes. tax rates cigarettes correlated number cigarettes smoked, correlated lung cancer, impact cigarette smoking, can assess effect cigarettes smoked lung cancer.implement instrumental variables first regress tax rates cigarette smoking get coefficient instrumental variable, (separate regression) regress tax rates lung cancer get coefficient instrumental variable. estimate ratio coefficients. (Gelman Hill 2007, 219) describe ratio ‘Wald estimate.’Following language (Gelman Hill 2007, 216) use instrumental variables make variety assumptions including:Ignorability instrument.Correlation instrumental variable treatment variable.Monotonicity.Exclusion restriction.summarise exactly instrumental variables , better recommend first pages ‘Instrumental Variables’ chapter Cunningham (2021), key paragraph particular (way background, Cunningham explained impossible randomly allocate ‘clean’ ‘dirty’ water randomised controlled trial continues…):Snow need way trick data allocation clean dirty water people associated determinants cholera mortality, hygiene poverty. just need someone something making treatment assignment .Fortunately Snow, rest London, someone something existed. London 1800s, many different water companies serving different areas city. served one company. Several took water Thames, heavily polluted sewage. service areas companies much higher rates cholera. Chelsea water company exception, exceptionally good filtration system. ’s Snow major insight. 1849, Lambeth water company moved intake point upstream along Thames, main sewage discharge point, giving customers purer water. Southwark Vauxhall water company, hand, left intake point downstream sewage discharged. Insofar kinds people company serviced approximately , comparing cholera rates two houses experiment Snow desperately needed test hypothesis.","code":""},{"path":"causality.html","id":"history","chapter":"16 Causality from observational data","heading":"16.9.2 History","text":"history instrumental variables rare statistical mystery, Stock Trebbi (2003) provide brief overview. method first published Wright (1928). book effect tariffs animal vegetable oil. might instrumental variables important book tariffs animal vegetable oil? fundamental problem effect tariffs depends supply demand. know prices quantities, don’t know driving effect. can use instrumental variables pin causality.gets interesting, becomes something mystery, instrumental variables discussion Appendix B. made major statistical break-hide appendix? , Philip G. Wright, book’s author, son Sewall Wright, considerable expertise statistics specific method used Appendix B. Hence mystery Appendix B - Philip Sewall write ? Cunningham (2021) Stock Trebbi (2003) go detail, balance feel likely Philip actually author work.","code":""},{"path":"causality.html","id":"simulated-example-2","chapter":"16 Causality from observational data","heading":"16.9.3 Simulated example","text":"Let’s generate data. explore simulation related canonical example health status, smoking, tax rates. looking explain healthy someone based amount smoke, via tax rate smoking. going generate different tax rates provinces. understanding tax rate cigarettes now pretty much provinces, fairly recent. ’ll pretend Alberta low tax, Nova Scotia high tax.reminder, simulating data illustrative purposes, need impose answer want. actually use instrumental variables reversing process.Now need relate number cigarettes someone smoked health. ’ll model health status draw normal distribution, either high low mean depending whether person smokes.Now need relationship cigarettes province (illustration, provinces different tax rates).Table 16.5:  Now can look data.Finally, can use tax rate instrumental variable estimate effect smoking health.find, luckily, smoke health likely worse don’t smoke.Equivalently, can think instrumental variables two-stage regression context.","code":"\nlibrary(broom)\nlibrary(tidyverse)\n\nset.seed(853)\n\nnumber_of_observation <- 10000\n\niv_example_data <- tibble(person = c(1:number_of_observation),\n                          smoker = sample(x = c(0:1),\n                                          size = number_of_observation, \n                                          replace = TRUE)\n                          )\niv_example_data <- \n  iv_example_data %>% \n  mutate(health = if_else(smoker == 0,\n                          rnorm(n = n(), mean = 1, sd = 1),\n                          rnorm(n = n(), mean = 0, sd = 1)\n                          )\n         )\n## So health will be one standard deviation higher for people who don't or barely smoke.\niv_example_data <- \n  iv_example_data %>% \n  rowwise() %>% \n  mutate(province = case_when(smoker == 0 ~ sample(x = c(\"Nova Scotia\", \"Alberta\"),\n                                                                       size = 1, \n                                                                       replace = FALSE, \n                                                                       prob = c(1/2, 1/2)),\n                              smoker == 1 ~ sample(x = c(\"Nova Scotia\", \"Alberta\"),\n                                                                       size = 1, \n                                                                       replace = FALSE, \n                                                                       prob = c(1/4, 3/4)))) %>% \n  ungroup()\n\niv_example_data <- \n  iv_example_data %>% \n  mutate(tax = case_when(province == \"Alberta\" ~ 0.3,\n                         province == \"Nova Scotia\" ~ 0.5,\n                         TRUE ~ 9999999\n  )\n  )\n\niv_example_data$tax %>% table()\n#> .\n#>  0.3  0.5 \n#> 6206 3794\n\nhead(iv_example_data)\niv_example_data %>% \n  mutate(smoker = as_factor(smoker)) %>% \n  ggplot(aes(x = health, fill = smoker)) +\n  geom_histogram(position = \"dodge\", binwidth = 0.2) +\n  theme_minimal() +\n  labs(x = \"Health rating\",\n       y = \"Number of people\",\n       fill = \"Smoker\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  facet_wrap(vars(province))\nhealth_on_tax <- lm(health ~ tax, data = iv_example_data)\nsmoker_on_tax <- lm(smoker ~ tax, data = iv_example_data)\n\ncoef(health_on_tax)[\"tax\"] / coef(smoker_on_tax)[\"tax\"]\n#>        tax \n#> -0.8554502\nfirst_stage <- lm(smoker ~ tax, data = iv_example_data)\nhealth_hat <- first_stage$fitted.values\nsecond_stage <- lm(health ~ health_hat, data = iv_example_data)\n\nsummary(second_stage)\n#> \n#> Call:\n#> lm(formula = health ~ health_hat, data = iv_example_data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.9867 -0.7600  0.0068  0.7709  4.3293 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  0.91632    0.04479   20.46   <2e-16 ***\n#> health_hat  -0.85545    0.08911   -9.60   <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.112 on 9998 degrees of freedom\n#> Multiple R-squared:  0.009134,   Adjusted R-squared:  0.009034 \n#> F-statistic: 92.16 on 1 and 9998 DF,  p-value: < 2.2e-16"},{"path":"causality.html","id":"implementation-1","chapter":"16 Causality from observational data","heading":"16.9.4 Implementation","text":"regression discontinuity, although possible use existing functions, might worth looking specialised packages. Instrumental variables moving pieces, specialised package can help keep everything organised, additionally, standard errors need adjusted specialised packages make easier. package estimatr recommendation, although others available try interested. estimatr package team DeclareDesign.Let’s look example using iv_robust().","code":"\nlibrary(estimatr)\niv_robust(health ~ smoker | tax, data = iv_example_data) %>% \n  summary()\n#> \n#> Call:\n#> iv_robust(formula = health ~ smoker | tax, data = iv_example_data)\n#> \n#> Standard error type:  HC2 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value   Pr(>|t|) CI Lower\n#> (Intercept)   0.9163    0.04057   22.59 3.163e-110   0.8368\n#> smoker       -0.8555    0.08047  -10.63  2.981e-26  -1.0132\n#>             CI Upper   DF\n#> (Intercept)   0.9958 9998\n#> smoker       -0.6977 9998\n#> \n#> Multiple R-squared:  0.1971 ,    Adjusted R-squared:  0.197 \n#> F-statistic:   113 on 1 and 9998 DF,  p-value: < 2.2e-16"},{"path":"causality.html","id":"assumptions-1","chapter":"16 Causality from observational data","heading":"16.9.5 Assumptions","text":"discussed earlier, variety assumptions made using instrumental variables. two important :Exclusion Restriction. assumption instrumental variable affects dependent variable independent variable interest.Relevance. must actually relationship instrumental variable independent variable.typically trade-two. plenty variables thatWhen thinking potential instrumental variables Cunningham (2021), p. 211, puts brilliantly:, let’s say think good instrument. might defend someone else? necessary sufficient condition instrument can satisfy exclusion restriction people confused tell instrument’s relationship outcome. Let explain. one going confused tell think family size reduce female labor supply. don’t need Becker model convince women children probably work less fewer children. ’s common sense. , think told mothers whose first two children gender worked less whose children balanced sex ratio? probably give confused look. gender composition children whether woman works?doesn’t – matters, fact, people whose first two children gender decide third child. brings us back original point – people buy family size can cause women work less, ’re confused say women work less first two kids gender. point two children’s gender induces people larger families otherwise, person\n“gets ,” might excellent instrument.Relevance can tested using regression tests correlation. exclusion restriction tested. need present evidence convincing arguments. Cunningham (2021) p. 225 says ‘Instruments certain ridiculousness [.] , know good instrument instrument doesn’t seem relevant explaining outcome interest ’s exclusion restriction implies.’","code":""},{"path":"causality.html","id":"conclusion","chapter":"16 Causality from observational data","heading":"16.9.6 Conclusion","text":"Instrumental variables useful approach one can obtain causal estimates even without explicit randomisation. Finding instrumental variables used bit white whale, especially academia. However, leave final (hopefully motivating) word Taddy (2019), p. 162:final point importance IV models analysis, note inside firm—especially inside modern technology firm—explicitly randomised instruments everywhere…. often case decision-makers want understand effects policies randomised rather downstream things AB tested. example, suppose algorithm used predict creditworthiness potential borrowers assign loans. Even process loan assignment never randomised, parameters machine learning algorithms used score credit AB tested, experiments can used instruments loan assignment treatment. ‘upstream randomisation’ extremely common IV analysis key tool causal inference setting.’","code":""},{"path":"causality.html","id":"case-study---effect-of-police-on-crime","chapter":"16 Causality from observational data","heading":"16.10 Case study - Effect of Police on Crime","text":"","code":""},{"path":"causality.html","id":"overview-5","chapter":"16 Causality from observational data","heading":"16.10.1 Overview","text":"’ll use example Levitt (2002) looks effect police crime. interesting might think, police associated lower crime. , actually opposite, crime causes police hired - many police hypothetical country crime need? Hence need find sort instrumental variable affects crime relationship number police (, , related crime), yet also correlated police numbers. Levitt (2002) suggests number firefighters city.Levitt (2002) argues firefighters appropriate instrument, ‘(f)actors power public sector unions, citizen tastes government services, affirmative action initiatives, mayor’s desire provide spoils might expected jointly influence number firefighters police.’ Levitt (2002) also argues relevance assumption met showing ‘changes number police officers firefighters within city highly correlated time.’terms satisfying exclusion restriction, Levitt (2002) argues number firefighters ‘direct impact crime.’ However, may common factors, Levitt (2002) adjusts regression.","code":""},{"path":"causality.html","id":"data-1","chapter":"16 Causality from observational data","heading":"16.10.2 Data","text":"dataset based 122 US cities 1975 1995. Summary statistics provided Figure 16.16.\nFigure 16.16: Summary statistics Levitt 2002.\nSource: Levitt (2002) p. 1,246.","code":""},{"path":"causality.html","id":"model-1","chapter":"16 Causality from observational data","heading":"16.10.3 Model","text":"first stage Levitt (2002) looks police function firefighters, bunch adjustment variables:\n\\[\\ln(\\mbox{Police}_{ct}) = \\gamma \\ln(\\mbox{Fire}_{ct}) + X'_{ct}\\Gamma + \\lambda_t + \\phi_c + \\epsilon_{ct}.\\]\nimportant part police firefighters numbers per capita basis. bunch adjustment variables \\(X\\) includes things like state prisoners per capita, unemployment rate, etc, well year dummy variables fixed-effects city.established relationship police firefights, Levitt (2002) can use estimates number police, based number firefighters, explain crime rates:\n\\[\\Delta\\ln(\\mbox{Crime}_{ct}) = \\beta_1 \\ln(\\mbox{Police}_{ct-1}) + X'_{ct}\\Gamma + \\Theta_c + \\mu_{ct}.\\]typical way present instrumental variable results show stages. Figure 16.17 shows relationship police firefighters.\nFigure 16.17: relationship firefighters, police crime.\nSource: Levitt (2002) p. 1,247.Figure 16.18 shows relationship police crime, IV results ones interest.\nFigure 16.18: impact police crime.\nSource: Levitt (2002) p. 1,248.","code":""},{"path":"causality.html","id":"discussion","chapter":"16 Causality from observational data","heading":"16.10.4 Discussion","text":"key finding Levitt (2002) negative effect number police amount crime.variety points want raise regard paper. come across little negative, mostly just paper 2002, reading today, standards changed.’s fairly remarkable reliant various model specifications results . results bounce around fair bit ’s just ones reported. Chances bunch results reported, interest see impact.note, fairly limited model validation. probably something aware days, seems likely fair degree -fitting .Levitt (2002) actually response, another researcher, McCrary (2002), found issues original paper: Levitt (1997). Levitt appears quite decent , jarring see Levitt thanked McCrary (2002) providing ‘data computer code.’ Levitt decent providing data code? code unintelligible? ways nice see far come - author similar paper days forced make code data available part paper, wouldn’t ask . reinforces importance open data reproducible science.","code":""},{"path":"causality.html","id":"exercises-and-tutorial-15","chapter":"16 Causality from observational data","heading":"16.11 Exercises and tutorial","text":"","code":""},{"path":"causality.html","id":"exercises-15","chapter":"16 Causality from observational data","heading":"16.11.1 Exercises","text":"","code":""},{"path":"causality.html","id":"tutorial-15","chapter":"16 Causality from observational data","heading":"16.11.2 Tutorial","text":"","code":""},{"path":"mrp.html","id":"mrp","chapter":"17 Multilevel regression with post-stratification","heading":"17 Multilevel regression with post-stratification","text":"STATUS: construction.Required materialRead Analyzing name changes marriage using non-representative survey, (M. Alexander 2019b).Read Chapter 17 Regression Stories, (Gelman, Hill, Vehtari 2020).Read introduction multilevel regression post-stratification estimating constituency opinion, (Hanretty 2020).Read Introduction Estimating State Public Opinion Multi-Level Regression Poststratification using R, (Kastellec, Lax, Phillips 2016).Read Using sex gender survey adjustment, (Kennedy et al. 2020).Read MRP rstanarm, (Kennedy Gabry 2020).Read Know population know model: Using model-based regression poststratification generalize findings beyond observed sample, (Kennedy Gelman 2020).Read Forecasting elections non-representative polls, (W. Wang et al. 2015).Read Chapter 17 Sampling Theory Practice, (Wu Thompson 2020).Watch Statistical Models Election Outcomes, (Gelman 2020).Listen Episode 248: Democrats irrational? (David Shor), (Galef 2020).Recommended readingArnold, Jeffrey B., 2018, ‘Simon Jackman’s Bayesian Model Examples Stan,’ Ch 13, 7 May, https://jrnold.github.io/bugs-examples--stan/campaign.html.Cohn, Nate, 2016, ‘Gave Four Good Pollsters Raw Data. Four Different Results,’ New York Times, Upshot, 20 September, https://www.nytimes.com/interactive/2016/09/20/upshot/-error--polling-world-rarely-talks-.html.Edelman, M., Vittert, L., & Meng, X.-L., 2021, ‘Interview Murray Edelman History Exit Poll,’ Harvard Data Science Review, https://doi.org/10.1162/99608f92.3a25cd24 https://hdsr.mitpress.mit.edu/pub/fekmqbv4/release/2.Gelman, Andrew, Julia Azari, 2017, ‘19 things learned 2016 election,’ Statistics Public Policy, 4 (1), pp. 1-10.Gelman, Andrew, Jessica Hullman, Christopher Wlezien, 2020, ‘Information, incentives, goals election forecasts,’ 8 September, available : http://www.stat.columbia.edu/~gelman/research/unpublished/forecast_incentives3.pdfGelman, Andrew, Merlin Heidemanns, Elliott Morris, 2020, ‘2020 US POTUS model,’ Economist, freely available: https://github.com/TheEconomist/us-potus-model.Ghitza, Yair, Andrew Gelman, 2013, ‘Deep Interactions MRP: Election Turnout Voting Patterns Among Small Electoral Subgroups,’ American Journal Political Science, 57 (3), pp. 762-776.Ghitza, Yair, Andrew Gelman, 2020, ‘Voter Registration Databases MRP: Toward Use Large-Scale Databases Public Opinion Research,’ Political Analysis, pp. 1-25.Imai, Kosuke, 2017, Quantitative Social Science: Introduction, Princeton University Press, Ch 4.1, 5.3.Jackman, Simon, 2005, ‘Pooling polls election campaign,’ Australian Journal Political Science, 40 (4), pp. 499-517.Jackman, Simon, Shaun Ratcliff Luke Mansillo, 2019, ‘Small area estimates public opinion: Model-assisted post-stratification data voter advice applications,’ 4 January, https://www.cambridge.org/core/membership/services/aop-file-manager/file/5c2f6ebb7cf9ee1118d11c0a/APMM-2019-Simon-Jackman.pdf.Lauderdale, Ben, Delia Bailey, Jack Blumenau, Doug Rivers, 2020, ‘Model-based pre-election polling national sub-national outcomes US UK,’ International Journal Forecasting, 36 (2), pp. 399-413.Leigh, Andrew, Justin Wolfers, 2006, ‘Competing approaches forecasting elections: Economic models, opinion polling prediction markets,’ Economic Record, 82 (258), pp.325-340.Nickerson, David W., Todd Rogers, 2014, ‘Political campaigns big data,’ Journal Economic Perspectives, 28 (2), pp. 51-74.Shirani-Mehr, Houshmand, David Rothschild, Sharad Goel, Andrew Gelman, 2018, ‘Disentangling bias variance election polls,’ Journal American Statistical Association, 113 (522), pp. 607-614.Recommended viewingJackman, Simon, 2020, ‘triumph quants?: Model-based poll aggregation election forecasting,’ Ihaka Lecture Series, https://youtu./MvGYsKIsLFs.Key librariesbrmsbroomgtsummaryhavenlabelledlme4modelsummaryrstanarmtidybayestidyverseQuizYour Mum asked ’ve learning term. decide tell multilevel regression post-stratification (MRP). Please explain MRP . Mum university-education, necessarily taken statistics, need explain technical terms use. [Please write two three paragraphs; strong answers clear strengths weaknesses.]respect W. Wang et al. (2015): paper interesting? like paper? wish better? extent can reproduce paper? [Please write one two paragraphs aspect.]respect W. Wang et al. (2015), feature mention election forecasts need?\nExplainable.\nAccurate.\nCost-effective.\nRelevant.\nTimely.\nExplainable.Accurate.Cost-effective.Relevant.Timely.respect W. Wang et al. (2015), weakness MRP?\nDetailed data requirement.\nAllows use biased data.\nExpensive conduct.\nDetailed data requirement.Allows use biased data.Expensive conduct.respect W. Wang et al. (2015), concerning Xbox sample?\nNon-representative.\nSmall sample size.\nMultiple responses respondent.\nNon-representative.Small sample size.Multiple responses respondent.interested studying voting intentions 2020 US presidential election vary individual’s income. set logistic regression model study relationship. study, possible independent variables : [Please check apply.]\nWhether respondent registered vote (yes/).\nWhether respondent going vote Biden (yes/).\nrace respondent (white/white).\nrespondent’s marital status (married/).\nWhether respondent registered vote (yes/).Whether respondent going vote Biden (yes/).race respondent (white/white).respondent’s marital status (married/).Please think N. Cohn (2016) type exercise carried ? think different groups, even background level quantitative sophistication, different estimates even use data? [Please write paragraph two aspect.]think multilevel regression post-stratification, key assumptions making? [Please write one two paragraphs aspect.]train model based survey, post-stratify using 2020 ACS dataset. practical considerations may contend ? [Please write paragraph least three considerations.]similar manner Ghitza Gelman (2020) pretend ’ve got access US voter file record private company. train model 2020 US CCES, post-stratify , individual-basis, based voter file.\nplease put-together datasheet voter file dataset following Gebru et al. (2020)? reminder, datasheets accompany datasets document ‘motivation, composition, collection process, recommended uses,’ among aspects.\nalso please put together model card model, following Mitchell et al. (2019)? reminder, model cards deliberately straight-forward one- two-page documents report aspects : model details; intended use; metrics; training data; ethical considerations; well caveats recommendations (Mitchell et al. 2019).\nplease discuss three ethical aspects around features using model? [Please write paragraph two point.]\nplease detail protections put place terms dataset, model, predictions?\nplease put-together datasheet voter file dataset following Gebru et al. (2020)? reminder, datasheets accompany datasets document ‘motivation, composition, collection process, recommended uses,’ among aspects.also please put together model card model, following Mitchell et al. (2019)? reminder, model cards deliberately straight-forward one- two-page documents report aspects : model details; intended use; metrics; training data; ethical considerations; well caveats recommendations (Mitchell et al. 2019).please discuss three ethical aspects around features using model? [Please write paragraph two point.]please detail protections put place terms dataset, model, predictions?model output lm() called ‘my_model_output’ can use modelsummary display output (assume package loaded) [please select apply]?\nmodelsummary::modelsummary(my_model_output)\nmodelsummary(my_model_output)\nmy_model_output %>% modelsummary()\nmy_model_output %>% modelsummary(statistic = NULL)\nmodelsummary::modelsummary(my_model_output)modelsummary(my_model_output)my_model_output %>% modelsummary()my_model_output %>% modelsummary(statistic = NULL)following examples linear models [please select apply]?\nlm(y ~ x_1 + x_2 + x_3, data = my_data)\nlm(y ~ x_1 + x_2^2 + x_3, data = my_data)\nlm(y ~ x_1 * x_2 + x_3, data = my_data)\nlm(y ~ x_1 + x_1^2 + x_2 + x_3, data = my_data)\nlm(y ~ x_1 + x_2 + x_3, data = my_data)lm(y ~ x_1 + x_2^2 + x_3, data = my_data)lm(y ~ x_1 * x_2 + x_3, data = my_data)lm(y ~ x_1 + x_1^2 + x_2 + x_3, data = my_data)Consider situation survey dataset age-groups: 18-29; 30-44; 45- 60; 60+. post-stratification dataset age-groups: 18-24; 25-29; 30-34; 35-39; 40-44; 45-49; 50-54; 55-59; 60+. approach take bringing together? [Please write paragraph.]Consider situation survey dataset age-groups: 18-29; 30-44; 45- 60; 60+. time post-stratification dataset age-groups: 18-34; 35-49; 50-64; 65+. approach take bringing together? [Please write paragraph.]Please consider Kennedy et al. (2020). statistical facets considering survey focused gender, post-stratification survey ? [Please check apply.]\nImpute non-male female\nEstimate gender using auxiliary information\nImpute population\nImpute sample values\nModel population distribution using auxiliary data\nRemove non-binary respondents\nRemove respondents\nAssume population distribution\nImpute non-male femaleEstimate gender using auxiliary informationImpute populationImpute sample valuesModel population distribution using auxiliary dataRemove non-binary respondentsRemove respondentsAssume population distributionPlease consider Kennedy et al. (2020). ethical facets considering survey focused gender, post-stratification survey ? [Please check apply.]\nImpute non-male female\nEstimate gender using auxiliary information\nImpute population\nImpute sample values\nModel population distribution using auxiliary data\nRemove non-binary respondents\nRemove respondents\nAssume population distribution\nImpute non-male femaleEstimate gender using auxiliary informationImpute populationImpute sample valuesModel population distribution using auxiliary dataRemove non-binary respondentsRemove respondentsAssume population distributionPlease consider Kennedy et al. (2020). define ethics?\nRespecting perspectives dignity individual survey respondents.\nGenerating estimates general population subpopulations interest.\nUsing complicated procedures serve useful function.\nRespecting perspectives dignity individual survey respondents.Generating estimates general population subpopulations interest.Using complicated procedures serve useful function.","code":""},{"path":"mrp.html","id":"introduction-23","chapter":"17 Multilevel regression with post-stratification","heading":"17.1 Introduction","text":"[Presidential election ] 2016 largest analytics failure US political history.David Shor, 13 August 2020Multilevel regression post-stratification (MRP) popular way adjust non-representative samples better analyse opinion survey responses.8 uses regression model relate individual-level survey responses various characteristics rebuilds sample better match population. way MRP can allow better understanding responses, also allow us analyse data may otherwise unusable. However, can challenge get started MRP terminology may unfamiliar, data requirements can onerous.Let’s say biased survey. Maybe conducted survey computers academic seminar, folks post-graduate degrees likely -represented. nonetheless interested making claims population. Let’s say found 37.5 per cent respondents prefer Macs. One way forward just ignore bias say ‘37.5 per cent people prefer Macs.’ Another way say, well 50 per cent respondents post-graduate degree prefer Macs, without post-graduate degree, 25 per cent prefer Macs. knew proportion broader population post-graduate degree, let’s assume 10 per cent, conduct re-weighting, post-stratification, follows: \\(0.5 * 0.1 + 0.25 * 0.9 = 0.275\\), estimate 27.5 per cent people prefer Macs. MRP third approach, uses model help re-weighting. use logistic regression estimate relationship preferring Macs highest educational attainment survey. apply relationship population dataset.MRP handy approach dealing survey data. Hanretty (2020) puts well says ‘MRP used alternatives either poor expensive.’ Essentially, trains model based survey, applies trained model another dataset. two main, related, advantages:can allow us ‘re-weight’ way includes uncertainty front--mind isn’t hamstrung small samples. alternative way deal small sample either go gather data throw away.can allow us use broad surveys speak subsets. Hanretty (2020) says ‘poor alternative [using MRP] simply splitting large sample (much) smaller geographic subsamples. poor alternative guarantee sample representative national level representative broken smaller groups.’practical perspective, tends less expensive collect non-probability samples benefits able use types data. said, magic-bullet laws statistics still apply. larger uncertainty around estimates still subject usual biases. Lauren Kennedy points , ‘MRP traditionally used probability surveys potential non-probability surveys, ’re sure limitations moment.’ ’s exciting area research academia industry.workflow need MRP straight-forward, details tiny decisions made step can become overwhelming. point need keep mind trying create relationship two datasets using statistical model, need establish similarity two datasets terms variables levels. steps :gather prepare survey dataset, thinking needed coherence post-stratification dataset;gather prepare post-stratification dataset thinking needed coherence survey dataset;model variable interest survey using independent variables levels available survey post-stratification datasets;apply model post-stratification data.notes, begin simulating situation pretend know features population. move famous example MRP used survey data Xbox platform exit poll data forecast 2012 US election. move examples Australian political situation. discuss features aware conducting MRP.","code":""},{"path":"mrp.html","id":"simulation---toddler-bedtimes","chapter":"17 Multilevel regression with post-stratification","heading":"17.2 Simulation - Toddler bedtimes","text":"","code":""},{"path":"mrp.html","id":"construct-a-population","chapter":"17 Multilevel regression with post-stratification","heading":"17.2.1 Construct a population","text":"get started simulate data population various properties, take biased sample, conduct MRP demonstrate can get properties back. going two ‘explanatory variables’ - age-group toilet-trained - one dependent variable - bedtime. Bed-time increase age-group increases, later children toilet-trained, compared . clear, example ‘know’ ‘true’ features population, isn’t something occurs use real data - just help understand MRP . ’re going rely heavily tidyverse package (Wickham et al. 2019a).point, Figure 17.1 provides invaluable advice (thank Mahfouz).\nFigure 17.1: Bernie ask us ?\n, always, dataset, first try plot better understand going (million points, ’ll just grab first 1,000 plots nicely).can also work ‘truth’ information interested (remembering ’d never actually know move away simulated examples).","code":"\n# Uncomment this (by deleting the #) if you need to install the packages\n# install.packages('tidyverse')\nlibrary(tidyverse)\n\n# This helps reproducibility\n# It makes it more likely that you're able to get the same random numbers as in this example.\nset.seed(853)\n\n# One million people in our population.\nsize_of_population <- 1000000\n\npopulation_for_mrp_example <- \n  tibble(age_group = sample(x = c(1:3), # Draw from any of 1, 2, 3.\n                            size = size_of_population,\n                            replace = TRUE # After you draw a number, allow that number to be drawn again.\n                            ),\n         toilet_trained = sample(x = c(0, 1),\n                                 size = size_of_population,\n                                 replace = TRUE\n                                 ),\n         noise = rnorm(size_of_population, mean = 0, sd = 1), \n         bed_time = 5 + 0.5 * age_group + 1 * toilet_trained + noise, # Make bedtime a linear function of the variables that we just generated and a intercept (no special reason for that intercept to be five; it could be anything).\n         ) %>% \n  select(-noise) %>% \n  mutate(age_group = as_factor(age_group),\n         toilet_trained = as_factor(toilet_trained)\n         )\n\npopulation_for_mrp_example %>% \n  head()\n#> # A tibble: 6 × 3\n#>   age_group toilet_trained bed_time\n#>   <fct>     <fct>             <dbl>\n#> 1 1         0                  5.74\n#> 2 2         1                  6.48\n#> 3 1         0                  6.53\n#> 4 1         1                  5.39\n#> 5 1         1                  8.40\n#> 6 3         0                  6.54\npopulation_for_mrp_example %>% \n  slice(1:1000) %>% \n  ggplot(aes(x = age_group, y = bed_time)) +\n  geom_jitter(aes(color = toilet_trained), \n              alpha = 0.4, \n              width = 0.1, \n              height = 0) +\n  labs(x = \"Age-group\",\n       y = \"Bed time\",\n       color = \"Toilet trained\") +\n  theme_classic() +\n  scale_color_brewer(palette = \"Set1\")\npopulation_for_mrp_example_summarised <- \n  population_for_mrp_example %>% \n  group_by(age_group, toilet_trained) %>% \n  summarise(median_bed_time = median(bed_time)) \n\npopulation_for_mrp_example_summarised %>% \n  knitr::kable(digits = 2,\n               col.names = c(\"Age-group\", \"Is toilet trained\", \"Average bed time\"))"},{"path":"mrp.html","id":"get-a-biased-sample-from-it","chapter":"17 Multilevel regression with post-stratification","heading":"17.2.2 Get a biased sample from it","text":"Now want pretend survey biased sample. ’ll allow -samples children younger toilet-trained. instance, perhaps gathered sample based records paediatrician, ’s likely see biased sample children. interested knowing proportion children toilet-trained various age-groups.can plot also.’s pretty clear sample different bedtime overall population, let’s just exercise look median, age toilet-trained status.","code":"\n# Thanks to Monica Alexander\nset.seed(853)\n\n# Add a weight for each 'type' (has to sum to one)\npopulation_for_mrp_example <- \n  population_for_mrp_example %>% \n  mutate(weight = \n           case_when(toilet_trained == 0 & age_group == 1 ~ 0.7,\n                     toilet_trained == 0 ~ 0.1,\n                     age_group %in% c(1, 2, 3) ~ 0.2\n                     ),\n         id = 1:n()\n         )\n\nget_these <- \n  sample(\n    x = population_for_mrp_example$id,\n    size = 1000,\n    prob = population_for_mrp_example$weight\n    )\n\nsample_for_mrp_example <- \n  population_for_mrp_example %>% \n  filter(id %in% get_these) %>% \n  select(-weight, -id)\n\n# Clean up\npoststratification_dataset <- \n  population_for_mrp_example %>% \n  select(-weight, -id)\nsample_for_mrp_example %>% \n  mutate(toilet_trained = as_factor(toilet_trained)) %>% \n  ggplot(aes(x = age_group, y = bed_time)) +\n  geom_jitter(aes(color = toilet_trained), alpha = 0.4, width = 0.1, height = 0) +\n  labs(x = \"Age-group\",\n       y = \"Bed time\",\n       color = \"Toilet trained\") +\n  theme_classic() +\n  scale_color_brewer(palette = \"Set1\")\nsample_for_mrp_example_summarized <- \n  sample_for_mrp_example %>% \n  group_by(age_group, toilet_trained) %>% \n  summarise(median_bed_time = median(bed_time))\n\nsample_for_mrp_example_summarized %>% \n  knitr::kable(digits = 2,\n               col.names = c(\"Age-group\", \"Is toilet trained\", \"Average bed time\"))"},{"path":"mrp.html","id":"model-the-sample","chapter":"17 Multilevel regression with post-stratification","heading":"17.2.3 Model the sample","text":"quickly train model based (biased) survey. ’ll use modelsummary (Arel-Bundock 2021) format estimates.‘multilevel regression’ part MRP (although isn’t really multilevel model just keep things simple now).","code":"\nlibrary(modelsummary)\n\nmrp_example_model <- \n  sample_for_mrp_example %>% \n  lm(bed_time ~ age_group + toilet_trained, data = .)\n\nmrp_example_model %>% \n  modelsummary::modelsummary(fmt = 2)"},{"path":"mrp.html","id":"get-a-post-stratification-dataset","chapter":"17 Multilevel regression with post-stratification","heading":"17.2.4 Get a post-stratification dataset","text":"Now use post-stratification dataset get estimates number cell. typically use larger dataset may closely reflection population. US popular choice ACS, countries typically use census.simulation example, ’ll just take 10 per cent sample population use post-stratification dataset.ideal world individual-level data post-stratification dataset (’s case ). world can apply model individual. likely situation, reality, just counts groups, ’re going try construct estimate group.","code":"\nset.seed(853)\n\npoststratification_dataset <- \n  population_for_mrp_example %>% \n  slice(1:100000) %>% \n  select(-bed_time)\n\npoststratification_dataset %>% \n  head()\n#> # A tibble: 6 × 4\n#>   age_group toilet_trained weight    id\n#>   <fct>     <fct>           <dbl> <int>\n#> 1 1         0                 0.7     1\n#> 2 2         1                 0.2     2\n#> 3 1         0                 0.7     3\n#> 4 1         1                 0.2     4\n#> 5 1         1                 0.2     5\n#> 6 3         0                 0.1     6\npoststratification_dataset_grouped <- \n  poststratification_dataset %>% \n  group_by(age_group, toilet_trained) %>% \n  count()\n\npoststratification_dataset_grouped %>% \n  head()\n#> # A tibble: 6 × 3\n#> # Groups:   age_group, toilet_trained [6]\n#>   age_group toilet_trained     n\n#>   <fct>     <fct>          <int>\n#> 1 1         0              16766\n#> 2 1         1              16649\n#> 3 2         0              16801\n#> 4 2         1              16617\n#> 5 3         0              16625\n#> 6 3         1              16542"},{"path":"mrp.html","id":"post-stratify-our-model-estimates","chapter":"17 Multilevel regression with post-stratification","heading":"17.2.5 Post-stratify our model estimates","text":"Now create estimate group, add confidence intervals.point can look MRP estimates (circles) along confidence intervals, compare raw estimates data (squares). case, know truth, can also compare known truth (triangles) (’s something can normally).","code":"\npoststratification_dataset_grouped <- \n  mrp_example_model %>% \n  predict(newdata = poststratification_dataset_grouped, interval = \"confidence\") %>% \n  as_tibble() %>% \n  cbind(poststratification_dataset_grouped, .) # The dot modifies the behaviour of the pipe; it pipes to the dot instead of the first argument as normal.\npoststratification_dataset_grouped %>% \n  ggplot(aes(x = age_group, y = fit)) +\n  geom_point(data = population_for_mrp_example_summarised,\n             aes(x = age_group, y = median_bed_time, color = toilet_trained), \n             shape = 17) +\n  geom_point(data = sample_for_mrp_example_summarized,\n             aes(x = age_group, y = median_bed_time, color = toilet_trained), \n             shape = 15) +\n  geom_pointrange(aes(ymin=lwr, ymax=upr, color = toilet_trained)) +\n  labs(x = \"Age-group\",\n       y = \"Bed time\",\n       color = \"Toilet trained\") +\n  theme_classic() +\n  scale_color_brewer(palette = \"Set1\")"},{"path":"mrp.html","id":"case-study---xbox-paper","chapter":"17 Multilevel regression with post-stratification","heading":"17.3 Case study - Xbox paper","text":"","code":""},{"path":"mrp.html","id":"overview-6","chapter":"17 Multilevel regression with post-stratification","heading":"17.3.1 Overview","text":"One famous MRP example W. Wang et al. (2015). used data Xbox gaming platform forecast 2012 US Presidential Election.Key facts set-:Data opt-poll available Xbox gaming platform 45 days leading 2012 US presidential election (Obama Romney).day three five questions, including voter intention: ‘election held today, vote ?’Respondents allowed answer per day.First-time respondents asked provide information , including sex, race, age, education, state, party ID, political ideology, voted 2008 presidential election.total, 750,148 interviews conducted, 345,858 unique respondents - 30,000 completed five polls.Young men dominate Xbox population: 18--29-year-olds comprise 65 per cent Xbox dataset, compared 19 per cent exit poll; men make 93 per cent Xbox sample 47 per cent electorate.","code":""},{"path":"mrp.html","id":"model-2","chapter":"17 Multilevel regression with post-stratification","heading":"17.3.2 Model","text":"Given structure US electorate, use two-stage modelling approach. details don’t really matter much, essentially model likely respondent vote Obama, given various information state, education, sex, etc:\\[\nPr\\left(Y_i = \\mbox{Obama} | Y_i\\\\{\\mbox{Obama, Romney}\\}\\right) = \\mbox{logit}^{-1}(\\alpha_0 + \\alpha_1(\\mbox{state last vote share}) + \\alpha_{j[]}^{\\mbox{state}} + \\alpha_{j[]}^{\\mbox{edu}} + \\alpha_{j[]}^{\\mbox{sex}} + ...)\n\\]run R using glmer() ‘lme4’ (Bates et al. 2015).","code":""},{"path":"mrp.html","id":"post-stratify","chapter":"17 Multilevel regression with post-stratification","heading":"17.3.3 Post-stratify","text":"trained model considers effect various independent variables support candidates, now post-stratify, ‘cell-level estimates weighted proportion electorate cell aggregated appropriate level (.e., state national).’means need cross-tabulated population data. general, census worked, one large surveys available US, difficulty variables need available cross-tab basis. , use exit polls (viable option countries).make state-specific estimates post-stratifying features state (Figure 17.2).\nFigure 17.2: Post-stratified estimates state based Xbox survey MRP\nSimilarly, can examine demographic-differences (Figure 17.3).\nFigure 17.3: Post-stratified estimates demographic basis based Xbox survey MRP\nFinally, convert estimates electoral college estimates (Figure 17.4).\nFigure 17.4: Post-stratified estimates electoral college outcomes based Xbox survey MRP\n","code":""},{"path":"mrp.html","id":"simulation---australian-voting","chapter":"17 Multilevel regression with post-stratification","heading":"17.4 Simulation - Australian voting","text":"","code":""},{"path":"mrp.html","id":"overview-7","chapter":"17 Multilevel regression with post-stratification","heading":"17.4.1 Overview","text":"reminder, workflow use :read poll;model poll;read post-stratification data; andapply model post-stratification data.earlier example, didn’t really much modelling step, despite name ‘multilevel modelling post-stratification,’ didn’t actually use multilevel model. ’s nothing says use multilevel model, lot situations circumstances ’s likely worse. clear, means although individual-level data, grouping individuals ’ll take advantage . instance, case trying model elections, usually districts/divisions/electorates/ridings/etc exist within provinces/states likely make sense , least, include coefficient adjusts intercept province.section ’re simulate another dataset fit different models . ’re going draw Australian elections set-. Australia parliamentary system, 151 seats parliament, one electorate. electorates grouped within six states two territories. two major parties - Australian Labor Party (ALP) Liberal Party (LP). Somewhat confusingly, Liberal party actually conservative, right-wing party, Labor party progressive, left-wing, party.","code":""},{"path":"mrp.html","id":"construct-a-survey","chapter":"17 Multilevel regression with post-stratification","heading":"17.4.2 Construct a survey","text":"move us slightly closer reality, going simulate survey (rather sample population earlier) post-stratify using real data. dependent variable ‘supports_ALP,’ binary variable - either 0 1. ’ll just start three independent variables :‘gender,’ either ‘female’ ‘male’ (available Australian Bureau Statistics);‘age_group,’ one four groups: ‘ages 18 29,’ ‘ages 30 44,’ ‘ages 45 59,’ ‘ages 60 plus’;‘state,’ one eight integers: 1 - 8 (inclusive).point, ’s worth briefly discussing role sex gender survey research, following Kennedy et al. (2020). Sex based biological attributes, gender socially constructed. likely interested effect gender dependent variable. Moving away non-binary concept gender, terms official statistics, something happened recently. researcher one problems insisting binary , Kennedy et al. (2020, 2) say ‘…measuring gender simply two categories, failure capture unique experiences identify either male female, whose gender align sex classification.’ researcher variety ways proceeding, Kennedy et al. (2020) discuss based : ethics, accuracy, practicality, flexibility. However, ‘single good solution can applied situations. Instead, important recognize compromise ethical concerns, statistical concerns, appropriate decision reflective ’ [p. 16]. important consideration ensure appropriate ‘respect consideration survey respondent.’Finally, want survey -sample females, ’ll just get rid 300 males.","code":"\nlibrary(tidyverse)\nset.seed(853)\n\nsize_of_sample_for_australian_polling <- 2000\n\nsample_for_australian_polling <- \n  tibble(age_group = \n           sample(x = c(0:3), \n                  size = size_of_sample_for_australian_polling,\n                  replace = TRUE\n                  ),\n         gender = \n           sample(x = c(0:1),\n                  size = size_of_sample_for_australian_polling,\n                  replace = TRUE\n                  ),\n         state = \n           sample(x = c(1:8),\n                  size = size_of_sample_for_australian_polling,\n                  replace = TRUE\n                  ),\n         noise = rnorm(size_of_sample_for_australian_polling, mean = 0, sd = 1), \n         support_alp = 1 + 0.5 * age_group + 0.5 * gender + 0.01 * state + noise\n         ) \n\n# Normalize the outcome variable\nsample_for_australian_polling <- \n  sample_for_australian_polling %>% \n  mutate(support_alp = \n           if_else(support_alp > median(support_alp, na.rm = TRUE), \n                   'Supports ALP', \n                   'Does not')\n         )\n\n# Clean up the simulated data\nsample_for_australian_polling <- \n  sample_for_australian_polling %>% \n  mutate(\n    age_group = case_when(\n      age_group == 0 ~ 'Ages 18 to 29',\n      age_group == 1 ~ 'Ages 30 to 44',\n      age_group == 2 ~ 'Ages 45 to 59',\n      age_group == 3 ~ 'Ages 60 plus',\n      TRUE ~ 'Problem'\n      ),\n    gender = case_when(\n      gender == 0 ~ 'Male',\n      gender == 1 ~ 'Female',\n      TRUE ~ 'Problem'\n      ),\n    state = case_when(\n      state == 1 ~ 'Queensland',\n      state == 2 ~ 'New South Wales',\n      state == 3 ~ 'Australian Capital Territory',\n      state == 4 ~ 'Victoria',\n      state == 5 ~ 'Tasmania',\n      state == 6 ~ 'Northern Territory',\n      state == 7 ~ 'South Australia',\n      state == 8 ~ 'Western Australia',\n      TRUE ~ 'Problem'\n      ),\n    \n    ) %>% \n  select(-noise)\n\n# Tidy the class\nsample_for_australian_polling <- \n  sample_for_australian_polling %>% \n  mutate(across(c(age_group, gender, state, support_alp), as_factor))\n\nsample_for_australian_polling %>%   \n  head()\n#> # A tibble: 6 × 4\n#>   age_group     gender state           support_alp \n#>   <fct>         <fct>  <fct>           <fct>       \n#> 1 Ages 18 to 29 Female South Australia Supports ALP\n#> 2 Ages 60 plus  Male   South Australia Supports ALP\n#> 3 Ages 30 to 44 Male   Victoria        Does not    \n#> 4 Ages 18 to 29 Male   Tasmania        Does not    \n#> 5 Ages 18 to 29 Female Victoria        Does not    \n#> 6 Ages 18 to 29 Male   Queensland      Supports ALP\nsample_for_australian_polling <- \n  sample_for_australian_polling %>% \n  arrange(gender) %>% \n  slice(1:1700)"},{"path":"mrp.html","id":"model-the-survey","chapter":"17 Multilevel regression with post-stratification","heading":"17.4.3 Model the survey","text":"polling data generated make males older people less likely vote ALP; females younger people likely vote Labor Party. Females -sampled. , ALP skew dataset. ’re going use gtsummary package quickly make summary table (Sjoberg et al. 2021).\n          1\n          \n           \n          n (%)\n          Now ’d like see can get results back (find females less likely males vote Australian Labor Party people less likely vote Australian Labor Party get older). model :ADD MODEL.model says probability person, \\(j\\), vote Australian Labor Party depends gender age-group. Based simulated data, like older age-groups less likely vote Australian Labor Party males less likely vote Australian Labor Party.Essentially ’ve got inputs back. dependent variable binary, used logistic regression results little difficult interpret.","code":"\nlibrary(gtsummary)\n\nsample_for_australian_polling %>% \n  gtsummary::tbl_summary()\nalp_support <- \n  glm(support_alp ~ gender + age_group + state, \n      data = sample_for_australian_polling,\n      family = \"binomial\"\n      )\n\nalp_support %>% \n  modelsummary::modelsummary(fmt = 2, exponentiate = TRUE)"},{"path":"mrp.html","id":"post-stratify-1","chapter":"17 Multilevel regression with post-stratification","heading":"17.4.4 Post-stratify","text":"Now ’d like see can use found poll get estimate state based demographic features.First read real demographic data, state basis, ABS.point, ’ve got decision make need variables survey post-stratification dataset, state abbreviations used, survey, full names used. ’ll change post-stratification dataset survey data already modelled.’re just going rough forecasts. gender age-group want relevant coefficient example data can construct estimates.now post-stratified estimates state model fair weaknesses. instance, small cell counts going problematic. approach ignores uncertainty, now something working can complicate .","code":"\npost_strat_census_data <- \n  read_csv(\"outputs/data/census_data.csv\")\n\nhead(post_strat_census_data)\n#> # A tibble: 6 × 5\n#>   state gender age_group  number cell_prop_of_division_total\n#>   <chr> <chr>  <chr>       <dbl>                       <dbl>\n#> 1 ACT   Female ages18to29  34683                       0.125\n#> 2 ACT   Female ages30to44  42980                       0.155\n#> 3 ACT   Female ages45to59  33769                       0.122\n#> 4 ACT   Female ages60plus  30322                       0.109\n#> 5 ACT   Male   ages18to29  34163                       0.123\n#> 6 ACT   Male   ages30to44  41288                       0.149\npost_strat_census_data <- \n  post_strat_census_data %>% \n  mutate(\n    state = \n      case_when(\n        state == 'ACT' ~ 'Australian Capital Territory',\n        state == 'NSW' ~ 'New South Wales',\n        state == 'NT' ~ 'Northern Territory',\n        state == 'QLD' ~ 'Queensland',\n        state == 'SA' ~ 'South Australia',\n        state == 'TAS' ~ 'Tasmania',\n        state == 'VIC' ~ 'Victoria',\n        state == 'WA' ~ 'Western Australia',\n        TRUE ~ \"Problem\"\n      ),\n    age_group = \n      case_when(\n        age_group == 'ages18to29' ~ 'Ages 18 to 29',\n        age_group == 'ages30to44' ~ 'Ages 30 to 44',\n        age_group == 'ages45to59' ~ 'Ages 45 to 59',\n        age_group == 'ages60plus' ~ 'Ages 60 plus',\n        TRUE ~ \"Problem\"\n      )\n  )\npost_strat_census_data <- \n  alp_support %>% \n  predict(newdata = post_strat_census_data, type = 'response', se.fit = TRUE) %>% \n  as_tibble() %>% \n  cbind(post_strat_census_data, .)\n\npost_strat_census_data %>% \n  mutate(alp_predict_prop = fit*cell_prop_of_division_total) %>% \n  group_by(state) %>% \n  summarise(alp_predict = sum(alp_predict_prop))\n#> # A tibble: 8 × 2\n#>   state                        alp_predict\n#>   <chr>                              <dbl>\n#> 1 Australian Capital Territory       0.551\n#> 2 New South Wales                    0.487\n#> 3 Northern Territory                 0.546\n#> 4 Queensland                         0.491\n#> 5 South Australia                    0.403\n#> 6 Tasmania                           0.429\n#> 7 Victoria                           0.521\n#> 8 Western Australia                  0.460"},{"path":"mrp.html","id":"improving-the-model","chapter":"17 Multilevel regression with post-stratification","heading":"17.4.5 Improving the model","text":"’d like address major issues approach, specifically able deal small cell counts, also taking better account uncertainty. dealing survey data, prediction intervals something similar critical, ’s appropriate report central estimates. ’ll use broad approach , just improve model. ’re going change Bayesian model use rstanarm package (Goodrich et al. 2020).Now, using basic model , Bayesian setting., ’d like estimate state based demographic features. ’re just going rough forecasts. gender age-group want relevant coefficient example data can construct estimates (code Monica Alexander). ’re going use tidybayes (Kay 2020).now post-stratified estimates division. new Bayesian approach enable us think deeply uncertainty. complicate variety ways including adding coefficients (remember ’d need get new cell counts), adding layers.One interesting aspect multilevel approach allow us deal small cell counts borrowing information cells. Even remove , say, 18--29-year-old, male respondents Tasmania model still provide estimates. pooling, effect young, male, Tasmanians partially determined cells respondents.many interesting aspects may like communicate others. instance, may like show model affecting results. can make graph compares raw estimate model estimate.Similarly, may like plot distribution coefficients.9","code":"\nlibrary(rstanarm)\n\nimproved_alp_support <- \n  rstanarm::stan_glm(support_alp ~ gender + age_group + state,\n                     data = sample_for_australian_polling,\n                     family = binomial(link = \"logit\"),\n                     prior = normal(0, 1), \n                     prior_intercept = normal(0, 1),\n                     cores = 2, \n                     seed = 12345)\nlibrary(tidybayes)\n\npost_stratified_estimates <- \n  improved_alp_support %>% \n  tidybayes::add_fitted_draws(newdata = post_strat_census_data) %>% \n  rename(alp_predict = .value) %>% \n  mutate(alp_predict_prop = alp_predict*cell_prop_of_division_total) %>% \n  group_by(state, .draw) %>% \n  summarise(alp_predict = sum(alp_predict_prop)) %>% \n  group_by(state) %>% \n  summarise(mean = mean(alp_predict), \n            lower = quantile(alp_predict, 0.025), \n            upper = quantile(alp_predict, 0.975))\n\npost_stratified_estimates\n#> # A tibble: 8 × 4\n#>   state                         mean lower upper\n#>   <chr>                        <dbl> <dbl> <dbl>\n#> 1 Australian Capital Territory 0.550 0.494 0.604\n#> 2 New South Wales              0.486 0.429 0.544\n#> 3 Northern Territory           0.544 0.483 0.607\n#> 4 Queensland                   0.491 0.432 0.548\n#> 5 South Australia              0.412 0.361 0.464\n#> 6 Tasmania                     0.429 0.372 0.487\n#> 7 Victoria                     0.519 0.453 0.583\n#> 8 Western Australia            0.460 0.401 0.520\npost_stratified_estimates %>% \n  ggplot(aes(y = mean, x = forcats::fct_inorder(state), color = \"MRP estimate\")) + \n  geom_point() +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0) + \n  labs(y = \"Proportion ALP support\",\n       x = \"State\") + \n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\") +\n  theme(legend.title = element_blank()) +\n  coord_flip()\n# tidybayes::get_variables(improved_alp_support)\n# improved_alp_support %>%\n#   gather_draws(genderMale) %>%\n#   ungroup() %>%\n#   # mutate(coefficient = stringr::str_replace_all(.variable, c(\"b_\" = \"\"))) %>%\n#   mutate(coefficient = forcats::fct_recode(coefficient,\n#                                            Intercept = \"Intercept\",\n#                                            `Is male` = \"genderMale\",\n#                                            `Age 30-44` = \"age_groupages30to44\",\n#                                            `Age 45-59` = \"age_groupages45to59\",\n#                                            `Age 60+` = \"age_groupages60plus\"\n#                                            )) %>% \n# \n# # both %>% \n#   ggplot(aes(y=fct_rev(coefficient), x = .value)) + \n#   ggridges::geom_density_ridges2(aes(height = ..density..),\n#                                  rel_min_height = 0.01, \n#                                  stat = \"density\",\n#                                  scale=1.5) +\n#   xlab(\"Distribution of estimate\") +\n#   ylab(\"Coefficient\") +\n#   scale_fill_brewer(name = \"Dataset: \", palette = \"Set1\") +\n#   theme_minimal() +\n#   theme(panel.grid.major = element_blank(),\n#         panel.grid.minor = element_blank()) +\n#   theme(legend.position = \"bottom\")"},{"path":"mrp.html","id":"forecasting-the-2020-us-election","chapter":"17 Multilevel regression with post-stratification","heading":"17.5 Forecasting the 2020 US election","text":"US election lot features unique US, model going build going fairly generic , largely generalization earlier model Australian election. One good thing forecasting US election lot data around. case can use survey data Democracy Fund Voter Study Group.10 conducted polling lead-US election make publicly available registration. use Integrated Public Use Microdata Series (IPUMS), access 2018 American Community Survey (ACS) post-stratification dataset. use state, age-group, gender, education explanatory variables.","code":""},{"path":"mrp.html","id":"survey-data","chapter":"17 Multilevel regression with post-stratification","heading":"17.5.1 Survey data","text":"first step need actually get survey data. Go website:https://www.voterstudygroup.org ’re looking ‘Nationscape’ button along lines ‘Get latest Nationscape data.’ get dataset, need fill form, process email . real person side form, request take days.get access ’ll want download .dta files. Nationscape conducted many surveys many files. filename reference date, ‘ns20200625’ refers 25 June 2020, one ’ll use . (data mine share, ’ll refer)can read ‘.dta’ files using haven package (Wickham Miller 2020). ’ve based code written Alen Mitrovski, Xiaoyan Yang, Matthew Wankiewicz, available : https://github.com/matthewwankiewicz/US_election_forecast.’ve seen, one difficult aspects MRP ensuring consistency datasets. case, need work make variables consistent.","code":"\nlibrary(haven)\nlibrary(tidyverse)\n\nraw_nationscape_data <- \n  read_dta(here::here(\"dont_push/ns20200625.dta\"))\n\n# The Stata format separates labels so reunite those\nraw_nationscape_data <- \n  labelled::to_factor(raw_nationscape_data)\n\n# Just keep relevant variables\nnationscape_data <- \n  raw_nationscape_data %>% \n  select(vote_2020,\n         gender,\n         education,\n         state,\n         age)\n\n# For simplicity, remove anyone undecided or planning to vote for someone other than Biden/Trump and make vote a binary variable: 1 for Biden, 0 for Trump.\nnationscape_data <- \n  nationscape_data %>% \n  filter(vote_2020 == \"Joe Biden\" | vote_2020 == \"Donald Trump\") %>% \n  mutate(vote_biden = if_else(vote_2020 == \"Joe Biden\", 1, 0)) %>% \n  select(-vote_2020)\n\n# Create the dependent variables by grouping the existing variables\nnationscape_data <- \n  nationscape_data %>% \n  mutate(\n    age_group = case_when( # case_when works in order and exits when there's a match\n      age <= 29 ~ 'age_18-29',\n      age <= 44 ~ 'age_30-44',\n      age <= 59 ~ 'age_45-59',\n      age >= 60 ~ 'age_60_or_more',\n      TRUE ~ 'Trouble'\n      ),\n    gender = case_when(\n      gender == \"Female\" ~ 'female',\n      gender == \"Male\" ~ 'male',\n      TRUE ~ 'Trouble'\n      ),\n    education_level = case_when(\n      education == \"3rd Grade or less\" ~ \"High school or less\",\n      education == \"Middle School - Grades 4 - 8\" ~ \"High school or less\",\n      education == \"Completed some high school\" ~ \"High school or less\",\n      education == \"High school graduate\" ~ \"High school or less\",\n      education == \"Other post high school vocational training\" ~ \"Some post secondary\",\n      education == \"Completed some college, but no degree\" ~ \"Some post secondary\",\n      education == \"Associate Degree\" ~ \"Post secondary or higher\",\n      education == \"College Degree (such as B.A., B.S.)\" ~ \"Post secondary or higher\",\n      education == \"Completed some graduate, but no degree\" ~ \"Post secondary or higher\",\n      education == \"Masters degree\" ~ \"Graduate degree\",\n      education == \"Doctorate degree\" ~ \"Graduate degree\",\n      TRUE ~ 'Trouble'\n      )\n    ) %>% \n  select(-education, -age)\n\ntests <- \n  nationscape_data %>% \n  mutate(test = stringr::str_detect(age_group, 'Trouble'),\n         test = if_else(test == TRUE, TRUE, \n                        stringr::str_detect(education_level, 'Trouble')),\n         test = if_else(test == TRUE, TRUE, \n                        stringr::str_detect(gender, 'Trouble'))\n         ) %>% \n  filter(test == TRUE)\n\nif(nrow(tests) != 0) {\n  print(\"Check nationscape_data\")\n  } else {\n    rm(tests)\n    }\n\nnationscape_data %>% \n  head()\n#> # A tibble: 6 × 5\n#>   gender state vote_biden age_group      education_level         \n#>   <chr>  <chr>      <dbl> <chr>          <chr>                   \n#> 1 female WI             0 age_45-59      Post secondary or higher\n#> 2 female VA             0 age_45-59      Post secondary or higher\n#> 3 female TX             0 age_60_or_more High school or less     \n#> 4 female WA             0 age_45-59      High school or less     \n#> 5 female MA             1 age_18-29      Some post secondary     \n#> 6 female TX             1 age_30-44      Some post secondary\n# This code is very directly from Alen Mitrovski, Xiaoyan Yang, and Matthew Wankiewicz.\n# Format state names so the whole state name is written out, to match IPUMS data\nstates_names_and_abbrevs <- \n  tibble(stateicp = state.name, state = state.abb)\n\nnationscape_data <-\n  nationscape_data %>%\n  left_join(states_names_and_abbrevs)\n\nrm(states_names_and_abbrevs)\n\n# Make lowercase to match IPUMS data\nnationscape_data <- \n  nationscape_data %>% \n  mutate(stateicp = tolower(stateicp))\n\n# Replace NAs with DC\nnationscape_data$stateicp <- \n  replace_na(nationscape_data$stateicp, \"district of columbia\")\n\n# Tidy the class\nnationscape_data <- \n  nationscape_data %>% \n  mutate(across(c(gender, stateicp, education_level, age_group), as_factor))\n\n# Save data\nwrite_csv(nationscape_data, \"outputs/data/polling_data.csv\")\n\nnationscape_data %>% \n  head()\n#> # A tibble: 6 × 6\n#>   gender state vote_biden age_group      education_level          stateicp     \n#>   <fct>  <chr>      <dbl> <fct>          <fct>                    <fct>        \n#> 1 female WI             0 age_45-59      Post secondary or higher wisconsin    \n#> 2 female VA             0 age_45-59      Post secondary or higher virginia     \n#> 3 female TX             0 age_60_or_more High school or less      texas        \n#> 4 female WA             0 age_45-59      High school or less      washington   \n#> 5 female MA             1 age_18-29      Some post secondary      massachusetts\n#> 6 female TX             1 age_30-44      Some post secondary      texas"},{"path":"mrp.html","id":"post-stratification-data","chapter":"17 Multilevel regression with post-stratification","heading":"17.5.2 Post-stratification data","text":"lot options dataset post-stratify various considerations. dataset better quality (however defined), likely larger. strictly data perspective, best choice probably something like Cooperative Congressional Election Study (CCES), however whatever reason released election ’s reasonable choice. W. Wang et al. (2015) use exit poll data, ’s available election.countries ’d stuck using census, course quite large, likely --date. Luckily US opportunity use American Community Survey (ACS) asks analogous questions census, conducted every month, course year, end million responses. case ’re going access ACS IPUMS.go IPUMS website - https://ipums.org - ’re looking something like IPUMS USA ‘get data.’ Create account, need . ’ll take process. account, go ‘Select Samples’ de-select everything apart 2019 ACS. need get variables ’re interested . household want ‘STATEICP,’ person want ‘SEX,’ ‘AGE,’ ‘EDUC.’ everything selected, ‘view cart,’ want careful change ‘data format’ ‘.dta’ (’s nothing wrong formats, ’ve just already got code earlier deal type). Briefly just check many rows columns ’re requesting. around million rows, around ten twenty columns. ’s much 300MB maybe just see ’ve accidently selected something don’t need. Submit request within day, get email saying data can downloaded. take 30 minutes , don’t get email within day check size dataset, customize sample size reduce size initially.case let’s tidy data.dataset individual level. ’ll create counts sub-cell, proportions state.Now ’d like add proportions state.","code":"\n# Again, closely following code from Alen Mitrovski, Xiaoyan Yang, and Matthew Wankiewicz.\nlibrary(haven)\nlibrary(tidyverse)\n\nraw_poststrat_data <- \n  read_dta(here::here(\"dont_push/usa_00004.dta\"))\n\n# The Stata format separates labels so reunite those\nraw_poststrat_data <- \n  labelled::to_factor(raw_poststrat_data)\nhead(raw_poststrat_data)\n#> # A tibble: 6 × 28\n#>   year  sample serial cbserial  hhwt cluster region stateicp strata gq    pernum\n#>   <fct> <fct>   <dbl>    <dbl> <dbl>   <dbl> <fct>  <fct>     <dbl> <fct>  <dbl>\n#> 1 2018  2018 …      2  2.02e12 392.  2.02e12 east … alabama  190001 othe…      1\n#> 2 2018  2018 …      7  2.02e12  94.1 2.02e12 east … alabama   40001 grou…      1\n#> 3 2018  2018 …     13  2.02e12  83.7 2.02e12 east … alabama  130301 othe…      1\n#> 4 2018  2018 …     18  2.02e12  57.5 2.02e12 east … alabama  100001 grou…      1\n#> 5 2018  2018 …     23  2.02e12 157.  2.02e12 east … alabama  190001 grou…      1\n#> 6 2018  2018 …     28  2.02e12 157.  2.02e12 east … alabama  220001 othe…      1\n#> # … with 17 more variables: perwt <dbl>, sex <fct>, age <fct>, marst <fct>,\n#> #   race <fct>, raced <fct>, hispan <fct>, hispand <fct>, bpl <fct>,\n#> #   bpld <fct>, citizen <fct>, educ <fct>, educd <fct>, empstat <fct>,\n#> #   empstatd <fct>, labforce <fct>, inctot <dbl>\n\nraw_poststrat_data$age <- as.numeric(raw_poststrat_data$age)\n\npoststrat_data <- \n  raw_poststrat_data %>% \n  filter(inctot < 9999999) %>% \n  filter(age >= 18) %>% \n  mutate(gender = sex) %>% \n  mutate(\n    age_group = case_when( # case_when works in order and exits when there's a match\n      age <= 29 ~ 'age_18-29',\n      age <= 44 ~ 'age_30-44',\n      age <= 59 ~ 'age_45-59',\n      age >= 60 ~ 'age_60_or_more',\n      TRUE ~ 'Trouble'\n      ),\n    education_level = case_when(\n      educd == \"nursery school, preschool\" ~ \"High school or less\",\n      educd == \"kindergarten\" ~ \"High school or less\",\n      educd == \"grade 1\" ~ \"High school or less\",\n      educd == \"grade 2\" ~ \"High school or less\",\n      educd == \"grade 3\" ~ \"High school or less\",\n      educd == \"grade 4\" ~ \"High school or less\",\n      educd == \"grade 5\" ~ \"High school or less\",\n      educd == \"grade 6\" ~ \"High school or less\",\n      educd == \"grade 7\" ~ \"High school or less\",\n      educd == \"grade 8\" ~ \"High school or less\",\n      educd == \"grade 9\" ~ \"High school or less\",\n      educd == \"grade 10\" ~ \"High school or less\",\n      educd == \"grade 11\" ~ \"High school or less\",\n      educd == \"12th grade, no diploma\" ~ \"High school or less\",\n      educd == \"regular high school diploma\" ~ \"High school or less\",\n      educd == \"ged or alternative credential\" ~ \"High school or less\",\n      educd == \"some college, but less than 1 year\" ~ \"Some post secondary\",\n      educd == \"1 or more years of college credit, no degree\" ~ \"Some post secondary\",\n      educd == \"associate's degree, type not specified\" ~ \"Post secondary or higher\",\n      educd == \"bachelor's degree\" ~ \"Post secondary or higher\",\n      educd == \"master's degree\" ~ \"Graduate degree\",\n      educd == \"professional degree beyond a bachelor's degree\" ~ \"Graduate degree\",\n      educd == \"doctoral degree\" ~ \"Graduate degree\",\n      educd == \"no schooling completed\" ~ \"High school or less\",\n      TRUE ~ 'Trouble'\n      )\n    )\n\n# Just keep relevant variables\npoststrat_data <- \n  poststrat_data %>% \n  select(gender,\n         age_group,\n         education_level,\n         stateicp)\n\n# Tidy the class\npoststrat_data <- \n  poststrat_data %>% \n  mutate(across(c(gender, stateicp, education_level, age_group), as_factor))\n\n# Save data\nwrite_csv(poststrat_data, \"outputs/data/us_poststrat.csv\")\n\npoststrat_data %>% \n  head()\n#> # A tibble: 6 × 4\n#>   gender age_group      education_level     stateicp\n#>   <fct>  <fct>          <fct>               <fct>   \n#> 1 female age_18-29      Some post secondary alabama \n#> 2 female age_60_or_more Some post secondary alabama \n#> 3 male   age_45-59      Some post secondary alabama \n#> 4 male   age_30-44      High school or less alabama \n#> 5 female age_60_or_more High school or less alabama \n#> 6 male   age_30-44      High school or less alabama\npoststrat_data_cells <- \n  poststrat_data %>% \n  group_by(stateicp, gender, age_group, education_level) %>% \n  count()\npoststrat_data_cells <- \n  poststrat_data_cells %>% \n  group_by(stateicp) %>% \n  mutate(prop = n/sum(n)) %>% \n  ungroup()\n\npoststrat_data_cells %>% head()\n#> # A tibble: 6 × 6\n#>   stateicp    gender age_group      education_level              n    prop\n#>   <fct>       <fct>  <fct>          <fct>                    <int>   <dbl>\n#> 1 connecticut male   age_18-29      Some post secondary        149 0.0260 \n#> 2 connecticut male   age_18-29      High school or less        232 0.0404 \n#> 3 connecticut male   age_18-29      Post secondary or higher    96 0.0167 \n#> 4 connecticut male   age_18-29      Graduate degree             25 0.00436\n#> 5 connecticut male   age_60_or_more Some post secondary        142 0.0248 \n#> 6 connecticut male   age_60_or_more High school or less        371 0.0647"},{"path":"mrp.html","id":"model-3","chapter":"17 Multilevel regression with post-stratification","heading":"17.5.3 Model","text":"’re going use logistic regression estimate model binary support Biden explained gender, age-group, education-level, state. ’re going Bayesian framework using rstanarm (Goodrich et al. 2020). variety reasons using rstanarm , main one Stan pre-compiled eases computer set-issues may otherwise . great resource implementing MRP rstanarm Kennedy Gabry (2020).variety options ’ve largely unthinkingly set, exploring effect good idea, now can just quick look model.","code":"\nlibrary(rstanarm)\n\nus_election_model <- \n  rstanarm::stan_glmer(vote_biden ~ gender + age_group + (1 | stateicp) + education_level,\n                       data = nationscape_data,\n                       family = binomial(link = \"logit\"),\n                       prior = normal(0, 1), \n                       prior_intercept = normal(0, 1),\n                       cores = 2, \n                       seed = 853)\nmodelsummary::get_estimates(us_election_model)\n#>                                      term effect    estimate conf.level\n#> 1                             (Intercept)  fixed  0.27591651       0.95\n#> 2                              gendermale  fixed -0.54094841       0.95\n#> 3                 age_groupage_60_or_more  fixed  0.04886803       0.95\n#> 4                      age_groupage_18-29  fixed  0.87817445       0.95\n#> 5                      age_groupage_30-44  fixed  0.12455081       0.95\n#> 6      education_levelHigh school or less  fixed -0.35080948       0.95\n#> 7      education_levelSome post secondary  fixed -0.14970941       0.95\n#> 8          education_levelGraduate degree  fixed -0.21988079       0.95\n#> 9 Sigma[stateicp:(Intercept),(Intercept)] random  0.08002654       0.95\n#>      conf.low    conf.high      pd rope.percentage      rhat      ess\n#> 1  0.10322566  0.447878764 0.99850       0.1281242 1.0002399 2262.704\n#> 2 -0.65227570 -0.430121742 1.00000       0.0000000 0.9996776 5264.121\n#> 3 -0.10847716  0.200831286 0.72600       0.9765851 0.9998468 3726.472\n#> 4  0.69830955  1.061899702 1.00000       0.0000000 0.9997188 3931.709\n#> 5 -0.02530517  0.284994549 0.94125       0.7729545 0.9995061 3567.152\n#> 6 -0.51262507 -0.204222132 1.00000       0.0000000 0.9997706 3700.321\n#> 7 -0.29927232  0.006135831 0.96625       0.6771902 1.0005471 4090.628\n#> 8 -0.38172115 -0.036764485 0.99100       0.3249145 0.9999286 4589.040\n#> 9  0.02912381  0.160211514 1.00000       1.0000000 1.0031942 1284.436\n#>   prior.distribution prior.location prior.scale\n#> 1             normal              0           1\n#> 2             normal              0           1\n#> 3             normal              0           1\n#> 4             normal              0           1\n#> 5             normal              0           1\n#> 6             normal              0           1\n#> 7             normal              0           1\n#> 8             normal              0           1\n#> 9               <NA>             NA          NA\n# The default usage of modelsummary requires statistics that we don't have.\n# Uncomment the following line if you want to look at what is available and specify your own:\n# modelsummary::get_estimates(us_election_model)\nmodelsummary::modelsummary(us_election_model,\n                            statistic = c('conf.low', 'conf.high')\n                            )"},{"path":"mrp.html","id":"post-stratify-2","chapter":"17 Multilevel regression with post-stratification","heading":"17.5.4 Post-stratify","text":"can look estimates, like.","code":"\nbiden_support_by_state <- \n  us_election_model %>%\n  tidybayes::add_fitted_draws(newdata=poststrat_data_cells) %>%\n  rename(support_biden_predict = .value) %>% \n  mutate(support_biden_predict_prop = support_biden_predict*prop) %>% \n  group_by(stateicp, .draw) %>% \n  summarise(support_biden_predict = sum(support_biden_predict_prop)) %>% \n  group_by(stateicp) %>% \n  summarise(mean = mean(support_biden_predict), \n            lower = quantile(support_biden_predict, 0.025), \n            upper = quantile(support_biden_predict, 0.975))\nbiden_support_by_state %>% \n  ggplot(aes(y = mean, x = stateicp, color = \"MRP estimate\")) + \n  geom_point() +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0) + \n  geom_point(data = \n               nationscape_data %>% \n               group_by(stateicp, vote_biden) %>%\n               summarise(n = n()) %>% \n               group_by(stateicp) %>% \n               mutate(prop = n/sum(n)) %>% \n               filter(vote_biden==1), \n             aes(y = prop, x = stateicp, color = 'Nationscape raw data')) +\n  geom_hline(yintercept = 0.5, linetype = 'dashed') +\n  labs(x = 'State',\n       y = 'Estimated proportion support for Biden',\n       color = 'Source') +\n  theme_classic() +\n  scale_color_brewer(palette = 'Set1') +\n  coord_flip()"},{"path":"mrp.html","id":"concluding-remarks-and-next-steps","chapter":"17 Multilevel regression with post-stratification","heading":"17.6 Concluding remarks and next steps","text":"general, MRP good way accomplish specific aims, ’s without trade-offs. good quality survey, may way speak disaggregated aspects . concerned uncertainty good way think . biased survey ’s great place start, ’s panacea.’s lot work ’s done , ’s plenty scope exciting work variety approaches:statistical perspective, lot work terms thinking survey design modelling approaches interact extent underestimating uncertainty. ’m also interested thinking implications small samples uncertainty post-stratification dataset. ’s awful lot terms thinking appropriate model use, even evaluate ‘appropriate’ means ? statistical interests, probably go next Gao et al. (2021) well pretty much anything Yajuan Si, Si (2020) starting point.’s lot done sociology perspective terms survey responses can better design surveys, knowing going used MRP putting respect respondents first.political science perspective, just little idea conditions stable preferences relationships required MRP accurate, understanding relates uncertainty survey design. political science interests, natural next step go Lauderdale et al. (2020) Ghitza Gelman (2020).Economists might interested think use MRP better understand inflation unemployment rates local levels.statistical software side things, really need develop better packages around .’s information side things, ’m excited MRP. best store protect datasets, yet retain ability correspond ? put levels together way meaningful? extent people appreciate uncertainty estimates can better communicate estimates?generally, pretty much use MRP anywhere samples. Determining conditions actually , work whole generations.","code":""},{"path":"mrp.html","id":"exercises-and-tutorial-16","chapter":"17 Multilevel regression with post-stratification","heading":"17.7 Exercises and tutorial","text":"","code":""},{"path":"mrp.html","id":"exercises-16","chapter":"17 Multilevel regression with post-stratification","heading":"17.7.1 Exercises","text":"","code":""},{"path":"mrp.html","id":"tutorial-16","chapter":"17 Multilevel regression with post-stratification","heading":"17.7.2 Tutorial","text":"","code":""},{"path":"text-as-data.html","id":"text-as-data","chapter":"18 Text as data","heading":"18 Text as data","text":"STATUS: construction.Required readingHvitfeldt, Emil, Julia Silge, 2021, Supervised Machine Learning Text Analysis R, Chapters 2, 5, 6, 7, https://smltar.com.Silge, Julia, David Robinson, 2017, Text Mining R, https://www.tidytextmining.com.Required viewingRecommended readingAmaka, Ofunne, Amber Thomas, 2020, ‘Naked Truth: names 6,816 complexion products can reveal bias beauty,’ Pudding, March, https://pudding.cool/2021/03/foundation-names/.Key concepts/skills/etcKey librariesKey functions/etcQuiz","code":""},{"path":"text-as-data.html","id":"introduction-24","chapter":"18 Text as data","heading":"18.1 Introduction","text":"Text can thought unwieldy, generally","code":""},{"path":"text-as-data.html","id":"lasso-regression","chapter":"18 Text as data","heading":"18.2 Lasso regression","text":"subsection, much code used, directly draws Julia Silge’s notes, particular: https://juliasilge.com/blog/tidy-text-classification/ (Silge 2018).One nice aspects text can adapt existing methods use input. going use variation logistics regression, along text inputs, forecast. want learn Lasso regression, consider taking Arik’s course summer, dive machine learning using Python.section going two different text inputs, train model sample text , try use model forecast text training set. Although arbitrary example, imagine many real-world applications. instance, work Twitter may want know tweet likely written bot, human. similarly, imagine work political party - may like know email likely email campaign organised group, individual.First need get data. Julia Silge’s example, nicely, uses book text input. Seeing jointly appointed Faculty Information, seems especially nice. wonderful thing R package - gutenbergr - makes easy get text Project Gutenberg R. key function gutenberg_download(), needs key book want. ’ll consider Jane Eyre Alice’s Adventures Wonderland, keys 1260 11, respectively.One great things dataset tibble. can just work familiar skills. package lot functionality, ’d encourage look package’s website: https://github.com/ropensci/gutenbergr. line book read different row dataset. Notice downloaded two books , added title. two books one . can see looking summary statistics.looks like Jane Eyre much longer Alice Wonderland, isn’t surprise read . don’t want step Digital Humanities much, don’t know anything , looking things like broader context books written, books written similar times, likely fascinating area.’ll just get rid blank linesThere’s still overwhelming amount Jane Eyre . ’ll just sample Jane Eyre make equal.’s bunch issues , instance, whole Alice, random bits Jane, nonetheless let’s continue ’ll try something moment.Now want get sample text book. use lines distinguish samples. use counter add line number.now want separate words. ’ll just use tidytext, focus modelling, bunch alternatives one especially good one quanteda package, specifically, tokens() function.Notice removed word wasn’t used 10 times. Nonetheless still lot unique words. (didn’t require word used author least 10 times end 6,000 words.)reason relevant independent variables. may used something less 10 explanatory variables, case going 585 , need model can handle .However, mentioned , going rows essentially just one word. allow , might also nice give model least words work .’ll create test/training split, load tidymodels.Now need create document-term matrix.independent variables sorted, now need binary dependent variable, whether book Alice Wonderland Jane Eyre.Now can run model.Perhaps unsurprisingly, mention Alice ’s likely Alice Wonderland mention Jane ’s likely Jane Eyre.","code":"\nlibrary(gutenbergr)\nalice_and_jane <- gutenbergr::gutenberg_download(c(1260, 11), meta_fields = \"title\")\n\n# Save the dataset so that we don't need to overwhelm the servers each time\nwrite_csv(alice_and_jane, \"inputs/books/alice_and_jane.csv\")\n\nhead(alice_and_jane)\nlibrary(gutenbergr)\n\nalice_and_jane <- read_csv(\"inputs/books/alice_and_jane.csv\")\n\nhead(alice_and_jane)\n#> # A tibble: 6 × 3\n#>   gutenberg_id text                               title                         \n#>          <dbl> <chr>                              <chr>                         \n#> 1           11 ALICE'S ADVENTURES IN WONDERLAND   Alice's Adventures in Wonderl…\n#> 2           11 <NA>                               Alice's Adventures in Wonderl…\n#> 3           11 Lewis Carroll                      Alice's Adventures in Wonderl…\n#> 4           11 <NA>                               Alice's Adventures in Wonderl…\n#> 5           11 THE MILLENNIUM FULCRUM EDITION 3.0 Alice's Adventures in Wonderl…\n#> 6           11 <NA>                               Alice's Adventures in Wonderl…\ntable(alice_and_jane$title)\n#> \n#> Alice's Adventures in Wonderland      Jane Eyre: An Autobiography \n#>                             3339                            20659\nlibrary(janitor)\n# TODO There's a way to do this within janitor, but I forget, need to look it up.\nalice_and_jane <- \n  alice_and_jane %>% \n  mutate(blank_line = if_else(text == \"\", 1, 0)) %>% \n  filter(blank_line == 0) %>% \n  select(-blank_line)\n\ntable(alice_and_jane$title)\n#> \n#> Alice's Adventures in Wonderland      Jane Eyre: An Autobiography \n#>                             2481                            16395\nset.seed(853)\n\nalice_and_jane$rows <- c(1:nrow(alice_and_jane))\nsample_from_me <- alice_and_jane %>% filter(title == \"Jane Eyre: An Autobiography\")\nkeep_me <- sample(x = sample_from_me$rows, size = 2481, replace = FALSE)\n\nalice_and_jane <- \n  alice_and_jane %>% \n  filter(title == \"Alice's Adventures in Wonderland\" | rows %in% keep_me) %>% \n  select(-rows)\n\ntable(alice_and_jane$title)\n#> \n#> Alice's Adventures in Wonderland      Jane Eyre: An Autobiography \n#>                             2481                             2481\nalice_and_jane <- \n  alice_and_jane %>% \n  group_by(title) %>% \n  mutate(line_number = paste(gutenberg_id, row_number(), sep = \"_\")) %>% \n  ungroup()\nlibrary(tidytext)\nalice_and_jane_by_word <- \n  alice_and_jane %>% \n  unnest_tokens(word, text) %>%\n  group_by(word) %>%\n  filter(n() > 10) %>%\n  ungroup()\nalice_and_jane_by_word$word %>% unique() %>% length()\n#> [1] 585\nalice_and_jane_by_word <- \n  alice_and_jane_by_word %>% \n  group_by(title, line_number) %>% \n  mutate(number_of_words_in_line = n()) %>% \n  ungroup() %>% \n  filter(number_of_words_in_line > 2) %>% \n  select(-number_of_words_in_line)\nlibrary(tidymodels)\n\nset.seed(853)\n\nalice_and_jane_by_word_split <- \n  alice_and_jane_by_word %>%\n  select(title, line_number) %>% \n  distinct() %>% \n  initial_split(prop = 3/4, strata = title)\n\n# alice_and_jane_by_word_train <- training(alice_and_jane_by_word_split) %>% select(line_number)\n# alice_and_jane_by_word_test <- testing(alice_and_jane_by_word_split)\n# \n# rm(alice_and_jane_by_word_split)\nalice_and_jane_dtm_training <- \n  alice_and_jane_by_word %>% \n  count(line_number, word) %>% \n  inner_join(training(alice_and_jane_by_word_split) %>% select(line_number)) %>% \n  cast_dtm(term = word, document = line_number, value = n)\n\ndim(alice_and_jane_dtm_training)\n#> [1] 3413  585\n\nresponse <- \n  data.frame(id = dimnames(alice_and_jane_dtm_training)[[1]]) %>% \n  separate(id, into = c(\"book\", \"line\", sep = \"_\")) %>% \n  mutate(is_alice = if_else(book == 11, 1, 0)) \n  \n\npredictor <- alice_and_jane_dtm_training[] %>% as.matrix()\nlibrary(glmnet)\n\nmodel <- cv.glmnet(x = predictor,\n                   y = response$is_alice,\n                   family = \"binomial\",\n                   keep = TRUE\n                   )\n\nsave(model, file = \"outputs/models/alice_vs_jane.rda\")\nload(\"outputs/models/alice_vs_jane.rda\")\nlibrary(glmnet)\nlibrary(broom)\n\ncoefs <- model$glmnet.fit %>%\n  tidy() %>%\n  filter(lambda == model$lambda.1se)\n\ncoefs %>% head()\n#> # A tibble: 6 × 5\n#>   term         step estimate  lambda dev.ratio\n#>   <chr>       <dbl>    <dbl>   <dbl>     <dbl>\n#> 1 (Intercept)    36 -0.335   0.00597     0.562\n#> 2 in             36 -0.144   0.00597     0.562\n#> 3 she            36  0.390   0.00597     0.562\n#> 4 so             36  0.00249 0.00597     0.562\n#> 5 a              36 -0.117   0.00597     0.562\n#> 6 about          36  0.279   0.00597     0.562\ncoefs %>%\n  group_by(estimate > 0) %>%\n  top_n(10, abs(estimate)) %>%\n  ungroup() %>%\n  ggplot(aes(fct_reorder(term, estimate), estimate, fill = estimate > 0)) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  coord_flip() +\n  theme_minimal() +\n  labs(x = \"Coefficient\",\n       y = \"Word\") +\n  scale_fill_brewer(palette = \"Set1\")"},{"path":"text-as-data.html","id":"topic-models","chapter":"18 Text as data","heading":"18.3 Topic models","text":"version notes previously circulated part R. Alexander Alexander (2020).","code":""},{"path":"text-as-data.html","id":"overview-8","chapter":"18 Text as data","heading":"18.3.1 Overview","text":"Sometimes statement want know . Sometimes easy, don’t always titles statements, even , sometimes titles define topics well-defined consistent way. One way get consistent estimates topics statement use topic models. many variants, one way use latent Dirichlet allocation (LDA) method Blei, Ng, Jordan (2003), implemented R package ‘topicmodels’ Grün Hornik (2011).key assumption behind LDA method statement, ‘document,’ made person decides topics like talk document, chooses words, ‘terms,’ appropriate topics. topic thought collection terms, document collection topics. topics specified ex ante; outcome method. Terms necessarily unique particular topic, document one topic. provides flexibility approaches strict word count method. goal words found documents group define topics.","code":""},{"path":"text-as-data.html","id":"document-generation-process","chapter":"18 Text as data","heading":"18.3.2 Document generation process","text":"LDA method considers statement result process person first chooses topics want speak . choosing topics, person chooses appropriate words use topics. generally, LDA topic model works considering document generated probability distribution topics. instance, five topics two documents, first document may comprised mostly first topics; document may mostly final topics (Figure 18.1).\nFigure 18.1: Probability distributions topics\nSimilarly, topic considered probability distribution terms. choose terms used document speaker picks terms topic appropriate proportion. instance, ten terms, one topic defined giving weight terms related immigration; topic may give weight terms related economy (Figure 18.2).\nFigure 18.2: Probability distributions terms\nFollowing Blei Lafferty (2009), Blei (2012) Griffiths Steyvers (2004), process document generated formally considered :\\(1, 2, \\dots, k, \\dots, K\\) topics vocabulary consists \\(1, 2, \\dots, V\\) terms. topic, decide terms topic uses randomly drawing distributions terms. distribution terms \\(k\\)th topic \\(\\beta_k\\). Typically topic small number terms Dirichlet distribution hyperparameter \\(0<\\eta<1\\) used: \\(\\beta_k \\sim \\mbox{Dirichlet}(\\eta)\\).11 Strictly, \\(\\eta\\) actually vector hyperparameters, one \\(K\\), practice tend value.Decide topics document cover randomly drawing distributions \\(K\\) topics \\(1, 2, \\dots, d, \\dots, D\\) documents. topic distributions \\(d\\)th document \\(\\theta_d\\), \\(\\theta_{d,k}\\) topic distribution topic \\(k\\) document \\(d\\). , Dirichlet distribution hyperparameter \\(0<\\alpha<1\\) used usually document cover handful topics: \\(\\theta_d \\sim \\mbox{Dirichlet}(\\alpha)\\). , strictly \\(\\alpha\\) vector length \\(K\\) hyperparameters, practice usually value.\\(1, 2, \\dots, n, \\dots, N\\) terms \\(d\\)th document, choose \\(n\\)th term, \\(w_{d, n}\\):\nRandomly choose topic term \\(n\\), document \\(d\\), \\(z_{d,n}\\), multinomial distribution topics document, \\(z_{d,n} \\sim \\mbox{Multinomial}(\\theta_d)\\).\nRandomly choose term relevant multinomial distribution terms topic, \\(w_{d,n} \\sim \\mbox{Multinomial}(\\beta_{z_{d,n}})\\).\nRandomly choose topic term \\(n\\), document \\(d\\), \\(z_{d,n}\\), multinomial distribution topics document, \\(z_{d,n} \\sim \\mbox{Multinomial}(\\theta_d)\\).Randomly choose term relevant multinomial distribution terms topic, \\(w_{d,n} \\sim \\mbox{Multinomial}(\\beta_{z_{d,n}})\\).Given set-, joint distribution variables (Blei (2012), p.6):\n\\[p(\\beta_{1:K}, \\theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}) = \\prod^{K}_{=1}p(\\beta_i) \\prod^{D}_{d=1}p(\\theta_d) \\left(\\prod^N_{n=1}p(z_{d,n}|\\theta_d)p\\left(w_{d,n}|\\beta_{1:K},z_{d,n}\\right) \\right).\\]Based document generation process analysis problem, discussed next section, compute posterior \\(\\beta_{1:K}\\) \\(\\theta_{1:D}\\), given \\(w_{1:D, 1:N}\\). intractable directly, can approximated (Griffiths Steyvers (2004) Blei (2012)).","code":""},{"path":"text-as-data.html","id":"analysis-process","chapter":"18 Text as data","heading":"18.3.3 Analysis process","text":"documents created, analyse. term usage document, \\(w_{1:D, 1:N}\\), observed, topics hidden, ‘latent.’ know topics document, terms defined topics. , know probability distributions Figures 18.1 18.2. sense trying reverse document generation process – terms like discover topics.earlier process around documents generated assumed observe terms document, can obtain estimates topics (Steyvers Griffiths (2006)). outcomes LDA process probability distributions define topics. term given probability member particular topic, document given probability particular topic. , trying calculate posterior distribution topics given terms observed document (Blei (2012), p.7):\n\\[p(\\beta_{1:K}, \\theta_{1:D}, z_{1:D, 1:N} | w_{1:D, 1:N}) = \\frac{p\\left(\\beta_{1:K}, \\theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}\\right)}{p(w_{1:D, 1:N})}.\\]initial practical step implementing LDA given corpus documents remove ‘stop words.’ words common, don’t typically help define topics. general list stop words : “”; “’s”; “able”; “”; “”… also remove punctuation capitalisation. documents need transformed document-term-matrix. essentially table column number times term appears document.dataset ready, R package ‘topicmodels’ Grün Hornik (2011) can used implement LDA approximate posterior. using Gibbs sampling variational expectation-maximization algorithm. Following Steyvers Griffiths (2006) Darling (2011), Gibbs sampling process attempts find topic particular term particular document, given topics terms documents. Broadly, first assigning every term every document random topic, specified Dirichlet priors \\(\\alpha = \\frac{50}{K}\\) \\(\\eta = 0.1\\) (Steyvers Griffiths (2006) recommends \\(\\eta = 0.01\\)), \\(\\alpha\\) refers distribution topics \\(\\eta\\) refers distribution terms (Grün Hornik (2011), p.7). selects particular term particular document assigns new topic based conditional distribution topics terms documents taken given (Grün Hornik (2011), p.6):\n\\[p(z_{d, n}=k | w_{1:D, 1:N}, z'_{d, n}) \\propto \\frac{\\lambda'_{n\\rightarrow k}+\\eta}{\\lambda'_{.\\rightarrow k}+V\\eta} \\frac{\\lambda'^{(d)}_{n\\rightarrow k}+\\alpha}{\\lambda'^{(d)}_{-}+K\\alpha} \\]\n\\(z'_{d, n}\\) refers topic assignments; \\(\\lambda'_{n\\rightarrow k}\\) count many times term assigned topic \\(k\\); \\(\\lambda'_{.\\rightarrow k}\\) count many times term assigned topic \\(k\\); \\(\\lambda'^{(d)}_{n\\rightarrow k}\\) count many times term assigned topic \\(k\\) particular document; \\(\\lambda'^{(d)}_{-}\\) count many times term assigned document. \\(z_{d,n}\\) estimated, estimates distribution words topics topics documents can backed .conditional distribution assigns topics depending often term assigned topic previously, common topic document (Steyvers Griffiths (2006)). initial random allocation topics means results early passes corpus document poor, given enough time algorithm converges appropriate estimate.","code":""},{"path":"text-as-data.html","id":"warnings-and-extensions","chapter":"18 Text as data","heading":"18.3.4 Warnings and extensions","text":"choice number topics, k, affects results, must specified priori. strong reason particular number, can used. Otherwise, one way choose appropriate number use test training set process. Essentially, means running process variety possible values k picking appropriate value performs well.One weakness LDA method considers ‘bag words’ order words matter (Blei (2012)). possible extend model reduce impact bag--words assumption add conditionality word order. Additionally, alternatives Dirichlet distribution can used extend model allow correlation. instance, Hansard topics related army may expected commonly found topics related navy, less commonly topics related banking.","code":""},{"path":"text-as-data.html","id":"word-embedding","chapter":"18 Text as data","heading":"18.4 Word embedding","text":"","code":""},{"path":"text-as-data.html","id":"conclusion-1","chapter":"18 Text as data","heading":"18.5 Conclusion","text":"Using text data exciting quantity variety text available us. general, dealing text datasets messy. lot cleaning preparation typically required. Often text datasets large. , workflow place, work reproducible way, simulating data first, clearly communicating findings becomes critical, keep everything organised mind. Nonetheless, exciting area, encourage regularly use text analysis possible.terms next steps two, related, concerns: data analysis.terms data many places get large amounts text data relatively easily, including:r package rtweets makes easy get Twitter data (although typically going looking forward start using , rather able look back). Plenty people U T work Twitter data including Jia Xue iSchool, Ludovic Rheault political science.inside Airbnb dataset used earlier provides text reviews.’ve seen gutenbergr package already notes, provides easy access text Project Gutenberg.’ve seen scraping Wikipedia, going bit may find better use package, instance WikipediR.terms analysis:Start going tidytext book, tidytext, lot nice explanations, code, examples.worthwhile working Quanteda package quanteda tutorials.Finally, consider packages text2vec, spacyr.","code":""},{"path":"text-as-data.html","id":"exercises-and-tutorial-17","chapter":"18 Text as data","heading":"18.6 Exercises and tutorial","text":"","code":""},{"path":"text-as-data.html","id":"exercises-17","chapter":"18 Text as data","heading":"18.6.1 Exercises","text":"","code":""},{"path":"text-as-data.html","id":"tutorial-17","chapter":"18 Text as data","heading":"18.6.2 Tutorial","text":"","code":""},{"path":"using-the-cloud.html","id":"using-the-cloud","chapter":"19 Using the cloud","heading":"19 Using the cloud","text":"STATUS: construction.Required readingRecommended readingEdmondson, Mark, 2020, ‘googleComputeEngineR documentation,’ version 0.3.0.9000, freely available : https://cloudyr.github.io/googleComputeEngineR/.McDermott, Grant R., 2020, ‘Cloud computing Google Compute Engine,’ Data Science Economists, freely available : https://raw.githack.com/uo-ec607/lectures/master/14-gce/14-gce.html.Morris, Mitzi, 2020, ‘Stan Notebooks Cloud,’ freely available : https://mc-stan.org/users/documentation/case-studies/jupyter_colab_notebooks_2020.html.Key concepts/skills/etcBenefits/costs cloud.Getting started cloud.Starting virtual machines R Studio.Stopping virtual machines.","code":""},{"path":"using-the-cloud.html","id":"introduction-25","chapter":"19 Using the cloud","heading":"19.1 Introduction","text":"Cloud benefits:\n- Costs can reduced, easily amortized.\n- Can scale need.\n- Many platforms already sorted e.g. R Studio just works.stole someone can’t remember , cloud another name ‘someone else’s computer.’ ’s . Nonetheless, learning use someone else’s computer can great number reasons including:Scalability: can quite expensive buy new computer, especially need run something every now , using someone else’s computer, can just rent hours days.Portability: can shift analysis workflow laptop cloud, suggests likely good things terms reproducibility portability. least, code capable running laptop cloud.Set--forget: something take , can great worry laptop’s fan running overnight, partner/baby/pet/housemate/etc accidently closing computer, able watch Netflix computer.use cloud running code ‘virtual machine.’ part larger bunch computers designed act like computer specific features. instance may specify virtual machine 8 GB RAM, 128 storage, 4 CPUs. VM act like computer specifications. cost use cloud options increases based specifications virtual machine choose.downsides:Cost: cloud options cheap, rarely free. (free options, tend powerful, end pay get computer better laptop.) give idea cost, use AWS, typically end spending five ten dollars couple days. ’s fairly cheap, ’s nothing. ’s also pretty easy accidently forget something run unexpected bill, especially initially.Public: pretty easy make mistakes accidently make everything public.Time: takes time get set-comfortable cloud.notes going introduce cloud starting options pretty much anyone can () take advantage : Google Colab; moving general cloud options including Google Compute Engine, AWS, Azure, may useful cases. want get job industry, advice pretty much every speaker industry Toronto Data Workshop learn least one cloud options. instance, Munich Re Azure shop, Receptiviti uses AWS, etc.","code":""},{"path":"using-the-cloud.html","id":"google-colab","chapter":"19 Using the cloud","heading":"19.2 Google Colab","text":"Google Colab similar R Studio Cloud, set-allow just log get started. case, need Google account. ’s better R Studio resources put development can use GPUs, hand designed Python, can use R, ’s really focused .get started need tell Google Colab want use R. can using : https://colab.research.google.com/notebook#create=true&language=r.point Jupyter notebook open run R. (R Markdown document.) can install packages normal, e.g. install.packages(\"tidyverse\"), call package e.g. library(tidyverse).Google Colab good option good reason using broader capabilities . want go deeper Morris reading bunch options can explore, Morris puts ‘Colab gateway drug - large-scale processing pipelines ’ll need move Google Cloud Platform one competitors AWS, Azure, etc.’ now.","code":""},{"path":"using-the-cloud.html","id":"aws","chapter":"19 Using the cloud","heading":"19.3 AWS","text":"Amazon Web Services cloud service Amazon. get started need AWS Developer account can create : https://aws.amazon.com/developer/.created account, need select region computer access located. , want “Launch virtual machine” (EC2).first step choose Amazon Machine Image (AMI). provides details computer using. instance, local computer may MacBook running Catalina. Helpfully, Louis Aslett provides bunch already set - http://www.louisaslett.com/RStudio_AMI/. can either select code region registered , can click link. benefit AMI set-specifically R Studio, however trade-little -dated, compiled May 2019.next step can choose powerful computer . free tier fairly basic computer, can choose better ones need . point can pretty much just launch instance. start using AWS seriously look different security settings.instance now running. can go pasting ‘public DNS’ browser. username ‘rstudio’ password instance ID.R Studio running, exciting. first thing probably change default password using instructions instance.don’t need install, say, tidyverse, instead can just call library keep going. can see list packages installed installed.packages(). instance, rstan already installed. can use GPUs want.Perhaps important able start AWS instance able stop (don’t get billed). free tier pretty great, need turn . stop instance, AWS instances page, select , ‘Actions -> Instance State -> Terminate.’","code":""},{"path":"using-the-cloud.html","id":"google-compute-engine","chapter":"19 Using the cloud","heading":"19.4 Google Compute Engine","text":"main R package related Google Compute Engine seems : googleComputeEngineR.reading Grant McDermott pretty good walk-.","code":""},{"path":"using-the-cloud.html","id":"azure","chapter":"19 Using the cloud","heading":"19.5 Azure","text":"bunch R packages related Azure : https://github.com/Azure/AzureR.","code":""},{"path":"using-the-cloud.html","id":"exercises-and-tutorial-18","chapter":"19 Using the cloud","heading":"19.6 Exercises and tutorial","text":"","code":""},{"path":"using-the-cloud.html","id":"exercises-18","chapter":"19 Using the cloud","heading":"19.6.1 Exercises","text":"","code":""},{"path":"using-the-cloud.html","id":"tutorial-18","chapter":"19 Using the cloud","heading":"19.6.2 Tutorial","text":"","code":""},{"path":"deploying-models.html","id":"deploying-models","chapter":"20 Deploying models","heading":"20 Deploying models","text":"STATUS: construction.Required readingChip Huyen, 2020, ‘Machine learning going real-time,’ 27 December, https://huyenchip.com/2020/12/27/real-time-machine-learning.html.Required viewingBlair, James, 2019, ‘Democratizing R Plumber APIs,’ RStudio Conference, 24 January, https://www.rstudio.com/resources/rstudioconf-2019/democratizing-r--plumber-apis/.Nolis, Heather, Jacqueline Nolis, ‘’re hitting R million times day made talk ,’ RStudio Conference, 30 January, https://www.rstudio.com/resources/rstudioconf-2020/-re-hitting-r--million-times--day---made--talk--/.Recommended readingKey concepts/skills/etcPutting models production requires different set skills building model. need familiarity cloud provider, APIs, course modelling. biggest difficulty, , getting things set-.Key librariesplumbershinyKey functionsQuiz","code":""},{"path":"deploying-models.html","id":"introduction-26","chapter":"20 Deploying models","heading":"20.1 Introduction","text":"key troupe R ’s production. ’m convince one way another, however section go bunch different tools allow lot R wanted. topics cover :SQL databases.Docker.Plumber APIs models.ShinyPackagesThe general idea need know whole workflow. point, ’ve able scrape data website, bring order chaos, make charts, appropriately model , write . academic settings enough. many industry settings ’re going want use model something. instance, set-website allows model used generate insurance quote given several inputs.One way deploy model use Shiny, seen examples earlier notes. enables individual use model. doesn’t really scale well. instance, wanted sell model forecasts businesses, might way like users interact results. general problem want model results available machines want make APIs.","code":""},{"path":"deploying-models.html","id":"packages","chapter":"20 Deploying models","heading":"20.2 Packages","text":"point ’ve largely using R Packages things us. However, another way loadedDoSS Toolkit","code":""},{"path":"deploying-models.html","id":"shiny-1","chapter":"20 Deploying models","heading":"20.3 Shiny","text":"","code":""},{"path":"deploying-models.html","id":"plumber-and-model-apis","chapter":"20 Deploying models","heading":"20.4 Plumber and model APIs","text":"","code":""},{"path":"deploying-models.html","id":"hello-toronto","chapter":"20 Deploying models","heading":"20.4.1 Hello Toronto","text":"general idea behind plumber package (Schloerke Allen 2021) can train model make available via API can call want forecast. ’s pretty great.Just get something working, let’s make function returns ‘Hello Toronto’ regardless output. Open new R file, add following, save ‘plumber.R’ (may need install plumber package ’ve done yet).saved, top right editor get button ‘Run API.’ Click , API load. ’ll ‘Swagger’ application, provides GUI around API. Expand GET method, clik ‘Try ’ ‘Execute.’ response body, get ‘Toronto.’closely reflect fact API designed computers, can copy/paste ‘request HTML’ browser return ‘Hello Toronto.’","code":"\nlibrary(plumber)\n\n#* @get /print_toronto\nprint_toronto <- function() {\n  result <- \"Hello Toronto\"\n  return(result)\n}"},{"path":"deploying-models.html","id":"local-model","chapter":"20 Deploying models","heading":"20.4.2 Local model","text":"Now, ’re going update API serves model output, given input. ’re going follow Buhr (2017) fairly closely.point, ’d recommend starting new R Project. get started, let’s simulate data train model . case ’re interested forecasting long baby may sleep overnight, given know long slept afternoon nap.Let’s now use tidymodels quickly make dodgy model.point, model. One difference might used ’ve saved model ‘.rds’ file. going read .Now model want put file use API access, called ‘plumber.R.’ also want file sets API, called ‘server.R.’ make R script called ‘server.R’ add following content:‘plumber.R’ add following content:, save ‘plumber.R’ file option ‘Run API.’ Click can try API locally way .","code":"\nlibrary(tidyverse)\nset.seed(853)\n\nnumber_of_observations <- 1000\n\nbaby_sleep <- \n  tibble(afternoon_nap_length = rnorm(number_of_observations, 120, 5) %>% abs(),\n         noise = rnorm(number_of_observations, 0, 120),\n         night_sleep_length = afternoon_nap_length * 4 + noise,\n         )\n\nbaby_sleep %>% \n  ggplot(aes(x = afternoon_nap_length, y = night_sleep_length)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Baby's afternoon nap length (minutes)\",\n       y = \"Baby's overnight sleep length (minutes)\") +\n  theme_classic()\nset.seed(853)\nlibrary(tidymodels)\n\nbaby_sleep_split <- rsample::initial_split(baby_sleep, prop = 0.80)\nbaby_sleep_train <- rsample::training(baby_sleep_split)\nbaby_sleep_test <- rsample::testing(baby_sleep_split)\n\nmodel <- \n  parsnip::linear_reg() %>%\n  parsnip::set_engine(engine = \"lm\") %>% \n  parsnip::fit(night_sleep_length ~ afternoon_nap_length, \n               data = baby_sleep_train\n               )\n\nwrite_rds(x = model, file = \"baby_sleep.rds\")\nlibrary(plumber)\n\nserve_model <- plumb(\"plumber.R\")\nserve_model$run(port = 8000)\nlibrary(plumber)\nlibrary(tidyverse)\n\nmodel <- readRDS(\"baby_sleep.rds\")\n\nversion_number <- \"0.0.1\"\n\nvariables <- \n  list(\n    afternoon_nap_length = \"A value in minutes, likely between 0 and 240.\",\n    night_sleep_length = \"A forecast, in minutes, likely between 0 and 1000.\"\n  )\n\n#* @param afternoon_nap_length\n#* @get /survival\npredict_sleep <- function(afternoon_nap_length=0) {\n  afternoon_nap_length = as.integer(afternoon_nap_length)\n  \n  payload <- data.frame(afternoon_nap_length=afternoon_nap_length)\n  \n  prediction <- predict(model, payload)\n\n  result <- list(\n    input = list(payload),\n    response = list(\"estimated_night_sleep\" = prediction),\n    status = 200,\n    model_version = version_number)\n\n  return(result)\n}"},{"path":"deploying-models.html","id":"cloud-model","chapter":"20 Deploying models","heading":"20.4.3 Cloud model","text":"point, ’ve got API working machine, really want get working computer API can accessed anyone. going use DigitalOcean - https://www.digitalocean.com. charged service, create account, come $100 credit, enough get started.set-process pain take time, need . Install two additional packages assist :plumberDeploy (J. Allen 2021).analogsea (Chamberlain, Wickham, Chang 2021).Now need connect local computer DigitalOcean account. Get started :Now need authenticate connection done using SSH public key. can using:first time may useful visual process, case follow instructions : https://docs.digitalocean.com/products/droplets/-/add-ssh-keys/-account/. want ‘.pub’ file computer. copy public key aspect file, add SSH keys section account security settings. key local computer can check using:, take validate. DigitalOcean calls every computer start ‘droplet.’ start three computers, ’ll started three droplets. can check droplets running using:everything set-properly, print information droplets associated account (point, probably none).create droplet, run:’ll get asked SSH passphrase ’ll just set-bunch things. ’re going need install whole bunch things onto droplet:finally set-(’ll seriously take 30 min ) can deploy API!","code":"\ninstall.packages(\"plumberDeploy\")\nremotes::install_github(\"sckott/analogsea\")\nanalogsea::account()\nanalogsea::key_create()\nssh::ssh_key_info()\nanalogsea::droplets()\nid <- plumberDeploy::do_provision(example = FALSE)\nanalogsea::install_r_package(droplet = id, c(\"plumber\", \n                                             \"remotes\", \n                                             \"here\"))\nanalogsea::debian_apt_get_install(id, \"libssl-dev\", \n                                  \"libsodium-dev\", \n                                  \"libcurl4-openssl-dev\")\nanalogsea::debian_apt_get_install(id, \n                                  \"libxml2-dev\")\n\nanalogsea::install_r_package(id, c(\"config\",\n                                   \"httr\",\n                                   \"urltools\",\n                                   \"plumber\"))\n\nanalogsea::install_r_package(id, c(\"xml2\"))\nanalogsea::install_r_package(id, c(\"tidyverse\"))\n\nanalogsea::install_r_package(id, c(\"tidymodels\"))\nplumberDeploy::do_deploy_api(droplet = id, \n                             path = \"example\", \n                             localPath = getwd(), \n                             port = 8000, \n                             docs = TRUE, \n                             overwrite=TRUE)"},{"path":"deploying-models.html","id":"exercises-and-tutorial-19","chapter":"20 Deploying models","heading":"20.5 Exercises and tutorial","text":"","code":""},{"path":"deploying-models.html","id":"exercises-19","chapter":"20 Deploying models","heading":"20.5.1 Exercises","text":"","code":""},{"path":"deploying-models.html","id":"tutorial-19","chapter":"20 Deploying models","heading":"20.5.2 Tutorial","text":"","code":""},{"path":"efficiency.html","id":"efficiency","chapter":"21 Efficiency","heading":"21 Efficiency","text":"STATUS: construction.Required readingRequired viewingRecommended readingKey concepts/skills/etcKey librariesKey functionsQuiz","code":""},{"path":"efficiency.html","id":"introduction-27","chapter":"21 Efficiency","heading":"21.1 Introduction","text":"","code":""},{"path":"efficiency.html","id":"data-efficiency","chapter":"21 Efficiency","heading":"21.2 Data efficiency","text":"","code":""},{"path":"efficiency.html","id":"sql","chapter":"21 Efficiency","heading":"21.2.1 SQL","text":"","code":""},{"path":"efficiency.html","id":"feather","chapter":"21 Efficiency","heading":"21.2.2 Feather","text":"","code":""},{"path":"efficiency.html","id":"code-efficiency","chapter":"21 Efficiency","heading":"21.3 Code efficiency","text":"large, worrying performance waste time. part far better , just pushing things cloud, letting run reasonable time, using time worry aspects pipeline. However, eventually becomes unfeasible. , something takes day run just becomes pain. rarely common area obvious performance gains. Instead need learn measure cut.fast valuable ’s mostly able iterate fast code running fast. find speed code completes bottle neck shard. throw machines . shard . throw machines .","code":""},{"path":"efficiency.html","id":"code-refactoring","chapter":"21 Efficiency","heading":"21.4 Code refactoring","text":"baby examples, focused data science, along lines : https://indrajeetpatil.github.io/Refactoring-ggstatsplot/refactoring-ggstatsplot#1Start example bad code, gets fixed.","code":""},{"path":"efficiency.html","id":"measure","chapter":"21 Efficiency","heading":"21.4.1 Measure","text":"Using tic() tic().Measuring","code":""},{"path":"efficiency.html","id":"experimental-efficiency","chapter":"21 Efficiency","heading":"21.5 Experimental efficiency","text":"Multi-armed bandit","code":""},{"path":"efficiency.html","id":"other-languages","chapter":"21 Efficiency","heading":"21.6 Other languages","text":"","code":""},{"path":"efficiency.html","id":"python","chapter":"21 Efficiency","heading":"21.6.1 Python","text":"","code":""},{"path":"efficiency.html","id":"julia","chapter":"21 Efficiency","heading":"21.6.2 Julia","text":"","code":""},{"path":"efficiency.html","id":"exercises-and-tutorial-20","chapter":"21 Efficiency","heading":"21.7 Exercises and tutorial","text":"","code":""},{"path":"efficiency.html","id":"exercises-20","chapter":"21 Efficiency","heading":"21.7.1 Exercises","text":"","code":""},{"path":"efficiency.html","id":"tutorial-20","chapter":"21 Efficiency","heading":"21.7.2 Tutorial","text":"","code":""},{"path":"concludingremarks.html","id":"concludingremarks","chapter":"22 Concluding remarks, open issues, next steps","heading":"22 Concluding remarks, open issues, next steps","text":"STATUS: construction.","code":""},{"path":"concludingremarks.html","id":"concluding-remarks","chapter":"22 Concluding remarks, open issues, next steps","heading":"22.1 Concluding remarks","text":"’s old saying, something along lines ‘may live interesting times.’ ’m sure every generation feels , sure live interesting times. book, tried convey essentials think allow contribute. just getting started.’m 35 ‘data science didn’t exist undergraduate’ generation. little decade data science gone something barely existed defining part academia industry. imply ? may imply one just making decisions optimize data science looks like right now, also happen. ’s little difficult, ’s also one things makes data science exciting. might mean choices like:taking courses fundamentals, just fashionable applications;reading books, just whatever trending; andtrying intersection least different areas, rather hyper-specialized.’m just someone likes play data using R. decade ago wouldn’t fit particular department. ’m lucky days space data science someone like . nice thing now call data science ’s space well.Data science needs diversity. Data science needs intelligence enthusiasm. needs room, able make contributions. live interesting times ’s just exciting time enthusiastic data. can’t wait see build.","code":""},{"path":"concludingremarks.html","id":"some-issues","chapter":"22 Concluding remarks, open issues, next steps","heading":"22.2 Some issues","text":"write unit tests data science?UPDATE add functional tests stuffOne thing working real computer scientists taught importance unit tests. Basically just means writing small checks heads time. Like column purports year, ’s unlikely ’s character, ’s unlikely ’s integer larger 2500, ’s unlikely ’s negative integer. know , writing unit tests us write .case ’s obvious unit test looks like. generally, often little idea results look like ’re running well. approach ’ve taken add simulation—simulate reasonable results, write unit tests based , bring real data bear adjust necessary. really think need extensive work area current state---art lacking.happened machine learning revolution?don’t understand happened promised machine learning revolution social sciences. Specifically, ’m yet see convincing application machine learning methods designed prediction social sciences problem care understanding. like either see evidence definitive thesis can’t happen. current situation untenable folks, especially fields historically female, made feel inferior even though results worse.think power?someone learnt statistics economists, now partly statistics department, think everyone learn statistics statisticians. isn’t anything economists, conversations statistics department statistical methods used different ’ve departments.think problem people outside statistics, treat statistics recipe follow various steps comes cake. regard ‘power’—turns bunch instructions one bothered check—turned oven temperature without checking 180C, ’s fine whatever mess came accepted people evaluating cake didn’t know needed check temperature appropriately set. (’m ditching analogy right now).know, issue power related broader discussion p-values, basically one taught properly, require changing awful lot teach statistics .e. moving away recipe approach., specific issue people think statistics recipe followed. think ’s trained especially social sciences like political science economics, ’s rewarded. ’s methods . Instead, statistics collection different instruments let us look data certain way. think need revolution , metaphorical tucking one’s shirt.","code":""},{"path":"concludingremarks.html","id":"next-steps-1","chapter":"22 Concluding remarks, open issues, next steps","heading":"22.3 Next steps","text":"book covered much ground, toward end , butler Stevens told novel Remains Day (Ishiguro 1989):evening’s best part day. ’ve done day’s work. Now can put feet enjoy .Chances aspects want explore , building foundation established. , ’ve accomplished set .new data science start book, next step backfill skipped , recommend Timbers, Campbell, Lee (2022). , learn R terms data science going Wickham Grolemund (2017). deepen understanding R , go next Wickham (2019a).’re interested learning causality start Cunningham (2021) Huntington-Klein (2021).’re interested learn statistics begin McElreath (2020), backfill Johnson, Ott, Dogucu (2022) solidify foundation Gelman et al. (2014). probably also backfill fundamentals around probability, starting Wasserman (2005).one next natural step ’re interested learning statistical (’s come called machine) learning ’s James et al. (2017) followed Friedman, Tibshirani, Hastie (2009).’re interested sampling next book turn Lohr (2019). deepen understanding surveys experiments, go next Gerber Green (2012) combination Kohavi, Tang, Xu (2020).graphs turn Healy (2018).Writing go …Thinking production SQL things like, next natural step …often hear phrase let data speak. Hopefully point understand never happens. can acknowledge ones using data tell stories, strive seek make worthy.","code":""},{"path":"oh-you-think-and-shoulders.html","id":"oh-you-think-and-shoulders","chapter":"A Oh you think and shoulders","heading":"A Oh you think and shoulders","text":"","code":""},{"path":"oh-you-think-and-shoulders.html","id":"oh-you-think-we-have-good-data-on-that","chapter":"A Oh you think and shoulders","heading":"A.1 Oh, you think we have good data on that!","text":"Oh, think good data ! economy. https://www.nber.org/system/files/chapters/c4224/c4224.pdf.Oh, think good data ! Migration.Oh, think good data ! Weather stationsOh, think good data ! Olympics events. decides scoring?. timing?Oh, think good data ! Personality scores. Myers Briggs Big 5 generally.Oh, think good data ! Cause deathOh, think good data ! City boundaries. constitues ‘Atlanta?’ Different definitions - metro, X, Y. (also issue countries boundaries changing time)Oh, think good data ! Timing","code":""},{"path":"oh-you-think-and-shoulders.html","id":"shoulders-of-giants","chapter":"A Oh you think and shoulders","heading":"A.2 Shoulders of giants","text":"Andrew GelmanBarbara BailarDaniela WittenDi CookElizabeth ScottEvelyn KitagawaGertrude Mary CoxHadley WickhamJohn TukeyKatherine WallmanMichael JordanNancy ReidRobert GentlemanRoss IhakaSimon KuznetsStella CunliffeRob TibshiraniTimnit Gebru","code":""},{"path":"oh-you-think-and-shoulders.html","id":"possible-datasets","chapter":"A Oh you think and shoulders","heading":"A.3 Possible datasets","text":"https://som.yale.edu/faculty-research/-centers/international-center-finance/dataAlex cookson’s","code":""},{"path":"papers.html","id":"papers","chapter":"B Papers","heading":"B Papers","text":"","code":""},{"path":"papers.html","id":"mandatory-minimums","chapter":"B Papers","heading":"B.1 ‘Mandatory Minimums’","text":"","code":""},{"path":"papers.html","id":"task","chapter":"B Papers","heading":"B.1.1 Task","text":"Working individually entirely reproducible way, please find dataset interest Open Data Toronto write short paper telling story data.","code":""},{"path":"papers.html","id":"guidance","chapter":"B Papers","heading":"B.1.2 Guidance","text":"Find dataset interest Open Data Toronto download reproducible way using R package opendatatoronto (Gelfand 2020).Create folder appropriate sub-folders, add GitHub, prepare PDF using R Markdown sections (welcome use starter folder: https://github.com/RohanAlexander/starter_folder):\ntitle,\nauthor,\ndate,\nabstract,\nintroduction,\ndata, \nreferences.\ntitle,author,date,abstract,introduction,data, andreferences.data section thoroughly precisely discuss source data bias brings (ethical, statistical, otherwise). Comprehensively describe summarize data using text least one graph one table. Graphs must made ggplot (Wickham 2016) tables must made using knitr::kable() (without kableExtra) gt (Iannone, Cheng, Schloerke 2020). Make sure cross-reference graphs tables.Use bibtex add references. sure reference R R packages use, well dataset. Check referenced everything. Strong submissions draw related literature sure also reference . various options R Markdown references style; just pick one used .Go back write introduction. two three paragraphs. last paragraph set remainder paper.Add abstract. three four sentences. add descriptive title (Hint: ‘Paper 1’ descriptive.)Add link GitHub repo via footnote.Check GitHub repo well-organized, add informative README. (Hint: Comment. . Code.). Make sure ’ve got least one R script , addition, R Markdown file.Pull together PDF check paper well-written able understood average reader , say, FiveThirtyEight. means allowed use mathematical notation, must explain plain language. statistical concepts terminology must explained. reader someone university education, necessarily someone understands p-value - explain everything use.Check evidence class assignment.Via Quercus, submit PDF.","code":""},{"path":"papers.html","id":"check-offs-points","chapter":"B Papers","heading":"B.1.3 Check offs points","text":"Check ’ve included R code raw R output final PDF.Check although ’ll probably code R Markdown, make sure least one R script scripts folder.Check thoroughly commented code directly creates PDF. knit html save PDF. knit Word save PDFCheck graph discussion extremely clear, comparable quality FiveThirtyEight.Check date updated.Check entire workflow entirely reproducible.Check typos.","code":""},{"path":"papers.html","id":"faq","chapter":"B Papers","heading":"B.1.4 FAQ","text":"Can use dataset Kaggle instead? , done hard work .can’t use code download dataset, can just manually download ? , entire workflow needs reproducible. Please fix download problem pick different dataset.much write? students submit something two--six-page range, ’s really . precise thorough.data apartment blocks/NBA/League Legends ’s ethical bias aspect, ? Please re-read readings better understand bias ethics. really can’t think something, might worth picking different dataset.Can use Python? . already know Python doesn’t hurt learn another language.need cite R, don’t need cite Word? R free statistical programming language academic origins ’s appropriate acknowledge work others. ’s also important reproducibility.","code":""},{"path":"papers.html","id":"rubric","chapter":"B Papers","heading":"B.1.5 Rubric","text":"Go/-go #1: R cited - [1 ‘Yes,’ 0 ‘’]\nreferred main content included reference list.\n, need continue marking, just give paper 0 overall.\nreferred main content included reference list., need continue marking, just give paper 0 overall.Title - [2 ‘Exceptional,’ 1 ‘Yes,’ 0 ‘Poor done’]\ninformative title included.\nTell reader story - don’t waste time.\nIdeally tell happens end story.\n‘Problem Set X’ informative title. evidence school paper.\ninformative title included.Tell reader story - don’t waste time.Ideally tell happens end story.‘Problem Set X’ informative title. evidence school paper.Author, date, repo - [2 ‘Yes,’ 0 ‘Poor done’]\nauthor, date submission, link GitHub repo clearly included. (later likely, necessarily, statement : ‘Code data supporting analysis available : LINK’).\nauthor, date submission, link GitHub repo clearly included. (later likely, necessarily, statement : ‘Code data supporting analysis available : LINK’).Abstract - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor done’]\nabstract included appropriately pitched general audience.\nabstract answers: 1) done, 2) found, 3) matters (high level).\nabstract longer four sentences need think lot whether long. may fine (always exceptions) probably good reason.\nabstract must tell reader top-level finding. one thing learn world paper?\nabstract included appropriately pitched general audience.abstract answers: 1) done, 2) found, 3) matters (high level).abstract longer four sentences need think lot whether long. may fine (always exceptions) probably good reason.abstract must tell reader top-level finding. one thing learn world paper?Introduction - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor done’]\nintroduction self-contained tells reader everything need know, including putting broader context.\nintroduction provide bit broader context motivate reader, well providing bit detail ’re interested , , found, ’s important, etc.\nreader able read introduction good idea research carried found.\nrare tables figures introduction (always exceptions think deeply whether one).\nmust outline structure paper.\ninstance (just rough guide) introduction 10 page paper, probably 3 4 paragraphs, 10 per cent, depends specifics.\nintroduction self-contained tells reader everything need know, including putting broader context.introduction provide bit broader context motivate reader, well providing bit detail ’re interested , , found, ’s important, etc.reader able read introduction good idea research carried found.rare tables figures introduction (always exceptions think deeply whether one).must outline structure paper.instance (just rough guide) introduction 10 page paper, probably 3 4 paragraphs, 10 per cent, depends specifics.Data - [10 ‘Exceptional,’ 8 ‘Great,’ 6 ‘Good,’ 4 ‘issues,’ 2 ‘Many issues,’ 0 ‘Poor done’]\ndiscuss dataset (data section) make sure discuss least:\nsource data.\nmethodology approach used collect process data.\npopulation, frame, sample (appropriate).\nInformation respondents found. happened non-response?\nkey features, strengths, weaknesses survey generally.\nthoroughly discuss variables dataset use. similar nonetheless don’t use? construct variables combining various ones?\ndata look like?\nPlot actual data ’re using (close can get ).\nDiscuss plots features data.\njust issues strong submissions consider. Show knowledge. becomes detailed, push footnotes appendix.\n‘Exceptional’ means read submission learn something dataset don’t learn submission (within reasonable measure course).\ndiscuss dataset (data section) make sure discuss least:source data.methodology approach used collect process data.population, frame, sample (appropriate).Information respondents found. happened non-response?key features, strengths, weaknesses survey generally.thoroughly discuss variables dataset use. similar nonetheless don’t use? construct variables combining various ones?data look like?Plot actual data ’re using (close can get ).Discuss plots features data.just issues strong submissions consider. Show knowledge. becomes detailed, push footnotes appendix.‘Exceptional’ means read submission learn something dataset don’t learn submission (within reasonable measure course).Numbering - [2 ‘Yes,’ 0 ‘Poor done’]\nfigures, tables, equations, etc numbered referred text.\nfigures, tables, equations, etc numbered referred text.Proofreading - [2 ‘Yes,’ 0 ‘Poor done’]\naspects submission free noticeable typos.\naspects submission free noticeable typos.Graphs/tables/etc - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor done’]\nmust include graphs tables paper must high standard.\nmust well formatted camera-ready clear digestible.\nmust: 1) serve clear purpose; 2) fully self-contained appropriate use labels/explanations, etc; 3) appropriately sized coloured (appropriate significant figures case stats).\nmust include graphs tables paper must high standard.must well formatted camera-ready clear digestible.must: 1) serve clear purpose; 2) fully self-contained appropriate use labels/explanations, etc; 3) appropriately sized coloured (appropriate significant figures case stats).References - [4 ‘Perfect,’ 3 ‘One minor issue,’ 0 ‘Poor done’]\ndata/software/literature/etc appropriately noted cited.\nmust cite software software packages use.\nmust cite datasets use.\nmust cite literature refer (refer literature).\ntake small chunk code Stack Overflow add page comment next code.\ntake large chunk code cite fully.\n3 means one minor issue. one minor issue receives 0.\ndata/software/literature/etc appropriately noted cited.must cite software software packages use.must cite datasets use.must cite literature refer (refer literature).take small chunk code Stack Overflow add page comment next code.take large chunk code cite fully.3 means one minor issue. one minor issue receives 0.Reproducibility - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor done’]\npaper analysis must fully reproducible.\ndetailed README included.\ncode thoroughly documented.\nR project used. use setwd().\ncode must appropriately read data, prepare , create plots, conduct analysis, generate documents. Seeds used needed.\nCode must preamble etc.\nmust appropriately document scripts someone coming follow .\nrepo must thoroughly organized.\npaper analysis must fully reproducible.detailed README included.code thoroughly documented.R project used. use setwd().code must appropriately read data, prepare , create plots, conduct analysis, generate documents. Seeds used needed.Code must preamble etc.must appropriately document scripts someone coming follow .repo must thoroughly organized.General excellence - [3 ‘Exceptional,’ 2 ‘Wow,’ 1 ‘Huh, ’s interesting,’ 0 ‘None’]\nalways students excel way anticipated rubric. item accounts .\nalways students excel way anticipated rubric. item accounts .","code":""},{"path":"papers.html","id":"previous-examples","chapter":"B Papers","heading":"B.1.6 Previous examples","text":"examples papers well past include : Amy Farrow, Morgaine Westin, Rachel Lam.","code":""},{"path":"papers.html","id":"these-numbers-mean-dial-it-up","chapter":"B Papers","heading":"B.2 ‘These numbers mean dial it up’","text":"","code":""},{"path":"papers.html","id":"task-1","chapter":"B Papers","heading":"B.2.1 Task","text":"Please consider scenario:employed junior data scientist Petit Poll - Canadian polling company. Petit Poll contract ‘client’ - Ontario government department - provide advice. particular, client wants understand effect COVID shut-downs restaurant businesses asked Petit Poll design experiment aspect .Working part small team 1-3 people, entirely reproducible way, please decide intervention, measurement strategies, write short paper telling story effect.","code":""},{"path":"papers.html","id":"guidance-1","chapter":"B Papers","heading":"B.2.2 Guidance","text":"Working part team 1-3 people, prepare PDF R Markdown following features (welcome use starter folder: https://github.com/RohanAlexander/starter_folder):\ntitle,\nauthor/s,\ndate,\nabstract,\nintroduction,\ndata,\ndiscussion,\nappendix survey details, \nreferences.\ntitle,author/s,date,abstract,introduction,data,discussion,appendix survey details, andreferences.Decide intervention. aspects address include:\ndesigned implemented?\nrandom ?\nensure separation treatment non-treatment?\nlong run?\ndesigned implemented?random ?ensure separation treatment non-treatment?long run?’ll need run surveys gather information intervention. Decide survey methodology. aspects address include:\npopulation, frame, sample?\nsampling methods use ? statistical properties method brings table?\ngoing reach desired respondents?\nmuch estimate cost?\nsteps take deal non-response non-response affect survey?\ngoing protect respondent privacy?\nRemember consider context ‘client’ - instance, interested ?\npopulation, frame, sample?sampling methods use ? statistical properties method brings table?going reach desired respondents?much estimate cost?steps take deal non-response non-response affect survey?going protect respondent privacy?Remember consider context ‘client’ - instance, interested ?Develop survey platform introduced class, another ’re familiar .\nsure test . want test much possible, maybe even swap informally another group?\nsure test . want test much possible, maybe even swap informally another group?Now release surveys (simulated) ‘field.’\nPlease simulating appropriate number responses survey R.\nDon’t forget simulate relation intervention proposed.\nneed two, even , surveys (hence multiple sets simulated results)?\nPlease simulating appropriate number responses survey R.Don’t forget simulate relation intervention proposed.need two, even , surveys (hence multiple sets simulated results)?Show results discuss ‘findings.’ Everything must entirely reproducible.may wish scrape data /use open data sources appropriately parameterize simulations. Don’t forget cite .Use R Markdown write PDF report . Discuss intervention, results findings, survey design motivations, etc - . writing report eventually go client, must set scene, use language demonstrates command statistical concepts brings reader along . sure include graphs tables reference discussion. sure clear weaknesses biases, opportunities future work.report must well written. allowed , , use mathematical notation statistical concepts, must explain plain language. Similarly, can, , use experimental/survey/sampling/observational data terminology, , need explain .data section specify intervention data gathering methodology. also show data, tables graphs necessary. addition summaries, sure plot raw data extent possible.graphs tables must incredibly high standard. Graphs tables well formatted report-ready. clean digestible. Furthermore, label describe table/figure/equation.discussion section relevant section, please sure discuss ethics bias reference relevant literature. instance, think dataset. statistical ethical implications ?Often folks struggle Discussion section reports. 10 page report, ’re looking 2-3 pages content. ’s example sub-sections include:\nsub-section containing brief overview paper, also fits literature improves existing contributions.\nThree sub-sections detail three main points learn world paper.\nsub-section limitations, discussed sophisticated way. means just stating , explaining justifying.\nsub-section extended future directions.\nsub-section containing brief overview paper, also fits literature improves existing contributions.Three sub-sections detail three main points learn world paper.sub-section limitations, discussed sophisticated way. means just stating , explaining justifying.sub-section extended future directions.client stats graduates working need impressed main content report, also people barely know average people need impressed also.Check referenced everything, including R, R packages, datasets. Strong submissions draw related literature sure also reference . style references matter - various options R Markdown - provided consistent bibtex used.Via Quercus, submit PDF report. must provide link GitHub repo code used assignment lives. Comment. . Code. entire workflow must entirely reproducible. repo clearly organised, useful README included. must include separate R script accomplishes something (probably simulations makes sense). must include R Markdown file produced PDF repo.Please sure include link survey/s report screenshots survey/s appendix report.Everyone team receives mark.evidence class assignment.","code":""},{"path":"papers.html","id":"check-offs-points-1","chapter":"B Papers","heading":"B.2.3 Check offs points","text":"Check ’ve included R code raw R output final PDF.Check ’ve cited R R packages used.Check although ’ll probably code R Markdown, make sure least one R script scripts folder.Check thoroughly commented code directly creates PDF. knit html save PDF. knit Word save PDFCheck graph discussion extremely clear, comparable quality FiveThirtyEight.Check date updated date submission.Check entire workflow entirely reproducible.Check typos.Check ’ve got appendix details survey/s link live survey.","code":""},{"path":"papers.html","id":"faq-1","chapter":"B Papers","heading":"B.2.4 FAQ","text":"Can work ? Yes. recommend forming group workload course assumes ’ll work second third paper part group four.Can switch groups third paper? Yes.can find group? randomly create groups four Quercus. welcome shift groups form groups ’d like.Can get different mark rest group? . Everyone group gets mark.wrote paper , can graded different scale? . papers graded way.much write? students submit something 10--15-page range, ’s really . precise thorough.students collaborate successfully? Groups split work typically seem best. one student worries survey, one simulating analysing data, another write-. ’re worried using GitHub collaborate, just create different folders GitHub place separate bits work, one person bring together end.intervention use? intention something interest . well-written introduction make intervention clear.","code":""},{"path":"papers.html","id":"rubric-1","chapter":"B Papers","heading":"B.2.5 Rubric","text":"Go/-go #1: R cited - [1 ‘Yes,’ 0 ‘’]\nreferred main content included reference list.\n, need continue marking, just give paper 0 overall.\nreferred main content included reference list., need continue marking, just give paper 0 overall.Title - [2 ‘Exceptional,’ 1 ‘Yes,’ 0 ‘Poor done’]\ninformative title included.\nTell reader story - don’t waste time.\nIdeally tell happens end story.\n‘Problem Set X’ informative title. evidence school paper.\ninformative title included.Tell reader story - don’t waste time.Ideally tell happens end story.‘Problem Set X’ informative title. evidence school paper.Author, date, repo - [2 ‘Yes,’ 0 ‘Poor done’]\nauthor, date submission, link GitHub repo clearly included. (later likely, necessarily, statement : ‘Code data supporting analysis available : LINK’).\nauthor, date submission, link GitHub repo clearly included. (later likely, necessarily, statement : ‘Code data supporting analysis available : LINK’).Abstract - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor done’]\nabstract included appropriately pitched general audience.\nabstract answers: 1) done, 2) found, 3) matters (high level).\nabstract longer four sentences need think lot whether long. may fine (always exceptions) probably good reason.\nabstract must tell reader top-level finding. one thing learn world paper?\nabstract included appropriately pitched general audience.abstract answers: 1) done, 2) found, 3) matters (high level).abstract longer four sentences need think lot whether long. may fine (always exceptions) probably good reason.abstract must tell reader top-level finding. one thing learn world paper?Introduction - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor done’]\nintroduction self-contained tells reader everything need know, including putting broader context.\nintroduction provide bit broader context motivate reader, well providing bit detail ’re interested , , found, ’s important, etc.\nreader able read introduction good idea research carried .\nrare tables figures introduction (always exceptions think deeply whether one).\nmust outline structure paper.\ninstance (just rough guide) introduction 10 page paper, probably 3 4 paragraphs, 10 per cent, depends specifics.\nintroduction self-contained tells reader everything need know, including putting broader context.introduction provide bit broader context motivate reader, well providing bit detail ’re interested , , found, ’s important, etc.reader able read introduction good idea research carried .rare tables figures introduction (always exceptions think deeply whether one).must outline structure paper.instance (just rough guide) introduction 10 page paper, probably 3 4 paragraphs, 10 per cent, depends specifics.Data - [10 ‘Exceptional,’ 8 ‘Great,’ 6 ‘Good,’ 4 ‘issues,’ 2 ‘Many issues,’ 0 ‘Poor done’]\nsurvey clear, detailed, justified methodology thoroughly discussed. statistical basis approach used clear.\nachieve aims report.\nPopulation, frame, sample, key aspects described.\ndetailed plan reaching respondents.\nCost discussed appropriate.\nNon-response discussed plan dealing clearly articulated.\nclear respect survey respondents.\n\ndiscuss dataset (’ve likely largely simulated) make sure discuss least:\nsource data.\nmethodology approach used collect process data.\npopulation, frame, sample (appropriate).\nInformation respondents found. happened non-response?\nkey features, strengths, weaknesses survey generally.\n\nthoroughly discuss variables dataset use. similar nonetheless don’t use? construct variables combining various ones?\ndata look like?\nPlot actual data ’re using (close can get ).\nDiscuss plots features data.\njust issues strong submissions consider. Show knowledge. becomes detailed, push footnotes appendix.\n‘Exceptional’ means read submission learn something dataset don’t learn submission (within reasonable measure course).\nsurvey clear, detailed, justified methodology thoroughly discussed. statistical basis approach used clear.\nachieve aims report.\nPopulation, frame, sample, key aspects described.\ndetailed plan reaching respondents.\nCost discussed appropriate.\nNon-response discussed plan dealing clearly articulated.\nclear respect survey respondents.\nachieve aims report.Population, frame, sample, key aspects described.detailed plan reaching respondents.Cost discussed appropriate.Non-response discussed plan dealing clearly articulated.clear respect survey respondents.discuss dataset (’ve likely largely simulated) make sure discuss least:\nsource data.\nmethodology approach used collect process data.\npopulation, frame, sample (appropriate).\nInformation respondents found. happened non-response?\nkey features, strengths, weaknesses survey generally.\nsource data.methodology approach used collect process data.population, frame, sample (appropriate).Information respondents found. happened non-response?key features, strengths, weaknesses survey generally.thoroughly discuss variables dataset use. similar nonetheless don’t use? construct variables combining various ones?data look like?Plot actual data ’re using (close can get ).Discuss plots features data.just issues strong submissions consider. Show knowledge. becomes detailed, push footnotes appendix.‘Exceptional’ means read submission learn something dataset don’t learn submission (within reasonable measure course).Discussion - [10 ‘Exceptional,’ 8 ‘Great,’ 6 ‘Good,’ 4 ‘issues,’ 2 ‘Many issues,’ 0 ‘Poor done’]\nquestions good discussion cover include:\ndone paper?\nmain points learn world?\nweaknesses done?\nleft learn?\n\nquestions good discussion cover include:\ndone paper?\nmain points learn world?\nweaknesses done?\nleft learn?\ndone paper?main points learn world?weaknesses done?left learn?Details survey - [6 ‘Exceptional,’ 5 ‘Great,’ 4 ‘Good,’ 3 ‘issues,’ 2 ‘Many issues,’ 0 ‘Poor done’]\nworking link survey included appendix.\nScreenshots least survey questions included appendix.\nsurvey well put together.\nnumber length questions appropriate.\nquestion type potential answers appropriate.\nclear survey accomplish aims report.\nquestions flow appropriate way.\n\nworking link survey included appendix.Screenshots least survey questions included appendix.\nsurvey well put together.\nnumber length questions appropriate.\nquestion type potential answers appropriate.\nclear survey accomplish aims report.\nquestions flow appropriate way.\nsurvey well put together.number length questions appropriate.question type potential answers appropriate.clear survey accomplish aims report.questions flow appropriate way.simulation survey responses appropriate survey scenario - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor done’]\ndone reproducible way either contained separate R file, report appendix.\ndone reproducible way either contained separate R file, report appendix.Numbering - [2 ‘Yes,’ 0 ‘Poor done’]\nfigures, tables, equations, etc numbered referred text.\nfigures, tables, equations, etc numbered referred text.Proofreading - [2 ‘Yes,’ 0 ‘Poor done’]\naspects submission free noticeable typos.\naspects submission free noticeable typos.Graphs/tables/etc - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor done’]\nmust include graphs tables paper must high standard.\nmust well formatted camera-ready clear digestible.\nmust: 1) serve clear purpose; 2) fully self-contained appropriate use labels/explanations, etc; 3) appropriately sized coloured (appropriate significant figures case stats).\nmust include graphs tables paper must high standard.must well formatted camera-ready clear digestible.must: 1) serve clear purpose; 2) fully self-contained appropriate use labels/explanations, etc; 3) appropriately sized coloured (appropriate significant figures case stats).References - [4 ‘Perfect,’ 3 ‘One minor issue,’ 0 ‘Poor done’]\ndata/software/literature/etc appropriately noted cited.\nmust cite software software packages use.\nmust cite datasets use.\nmust cite literature refer (refer literature).\ntake small chunk code Stack Overflow add page comment next code.\ntake large chunk code cite fully.\n3 means one minor issue. one minor issue receives 0.\ndata/software/literature/etc appropriately noted cited.must cite software software packages use.must cite datasets use.must cite literature refer (refer literature).take small chunk code Stack Overflow add page comment next code.take large chunk code cite fully.3 means one minor issue. one minor issue receives 0.Reproducibility - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor done’]\npaper analysis must fully reproducible.\ndetailed README included.\ncode thoroughly documented.\nR project used. use setwd().\ncode must appropriately read data, prepare , create plots, conduct analysis, generate documents, etc. Seeds used needed.\nCode must preamble etc.\nmust appropriately document scripts someone coming follow .\nrepo must thoroughly organized contain extraneous files.\npaper analysis must fully reproducible.detailed README included.code thoroughly documented.R project used. use setwd().code must appropriately read data, prepare , create plots, conduct analysis, generate documents, etc. Seeds used needed.Code must preamble etc.must appropriately document scripts someone coming follow .repo must thoroughly organized contain extraneous files.General excellence - [3 ‘Exceptional,’ 2 ‘Wow,’ 1 ‘Huh, ’s interesting,’ 0 ‘None’]\nalways students excel way anticipated rubric. item accounts .\nalways students excel way anticipated rubric. item accounts .","code":""},{"path":"papers.html","id":"the-short-list","chapter":"B Papers","heading":"B.3 ‘The Short List’","text":"","code":""},{"path":"papers.html","id":"task-2","chapter":"B Papers","heading":"B.3.1 Task","text":"Working part small team 1-3 people, entirely reproducible way, please pick paper reproduce approved list write short paper telling story based . story talk (reproduced) findings, also (bit ‘meta’) learnt process.","code":""},{"path":"papers.html","id":"guidance-2","chapter":"B Papers","heading":"B.3.2 Guidance","text":"Working part team 1-3 people, prepare PDF R Markdown following features:\ntitle,\nauthor/s,\ndate,\nabstract,\nintroduction,\ndata,\nmodel,\nresults,\ndiscussion, \nreferences.\ntitle,author/s,date,abstract,introduction,data,model,results,discussion, andreferences.discussion section relevant section, please sure discuss ethics bias reference relevant literature.reproduce one following papers:\nBarari, Soubhik, Christopher Lucas, Kevin Munger, 2021, ‘Political Deepfake Videos Misinform Public, Fake Media,’ 13 January, https://osf.io/cdfh3/.\nCohn, Alain, Michel André Maréchal, David Tannenbaum, Christian Lukas Zünd, 2019, ‘Civic Honesty Around Globe.’\nLiran Einav, Amy Finkelstein, Tamar Oostrom, Abigail Ostriker, Heidi Williams, 2020, ‘Screening Selection: Case Mammograms,’ American Economic Review.\nPons, Vincent, 2018, ‘Five-Minute Discussion Change Mind? Countrywide Experiment Voter Choice France’ American Economic Review.\nAbramitzky, Ran, Leah Boustan, Katherine Eriksson, Stephanie Hao. “Discrimination Returns Cultural Assimilation Age Mass Migration.” AEA Papers Proceedings 110 (May 2020): 340–46. https://doi.org/10.1257/pandp.20201090.\nChen, Yan, Ming Jiang, Erin L. Krupka. “Hunger Gender Gap.” Experimental Economics 22, . 4 (December 2019): 885–917. https://doi.org/10.1007/s10683-018-9589-9.\nLise, Jeremy, Fabien Postel-Vinay. “Multidimensional Skills, Sorting, Human Capital Accumulation.” American Economic Review 110, . 8 (August 2020): 2328–76. https://doi.org/10.1257/aer.20162002.\nKolev, Julian, Yuly Fuentes-Medel, Fiona Murray. “Gender Differences Scientific Communication Impact Grant Funding Decisions.” AEA Papers Proceedings 110 (May 2020): 245–49. https://doi.org/10.1257/pandp.20201043.\nCowgill, Bo, Fabrizio Dell’Acqua, Sandra Matz. “Managerial Effects Algorithmic Fairness Activism.” AEA Papers Proceedings 110 (May 2020): 85–90. https://doi.org/10.1257/pandp.20201035.\nBarari, Soubhik, Christopher Lucas, Kevin Munger, 2021, ‘Political Deepfake Videos Misinform Public, Fake Media,’ 13 January, https://osf.io/cdfh3/.Cohn, Alain, Michel André Maréchal, David Tannenbaum, Christian Lukas Zünd, 2019, ‘Civic Honesty Around Globe.’Liran Einav, Amy Finkelstein, Tamar Oostrom, Abigail Ostriker, Heidi Williams, 2020, ‘Screening Selection: Case Mammograms,’ American Economic Review.Pons, Vincent, 2018, ‘Five-Minute Discussion Change Mind? Countrywide Experiment Voter Choice France’ American Economic Review.Abramitzky, Ran, Leah Boustan, Katherine Eriksson, Stephanie Hao. “Discrimination Returns Cultural Assimilation Age Mass Migration.” AEA Papers Proceedings 110 (May 2020): 340–46. https://doi.org/10.1257/pandp.20201090.Chen, Yan, Ming Jiang, Erin L. Krupka. “Hunger Gender Gap.” Experimental Economics 22, . 4 (December 2019): 885–917. https://doi.org/10.1007/s10683-018-9589-9.Lise, Jeremy, Fabien Postel-Vinay. “Multidimensional Skills, Sorting, Human Capital Accumulation.” American Economic Review 110, . 8 (August 2020): 2328–76. https://doi.org/10.1257/aer.20162002.Kolev, Julian, Yuly Fuentes-Medel, Fiona Murray. “Gender Differences Scientific Communication Impact Grant Funding Decisions.” AEA Papers Proceedings 110 (May 2020): 245–49. https://doi.org/10.1257/pandp.20201043.Cowgill, Bo, Fabrizio Dell’Acqua, Sandra Matz. “Managerial Effects Algorithmic Fairness Activism.” AEA Papers Proceedings 110 (May 2020): 85–90. https://doi.org/10.1257/pandp.20201035.follow lead author/s paper ’re reproducing, thoroughly think , discuss, done. Regardless particular model using, (possibly lack ) extent done paper, model must well explained, thoroughly justified, explained appropriate task hand, results must beautifully described.must include DAG (probably model section).must discussion power experimental design (probably data section)paper must well-written, draw relevant literature, show statistical skills explaining statistical concepts draw .welcome use appendices supporting, critical, material. discussion must include sub-sections focus three four interesting points, also sub-sections weaknesses next steps.report must provide link GitHub repo fully contains analysis. code must entirely reproducible, documented, readable. repo must well-organised appropriately use folders.graphs tables must incredibly high standard. Graphs tables well formatted report-ready. clean digestible. Furthermore, label describe table/figure.discuss dataset (data section) make sure discuss (least):\nkey features, strengths, weaknesses generally.\ndiscussion questionnaire - good bad ?\ndiscussion methodology including find people take survey; population, frame, sample ; sampling approach took trade-offs may ; non-response; cost.\ndiscussion intervention experimental design.\njust issues strong submissions consider. Show knowledge. becomes detailed push footnotes appendix.\nkey features, strengths, weaknesses generally.discussion questionnaire - good bad ?discussion methodology including find people take survey; population, frame, sample ; sampling approach took trade-offs may ; non-response; cost.discussion intervention experimental design.just issues strong submissions consider. Show knowledge. becomes detailed push footnotes appendix.discuss model (model section), must extremely careful spell statistical model using, defining explaining aspect important. (Bayesian model, discussion priors regularization almost always important.) mention software used run model. clear model convergence, model checks, diagnostic issues. sampling survey aspects discussed assert modelling decisions make? , becomes detailed push details footnotes appendix. original paper guide , ’ll likely need go well-beyond included.present model results, graphs, figures, etc, results section. section strictly relay results. Interpretation results conclusions drawn results left discussion section.discussion focus model results. Interpret explain mean. Put context. learn world understood model results? caveats apply? extent model represent small world large world (use language McElreath, Ch 2)? weaknesses opportunities future work? Additionally, reproduction include sub-section differences found difficulties .Check referenced everything. Strong submissions draw related literature discussion (sections) sure also reference . style references matter, provided consistent.team, via Quercus, submit PDF paper. , paper must link associated GitHub repo. must include R Markdown file produced PDF repo. must include R Markdown file produced PDF repo. repo must well-organized detailed README.good way work team split work, one person section. people sections rely data (analysis graphs) just simulate waiting person putting together data finish.expected submission well written able understood average reader say 538. means allowed use mathematical notation, must able explain plain English. Similarly, can (hint: ) use survey, sampling, observational, statistical terminology, need explain . work flow easy follow understand. communicate well, anyone university level able read report relay back methodology, overall results, findings, weaknesses next steps without confusion.Everyone team receives mark.evidence class assignment.","code":""},{"path":"papers.html","id":"check-offs-points-2","chapter":"B Papers","heading":"B.3.3 Check offs points","text":"just copy-pasted code original paper, instead used foundation work ?","code":""},{"path":"papers.html","id":"faq-2","chapter":"B Papers","heading":"B.3.4 FAQ","text":"stay group second paper? . ’re welcome change. However, ’s important don’t change second paper group Quercus - sure change third paper group.Can switch groups third paper? Yes.much write? students submit something 10--15-page range, ’s really . precise thorough.paper doesn’t DAG, ? need make DAG.","code":""},{"path":"papers.html","id":"rubric-2","chapter":"B Papers","heading":"B.3.5 Rubric","text":"Go/-go #1: R cited - [1 ‘Yes,’ 0 ‘’]\nreferred main content included reference list.\n, need continue marking, just give paper 0 overall.\nreferred main content included reference list., need continue marking, just give paper 0 overall.Title - [2 ‘Exceptional,’ 1 ‘Yes,’ 0 ‘Poor done’]\ninformative title included.\nTell reader story - don’t waste time.\nIdeally tell happens end story.\n‘Problem Set X’ informative title. evidence school paper.\ninformative title included.Tell reader story - don’t waste time.Ideally tell happens end story.‘Problem Set X’ informative title. evidence school paper.Author, date, repo - [2 ‘Yes,’ 0 ‘Poor done’]\nauthor, date submission, link GitHub repo clearly included. (later likely, necessarily, statement : ‘Code data supporting analysis available : LINK’).\nauthor, date submission, link GitHub repo clearly included. (later likely, necessarily, statement : ‘Code data supporting analysis available : LINK’).Abstract - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor done’]\nabstract included appropriately pitched general audience.\nabstract answers: 1) done, 2) found, 3) matters (high level).\nabstract longer four sentences need think lot whether long. may fine (always exceptions) probably good reason.\nabstract must tell reader top-level finding. one thing learn world paper?\nabstract included appropriately pitched general audience.abstract answers: 1) done, 2) found, 3) matters (high level).abstract longer four sentences need think lot whether long. may fine (always exceptions) probably good reason.abstract must tell reader top-level finding. one thing learn world paper?Introduction - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor done’]\nintroduction self-contained tells reader everything need know, including putting broader context.\nintroduction provide bit broader context motivate reader, well providing bit detail ’re interested , , found, ’s important, etc.\nreader able read introduction good idea research carried .\nrare tables figures introduction (always exceptions think deeply whether one).\nmust outline structure paper.\ninstance (just rough guide) introduction 10 page paper, probably 3 4 paragraphs, 10 per cent, depends specifics.\nintroduction self-contained tells reader everything need know, including putting broader context.introduction provide bit broader context motivate reader, well providing bit detail ’re interested , , found, ’s important, etc.reader able read introduction good idea research carried .rare tables figures introduction (always exceptions think deeply whether one).must outline structure paper.instance (just rough guide) introduction 10 page paper, probably 3 4 paragraphs, 10 per cent, depends specifics.Data - [10 ‘Exceptional,’ 8 ‘Great,’ 6 ‘Good,’ 4 ‘issues,’ 2 ‘Many issues,’ 0 ‘Poor done’]\nthoroughly discuss variables dataset use. similar nonetheless don’t use? construct variables combining various ones?\ndata look like?\nPlot actual data ’re using (close can get ).\nDiscuss plots features data.\njust issues strong submissions consider. Show knowledge. becomes detailed, push footnotes appendix.\n‘Exceptional’ means read submission learn something dataset don’t learn submission (within reasonable measure course).\nthoroughly discuss variables dataset use. similar nonetheless don’t use? construct variables combining various ones?data look like?Plot actual data ’re using (close can get ).Discuss plots features data.just issues strong submissions consider. Show knowledge. becomes detailed, push footnotes appendix.‘Exceptional’ means read submission learn something dataset don’t learn submission (within reasonable measure course).Model - [10 ‘Exceptional,’ 8 ‘Great,’ 6 ‘Good,’ 4 ‘issues,’ 2 ‘Many issues,’ 0 ‘Poor done’]\nmodel nicely written , well-explained, justified, appropriate.\ndiscuss model must extremely careful spell statistical model using defining explaining aspect important. Failure suggests don’t understand model.\nmodel appropriately complex - , simple, unnecessarily complicated.\nmodel well-defined variables correspond discussed data section.\nmodel needs written appropriate mathematical notation also plain English.\nEvery aspect notation must defined otherwise section can receive poor.\nmodel makes sense based substantive area, also form model.\nmodel Bayesian, priors need defined sensible.\nDiscussion needs occur around features enter model . instance, (just examples) use ages rather age-groups, province levels effect, gender categorical, etc?\ngeneral, order adequate, needs clear justification model situation.\nassumptions underpinning model clearly discussed.\nAlternative models, variants, must discussed strengths weaknesses made clear. model chosen?\nmention software used run model.\nevidence thought circumstances model may appropriate.\nevidence model validation checking, whether sample comparison straw man RMSE, test/training, appropriate sensitivity checks.\nclear model convergence, model checks, diagnostic issues, becomes detailed push appendix.\nGreat answers discuss things , aspects discussed data section assert modelling decisions make. becomes detailed push details footnotes appendix.\n, explain model going .\nmodel nicely written , well-explained, justified, appropriate.discuss model must extremely careful spell statistical model using defining explaining aspect important. Failure suggests don’t understand model.model appropriately complex - , simple, unnecessarily complicated.model well-defined variables correspond discussed data section.model needs written appropriate mathematical notation also plain English.Every aspect notation must defined otherwise section can receive poor.model makes sense based substantive area, also form model.model Bayesian, priors need defined sensible.Discussion needs occur around features enter model . instance, (just examples) use ages rather age-groups, province levels effect, gender categorical, etc?general, order adequate, needs clear justification model situation.assumptions underpinning model clearly discussed.Alternative models, variants, must discussed strengths weaknesses made clear. model chosen?mention software used run model.evidence thought circumstances model may appropriate.evidence model validation checking, whether sample comparison straw man RMSE, test/training, appropriate sensitivity checks.clear model convergence, model checks, diagnostic issues, becomes detailed push appendix.Great answers discuss things , aspects discussed data section assert modelling decisions make. becomes detailed push details footnotes appendix., explain model going .Results - [10 ‘Exceptional,’ 8 ‘Great,’ 6 ‘Good,’ 4 ‘issues,’ 2 ‘Many issues,’ 0 ‘Poor done’]\nResults likely require summary statistics, tables, graphs, images, possibly statistical analysis maps.\nclear, also text associated aspects.\nShow reader results plotting . Talk . Explain . said, section strictly relay results.\nResults likely require summary statistics, tables, graphs, images, possibly statistical analysis maps.clear, also text associated aspects.Show reader results plotting . Talk . Explain . said, section strictly relay results.Discussion - [10 ‘Exceptional,’ 8 ‘Great,’ 6 ‘Good,’ 4 ‘issues,’ 2 ‘Many issues,’ 0 ‘Poor done’]\nquestions good discussion cover include (sub-section something like half page page):\ndone paper?\nsomething learn world?\nanother thing learn world?\nweaknesses done?\nleft learn proceed future?\n\nquestions good discussion cover include (sub-section something like half page page):\ndone paper?\nsomething learn world?\nanother thing learn world?\nweaknesses done?\nleft learn proceed future?\ndone paper?something learn world?another thing learn world?weaknesses done?left learn proceed future?Numbering - [2 ‘Yes,’ 0 ‘Poor done’]\nfigures, tables, equations, etc numbered referred text.\nfigures, tables, equations, etc numbered referred text.Proofreading - [2 ‘Yes,’ 0 ‘Poor done’]\naspects submission free noticeable typos.\naspects submission free noticeable typos.Graphs/tables/etc - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor done’]\nmust include graphs tables paper must high standard.\nmust well formatted camera-ready clear digestible.\nmust: 1) serve clear purpose; 2) fully self-contained appropriate use labels/explanations, etc; 3) appropriately sized coloured (appropriate significant figures case stats).\nmust include graphs tables paper must high standard.must well formatted camera-ready clear digestible.must: 1) serve clear purpose; 2) fully self-contained appropriate use labels/explanations, etc; 3) appropriately sized coloured (appropriate significant figures case stats).References - [4 ‘Perfect,’ 3 ‘One minor issue,’ 0 ‘Poor done’]\ndata/software/literature/etc appropriately noted cited.\nmust cite software software packages use.\nmust cite datasets use.\nmust cite literature refer (refer literature).\ntake small chunk code Stack Overflow add page comment next code.\ntake large chunk code cite fully.\n3 means one minor issue. one minor issue receives 0.\ndata/software/literature/etc appropriately noted cited.must cite software software packages use.must cite datasets use.must cite literature refer (refer literature).take small chunk code Stack Overflow add page comment next code.take large chunk code cite fully.3 means one minor issue. one minor issue receives 0.Reproducibility - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor done’]\npaper analysis must fully reproducible.\ndetailed README included.\ncode thoroughly documented.\nR project used. use setwd().\ncode must appropriately read data, prepare , create plots, conduct analysis, generate documents, etc. Seeds used needed.\nCode must preamble etc.\nmust appropriately document scripts someone coming follow .\nrepo must thoroughly organized contain extraneous files.\npaper analysis must fully reproducible.detailed README included.code thoroughly documented.R project used. use setwd().code must appropriately read data, prepare , create plots, conduct analysis, generate documents, etc. Seeds used needed.Code must preamble etc.must appropriately document scripts someone coming follow .repo must thoroughly organized contain extraneous files.General excellence - [3 ‘Exceptional,’ 2 ‘Wow,’ 1 ‘Huh, ’s interesting,’ 0 ‘None’]\nalways students excel way anticipated rubric. item accounts .\nalways students excel way anticipated rubric. item accounts .","code":""},{"path":"papers.html","id":"two-cathedrals","chapter":"B Papers","heading":"B.4 ‘Two Cathedrals’","text":"","code":""},{"path":"papers.html","id":"task-3","chapter":"B Papers","heading":"B.4.1 Task","text":"Working individually, please conduct original research applies methods statistics question involves experiment.","code":""},{"path":"papers.html","id":"guidance-3","chapter":"B Papers","heading":"B.4.2 Guidance","text":"various options topics (pick one):Develop research question interest obtain create relevant dataset. option involves developing research question based interests, background, expertise. encourage take option, please discuss plans . one come ideas? One way question-driven, keep informal log small ideas, questions, puzzles, ’re reading working. Often, dwelling can manage find questions interest. Another way data-driven - try find interesting dataset work backward. Finally, yet another way, methods-driven - let’s say happen understand Gaussian processes, just apply expertise.Develop research question interest obtain create relevant dataset. option involves developing research question based interests, background, expertise. encourage take option, please discuss plans . one come ideas? One way question-driven, keep informal log small ideas, questions, puzzles, ’re reading working. Often, dwelling can manage find questions interest. Another way data-driven - try find interesting dataset work backward. Finally, yet another way, methods-driven - let’s say happen understand Gaussian processes, just apply expertise.replication exercise, sure use paper foundation rather ends--.replication exercise, sure use paper foundation rather ends--.know expectations now. need refresher review past problem sets. essentially:\nEverything entirely reproducible.\npaper must written R Markdown.\npaper must following sections:\nTitle, date, author, keywords, abstract, introduction, data, model, results, discussion, appendix (optional, supporting, critical, material), reference list.\n\npaper must well-written, draw relevant literature, show statistical skills explaining statistical concepts draw .\ndiscussion needs substantial. instance, paper 10 pages long discussion least 2.5 pages. discussion, paper must include subsections weaknesses next steps - must proportion.\nreport must provide link GitHub repo contains everything (apart raw data git ignored share). code must entirely reproducible, documented, readable. repo must well-organised appropriately use folders README files.\nknow expectations now. need refresher review past problem sets. essentially:Everything entirely reproducible.paper must written R Markdown.paper must following sections:\nTitle, date, author, keywords, abstract, introduction, data, model, results, discussion, appendix (optional, supporting, critical, material), reference list.\nTitle, date, author, keywords, abstract, introduction, data, model, results, discussion, appendix (optional, supporting, critical, material), reference list.paper must well-written, draw relevant literature, show statistical skills explaining statistical concepts draw .discussion needs substantial. instance, paper 10 pages long discussion least 2.5 pages. discussion, paper must include subsections weaknesses next steps - must proportion.report must provide link GitHub repo contains everything (apart raw data git ignored share). code must entirely reproducible, documented, readable. repo must well-organised appropriately use folders README files.","code":""},{"path":"papers.html","id":"peer-review-submission","chapter":"B Papers","heading":"B.4.3 Peer review submission","text":"expectations paper high. ’m excited read submit. help achieve standard, initial ‘submission’ can get comments feedback final, actual, submission.Submit initial materials peer-review.\nindividual, via Quercus, submit PDF rough draft Quercus.\nminimum must include:\ntop-matter (title, author (can use pseudonym want), date, keywords, abstract) completely filled .\nfully written Introduction section.\nindividual, via Quercus, submit PDF rough draft Quercus.minimum must include:top-matter (title, author (can use pseudonym want), date, keywords, abstract) completely filled .fully written Introduction section.sections must present paper, don’t filled (e.g. must ‘Data’ heading, don’t need content section).clear - fine later change aspect submit checkpoint.awarded one percentage point just submitting draft meets minimum.point get feedback work (make sure least started thinking project) welcome (, possible) include sections wish get feedback .extensions granted submission since following submission dependent date.","code":""},{"path":"papers.html","id":"conduct-peer-review","chapter":"B Papers","heading":"B.4.4 Conduct peer-review","text":"individual, randomly assigned handful rough drafts provide feedback. three days provide feedback peers.provide feedback one peer receive one percentage point, provide feedback two peers receive two percentage points, provide feedback three () peers receive full three percentage points.feedback must include least five comments (meaningful/useful bullet points). must well-written thoughtful.extensions granted submission since following submission dependent date.Please remember providing feedback help colleagues. comments professional kind. challenging receive criticism. Please remember goal help peers advance writing/analysis. feedback inappropriate standard receive 0.","code":""},{"path":"papers.html","id":"check-offs-points-3","chapter":"B Papers","heading":"B.4.5 Check offs points","text":"causal story, least sub-section discussion talks causality can’t speak , ?","code":""},{"path":"papers.html","id":"faq-3","chapter":"B Papers","heading":"B.4.6 FAQ","text":"Can work part team? . ’s important work entirely . really need work show job applications etc.much write? students submit something 10--16-pages main content, additional pages devoted appendices, ’s really . precise thorough.","code":""},{"path":"papers.html","id":"rubric-3","chapter":"B Papers","heading":"B.4.7 Rubric","text":"Go/-go #1: R cited - [1 ‘Yes,’ 0 ‘’]\nreferred main content included reference list.\n, need continue marking, just give paper 0 overall.\nreferred main content included reference list., need continue marking, just give paper 0 overall.Title - [2 ‘Exceptional,’ 1 ‘Yes,’ 0 ‘Poor done’]\ninformative title included.\nTell reader story - don’t waste time.\nIdeally tell happens end story.\n‘Problem Set X’ informative title. evidence school paper.\ninformative title included.Tell reader story - don’t waste time.Ideally tell happens end story.‘Problem Set X’ informative title. evidence school paper.Author, date, repo - [2 ‘Yes,’ 0 ‘Poor done’]\nauthor, date submission, link GitHub repo clearly included. (later likely, necessarily, statement : ‘Code data supporting analysis available : LINK’).\nauthor, date submission, link GitHub repo clearly included. (later likely, necessarily, statement : ‘Code data supporting analysis available : LINK’).Abstract - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor done’]\nabstract included appropriately pitched general audience.\nabstract answers: 1) done, 2) found, 3) matters (high level).\nabstract longer four sentences need think lot whether long. may fine (always exceptions) probably good reason.\nabstract must tell reader top-level finding. one thing learn world paper?\nabstract included appropriately pitched general audience.abstract answers: 1) done, 2) found, 3) matters (high level).abstract longer four sentences need think lot whether long. may fine (always exceptions) probably good reason.abstract must tell reader top-level finding. one thing learn world paper?Introduction - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor done’]\nintroduction self-contained tells reader everything need know, including putting broader context.\nintroduction provide bit broader context motivate reader, well providing bit detail ’re interested , , found, ’s important, etc.\nreader able read introduction good idea research carried .\nrare tables figures introduction (always exceptions think deeply whether one).\nmust outline structure paper.\ninstance (just rough guide) introduction 10 page paper, probably 3 4 paragraphs, 10 per cent, depends specifics.\nintroduction self-contained tells reader everything need know, including putting broader context.introduction provide bit broader context motivate reader, well providing bit detail ’re interested , , found, ’s important, etc.reader able read introduction good idea research carried .rare tables figures introduction (always exceptions think deeply whether one).must outline structure paper.instance (just rough guide) introduction 10 page paper, probably 3 4 paragraphs, 10 per cent, depends specifics.Data - [10 ‘Exceptional,’ 8 ‘Great,’ 6 ‘Good,’ 4 ‘issues,’ 2 ‘Many issues,’ 0 ‘Poor done’]\nthoroughly discuss variables dataset use. similar nonetheless don’t use? construct variables combining various ones?\ndata look like?\nPlot actual data ’re using (close can get ).\nDiscuss plots features data.\njust issues strong submissions consider. Show knowledge. becomes detailed, push footnotes appendix.\n‘Exceptional’ means read submission learn something dataset don’t learn submission (within reasonable measure course).\nthoroughly discuss variables dataset use. similar nonetheless don’t use? construct variables combining various ones?data look like?Plot actual data ’re using (close can get ).Discuss plots features data.just issues strong submissions consider. Show knowledge. becomes detailed, push footnotes appendix.‘Exceptional’ means read submission learn something dataset don’t learn submission (within reasonable measure course).Model - [10 ‘Exceptional,’ 8 ‘Great,’ 6 ‘Good,’ 4 ‘issues,’ 2 ‘Many issues,’ 0 ‘Poor done’]\nmodel nicely written , well-explained, justified, appropriate.\ndiscuss model must extremely careful spell statistical model using defining explaining aspect important. Failure suggests don’t understand model.\nmodel appropriately complex - , simple, unnecessarily complicated.\nmodel well-defined variables correspond discussed data section.\nmodel needs written appropriate mathematical notation also plain English.\nEvery aspect notation must defined otherwise section can receive poor.\nmodel makes sense based substantive area, also form model.\nmodel Bayesian, priors need defined sensible.\nDiscussion needs occur around features enter model . instance, (just examples) use ages rather age-groups, province levels effect, gender categorical, etc?\ngeneral, order adequate, needs clear justification model situation.\nassumptions underpinning model clearly discussed.\nAlternative models, variants, must discussed strengths weaknesses made clear. model chosen?\nmention software used run model.\nevidence thought circumstances model may appropriate.\nevidence model validation checking, whether sample comparison straw man RMSE, test/training, appropriate sensitivity checks.\nclear model convergence, model checks, diagnostic issues, becomes detailed push appendix.\nGreat answers discuss things , aspects discussed data section assert modelling decisions make. becomes detailed push details footnotes appendix.\n, explain model going .\nmodel nicely written , well-explained, justified, appropriate.discuss model must extremely careful spell statistical model using defining explaining aspect important. Failure suggests don’t understand model.model appropriately complex - , simple, unnecessarily complicated.model well-defined variables correspond discussed data section.model needs written appropriate mathematical notation also plain English.Every aspect notation must defined otherwise section can receive poor.model makes sense based substantive area, also form model.model Bayesian, priors need defined sensible.Discussion needs occur around features enter model . instance, (just examples) use ages rather age-groups, province levels effect, gender categorical, etc?general, order adequate, needs clear justification model situation.assumptions underpinning model clearly discussed.Alternative models, variants, must discussed strengths weaknesses made clear. model chosen?mention software used run model.evidence thought circumstances model may appropriate.evidence model validation checking, whether sample comparison straw man RMSE, test/training, appropriate sensitivity checks.clear model convergence, model checks, diagnostic issues, becomes detailed push appendix.Great answers discuss things , aspects discussed data section assert modelling decisions make. becomes detailed push details footnotes appendix., explain model going .Results - [10 ‘Exceptional,’ 8 ‘Great,’ 6 ‘Good,’ 4 ‘issues,’ 2 ‘Many issues,’ 0 ‘Poor done’]\nResults likely require summary statistics, tables, graphs, images, possibly statistical analysis maps.\nclear, also text associated aspects.\nShow reader results plotting . Talk . Explain . said, section strictly relay results.\nResults likely require summary statistics, tables, graphs, images, possibly statistical analysis maps.clear, also text associated aspects.Show reader results plotting . Talk . Explain . said, section strictly relay results.Discussion - [10 ‘Exceptional,’ 8 ‘Great,’ 6 ‘Good,’ 4 ‘issues,’ 2 ‘Many issues,’ 0 ‘Poor done’]\nquestions good discussion cover include (sub-section something like half page page):\ndone paper?\nsomething learn world?\nanother thing learn world?\nweaknesses done?\nleft learn proceed future?\n\nquestions good discussion cover include (sub-section something like half page page):\ndone paper?\nsomething learn world?\nanother thing learn world?\nweaknesses done?\nleft learn proceed future?\ndone paper?something learn world?another thing learn world?weaknesses done?left learn proceed future?Numbering - [2 ‘Yes,’ 0 ‘Poor done’]\nfigures, tables, equations, etc numbered referred text.\nfigures, tables, equations, etc numbered referred text.Proofreading - [2 ‘Yes,’ 0 ‘Poor done’]\naspects submission free noticeable typos.\naspects submission free noticeable typos.Graphs/tables/etc - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor done’]\nmust include graphs tables paper must high standard.\nmust well formatted camera-ready clear digestible.\nmust: 1) serve clear purpose; 2) fully self-contained appropriate use labels/explanations, etc; 3) appropriately sized coloured (appropriate significant figures case stats).\nmust include graphs tables paper must high standard.must well formatted camera-ready clear digestible.must: 1) serve clear purpose; 2) fully self-contained appropriate use labels/explanations, etc; 3) appropriately sized coloured (appropriate significant figures case stats).References - [4 ‘Perfect,’ 3 ‘One minor issue,’ 0 ‘Poor done’]\ndata/software/literature/etc appropriately noted cited.\nmust cite software software packages use.\nmust cite datasets use.\nmust cite literature refer (refer literature).\ntake small chunk code Stack Overflow add page comment next code.\ntake large chunk code cite fully.\n3 means one minor issue. one minor issue receives 0.\ndata/software/literature/etc appropriately noted cited.must cite software software packages use.must cite datasets use.must cite literature refer (refer literature).take small chunk code Stack Overflow add page comment next code.take large chunk code cite fully.3 means one minor issue. one minor issue receives 0.Reproducibility - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor done’]\npaper analysis must fully reproducible.\ndetailed README included.\ncode thoroughly documented.\nR project used. use setwd().\ncode must appropriately read data, prepare , create plots, conduct analysis, generate documents, etc. Seeds used needed.\nCode must preamble etc.\nmust appropriately document scripts someone coming follow .\nrepo must thoroughly organized contain extraneous files.\npaper analysis must fully reproducible.detailed README included.code thoroughly documented.R project used. use setwd().code must appropriately read data, prepare , create plots, conduct analysis, generate documents, etc. Seeds used needed.Code must preamble etc.must appropriately document scripts someone coming follow .repo must thoroughly organized contain extraneous files.Enhancements - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor done’]\npick least one following include enhance submission:\nDatasheets datasets (see: https://arxiv.org/abs/1803.09010) model cards models (see: https://arxiv.org/pdf/1810.03993.pdf).\nShiny application.\nR package.\nAPI model.\n\nalways students excel way anticipated rubric. item accounts .\npick least one following include enhance submission:\nDatasheets datasets (see: https://arxiv.org/abs/1803.09010) model cards models (see: https://arxiv.org/pdf/1810.03993.pdf).\nShiny application.\nR package.\nAPI model.\nDatasheets datasets (see: https://arxiv.org/abs/1803.09010) model cards models (see: https://arxiv.org/pdf/1810.03993.pdf).Shiny application.R package.API model.always students excel way anticipated rubric. item accounts .General excellence - [3 ‘Exceptional,’ 2 ‘Wow,’ 1 ‘Huh, ’s interesting,’ 0 ‘None’]\nalways students excel way anticipated rubric. item accounts .\nalways students excel way anticipated rubric. item accounts .","code":""},{"path":"papers.html","id":"previous-examples-1","chapter":"B Papers","heading":"B.4.8 Previous examples","text":"examples papers well past include : Amy Farrow, Laura Cline, Hong Shi, Jia Jia Ji, \nRachael Lam.","code":""},{"path":"papers.html","id":"a-proportional-response","chapter":"B Papers","heading":"B.5 ‘A Proportional Response’","text":"","code":""},{"path":"papers.html","id":"task-4","chapter":"B Papers","heading":"B.5.1 Task","text":"Working teams one four people, please consider scenario:‘employed junior statistician Petit Poll - Canadian polling company. Petit Poll contract Canadian political party provide monthly polling updates.’Working part small team 1-4 people, entirely reproducible way, please write short paper tells client story standing.","code":""},{"path":"papers.html","id":"recommended-steps","chapter":"B Papers","heading":"B.5.2 Recommended steps","text":"Please pick political party ‘working ,’ pick geographic focus: 1) overall election, 2) particular province, 3) specific riding.decide survey methodology (hint: p. 13 Wu & Thompson provides handy checklist). questions address include:population, frame, sample?sampling methods use (e.g. choose SRSWOR, stratified, etc). statistical properties method brings table (e.g. SRSWOR discuss Wu & Thompson, Theorem 2.2, etc, appropriate)?going reach desired respondents?much estimate cost?steps take deal non-response non-response affect survey?going protect respondent privacy?Remember consider context ‘client’ - instance, interested Alberta ridings: Bloc Québécois Conservatives? likely money spend - Liberals Greens?Develop survey platform introduced class. sure test . want test much possible, maybe even swap informally another group?Now release surveys (simulated) ‘field.’ Please simulating appropriate number responses survey R. Don’t forget simulate relation survey methodology proposed. Show results discuss ‘findings.’ Everything must entirely reproducible. may like consider linking survey ‘responses’ data census GSS.Use R Markdown write PDF report . Discuss results findings, survey design motivations, etc - . writing report eventually go ‘client,’ must set scene, use language demonstrates command statistical concepts brings reader along . sure include graphs tables reference discussion. sure clear weaknesses biases, opportunities future work.report must well written. allowed , , use mathematical notation, must explain plain english. Similarly, can, , use surveys/sampling/observational data terminology, , need explain .report must include least following aspects: title, date, authorship, non-technical executive summary, introduction, survey methodology, results, discussion, appendices detail survey, references. ‘client’ stats graduates working need impressed main content report, also people barely know average people need impressed also. report include non-technical executive summary. terms length, typically roughly 10 per cent report. detailed introduction, still high level.graphs must extremely high standard.Check referenced everything. Strong submissions draw related literature discussion sure also reference . style references matter, provided consistent.Via Quercus, submit link PDF report hosted GitHub. point introduction report, must provide link GitHub repo code used assignment lives (Hint: Comment. . Code.). entire workflow must entirely reproducible.Please sure include link survey report screenshots survey appendix report.evidence class assignment.","code":""},{"path":"papers.html","id":"check-offs-points-4","chapter":"B Papers","heading":"B.5.3 Check offs points","text":"","code":""},{"path":"papers.html","id":"faq-4","chapter":"B Papers","heading":"B.5.4 FAQ","text":"","code":""},{"path":"papers.html","id":"mr-willis-of-ohio","chapter":"B Papers","heading":"B.6 ‘Mr Willis of Ohio’","text":"","code":""},{"path":"papers.html","id":"task-5","chapter":"B Papers","heading":"B.6.1 Task","text":"Working teams one four people, entirely reproducible way, please use Canadian General Social Survey (GSS) regression model tell story.","code":""},{"path":"papers.html","id":"recommended-steps-1","chapter":"B Papers","heading":"B.6.2 Recommended steps","text":"Depending focus background, may like use Bayesian hierarchical model, regardless particular model use must well explained, thoroughly justified, appropriate task hand, results must beautifully described.may focus year, aspect, geography reasonable given focus constraints GSS. reminder, GSS ‘program designed series independent, annual, cross-sectional surveys, covering one topic -depth.’ please consider topic year.GSS available University Toronto students via library. order use need clean prepare . Code one year distributed alongside problem set discussed lectures.welcome simply use code year, topic year constrain focus. Naturally, welcome adapt code years. use code exactly must cite . adapt code don’t cite , MIT license, appropriate least mention acknowledge , depending close adaption .Using R Markdown, please write paper analysis compile PDF.paper must well-written, draw relevant literature, show statistical skills explaining statistical concepts draw .paper must following sections: title, name/s, date, abstract, introduction, data, model, results, discussion, references.welcome use appendices supporting, critical, material. discussion must include sub-sections weaknesses next steps.report must provide link GitHub repo fully contains analysis. code must entirely reproducible, documented, readable. repo must well-organised appropriately use folders.graphs tables must incredibly high standard. Graphs tables well formatted report-ready. clean digestible. Furthermore, label describe table/figure.discuss dataset (data section) make sure discuss (least):\nkey features, strengths, weaknesses generally.\ndiscussion questionnaire - good bad ?\ndiscussion methodology including find people take survey; population, frame, sample ; sampling approach took trade-offs may ; non-response; cost.\njust issues strong submissions consider. Show knowledge. becomes detailed push footnotes appendix.\nkey features, strengths, weaknesses generally.discussion questionnaire - good bad ?discussion methodology including find people take survey; population, frame, sample ; sampling approach took trade-offs may ; non-response; cost.just issues strong submissions consider. Show knowledge. becomes detailed push footnotes appendix.discuss model (model section), must extremely careful spell statistical model using, defining explaining aspect important. (Bayesian model, discussion priors regularization almost always important.) mention software used run model. clear model convergence, model checks, diagnostic issues. sampling survey aspects discussed assert modelling decisions make? , becomes detailed push details footnotes appendix.present model results, graphs, figures, etc, results section. section strictly relay results. Interpretation results conclusions drawn results left discussion section.discussion focus model results. Interpret explain mean. Put context. learn world understood model results? caveats apply? extent model represent small world large world (use language McElreath, Ch 2)? weaknesses opportunities future work?Check referenced everything. Strong submissions draw related literature discussion (sections) sure also reference . style references matter, provided consistent.team, via Quercus, submit PDF paper. , paper must link associated GitHub repo appendix. must include R Markdown file produced PDF repo.good way work team split work, one person section. people sections rely data (analysis graphs) just simulate waiting person putting together data finish.expected submission well written able understood average reader say 538. means allowed use mathematical notation, must able explain plain English. Similarly, can (hint: ) use survey, sampling, observational, statistical terminology, need explain . work flow easy follow understand. communicate well, anyone university level able read report relay back methodology, overall results, findings, weaknesses next steps without confusion.","code":""},{"path":"papers.html","id":"check-offs-points-5","chapter":"B Papers","heading":"B.6.3 Check offs points","text":"recommended (informally) proofread one another’s sections - exchange papers another group?Everyone team receives mark.evidence class assignment.","code":""},{"path":"papers.html","id":"faq-5","chapter":"B Papers","heading":"B.6.4 FAQ","text":"","code":""},{"path":"papers.html","id":"five-votes-down","chapter":"B Papers","heading":"B.7 ‘Five Votes Down’","text":"","code":""},{"path":"papers.html","id":"task-6","chapter":"B Papers","heading":"B.7.1 Task","text":"primary goal paper predict overall popular vote 2020 American presidential election using multilevel regression post-stratification.","code":""},{"path":"papers.html","id":"recommended-steps-2","chapter":"B Papers","heading":"B.7.2 Recommended steps","text":"expect work part group 4 people, groups size 1-4 fine. suggested split work based 4-person group, just suggestions.Individual-level survey data:\nRequest access Democracy Fund + UCLA Nationscape ‘Full Data Set’: https://www.voterstudygroup.org/publication/nationscape-data-set. take day two. Please start early.\nGiven expense collecting data, privilege access , don’t properly cite dataset get zero problem set.\naccess pick survey interest. use “ns20200102.dta” example (number may different).\nlarge file share. push GitHub (use .gitignore file - see : https://carpentries-incubator.github.io/git-Rstudio-course/02-ignore/index.html).\nUse example R code get started preparing dataset, go cleaning preparing based need.\nMake graphs tables survey data write beautiful sentences paragraphs explaining everything.\nRequest access Democracy Fund + UCLA Nationscape ‘Full Data Set’: https://www.voterstudygroup.org/publication/nationscape-data-set. take day two. Please start early.Given expense collecting data, privilege access , don’t properly cite dataset get zero problem set.access pick survey interest. use “ns20200102.dta” example (number may different).large file share. push GitHub (use .gitignore file - see : https://carpentries-incubator.github.io/git-Rstudio-course/02-ignore/index.html).Use example R code get started preparing dataset, go cleaning preparing based need.Make graphs tables survey data write beautiful sentences paragraphs explaining everything.Post-stratification data:\nuse American Community Surveys (ACS).\nPlease create account IPUMS: https://usa.ipums.org/usa/index.shtml\nwant 2018 1-year ACS. need select variables. depend want model survey data, options include: REGION, STATEICP, AGE, SEX, MARST, RACE, HISPAN, BPL, CITIZEN, EDUC, LABFORCE, INCTOT. look around see interested , remembering need establish correspondence survey.\nDownload relevant post-stratification data (’s probably easiest change data format .dta). , can take time. Please start early.\nlarge file share. push GitHub (use .gitignore file - see : https://carpentries-incubator.github.io/git-Rstudio-course/02-ignore/index.html).\nGiven expense collecting data, privilege access , don’t properly cite dataset get zero problem set.\nClean prepare post-stratification dataset.\nRemember need cell counts sub-populations model. See examples readings.\nuse American Community Surveys (ACS).Please create account IPUMS: https://usa.ipums.org/usa/index.shtmlYou want 2018 1-year ACS. need select variables. depend want model survey data, options include: REGION, STATEICP, AGE, SEX, MARST, RACE, HISPAN, BPL, CITIZEN, EDUC, LABFORCE, INCTOT. look around see interested , remembering need establish correspondence survey.Download relevant post-stratification data (’s probably easiest change data format .dta). , can take time. Please start early.large file share. push GitHub (use .gitignore file - see : https://carpentries-incubator.github.io/git-Rstudio-course/02-ignore/index.html).Given expense collecting data, privilege access , don’t properly cite dataset get zero problem set.Clean prepare post-stratification dataset.Remember need cell counts sub-populations model. See examples readings.(may efficient start simulated data waiting real data) Modelling.\nwant explain vote intention based variety explanatory variables. Construct vote intention variable binary (either ‘supports Trump’ ‘supports Biden’).\nwelcome use lm() need explain nuances decision model section (Hint: start : https://statmodeling.stat.columbia.edu/2020/01/10/linear--logistic-regression--binary-outcomes/).\nsaid, probably use logistic regression possible . don’t know start look (increasing levels complexity) glm(), lme4::glmer(), brms::brm(). examples readings.\nThink deeply model fit, diagnostics, similar things need order convince someone model appropriate.\nflexibility model use, (hence cells ’ll need create next). general, cells better, may want fewer cells simplicity writing process ensure decent sample cell.\nApply trained model post-stratification dataset make best estimate election result can. specifics depend modelling approach likely involve predict(), add_predicted_draws(), similar. See examples readings. primarily interested distribution forecast overall Presidential popular vote, explanatory variables affect . great submissions go beyond . Also, ’re taking statistics course, just gave central estimate nothing else, great.\nCreate beautiful graphs tables model results.\nCreate wonderful paragraphs talking explaining everything.\nwant explain vote intention based variety explanatory variables. Construct vote intention variable binary (either ‘supports Trump’ ‘supports Biden’).welcome use lm() need explain nuances decision model section (Hint: start : https://statmodeling.stat.columbia.edu/2020/01/10/linear--logistic-regression--binary-outcomes/).said, probably use logistic regression possible . don’t know start look (increasing levels complexity) glm(), lme4::glmer(), brms::brm(). examples readings.Think deeply model fit, diagnostics, similar things need order convince someone model appropriate.flexibility model use, (hence cells ’ll need create next). general, cells better, may want fewer cells simplicity writing process ensure decent sample cell.Apply trained model post-stratification dataset make best estimate election result can. specifics depend modelling approach likely involve predict(), add_predicted_draws(), similar. See examples readings. primarily interested distribution forecast overall Presidential popular vote, explanatory variables affect . great submissions go beyond . Also, ’re taking statistics course, just gave central estimate nothing else, great.Create beautiful graphs tables model results.Create wonderful paragraphs talking explaining everything.(, ’s probably efficient start simulated data/results waiting)\nWrite .\nUsing R Markdown, please write thorough paper analysis compile PDF.\npaper must well-written, draw relevant literature, show statistical skills explaining statistical concepts draw .\npaper must following sections: title, name/s, date, abstract keywords, introduction, data, model, results, discussion, references.\npaper may use appendices supporting, critical, material.\ndiscussion needs substantial. instance, paper 10 pages long discussion least 2.5 pages. discussion, paper must include subsections weaknesses next steps - must proportion.\nreport must provide link GitHub repo contains everything (apart raw data git ignored share). code must entirely reproducible, documented, readable. repo must well-organised appropriately use folders README files.\ngraphs tables must incredibly high standard, well formatted, report-ready. clean digestible. Furthermore, label describe table/figure.\ndiscuss datasets (data section) (remember least two datasets discuss) make sure discuss (least):\nkey features, strengths, weaknesses generally.\nsurvey questionnaire - good bad ?\ndiscussion methodology including find people take survey; population, frame, sample ; sampling approach took trade-offs may ; non-response; cost.\njust issues strong submissions consider. Show knowledge. becomes detailed push footnotes appendix.\n\ndataset section probably appropriate place include explanation post-stratification (non-statistical language) strengths weaknesses , although discussion may fit naturally another section. Regardless, sure justify inclusion explanatory variable.\ndiscuss model (model section), must extremely careful spell statistical model using, defining explaining aspect important. (Bayesian model, discussion priors regularization almost always important.) mention software used run model. clear model convergence, model checks, diagnostic issues, although may push details appendix depending detailed get. sampling survey aspects discussed assert modelling decisions make? can convince reader ’ve neither overfit underfit data? , becomes detailed push details footnotes appendix.\npresent model results, graphs, figures, etc, results section. section strictly relay results. must include text explaining summary statistics similar. However, interpretation results conclusions drawn results left discussion section.\ndiscussion focus model results, time interpreting , explaining mean. Put context. learn world understood model results? caveats apply? extent model represent small world large world (use language McElreath, Ch 2)? weaknesses opportunities future work? going win election? confident forecast? small large distribution? mean? confident certain states? certain explanatory variables carry weight others? Etc.\nCheck referenced everything. Strong submissions draw related literature discussion (sections) sure also reference . style references matter, must consistent.\ndon’t cite R get zero problem set.\nteam, via Quercus, submit PDF paper. , paper must link associated GitHub repo. must include R Markdown file produced PDF repo.\nRMarkdown file must exactly produce PDF. Don’t edit manually ex post - isn’t reproducible.\nWrite .Using R Markdown, please write thorough paper analysis compile PDF.paper must well-written, draw relevant literature, show statistical skills explaining statistical concepts draw .paper must following sections: title, name/s, date, abstract keywords, introduction, data, model, results, discussion, references.paper may use appendices supporting, critical, material.discussion needs substantial. instance, paper 10 pages long discussion least 2.5 pages. discussion, paper must include subsections weaknesses next steps - must proportion.report must provide link GitHub repo contains everything (apart raw data git ignored share). code must entirely reproducible, documented, readable. repo must well-organised appropriately use folders README files.graphs tables must incredibly high standard, well formatted, report-ready. clean digestible. Furthermore, label describe table/figure.discuss datasets (data section) (remember least two datasets discuss) make sure discuss (least):\nkey features, strengths, weaknesses generally.\nsurvey questionnaire - good bad ?\ndiscussion methodology including find people take survey; population, frame, sample ; sampling approach took trade-offs may ; non-response; cost.\njust issues strong submissions consider. Show knowledge. becomes detailed push footnotes appendix.\nkey features, strengths, weaknesses generally.survey questionnaire - good bad ?discussion methodology including find people take survey; population, frame, sample ; sampling approach took trade-offs may ; non-response; cost.just issues strong submissions consider. Show knowledge. becomes detailed push footnotes appendix.dataset section probably appropriate place include explanation post-stratification (non-statistical language) strengths weaknesses , although discussion may fit naturally another section. Regardless, sure justify inclusion explanatory variable.discuss model (model section), must extremely careful spell statistical model using, defining explaining aspect important. (Bayesian model, discussion priors regularization almost always important.) mention software used run model. clear model convergence, model checks, diagnostic issues, although may push details appendix depending detailed get. sampling survey aspects discussed assert modelling decisions make? can convince reader ’ve neither overfit underfit data? , becomes detailed push details footnotes appendix.present model results, graphs, figures, etc, results section. section strictly relay results. must include text explaining summary statistics similar. However, interpretation results conclusions drawn results left discussion section.discussion focus model results, time interpreting , explaining mean. Put context. learn world understood model results? caveats apply? extent model represent small world large world (use language McElreath, Ch 2)? weaknesses opportunities future work? going win election? confident forecast? small large distribution? mean? confident certain states? certain explanatory variables carry weight others? Etc.Check referenced everything. Strong submissions draw related literature discussion (sections) sure also reference . style references matter, must consistent.don’t cite R get zero problem set.team, via Quercus, submit PDF paper. , paper must link associated GitHub repo. must include R Markdown file produced PDF repo.\nRMarkdown file must exactly produce PDF. Don’t edit manually ex post - isn’t reproducible.good way work team split work, one person section. people sections rely data (analysis graphs) just simulate waiting person putting together data finish. recommended split , works .","code":""},{"path":"papers.html","id":"check-offs-points-6","chapter":"B Papers","heading":"B.7.3 Check offs points","text":"expected submission well written able understood average reader say 538. means allowed use mathematical notation, must able explain plain English. Similarly, can (hint: ) use survey, sampling, observational, statistical terminology, need explain . average person doesn’t know p-value confidence interval . need explain plain language first time use . work flow easy follow understand. communicate well, anyone university level able read report relay back methodology, overall results, findings, weaknesses next steps without confusion.recommended (informally) proofread one another’s work - exchange papers another group?Everyone team receives mark.evidence class assignment.","code":""},{"path":"papers.html","id":"faq-6","chapter":"B Papers","heading":"B.7.4 FAQ","text":"","code":""},{"path":"papers.html","id":"whats-next","chapter":"B Papers","heading":"B.8 ‘What’s next?’","text":"","code":""},{"path":"papers.html","id":"task-7","chapter":"B Papers","heading":"B.8.1 Task","text":"Please work individually. paper, conduct original research applies methods statistics question involving surveys, sampling observational data.","code":""},{"path":"papers.html","id":"recommended-steps-3","chapter":"B Papers","heading":"B.8.2 Recommended steps","text":"various options topics (pick one):\nDevelop research question interest obtain create relevant dataset. option involves developing research question based interests, background, expertise. encourage take option, please discuss plans . one come ideas? One way question-driven, keep informal log small ideas, questions, puzzles, ’re reading working. Often, dwelling can manage find questions interest. Another way data-driven - try find interesting dataset work backward. Finally, yet another way, methods-driven - let’s say happen understand Gaussian processes, just apply expertise.\n(Thanks Jack Bailey idea.) Build MRP model based CES post-stratification dataset obtain, identify 2019 Canadian Federal Election different ‘everyone’ voted. learn importance turnout based model results? (option involves logistic regression either frequentist Bayesian settings.)\nReproduce paper. means download data write code (using code paper guide ’s available) try get results write found. Options include:\nAngelucci, Charles, Julia Cagé, 2019, ‘Newspapers times low advertising revenues,’ American Economic Journal: Microeconomics, please see: https://www.openicpsr.org/openicpsr/project/116438/version/V1/view. (option can accomplished just OLS. ‘safe’ pick. even already provided code class get started - see notes! ).\nBailey, Michael ., Daniel J. Hopkins & Todd Rogers, 2016, ‘Unresponsive Unpersuaded: Unintended Consequences Voter Persuasion Effort,’ Political Behavior.\nClark, Sam, 2019, ‘General Age-Specific Mortality Model Example Indexed Child Mortality Child Adult Mortality,’ Demography, please see: https://github.com/sinafala/svd-comp. (ambiguous pick!)\nSkinner, Ben, 2019, ‘Making connection: Broadband access online course enrollment public open admissions institutions,’ Research Higher Education, please see: https://github.com/btskinner/oa_online_broadband_rep.\nPons, Vincent, 2018, ‘Five-Minute Discussion Change Mind? Countrywide Experiment Voter Choice France’ American Economic Review.\nValencia Caicedo, Felipe, 2019, ‘Mission: Human Capital Transmission, Economic Persistence, Culture South America,’ Quarterly Journal Economics, please see: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ML1155.\nfavourite paper want reproduce , please let know end Week 12 can check ’s appropriate.\nPretend work Upworthy. Request Upworthy dataset use evaluate result /B test. request take week. Please plan ahead choose option.\nCritique following paper: AlShebli, Bedoor, Kinga Makovi & Talal Rahwan, 2020, ‘association early career informal mentorship academic collaborations junior author performance,’ Nature Communications. able download data : https://github.com/bedoor/Mentorship paper : https://www.nature.com/articles/s41467-020-19723-8. background starting points critique, please see: https://statmodeling.stat.columbia.edu/2020/11/19/-female-scientists-worse-mentors--study-pretends--know/ https://danieleweeks.github.io/Mentorship/#summary. (option involves extensive data exploration thinking really hard trying .)\nUse post Andrew Whitby - https://andrewwhitby.com/2020/11/24/contact-tracing-biased/ - starting point explore biased sampling effects know COVID affects public policy. (option involves extensive simulation.)\n‘Bias Behind Bars,’ data journalist Tom Cardoso finds ‘()fter controlling number variables, … Black Indigenous inmates likely get worse scores white inmates, based solely race.’\nmain story : https://www.theglobeandmail.com/canada/article-investigation-racial-bias--canadian-prison-risk-assessments/\nmethodology discussion : https://www.theglobeandmail.com/canada/article-investigation-racial-bias--canadian-prisons-methodology/\nobservational data available : https://www.theglobeandmail.com/files/editorial/News/nw-na-risk-1023/The_Globe_and_Mail_CSC_OMS_2012-2018_20201022235635.zip\ntask follow methodology Tom published attempt replicate results. able replicate ? results change significantly slightly different assumptions? (option involves frequentist logistic regression, although everything Bayesian setting lovely ).\nknow expectations now. need refresher review past problem sets.\nEverything entirely reproducible.\npaper must written R Markdown.\npaper must following sections:\nTitle, date, author, keywords, abstract, introduction, data, model, results, discussion, appendix (optional, supporting, critical, material), reference list.\npaper must well-written, draw relevant literature, show statistical skills explaining statistical concepts draw .\ndiscussion needs substantial. instance, paper 10 pages long discussion least 2.5 pages. discussion, paper must include subsections weaknesses next steps - must proportion.\nreport must provide link GitHub repo contains everything (apart raw data git ignored share). code must entirely reproducible, documented, readable. repo must well-organised appropriately use folders README files.\nexpectations paper high. ’m excited read submit. help achieve standard, two initial ‘submissions’ can get comments feedback final, actual, submission.\nDue dates:\n(Optional) December 9 11:59pm ET\nSubmit initial materials peer-review.\nindividual, via Quercus, submit PDF rough draft Quercus 11:59pm ET Wednesday, December 9, 2020.\nminimum must include:\ntop-matter (title, author (can use pseudonym want), date, keywords, abstract) completely filled .\nfully written Introduction section.\nsections must present paper, don’t filled (e.g. must ‘Data’ heading, don’t need content section).\nclear - fine later change aspect submit checkpoint.\nawarded 1 percentage point just submitting draft meets minimum (1 30 available final paper). don’t submit, percentage point pushed part d).\npoint get feedback work (make sure least started thinking project) welcome include sections wish get feedback .\nextensions granted submission since following submission dependent date.\n(Optional) December 12 11:59pm ET\nConduct peer-review.\nindividual, December 10, randomly assigned handful rough drafts provide feedback. December 12, 2020 11:59pm ET provide feedback peers.\nprovide feedback one peer receive 1 percentage point, provide feedback two peers receive 2 percentage points, provide feedback three () peers receive full 3 percentage points.\nmay complete aspect whether submitted something part ). don’t complete percentage points pushed part d).\nfeedback must include least six comments (meaningful/useful bullet points). must well-written thoughtful.\nextensions granted submission since following submission dependent date.\nPlease remember providing feedback help colleagues. comments professional kind. challenging receive criticism. Please remember goal help peers advance writing/analysis. feedback inappropriate standard receive 0 redeemed later.\n(Optional) December 16 11:59pm ET\nSubmit materials TA review.\nSubmit PDF Quercus. TA provide high-level comments December 17.\nminimum must include:\ntop-matter.\nFully written Introduction, Data, Model, Results sections.\nsections must present paper, don’t filled (e.g. must ‘Discussion’ heading, don’t need content section).\nclear - fine later change aspect submit checkpoint.\nreceive 1 percentage point submitting something meets minimum. don’t submit anything pushed final paper.\nextensions possible aspect.\n(Compulsory) December 20 11:59pm ET\nindividual, via Quercus, submit PDF paper. , paper, must link associated GitHub repo.\nsubmission graded based rubric posted Quercus worth 25-30 percentage points depending parts ) - c).","code":""},{"path":"papers.html","id":"check-offs-points-7","chapter":"B Papers","heading":"B.8.3 Check offs points","text":"","code":""},{"path":"papers.html","id":"faq-7","chapter":"B Papers","heading":"B.8.4 FAQ","text":"submit initial paper order peer-review? Yes.","code":""},{"path":"references-1.html","id":"references-1","chapter":"References","heading":"References","text":"","code":""}]
