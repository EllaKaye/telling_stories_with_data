[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"\nFigure 0.1: Telling stories data\nbook help tell stories data. establishes foundation can build share knowledge aspect world interest based data observe. Telling stories small groups around fire played critical role development humans society (Wiessner 2014). Today stories, based data, can influence millions.book explore, prod, push, manipulate, knead, ultimately, try understand implications data. variety features drive choices book.motto university took PhD naturam primum cognoscere rerum roughly ‘learn first nature things.’ original quote continues temporis aeterni quoniam, roughly ‘eternal time.’ things. focus tools, approaches, workflows enable establish lasting reproducible knowledge.talk data book, typically related humans. Humans centre stories, tell social, cultural, economic stories. particular, throughout book draw attention inequity social phenomena data. data analysis reflects world . Many least well-face double burden regard: disadvantaged, extent difficult measure. Respecting whose data dataset primary concern, thinking systematically dataset.data often specific various contexts disciplines, approaches used understand tend similar. Data also increasingly global resources opportunities available variety sources. Hence, draw examples many disciplines geographies.become knowledge, findings must communicated , understood, trusted people. Scientific economic progress can made building work others. possible can understand . Similarly, create knowledge world, must enable others understand precisely , found, went tasks. , book particularly prescriptive communication reproducibility.Improving quality quantitative work enormous challenge, yet challenge time. Data around us, little enduring knowledge created. book hopes contribute, small way, changing .","code":""},{"path":"index.html","id":"audience-and-assumed-background","chapter":"Preface","heading":"Audience and assumed background","text":"typical person reading book familiarity first-year statistics, instance run regression. targeted particular level, instead providing aspects relevant almost quantitative course. taught book high school, undergraduate, graduate levels. Everyone unique needs, hopefully aspect book speaks .book especially complements Statistical Rethinking (McElreath 2020), R Data Science (Wickham Grolemund 2017), Introduction Statistical Learning (James et al. 2017), Causal Inference: Mixtape (Cunningham 2021), Building Software Together (Wilson 2021). instance, book may interested learning Bayesian statistics, data science, statistical learning, causal inference, building software.said, successful students quantitative coding background. Enthusiasm interest taken folks far. , don’t worry much else.","code":""},{"path":"index.html","id":"structure-and-content","chapter":"Preface","heading":"Structure and content","text":"book structured around six parts: ) Foundations, II) Communication, III) Acquisition, IV) Preparation, V) Modelling, VI) Enrichment.Part – Foundations – begins Chapter 1, provides overview trying achieve book read . Chapter 2 provides worked examples. intention can experience full workflow recommended book without worrying much specifics happening. workflow : plan, simulate, acquire, model, communicate. normal follow everything chapter, go , typing executing code . time read one chapter book, recommend one. Chapter 3 goes essential tasks R, statistical programming language used book. reference chapter, may find returning time time. Chapter 4 introduces key tools reproducibility used workflow advocate. things like using command line, R Markdown, R Projects, Git GitHub, using R practice.Part II – Communication – considers three types communication: written, static, interactive. Chapter 5 details features quantitative writing go writing crisp, technical, paper. Static communication Chapter 6 introduces features like graphs, tables, maps. Interactive communication Chapter 7 covers aspects websites, web applications, maps can manipulated.Part III – Acquisition – focuses three aspects: gathering data, hunting data, farming data. Gathering data Chapter 8 covers things like using Application Programming Interface (APIs), scraping data, getting data PDFs, Optical Character Recognition (OCR). idea data available, necessarily designed datasets, must go get . Hunting data Chapter 9 covers aspects expected us. instance, may need conduct experiment, run /B test, surveys. Finally, farming data Chapter 10 covers datasets explicitly provided us use data, instance censuses government statistics. typically clean, pre-packaged datasets.Part IV – Preparation – covers respectfully transform raw data something can explored shared. Chapter 11 begins detailing principles follow approaching task cleaning preparing data, goes specific steps take checks implement. Chapter 12 focuses methods storing retrieving datasets, including use R packages. Chapter 13 discusses considerations steps take wanting disseminate datasets broadly possible, time respecting whose data based .Part V – Modelling – begins exploratory data analysis Chapter 14. critical process coming understand nature dataset, something typically finds final product. Chapter 15 use statistical models explore data introduced. Chapter 16 first three applications modelling. focuses attempts make causal claims observational data covers approaches difference--differences, regression discontinuity, instrumental variables. Chapter 17 second modelling applications chapters focuses multilevel regression post-stratification use statistical model adjust sample known biases. Chapter 18 third final modelling application focused text--data.Part VI – Enrichment – introduces various next steps improve aspects workflow approaches introduced previous chapters. Chapter 19 goes moving away computer toward using cloud. Chapter 20 discusses deploying models use packages, web applications, APIs. Chapter 21 discusses various alternatives storage data including feather SQL; also covers ways improve performance code. Finally, Chapter 22 offers concluding remarks, details open problems, suggests next steps.","code":""},{"path":"index.html","id":"pedagogy-and-key-features","chapter":"Preface","heading":"Pedagogy and key features","text":"work. actively go material code . S. King (2000) says ‘[]mateurs sit wait inspiration, rest us just get go work.’ passively read book. role best described Hamming (1996, 2–3):, , coach. run mile ; best can discuss styles criticize . know must run mile athletics course benefit —hence must think carefully hear read book effective changing —must obviously purpose…book structured around dense 12-week course. provides enough material advanced readers challenged, establishing core readers master. Typically courses cover material Chapter 15, pick another couple chapters particular interest.early Chapter 2 workflow—plan, simulate, acquire, model, communicate—allowing tell convincing story data. subsequent chapter add aspects depth workflow allow speak increasing sophistication credibility. workflow expands addresses skills typically sought industry. instance, features : communication, ethics, reproducibility, research question development, data collection, data cleaning, data protection dissemination, exploratory data analysis, statistical modelling, scaling.One defining aspects book ethics inequity concerns integrated throughout, rather clustered one, easily ignorable, chapter. aspects critical, yet can difficult immediately see value, hence tight integration.book also designed enable build portfolio work show potential employer. want industry job, arguably important thing . E. Robinson Nolis (2020, 55) describe portfolio collection projects show can something can help successful job search.novel Last Samurai (DeWitt 2000, 326), character says:[] scholar able look word passage instantly think another passage occurred; … [] text like pack icebergs word snowy peak huge frozen mass cross-references beneath surface.analogous way, book provides text instruction self-contained, also helps develop critical masses knowledge expertise built. chapter positions last word, instead written relation work.chapter following features:list required materials go read chapter. clear, first read material return book. chapter also contains recommended materials particularly interested topic want starting place exploration.summary key concepts skills developed chapter. Technical chapters additionally contain list main packages functions used chapter. combination features acts checklist learning, return completing chapter.series short exercises complete going required materials, going chapter, test knowledge. completing chapter, go back exercises make sure understand aspect.One two tutorial questions included end chapter encourage actively engage material. consider forming small groups discuss answers questions.chapters additionally feature:section called ‘Oh, think good data !’ focuses particular setting, cause death, often assumed unimpeachable unambiguous data reality tends quite far .section called ‘Shoulders giants,’ focuses created intellectual foundation build.Finally, set six papers included Appendix B. write , conducting original research topic interest . Although open-ended research may new , extent able : develop questions, use quantitative methods explore , communicates findings, measure success book.","code":""},{"path":"index.html","id":"software-information-and-conventions","chapter":"Preface","heading":"Software information and conventions","text":"software use book R (R Core Team 2021). language chosen open source, widely used, general enough cover entire workflow, yet specific enough plenty well-developed features. assume used R , another reason selecting R book community R users. community especially welcoming new-comers lot complementary beginner-friendly material available. R package, DoSSToolkit (R. Alexander et al. 2021), contains learnr modules (Schloerke et al. 2021). may useful newer R especially complementary book.don’t programming language, R great one start . preferred programming language already, wouldn’t hurt pick R well. said, good reason prefer another open-source programming language (instance use Python daily work) may wish stick . However, examples book R.Please download R R Studio onto computer. can download R free : http://cran.utstat.utoronto.ca/, can download R Studio Desktop free : https://rstudio.com/products/rstudio/download/#download.\nPlease also create account R Studio Cloud: https://rstudio.cloud/. allow run R cloud.Packages typewriter text, instance, tidyverse, functions also typewriter text, include brackets, instance dplyr::filter().","code":""},{"path":"index.html","id":"acknowledgments","chapter":"Preface","heading":"Acknowledgments","text":"Many people generously gave code, data, examples, guidance, opportunities, thoughts, time, helped develop book.Thank David Grubbs team CRC Press taking chance providing invaluable support.Thank Michael Chong Sharla Gelfand greatly helping shape approaches advocate. However, much contribute enormous way spirit generosity characterises R community.Thank Kelly Lyons support, guidance, mentorship, friendship. Every day demonstrates academic , broadly, aspire person.Thank Greg Wilson providing structure think teaching, catalyst book, helpful comments drafts. Every day provides example contribute intellectual community.Thank anonymous reviewer Isabella Ghement, thoroughly went early draft book provided detailed feedback improved book.Thank Hareem Naveed helpful feedback encouragement. industry experience invaluable resource grappled questions coverage focus.Thank Leslie Root, came idea around ‘Oh, think good data !’Thank PhD supervisory panel John Tang, Martine Mariotti, Tim Hatton, Zach Ward gave freedom explore intellectual space interest , support follow interests, guidance ensure resulted something tangible.Thank Elle Côtè enabling book written.book greatly benefited notes teaching materials others freely available online, especially: Chris Bail, Scott Cunningham, Andrew Heiss, Lisa Lendway, Grant McDermott, Nathan Matias, David Mimno, Ed Rubin. Thank folks. changed norm academics making materials freely available online great one one hope free online version book helps contribute .Aspects rubrics used Appendix B based rubrics developed Lisa Romkey Alan Chong. grateful allowed build foundation.Thank students contributed substantially development book, including: Mahfouz, Faria Khandaker, Keli Chiu, Paul Hodgetts, Thomas William Rosenthal. discussed aspects book , made specific contributions, also changed sharpened way thought almost everything covered . Paul additionally made art book.Thank students identified specific improvements, including: Aaron Miller, Amy Farrow, Arsh Lakhanpal, Cesar Villarreal Guzman, Flavia López, Hong Shi, Laura Cline, Lorena Almaraz De La Garza, Mounica Thanam, Reem Alasadi, Wijdan Tariq, Yang Wu, Yewon Han.Finally, thank Monica Alexander. Without written book; even thought possible. Thank inestimable help writing book, providing base builds (remember library showing many times get certain rows R!), giving time needed write, encouragement turned writing book just meant endlessly re-writing perfect day , reading everything book many times, providing perfect hydration form coffee cocktails appropriate, much .can contact : rohan.alexander@utoronto.ca.\nRohan Alexander\nToronto, Canada\n","code":""},{"path":"about-the-author.html","id":"about-the-author","chapter":"About the author","heading":"About the author","text":"Rohan Alexander assistant professor University Toronto Information Statistical Sciences. also assistant director CANSSI Ontario, senior fellow Massey College, faculty affiliate Schwartz Reisman Institute Technology Society, co-lead Data Sciences Institute Thematic Program Reproducibility. holds PhD Economics Australian National University supervised John Tang (chair), Martine Mariotti, Tim Hatton, Zach Ward.interested using statistical models try understand world. particularly get data go models; whose data systematically missing; clean, prepare, tidy data modelled; effects implications models; can reproducibly share totality process. tries develop students skilled using statistical methods across various disciplines, also appreciate limitations, think deeply broader contexts work.enjoys teaching aims help students wide range backgrounds learn use data tell convincing stories. teaches Faculty Information Department Statistical Sciences undergraduate graduate levels. RStudio Certified Tidyverse Trainer.married Monica Alexander two children. probably spends much money books, certainly much time libraries. book recommendations , ’d love hear .","code":""},{"path":"telling-stories-with-data.html","id":"telling-stories-with-data","chapter":"1 Telling stories with data","heading":"1 Telling stories with data","text":"Required materialRead Counting Countless, (Keyes 2019).Watch Data Science Ethics 6 Minutes, (Register 2020).","code":""},{"path":"telling-stories-with-data.html","id":"on-telling-stories","chapter":"1 Telling stories with data","heading":"1.1 On telling stories","text":"Like many parents, children born, one first things wife regularly read stories . carried tradition occurred millennia. Myths, fables, fairy tales can seen heard around us. entertaining enable us learn something world. Hungry Caterpillar (Carle 1969) may seem quite far world dealing data, similarities. trying tell story teaching us something world.using data, try tell convincing story. may exciting predicting elections, banal increasing internet advertising click rates, serious finding cause disease, fun forecasting basketball games. case key elements . English author, E. M. Forster, described aspects common novels : story, people, plot, fantasy, prophecy, pattern, rhythm (Forster 1927). Similarly, tell stories data, common concerns, regardless setting:dataset? generated dataset ?process underpins dataset? Given process, missing dataset poorly measured? datasets generated, , different one ?dataset trying say, can let say ? else say? decide ?hoping others see dataset, can convince ? much work must convince ? extent can share came believe ?affected processes outcomes related dataset? extent represented dataset, part conducting analysis?past, certain elements telling stories data easier. instance, experimental design long robust tradition within agricultural medical sciences, physics, chemistry. Student’s t-distribution identified early 1900s chemist, William Sealy Gosset, working Guinness, beer manufacturer (Boland 1984)! possible randomly sample beer change one aspect time.Many fundamentals statistical methods use today developed settings. , typically possible establish control groups randomize; settings fewer ethical concerns. story told resulting data likely fairly convincing.Unfortunately, little applies days, given diversity settings statistical methods applied. hand, many advantages. instance, well-developed statistical techniques, easier access large datasets, open-source statistical languages R (R Core Team 2021). difficulty conducting traditional experiments means must also turn aspects tell convincing story.","code":""},{"path":"telling-stories-with-data.html","id":"workflow-components","chapter":"1 Telling stories with data","heading":"1.2 Workflow components","text":"five core components workflow needed tell stories data:Plan sketch endpoint.Simulate reasonable data consider .Acquire prepare real data.Explore understand dataset.Share found.begin planning sketching endpoint ensures think carefully want go. forces us deeply consider situation, acts keep us focused efficient, helps reduce scope creep. Alice’s Adventures Wonderland (Carroll 1865), Alice asks Cheshire Cat way go. Cheshire Cat replies asking Alice like go. Alice replies mind, long gets somewhere, Cheshire Cat says direction matter always get somewhere ‘walk long enough.’ issue, case, typically afford walk aimlessly long. may endpoint needs change, important deliberate, reasoned, decision. possible given initial target. need spend much time get lot value . Often five minutes paper pen, enough.next step simulate data forces us details. helps cleaning preparing dataset focuses us classes dataset distribution values expect. instance, interested effect age-groups political preferences, may expect age-group column factor, four possible values: ‘18-29,’ ‘30-44,’ ‘45-59,’ ‘60+.’ process simulation provides us clear features real dataset satisfy. use features define tests guide data cleaning preparation. instance, check real dataset age-groups one four values. tests pass, confident age-group column contains values expect.Simulating data also important turn statistical modelling stage. stage, concerned whether model reflects dataset. issue go straight modelling real dataset, know whether problem model. simulate data precisely know underlying data generation process. apply model simulated dataset. get put , know model performing appropriately, can turn real dataset. Without initial application simulated data, difficult confidence model.Simulation often cheap—almost free given modern computing resources statistical programming languages—fast. provides ‘intimate feeling situation’ (Hamming 1996, 239). way proceed start simulation just contains essentials, get working, complicate .Acquiring preparing data interested often-overlooked stage workflow. surprising can one difficult stages requires many decisions made. increasingly subject research. instance, found decisions made stage greatly affect statistical results (Huntington-Klein et al. 2021).stage workflow, common feel little overwhelmed. Typically, data can acquire leave us little scared. may little , case worry going able make statistical machinery work. Alternatively, may problem worried can even begin deal large amount data.Perhaps dragons lives princesses waiting see us act, just , beauty courage. Perhaps everything frightens us , deepest essence, something helpless wants love.Rilke (1929)Developing comfort stage workflow unlocks rest . dataset needed tell convincing story , need iteratively remove everything data need, shape .dataset, want explore understand  certain relationships dataset. use statistical models understand implications data free bias, ‘truth’; tell . Within workflow tell stories data, statistical models tools approaches use explore dataset, way may use graphs tables. something provide us definitive result enable us understand dataset clearly particular way.time get step workflow, large extent, model reflect decisions made earlier stages, especially acquisition cleaning, much reflects type underlying process. Sophisticated modelers know statistical models like bit iceberg surface; build , possible due , majority underneath, case, data. expert whole workflow uses modelling, recognize results obtained additionally due choices whose data matters, decisions measure record data, aspects reflect world , well data available specific workflow.Finally, must share found, high fidelity possible. Talking knowledge , make knowledgeable, includes knowledge ‘past ’ . communicating, need clear decisions made, made , findings, weaknesses approach. aiming uncover something important (otherwise, bother) write everything, first instance, although written communication may supplemented forms communication later. many decisions must make workflow want sure open entire thing—start finish. means much just statistical modelling creation graphs tables, everything. Without , stories based data credibility.world rational meritocracy everything carefully judiciously evaluated. Instead, use shortcuts, hacks, heuristics, based experience. Unclear communication render even best work moot, thoroughly engaged . minimum comes communication, upper limit impressive can . culmination thought-workflow, best, obtains certain sprezzatura, studied carelessness. Achieving mastery, work years.","code":""},{"path":"telling-stories-with-data.html","id":"telling-stories-with-data-1","chapter":"1 Telling stories with data","heading":"1.3 Telling stories with data","text":"compelling story based data can likely told around ten--twenty pages. Much less , likely light details. easy write much , often reflection enables succinctness multiple stories separated. best stories typically based research independent learning.possible tell convincing stories even possible conduct traditional experiments. approaches rely ‘big data’—panacea (Meng 2018)—instead better using data available. blend theory application, combined practical skills, sophisticated workflow, appreciation one know, often enough create lasting knowledge.best stories based data tend multi-disciplinary. take whatever field need , almost always draw : statistics, data visualization, computer science, experimental design, economics, engineering, information science (name ). , end--end workflow requires blend skills areas. best way learn skills use real-world data conduct research projects :obtain clean relevant datasets;develop research questions;use statistical techniques explore questions; andcommunicate meaningful way.key elements telling convincing stories data :Communication.Ethics.Reproducibility.Questions.Measurement.Data collection.Data cleaning.Exploratory data analysis.Modelling.Scaling.elements foundation workflow built (Figure 1.1).\nFigure 1.1: workflow builds various elements\nlot master, communication important. Simple analysis, communicated well, valuable complicated analysis communicated poorly. latter understood trusted others. lack clear communication sometimes reflects failure researcher understand going , even . , level analysis match dataset, instruments, task, skillset, trade-required clarity complication, can sensible err side clarity.Clear communication means writing plain language, help tables, graphs, technical terms, way brings audience along . means setting done , well found. minimum hurdle way enables another person independently find found. One challenge immerse data, can difficult remember like first came . audience coming . Learning provide appropriate level nuance detail especially difficult made easier trying write audience’s benefit.Active consideration ethics needed dataset likely concerns humans. means considering things like: dataset, missing, ? extent story perpetuate past? something happen? Even dataset concern humans, story likely put together humans, affect almost everything else. means moral responsibility use data ethically, concern environmental impact, inequity.many definitions ethics, comes telling stories data, minimum means considering full context dataset (D’Ignazio Klein 2020). jurisprudence, textual approach law means literally considering words law printed, purposive approach means laws interpreted within broader context. ethical approach telling stories data means adopting latter approach, considering social, cultural, historical, political forces shape world, hence data (Crawford 2021).Reproducibility required create lasting knowledge world. means everything done—, end--end—can independently redone. Ideally, autonomous end--end reproducibility possible; anyone can get code, data, environment, verify everything done. Unfettered access code almost always possible. default data also, always reasonable. instance, studies psychology may small, personally identifying, samples. One way forward openly share simulated data similar properties, along defining process real data accessed, given appropriate bona fides.Curiosity provides internal motivation explore dataset, associated process, proper extent. Questions tend beget questions, usually improve refine process coming understand dataset carries . contrast stock Popperian approach hypothesis testing often taught, questions typically developed continuous evolving process (Franklin 2005). can difficult find initial question. Selecting area interest can help, can sketching broad claim intent evolving specific question, finally, bringing together two different areas.Developing comfort ease messiness real-world data means getting ask new questions time data update. knowing dataset detail tends surface unexpected groupings values can work subject-area experts understand. Becoming bit ‘mongrel’ developing base knowledge across variety areas especially valuable, becoming comfortable possibility initially asking dumb questions.Measurement data collection deciding world become data. challenging. world vibrant difficult reduce something possible consistently measure collect. Take, instance, someone’s height. can, probably, agree take shoes measure height. height changes course day. measuring someone’s height tape measure give different results using laser. comparing heights people time, therefore becomes important measure time day, using method. quickly becomes unfeasible.questions interested use data complicated height. measure sad someone ? measure pain? decides measure measure ? certain arrogance required think can reduce world value compare . Ultimately, must, difficult consistently define measured. process value-free. way reasonably come terms brutal reduction deeply understand, respect measuring collecting. central essence, can stripped away?Pablo Picasso, twentieth century Spanish painter, series drawings depicts outline animal using one line (Figure 1.2). Despite simplicity, recognize animal depicted—drawing sufficient tell animal dog, cat. used determine whether dog sick? Probably . likely want detailed drawing. decision features measured collected, ignore, turns context purpose.\nFigure 1.2: drawing clearly dog, even though just one line\nData cleaning preparation critical part using data. need massage data available us dataset can use. requires making lot decisions. data cleaning preparation stage critical, worthy much attention care .Consider survey collected information gender using four options: ‘man,’ ‘woman,’ ‘prefer say,’ ‘,’ ‘’ dissolved open textbox. come dataset, likely find responses either ‘man’ ‘woman.’ need decide ‘prefer say.’ drop dataset, actively ignoring respondents. drop , makes analysis complicated. Similarly, need decide deal open text responses. , drop responses, ignores experiences respondents. Another option merge ‘prefer say,’ shows disregard respondents, specifically choose option.easy, always-correct, choice many data cleaning preparation situations. depends context purpose. Data cleaning preparation involves making many choices like , vital record every step others can understand done . Data never speak ; dummies ventriloquists cleaned prepared .process coming understand look feel dataset termed exploratory data analysis (EDA). open-ended process. need understand shape dataset can formally model . process EDA iterative one involves producing summary statistics, graphs, tables, sometimes even modelling. process never formally finishes requires variety skills.difficult delineate EDA ends formal statistical modelling begins, especially considering beliefs understanding develop (Hullman Gelman 2021). core, ‘EDA starts data,’ involves immersing (Cook, Reid, Tanaka 2021). EDA something typically explicitly part final story. central role come understand story telling. , critical steps taken EDA recorded shared.Statistical modelling long robust history. knowledge statistics built hundreds years. Statistics series dry theorems proofs instead way exploring world. analogous ‘knowledge foreign languages algebra: may prove use time circumstances’ (Bowley 1901, 4). statistical model recipe blindly followed ---way instead way understanding data (James et al. 2017). Modelling usually required infer statistical patterns data. formally, ‘statistical inference, “learning” called computer science, process using data infer distribution generated data’ (Wasserman 2005, 87).Statistical significance scientific significance, realizing cost dominant paradigm. rarely appropriate put data arbitrary pass/fail statistical test. Instead, proper use statistical modelling kind echolocation. listen comes back us model, help learn shape world, recognizing one representation world.use statistical programming languages, R, enables us rapidly scale work. refers inputs outputs. basically just easy consider 10 observations 1,000, even 1,000,000. enables us quickly see extent stories apply. also case outputs can consumed easily one person 10, 100. Using Application Programming Interface (API) even possible stories considered many thousands times second.","code":""},{"path":"telling-stories-with-data.html","id":"how-do-our-worlds-become-data","chapter":"1 Telling stories with data","heading":"1.4 How do our worlds become data?","text":"famous story Eddington people went fishing sea net. Upon examining size fish caught, decided minimum size fish sea! conclusion arose tool used reality.Hamming (1996, 177)certain extent wasting time. perfect model world—world! complicated. knew perfectly everything affected uncountable factors influence , perfectly forecast coin toss, dice roll, every seemingly random process time. . Instead, must simplify things plausibly measurable, define data. data simplification messy, complex, world generated.different approximations ‘plausibly measurable.’ Hence, datasets always result choices. must decide whether nonetheless reasonable task hand. use statistical models help us think deeply , explore, hopefully come better understand, data.Much statistics focused considering, thoroughly, data . appropriate data predominately agricultural, astronomical, physical sciences. rise data science, mostly value application datasets generated humans, must also actively consider dataset. systematically missing dataset? Whose data fit nicely approach using hence inappropriately simplified? process world becoming data necessarily involves abstraction simplification, need clear points can reasonably simplify, inappropriate, recognizing application specific.process world becoming data necessarily involves measurement. Paradoxically, often measurement deeply immersed details less trust data removed . Even seemingly clear tasks, measuring distance, defining boundaries, counting populations, surprisingly difficult practice. Turning world data requires many decisions imposes much error. Among many considerations, need decide measured, accurately , measurement.Oh, think good data ! important example something seemingly simple quickly becomes difficult maternal mortality. refers number women die pregnant, soon termination, cause related pregnancy management (2019). difficult critical turn tragedy death cause-specific data helps mitigate future deaths. countries well-developed civil registration vital statistics (CRVS). collect data every death. many countries CRVS every death recorded. Even death recorded, defining cause death may difficult, especially lack qualified medical personal equipment. Maternal mortality especially difficult typically many causes. CRVS checkbox form specify whether death counted maternal mortality. even developed countries recently adopted . instance, introduced US 2003, even 2015 Alabama, California, West Virginia adopted standard question (MacDorman Declercq 2018).typically use various instruments turn world data. astronomy, development better telescopes, eventually satellites probes, enabled new understanding worlds. Similarly, new instruments turning world data developed day. census generational-defining event, now regular surveys, transactions data available second, almost interactions internet become data kind. development instruments enabled exciting new stories.world imperfectly becomes data. nonetheless use data learn world, need actively seek understand ways imperfect implications imperfections.","code":""},{"path":"telling-stories-with-data.html","id":"what-is-data-science-and-how-should-we-use-it-to-learn-about-the-world","chapter":"1 Telling stories with data","heading":"1.5 What is data science and how should we use it to learn about the world","text":"agreed definition data science, lot people tried. instance, Wickham Grolemund (2017) say ‘…exciting discipline allows turn raw data understanding, insight, knowledge.’ Similarly, Leek Peng (2020) say ‘…process formulating quantitative question can answered data, collecting cleaning data, analyzing data, communicating answer question relevant audience.’ Baumer, Kaplan, Horton (2021) say ‘…science extracting meaningful information data.’ T.-. Timbers, Campbell, Lee (2022) say define ‘data science process generating insight data reproducible auditable processes.’ Foster (1968) points clearly data science says: ‘(s)tatistics concerned processing analysis masses data development mathematical methods extracting information data. Combine activity computer methods something sum parts.’Craiu (2019) argues lack certainty data science might matter ‘…can really say makes someone poet scientist?’ goes broadly say data scientist ‘…someone data driven research agenda, adheres aspires using principled implementation statistical methods uses efficient computation skills.’case, alongside specific, technical, definitions, value simple definition, even lose bit specificity. Probability often informally defined ‘counting things’ (McElreath 2020, 10). similar informal sense, data science can defined something like: humans measuring stuff, typically related humans, using sophisticated averaging explain predict.may sound touch cute, Francis Edgeworth, nineteenth century statistician economist, considered statistics science ‘Means presented social phenomena,’ good company (Edgeworth 1885). case, one feature definition treat data terra nullius, nobody’s land. Statisticians tend see data result process can never know, try use data come understand. Many statisticians care deeply data measurement, many cases statistics data kind just appear; belong one. never actually case.Data generated, must gathered, cleaned, prepared, decisions matter. Every dataset sui generis, class , come know one dataset well, just know one dataset, datasets.Much data science focuses ‘science,’ important also focus ‘data.’ another feature cutesy definition data science. lot data scientists generalists, interested broad range problems. Often, thing unites need gather, clean, prepare messy data. often specifics data requires time, updates often, worthy full attention.Jordan (2019) describes medical office given probability, based prenatal screening, child, fetus, syndrome. way background, one can test know sure, test comes risk fetus surviving, initial screening probability matters. Jordan (2019) found probabilities determined based study done decade earlier UK. issue ensuing 10 years, imaging technology improved test expecting high-resolution images subsequent (false) increase syndrome diagnoses images improved. problem science, data.Shoulders giants Dr Michael Jordan Pehong Chen Distinguished Professor University California, Berkeley. taking PhD Cognitive Science University California, San Diego, 1985, appointed assistant professor MIT, promoted full professor 1997, 1998 moved Berkeley. One area research statistical machine learning. One particularly important paper Blei, Ng, Jordan (2003), enables text grouped together define topics.just ‘science’ bit hard, ‘data’ bit well. instance, researchers went back examined one popular text datasets computer science, found around 30 per cent data inappropriately duplicated (Bandy Vincent 2021). entire field—linguistics—specializes types datasets, inappropriate use data one dangers one field hegemonic. strength data science brings together folks variety backgrounds training task learning dataset. constrained done past. means must go way show respect come tradition, nonetheless similarly interested dataset . Data science multi-disciplinary increasingly critical; hence must reflect world. pressing need diversity backgrounds, approaches, disciplines data science.world messy, data. successfully tell stories data need become comfortable fact process difficult. Hannah Fry, British mathematician, describes spending six months rewriting code solved problem (Thornhill 2021). need learn stick . also need countenance failure, developing resilience intrinsic motivation. world data considering possibilities probabilities, learning make trade-offs . almost never anything know certain, perfect analysis.Ultimately, just telling stories data, stories increasingly among important world.","code":""},{"path":"telling-stories-with-data.html","id":"exercises-and-tutorial","chapter":"1 Telling stories with data","heading":"1.6 Exercises and tutorial","text":"","code":""},{"path":"telling-stories-with-data.html","id":"exercises","chapter":"1 Telling stories with data","heading":"1.6.1 Exercises","text":"According Register (2020) data decisions affect (pick one)?\nReal people.\none.\ntraining set.\ntest set.\nReal people.one.training set.test set.data science (words)?According Keyes (2019) perhaps accurate definition data science (pick one)?\ninhumane reduction humanity can counted.\nquantitative analysis large amounts data purpose decision-making.\nData science inter-disciplinary field uses scientific methods, processes, algorithms, systems extract knowledge insights many structural unstructured data.\ninhumane reduction humanity can counted.quantitative analysis large amounts data purpose decision-making.Data science inter-disciplinary field uses scientific methods, processes, algorithms, systems extract knowledge insights many structural unstructured data.Imagine job including ‘race’ /sexuality explanatory variables improves performance model. types issues consider deciding whether include variable production (words)?Re-order following steps workflow correct:\nSimulate.\nAcquire.\nShare.\nPlan.\nExplore.\nSimulate.Acquire.Share.Plan.Explore.According Crawford (2021), following forces shape world, hence data (select apply)?\nPolitical.\nHistorical.\nCultural.\nSocial.\nPolitical.Historical.Cultural.Social.required tell convincing stories (select apply)?\nSophisticated workflow.\nPractical skills.\nBig data.\nHumility one’s knowledge.\nTheory application.\nSophisticated workflow.Practical skills.Big data.Humility one’s knowledge.Theory application.ethics key element telling convincing stories (words)?","code":""},{"path":"telling-stories-with-data.html","id":"tutorial","chapter":"1 Telling stories with data","heading":"1.6.2 Tutorial","text":"purpose tutorial clarify mind difficulty measurement, even seemingly simple things, hence likelihood measurement issues complicated areas.Please obtain seeds fast-growing plant radishes, mustard greens, arugula. Plant seeds measure much soil used. Water measure water used. day take note changes. generally, measure record much can. Note thoughts difficulty measurement. Eventually seeds sprout, measure big . return use data put together.waiting seeds sprout, one week , please measure length hair daily. Write one--two-page paper found learned difficulty measurement.","code":""},{"path":"drinking-from-a-fire-hose.html","id":"drinking-from-a-fire-hose","chapter":"2 Drinking from a fire hose","heading":"2 Drinking from a fire hose","text":"Required materialRead Data science atomic habit, (Barrett 2021a)Read AI bias really happens—’s hard fix, (Hao 2019)Read mundanity excellence: ethnographic report stratification Olympic swimmers, (Chambliss 1989)Key concepts skillsThe statistical programming language R enables us tell interesting stories using data. language like , path mastery can slow.framework use approach projects : plan, simulate, gather, explore, share.way learn R start small project break required achieve tiny steps, look people’s code, draw achieve step. Complete project move onto next project. project get little better.key start actively working regularly.Key librariesggplot2 (Wickham 2016)janitor (Firke 2020)opendatatoronto (Gelfand 2020)tidyr (Wickham 2021c)tidyverse (Wickham 2017)Key functions<- ‘assign’|> ‘pipe’+ ‘add’c()citation()class()dplyr::arrange()dplyr::filter()dplyr::mutate()dplyr::recode()dplyr::rename()dplyr::select()dplyr::summarize()ggplot2::geom_bar()ggplot2::geom_point()ggplot2::ggplot()head()janitor::clean_names()library()names()readr::read_csv()readr::write_csv()rep()rpois()runif()sample()set.seed()stringr::str_remove()sum()tail()tidyr::separate()","code":""},{"path":"drinking-from-a-fire-hose.html","id":"hello-world","chapter":"2 Drinking from a fire hose","heading":"2.1 Hello, World!","text":"way start, start. chapter go three complete examples workflow advocated book. means : plan, simulate, acquire, explore, share. new R, code may bit unfamiliar . new statistics, concepts may unfamiliar. worry. soon become familiar.\nway learn tell stories, start telling stories . means try get examples working. sketches , type everything (using R Studio Cloud new R installed computer), execute . important, normal, realize challenging start.Whenever ’re learning new tool, long time, ’re going suck… good news typical; ’s something happens everyone, ’s temporary.Hadley Wickham quoted Barrett (2021a).guided thoroughly . Hopefully experiencing power telling stories data, feel empowered stick .get started, go R Studio Cloud – https://rstudio.cloud/ – create account. anything involved free version fine now. account log , look something like Figure 2.1.\nFigure 2.1: Opening R Studio Cloud first time\n(‘Workspace,’ ‘Example Workspace.’) start ‘New Project.’ can give project name clicking ‘Untitled Project’ replacing .now go three worked examples: Canadian elections, Toronto homelessness, neonatal mortality. examples build increasing complexity, first one, telling story data.","code":""},{"path":"drinking-from-a-fire-hose.html","id":"canadian-elections","chapter":"2 Drinking from a fire hose","heading":"2.2 Canadian elections","text":"Canada parliamentary democracy 338 seats House Commons, lower house government formed. two major parties – ‘Liberal’ ‘Conservative’ – three minor parties – ‘Bloc Québécois,’ ‘New Democratic,’ ‘Green’ – many smaller parties independents. example create graph number seats party won 2019 Federal Election.","code":""},{"path":"drinking-from-a-fire-hose.html","id":"plan","chapter":"2 Drinking from a fire hose","heading":"2.2.1 Plan","text":"example, need plan two aspects. first dataset need look like, second final graph look like.basic requirement dataset name seat (sometimes called ‘riding’ Canada) party person elected. , quick sketch dataset need look something like Figure 2.2.\nFigure 2.2: Quick sketch dataset useful analysing Canadian elections\nalso need plan graph interested . Given want display number seats party won, quick sketch might aim Figure 2.3.\nFigure 2.3: Quick sketch possible graph number ridings won party\n","code":""},{"path":"drinking-from-a-fire-hose.html","id":"simulate","chapter":"2 Drinking from a fire hose","heading":"2.2.2 Simulate","text":"now simulate data, bring specificity sketches.get started, within R Studio Cloud, make new R Markdown file (‘File’ -> ‘New File’ -> ‘R Markdown’). , asked install packages, agree . example, put everything one R Markdown document. save ‘canadian_elections.Rmd’ (‘File’ -> ‘Save …’)R Markdown document create new R code chunk (‘Code’ -> ‘Insert Chunk’) add preamble documentation explains:purpose document;author contact details;file written last updated; andpre-requisites file relies .R, lines start ‘#’ comments. means run code R, instead designed read humans. line preamble start ‘#.’ Also make clear preamble section surrounding ‘####.’need set-workspace. involves installing loading packages needed. package needs installed computer, needs loaded time used. case going use tidyverse (Wickham 2017), janitor (Firke 2020), tidyr (Wickham 2021c). need installed first time used, need loaded.example installing packages follows (excessive comments added clear going ; general, level commenting unnecessary). Run code clicking small green arrow associated R code chunk.Now packages installed, need loaded. installation step needs done per computer, code can commented accidentally run.packages contain help file provides information functions. can accessed appending question mark package name running code. instance ?tidyverse.simulate data, need create dataset two columns: ‘Riding’ ‘Party,’ values . case ‘Riding’ reasonable values name one 338 Canadian ridings. case ‘Party’ reasonable values one following six: ‘Liberal,’ ‘Conservative,’ ‘Bloc Québécois,’ ‘New Democratic,’ ‘Green,’ ‘.’ , code can run clicking small green arrow associated R code chunk.","code":"\n#### Preamble ####\n# Purpose: Read in data from the 2019 Canadian Election and make a\n# graph of the number of ridings each party won.\n# Author: Rohan Alexander\n# Email: rohan.alexander@utoronto.ca\n# Date: 1 January 2022\n# Prerequisites: Need to know where to get Canadian elections data.\n#### Workspace set-up ####\ninstall.packages(\"tidyverse\") # Only need to do this once per computer\ninstall.packages(\"janitor\") # Only need to do this once per computer\ninstall.packages(\"tidyr\") # Only need to do this once per computer\n#### Workspace set-up ####\n# install.packages(\"tidyverse\") # Only need to do this once per computer\n# install.packages(\"janitor\") # Only need to do this once per computer\n# install.packages(\"tidyr\") # Only need to do this once per computer\n\nlibrary(tidyverse) # A collection of data-related packages\nlibrary(janitor) # Helps clean datasets\nlibrary(tidyr) # Helps make tidy datasets\nsimulated_data <-\n  tibble(\n    # Use 1 through to 338 to represent each riding\n    'Riding' = 1:338,\n    # Randomly choose one of six options, with replacement, 338 times\n    'Party' = sample(\n      x = c(\n        'Liberal',\n        'Conservative',\n        'Bloc Québécois',\n        'New Democratic',\n        'Green',\n        'Other'\n      ),\n      size = 338,\n      replace = TRUE\n    ))\n\nsimulated_data\n#> # A tibble: 338 × 2\n#>    Riding Party         \n#>     <int> <chr>         \n#>  1      1 Other         \n#>  2      2 Liberal       \n#>  3      3 Liberal       \n#>  4      4 Bloc Québécois\n#>  5      5 Liberal       \n#>  6      6 Bloc Québécois\n#>  7      7 Bloc Québécois\n#>  8      8 Conservative  \n#>  9      9 Green         \n#> 10     10 Liberal       \n#> # … with 328 more rows"},{"path":"drinking-from-a-fire-hose.html","id":"acquire","chapter":"2 Drinking from a fire hose","heading":"2.2.3 Acquire","text":"Now want get actual data. data need Elections Canada, non-partisan agency organizes Canadian Federal elections. can pass website read_csv() readr package (Wickham, Hester, Bryan 2021). need explicitly load readr package part tidyverse. ‘<-’ ‘assignment operator’ allocating output read_csv() object called ‘raw_elections_data.’can take quick look dataset using head() show first six rows, tail() show last six rows.need clean data can use . trying make similar dataset thought wanted planning stage. fine move away plan, needs deliberate, reasoned, decision. reading dataset saved, first thing adjust names make easier type. Removing spaces helps type column names. using clean_names() janitor (Firke 2020) changes names ‘snake_case.’names faster type R Studio auto-complete . , begin typing name column use ‘tab’ auto-complete .many columns dataset, primarily interested two: ‘electoral_district_name_nom_de_circonscription,’ ‘elected_candidate_candidat_elu.’ can choose certain columns interest using select() dplyr (Wickham et al. 2020) loaded part tidyverse. ‘pipe operator,’ |>, pushes output one line first input function next line.names columns still quite long English French . can look names columns names(). can change names using rename() dplyr (Wickham et al. 2020).now look dataset, ‘elected_candidate’ column particular.can see surname elected candidate, followed comma, followed first name, followed space, followed name party English French, separated slash. can break-column pieces using separate() tidyr (Wickham 2021c).Finally want change party names French English match simulated, using recode() dplyr (Wickham et al. 2020).data now matches plan (Figure 2.2) pretty well. every electoral district party person won .now nicely cleaned dataset, save , can start cleaned dataset next stage.","code":"\n#### Read in the data ####\nraw_elections_data <- \n  read_csv(\n    file =\n      \"https://www.elections.ca/res/rep/off/ovr2019app/51/data_donnees/table_tableau11.csv\",\n    show_col_types = FALSE\n    ) \n\n# We have read the data from the Elections Canada website. We may like\n# to save it in case something happens or they move it. \nwrite_csv(\n  x = raw_elections_data, \n  file = \"canadian_voting.csv\"\n  )\nhead(raw_elections_data)\n#> # A tibble: 6 × 13\n#>   Province      `Electoral Dis…` `Electoral Dis…` Population\n#>   <chr>         <chr>                       <dbl>      <dbl>\n#> 1 Newfoundland… Avalon                      10001      86494\n#> 2 Newfoundland… Bonavista--Buri…            10002      74116\n#> 3 Newfoundland… Coast of Bays--…            10003      77680\n#> 4 Newfoundland… Labrador                    10004      27197\n#> 5 Newfoundland… Long Range Moun…            10005      86553\n#> 6 Newfoundland… St. John's East…            10006      85697\n#> # … with 9 more variables: `Electors/Électeurs` <dbl>,\n#> #   `Polling Stations/Bureaux de scrutin` <dbl>,\n#> #   `Valid Ballots/Bulletins valides` <dbl>,\n#> #   `Percentage of Valid Ballots /Pourcentage des bulletins valides` <dbl>,\n#> #   `Rejected Ballots/Bulletins rejetés` <dbl>,\n#> #   `Percentage of Rejected Ballots /Pourcentage des bulletins rejetés` <dbl>,\n#> #   `Total Ballots Cast/Total des bulletins déposés` <dbl>, …\ntail(raw_elections_data)\n#> # A tibble: 6 × 13\n#>   Province      `Electoral Dis…` `Electoral Dis…` Population\n#>   <chr>         <chr>                       <dbl>      <dbl>\n#> 1 British Colu… Vancouver South…            59040     102927\n#> 2 British Colu… Victoria                    59041     117133\n#> 3 British Colu… West Vancouver-…            59042     119113\n#> 4 Yukon         Yukon                       60001      35874\n#> 5 Northwest Te… Northwest Terri…            61001      41786\n#> 6 Nunavut       Nunavut                     62001      35944\n#> # … with 9 more variables: `Electors/Électeurs` <dbl>,\n#> #   `Polling Stations/Bureaux de scrutin` <dbl>,\n#> #   `Valid Ballots/Bulletins valides` <dbl>,\n#> #   `Percentage of Valid Ballots /Pourcentage des bulletins valides` <dbl>,\n#> #   `Rejected Ballots/Bulletins rejetés` <dbl>,\n#> #   `Percentage of Rejected Ballots /Pourcentage des bulletins rejetés` <dbl>,\n#> #   `Total Ballots Cast/Total des bulletins déposés` <dbl>, …\n#### Basic cleaning ####\nraw_elections_data <- \n  read_csv(file = \"canadian_voting.csv\",\n           show_col_types = FALSE\n           )\n# Make the names easier to type\ncleaned_elections_data <- \n  clean_names(raw_elections_data)\n\n# Have a look at the first six rows\nhead(cleaned_elections_data)\n#> # A tibble: 6 × 13\n#>   province      electoral_distr… electoral_distr… population\n#>   <chr>         <chr>                       <dbl>      <dbl>\n#> 1 Newfoundland… Avalon                      10001      86494\n#> 2 Newfoundland… Bonavista--Buri…            10002      74116\n#> 3 Newfoundland… Coast of Bays--…            10003      77680\n#> 4 Newfoundland… Labrador                    10004      27197\n#> 5 Newfoundland… Long Range Moun…            10005      86553\n#> 6 Newfoundland… St. John's East…            10006      85697\n#> # … with 9 more variables: electors_electeurs <dbl>,\n#> #   polling_stations_bureaux_de_scrutin <dbl>,\n#> #   valid_ballots_bulletins_valides <dbl>,\n#> #   percentage_of_valid_ballots_pourcentage_des_bulletins_valides <dbl>,\n#> #   rejected_ballots_bulletins_rejetes <dbl>,\n#> #   percentage_of_rejected_ballots_pourcentage_des_bulletins_rejetes <dbl>,\n#> #   total_ballots_cast_total_des_bulletins_deposes <dbl>, …\ncleaned_elections_data <- \n  cleaned_elections_data |> \n  # Select only certain columns\n  select(electoral_district_name_nom_de_circonscription,\n         elected_candidate_candidat_elu\n         )\n\n# Have a look at the first six rows\nhead(cleaned_elections_data)\n#> # A tibble: 6 × 2\n#>   electoral_district_name_nom_de_circonscr… elected_candida…\n#>   <chr>                                     <chr>           \n#> 1 Avalon                                    McDonald, Kenne…\n#> 2 Bonavista--Burin--Trinity                 Rogers, Churenc…\n#> 3 Coast of Bays--Central--Notre Dame        Simms, Scott Li…\n#> 4 Labrador                                  Jones, Yvonne L…\n#> 5 Long Range Mountains                      Hutchings, Gudi…\n#> 6 St. John's East/St. John's-Est            Harris, Jack ND…\nnames(cleaned_elections_data)\n#> [1] \"electoral_district_name_nom_de_circonscription\"\n#> [2] \"elected_candidate_candidat_elu\"\ncleaned_elections_data <- \n  cleaned_elections_data |> \n  rename(\n    riding = electoral_district_name_nom_de_circonscription,\n    elected_candidate = elected_candidate_candidat_elu\n    )\n\nhead(cleaned_elections_data)\n#> # A tibble: 6 × 2\n#>   riding                             elected_candidate      \n#>   <chr>                              <chr>                  \n#> 1 Avalon                             McDonald, Kenneth Libe…\n#> 2 Bonavista--Burin--Trinity          Rogers, Churence Liber…\n#> 3 Coast of Bays--Central--Notre Dame Simms, Scott Liberal/L…\n#> 4 Labrador                           Jones, Yvonne Liberal/…\n#> 5 Long Range Mountains               Hutchings, Gudie Liber…\n#> 6 St. John's East/St. John's-Est     Harris, Jack NDP-New D…\nhead(cleaned_elections_data$elected_candidate)\n#> [1] \"McDonald, Kenneth Liberal/Libéral\"                                   \n#> [2] \"Rogers, Churence Liberal/Libéral\"                                    \n#> [3] \"Simms, Scott Liberal/Libéral\"                                        \n#> [4] \"Jones, Yvonne Liberal/Libéral\"                                       \n#> [5] \"Hutchings, Gudie Liberal/Libéral\"                                    \n#> [6] \"Harris, Jack NDP-New Democratic Party/NPD-Nouveau Parti démocratique\"\ncleaned_elections_data <- \n  cleaned_elections_data |> \n  # Separate the column into two based on the slash\n  separate(col = elected_candidate,\n           into = c('other', 'party'),\n           sep = '/') |> \n  # Remove the 'other' column\n  select(-other)\n\nhead(cleaned_elections_data)\n#> # A tibble: 6 × 2\n#>   riding                             party                  \n#>   <chr>                              <chr>                  \n#> 1 Avalon                             Libéral                \n#> 2 Bonavista--Burin--Trinity          Libéral                \n#> 3 Coast of Bays--Central--Notre Dame Libéral                \n#> 4 Labrador                           Libéral                \n#> 5 Long Range Mountains               Libéral                \n#> 6 St. John's East/St. John's-Est     NPD-Nouveau Parti démo…\ncleaned_elections_data <-\n  cleaned_elections_data |>\n  mutate(\n    party =\n      recode(\n        party,\n        'Conservateur' = 'Conservative',\n        'Indépendant(e)' = 'Other',\n        'Libéral' = 'Liberal',\n        'NPD-Nouveau Parti démocratique' = 'New Democratic',\n        'Parti Vert' = 'Green'\n      )\n  )\n\nhead(cleaned_elections_data)\n#> # A tibble: 6 × 2\n#>   riding                             party         \n#>   <chr>                              <chr>         \n#> 1 Avalon                             Liberal       \n#> 2 Bonavista--Burin--Trinity          Liberal       \n#> 3 Coast of Bays--Central--Notre Dame Liberal       \n#> 4 Labrador                           Liberal       \n#> 5 Long Range Mountains               Liberal       \n#> 6 St. John's East/St. John's-Est     New Democratic\nwrite_csv(\n  x = cleaned_elections_data,\n  file = \"cleaned_elections_data.csv\"\n  )"},{"path":"drinking-from-a-fire-hose.html","id":"explore","chapter":"2 Drinking from a fire hose","heading":"2.2.4 Explore","text":"point like explore dataset created. One way better understand dataset make graph. particular, like build graph planned Figure 2.3.First, read dataset just created.can get quick count many seats party won using count() dplyr (Wickham et al. 2020).build graph interested , rely ggplot2 package (Wickham 2016). key aspect package build graphs adding layers using ‘+,’ call ‘add operator.’ particular create bar chart using geom_bar() ggplot2 (Wickham 2016).accomplishes set . can make look bit nicer modifying default options (Figure 2.4).\nFigure 2.4: Number seats won, political party, 2019 Canadian Federal Election\n","code":"\n#### Read in the data ####\ncleaned_elections_data <- \n  read_csv(\n    file = \"cleaned_elections_data.csv\",\n    show_col_types = FALSE\n    )\ncleaned_elections_data |> \n  count(party)\n#> # A tibble: 6 × 2\n#>   party              n\n#>   <chr>          <int>\n#> 1 Bloc Québécois    32\n#> 2 Conservative     121\n#> 3 Green              3\n#> 4 Liberal          157\n#> 5 New Democratic    24\n#> 6 Other              1\ncleaned_elections_data |> \n  ggplot(aes(x = party)) + # aes abbreviates aesthetics and enables us \n  # to specify the x axis variable\n  geom_bar()\ncleaned_elections_data |> \n  ggplot(aes(x = party)) +\n  geom_bar() +\n  theme_minimal() + # Make the theme neater\n  coord_flip() + # Swap the x and y axis to make parties easier to read\n  labs(x = \"Party\",\n       y = \"Number of seats\") # Make the labels more meaningful"},{"path":"drinking-from-a-fire-hose.html","id":"communicate","chapter":"2 Drinking from a fire hose","heading":"2.2.5 Communicate","text":"point downloaded data, cleaned , made graph. typically need communicate done length. case, can write paragraphs , , found. example follows.Canada parliamentary democracy 338 seats House Commons, house forms government. two major parties—‘Liberal’ ‘Conservative’—three minor parties—‘Bloc Québécois,’ ‘New Democratic,’ ‘Green’—many smaller parties. 2019 Federal Election occurred 21 October, 17 million votes cast. interested number seats won party.downloaded results, seat-specific basis, Elections Canada website. cleaned tidied dataset using statistical programming language R (R Core Team 2021) well packages tidyverse (Wickham et al. 2019a) janitor (Firke 2020). created graph number seats political party won (Figure 2.4).found Liberal Party won 157 seats, followed Conservative Party 121 seats. minor parties won following number seats: Bloc Québécois, 32 seats, New Democratic Party, 24 seats, Green Party, 3 seats. Finally, one independent candidate won seat.distribution seats skewed toward two major parties reflect relatively stable preferences part Canadian voters, possibly inertia due benefits already major party national network funding, reason. better understanding reasons distribution interest future work. dataset consists everyone voted, worth noting Canada systematically excluded voting; much difficult vote others.","code":""},{"path":"drinking-from-a-fire-hose.html","id":"toronto-homelessness","chapter":"2 Drinking from a fire hose","heading":"2.3 Toronto homelessness","text":"Toronto large homeless population (City Toronto 2021). Freezing winters mean important enough places shelters. example make table shelter usage second half 2021 compares average use month. expectation greater usage colder months, instance, December, compared warmer months, instance, July.","code":""},{"path":"drinking-from-a-fire-hose.html","id":"plan-1","chapter":"2 Drinking from a fire hose","heading":"2.3.1 Plan","text":"dataset interested need date, shelter, number beds occupied night. quick sketch dataset work Figure 2.5.\nFigure 2.5: Quick sketch dataset useful understanding shelter usage Toronto\ninterested creating table monthly average number beds occupied night. table probably look something like Figure 2.6.\nFigure 2.6: Quick sketch table average number beds occupied month\n","code":""},{"path":"drinking-from-a-fire-hose.html","id":"simulate-1","chapter":"2 Drinking from a fire hose","heading":"2.3.2 Simulate","text":"next step simulate data resemble dataset.Within R Studio Cloud make new R Markdown file, save , make new R code chunk add preamble documentation. install /load libraries needed. use tidyverse (Wickham 2017), janitor (Firke 2020), tidyr (Wickham 2021c). installed earlier, need installed . example also use lubridate (Grolemund Wickham 2011), part tidyverse need installed independently. also use opendatatoronto (Gelfand 2020), knitr (Xie 2021) need installed.add bit detail earlier example, libraries contain code people written. common ones see regularly, especially tidyverse. use package, must first install need load . package needs installed per computer must loaded every time. , packages installed earlier need reinstalled .Given folks freely gave time make R packages use, important cite . get information needed, can use citation(). run without arguments, provides citation information R , run argument name package, provides citation information package.Turning simulation, need three columns: ‘date,’ ‘shelter,’ ‘occupancy.’ example build earlier one adding seed using set.seed(). seed enables us always generate random data. integer can used seed. case seed 853. use seed, get random numbers example. use different seed, expect different random numbers. Finally, use rep() repeat something certain number times. instance, repeat ‘Shelter 1,’ 184 times accounts half year.simulation first create list dates 2021. repeat list three times. assume data three shelters every day year. simulate number beds occupied night, draw Poisson distribution, assuming mean number 30 beds occupied per shelter.","code":"\n#### Preamble ####\n# Purpose: Get data about 2021 houseless shelter usage and make a table\n# Author: Rohan Alexander\n# Email: rohan.alexander@utoronto.ca\n# Date: 1 January 2022\n# Prerequisites: - \n\n#### Workspace set-up ####\ninstall.packages(\"opendatatoronto\")\ninstall.packages(\"lubridate\")\ninstall.packages(\"knitr\")\n\nlibrary(knitr)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(opendatatoronto)\nlibrary(tidyverse)\nlibrary(tidyr)\ncitation() # Get the citation information for R\n#> \n#> To cite R in publications use:\n#> \n#>   R Core Team (2021). R: A language and environment\n#>   for statistical computing. R Foundation for\n#>   Statistical Computing, Vienna, Austria. URL\n#>   https://www.R-project.org/.\n#> \n#> A BibTeX entry for LaTeX users is\n#> \n#>   @Manual{,\n#>     title = {R: A Language and Environment for Statistical Computing},\n#>     author = {{R Core Team}},\n#>     organization = {R Foundation for Statistical Computing},\n#>     address = {Vienna, Austria},\n#>     year = {2021},\n#>     url = {https://www.R-project.org/},\n#>   }\n#> \n#> We have invested a lot of time and effort in creating\n#> R, please cite it when using it for data analysis.\n#> See also 'citation(\"pkgname\")' for citing R packages.\ncitation('tidyverse') # Get the citation information for a particular package\n#> \n#>   Wickham et al., (2019). Welcome to the tidyverse.\n#>   Journal of Open Source Software, 4(43), 1686,\n#>   https://doi.org/10.21105/joss.01686\n#> \n#> A BibTeX entry for LaTeX users is\n#> \n#>   @Article{,\n#>     title = {Welcome to the {tidyverse}},\n#>     author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},\n#>     year = {2019},\n#>     journal = {Journal of Open Source Software},\n#>     volume = {4},\n#>     number = {43},\n#>     pages = {1686},\n#>     doi = {10.21105/joss.01686},\n#>   }\n#### Simulate ####\nset.seed(853)   \n\nsimulated_occupancy_data <- \n  tibble(\n    date = rep(x = as.Date(\"2021-07-01\") + c(0:183), times = 3), \n    # Based on Dirk Eddelbuettel: https://stackoverflow.com/a/21502386\n    shelter = c(rep(x = \"Shelter 1\", times = 184), \n                rep(x = \"Shelter 2\", times = 184),\n                rep(x = \"Shelter 3\", times = 184)),\n    number_occupied = \n      rpois(n = 184*3,\n            lambda = 30) # Draw 552 times from the Poisson distribution\n    )\n\nhead(simulated_occupancy_data)\n#> # A tibble: 6 × 3\n#>   date       shelter   number_occupied\n#>   <date>     <chr>               <int>\n#> 1 2021-07-01 Shelter 1              28\n#> 2 2021-07-02 Shelter 1              29\n#> 3 2021-07-03 Shelter 1              35\n#> 4 2021-07-04 Shelter 1              25\n#> 5 2021-07-05 Shelter 1              21\n#> 6 2021-07-06 Shelter 1              30"},{"path":"drinking-from-a-fire-hose.html","id":"acquire-1","chapter":"2 Drinking from a fire hose","heading":"2.3.3 Acquire","text":"use data made available Toronto homeless shelters City Toronto. premise data night 4am count made occupied beds. access data, use opendatatoronto (Gelfand 2020) save copy.much needs done make similar dataset interested (Figure 2.5). need change names make easier type using clean_names(), reduce columns relevant using select(), keep second half year using filter().remains save cleaned dataset.","code":"\n#### Acquire ####\n# Based on code from: \n# https://open.toronto.ca/dataset/daily-shelter-overnight-service-occupancy-capacity/\n# Thank you to Heath Priston for assistance\ntoronto_shelters <- \n  # Each package is associated with a unique id which can be found in \n  # 'For Developers':\n  # https://open.toronto.ca/dataset/daily-shelter-overnight-service-occupancy-capacity/\n  list_package_resources(\"21c83b32-d5a8-4106-a54f-010dbe49f6f2\") |> \n  # Within that package, we are interested in the 2021 dataset\n  filter(name == \"daily-shelter-overnight-service-occupancy-capacity-2021\") |> \n  # Having reduce the dataset down to one row we can get the resource\n  get_resource()\n\nwrite_csv(\n  x = toronto_shelters, \n  file = \"toronto_shelters.csv\"\n  )\n\nhead(toronto_shelters)#> # A tibble: 6 × 32\n#>     `_id` OCCUPANCY_DATE ORGANIZATION_ID ORGANIZATION_NAME  \n#>     <dbl> <date>                   <dbl> <chr>              \n#> 1 7272806 2021-01-01                  24 COSTI Immigrant Se…\n#> 2 7272807 2021-01-01                  24 COSTI Immigrant Se…\n#> 3 7272808 2021-01-01                  24 COSTI Immigrant Se…\n#> 4 7272809 2021-01-01                  24 COSTI Immigrant Se…\n#> 5 7272810 2021-01-01                  24 COSTI Immigrant Se…\n#> 6 7272811 2021-01-01                  24 COSTI Immigrant Se…\n#> # … with 28 more variables: SHELTER_ID <dbl>,\n#> #   SHELTER_GROUP <chr>, LOCATION_ID <dbl>,\n#> #   LOCATION_NAME <chr>, LOCATION_ADDRESS <chr>,\n#> #   LOCATION_POSTAL_CODE <chr>, LOCATION_CITY <chr>,\n#> #   LOCATION_PROVINCE <chr>, PROGRAM_ID <dbl>,\n#> #   PROGRAM_NAME <chr>, SECTOR <chr>, PROGRAM_MODEL <chr>,\n#> #   OVERNIGHT_SERVICE_TYPE <chr>, PROGRAM_AREA <chr>, …\ntoronto_shelters_clean <- \n  clean_names(toronto_shelters) |> \n  select(occupancy_date, id, occupied_beds) |> \n  filter(occupancy_date >= as_date(\"2021-07-01\"))\n\nhead(toronto_shelters_clean)\n#> # A tibble: 6 × 3\n#>   occupancy_date      id occupied_beds\n#>   <date>           <dbl>         <dbl>\n#> 1 2021-12-27     7323151            50\n#> 2 2021-12-27     7323152            18\n#> 3 2021-12-27     7323153            28\n#> 4 2021-12-27     7323154            50\n#> 5 2021-12-27     7323155            NA\n#> 6 2021-12-27     7323156            NA\nwrite_csv(\n  x = toronto_shelters_clean, \n  file = \"cleaned_toronto_shelters.csv\"\n  )"},{"path":"drinking-from-a-fire-hose.html","id":"explore-1","chapter":"2 Drinking from a fire hose","heading":"2.3.4 Explore","text":"First, load dataset just created.dataset daily basis shelter. interested understanding average usage month. , need add month column, using month() lubridate (Grolemund Wickham 2011). default, month() provides number month, include two arguments ‘label’ ‘abbr’ get full name month. remove rows data number beds using drop_na() tidyr. create summary statistic basis monthly groups, using summarize() dplyr (Wickham et al. 2020). use kable() knitr (Xie 2021) create table., looks fine, achieves set . can make tweaks defaults make look even better (Table 2.1). can add caption, make column names easier read, show appropriate level decimal places, improve formatting.Table 2.1: Homeless shelter usage Toronto 2021","code":"\n#### Explore ####\ntoronto_shelters_clean <- \n  read_csv(\n    \"cleaned_toronto_shelters.csv\",\n    show_col_types = FALSE\n    )\n# Based on code from Florence Vallée-Dubois and Lisa Lendway\ntoronto_shelters_clean |>\n  mutate(occupancy_month = month(occupancy_date, \n                                 label = TRUE, \n                                 abbr = FALSE)) |>\n  drop_na(occupied_beds) |> # We only want rows that have data\n  group_by(occupancy_month) |> # We want to know the occupancy by month\n  summarize(number_occupied = mean(occupied_beds)) |>\n  kable()\ntoronto_shelters_clean |>\n  mutate(occupancy_month = month(occupancy_date, \n                                 label = TRUE, \n                                 abbr = FALSE)) |>\n  drop_na(occupied_beds) |> # We only want rows that have data\n  group_by(occupancy_month) |> # We want to know the occupancy by month\n  summarize(number_occupied = mean(occupied_beds)) |> \n  kable(caption = \"Homeless shelter usage in Toronto in 2021\", \n        col.names = c(\"Month\", \"Average daily number of occupied beds\"),\n        digits = 1,\n        booktabs = TRUE,\n        linesep = \"\"\n        )"},{"path":"drinking-from-a-fire-hose.html","id":"communicate-1","chapter":"2 Drinking from a fire hose","heading":"2.3.5 Communicate","text":"need write brief paragraphs , , found. example follows.Toronto large homeless population. Freezing winters mean critical enough places shelters. interested understand usage shelters changes colder months, compared warmer months.use data provided City Toronto Toronto homeless shelter bed occupancy. Specifically, 4am night count made occupied beds. interested averaging month. cleaned, tidied, analyzed dataset using statistical programming language R (R Core Team 2021) well packages tidyverse (Wickham 2017), janitor (Firke 2020), tidyr (Wickham 2021c), opendatatoronto (Gelfand 2020), lubridate (Grolemund Wickham 2011), knitr (Xie 2021). made table average number occupied beds night month (Table 2.1).found daily average number occupied beds higher December 2021 July 2021, 34 occupied beds December, compared 30 July (Table 2.1). generally, steady increase daily average number occupied beds July December, slight increase month.dataset basis shelters, results may skewed changes specific especially large especially small shelters. may particular shelters especially attractive colder months. Additionally, concerned counts number occupied beds, supply beds changes season, additional statistic interest proportion occupied.Although example paragraphs, reduced form abstract, increased form full report. first paragraph general overview, second focuses data, third results, fourth discussion. expanded form sections short report.","code":""},{"path":"drinking-from-a-fire-hose.html","id":"neonatal-mortality","chapter":"2 Drinking from a fire hose","heading":"2.4 Neonatal mortality","text":"Neonatal mortality refers death occurs within first month life, particular, neonatal mortality rate (NMR) number neonatal deaths per 1,000 live births (UN IGME 2021). Reducing part third Sustainable Development Goal (Hug et al. 2019). example create graph estimated NMR past fifty years : Argentina, Australia, Canada, Kenya.","code":""},{"path":"drinking-from-a-fire-hose.html","id":"plan-2","chapter":"2 Drinking from a fire hose","heading":"2.4.1 Plan","text":"example, need think dataset look like, graph look like.dataset needs columns specify country, year. also needs column NMR estimate year country. Roughly, look like Figure 2.7.\nFigure 2.7: Quick sketch potentially useful NMR dataset\ninterested make graph year x-axis estimated NMR y-axis. country series Roughly similar Figure 2.8.\nFigure 2.8: Quick sketch graph NMR country time\n","code":""},{"path":"drinking-from-a-fire-hose.html","id":"simulate-2","chapter":"2 Drinking from a fire hose","heading":"2.4.2 Simulate","text":"like simulate data aligns plan. case need three columns: country, year, NMR.Within R Studio Cloud make new R Markdown file save . Add preamble documentation set-workspace. use tidyverse (Wickham 2017), janitor (Firke 2020), lubridate (Grolemund Wickham 2011).code contained libraries can change time time authors update release new versions. can see version package using packageVersion(). instance, using version 1.3.1 tidyverse version 2.1.0 janitor.update version package, use update.packages().need run, say, every day, time--time worth updating packages. many packages take care ensure backward compatibility, certain point become reasonable, important aware updating packages can result old code needing updated.Returning simulation, repeat name country 50 times rep(), enable passing 50 years. Finally, draw uniform distribution runif() simulate estimated NMR value year country.simulation works, time-consuming error-prone decided instead fifty years, interested simulating, say, sixty years. One way make easier replace instances 50 variable. example follows.result , now want change fifty sixty years make change one place.can confidence simulated dataset relatively straight-forward, wrote code . turn real dataset, difficult sure claims . Even trust data, important can share confidence others. One way forward establish checks prove data . instance, expect:‘country’ , , one four: ‘Argentina,’ ‘Australia,’ ‘Canada,’ ‘Kenya.’Conversely, ‘country’ contains four countries.‘year’ smaller 1971 larger 2020, number, letter.‘nmr’ value somewhere 0 1,000, number.can write series tests based features, expect dataset pass.passed tests, can confidence simulated dataset. importantly, can apply tests real dataset. enables us greater confidence dataset share confidence others.","code":"\n#### Preamble ####\n# Purpose: Obtain and prepare data about neonatal mortality for four\n# countries for the past fifty years and create a graph.\n# Author: Rohan Alexander\n# Email: rohan.alexander@utoronto.ca\n# Date: 1 January 2022\n# Prerequisites: - \n\n#### Workspace set-up ####\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(tidyverse)\npackageVersion('tidyverse')\n#> [1] '1.3.1'\npackageVersion('janitor')\n#> [1] '2.1.0'\nupdate.packages()\n#### Simulate data ####\nset.seed(853)\n\nsimulated_nmr_data <- \n  tibble(\n    country = \n      c(\n        rep('Argentina', 50),\n        rep('Australia', 50),\n        rep('Canada', 50),\n        rep('Kenya', 50)\n        ),\n    year = \n      rep(c(1971:2020), 4),\n    nmr = \n      runif(n = 200,\n            min = 0, \n            max = 100)\n  )\n\nhead(simulated_nmr_data)\n#> # A tibble: 6 × 3\n#>   country    year   nmr\n#>   <chr>     <int> <dbl>\n#> 1 Argentina  1971 35.9 \n#> 2 Argentina  1972 12.0 \n#> 3 Argentina  1973 48.4 \n#> 4 Argentina  1974 31.6 \n#> 5 Argentina  1975  3.74\n#> 6 Argentina  1976 40.4\n#### Simulate data ####\nset.seed(853)\n\nnumber_of_years <- 50\n\nsimulated_nmr_data <- \n  tibble(\n    country = \n      c(\n        rep('Argentina', number_of_years),\n        rep('Australia', number_of_years),\n        rep('Canada', number_of_years),\n        rep('Kenya', number_of_years)\n        ),\n    year = \n      rep(c(1:number_of_years + 1970), 4),\n    nmr = \n      runif(n = number_of_years * 4,\n            min = 0, \n            max = 100)\n  )\n\nhead(simulated_nmr_data)\n#> # A tibble: 6 × 3\n#>   country    year   nmr\n#>   <chr>     <dbl> <dbl>\n#> 1 Argentina  1971 35.9 \n#> 2 Argentina  1972 12.0 \n#> 3 Argentina  1973 48.4 \n#> 4 Argentina  1974 31.6 \n#> 5 Argentina  1975  3.74\n#> 6 Argentina  1976 40.4\n# Tests for simulated data\nsimulated_nmr_data$country |> \n  unique() == c(\"Argentina\", \n                \"Australia\", \n                \"Canada\", \n                \"Kenya\")\n#> [1] TRUE TRUE TRUE TRUE\n\nsimulated_nmr_data$country |> unique() |> length() == 4\n#> [1] TRUE\n\nsimulated_nmr_data$year |> min() == 1971\n#> [1] TRUE\n\nsimulated_nmr_data$year |> max() == 2020\n#> [1] TRUE\n\nsimulated_nmr_data$nmr |> min() >= 0\n#> [1] TRUE\n\nsimulated_nmr_data$nmr |> max() <= 1000\n#> [1] TRUE\n\nsimulated_nmr_data$nmr |> class() == \"numeric\"\n#> [1] TRUE"},{"path":"drinking-from-a-fire-hose.html","id":"acquire-2","chapter":"2 Drinking from a fire hose","heading":"2.4.3 Acquire","text":"UN Inter-agency Group Child Mortality Estimation (IGME) provides estimates NMR – https://childmortality.org/ – can download save.can take quick look get better sense . might interested dataset seems look like (using head() tail()), names columns (using names()).like clean names keep rows columns interested . Based plan, interested rows ‘Sex’ ‘Total,’ ‘Series Name’ ‘UN IGME estimate,’ ‘Geographic area’ one ‘Argentina,’ ‘Australia,’ ‘Canada,’ ‘Kenya,’ ‘Indicator’ ‘Neonatal mortality rate.’ interested just columns: ‘geographic_area,’ ‘time_period,’ ‘obs_value.’Finally, need fix two final aspects: class ‘time_period’ character need year, name ‘obs_value’ ‘nmr’ informative.Finally, can check dataset passes tests developed based simulated dataset.remains save nicely cleaned dataset.","code":"\n#### Acquire data ####\nraw_igme_data <- \n  read_csv(\n    file =\n      \"https://childmortality.org/wp-content/uploads/2021/09/UNIGME-2021.csv\",\n    show_col_types = FALSE) \n\nwrite_csv(\n  x = raw_igme_data, \n  file = \"igme.csv\"\n  )\nhead(raw_igme_data)\n#> # A tibble: 6 × 29\n#>   `Geographic area` Indicator         Sex   `Wealth Quinti…`\n#>   <chr>             <chr>             <chr> <chr>           \n#> 1 Afghanistan       Neonatal mortali… Total Total           \n#> 2 Afghanistan       Neonatal mortali… Total Total           \n#> 3 Afghanistan       Neonatal mortali… Total Total           \n#> 4 Afghanistan       Neonatal mortali… Total Total           \n#> 5 Afghanistan       Neonatal mortali… Total Total           \n#> 6 Afghanistan       Neonatal mortali… Total Total           \n#> # … with 25 more variables: `Series Name` <chr>,\n#> #   `Series Year` <chr>, `Regional group` <chr>,\n#> #   TIME_PERIOD <chr>, OBS_VALUE <dbl>,\n#> #   COUNTRY_NOTES <chr>, CONNECTION <lgl>,\n#> #   DEATH_CATEGORY <lgl>, CATEGORY <chr>,\n#> #   `Observation Status` <chr>, `Unit of measure` <chr>,\n#> #   `Series Category` <chr>, `Series Type` <chr>, …\nnames(raw_igme_data)\n#>  [1] \"Geographic area\"        \"Indicator\"             \n#>  [3] \"Sex\"                    \"Wealth Quintile\"       \n#>  [5] \"Series Name\"            \"Series Year\"           \n#>  [7] \"Regional group\"         \"TIME_PERIOD\"           \n#>  [9] \"OBS_VALUE\"              \"COUNTRY_NOTES\"         \n#> [11] \"CONNECTION\"             \"DEATH_CATEGORY\"        \n#> [13] \"CATEGORY\"               \"Observation Status\"    \n#> [15] \"Unit of measure\"        \"Series Category\"       \n#> [17] \"Series Type\"            \"STD_ERR\"               \n#> [19] \"REF_DATE\"               \"Age Group of Women\"    \n#> [21] \"Time Since First Birth\" \"DEFINITION\"            \n#> [23] \"INTERVAL\"               \"Series Method\"         \n#> [25] \"LOWER_BOUND\"            \"UPPER_BOUND\"           \n#> [27] \"STATUS\"                 \"YEAR_TO_ACHIEVE\"       \n#> [29] \"Model Used\"\ncleaned_igme_data <- \n  clean_names(raw_igme_data) |> \n  filter(sex == 'Total',\n         series_name == 'UN IGME estimate',\n         geographic_area %in% \n           c('Argentina', 'Australia', 'Canada', 'Kenya'),\n         indicator == 'Neonatal mortality rate') |> \n  select(geographic_area,\n         time_period,\n         obs_value)\n\nhead(cleaned_igme_data)\n#> # A tibble: 6 × 3\n#>   geographic_area time_period obs_value\n#>   <chr>           <chr>           <dbl>\n#> 1 Argentina       1970-06          24.9\n#> 2 Argentina       1971-06          24.7\n#> 3 Argentina       1972-06          24.6\n#> 4 Argentina       1973-06          24.6\n#> 5 Argentina       1974-06          24.5\n#> 6 Argentina       1975-06          24.1\ncleaned_igme_data <- \n  cleaned_igme_data |> \n  mutate(time_period = str_remove(time_period, \"-06\"),\n         time_period = as.integer(time_period)) |> \n  filter(time_period >= 1971) |> \n  rename(nmr = obs_value,\n         year = time_period,\n         country = geographic_area)\n\nhead(cleaned_igme_data)\n#> # A tibble: 6 × 3\n#>   country    year   nmr\n#>   <chr>     <int> <dbl>\n#> 1 Argentina  1971  24.7\n#> 2 Argentina  1972  24.6\n#> 3 Argentina  1973  24.6\n#> 4 Argentina  1974  24.5\n#> 5 Argentina  1975  24.1\n#> 6 Argentina  1976  23.3\n# Test the cleaned dataset\ncleaned_igme_data$country |> \n  unique() == c(\"Argentina\", \n                \"Australia\", \n                \"Canada\", \n                \"Kenya\")\n#> [1] TRUE TRUE TRUE TRUE\n\ncleaned_igme_data$country |> unique() |> length() == 4\n#> [1] TRUE\n\ncleaned_igme_data$year |> min() == 1971\n#> [1] TRUE\n\ncleaned_igme_data$year |> max() == 2020\n#> [1] TRUE\n\ncleaned_igme_data$nmr |> min() >= 0\n#> [1] TRUE\n\ncleaned_igme_data$nmr |> max() <= 1000\n#> [1] TRUE\n\ncleaned_igme_data$nmr |> class() == \"numeric\"\n#> [1] TRUE\nwrite_csv(\n  x = cleaned_igme_data, \n  file = \"cleaned_igme_data.csv\"\n  )"},{"path":"drinking-from-a-fire-hose.html","id":"explore-2","chapter":"2 Drinking from a fire hose","heading":"2.4.4 Explore","text":"like make graph estimated NMR using cleaned dataset. First, read dataset.can now make graph interested (Figure 2.9). interested showing NMR changed time difference countries.\nFigure 2.9: Neonatal Mortality Rate (NMR), Argentina, Australia, Canada, Kenya, (1971-2020)\n","code":"\n#### Explore ####\ncleaned_igme_data <- \n  read_csv(\n    file = \"cleaned_igme_data.csv\",\n    show_col_types = FALSE\n    )\ncleaned_igme_data |> \n  ggplot(aes(x = year, y = nmr, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Year\",\n       y = \"Neonatal Mortality Rate (NMR)\",\n       color = \"Country\") +\n  scale_color_brewer(palette = \"Set1\")"},{"path":"drinking-from-a-fire-hose.html","id":"communicate-2","chapter":"2 Drinking from a fire hose","heading":"2.4.5 Communicate","text":"point downloaded data, cleaned , wrote tests, made graph. typically need communicate done length. case, write paragraphs , , found.Neonatal mortality refers death occurs within first month life. particular, neonatal mortality rate (NMR) number neonatal deaths per 1,000 live births (M. Alexander Alkema 2018). obtain estimates NMR four countries—Argentina, Australia, Canada, China, Kenya—past fifty years.UN Inter-agency Group Child Mortality Estimation (IGME) provides estimates NMR website: https://childmortality.org/. downloaded estimates cleaned tidied dataset using statistical programming language R (R Core Team 2021).found considerable change estimated NMR time four countries interest (Figure 2.9). found 1970s tended associated reductions estimated NMR. Australia Canada estimated low NMR point remained 2020, slight improvements. estimates Argentina Kenya continued substantial reductions 2020. Data available 1990 China estimates show substantial reduction NMR, especially 1990s 2000s.results suggest considerable improvements estimated NMR time. worth emphasizing estimates NMR based statistical model underlying data. paradox data availability often high-quality data less easily available countries worse outcomes. instance, M. Alexander Alkema (2018) say ‘[t]large variability availability data neonatal mortality.’ conclusions subject model underpins estimates, quality underlying data independently verify either .","code":""},{"path":"drinking-from-a-fire-hose.html","id":"exercises-and-tutorial-1","chapter":"2 Drinking from a fire hose","heading":"2.5 Exercises and tutorial","text":"","code":""},{"path":"drinking-from-a-fire-hose.html","id":"exercises-1","chapter":"2 Drinking from a fire hose","heading":"2.5.1 Exercises","text":"Following Barrett (2021a), please write stack four five atomic habits implement week.one four challenges mitigating bias mentioned Hao (2019) (pick one)?\nUnknown unknowns.\nImperfect processes.\ndefinitions fairness.\nLack social context.\nDisinterest given profit considerations.\nUnknown unknowns.Imperfect processes.definitions fairness.Lack social context.Disinterest given profit considerations.dataset underpins Chambliss (1989) collected (pick one)?\nAugust 1983 August 1984\nJanuary 1983 August 1984\nJanuary 1983 January 1984\nAugust 1983 January 1984\nAugust 1983 August 1984January 1983 August 1984January 1983 January 1984August 1983 January 1984When Chambliss (1989) talks stratification, talking ?Chambliss (1989) define ‘excellence’ (pick one)?\nProlonged performance world-class level.\nOlympic medal winners.\nConsistent superiority performance.\nnational-level athletes.\nProlonged performance world-class level.Olympic medal winners.Consistent superiority performance.national-level athletes.Think following quote Chambliss (1989, 81) list three small skills activities help achieve excellence data science.Excellence mundane. Superlative performance really confluence dozens small skills activities, one learned stumbled upon, carefully drilled habit fitted together synthesized whole. nothing extraordinary super-human one actions; fact done consistently correctly, together, produce excellence.following arguments read_csv() readr (Wickham, Hester, Bryan 2021) (select apply)? (Hint: can access help function ?readr::read_csv().)\n‘all_cols’\n‘file’\n‘show_col_types’\n‘number’\n‘all_cols’‘file’‘show_col_types’‘number’used rpois() runif() draw Poisson Uniform distributions, respectively. following can used draw Normal Binomial distributions (select apply)?\nrnormal() rbinom()\nrnorm() rbinomial()\nrnormal() rbinomial()\nrnorm() rbinom()\nrnormal() rbinom()rnorm() rbinomial()rnormal() rbinomial()rnorm() rbinom()result sample(x = letters, size = 2) seed set ‘853?’ seed set ‘1234’ (pick one)?\n‘“” “q”’ ‘“p” “v”’\n‘“e” “l”’ ‘“e” “r”’\n‘“” “q”’ ‘“e” “r”’\n‘“e” “l”’ ‘“p” “v”’\n‘“” “q”’ ‘“p” “v”’‘“e” “l”’ ‘“e” “r”’‘“” “q”’ ‘“e” “r”’‘“e” “l”’ ‘“p” “v”’function provides recommended citation cite R (pick one)?\ncite('R').\ncite().\ncitation('R').\ncitation().\ncite('R').cite().citation('R').citation().get citation information opendatatoronto (pick one)?\ncite()\ncitation()\ncite(‘opendatatoronto’)\ncitation(‘opendatatoronto’)\ncite()citation()cite(‘opendatatoronto’)citation(‘opendatatoronto’)argument needs changed change headings kable() knitr (Xie 2021) (pick one)?\n‘booktabs’\n‘col.names’\n‘digits’\n‘linesep’\n‘caption’\n‘booktabs’‘col.names’‘digits’‘linesep’‘caption’function used update packages (pick one)?\nupdate.packages()\nupgrade.packages()\nrevise.packages()\nrenovate.packages()\nupdate.packages()upgrade.packages()revise.packages()renovate.packages()features might typically expect column claimed year (select apply)?\nclass ‘character.’\nnegative numbers.\nletters column.\nentry four digits.\nclass ‘character.’negative numbers.letters column.entry four digits.","code":""},{"path":"drinking-from-a-fire-hose.html","id":"tutorial-1","chapter":"2 Drinking from a fire hose","heading":"2.5.2 Tutorial","text":"Please pick either seed-growing, hair-length, example Chapter 1. already started record data, probably much . First sketch example dataset look like. Please use sample() create tibble twelve weeks’ worth simulated data.Pretend dataset just generated actual data end . Please write page communicate , , found.Reflecting Chambliss (1989), please write page stratification excellence relates using programming languages, R Python, data science.","code":""},{"path":"r-essentials.html","id":"r-essentials","chapter":"3 R essentials","heading":"3 R essentials","text":"Required materialRead Kitchen Counter Observatory, (Healy 2020)Read R Data Science, Chapter 5 ‘Data transformation,’ (Wickham Grolemund 2017)Read Data Feminism, Chapter 6 ‘Numbers Don’t Speak ,’ (D’Ignazio Klein 2020)Key concepts skillsUnderstanding foundational aspects R R Studio.able use key dplyr verbs.Know fundamentals class manipulate .Ability simulate data.Ability make graphs ggplot2.Comfort aspects tidyverse including importing data, dataset manipulation, string manipulation, factors.Develop strategies things work.Key librariesforcats (Wickham 2020a)ggplot2 (Wickham 2016)haven (Wickham Miller 2020)stringr (Wickham 2019e)tidyr (Wickham 2021c)tidyverse (Wickham et al. 2019a)Key functions| ‘’& ‘’|> ‘pipe’$ ‘extract’.character().integer()c()citation()class()dplyr::arrange()dplyr::case_when()dplyr::count()dplyr::filter()dplyr::group_by()dplyr::if_else()dplyr::left_join()dplyr::mutate()dplyr::pull()dplyr::rename()dplyr::select()dplyr::slice()dplyr::summarise()forcats::as_factor()forcats::fct_relevel()function()ggplot2::facet_wrap()ggplot2::geom_density()ggplot2::geom_histogram()ggplot2::geom_point()ggplot2::ggplot()head()janitor::clean_names()library()lubridate::ymd()max()mean()print()readr::read_csv()rnorm()round()runif()sample()set.seed()stringr::str_detect()stringr::str_replace()stringr::str_squish()sum()tibble::tibble()tidyr::pivot_longer()tidyr::pivot_wider()","code":""},{"path":"r-essentials.html","id":"background","chapter":"3 R essentials","heading":"3.1 Background","text":"chapter focus foundational skills needed use statistical programming language R (R Core Team 2021) tell stories data. may make sense first, skills approaches often use. initially just go chapter quickly, noting aspects understand. come back chapter time time continue rest book. way see various bits fit context.R open-source language statistical programming. can download R free Comprehensive R Archive Network (CRAN): https://cran.r-project.org. R Studio Integrated Development Environment (IDE) R makes language easier use can downloaded free: https://www.rstudio.com/products/rstudio/.past ten years , characterized increased use tidyverse. ‘…opinionated collection R packages designed data science. packages share underlying design philosophy, grammar, data structures’ (Wickham 2020b). three distinctions clear : original R language, typically referred ‘base’; ‘tidyverse’ coherent collection packages build top base, packages.Essentially everything can tidyverse, can also base. , tidyverse built especially data science often easier use tidyverse, especially learning. Additionally, often everything can tidyverse, can also packages. , tidyverse coherent collection packages, often easier use tidyverse, , especially learning. Eventually cases makes sense trade-convenience coherence tidyverse features base packages. Indeed, see various points later book. instance, tidyverse can slow, one needs import thousands CSVs can make sense switch away read_csv(). appropriate use base non-tidyverse packages, even languages, rather dogmatic insistence particular solution, sign intellectual maturity.Central use statistical programming language R data, data use humans heart . Sometimes, dealing human-centered data way can numbing effect, results -generalization, potentially problematic work. Another sign intellectual maturity opposite effect.practice, find far distancing questions meaning, quantitative data forces confront . numbers draw . Working data like unending exercise humility, constant compulsion think can see, standing invitation understand measures really capture—mean, .Healy (2020)","code":""},{"path":"r-essentials.html","id":"broader-impacts","chapter":"3 R essentials","heading":"3.2 Broader impacts","text":"“shouldn’t think societal impact work ’s hard people can us” really bad argument. stopped CV [computer vision] research saw impact work . loved work military applications privacy concerns eventually became impossible ignore. basically facial recognition work get published took Broader Impacts sections seriously. almost upside enormous downside risk. fair though lot humility . grad school bought myth science apolitical research objectively moral good matter subject .Joe Redmon, 20 February 2020Although term ‘data science’ ubiquitous academia, industry, even generally, difficult define. One deliberately antagonistic definition data science ‘[t]inhumane reduction humanity can counted’ (Keyes 2019). purposefully controversial, definition highlights one reason increased demand data science quantitative methods past decade—individuals behavior now heart . Many techniques around many decades, makes popular now human focus.Unfortunately, even though much work may focused individuals, issues privacy consent, ethical concerns broadly, rarely seem front mind. exceptions, general, even time claiming AI, machine learning, data science going revolutionize society, consideration types issues appears largely treated something nice , rather something may like think embrace revolution.part, types issues new. sciences, considerable recent ethical consideration around CRISPR technology gene editing (Brokowski Adli 2019), earlier time similar conversations , instance, Wernher von Braun allowed building rockets US (Neufeld 2002). medicine, course, concerns front--mind time (Association Medicine 1848). Data science seems determined Tuskegee-moment rather think , proactively deal appropriately , issues, based experiences fields.said, evidence data scientists beginning concerned ethics surrounding practice. instance, NeurIPS, prestigious machine learning conference, required statement ethics accompany submissions since 2020.order provide balanced perspective, authors required include statement potential broader impact work, including ethical aspects future societal consequences. Authors take care discuss positive negative outcomes.NeurIPS 2020 Conference Call PapersThe purpose ethical consideration concern broader impact data science prescriptively rule things , provide opportunity raise issues paramount. variety data science applications, relative youth field, speed change, mean considerations sometimes knowingly set aside, acceptable rest field. contrasts fields science, medicine, engineering, accounting. Possibly fields self-aware (Figure 3.1).\nFigure 3.1: Probability, XKCD\n","code":""},{"path":"r-essentials.html","id":"r-r-studio-and-r-studio-cloud","chapter":"3 R essentials","heading":"3.3 R, R Studio, and R Studio Cloud","text":"R R Studio complementary, thing. Liza Bolton, Assistant Professor, Teaching Stream, University Toronto explains relationship analogy R like engine R Studio like car. Although us use car engine directly, us use car interact engine.","code":""},{"path":"r-essentials.html","id":"r","chapter":"3 R essentials","heading":"3.3.1 R","text":"R – https://www.r-project.org/ – open-source free programming language focused general statistics. Free context refer price zero, instead freedom creators give users largely want , although also price zero. contrast open-source programming language designed general purpose, Python, open-source programming language focused probability, Stan. created Ross Ihaka Robert Gentleman University Auckland 1990s, traces provenance S, developed Bell Labs 1970s. maintained R Core Team changes ‘base’ code occur methodically concern given variety different priorities.Many people build stable base, extend capabilities R better quickly suit needs. creating packages. Typically, although always, package collection R code, mostly functions, allows us easily things want . packages managed repositories CRAN Bioconductor.want use package first need install computer, need load want use . Di Cook, Professor Business Analytics Monash University, describes analogous lightbulb. want light house, first need fit lightbulb, need turn switch . Installing package, say, install.packages(\"tidyverse\"), akin fitting lightbulb socket—need lightbulb. time want light need turn switch lightbulb, R packages case, means calling library, say, library(tidyverse).Shoulders giants Dr Di Cook Professor Business Analytics Monash University. taking PhD statistics Rutgers University 1993 focused statistical graphics, appointed assistant professor Iowa State University, promoted full professor 2005, 2015 moved Monash. One area research data visualisation, especially interactive dynamic graphics. One particularly important paper Buja, Cook, Swayne (1996) proposes taxonomy interactive data visualization associated software XGobi.install package computer (, need per computer) use install.packages().want use package, use library().downloaded , can open R use directly. primarily designed interacted command line. functional, can useful richer environment command line provides. particular, can useful install Integrated Development Environment (IDE), application brings together various bits pieces used often. One common IDE R R Studio, although others Visual Studio also used.","code":"\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)"},{"path":"r-essentials.html","id":"r-studio","chapter":"3 R essentials","heading":"3.3.2 R Studio","text":"R Studio distinct R, different entities. R Studio builds top R make easier use R. way one use internet command line, folks use browser Chrome, Firefox, Safari.R Studio free sense pay . also free sense able take code, modify , distribute code. important recognize R Studio company possible current situation change. can downloaded: https://www.rstudio.com/products/rstudio/.open R Studio look like Figure 3.2.\nFigure 3.2: Opening R Studio first time\nleft pane console can type execute R code line line. Try 2+2 clicking next prompt ‘>,’ typing ‘2+2,’ pressing ‘return/enter.’pane top right information environment. instance, create variables list names properties appear . Try type following code, replacing name name, next prompt, press enter:notice new value environment pane variable name value.pane bottom right file manager. moment just two files: R History file R Project file. get later, now create save file.Run following code, without worrying much details now. see new ‘.rds’ file list files.","code":"\n2 + 2\n#> [1] 4\nmy_name <- \"Rohan\"\nsaveRDS(object = my_name, file = \"my_first_file.rds\")"},{"path":"r-essentials.html","id":"r-studio-cloud","chapter":"3 R essentials","heading":"3.3.3 R Studio Cloud","text":"can download R Studio computer, initially use R Studio Cloud: https://rstudio.cloud/. online version provided R Studio. use can focus getting comfortable R R Studio environment consistent. way worry computer installation permissions, amongst things.free version R Studio Cloud free ‘financial cost.’ trade-powerful, sometimes slow, purposes getting started enough.","code":""},{"path":"r-essentials.html","id":"getting-started","chapter":"3 R essentials","heading":"3.4 Getting started","text":"now start going code. important actively write .working line--line console fine, easier write whole script can run. making R Script (‘File’ -> ‘New File’ -> ‘R Script’). console pane fall bottom left R Script open top left. write code get Australian federal politicians construct small table genders prime ministers. code make sense stage, just type get habit run . run whole script, can click ‘Run’ can highlight certain lines click ‘Run’ just run lines.can see , end 2021, one female prime minister (Julia Gillard), 29 prime ministers maleOne critical operator programming ‘pipe’: |>. read ‘.’ takes output line code uses first input next line code. makes code easier read.idea pipe take dataset, something . used earlier example. Another example follows look first six lines dataset piping head(). Notice head() explicitly take arguments example. knows data display pipe implicitly.can save R Script ‘my_first_r_script.R’ (‘File’ -> ‘Save ’). point, workspace look something like Figure 3.3.\nFigure 3.3: running R Script\nOne thing aware R Studio Cloud workspace essentially new computer. , need install package want use workspace. instance, can use tidyverse, need install install.packages(\"tidyverse\"). contrasts using one’s computer.final notes R Studio Cloud:Australian politician’s example, got data website GitHub using R package, can get data workspace local computer variety ways. One way use ‘upload’ button ‘Files’ panel.R Studio Cloud allows degree collaboration. instance, can give someone else access workspace create. useful collaborating assignment, although quite full featured yet workspace time, contrast , say, Google Docs.variety weaknesses R Studio Cloud, particular RAM limits. Additionally, like web application, things break time time go .","code":"\n# Install the packages that we need\ninstall.packages(\"tidyverse\")\ninstall.packages(\"AustralianPoliticians\")\n# Load the packages that we need to use this time\nlibrary(tidyverse)\nlibrary(AustralianPoliticians)\n\n# Make a table of the counts of genders of the prime ministers\nAustralianPoliticians::get_auspol('all') |> \n  as_tibble() |> \n  filter(wasPrimeMinister == 1) |> \n  count(gender)\n#> # A tibble: 2 × 2\n#>   gender     n\n#>   <chr>  <int>\n#> 1 female     1\n#> 2 male      29\nAustralianPoliticians::get_auspol('all') |> \n  head()\n#> # A tibble: 6 × 20\n#>   uniqueID   surname allOtherNames      firstName commonName\n#>   <chr>      <chr>   <chr>              <chr>     <chr>     \n#> 1 Abbott1859 Abbott  Richard Hartley S… Richard   <NA>      \n#> 2 Abbott1869 Abbott  Percy Phipps       Percy     <NA>      \n#> 3 Abbott1877 Abbott  Macartney          Macartney Mac       \n#> 4 Abbott1886 Abbott  Charles Lydiard A… Charles   Aubrey    \n#> 5 Abbott1891 Abbott  Joseph Palmer      Joseph    <NA>      \n#> 6 Abbott1957 Abbott  Anthony John       Anthony   Tony      \n#> # … with 15 more variables: displayName <chr>,\n#> #   earlierOrLaterNames <chr>, title <chr>, gender <chr>,\n#> #   birthDate <date>, birthYear <dbl>, birthPlace <chr>,\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>, wikidataID <chr>,\n#> #   wikipedia <chr>, adb <chr>, comments <chr>"},{"path":"r-essentials.html","id":"the-dplyr-verbs","chapter":"3 R essentials","heading":"3.5 The dplyr verbs","text":"One key packages use tidyverse (Wickham et al. 2019b). tidyverse actually package packages, means install tidyverse, actually install whole bunch different packages. key package tidyverse terms manipulating data dplyr (Wickham et al. 2020).five dplyr functions regularly used, now go . commonly referred dplyr verbs.select()filter()arrange()mutate()summarise() equally summarize()also cover group_by(), count() closely related.already installed tidyverse, just need load .begin using data Australian politicians AustralianPoliticians package (R. Alexander Hodgetts 2021).","code":"\nlibrary(tidyverse)\nlibrary(AustralianPoliticians)\n\naustralian_politicians <- \n  get_auspol('all')\n\nhead(australian_politicians)\n#> # A tibble: 6 × 20\n#>   uniqueID   surname allOtherNames      firstName commonName\n#>   <chr>      <chr>   <chr>              <chr>     <chr>     \n#> 1 Abbott1859 Abbott  Richard Hartley S… Richard   <NA>      \n#> 2 Abbott1869 Abbott  Percy Phipps       Percy     <NA>      \n#> 3 Abbott1877 Abbott  Macartney          Macartney Mac       \n#> 4 Abbott1886 Abbott  Charles Lydiard A… Charles   Aubrey    \n#> 5 Abbott1891 Abbott  Joseph Palmer      Joseph    <NA>      \n#> 6 Abbott1957 Abbott  Anthony John       Anthony   Tony      \n#> # … with 15 more variables: displayName <chr>,\n#> #   earlierOrLaterNames <chr>, title <chr>, gender <chr>,\n#> #   birthDate <date>, birthYear <dbl>, birthPlace <chr>,\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>, wikidataID <chr>,\n#> #   wikipedia <chr>, adb <chr>, comments <chr>"},{"path":"r-essentials.html","id":"select","chapter":"3 R essentials","heading":"3.5.1 select()","text":"use select() pick particular columns dataset. instance, might like select ‘firstName’ column.R, many ways things. Sometimes different ways thing, times different ways almost thing. instance, another way pick particular column dataset use ‘extract’ operator ‘$.’ base, opposed select() tidyverse.two appear similar—pick ‘firstName’ column—differ class return. sake completeness, combine select() pull() get class output used extract operator.can also use select() remove columns, negating column name.Finally, can select() based conditions. instance, can select() columns start , say, ‘birth.’variety similar ‘selection helpers’ including starts_with(), ends_with(), contains(). information available help page select() can accessed running ?select().point, use select() reduce width dataset.","code":"\naustralian_politicians |> \n  select(firstName) |> \n  head()\n#> # A tibble: 6 × 1\n#>   firstName\n#>   <chr>    \n#> 1 Richard  \n#> 2 Percy    \n#> 3 Macartney\n#> 4 Charles  \n#> 5 Joseph   \n#> 6 Anthony\naustralian_politicians$firstName |> \n  head()\n#> [1] \"Richard\"   \"Percy\"     \"Macartney\" \"Charles\"  \n#> [5] \"Joseph\"    \"Anthony\"\naustralian_politicians |> \n  select(firstName) |> \n  pull() |> \n  head()\n#> [1] \"Richard\"   \"Percy\"     \"Macartney\" \"Charles\"  \n#> [5] \"Joseph\"    \"Anthony\"\naustralian_politicians |> \n  select(-firstName) |> \n  head()\n#> # A tibble: 6 × 19\n#>   uniqueID   surname allOtherNames    commonName displayName\n#>   <chr>      <chr>   <chr>            <chr>      <chr>      \n#> 1 Abbott1859 Abbott  Richard Hartley… <NA>       Abbott, Ri…\n#> 2 Abbott1869 Abbott  Percy Phipps     <NA>       Abbott, Pe…\n#> 3 Abbott1877 Abbott  Macartney        Mac        Abbott, Mac\n#> 4 Abbott1886 Abbott  Charles Lydiard… Aubrey     Abbott, Au…\n#> 5 Abbott1891 Abbott  Joseph Palmer    <NA>       Abbott, Jo…\n#> 6 Abbott1957 Abbott  Anthony John     Tony       Abbott, To…\n#> # … with 14 more variables: earlierOrLaterNames <chr>,\n#> #   title <chr>, gender <chr>, birthDate <date>,\n#> #   birthYear <dbl>, birthPlace <chr>, deathDate <date>,\n#> #   member <dbl>, senator <dbl>, wasPrimeMinister <dbl>,\n#> #   wikidataID <chr>, wikipedia <chr>, adb <chr>,\n#> #   comments <chr>\naustralian_politicians |> \n  select(starts_with(\"birth\")) |> \n  head()\n#> # A tibble: 6 × 3\n#>   birthDate  birthYear birthPlace  \n#>   <date>         <dbl> <chr>       \n#> 1 NA              1859 Bendigo     \n#> 2 1869-05-14        NA Hobart      \n#> 3 1877-07-03        NA Murrurundi  \n#> 4 1886-01-04        NA St Leonards \n#> 5 1891-10-18        NA North Sydney\n#> 6 1957-11-04        NA London\naustralian_politicians <-\n  australian_politicians |>\n  select(uniqueID,\n         surname,\n         firstName,\n         gender,\n         birthDate,\n         birthYear,\n         deathDate,\n         member,\n         senator,\n         wasPrimeMinister)\n\naustralian_politicians |> head()\n#> # A tibble: 6 × 10\n#>   uniqueID   surname firstName gender birthDate  birthYear\n#>   <chr>      <chr>   <chr>     <chr>  <date>         <dbl>\n#> 1 Abbott1859 Abbott  Richard   male   NA              1859\n#> 2 Abbott1869 Abbott  Percy     male   1869-05-14        NA\n#> 3 Abbott1877 Abbott  Macartney male   1877-07-03        NA\n#> 4 Abbott1886 Abbott  Charles   male   1886-01-04        NA\n#> 5 Abbott1891 Abbott  Joseph    male   1891-10-18        NA\n#> 6 Abbott1957 Abbott  Anthony   male   1957-11-04        NA\n#> # … with 4 more variables: deathDate <date>, member <dbl>,\n#> #   senator <dbl>, wasPrimeMinister <dbl>"},{"path":"r-essentials.html","id":"filter","chapter":"3 R essentials","heading":"3.5.2 filter()","text":"use filter() pick particular rows dataset. instance, might interested politicians became prime minister.also give filter() two conditions. instance, look politicians become prime minister named Joseph, using ‘’ operator ‘&.’get result use comma instead ampersand.Similarly, look politicians named, say, Myles Ruth using ‘’ operator ‘|’also pipe result. instance pipe filter() select().happen know particular row number interest filter() particular row. instance, say row 853 interest.also dedicated function , slice().may seem somewhat esoteric, especially useful like remove particular row using negation, duplicate specific rows. instance, remove first row.also , say, keep first three rows.Finally, duplicate first two rows.","code":"\naustralian_politicians |> \n  filter(wasPrimeMinister == 1)\n#> # A tibble: 30 × 10\n#>    uniqueID    surname firstName gender birthDate  birthYear\n#>    <chr>       <chr>   <chr>     <chr>  <date>         <dbl>\n#>  1 Abbott1957  Abbott  Anthony   male   1957-11-04        NA\n#>  2 Barton1849  Barton  Edmund    male   1849-01-18        NA\n#>  3 Bruce1883   Bruce   Stanley   male   1883-04-15        NA\n#>  4 Chifley1885 Chifley Joseph    male   1885-09-22        NA\n#>  5 Cook1860    Cook    Joseph    male   1860-12-07        NA\n#>  6 Curtin1885  Curtin  John      male   1885-01-08        NA\n#>  7 Deakin1856  Deakin  Alfred    male   1856-08-03        NA\n#>  8 Fadden1894  Fadden  Arthur    male   1894-04-13        NA\n#>  9 Fisher1862  Fisher  Andrew    male   1862-08-29        NA\n#> 10 Forde1890   Forde   Francis   male   1890-07-18        NA\n#> # … with 20 more rows, and 4 more variables:\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>\naustralian_politicians |> \n  filter(wasPrimeMinister == 1 & firstName == \"Joseph\")\n#> # A tibble: 3 × 10\n#>   uniqueID    surname firstName gender birthDate  birthYear\n#>   <chr>       <chr>   <chr>     <chr>  <date>         <dbl>\n#> 1 Chifley1885 Chifley Joseph    male   1885-09-22        NA\n#> 2 Cook1860    Cook    Joseph    male   1860-12-07        NA\n#> 3 Lyons1879   Lyons   Joseph    male   1879-09-15        NA\n#> # … with 4 more variables: deathDate <date>, member <dbl>,\n#> #   senator <dbl>, wasPrimeMinister <dbl>\naustralian_politicians |> \n  filter(wasPrimeMinister == 1, firstName == \"Joseph\")\n#> # A tibble: 3 × 10\n#>   uniqueID    surname firstName gender birthDate  birthYear\n#>   <chr>       <chr>   <chr>     <chr>  <date>         <dbl>\n#> 1 Chifley1885 Chifley Joseph    male   1885-09-22        NA\n#> 2 Cook1860    Cook    Joseph    male   1860-12-07        NA\n#> 3 Lyons1879   Lyons   Joseph    male   1879-09-15        NA\n#> # … with 4 more variables: deathDate <date>, member <dbl>,\n#> #   senator <dbl>, wasPrimeMinister <dbl>\naustralian_politicians |> \n  filter(firstName == \"Myles\" | firstName == \"Ruth\")\n#> # A tibble: 3 × 10\n#>   uniqueID     surname firstName gender birthDate  birthYear\n#>   <chr>        <chr>   <chr>     <chr>  <date>         <dbl>\n#> 1 Coleman1931  Coleman Ruth      female 1931-09-27        NA\n#> 2 Ferricks1875 Ferric… Myles     male   1875-11-12        NA\n#> 3 Webber1965   Webber  Ruth      female 1965-03-24        NA\n#> # … with 4 more variables: deathDate <date>, member <dbl>,\n#> #   senator <dbl>, wasPrimeMinister <dbl>\naustralian_politicians |> \n  filter(firstName == \"Ruth\" | firstName == \"Myles\") |> \n  select(firstName, surname)\n#> # A tibble: 3 × 2\n#>   firstName surname \n#>   <chr>     <chr>   \n#> 1 Ruth      Coleman \n#> 2 Myles     Ferricks\n#> 3 Ruth      Webber\naustralian_politicians |> \n  filter(row_number() == 853)\n#> # A tibble: 1 × 10\n#>   uniqueID     surname firstName gender birthDate  birthYear\n#>   <chr>        <chr>   <chr>     <chr>  <date>         <dbl>\n#> 1 Jakobsen1947 Jakobs… Carolyn   female 1947-09-11        NA\n#> # … with 4 more variables: deathDate <date>, member <dbl>,\n#> #   senator <dbl>, wasPrimeMinister <dbl>\naustralian_politicians |> \n  slice(853)\n#> # A tibble: 1 × 10\n#>   uniqueID     surname firstName gender birthDate  birthYear\n#>   <chr>        <chr>   <chr>     <chr>  <date>         <dbl>\n#> 1 Jakobsen1947 Jakobs… Carolyn   female 1947-09-11        NA\n#> # … with 4 more variables: deathDate <date>, member <dbl>,\n#> #   senator <dbl>, wasPrimeMinister <dbl>\naustralian_politicians |> \n  slice(-1)\n#> # A tibble: 1,782 × 10\n#>    uniqueID    surname firstName gender birthDate  birthYear\n#>    <chr>       <chr>   <chr>     <chr>  <date>         <dbl>\n#>  1 Abbott1869  Abbott  Percy     male   1869-05-14        NA\n#>  2 Abbott1877  Abbott  Macartney male   1877-07-03        NA\n#>  3 Abbott1886  Abbott  Charles   male   1886-01-04        NA\n#>  4 Abbott1891  Abbott  Joseph    male   1891-10-18        NA\n#>  5 Abbott1957  Abbott  Anthony   male   1957-11-04        NA\n#>  6 Abel1939    Abel    John      male   1939-06-25        NA\n#>  7 Abetz1958   Abetz   Eric      male   1958-01-25        NA\n#>  8 Adams1943   Adams   Judith    female 1943-04-11        NA\n#>  9 Adams1951   Adams   Dick      male   1951-04-29        NA\n#> 10 Adamson1857 Adamson John      male   1857-02-18        NA\n#> # … with 1,772 more rows, and 4 more variables:\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>\naustralian_politicians |> \n  slice(1:3)\n#> # A tibble: 3 × 10\n#>   uniqueID   surname firstName gender birthDate  birthYear\n#>   <chr>      <chr>   <chr>     <chr>  <date>         <dbl>\n#> 1 Abbott1859 Abbott  Richard   male   NA              1859\n#> 2 Abbott1869 Abbott  Percy     male   1869-05-14        NA\n#> 3 Abbott1877 Abbott  Macartney male   1877-07-03        NA\n#> # … with 4 more variables: deathDate <date>, member <dbl>,\n#> #   senator <dbl>, wasPrimeMinister <dbl>\naustralian_politicians |> \n  slice(1:2, 1:n())\n#> # A tibble: 1,785 × 10\n#>    uniqueID   surname firstName gender birthDate  birthYear\n#>    <chr>      <chr>   <chr>     <chr>  <date>         <dbl>\n#>  1 Abbott1859 Abbott  Richard   male   NA              1859\n#>  2 Abbott1869 Abbott  Percy     male   1869-05-14        NA\n#>  3 Abbott1859 Abbott  Richard   male   NA              1859\n#>  4 Abbott1869 Abbott  Percy     male   1869-05-14        NA\n#>  5 Abbott1877 Abbott  Macartney male   1877-07-03        NA\n#>  6 Abbott1886 Abbott  Charles   male   1886-01-04        NA\n#>  7 Abbott1891 Abbott  Joseph    male   1891-10-18        NA\n#>  8 Abbott1957 Abbott  Anthony   male   1957-11-04        NA\n#>  9 Abel1939   Abel    John      male   1939-06-25        NA\n#> 10 Abetz1958  Abetz   Eric      male   1958-01-25        NA\n#> # … with 1,775 more rows, and 4 more variables:\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>"},{"path":"r-essentials.html","id":"arrange","chapter":"3 R essentials","heading":"3.5.3 arrange()","text":"use arrange() change order dataset based values particular columns. instance, arrange politicians birthday.modify arrange() desc() change ascending descending order.arrange based one column. instance, two politicians first name, arrange based birthday.achieve result piping two instances arrange().use arrange() important clear precedence. instance, changing birthday first name give different arrangement.nice way arrange variety columns use across(). enables us use ‘selection helpers’ starts_with() mentioned association select().","code":"\naustralian_politicians |> \n  arrange(birthDate)\n#> # A tibble: 1,783 × 10\n#>    uniqueID    surname firstName gender birthDate  birthYear\n#>    <chr>       <chr>   <chr>     <chr>  <date>         <dbl>\n#>  1 Braddon1829 Braddon Edward    male   1829-06-11        NA\n#>  2 Ferguson18… Fergus… John      male   1830-03-15        NA\n#>  3 Zeal1830    Zeal    William   male   1830-12-05        NA\n#>  4 Fraser1832  Fraser  Simon     male   1832-08-21        NA\n#>  5 Groom1833   Groom   William   male   1833-03-09        NA\n#>  6 Sargood1834 Sargood Frederick male   1834-05-30        NA\n#>  7 Fysh1835    Fysh    Philip    male   1835-03-01        NA\n#>  8 Playford18… Playfo… Thomas    male   1837-11-26        NA\n#>  9 Solomon1839 Solomon Elias     male   1839-09-02        NA\n#> 10 McLean1840  McLean  Allan     male   1840-02-03        NA\n#> # … with 1,773 more rows, and 4 more variables:\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>\naustralian_politicians |> \n  arrange(desc(birthDate))\n#> # A tibble: 1,783 × 10\n#>    uniqueID    surname firstName gender birthDate  birthYear\n#>    <chr>       <chr>   <chr>     <chr>  <date>         <dbl>\n#>  1 SteeleJohn… Steele… Jordon    male   1994-10-14        NA\n#>  2 Chandler19… Chandl… Claire    female 1990-06-01        NA\n#>  3 Roy1990     Roy     Wyatt     male   1990-05-22        NA\n#>  4 Thompson19… Thomps… Phillip   male   1988-05-07        NA\n#>  5 Paterson19… Paters… James     male   1987-11-21        NA\n#>  6 Burns1987   Burns   Joshua    male   1987-02-06        NA\n#>  7 Smith1986   Smith   Marielle  female 1986-12-30        NA\n#>  8 KakoschkeM… Kakosc… Skye      female 1985-12-19        NA\n#>  9 Simmonds19… Simmon… Julian    male   1985-08-29        NA\n#> 10 Gorman1984  Gorman  Patrick   male   1984-12-12        NA\n#> # … with 1,773 more rows, and 4 more variables:\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>\naustralian_politicians |> \n  arrange(firstName, birthDate)\n#> # A tibble: 1,783 × 10\n#>    uniqueID    surname firstName gender birthDate  birthYear\n#>    <chr>       <chr>   <chr>     <chr>  <date>         <dbl>\n#>  1 Blain1894   Blain   Adair     male   1894-11-21        NA\n#>  2 Dein1889    Dein    Adam      male   1889-03-04        NA\n#>  3 Armstrong1… Armstr… Adam      male   1909-07-01        NA\n#>  4 Bandt1972   Bandt   Adam      male   1972-03-11        NA\n#>  5 Ridgeway19… Ridgew… Aden      male   1962-09-18        NA\n#>  6 Bennett1933 Bennett Adrian    male   1933-01-21        NA\n#>  7 Gibson1935  Gibson  Adrian    male   1935-11-03        NA\n#>  8 Wynne1850   Wynne   Agar      male   1850-01-15        NA\n#>  9 Robertson1… Robert… Agnes     female 1882-07-31        NA\n#> 10 Pittard1902 Pittard Alan      male   1902-11-15        NA\n#> # … with 1,773 more rows, and 4 more variables:\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>\naustralian_politicians |> \n  arrange(birthDate) |> \n  arrange(firstName)\n#> # A tibble: 1,783 × 10\n#>    uniqueID    surname firstName gender birthDate  birthYear\n#>    <chr>       <chr>   <chr>     <chr>  <date>         <dbl>\n#>  1 Blain1894   Blain   Adair     male   1894-11-21        NA\n#>  2 Dein1889    Dein    Adam      male   1889-03-04        NA\n#>  3 Armstrong1… Armstr… Adam      male   1909-07-01        NA\n#>  4 Bandt1972   Bandt   Adam      male   1972-03-11        NA\n#>  5 Ridgeway19… Ridgew… Aden      male   1962-09-18        NA\n#>  6 Bennett1933 Bennett Adrian    male   1933-01-21        NA\n#>  7 Gibson1935  Gibson  Adrian    male   1935-11-03        NA\n#>  8 Wynne1850   Wynne   Agar      male   1850-01-15        NA\n#>  9 Robertson1… Robert… Agnes     female 1882-07-31        NA\n#> 10 Pittard1902 Pittard Alan      male   1902-11-15        NA\n#> # … with 1,773 more rows, and 4 more variables:\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>\naustralian_politicians |> \n  arrange(birthYear, firstName)\n#> # A tibble: 1,783 × 10\n#>    uniqueID    surname firstName gender birthDate birthYear\n#>    <chr>       <chr>   <chr>     <chr>  <date>        <dbl>\n#>  1 Edwards1842 Edwards Richard   male   NA             1842\n#>  2 Sawers1844  Sawers  William   male   NA             1844\n#>  3 Barker1846  Barker  Stephen   male   NA             1846\n#>  4 Corser1852  Corser  Edward    male   NA             1852\n#>  5 Lee1856     Lee     Henry     male   NA             1856\n#>  6 Grant1857   Grant   John      male   NA             1857\n#>  7 Palmer1859  Palmer  Albert    male   NA             1859\n#>  8 Riley1859   Riley   Edward    male   NA             1859\n#>  9 Abbott1859  Abbott  Richard   male   NA             1859\n#> 10 Kennedy1860 Kennedy Thomas    male   NA             1860\n#> # … with 1,773 more rows, and 4 more variables:\n#> #   deathDate <date>, member <dbl>, senator <dbl>,\n#> #   wasPrimeMinister <dbl>\naustralian_politicians |> \n  arrange(across(c(firstName, birthYear))) |> \n  head()\n#> # A tibble: 6 × 10\n#>   uniqueID     surname firstName gender birthDate  birthYear\n#>   <chr>        <chr>   <chr>     <chr>  <date>         <dbl>\n#> 1 Blain1894    Blain   Adair     male   1894-11-21        NA\n#> 2 Armstrong19… Armstr… Adam      male   1909-07-01        NA\n#> 3 Bandt1972    Bandt   Adam      male   1972-03-11        NA\n#> 4 Dein1889     Dein    Adam      male   1889-03-04        NA\n#> 5 Ridgeway1962 Ridgew… Aden      male   1962-09-18        NA\n#> 6 Bennett1933  Bennett Adrian    male   1933-01-21        NA\n#> # … with 4 more variables: deathDate <date>, member <dbl>,\n#> #   senator <dbl>, wasPrimeMinister <dbl>\n\naustralian_politicians |> \n  arrange(across(starts_with('birth'))) |> \n  head()\n#> # A tibble: 6 × 10\n#>   uniqueID     surname firstName gender birthDate  birthYear\n#>   <chr>        <chr>   <chr>     <chr>  <date>         <dbl>\n#> 1 Braddon1829  Braddon Edward    male   1829-06-11        NA\n#> 2 Ferguson1830 Fergus… John      male   1830-03-15        NA\n#> 3 Zeal1830     Zeal    William   male   1830-12-05        NA\n#> 4 Fraser1832   Fraser  Simon     male   1832-08-21        NA\n#> 5 Groom1833    Groom   William   male   1833-03-09        NA\n#> 6 Sargood1834  Sargood Frederick male   1834-05-30        NA\n#> # … with 4 more variables: deathDate <date>, member <dbl>,\n#> #   senator <dbl>, wasPrimeMinister <dbl>"},{"path":"r-essentials.html","id":"mutate","chapter":"3 R essentials","heading":"3.5.4 mutate()","text":"use mutate() want make new column. instance, perhaps want make new column 1 person member senator 0 otherwise. say new column denote politicians served upper lower house.use mutate() math, addition subtraction. instance, calculate age politicians () 2022.variety functions especially useful constructing new columns. include log() compute natural logarithm, lead() bring values one row, lag() push values one row, cumsum() creates cumulative sum column.earlier examples, can also use mutate() combination across(). includes potential use selection helpers. instance, count number characters first last names time.Finally, use case_when() need make new column basis two conditional statements. instance, may years want group decades.accomplish series if_else() statements, case_when() clear. cases evaluated order soon match case_when() continue remainder cases. can useful catch-end signal potential issue might like know .","code":"\naustralian_politicians <- \n  australian_politicians |> \n  mutate(was_both = if_else(member == 1 & senator == 1, 1, 0))\n\naustralian_politicians |> \n  select(member, senator, was_both)\n#> # A tibble: 1,783 × 3\n#>    member senator was_both\n#>     <dbl>   <dbl>    <dbl>\n#>  1      0       1        0\n#>  2      1       1        1\n#>  3      0       1        0\n#>  4      1       0        0\n#>  5      1       0        0\n#>  6      1       0        0\n#>  7      1       0        0\n#>  8      0       1        0\n#>  9      0       1        0\n#> 10      1       0        0\n#> # … with 1,773 more rows\naustralian_politicians <- \n  australian_politicians |> \n  mutate(age = 2022 - lubridate::year(birthDate))\n\naustralian_politicians |> \n  select(uniqueID, age)\n#> # A tibble: 1,783 × 2\n#>    uniqueID     age\n#>    <chr>      <dbl>\n#>  1 Abbott1859    NA\n#>  2 Abbott1869   153\n#>  3 Abbott1877   145\n#>  4 Abbott1886   136\n#>  5 Abbott1891   131\n#>  6 Abbott1957    65\n#>  7 Abel1939      83\n#>  8 Abetz1958     64\n#>  9 Adams1943     79\n#> 10 Adams1951     71\n#> # … with 1,773 more rows\naustralian_politicians |> \n  select(uniqueID, age) |> \n  mutate(log_age = log(age)) |> \n  head()\n#> # A tibble: 6 × 3\n#>   uniqueID     age log_age\n#>   <chr>      <dbl>   <dbl>\n#> 1 Abbott1859    NA   NA   \n#> 2 Abbott1869   153    5.03\n#> 3 Abbott1877   145    4.98\n#> 4 Abbott1886   136    4.91\n#> 5 Abbott1891   131    4.88\n#> 6 Abbott1957    65    4.17\n\naustralian_politicians |> \n  select(uniqueID, age) |> \n  mutate(lead_age = lead(age)) |> \n  head()\n#> # A tibble: 6 × 3\n#>   uniqueID     age lead_age\n#>   <chr>      <dbl>    <dbl>\n#> 1 Abbott1859    NA      153\n#> 2 Abbott1869   153      145\n#> 3 Abbott1877   145      136\n#> 4 Abbott1886   136      131\n#> 5 Abbott1891   131       65\n#> 6 Abbott1957    65       83\n\naustralian_politicians |> \n  select(uniqueID, age) |> \n  mutate(lag_age = lag(age)) |> \n  head()\n#> # A tibble: 6 × 3\n#>   uniqueID     age lag_age\n#>   <chr>      <dbl>   <dbl>\n#> 1 Abbott1859    NA      NA\n#> 2 Abbott1869   153      NA\n#> 3 Abbott1877   145     153\n#> 4 Abbott1886   136     145\n#> 5 Abbott1891   131     136\n#> 6 Abbott1957    65     131\n\naustralian_politicians |> \n  select(uniqueID, age) |> \n  filter(!is.na(age)) |> \n  mutate(cumulative_age = cumsum(age)) |> \n  head()\n#> # A tibble: 6 × 3\n#>   uniqueID     age cumulative_age\n#>   <chr>      <dbl>          <dbl>\n#> 1 Abbott1869   153            153\n#> 2 Abbott1877   145            298\n#> 3 Abbott1886   136            434\n#> 4 Abbott1891   131            565\n#> 5 Abbott1957    65            630\n#> 6 Abel1939      83            713\naustralian_politicians |> \n  mutate(across(c(firstName, surname), str_count)) |> \n  select(uniqueID, firstName, surname)\n#> # A tibble: 1,783 × 3\n#>    uniqueID   firstName surname\n#>    <chr>          <int>   <int>\n#>  1 Abbott1859         7       6\n#>  2 Abbott1869         5       6\n#>  3 Abbott1877         9       6\n#>  4 Abbott1886         7       6\n#>  5 Abbott1891         6       6\n#>  6 Abbott1957         7       6\n#>  7 Abel1939           4       4\n#>  8 Abetz1958          4       5\n#>  9 Adams1943          6       5\n#> 10 Adams1951          4       5\n#> # … with 1,773 more rows\naustralian_politicians |> \n  mutate(year_of_birth = lubridate::year(birthDate),\n         decade_of_birth = \n           case_when(\n             year_of_birth <= 1929 ~ \"pre-1930\",\n             year_of_birth <= 1939 ~ \"1930s\",\n             year_of_birth <= 1949 ~ \"1940s\",\n             year_of_birth <= 1959 ~ \"1950s\",\n             year_of_birth <= 1969 ~ \"1960s\",\n             year_of_birth <= 1979 ~ \"1970s\",\n             year_of_birth <= 1989 ~ \"1980s\",\n             TRUE ~ \"Unknown or error\"\n             )\n  ) |> \n  select(uniqueID, year_of_birth, decade_of_birth)\n#> # A tibble: 1,783 × 3\n#>    uniqueID   year_of_birth decade_of_birth \n#>    <chr>              <dbl> <chr>           \n#>  1 Abbott1859            NA Unknown or error\n#>  2 Abbott1869          1869 pre-1930        \n#>  3 Abbott1877          1877 pre-1930        \n#>  4 Abbott1886          1886 pre-1930        \n#>  5 Abbott1891          1891 pre-1930        \n#>  6 Abbott1957          1957 1950s           \n#>  7 Abel1939            1939 1930s           \n#>  8 Abetz1958           1958 1950s           \n#>  9 Adams1943           1943 1940s           \n#> 10 Adams1951           1951 1950s           \n#> # … with 1,773 more rows"},{"path":"r-essentials.html","id":"summarise","chapter":"3 R essentials","heading":"3.5.5 summarise()","text":"use summarise() like make new, condensed, summary variables. instance, perhaps like know minimum, average, maximum column.aside, summarise() summarize() equivalent can use either.default, summarise() provide one row output whole dataset. instance, earlier example found youngest, oldest, average across politicians. However, can create groups dataset using group_by(). can apply another function within context groups. use many functions basis groups, summarise() function particularly powerful conjunction group_by(). instance, group gender, get age-based summary statistics.Similarly, look youngest, oldest, mean age death gender.learn female members parliament average lived slightly longer male members parliament.can use group_by() basis one group. instance, look average number days lived gender house.can use count() create counts groups. instance, number politicians gender.addition count(), make proportion.Using count() essentially using group_by() summarise(), get result way.similarly helpful function mutate(), add_count(). difference number added column.","code":"\naustralian_politicians |> \n  summarise(youngest = min(age, na.rm = TRUE),\n            oldest = max(age, na.rm = TRUE),\n            average = mean(age, na.rm = TRUE))\n#> # A tibble: 1 × 3\n#>   youngest oldest average\n#>      <dbl>  <dbl>   <dbl>\n#> 1       28    193    101.\naustralian_politicians |> \n  summarize(youngest = min(age, na.rm = TRUE),\n            oldest = max(age, na.rm = TRUE),\n            average = mean(age, na.rm = TRUE))\n#> # A tibble: 1 × 3\n#>   youngest oldest average\n#>      <dbl>  <dbl>   <dbl>\n#> 1       28    193    101.\naustralian_politicians |> \n  group_by(gender) |> \n  summarise(youngest = min(age, na.rm = TRUE),\n            oldest = max(age, na.rm = TRUE),\n            average = mean(age, na.rm = TRUE))\n#> # A tibble: 2 × 4\n#>   gender youngest oldest average\n#>   <chr>     <dbl>  <dbl>   <dbl>\n#> 1 female       32    140    66.0\n#> 2 male         28    193   106.\naustralian_politicians |>\n  mutate(days_lived = deathDate - birthDate) |> \n  filter(!is.na(days_lived)) |> \n  group_by(gender) |> \n  summarise(\n    min_days = min(days_lived),\n    mean_days = mean(days_lived) |> round(),\n    max_days = max(days_lived)\n    )\n#> # A tibble: 2 × 4\n#>   gender min_days   mean_days  max_days  \n#>   <chr>  <drtn>     <drtn>     <drtn>    \n#> 1 female 14856 days 28857 days 35560 days\n#> 2 male   12380 days 27376 days 36416 days\naustralian_politicians |>\n  mutate(days_lived = deathDate - birthDate) |> \n  filter(!is.na(days_lived)) |> \n  group_by(gender, member) |> \n  summarise(\n    min_days = min(days_lived),\n    mean_days = mean(days_lived) |> round(),\n    max_days = max(days_lived)\n    )\n#> # A tibble: 4 × 5\n#> # Groups:   gender [2]\n#>   gender member min_days   mean_days  max_days  \n#>   <chr>   <dbl> <drtn>     <drtn>     <drtn>    \n#> 1 female      0 21746 days 29517 days 35560 days\n#> 2 female      1 14856 days 27538 days 33442 days\n#> 3 male        0 13619 days 27133 days 36416 days\n#> 4 male        1 12380 days 27496 days 36328 days\naustralian_politicians |> \n  group_by(gender) |> \n  count()\n#> # A tibble: 2 × 2\n#> # Groups:   gender [2]\n#>   gender     n\n#>   <chr>  <int>\n#> 1 female   240\n#> 2 male    1543\naustralian_politicians |> \n  group_by(gender) |> \n  count() |> \n  ungroup() |> \n  mutate(proportion = n/(sum(n)))\n#> # A tibble: 2 × 3\n#>   gender     n proportion\n#>   <chr>  <int>      <dbl>\n#> 1 female   240      0.135\n#> 2 male    1543      0.865\naustralian_politicians |> \n  group_by(gender) |> \n  summarise(n = n())\n#> # A tibble: 2 × 2\n#>   gender     n\n#>   <chr>  <int>\n#> 1 female   240\n#> 2 male    1543\naustralian_politicians |> \n  group_by(gender) |> \n  add_count() |> \n  select(uniqueID, gender, n)\n#> # A tibble: 1,783 × 3\n#> # Groups:   gender [2]\n#>    uniqueID   gender     n\n#>    <chr>      <chr>  <int>\n#>  1 Abbott1859 male    1543\n#>  2 Abbott1869 male    1543\n#>  3 Abbott1877 male    1543\n#>  4 Abbott1886 male    1543\n#>  5 Abbott1891 male    1543\n#>  6 Abbott1957 male    1543\n#>  7 Abel1939   male    1543\n#>  8 Abetz1958  male    1543\n#>  9 Adams1943  female   240\n#> 10 Adams1951  male    1543\n#> # … with 1,773 more rows"},{"path":"r-essentials.html","id":"base","chapter":"3 R essentials","heading":"3.6 Base","text":"tidyverse established relatively recently help data science, R existed long . host functionality built R especially around core needs programming statisticians.particular, cover:class()data simulationfunction(), (), apply()need install additional packages, functionality comes R.","code":""},{"path":"r-essentials.html","id":"class","chapter":"3 R essentials","heading":"3.6.1 class()","text":"everyday usage ‘, b, c, …’ letters ‘1, 2, 3,…’ numbers. use letters numbers differently, instance add letters. Similarly, R needs way distinguishing different classes content. define properties class , ‘behaves, relates types objects’ (Wickham 2019a).Classes hierarchy. instance, ‘human,’ ‘animal.’ ‘humans’ ‘animals,’ ‘animals’ ‘humans.’ Similarly, integers numbers, numbers integers. can find class object R class().classes cover ‘numeric,’ ‘character,’ ‘factor,’ ‘date,’ ‘data.frame.’first thing know , way frog can become prince, can sometimes change class object R. instance, start ‘numeric,’ change ‘character’ .character(), ‘factor’ .factor(). tried make date .Date() get error numbers properties needed date.Compared ‘numeric’ ‘character’ classes, ‘factor’ class might less familiar. ‘factor’ used categorical data can take certain values (Wickham 2019a). instance, typical usage factor variable binary, ‘day’ ‘night.’ also often used age-groups, ‘18-29,’ ‘30-44,’ ‘45-60,’ ‘60+’ (opposed age, often ‘numeric’); sometimes level education: ‘less high school,’ ‘high school,’ ‘college,’ ‘undergraduate degree,’ ‘postgraduate degree.’ can find allowed levels ‘factor’ using levels().Dates especially tricky class quickly become complicated. Nonetheless, foundational level, can use .Date() convert character looks like date actual date. enables us , say, perform addition subtraction, able character.final class discuss ‘data.frame.’ looks like spreadsheet commonly used store data analyze. Formally, ‘data frame list equal-length vectors’ (Wickham 2019a). column row names can see using colnames() rownames(), although often names rows just numbers.illustrate , use ‘ResumeNames’ dataset AER (Kleiber Zeileis 2008). package can installed way package CRAN. dataset comprises cross-sectional data resume content, especially name used resume, associated information whether candidate received call-back 4,870 fictitious resumes. dataset created Bertrand Mullainathan (2004) sent fictitious resumes response job advertisements Boston Chicago differed whether resume assigned ‘African American sounding name White sounding name.’ found considerable discrimination whereby ‘White names receive 50 percent callbacks interviews.’can examine class vectors, .e. columns, make-data frame specifying column name.Sometimes helpful able change classes many columns . can using mutate() across().many ways code run issue class always among first things check. Common issues variables think ‘character’ ‘numeric,’ actually ‘factor.’ variables think ‘numeric’ actually ‘character.’","code":"\na_number <- 8\nclass(a_number)\n#> [1] \"numeric\"\n\na_letter <- \"a\"\nclass(a_letter)\n#> [1] \"character\"\na_number <- 8\na_number\n#> [1] 8\nclass(a_number)\n#> [1] \"numeric\"\n\na_number <- as.character(a_number)\na_number\n#> [1] \"8\"\nclass(a_number)\n#> [1] \"character\"\n\na_number <- as.factor(a_number)\na_number\n#> [1] 8\n#> Levels: 8\nclass(a_number)\n#> [1] \"factor\"\nage_groups <- factor(\n  c('18-29', '30-44', '45-60', '60+')\n)\nage_groups\n#> [1] 18-29 30-44 45-60 60+  \n#> Levels: 18-29 30-44 45-60 60+\nclass(age_groups)\n#> [1] \"factor\"\nlevels(age_groups)\n#> [1] \"18-29\" \"30-44\" \"45-60\" \"60+\"\nlooks_like_a_date_but_is_not <- \"2022-01-01\"\nlooks_like_a_date_but_is_not\n#> [1] \"2022-01-01\"\nclass(looks_like_a_date_but_is_not)\n#> [1] \"character\"\nis_a_date <- as.Date(looks_like_a_date_but_is_not)\nis_a_date\n#> [1] \"2022-01-01\"\nclass(is_a_date)\n#> [1] \"Date\"\nis_a_date + 3\n#> [1] \"2022-01-04\"\ninstall.packages(\"AER\")\nlibrary(AER)\ndata(\"ResumeNames\", package = \"AER\")\nResumeNames |> \n  head()\n#>      name gender ethnicity quality call    city jobs\n#> 1 Allison female      cauc     low   no chicago    2\n#> 2 Kristen female      cauc    high   no chicago    3\n#> 3 Lakisha female      afam     low   no chicago    1\n#> 4 Latonya female      afam    high   no chicago    4\n#> 5  Carrie female      cauc    high   no chicago    3\n#> 6     Jay   male      cauc     low   no chicago    2\n#>   experience honors volunteer military holes school email\n#> 1          6     no        no       no   yes     no    no\n#> 2          6     no       yes      yes    no    yes   yes\n#> 3          6     no        no       no    no    yes    no\n#> 4          6     no       yes       no   yes     no   yes\n#> 5         22     no        no       no    no    yes   yes\n#> 6          6    yes        no       no    no     no    no\n#>   computer special college minimum equal     wanted\n#> 1      yes      no     yes       5   yes supervisor\n#> 2      yes      no      no       5   yes supervisor\n#> 3      yes      no     yes       5   yes supervisor\n#> 4      yes     yes      no       5   yes supervisor\n#> 5      yes      no      no    some   yes  secretary\n#> 6       no     yes     yes    none   yes      other\n#>   requirements reqexp reqcomm reqeduc reqcomp reqorg\n#> 1          yes    yes      no      no     yes     no\n#> 2          yes    yes      no      no     yes     no\n#> 3          yes    yes      no      no     yes     no\n#> 4          yes    yes      no      no     yes     no\n#> 5          yes    yes      no      no     yes    yes\n#> 6           no     no      no      no      no     no\n#>                           industry\n#> 1                    manufacturing\n#> 2                    manufacturing\n#> 3                    manufacturing\n#> 4                    manufacturing\n#> 5 health/education/social services\n#> 6                            trade\nclass(ResumeNames)\n#> [1] \"data.frame\"\ncolnames(ResumeNames)\n#>  [1] \"name\"         \"gender\"       \"ethnicity\"   \n#>  [4] \"quality\"      \"call\"         \"city\"        \n#>  [7] \"jobs\"         \"experience\"   \"honors\"      \n#> [10] \"volunteer\"    \"military\"     \"holes\"       \n#> [13] \"school\"       \"email\"        \"computer\"    \n#> [16] \"special\"      \"college\"      \"minimum\"     \n#> [19] \"equal\"        \"wanted\"       \"requirements\"\n#> [22] \"reqexp\"       \"reqcomm\"      \"reqeduc\"     \n#> [25] \"reqcomp\"      \"reqorg\"       \"industry\"\nclass(ResumeNames$name)\n#> [1] \"factor\"\nclass(ResumeNames$jobs)\n#> [1] \"integer\"\nclass(ResumeNames$name)\n#> [1] \"factor\"\nclass(ResumeNames$gender)\n#> [1] \"factor\"\nclass(ResumeNames$ethnicity)\n#> [1] \"factor\"\n\nResumeNames |>\n  mutate(across(c(name, gender, ethnicity), as.character)) |>\n  head()\n#>      name gender ethnicity quality call    city jobs\n#> 1 Allison female      cauc     low   no chicago    2\n#> 2 Kristen female      cauc    high   no chicago    3\n#> 3 Lakisha female      afam     low   no chicago    1\n#> 4 Latonya female      afam    high   no chicago    4\n#> 5  Carrie female      cauc    high   no chicago    3\n#> 6     Jay   male      cauc     low   no chicago    2\n#>   experience honors volunteer military holes school email\n#> 1          6     no        no       no   yes     no    no\n#> 2          6     no       yes      yes    no    yes   yes\n#> 3          6     no        no       no    no    yes    no\n#> 4          6     no       yes       no   yes     no   yes\n#> 5         22     no        no       no    no    yes   yes\n#> 6          6    yes        no       no    no     no    no\n#>   computer special college minimum equal     wanted\n#> 1      yes      no     yes       5   yes supervisor\n#> 2      yes      no      no       5   yes supervisor\n#> 3      yes      no     yes       5   yes supervisor\n#> 4      yes     yes      no       5   yes supervisor\n#> 5      yes      no      no    some   yes  secretary\n#> 6       no     yes     yes    none   yes      other\n#>   requirements reqexp reqcomm reqeduc reqcomp reqorg\n#> 1          yes    yes      no      no     yes     no\n#> 2          yes    yes      no      no     yes     no\n#> 3          yes    yes      no      no     yes     no\n#> 4          yes    yes      no      no     yes     no\n#> 5          yes    yes      no      no     yes    yes\n#> 6           no     no      no      no      no     no\n#>                           industry\n#> 1                    manufacturing\n#> 2                    manufacturing\n#> 3                    manufacturing\n#> 4                    manufacturing\n#> 5 health/education/social services\n#> 6                            trade\n\nclass(ResumeNames$name)\n#> [1] \"factor\"\nclass(ResumeNames$gender)\n#> [1] \"factor\"\nclass(ResumeNames$ethnicity)\n#> [1] \"factor\""},{"path":"r-essentials.html","id":"simulating-data","chapter":"3 R essentials","heading":"3.6.2 Simulating data","text":"Simulating data key skill telling believable stories data. order simulate data, need able randomly draw statistical distributions collections. R variety functions make easier, including: normal distribution, rnorm(); uniform distribution, runif(); Poisson distribution, rpois; binomial distribution, rbinom; many others. randomly sample collection items, can use sample().dealing randomness, need reproducibility makes important, paradoxically, randomness repeatable. say, another person needs able draw random numbers draw. setting seed random draws using set.seed().get observations standard normal distribution put data frame.add draws uniform, Poisson, binomial distributions, using cbind() bring columns original dataset new one together.Finally, add favorite color observation sample().set option ‘replace’ ‘TRUE’ choosing two items, time choose want possibility either chosen. Depending simulation may need think whether ‘replace’ ‘TRUE’ ‘FALSE.’ Another useful optional argument sample() adjust probability item drawn. default options equally likely, specify particular probabilities wanted ‘prob.’ always functions, can find help file, instance ?sample.","code":"\nset.seed(853)\n\nnumber_of_observations <- 5\n\nsimulated_data <- \n  data.frame(\n    person = c(1:number_of_observations),\n    std_normal_observations = rnorm(n = number_of_observations,\n                                    mean = 0,\n                                    sd = 1)\n    )\n\nsimulated_data\n#>   person std_normal_observations\n#> 1      1             -0.35980342\n#> 2      2             -0.04064753\n#> 3      3             -1.78216227\n#> 4      4             -1.12242282\n#> 5      5             -1.00278400\nsimulated_data <-\n  data.frame(\n    uniform_observations = \n      runif(n = number_of_observations, min = 0, max = 10),\n    poisson_observations = \n      rpois(n = number_of_observations, lambda = 100),\n    binomial_observations = \n      rbinom(n = number_of_observations, size = 2, prob = 0.5)\n  ) |>\n  cbind(simulated_data)\n\nsimulated_data\n#>   uniform_observations poisson_observations\n#> 1            9.6219155                   81\n#> 2            7.2269016                   91\n#> 3            0.8252921                   84\n#> 4            1.0379810                  100\n#> 5            3.0942004                   97\n#>   binomial_observations person std_normal_observations\n#> 1                     2      1             -0.35980342\n#> 2                     1      2             -0.04064753\n#> 3                     1      3             -1.78216227\n#> 4                     1      4             -1.12242282\n#> 5                     1      5             -1.00278400\nsimulated_data <- \n  data.frame(\n    favorite_color = sample(x = c(\"blue\", \" white \"), \n                             size = number_of_observations,\n                             replace = TRUE)\n    ) |>\n  cbind(simulated_data)\n\nsimulated_data\n#>   favorite_color uniform_observations poisson_observations\n#> 1           blue            9.6219155                   81\n#> 2           blue            7.2269016                   91\n#> 3           blue            0.8252921                   84\n#> 4         white             1.0379810                  100\n#> 5           blue            3.0942004                   97\n#>   binomial_observations person std_normal_observations\n#> 1                     2      1             -0.35980342\n#> 2                     1      2             -0.04064753\n#> 3                     1      3             -1.78216227\n#> 4                     1      4             -1.12242282\n#> 5                     1      5             -1.00278400"},{"path":"r-essentials.html","id":"function-for-and-apply","chapter":"3 R essentials","heading":"3.6.3 function(), for(), and apply()","text":"R ‘functional programming language’ (Wickham 2019a). means foundationally write, use, compose functions, collections code accomplish something specific.lot functions R people written, can use. Almost common statistical data science task might need accomplish likely already function written someone else made available us, either part base R installation package. need write functions time time, especially -specific tasks.\ndefine function using function(), assign name. likely need include inputs outputs function. Inputs specified round brackets. specific task function accomplish goes braces.can specify defaults inputs case person using function supply .One common scenario want apply function multiple times. Like many programming languages, can use () loop . look () loop R similar function(), define iterating round brackets, function apply braces.R programming language focused statistics, often interested arrays matrices. us apply() apply function rows (‘MARGIN = 1’) columns (‘MARGIN = 2’).","code":"\nprint_names <- function(some_names) {\n  print(some_names)\n}\n\nprint_names(c(\"rohan\", \"monica\"))\n#> [1] \"rohan\"  \"monica\"\nprint_names <- function(some_names = c(\"edward\", \"hugo\")) {\n  print(some_names)\n}\n\nprint_names()\n#> [1] \"edward\" \"hugo\"\nfor (i in 1:3) {\n  print(i)\n}\n#> [1] 1\n#> [1] 2\n#> [1] 3\nx <- cbind(x1 = 66, x2 = c(4:1, 2:5))\ndimnames(x)[[1]] <- letters[1:8]\nclass(x)\n#> [1] \"matrix\" \"array\"\napply(x, 2, mean, trim = .2)\n#> x1 x2 \n#> 66  3\nsimulated_data\n#>   favorite_color uniform_observations poisson_observations\n#> 1           blue            9.6219155                   81\n#> 2           blue            7.2269016                   91\n#> 3           blue            0.8252921                   84\n#> 4         white             1.0379810                  100\n#> 5           blue            3.0942004                   97\n#>   binomial_observations person std_normal_observations\n#> 1                     2      1             -0.35980342\n#> 2                     1      2             -0.04064753\n#> 3                     1      3             -1.78216227\n#> 4                     1      4             -1.12242282\n#> 5                     1      5             -1.00278400\napply(X = simulated_data, MARGIN = 2, FUN = unique)\n#> $favorite_color\n#> [1] \"blue\"    \" white \"\n#> \n#> $uniform_observations\n#> [1] \"9.6219155\" \"7.2269016\" \"0.8252921\" \"1.0379810\"\n#> [5] \"3.0942004\"\n#> \n#> $poisson_observations\n#> [1] \" 81\" \" 91\" \" 84\" \"100\" \" 97\"\n#> \n#> $binomial_observations\n#> [1] \"2\" \"1\"\n#> \n#> $person\n#> [1] \"1\" \"2\" \"3\" \"4\" \"5\"\n#> \n#> $std_normal_observations\n#> [1] \"-0.35980342\" \"-0.04064753\" \"-1.78216227\" \"-1.12242282\"\n#> [5] \"-1.00278400\""},{"path":"r-essentials.html","id":"making-graphs-with-ggplot2","chapter":"3 R essentials","heading":"3.7 Making graphs with ggplot2","text":"key package tidyverse terms manipulating data dplyr (Wickham et al. 2020), key package tidyverse terms creating graphs ggplot2 (Wickham 2016). part tidyverse collection packages need explicitly installed loaded tidyverse loaded.formally, ggplot2 works defining layers build form graph, based around ‘grammar graphics’ (hence, ‘gg’). Instead pipe operator (|>) ggplot uses add operator +.three key aspects need specified build graph ggplot2:data;aesthetics / mapping; andtype.get started obtain GDP data OECD countries (OECD 2022).interested, firstly, making bar chart GDP change third quarter 2021 ten countries: Australia, Canada, Chile, Indonesia, Germany, Great Britain, New Zealand, South Africa, Spain, US.start ggplot specify mapping/aesthetic, case means specifying x-axis y-axis.Now need specify type graph interested . case want bar chart adding geom_bar().can color bars whether country European adding another aesthetic, ‘fill.’Finally, make look nicer : adding labels, labs(); changing color, scale_fill_brewer(); background, theme_classic().Facets enable us create subplots focus specific aspects data. invaluable allow us add another variable graph without make 3D graph. use facet_wrap() add facet specify variable like facet . case, facet hemisphere.","code":"\nlibrary(tidyverse)\n\noecd_gdp <- \n  read_csv(\"https://stats.oecd.org/sdmx-json/data/DP_LIVE/.QGDP.../OECD?contentType=csv&detail=code&separator=comma&csv-lang=en\")\n\nwrite_csv(oecd_gdp, 'inputs/data/oecd_gdp.csv')#> # A tibble: 6 × 8\n#>   LOCATION INDICATOR SUBJECT MEASURE  FREQUENCY TIME  Value\n#>   <chr>    <chr>     <chr>   <chr>    <chr>     <chr> <dbl>\n#> 1 OECD     QGDP      TOT     PC_CHGPP A         1962   5.70\n#> 2 OECD     QGDP      TOT     PC_CHGPP A         1963   5.20\n#> 3 OECD     QGDP      TOT     PC_CHGPP A         1964   6.38\n#> 4 OECD     QGDP      TOT     PC_CHGPP A         1965   5.35\n#> 5 OECD     QGDP      TOT     PC_CHGPP A         1966   5.75\n#> 6 OECD     QGDP      TOT     PC_CHGPP A         1967   3.96\n#> # … with 1 more variable: `Flag Codes` <chr>\noecd_gdp_most_recent <- \n  oecd_gdp |> \n  filter(TIME == \"2021-Q3\",\n         SUBJECT == \"TOT\",\n         LOCATION %in% c(\"AUS\", \"CAN\", \"CHL\", \"DEU\", \"GBR\",\n                         \"IDN\", \"ESP\", \"NZL\", \"USA\", \"ZAF\"),\n         MEASURE == \"PC_CHGPY\") |> \n  mutate(european = if_else(LOCATION %in% c(\"DEU\", \"GBR\", \"ESP\"),\n                             \"European\",\n                             \"Not european\"),\n         hemisphere = if_else(LOCATION %in% c(\"CAN\", \"DEU\", \"GBR\", \"ESP\", \"USA\"),\n                             \"Northern Hemisphere\",\n                             \"Southern Hemisphere\"),\n         )\noecd_gdp_most_recent |> \n  ggplot(mapping = aes(x = LOCATION, y = Value))\noecd_gdp_most_recent |> \n  ggplot(mapping = aes(x = LOCATION, y = Value)) +\n  geom_bar(stat=\"identity\")\noecd_gdp_most_recent |> \n  ggplot(mapping = aes(x = LOCATION, y = Value, fill = european)) +\n  geom_bar(stat=\"identity\")\noecd_gdp_most_recent |> \n  ggplot(mapping = aes(x = LOCATION, y = Value, fill = european)) +\n  geom_bar(stat=\"identity\") + \n  labs(title = \"Quarterly change in GDP for ten OECD countries in 2021Q3\", \n       x = \"Countries\", \n       y = \"Change (%)\",\n       fill = \"Is European?\") +\n  theme_classic() +\n  scale_fill_brewer(palette = \"Set1\")\noecd_gdp_most_recent |> \n  ggplot(mapping = aes(x = LOCATION, y = Value, fill = european)) +\n  geom_bar(stat=\"identity\") + \n  labs(title = \"Quarterly change in GDP for ten OECD countries in 2021Q3\", \n       x = \"Countries\", \n       y = \"Change (%)\",\n       fill = \"Is European?\") +\n  theme_classic() +\n  scale_fill_brewer(palette = \"Set1\") +\n  facet_wrap(~hemisphere, \n              scales = \"free_x\")"},{"path":"r-essentials.html","id":"exploring-the-tidyverse","chapter":"3 R essentials","heading":"3.8 Exploring the tidyverse","text":"focused two aspects tidyverse: dplyr, ggplot2. However, tidyverse comprises variety different packages functions. now go four common aspects:Importing data tibble().Joining pivoting datasets.String manipulation stringr.Factor variables forcats.However, first task deal nomenclature, particular specific ‘tidy’ ‘tidyverse.’ name refers tidy data, benefit variety ways data messy, tidy data satisfy three rules. means structure datasets consistent regardless specifics, makes easier apply functions expect certain types input. Tidy data refers dataset (Wickham Grolemund 2017; Wickham 2014, 4):Every variable column .Every observation row.Every value cell.Table 3.1 tidy. Table 3.2 tidy age hair share column.Table 3.1: Example tidy dataTable 3.2: Example data tidy","code":"\ntibble(\n  person = c(\"Rohan\", \"Monica\", \"Edward\", \"Hugo\"),\n  age = c(35, 35, 2, 0),\n  hair = c(\"Black\", \"Blonde\", \"Brown\", \"None\")\n  ) |>\n  knitr::kable(\n    caption = \"Example of tidy data\",\n    col.names = c(\"Person\", \"Age\", \"Hair\"),\n    digits = 1,\n    booktabs = TRUE,\n    linesep = \"\"\n    )\ntibble(\n  person = c(\n    \"Rohan\",\n    \"Rohan\",\n    \"Monica\",\n    \"Monica\",\n    \"Edward\",\n    \"Edward\",\n    \"Hugo\",\n    \"Hugo\"\n  ),\n  variable = c(\"Age\", \"Hair\", \"Age\", \"Hair\", \"Age\", \"Hair\", \"Age\", \"Hair\"),\n  value = c(\"35\", \"Black\", \"35\", \"Blonde\", \"2\", \"Brown\", \"0\", \"None\")\n) |>\n  knitr::kable(\n    caption = \"Example of data that are not tidy\",\n    col.names = c(\"Person\", \"Variable\", \"Value\"),\n    digits = 1,\n    booktabs = TRUE,\n    linesep = \"\"\n  )"},{"path":"r-essentials.html","id":"importing-data-and-tibble","chapter":"3 R essentials","heading":"3.8.1 Importing data and tibble()","text":"variety ways get data R can use . CSV files, read_csv() readr (Wickham, Hester, Bryan 2021), dta files, read_dta() haven (Wickham Miller 2020).CSVs common format many advantages including fact typically modify data. column separated comma, row record. can provide read_csv() URL local file read. variety different options can passed read_csv() including ability specify whether dataset column names, types columns, many lines skip. specify types columns read_csv() make guess looking dataset.use read_dta() read .dta files, commonly produced statistical program Stata. means common fields sociology, political science, economics. format separates data labels typically reunite using to_factor() labelled (Larmarange 2021). haven part tidyverse, automatically loaded default, contrast package ggplot2, need run library(haven).Typically dataset enters R ‘data.frame.’ can useful, another helpful class dataset ‘tibble.’ can created using tibble() tibble package part tidyverse. tibble data frame, particular changes make easier work , including converting strings factors default, showing class columns, printing nicely.can make tibble manually, need , instance, simulate data. typically import data directly tibble, instance, use read_csv().","code":"\npeople_as_dataframe <- \n  data.frame(names = c(\"rohan\", \"monica\"),\n             website = c(\"rohanalexander.com\", \"monicaalexander.com\"),\n             fav_color = c(\"blue\", \" white \")\n             )\nclass(people_as_dataframe)\n#> [1] \"data.frame\"\npeople_as_dataframe\n#>    names             website fav_color\n#> 1  rohan  rohanalexander.com      blue\n#> 2 monica monicaalexander.com    white\n\npeople_as_tibble <- \n  tibble(names = c(\"rohan\", \"monica\"),\n         website = c(\"rohanalexander.com\", \"monicaalexander.com\"),\n         fav_color = c(\"blue\", \" white \")\n         )\npeople_as_tibble\n#> # A tibble: 2 × 3\n#>   names  website             fav_color\n#>   <chr>  <chr>               <chr>    \n#> 1 rohan  rohanalexander.com  \"blue\"   \n#> 2 monica monicaalexander.com \" white \"\nclass(people_as_tibble)\n#> [1] \"tbl_df\"     \"tbl\"        \"data.frame\""},{"path":"r-essentials.html","id":"dataset-manipulation-with-joins-and-pivots","chapter":"3 R essentials","heading":"3.8.2 Dataset manipulation with joins and pivots","text":"two dataset manipulations often needed: joins pivots.often situation two, , datasets interested combining . can join datasets together variety ways. common way use left_join() dplyr (Wickham et al. 2020). useful one main dataset using another dataset useful variables want add . critical aspect column/s can use link two datasets. create two tibbles join basis names.variety options join datasets, including inner_join(), right_join(), full_join().Another common dataset manipulation task pivoting . Datasets tend either long wide. Generally, tidyverse, certainly ggplot2, need long data. go one use pivot_longer() pivot_wider() tidyr (Wickham 2021c).create wide data whether ‘mark’ ‘lauren’ won running race three years.dataset wide format moment. get long format, need column specifies person, another specifies result. use pivot_longer() achieve .Occasionally, need go long data wide data. use pivot_wider() .","code":"\nmain_dataset <- \n  tibble(\n    names = c('rohan', 'monica', 'edward', 'hugo'),\n    status = c('adult', 'adult', 'child', 'infant')\n  )\nmain_dataset\n#> # A tibble: 4 × 2\n#>   names  status\n#>   <chr>  <chr> \n#> 1 rohan  adult \n#> 2 monica adult \n#> 3 edward child \n#> 4 hugo   infant\n\nsupplementary_dataset <- \n  tibble(\n    names = c('rohan', 'monica', 'edward', 'hugo'),\n    favorite_food = c('pasta', 'salmon', 'pizza', 'milk')\n  )\nsupplementary_dataset\n#> # A tibble: 4 × 2\n#>   names  favorite_food\n#>   <chr>  <chr>        \n#> 1 rohan  pasta        \n#> 2 monica salmon       \n#> 3 edward pizza        \n#> 4 hugo   milk\n\nmain_dataset <- \n  main_dataset |> \n  left_join(supplementary_dataset, by = \"names\")\n\nmain_dataset\n#> # A tibble: 4 × 3\n#>   names  status favorite_food\n#>   <chr>  <chr>  <chr>        \n#> 1 rohan  adult  pasta        \n#> 2 monica adult  salmon       \n#> 3 edward child  pizza        \n#> 4 hugo   infant milk\npivot_example_data <- \n  tibble(year = c(2019, 2020, 2021),\n         mark = c(\"first\", \"second\", \"first\"),\n         lauren = c(\"second\", \"first\", \"second\"))\n\npivot_example_data\n#> # A tibble: 3 × 3\n#>    year mark   lauren\n#>   <dbl> <chr>  <chr> \n#> 1  2019 first  second\n#> 2  2020 second first \n#> 3  2021 first  second\ndata_pivoted_longer <- \n  pivot_example_data |> \n  tidyr::pivot_longer(cols = c(\"mark\", \"lauren\"),\n              names_to = \"person\",\n               values_to = \"position\")\n\nhead(data_pivoted_longer)\n#> # A tibble: 6 × 3\n#>    year person position\n#>   <dbl> <chr>  <chr>   \n#> 1  2019 mark   first   \n#> 2  2019 lauren second  \n#> 3  2020 mark   second  \n#> 4  2020 lauren first   \n#> 5  2021 mark   first   \n#> 6  2021 lauren second\ndata_pivoted_wider <- \n  data_pivoted_longer |> \n  tidyr::pivot_wider(names_from = \"person\",\n                     values_from = \"position\")\n\nhead(data_pivoted_wider)\n#> # A tibble: 3 × 3\n#>    year mark   lauren\n#>   <dbl> <chr>  <chr> \n#> 1  2019 first  second\n#> 2  2020 second first \n#> 3  2021 first  second"},{"path":"r-essentials.html","id":"string-manipulation-and-stringr","chapter":"3 R essentials","heading":"3.8.3 String manipulation and stringr","text":"R often create string double quotes, although using single quotes works . instance c(\"\", \"b\") consists two strings ‘’ ‘b,’ contained character vector. variety ways manipulate strings R focus stringr (Wickham 2019e). automatically loaded load tidyverse.want look whether string contains certain content, can use str_detect(). want remove change particular content can use str_remove() str_replace().variety functions often especially useful data cleaning. instance, can use str_length() find long string , str_c() bring strings together.Finally, separate() tidyr, although part stringr, indispensable string manipulation. turns one character column many.","code":"\ndataset_of_strings <- \n  tibble(\n    names = c(\"rohan alexander\", \n              \"monica alexander\", \n              \"edward alexander\", \n              \"hugo alexander\")\n  )\n\ndataset_of_strings |> \n  mutate(is_rohan = str_detect(names, \"rohan\"),\n         make_howlett = str_replace(names, \"alexander\", \"howlett\"),\n         remove_rohan = str_remove(names, \"rohan\")\n         )\n#> # A tibble: 4 × 4\n#>   names            is_rohan make_howlett   remove_rohan     \n#>   <chr>            <lgl>    <chr>          <chr>            \n#> 1 rohan alexander  TRUE     rohan howlett  \" alexander\"     \n#> 2 monica alexander FALSE    monica howlett \"monica alexande…\n#> 3 edward alexander FALSE    edward howlett \"edward alexande…\n#> 4 hugo alexander   FALSE    hugo howlett   \"hugo alexander\"\ndataset_of_strings |> \n  mutate(length_is = str_length(string = names),\n         name_and_length = str_c(names, length_is, sep = \" - \")\n         )\n#> # A tibble: 4 × 3\n#>   names            length_is name_and_length      \n#>   <chr>                <int> <chr>                \n#> 1 rohan alexander         15 rohan alexander - 15 \n#> 2 monica alexander        16 monica alexander - 16\n#> 3 edward alexander        16 edward alexander - 16\n#> 4 hugo alexander          14 hugo alexander - 14\ndataset_of_strings |> \n  separate(col = names,\n           into = c(\"first\", \"last\"),\n           sep = \" \",\n           remove = FALSE)\n#> # A tibble: 4 × 3\n#>   names            first  last     \n#>   <chr>            <chr>  <chr>    \n#> 1 rohan alexander  rohan  alexander\n#> 2 monica alexander monica alexander\n#> 3 edward alexander edward alexander\n#> 4 hugo alexander   hugo   alexander"},{"path":"r-essentials.html","id":"factor-variables-and-forcats","chapter":"3 R essentials","heading":"3.8.4 Factor variables and forcats","text":"factor collection strings categories. Sometimes inherent ordering. instance, days week order – Monday, Tuesday, Wednesday, … – alphabetical. requirement case, instance gender: female, male, ; pregnancy status: pregnant pregnant. Factors feature prominently base R. can useful ensure appropriate strings allowed. instance, ‘days_of_the_week’ factor variable ‘January’ allowed. can add great deal complication, less prominent role tidyverse. Nonetheless taking advantage factors useful certain circumstances. instance, plotting days week probably want usual ordering alphabetical ordering result character variable. factors built base R, one tidyverse package especially useful using factors forcats (Wickham 2020a).Sometimes character vector, want ordered particular way. default character vector ordered alphabetically, may want . instance, days week look strange graph alphabetically ordered: Friday, Monday, Saturday, Sunday, Thursday, Tuesday, Wednesday!way change ordering change variable character factor. can use fct_relevel() forcats (Wickham 2020a) specify ordering.can compare results graphing first original character vector x-axis, another graph factor vector x-axis.","code":"\nset.seed(853)\n\ndays_data <-\n  tibble(\n    days =\n      c(\n        \"Monday\",\n        \"Tuesday\",\n        \"Wednesday\",\n        \"Thursday\",\n        \"Friday\",\n        \"Saturday\",\n        \"Sunday\"\n      ),\n    some_value = c(sample.int(100, 7))\n  )\n\ndays_data <-\n  days_data |>\n  mutate(\n    days_as_factor = factor(days),\n    days_as_factor = fct_relevel(\n      days,\n      \"Monday\",\n      \"Tuesday\",\n      \"Wednesday\",\n      \"Thursday\",\n      \"Friday\",\n      \"Saturday\",\n      \"Sunday\"\n    )\n  )\ndays_data |> \n  ggplot(aes(x = days, y = some_value)) +\n  geom_point()\n\ndays_data |> \n  ggplot(aes(x = days_as_factor, y = some_value)) +\n  geom_point()"},{"path":"r-essentials.html","id":"exercises-and-tutorial-2","chapter":"3 R essentials","heading":"3.9 Exercises and tutorial","text":"","code":""},{"path":"r-essentials.html","id":"exercises-2","chapter":"3 R essentials","heading":"3.9.1 Exercises","text":"R?\nopen-source statistical programming language\nprogramming language created Guido van Rossum\nclosed source statistical programming language\nintegrated development environment (IDE)\nopen-source statistical programming languageA programming language created Guido van RossumA closed source statistical programming languageAn integrated development environment (IDE)three advantages R? three disadvantages?R Studio?\nintegrated development environment (IDE).\nclosed source paid program.\nprogramming language created Guido van Rossum\nstatistical programming language.\nintegrated development environment (IDE).closed source paid program.programming language created Guido van RossumA statistical programming language.class output 2 + 2 (pick one)?\ncharacter\nfactor\nnumeric\ndate\ncharacterfactornumericdateSay run: my_name <- 'Rohan'. result running print(my_name) (pick one)?\n‘Edward’\n‘Monica’\n‘Hugo’\n‘Rohan’\n‘Edward’‘Monica’‘Hugo’‘Rohan’Say dataset two columns: ‘name,’ ‘age.’ verb use pick just ‘name’ (pick one)?\ntidyverse::select()\ntidyverse::mutate()\ntidyverse::filter()\ntidyverse::rename()\ntidyverse::select()tidyverse::mutate()tidyverse::filter()tidyverse::rename()Say loaded AustralianPoliticians tidyverse run following code: australian_politicians <- AustralianPoliticians::get_auspol(''). select columns end ‘Name’ (pick one)?\naustralian_politicians |> select(contains(\"Name\"))\naustralian_politicians |> select(starts_with(\"Name\"))\naustralian_politicians |> select(matches(\"Name\"))\naustralian_politicians |> select(ends_with(\"Name\"))\naustralian_politicians |> select(contains(\"Name\"))australian_politicians |> select(starts_with(\"Name\"))australian_politicians |> select(matches(\"Name\"))australian_politicians |> select(ends_with(\"Name\"))circumstances, terms names columns, use ‘contains()’ potentially give different answers using ‘ends_with()’ question?following tidyverse verbs (pick one)?\nselect()\nfilter()\narrange()\nmutate()\nvisualize()\nselect()filter()arrange()mutate()visualize()function make new column (pick one)?\nselect()\nfilter()\narrange()\nmutate()\nvisualize()\nselect()filter()arrange()mutate()visualize()function focus particular rows (pick one)?\nselect()\nfilter()\narrange()\nmutate()\nsummarise()\nselect()filter()arrange()mutate()summarise()combination two functions provide mean dataset, sex (pick two)?\nsummarise()\nfilter()\narrange()\nmutate()\ngroup_by()\nsummarise()filter()arrange()mutate()group_by()Assume variable called ‘age’ integer. line code create column exponential (pick one)?\nmutate(exp_age = exponential(age))\nmutate(exp_age = exponent(age))\nmutate(exp_age = exp(age))\nmutate(exp_age = expon(age))\nmutate(exp_age = exponential(age))mutate(exp_age = exponent(age))mutate(exp_age = exp(age))mutate(exp_age = expon(age))Assume column called ‘age.’ line code create column contains value five rows ?\nmutate(five_before = lag(age))\nmutate(five_before = lead(age))\nmutate(five_before = lag(age, n = 5))\nmutate(five_before = lead(age, n = 5))\nmutate(five_before = lag(age))mutate(five_before = lead(age))mutate(five_before = lag(age, n = 5))mutate(five_before = lead(age, n = 5))output class('edward') (pick one)?\n‘numeric’\n‘character’\n‘data.frame’\n‘vector’\n‘numeric’‘character’‘data.frame’‘vector’function enable us draw three options ‘blue, white, red,’ 10 per cent probability ‘blue’ ‘white,’ remainder ‘red?’\nsample(c('blue', 'white', 'red'), prob = c(0.1, 0.1, 0.8))\nsample(c('blue', 'white', 'red'), size = 1)\nsample(c('blue', 'white', 'red'), size = 1, prob = c(0.8, 0.1, 0.1))\nsample(c('blue', 'white', 'red'), size = 1, prob = c(0.1, 0.1, 0.8))\nsample(c('blue', 'white', 'red'), prob = c(0.1, 0.1, 0.8))sample(c('blue', 'white', 'red'), size = 1)sample(c('blue', 'white', 'red'), size = 1, prob = c(0.8, 0.1, 0.1))sample(c('blue', 'white', 'red'), size = 1, prob = c(0.1, 0.1, 0.8))code simulates 10,000 draws normal distribution mean 27 standard deviation 3 (pick one)?\nrnorm(10000, mean = 27, sd = 3)\nrnorm(27, mean = 10000, sd = 3)\nrnorm(3, mean = 10000, sd = 27)\nrnorm(27, mean = 3, sd = 1000)\nrnorm(10000, mean = 27, sd = 3)rnorm(27, mean = 10000, sd = 3)rnorm(3, mean = 10000, sd = 27)rnorm(27, mean = 3, sd = 1000)three key aspects grammar graphics (select )?\ndata\naesthetics\ntype\ngeom_histogram()\ndataaestheticstypegeom_histogram()","code":""},{"path":"r-essentials.html","id":"tutorial-2","chapter":"3 R essentials","heading":"3.9.2 Tutorial","text":"think suspicious find attracted data—, thin weak data—seem justify beliefs held great currency lots societies throughout history, way conducive oppression large segments populationAmia Srinivasan, 22 September 2021Reflect quote Amia Srinivasan, Chichele Professor Social Political Theory, Souls College, Oxford, D’Ignazio Klein (2020), especially Chapter 6, spend least two pages discussing relation dataset familiar .","code":""},{"path":"reproducible-workflows.html","id":"reproducible-workflows","chapter":"4 Reproducible workflows","heading":"4 Reproducible workflows","text":"Required materialRead happened winds changed, (Gelman 2016)Read Good enough practices scientific computing, (Wilson et al. 2017)Watch Overcoming barriers sharing code, (M. Alexander 2021)Watch Make reprex… Please, (Gelfand 2021)Read tidyverse style guide, ‘Part: Analyses,’ (Wickham 2021b)Key concepts skillsReproducibility requirement, implies sharing data, code, environment.Reproducibility enhanced using R Markdown, R Projects, Git GitHub.R Markdown involves marking text certain types building document.R Projects enable file structure dependent specific directory set-.Git GitHub make easier share code data.\nGet latest changes: git pull.\nAdd updates: git add -.\nCheck everything: git status.\nCommit changes: git commit -m \"Short description changes\".\nPush changes GitHub: git push.\nGet latest changes: git pull.Add updates: git add -.Check everything: git status.Commit changes: git commit -m \"Short description changes\".Push changes GitHub: git push.Restart R often (‘Session’ -> ‘Restart R Clear Output’).Debugging skill, improves practice.One key debugging skill able make reproducible example reproduces issue others.Appropriate code structure comments critical aspect reproducibility help others understand.","code":""},{"path":"reproducible-workflows.html","id":"introduction","chapter":"4 Reproducible workflows","heading":"4.1 Introduction","text":"Suppose cancer choose black box AI surgeon explain works 90% cure rate human surgeon 80% cure rate. want AI surgeon illegal?Geoffrey Hinton, 20 February 2020.number one thing keep mind machine learning performance evaluated samples one dataset, model used production samples may necessarily follow characteristics… finance industry saying : “past performance guarantee future results.” model scoring X test dataset doesn’t mean perform level X next N situations encounters real world. future may like past.asking question, “rather use model evaluated 90% accurate, human evaluated 80% accurate,” answer depends whether data typical per evaluation process. Humans adaptable, models . significant uncertainty involved, go human. may inferior pattern recognition capabilities (versus models trained enormous amounts data), understand , can reason , can improvise faced noveltyIf every possible situation known want prioritize scalability cost-reduction, go model. Models exist encode operationalize human cognition well-understood situations. (“well understood” meaning either can explicitly described programmer, can amass dataset densely samples distribution possible situations – must static)François Chollet, 20 February 2020.science systematically building organizing knowledge terms testable explanations predictions, data science takes focuses data. means building, organizing, sharing knowledge critical aspect. Creating knowledge, , way can , meet standard. Hence, need reproducible workflows data science.M. Alexander (2019a) says ‘research reproducible can reproduced exactly, given materials used study… [hence] materials need provided!… [M]aterials usually means data, code software.’ minimum requirement another person able ‘reproduce data, methods results (including figures, tables).’ Similarly, Gelman (2016) identifies large issue various social sciences. problem work reproducible, contribute stock knowledge world. Since Gelman (2016), great deal work done many social sciences situation improved little, much work remains. situation similar life sciences (Heil et al. 2021) computer science (Pineau et al. 2021).examples Gelman (2016) talks , turned reproduce important scheme things. time, saw, continue see, similar approaches used areas big impacts. instance, many governments created ‘nudge’ units implement public policy (Sunstein Reisch 2017) increasingly using algorithms make open (Chouldechova et al. 2018). Similarly, businesses increasingly implementing machine learning methods.minimum, exceptions, must release code, datasets, environment. Without data, know finding speaks (Miyakawa 2020). banally, also know mistakes aspects inadvertently overlooked (Merali 2010) (Hillel 2017) (Silver 2020).specific, consider Y. Wang Kosinski (2018) use deep neural networks train model distinguish gay heterosexual men. (Murphy (2017) provides summary paper associated issues, along comments authors.) , Y. Wang Kosinski (2018, 248) needed dataset photos folks ‘adult, Caucasian, fully visible, gender matched one reported user’s profile.’ verified using Amazon Mechanical Turk, online platform pays workers small amount money complete specific tasks. Figure 4.1, Y. Wang Kosinski (2018) supplemental materials, shows instructions provided Mechanical Turk workers task. issues instructions include Obama white mother black father classified ‘Black’; Latino ethnicity, rather race (Mattson 2017). classification task may seem objective, , perhaps unthinkingly, echoes views Americans certain class background.\nFigure 4.1: Instructions given Mechanical Turk workers removing incomplete, non-Caucasian, nonadult, nonhuman male faces\njust one specific concern one part Y. Wang Kosinski (2018) workflow. Broader concerns raised others including Gelman, Mattson, Simpson (2018). main issue statistical models specific data trained. reason can identify likely issues model Y. Wang Kosinski (2018) , despite releasing specific dataset used, nonetheless open procedure. work credible, needs reproducible others.steps can take make work reproducible include:Ensure entire workflow documented may involve addressing questions :\nraw dataset obtained access likely persistent available others?\nspecific steps taken transform raw data data analyzed, can made available others?\nanalysis done, clearly can shared?\nfinal paper report built extent can others follow process ?\nraw dataset obtained access likely persistent available others?specific steps taken transform raw data data analyzed, can made available others?analysis done, clearly can shared?final paper report built extent can others follow process ?worrying perfect reproducibility initially, instead focusing trying improve successive project. instance, following requirements increasingly onerous need concerned able last, can first:\nCan run entire workflow ?\nCan ‘another person’ run entire workflow ?\nCan ‘future’ run entire workflow ?\nCan ‘future’ ‘another person’ run entire workflow ?\nCan run entire workflow ?Can ‘another person’ run entire workflow ?Can ‘future’ run entire workflow ?Can ‘future’ ‘another person’ run entire workflow ?Including detailed discussion limitations dataset approach final paper report.workflow follow summarized Figure 4.2.\nFigure 4.2: Workflow telling stories data\nvarious tools can use different stages improve reproducibility workflow. includes use R Markdown, R Projects, Git GitHub.","code":""},{"path":"reproducible-workflows.html","id":"r-markdown","chapter":"4 Reproducible workflows","heading":"4.2 R Markdown","text":"","code":""},{"path":"reproducible-workflows.html","id":"getting-started-1","chapter":"4 Reproducible workflows","heading":"4.2.1 Getting started","text":"R Markdown mark-language similar HyperText Markup Language (HTML) LaTeX, comparison ‘See Get’ (WYSIWYG) language, Microsoft Word. means aspects consistent, instance, top-level heading look . , means must use symbols designate like certain aspects appear. build mark-get see looks like.R Markdown variant Markdown specifically designed allow R code chunks included. One advantage can get ‘live’ document code executes forms part document. Another advantage R Markdown similar code can compile variety documents, including html pages PDFs. R Markdown also default options set including title, author, date sections. One disadvantage can take document compile code needs run. N. Tierney (2020) especially useful instructions achieve specific outcomes R Markdown.can create new R Markdown document within R Studio (‘File’ -> ‘New File’ -> ‘R Markdown Document’).","code":""},{"path":"reproducible-workflows.html","id":"essential-commands","chapter":"4 Reproducible workflows","heading":"4.2.2 Essential commands","text":"Essential markdown commands include emphasis, headers, lists, links, images. reminder included R Studio (‘Help’ -> ‘Markdown Quick Reference’).Emphasis: *italic*, **bold**Headers (go line blank line ): # First level header, ## Second level header, ### Third level headerUnordered list, sub-lists:Ordered list, sub-lists:URLs can added including address auto-link: https://www.tellingstorieswithdata.com, linking text [address book](https://www.tellingstorieswithdata.com) results address book.added aspects, may want see actual document. build document click ‘Knit.’","code":"* Item 1\n* Item 2\n    + Item 2a\n    + Item 2b1. Item 1\n2. Item 2\n3. Item 3\n    + Item 3a\n    + Item 3b"},{"path":"reproducible-workflows.html","id":"r-chunks","chapter":"4 Reproducible workflows","heading":"4.2.3 R chunks","text":"can include code R many languages code chunks within R Markdown document. knit document, code run included document.create R chunk, start three backticks within curly braces tell R Markdown R chunk. Anything inside chunk considered R code run . instance, load tidyverse AER make graph number times survey respondent visited doctor past two weeks.output code Figure 4.3.\nFigure 4.3: Number doctor visits past two weeks, based 1977–1978 Australian Health Survey\nvarious evaluation options available chunks. include putting comma r specifying options closing curly brace. Helpful options include:echo = FALSE: run code include output, print code document.include = FALSE: run code output anything print code document.eval = FALSE: run code, hence include outputs, print code document.warning = FALSE: display warnings.message = FALSE: display messages.instance, include output, code, suppress warnings.","code":"```{r}\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |>\n  ggplot(aes(x = illness)) +\n  geom_histogram(stat = \"count\")\n``````{r, echo = FALSE, warning = FALSE}\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits %>%\n  ggplot(aes(x = visits)) +\n  geom_histogram(stat = \"count\")\n```"},{"path":"reproducible-workflows.html","id":"abstracts-and-pdf-outputs","chapter":"4 Reproducible workflows","heading":"4.2.4 Abstracts and PDF outputs","text":"abstract short summary paper. default preamble, can add section abstract. Similarly, can change output html_document pdf_document produce PDF. uses LaTeX background may require installation supporting packages.","code":"---\ntitle: My document\nauthor: Rohan Alexander\ndate: 1 January 2022\noutput: pdf_document\nabstract: \"This is my abstract.\"\n---"},{"path":"reproducible-workflows.html","id":"references","chapter":"4 Reproducible workflows","heading":"4.2.5 References","text":"can include references specifying bib file preamble calling within text, needed.need make separate file called ‘bibliography.bib’ save next R Markdown file. bib file need entry item referenced. instance, citation R can obtained citation() can added ‘bibliography.bib’ file. Similarly, citation package can found including package name, instance citation('tidyverse'). can helpful use Google Scholar, doi2bib, get citations books articles.need create unique key use refer item text. can anything, provided unique, meaningful ones can easier remember, instance ‘citeR.’cite R R Markdown document include @citeR, put brackets around year, like : R Core Team (2021) [@citeR], put brackets around whole thing, like : (Wickham et al. 2019a).","code":"---\ntitle: My document\nauthor: Rohan Alexander\ndate: 1 January 2022\noutput: pdf_document\nabstract: \"This is my abstract.\"\nbibliography: bibliography.bib\n---@Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2021},\n    url = {https://www.R-project.org/},\n  }\n@Article{,\n    title = {Welcome to the {tidyverse}},\n    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},\n    year = {2019},\n    journal = {Journal of Open Source Software},\n    volume = {4},\n    number = {43},\n    pages = {1686},\n    doi = {10.21105/joss.01686},\n  }@Manual{citeR,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2021},\n    url = {https://www.R-project.org/},\n  }\n@Article{citetidyverse,\n    title = {Welcome to the {tidyverse}},\n    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},\n    year = {2019},\n    journal = {Journal of Open Source Software},\n    volume = {4},\n    number = {43},\n    pages = {1686},\n    doi = {10.21105/joss.01686},\n  }"},{"path":"reproducible-workflows.html","id":"cross-references","chapter":"4 Reproducible workflows","heading":"4.2.6 Cross-references","text":"can useful cross-reference figures, tables, equations. makes easier refer text. figure refer name R chunk creates contains figure. instance, (Figure \\@ref(fig:uniquename)) produce: (Figure 4.4) name R chunk uniquename. also need add ‘fig’ front chunk name R Markdown knows figure. include ‘fig.cap’ R chunk specifies caption.\nFigure 4.4: Number illnesses past two weeks, based 1977–1978 Australian Health Survey\ncan take similar, slightly different, approach cross-reference tables. instance, (Table \\@ref(tab:docvisittable)) produce: (Table 4.1). case specify ‘tab’ unique reference table, R Markdown knows table. tables need include caption main content, ‘caption,’ rather ‘fig.cap’ chunk option case figures.Table 4.1: Number visits doctor past two weeks, based 1977–1978 Australian Health SurveyFinally, can also cross-reference equations. need add tag (\\#eq:macroidentity) reference. instance, use Equation \\@ref(eq:macroidentity). produce Equation (4.1).\\[\\begin{equation}\nY = C + + G + (X - M) \\tag{4.1}\n\\end{equation}\\]using cross-references, important R chunks simple labels. general, try keep names simple unique, possible, avoid punctuation stick letters. use underbars labels cause error.","code":"```{r uniquename, fig.cap = \"Number of illnesses in the past two weeks, based on the 1977--1978 Australian Health Survey\", echo = TRUE}\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |>\n  ggplot(aes(x = illness)) +\n  geom_histogram(stat = \"count\")\nDoctorVisits |> \n  count(visits) |> \n  knitr::kable(caption = \"Number of visits to the doctor in the past two weeks, based on the 1977--1978 Australian Health Survey\")\\begin{equation}\nY = C + I + G + (X - M) (\\#eq:macroidentity)\n\\end{equation}"},{"path":"reproducible-workflows.html","id":"r-projects-and-file-structure","chapter":"4 Reproducible workflows","heading":"4.3 R projects and file structure","text":"can use R Studio create R project. means can keep files (data, analysis, report, etc) associated particular project together. project can created R Studio ‘File’ -> ‘New Project,’ select ‘empty project,’ name project decide save . instance, project focused maternal mortality, may called ‘maternalmortality,’ might saved within folder projects. use R Projects ‘practical convention creates reliable, polite behavior across different computers users time.’ (Jennifer Bryan Hester 2020). , projects ‘neither new, unique R,’ well-established part software development.project created, new file extension ‘.RProj’ appear folder. example, folder R Project, example R Markdown document, appropriate file structure available: https://github.com/RohanAlexander/starter_folder. can downloaded: ‘Code’ -> ‘Download ZIP.’main advantage using R Project easily able reference files self-contained way. means others want reproduce work, know file references structure need changed. means files referenced relation ‘.Rproj’ file . instance, instead reading csv , say, \"~/Documents/projects/book/data/\" can read book/data/. may someone else ‘projects’ folder, former work , latter .use R projects required meet minimal level reproducibility. use functions setwd(), computer-specific file paths, bind work computer way appropriate.variety ways set-folder. variant Wilson et al. (2017) often useful shown example: https://github.com/RohanAlexander/starter_folder. ‘inputs’ folder contains raw data (never modified (Wilson et al. 2017)) literature related project (modified). ‘outputs’ folder contains data create using R, well paper writing. ‘scripts’ folder modifies raw data saves ‘outputs.’ Useful aspects include ‘README.md’ specify overview details project, LICENSE.","code":""},{"path":"reproducible-workflows.html","id":"git-and-github","chapter":"4 Reproducible workflows","heading":"4.4 Git and GitHub","text":"use combination Git GitHub :enhance reproducibility work making easier share code data;make easier share work;improve workflow encouraging systematic approaches; andmake easier work teams.Git version control system. One way one often starts version control various versions one file: ‘first_go.R,’ ‘first_go-fixed.R,’ ‘first_go-fixed--mons-edits.R.’ soon becomes cumbersome. One often soon turns dates, instance: ‘2022-01-01-analysis.R,’ ‘2022-01-02-analysis.R,’ ‘2022-01-03-analysis.R,’ etc. keeps record can difficult search need go back, can difficult remember date change made. case, quickly gets unwieldy project regularly worked .Instead , use Git can one version file, say, ‘analysis.R’ use Git keep record changes file, snapshot file given point time.determine Git takes snapshot, take snapshot, additionally include message saying changed snapshot last. way, ever one version file, history can easily searched.issue Git designed software developers. , works, can little ungainly non-developers (Figure 4.5).\nFigure 4.5: infamous response launch Dropbox 2007, trivializing use-case Dropbox, user’s approach probably work , probably folks.\nHence, GitHub, GitLab, various companies offer easier--use services build Git. introduce GitHub ‘far dominant code-hosting platform’ (Eghbal 2020, 21) built R Studio, options advantages.One challenging aspects Git terminology. Folders called ‘repos.’ Saving called ‘commit.’ One gets used eventually, initially feeling confused entirely normal.Jenny Bryan (2020) especially useful setting using Git GitHub.","code":""},{"path":"reproducible-workflows.html","id":"git","chapter":"4 Reproducible workflows","heading":"4.4.1 Git","text":"need git check whether Git installed. Open R Studio, go Terminal, type following, enter/return.get version number, done (Figure 4.6).\nFigure 4.6: access Terminal within R Studio\nMac Git come pre-installed, Windows chance, Linux probably need guide. get version number, need install . follow instructions specific operating system Chapter 5 Jenny Bryan (2020).Git, need tell username email. need Git adds information whenever take ‘snapshot,’ use Git’s language, whenever make commit., within Terminal, type following, replacing details , enter/return line.details enter public. various ways hide email address need GitHub provides instructions . , issues, need detailed instructions step, please see Chapter 7 Jenny Bryan (2020).","code":"git --versiongit config --global user.name 'Rohan Alexander'\ngit config --global user.email 'rohan.alexander@utoronto.ca'\ngit config --global --list"},{"path":"reproducible-workflows.html","id":"github","chapter":"4 Reproducible workflows","heading":"4.4.2 GitHub","text":"Now Git set-need set-GitHub. first step create account GitHub: https://github.com (Figure 4.7).\nFigure 4.7: GitHub sign-screen\nnow need make new folder (called ‘repo’ Git). Look ‘+’ top right, select ‘New Repository’ (Figure 4.8).\nFigure 4.8: Start process creating new repository\npoint can add sensible name repo. Leave public now, can always deleted later. check box ‘Initialize repository README.’ Leave ‘Add .gitignore’ set ‘None.’ , click ‘Create repository’ (Figure 4.9).\nFigure 4.9: Finish creating new repository\ntake us screen fairly empty, details need green ‘Clone Download’ button, can copy clicking clipboard (Figure 4.10).\nFigure 4.10: Get details new repository\nNow returning R Studio, open Terminal, use cd navigate directory want create folder. type following, replacing repo details , enter/return.point, new folder created can now use .use GitHub project actively working follow procedure:first thing almost always pull latest changes git pull. , open Terminal, navigate folder using cd. type git pull enter/return.can make change folder, instance, update README, save normal.done, need ‘add,’ ‘commit,’ ‘push.’\n, use cd navigate folder, type git status enter/return see changes (see reference change made).\ntype git add -enter/return. adds changes staging area.\ntype git status enter/return verify happy added.\ntype git commit -m \"Minor update README\" enter/return. message informative change made.\n, type git status enter/return check everything.\nFinally, type git push enter/return push everything GitHub.\n, use cd navigate folder, type git status enter/return see changes (see reference change made).type git add -enter/return. adds changes staging area.type git status enter/return verify happy added.type git commit -m \"Minor update README\" enter/return. message informative change made., type git status enter/return check everything.Finally, type git push enter/return push everything GitHub.summarize workflow (assuming already relevant folder):","code":"git clone https://github.com/RohanAlexander/test.gitgit pull\ngit status\ngit add -A\ngit status\ngit commit -m \"Short commit message\"\ngit status\ngit push"},{"path":"reproducible-workflows.html","id":"using-github-within-r-studio-with-the-git-pane","chapter":"4 Reproducible workflows","heading":"4.4.3 Using GitHub within R Studio with the Git pane","text":"procedure went useful better understand happening use Git GitHub can bit cumbersome. Usefully, GitHub built R Studio can use Git pane move away Terminal.Get started creating new repo GitHub, , copy repo information, . point, open R Studio, create new project using version control (‘Files’ -> ‘New Project’ -> ‘Version Control’ -> ‘Git,’ paste information repo). Follow rest set-naming project something sensible, saving somewhere sensible, clicking ‘Open new session,’ creating project. create new folder R Project Git-initialized linked GitHub repo.open R Project, ‘Git’ tab (Figure 4.11).\nFigure 4.11: Git pane R Studio\ncan use Git tab. , first want ‘pull,’ can clicking blue arrow. , want commit files changes, selecting ‘staged’ checkbox files like commit. ‘Commit.’ , want include message commit, typing message ‘Commit message’ box ‘Commit.’ Finally, can ‘Push.’ details workflow available Chapter 12 Jenny Bryan (2020).","code":""},{"path":"reproducible-workflows.html","id":"using-github-with-usethis","chapter":"4 Reproducible workflows","heading":"4.4.4 Using GitHub with usethis","text":"used Git pane R Studio reduce need use Terminal, remove need go GitHub set-new project. set-Git GitHub, can improve workflow using usethis much work (Wickham Bryan 2020).installing loading usethis need check set-Git usethis::git_sitrep(). print information user. can use usethis::use_git_config() update username email address. instance,can create new R Project (‘File’ -> ‘New Project’ -> ‘New Directory’ -> ‘New Project’ -> Add name save sensible location click ‘Open new session.’)use usethis::use_git() initiate things. ask happy commit files.committed, can use usethis::use_github() push GitHub.","code":"\nlibrary(usethis)\n\nuse_git_config(user.name = \"Rohan Alexander\", user.email = \"rohan.alexander@utoronto.ca\")"},{"path":"reproducible-workflows.html","id":"using-r-in-practice","chapter":"4 Reproducible workflows","heading":"4.5 Using R in practice","text":"","code":""},{"path":"reproducible-workflows.html","id":"dealing-with-errors","chapter":"4 Reproducible workflows","heading":"4.5.1 Dealing with errors","text":"programming, eventually code break, say eventually, mean like probably 10 20 times day.Gelfand (2021)Everyone uses R, programming language matter, trouble find point. normal. Programming hard everyone struggles sometimes. point code run throw error. normal, happens everyone. Everyone gets frustrated.move forward develop strategies work issues:get error message, sometimes useful. Try read carefully see anything use .Try search, say Google, error message. can useful include ‘tidyverse’ ‘R’ search help make results appropriate. Sometimes Stack Overflow results can useful.Look help file function, putting ‘?’ function, instance, ?pivot_wider(). common issue use slightly incorrect argument name format, accidentally including string instead object name.Look error happening remove code error resolved, slowly add code back .Check class object, class(), instance, class(data_set$data_column). Ensuring expected.Restart R (‘Session’ -> ‘Restart R Clear Output’) load everything .Restart computer.Search trying , rather error, sure include ‘tidyverse’ ‘R’ search help make results appropriate. instance, ‘save PDF graph R made using ggplot.’ Sometimes relevant blog posts Stack Overflow answers help.Making small, self-contained, reproducible example ‘reprex’ see issue can isolated enable others help.generally, rarely possible , almost always helpful take break come back next day.","code":""},{"path":"reproducible-workflows.html","id":"reproducible-examples","chapter":"4 Reproducible workflows","heading":"4.5.2 Reproducible examples","text":"one can advise help —one. one thing . Go .Rilke (1929)Asking help skill like . get better practice. important try say ‘doesn’t work,’ ‘tried everything,’ ‘code work,’ ‘error message, ?’ general, possible help based comments, many possible issues. need make easy others help . involves steps.Provide small, self-contained, example data, code, detail going wrong.Document tried far, including Stack Overflow R Studio Community pages looked , quite ?clear outcome like.Begin creating minimal REPRoducible EXample, ‘reprex.’ code contains needed reproduce error, needed. means code likely smaller, simpler, version nonetheless reproduces error.Sometimes process enables one solve problem. , gives someone else fighting chance able help. important recognize almost chance got problem someone addressed . likely main difficulty trying communicate trying happening, way allows others recognize . Developing tenacity important.develop reproducible examples, reprex package (Jennifer Bryan et al. 2019) especially useful. use :Load reprex package: library(reprex).Highlight copy code giving issues.Run reprex() Console.code self-contained, preview Viewer. , error, code needs re-written self-contained.need data reproduce error, use data built R. large number datasets built R can seen using library(help = \"datasets\"). possible, use common option ‘mtcars’ ‘faithful.’","code":""},{"path":"reproducible-workflows.html","id":"mentality","chapter":"4 Reproducible workflows","heading":"4.5.3 Mentality","text":"(Y)ou real, valid, competent user programmer matter IDE develop tools use make work work (L)et’s break gates, ’s enough room everyoneSharla Gelfand, 10 March 2020.write code, programmer regardless , using , . traits one tends notice great programmers common.Focused: Often aim ‘learn R’ something similar tends problematic, real end point . tends efficient smaller, specific goals, ‘make histogram 2019 Canadian Election ggplot.’ something can focused achieved hours. issue goals nebulous, ‘want learn R,’ becomes easy get lost tangents, much difficult get help. can demoralizing lead folks quitting early.Curious: almost always useful go. general, worst happens waste time. can rarely break something irreparably code. want know happens pass ‘vector’ instead ‘dataframe’ ggplot() try .Pragmatic: time, can useful stick within reasonable bounds, make one small change time. instance, say want run regressions, curious possibility using tidymodels package (Kuhn Wickham 2020) instead lm(). pragmatic way proceed use one aspect tidymodels package initially make another change next time.Tenacious: , balancing act. always unexpected problems issues every project. one hand, persevering despite good tendency. hand, sometimes one need prepared give something seem like break-possible. mentors can useful tend better judge reasonable. also appropriate planning useful.Planned: almost always useful excessively plan going . instance, may want make histogram 2019 Canadian Election. plan steps needed even sketch step might implemented. instance, first step get data. packages might useful? might data ? back-plan data exist ?Done better perfect: various perfectionist tendencies certain extent, can useful initially try turn certain extent. first instance, try write code works, especially early days. can always come back improve aspects . important actually ship. Ugly code gets job done, better beautiful code never finished.","code":""},{"path":"reproducible-workflows.html","id":"code-comments-and-style","chapter":"4 Reproducible workflows","heading":"4.5.4 Code comments and style","text":"Code must commented (Lee 2018). Comments focus certain code written, (lesser extent, common option selected).one way write code, especially R. However, general guidelines make easier even just working . important recognize projects evolve time, one purpose served code comments ‘[m]essages left future self (near-future others) [] help retrace justify decisions’ (Bowers 2011).Comments R can added including # symbol. put comment start line, can midway . general, need comment every aspect code comment parts obvious. instance, read value may like comment coming .comment something (Wickham 2021b). trying achieve?must comment explain weird things. Like removing specific row, say row 27, removing row? may seem obvious moment, future-six months remember.break code sections. instance, setting workspace, reading datasets, manipulating cleaning dataset, analyzing datasets, finally producing tables figures. separated comments explaining going , sometimes separate files, depending length.Additionally, top file important basic information, purpose file, pre-requisites dependencies, date, author contact information, finally red-flags todos.least every R script needs preamble clear demarcation sections.","code":"#### Preamble ####\n# Purpose: Brief sentence about what this script does\n# Author: Your name\n# Data: The date it was written\n# Contact: Add your email\n# License: Think about how your code may be used\n# Pre-requisites: \n# - Maybe you need some data or some other script to have been run?\n\n\n#### Workspace setup ####\n# do not keep the install.packages line - comment out if need be\n# Load libraries\nlibrary(tidyverse)\n\n# Read in the raw data. \nraw_data <- readr::read_csv(\"inputs/data/raw_data.csv\")\n\n\n#### Next section ####\n...\n"},{"path":"reproducible-workflows.html","id":"exercises-and-tutorial-3","chapter":"4 Reproducible workflows","heading":"4.6 Exercises and tutorial","text":"","code":""},{"path":"reproducible-workflows.html","id":"exercises-3","chapter":"4 Reproducible workflows","heading":"4.6.1 Exercises","text":"According M. Alexander (2019a) research reproducible (pick one)?\npublished peer-reviewed journals.\nmaterials used study provided.\ncan reproduced exactly without authors providing materials.\ncan reproduced exactly, given materials used study.\npublished peer-reviewed journals.materials used study provided.can reproduced exactly without authors providing materials.can reproduced exactly, given materials used study.According timeline Gelman (2016), ) Paul Meehl identify various issues; b) null hypothesis significance testing (NHST) become controversial (pick one)?\n1970s-1980s; 1990s-2000.\n1960s-1970s; 1980s-1990.\n1970s-1980s; 1980s-1990.\n1960s-1970s; 1990s-2000.\n1970s-1980s; 1990s-2000.1960s-1970s; 1980s-1990.1970s-1980s; 1980s-1990.1960s-1970s; 1990s-2000.following components project layout recommended Wilson et al. (2017) (select apply)?\nrequirements.txt\ndoc\ndata\nLICENSE\nCITATION\nREADME\nsrc\nresults\nrequirements.txtdocdataLICENSECITATIONREADMEsrcresultsBased M. Alexander (2021) please write paragraph barriers overcame, still face, regard sharing code wrote.According Gelfand (2021), key part ‘need help getting unstuck, first step create reprex, reproducible example. goal reprex package problematic code way people can run feel pain. , hopefully, can provide solution put misery.’ (pick one)?\npackage problematic code\npeople can run feel pain\nfirst step create reprex\ncan provide solution put misery\npackage problematic codeother people can run feel painthe first step create reprexthey can provide solution put miseryAccording Gelfand (2021), three key aspects reprex (select apply)?\ndata\nlibraries necessary libraries necessary\nrelevant code relevant code\ndataonly libraries necessary libraries necessaryrelevant code relevant codeAccording Wickham (2021b) naming files, files ‘00_get_data.R,’ ‘get data.R’ classified (pick one)?\nbad; bad.\ngood; bad.\nbad; good.\ngood; good.\nbad; bad.good; bad.bad; good.good; good.following result bold text R Markdown (pick one)?\n**bold**\n##bold##\n*bold*\n#bold#\n**bold**##bold##*bold*#bold#option hide warnings R Markdown R chunk (pick one)?\necho = FALSE\ninclude = FALSE\neval = FALSE\nwarning = FALSE\nmessage = FALSE\necho = FALSEinclude = FALSEeval = FALSEwarning = FALSEmessage = FALSEWhich options run R code chunk display results, show code R Markdown R chunk (pick one)?\necho = FALSE\ninclude = FALSE\neval = FALSE\nwarning = FALSE\nmessage = FALSE\necho = FALSEinclude = FALSEeval = FALSEwarning = FALSEmessage = FALSEWhy R Projects important (select apply)?\nhelp reproducibility.\nmake easier share code.\nmake workspace organized.\nensure reproducibility.\nhelp reproducibility.make easier share code.make workspace organized.ensure reproducibility.Please discuss circumstance R Project useful.Consider sequence: ‘git pull, git status, ________, git status, git commit -m \"message\", git push.’ missing step (pick one)?\ngit add -.\ngit status.\ngit pull.\ngit push.\ngit add -.git status.git pull.git push.Assuming libraries datasets loaded, mistake code: DoctorVisits |> select(\"visits\") (pick one)?\n\"visits\"\nDoctorVisits\nselect\n|>\n\"visits\"DoctorVisitsselect|>reprex important able make one (select apply)?\nreproducible example enables error reproduced.\nreproducible example helps others help .\nreproducible example construction may solve problem.\nreproducible example demonstrates actually tried help .\nreproducible example enables error reproduced.reproducible example helps others help .reproducible example construction may solve problem.reproducible example demonstrates actually tried help .following code produces error. Please use reprex (Jennifer Bryan et al. 2019) build reproducible example use get help , submit reprex.","code":"\nlibrary(tidyverse)\n\noecd_gdp <- \n  read_csv(\"https://stats.oecd.org/sdmx-json/data/DP_LIVE/.QGDP.../OECD?contentType=csv&detail=code&separator=comma&csv-lang=en\")\n\nhead(oecd_gdp)\n\nlibrary(forcats)\nlibrary(dplyr)\n\noecd_gdp_most_recent <- \n  oecd_gdp |> \n  filter(TIME == \"2021-Q3\",\n         SUBJECT == \"TOT\",\n         LOCATION %in% c(\"AUS\", \"CAN\", \"CHL\", \"DEU\", \"GBR\",\n                         \"IDN\", \"ESP\", \"NZL\", \"USA\", \"ZAF\"),\n         MEASURE == \"PC_CHGPY\") |> \n  mutate(european = if_else(LOCATION %in% c(\"DEU\", \"GBR\", \"ESP\"),\n                             \"European\",\n                             \"Not european\"),\n         hemisphere = if_else(LOCATION %in% c(\"CAN\", \"DEU\", \"GBR\", \"ESP\", \"USA\"),\n                             \"Northern Hemisphere\",\n                             \"Southern Hemisphere\"),\n         )\n\nlibrary(ggplot)\nlibrary(patchwork)\n\noecd_gdp_most_recent |> \n  ggplot(mapping = aes(x = LOCATION, y = Value)) |> \n  geom_bar(stat=\"identity\")"},{"path":"reproducible-workflows.html","id":"tutorial-3","chapter":"4 Reproducible workflows","heading":"4.6.2 Tutorial","text":"Please put together small R Markdown file downloads dataset using opendatatoronto, cleans , makes graph. exchange someone else. Ask read code run , provide feedback aspects. Write one--two pages single-spaced content, comments received changes make going forward.","code":""},{"path":"reproducible-workflows.html","id":"paper","chapter":"4 Reproducible workflows","heading":"4.6.3 Paper","text":"point, Paper One (Appendix B.1) appropriate.","code":""},{"path":"on-writing.html","id":"on-writing","chapter":"5 On writing","heading":"5 On writing","text":"want writer, must two things others: read lot write lot. ’s way around two things ’m aware , shortcut.S. King (2000, 145)Required materialRead Writing Well, (edition fine) Parts ‘Principles,’ II ‘Methods,’ (Zinsser 1976).Read Publication, publication, (G. King 2006).Read two following well-written quantitative papers:\nAsset prices exchange economy, (Lucas Jr 1978).\nIndividuals, institutions, innovation debates French Revolution, (Barron et al. 2018).\nModeling: optimal marathon performance basis physiological factors, (Joyner 1991).\nreproducible econometric research, (Koenker Zeileis 2009).\nPrevented mortality greenhouse gas emissions historical projected nuclear power, (Kharecha Hansen 2013).\nSample selection bias specification error, (Heckman 1979).\nSeeing like market, (Fourcade Healy 2017).\nSimpson’s paradox hot hand basketball, (Wardrop 1995).\nSmoking carcinoma lung, (Doll Hill 1950).\nstudies machine learning using game checkers, (Samuel 1959).\nStatistical methods assessing agreement two methods clinical measurement, (Bland Altman 1986).\nmundanity excellence: ethnographic report stratification Olympic swimmers, (Chambliss 1989).\nprobable error mean, (Student 1908).\nAsset prices exchange economy, (Lucas Jr 1978).Individuals, institutions, innovation debates French Revolution, (Barron et al. 2018).Modeling: optimal marathon performance basis physiological factors, (Joyner 1991).reproducible econometric research, (Koenker Zeileis 2009).Prevented mortality greenhouse gas emissions historical projected nuclear power, (Kharecha Hansen 2013).Sample selection bias specification error, (Heckman 1979).Seeing like market, (Fourcade Healy 2017).Simpson’s paradox hot hand basketball, (Wardrop 1995).Smoking carcinoma lung, (Doll Hill 1950).studies machine learning using game checkers, (Samuel 1959).Statistical methods assessing agreement two methods clinical measurement, (Bland Altman 1986).mundanity excellence: ethnographic report stratification Olympic swimmers, (Chambliss 1989).probable error mean, (Student 1908).Read two following articles New Yorker:\nFunny Like Guy, Tad Friend\nGoing Distance, David Remnick\nHappy Feet, Alexandra Jacobs\nLevels Game, John McPhee\nReporting Hiroshima, John Hersey\nCatastrophist, Elizabeth Kolbert\nQuiet German, George Packer\nFunny Like Guy, Tad FriendGoing Distance, David RemnickHappy Feet, Alexandra JacobsLevels Game, John McPheeReporting Hiroshima, John HerseyThe Catastrophist, Elizabeth KolbertThe Quiet German, George PackerRead two following articles miscellaneous publications:\nBlades Glory, Holly Anderson\nBorn Run, Walt Harrington\nDropped, Jason Fagone\nFederer Religious Experience, David Foster Wallace\nGeneration ?, Zadie Smith\nOne hundred years arm bars, David Samuels\nGreat Alone, Brian Phillips\nPearls Breakfast, Gene Weingarten\nCult ‘Jurassic Park’, Bryan Curtis\nHouse Hova Built, Zadie Smith\nRe-Education Chris Copeland, Flinder Boyd\nSea Crisis, Brian Phillips\nBlades Glory, Holly AndersonBorn Run, Walt HarringtonDropped, Jason FagoneFederer Religious Experience, David Foster WallaceGeneration ?, Zadie SmithOne hundred years arm bars, David SamuelsOut Great Alone, Brian PhillipsPearls Breakfast, Gene WeingartenThe Cult ‘Jurassic Park’, Bryan CurtisThe House Hova Built, Zadie SmithThe Re-Education Chris Copeland, Flinder BoydThe Sea Crisis, Brian PhillipsKey concepts skillsTo get better writing, write, ideally every day.Write reader.one message want communicate.Get first draft quickly possible.Rewrite brutally.Remove many words possible.","code":""},{"path":"on-writing.html","id":"introduction-1","chapter":"5 On writing","heading":"5.1 Introduction","text":"[T]duty scientist find new things, communicate successfully least three forms: 1) Writing papers books. 2) Prepared public talks. 3) Impromptu talks.Hamming (1996, 65)People need write: founders, VCs, lawyers, software engineers, designers, painters, data scientists, musicians, filmmakers, creative directors, physical trainers, teachers, writers.\nLearn write.Sahil Lavingia, 3 February 2020.Writing well done just much knowing code. ’d add ’re intimidated writing, start blog write often something ’re interested . ’ll get better. least ’s ’ve done past 10 years. :)Vicki Boykis, 3 February 2020.need write order tell stories. Writing allows us communicate efficiently. also way work believe allows us get feedback ideas. Effective papers tightly written well-organized, makes story flow easy follow. Proper sentence structure, spelling, vocabulary, grammar important remove distractions enable point clearly articulated. Effective papers demonstrate understanding topic confidently using relevant terms techniques considering issues without overly verbose. Graphs, tables, references used enhance story credibility.chapter writing. end , better idea write short, detailed, quantitative papers communicate want , waste reader’s time. write reader, . Specifically, write useful reader, ‘[u]seful writing tells people something true important didn’t already know, tells unequivocally possible’ (Graham 2020). said, greatest benefit writing nonetheless often accrues writer, even write audience. process writing way work think came believe .way piece writing three four times , never . , hardest part comes first, getting something—anything—front . Sometimes nervous frenzy just fling words flinging mud wall. Blurt , heave , babble something—anything—first draft. , achieved sort nucleus. , work alter , begin shape sentences score higher ear eye. Edit —top bottom. chances now ’ll seeing something sort eager others see. takes times. left interstitial time. finish first awful blurting, put thing aside. get car drive home. way, mind still knitting words. think better way say something, good phrase correct certain problem. Without drafted version—exist—obviously thinking things improve . short, may actually writing two three hours day, mind, one way another, working twenty-four hours day—yes, sleep—sort draft earlier version already exists. exists, writing really begun.McPhee (2017, 159)process writing process re-writing. critical task get first draft quickly possible. complete first draft five--ten-page quantitative paper can done day. complete first draft exists, useful try delete even revise anything written, regardless bad may seem. Just write.One intimidating things world blank page, deal immediately adding headings : ‘Introduction,’ ‘Data,’ ‘Model,’ ‘Results,’ ‘Discussion.’ add fields top matter various bits pieces needed, ‘title,’ ‘date,’ ‘author’ ‘abstract.’ creates generic outline, role akin placing counter, ingredients use prepare dinner (McPhee 2017).established generic outline, need develop understanding exploring developing research question. theory, develop research question, answer , writing; rarely actually happens (Franklin 2005). Instead, typically idea question, answer, become less vague write. process writing refine thinking (S. King 2000, 131). put thoughts research question, can start add dot points sections, adding sub-sections, informative sub-headings needed. go back expand dot points paragraphs.writing first draft important ignore feeling one good enough, impossible. Just write. need words paper, even bad, first draft accomplish . Remove distractions just write. Perfectionism enemy, set aside. Sometimes can accomplished getting early write, creating deadline, glass two wine. One friend puts baby sleep sound typing, result must keep typing otherwise baby wake . Creating sense urgency can useful rather adding proper citations go, slow us , just add something like ‘[TODO: CITE R ].’ similar graphs tables. , include textual descriptions ‘[TODO: ADD GRAPH SHOWS COUNTRY TIME ]’ instead actual graphs tables. Focus adding content, even poorly written, ideal. done, first draft exists!first draft bad. writing bad first draft can get good second draft, great third draft, eventually excellence (Lamott 1994, 20). first draft long, make sense, contain claims supported, claims . focused adding content writing first draft, turn second draft, use ‘delete’ key extensively, well ‘cut’ ‘paste.’ Printing paper using red pen move remove especially helpful. process going first draft second draft best done one sitting, help flow consistency story. One aspect first re-write enhancing story want tell. another aspect taking everything story (S. King 2000, 57).go written sections, try bring sense , special consideration supports story. revision process essence writing (McPhee 2017, 160). also fix references, add real graphs tables. part re-writing process, paper’s central message tends develop, answers research questions tend become clearer. point, aspects introduction can returned , finally, abstract. Typos issues affect credibility work, important fixed part second draft.now paper sensible. job now make brilliant. Print , go paper. especially important brutally remove everything contribute story. stage, may starting get close paper. write reader, great opportunity give someone else comments. ask feedback enables us better understand weak parts story. addressing , can helpful go paper , time reading aloud. paper tends never ‘done’ certain point either run time become sick sight .","code":""},{"path":"on-writing.html","id":"developing-research-questions","chapter":"5 On writing","heading":"5.2 Developing research questions","text":"qualitative quantitative approaches place, focus quantitative approaches. Qualitative research important well, often interesting work little . conducting quantitative analysis, subject issues data quality, scales, measurement, sources. often especially interested trying tease causality. Regardless, trying learn something world. research questions need take account.Broadly, two ways go research:data-first; orquestion-first.","code":""},{"path":"on-writing.html","id":"data-first","chapter":"5 On writing","heading":"5.2.1 Data-first","text":"data-first, main issue working questions can reasonably answered available data. deciding , useful consider:Theory: reasonable expectation something causal determined? instance, question involves charting stock market, might better consider haruspex least way something eat. Questions usually need plausible theoretical underpinning help avoid spurious relationships.Importance: plenty trivial questions can answered, important waste time reader. important question can also help motivation find , say, fourth straight week cleaning data de-bugging code. can also make easier attract talented employees funding. said, balance needed. important question decent chance answered. attacking generational-defining question might best broken smaller chunks.Availability: reasonable expectation additional data available future? allow us answer related questions turn one paper research agenda.Iteration: something run multiple times, -analysis? former, becomes possible start answering specific research questions iterate. can get access data need think broader questions.’s saying, sometimes attributed Xiao-Li Meng statistics missing data problem. paradoxically, another way ask data-first questions think data . instance, returning neonatal maternal mortality examples discussed , respectively, Chapters 1 2, fundamental problem perfect complete data cause death. , count number relevant deaths. established missing data problem, can take data-driven approach looking data , ask research questions speak extent can use approximate hypothetical perfect complete dataset.One way researchers data-first, develop particular expertise data geographical historical circumstance. instance, may especially knowledgeable present-day UK, late nineteenth century Japan. look questions researchers asking circumstances, bring data question. instance, common see particular question initially asked US, host researchers answer question UK, Canada, Australia, many countries.variant data-driven research model-driven research. researcher becomes expert particular statistical approach applies approach whenever appropriate datasets.","code":""},{"path":"on-writing.html","id":"question-first","chapter":"5 On writing","heading":"5.2.2 Question-first","text":"trying question-first, inverse different issue concerned data availability. ‘FINER framework’ used medicine help guide development research questions. recommends asking questions : Feasible, Interesting, Novel, Ethical, Relevant (Hulley 2007). Farrugia et al. (2010) builds FINER PICOT, recommends additional considerations: Population, Intervention, Comparison group, Outcome interest, Time. can feel overwhelming trying write question. One way go ask specific question. Another decide whether interested descriptive, predictive, inferential, causal analysis.lead different types questions, instance, descriptive analysis: ‘\\(x\\) look like?’; predictive analysis: ‘happen \\(x\\)?’; inferential: ‘can explain \\(x\\)?’; causal: ‘impact \\(x\\) \\(y\\)?’ role play.Often time constrained, possibly interesting ways can guide specifics research question. interested effect Trump’s tweets stock market, can done just looking minutes (milliseconds?) tweets. interested effect cancer drug long term outcomes? effect takes 20 years, must either wait , need look people treated 2000, selection effects different circumstances give drug today. Often reasonable thing build statistical model, need adequate sample sizes, etc.answering questions usually, creation counterfactual crucial. Briefly, counterfactual -statement ‘’ false. Consider example Humpty Dumpty Lewis Carroll’s Looking-Glass (Carroll 1871).‘tremendously easy riddles ask!’ Humpty Dumpty growled . ‘course don’t think ! , ever fall —’s chance ——’ pursed lips looked solemn grand Alice hardly help laughing. ‘fall,’ went , ‘King promised —mouth---’Humpty satisfied happen fall , even though similarly satisfied never happen. comparison group often determines answer question. instance, consider effect VO2 max outcome bike race. compare general population important variable, compare elite athletes, less important, selection.","code":""},{"path":"on-writing.html","id":"writing","chapter":"5 On writing","heading":"5.3 Writing","text":"indeed published anything commenced “Professor,” many crude effort, destroyed almost soon composed, got taste might ornamented redundant composition, come prefer plain homely.Professor (Brontë 1857).discuss following components: title, abstract, introduction, data, results, discussion, figures, tables, equations, technical terms. Throughout sections paper important brief specific possible.","code":""},{"path":"on-writing.html","id":"title","chapter":"5 On writing","heading":"5.3.1 Title","text":"title first opportunity engage reader story. Ideally, able tell reader exactly found. Effective titles critical otherwise papers ignored readers. title ‘cute,’ need effective. means needs make story clear.One example title good enough ‘2016 Brexit referendum.’ title useful reader least knows paper . particular informative enticing. slightly better variant ‘’Vote Leave’ outcome 2016 Brexit referendum’. variant adds specifically particularly informative. Finally, another variant ‘Vote Leave outperforms rural areas 2016 Brexit referendum: Evidence Bayesian hierarchical model.’ reader knows approach paper also main take-away.consider examples particularly effective titles. Hug et al. (2019) uses ‘National, regional, global levels trends neonatal mortality 1990 2017, scenario-based projections 2030: systematic analysis.’ clear paper methods used. R. Alexander Alexander (2021) uses ‘Increased Effect Elections Changing Prime Ministers Topics Discussed Australian Federal Parliament 1901 2018.’ method used paper clear title, main finding , along good deal information content . finally, M. J. Alexander, Kiang, Barbieri (2018) uses ‘Trends Black White Opioid Mortality United States, 1979–2015.’title often among last aspects paper finalized. getting first draft, typically just use working title good enough get job done. refine course redrafting. title needs reflect final story paper, usually something know start. interested striking balance getting reader interested enough read paper, conveying enough content useful (Hayot 2014). can think classic books, Macaulay’s History England Accession James Second, Churchill’s History English-Speaking Peoples. clear content , , target audience, spark interest.One specific approach form: ‘Exciting content: Specific content,’ instance, ‘Returning roots: Examining performance ’Vote Leave’ 2016 Brexit referendum’. Kennedy Gelman (2020) provides particular nice example approach ‘Know population know model: Using model-based regression poststratification generalize findings beyond observed sample,’ Craiu (2019) ‘Hiring Gambit: Search Twofer Data Scientist.’ close variant ‘question? answer.’ instance, Cahill, Weinberger, Alkema (2020) ‘increase modern contraceptive use needed FP2020 countries reach 75% demand satisfied 2030? assessment using Accelerated Transition Method Family Planning Estimation Model.’ one gains experience variant, becomes possible know appropriate drop answer part yet remain effective, Briggs (2021) ‘Aid Target Poorest?’ Another specific approach ‘Specific content broad content’ inversely. instance, ‘Rurality, elites, support ’Vote Leave’ 2016 Brexit referendum’ ‘Support ’Vote Leave’ 2016 Brexit referendum, rurality elites. approach used Tolley Paquet (2021) ‘Gender, municipal party politics, Montreal’s first woman mayor.’","code":""},{"path":"on-writing.html","id":"abstract","chapter":"5 On writing","heading":"5.3.2 Abstract","text":"five--ten-page paper, good abstract three five sentence paragraph. longer paper abstract can slightly longer. abstract needs specify story paper, objective abstract convey done matters. abstract typically touches context work, objectives, approach, findings.specifically, good recipe abstract : first sentence: specify general area paper encourage reader; second sentence: specify dataset methods general level; third sentence: specify headline result; fourth sentence implications.see pattern variety abstracts. instance, Tolley Paquet (2021) draw reader first sentence mentioning election first woman mayor 400 years. second sentence clear done paper. third paper tells reader done .e. survey. fourth sentence adds detail. fifth final sentence makes main take-away paper clear.2017, Montreal elected Valérie Plante, first woman mayor city’s 400-year history. Using election case study, show gender influence outcome. survey Montreal electors suggests gender salient factor vote choice. Although gender matter much voters, shape organization campaign party. argue Plante’s victory can explained part strategy showcased less leader-centric party degendered campaign helped counteract stereotypes women’s unsuitability positions political leadership.Similarly, Beauregard Sheppard (2021) make broader environment clear within first two sentences, specific contribution paper environment. third fourth sentences makes data source clear also main findings. fifth sixth sentences add specificity interest likely readers abstract .e. academic political science experts. final sentence makes clear position authors.Previous research support gender quotas focuses attitudes toward gender equality government intervention explanations. argue role attitudes toward women understanding support policies aiming increase presence women politics ambivalent—hostile benevolent forms sexism contribute understanding support, albeit different ways. Using original data survey conducted probability-based sample Australian respondents, findings demonstrate hostile sexists likely oppose increasing women’s presence politics adoption gender quotas. Benevolent sexists, hand, likely support policies respondents exhibiting low levels benevolent sexism. argue benevolent sexism holds women pure need protection; takes succeed politics without assistance quotas. Finally, show women likely support quotas, ambivalent sexism relationship support among women men. findings suggest aggregate levels public support gender quotas necessarily represent greater acceptance gender equality generally.finally, Briggs (2021) begins claim seems unquestionably true. second sentence claims found false. third sentence specifies extent claim, fourth sentence details comes position, providing detail. final two sentences speak broad implications importance.Foreign-aid projects typically local effects, need placed close poor reduce poverty. show , conditional local population levels, World Bank (WB) project aid targets richer parts countries. relationship holds time across world regions. test five donor-side explanations pro-rich targeting using pre-registered conjoint experiment WB Task Team Leaders (TTLs). TTLs perceive aid-receiving governments interested targeting aid politically controlling implementation. also believe aid works better poorer remote areas, implementation areas uniquely difficult. results speak debates distributive politics, international bargaining aid, principal-agent issues international organizations. results also suggest tweaks WB incentive structures make ease project implementation less important may encourage aid flow poorer parts countries.journal Nature provides guide constructing abstract. recommend structure results abstract six parts, add around 200 words.basic introductory sentence comprehensible wide audience.detailed sentence background relevant likely readers.sentence states general problem.Sentences summarize explain main results.sentence general context.finally, sentence broader perspective.critical first sentence abstract vacuous. Assuming reader continued past title, first sentence next opportunity implore keep reading paper. second sentence abstract, . Work re-work abstract good fine thing read.","code":""},{"path":"on-writing.html","id":"introduction-2","chapter":"5 On writing","heading":"5.3.3 Introduction","text":"introduction needs self-contained convey everything reader needs know. important recognize writing mystery story. Instead, want give-away important points introduction. six-page paper, introduction may two three paragraphs main content. Hayot (2014, 90) describes goal introduction engage reader, locate discipline background, tell happens rest paper. completely reader-focused.introduction set scene give reader background. instance, typically start little broader. provides context paper. describe paper fits context, give high-level results, especially focused one key result main part story. provide detail provided abstract, full extent. final bit main content broadly discuss next steps. Finally, finish introduction additional short final paragraph highlights structure paper.example (made-details):UK Conservative Party always done well rural electorates. 2016 Brexit vote different significant different support rural urban areas. even standard rural support conservative issues, support ‘Vote Leave’ unusually strong ‘Vote Leave’ heavily supported East Midlands East England, strongest support ‘Remain’ Greater London.paper look performance ‘Vote Leave’ 2016 Brexit referendum correlated rurality. construct model support ‘Vote Leave’ voting area level, explained number farms area, average internet connectivity, median age. find median age area increases, likelihood area supported ‘Vote Leave’ decreases 14 percentage points. Future work look effect Conservative MP allow nuanced understanding effects.remainder paper structured follows: Section 2 discusses data, Section 3 discusses model, Section 4 presents results, finally Section 5 discusses findings weaknesses.introduction needs self-contained tell reader everything need know. reader able read introduction accurate picture major aspects read whole paper. rare include graphs tables introduction. introduction always closes structure paper. instance (just rough guide) introduction 10-page paper, probably 3 4 paragraphs, 10 per cent, depends specifics.","code":""},{"path":"on-writing.html","id":"data","chapter":"5 On writing","heading":"5.3.4 Data","text":"Robert Caro, Lyndon B. Johnson’s biographer, describes importance conveying ‘sense place’ writing biography (Caro 2019, 141). defines ‘physical setting book’s action occurring: see clearly enough, sufficient detail, feels present action occurring.’ provides following example:Rebekah walked front door little house, nothing—roadrunner streaking behind rocks something long wet dangling beak, perhaps, rabbit disappearing around bush fast really saw flash white tail—otherwise nothing. movement except ripple leaves scattered trees, sound except constant whisper wind… Rebekah climbed, almost desperation, hill back house, saw crest hills, endless vista hills, hills visible single house… hills nothing moved, empty hills , , empty sky; hawk circling silently overhead event. Bus , nothing human, one talk .Caro (2019, 146)thoroughly can imagine circumstances Rebekah Baines Johnson (Lyndon B. Johnson’s mother). need provide reader sense place dataset.\n\nwriting papers, need achieve sense place, data, Caro able provide Hill county. explicit possible showing dataset. typically whole section designed show reader, closely possible, actual data underpin story.writing data section, beginning answer critical question claims, , possible know ? (McPhee 2017, 78). preeminent example data section provided Doll Hill (1950), interested effect smoking control treatment groups. begin clearly describing dataset. use tables display relevant cross-tabs. use graphs contrast groups.data section need thoroughly discuss variables dataset using. datasets used, , mentioned choices justified. variables constructed combined, process motivation explained.get sense data, important reader able understand data underpin results look like. means graph actual data used analysis, close possible. also include tables summary statistics. dataset created source, can also help include example original source. instance, dataset created survey responses survey form included, potentially appendix.data section also figures tables. judgment required. important reader opportunity understand details, may better placed appendix. Figure tables critical aspect convincing people story. graph can show data let reader decide . using table, can easily summarize dataset. least, every variable needs shown graph summarized table. Figures tables numbered cross-referenced text, instance, “Figure 1 shows…,” “Table 1 describes….” every graph table extensive accompanying text describes main aspects, adds additional detail.discuss components graphs tables, including titles labels, Chapter 6. discuss captions, text graph table. Captions need informative self-contained. Cleveland (1994, 57) says, ‘interplay graph, caption, text delicate one,’ however reader able read caption understand graph table shows. caption two three lines long necessarily inappropriate. aspects graph table explained. instance, consider Figures 5.1 5.2 Bowley (1901, 151), exceptionally clear, self-contained.\nFigure 5.1: Example well-captioned figure\n\nFigure 5.2: Example well-captioned table\nchoice table graph comes much information conveyed. general, specific information considered, summary statistic, table good option, interested reader making comparisons understanding trends graph good option (Gelman, Pasarica, Dodhia 2002).Finally, relevant literature discuss throughout paper appropriate. instance, literature relevant data discussed section, literature relevant model, results, discussion mentioned appropriate sections. rarely necessary separate literature review section.","code":""},{"path":"on-writing.html","id":"model","chapter":"5 On writing","heading":"5.3.5 Model","text":"often build statistical model use explore data, often specific section . minimum important clearly specify equation/s describe model used, explain components plain language cross-references.model section typically begins model written , explained, justified. Depending expected reader, background may needed. specifying model appropriate mathematical notation cross-referencing , components model typically defined explained. especially important define aspect notation. helps convince reader model well-chosen enhances credibility paper. model’s variables correspond discussed data section, making clear link two sections.discussion features enter model . instance, examples include, use ages rather age-groups, state/province levels effect, gender categorical variable. general, trying convey sense model situation. want reader understand aspects discussed data section assert modelling decisions made.model section close discussion assumptions underpin model, brief discussion alternative models, variants, strengths weaknesses made clear. clear reader’s mind model chosen.point section, usually appropriate specify software used run model, provide evidence thought circumstances model may appropriate. later point typically expanded discussion. evidence model validation checking, model convergence, /diagnostic issues. , balance needed , content may appropriate placed appendices.technical terms used, briefly explained plain language readers might familiar . instance, M. Alexander (2019b) integrates explanation Gini coefficient brings reader along.look concentration baby names, let’s calculate Gini coefficient country, sex year. Gini coefficient measures dispersion inequality among values frequency distribution. can take value 0 1. case income distributions, Gini coefficient 1 mean one person income. case, Gini coefficient 1 mean babies name. contrast, Gini coefficient 0 mean names evenly distributed across babies.","code":""},{"path":"on-writing.html","id":"results","chapter":"5 On writing","heading":"5.3.6 Results","text":"Two excellent examples results sections provided Kharecha Hansen (2013) Kiang et al. (2021). results section, want communicate outcomes model clear way without much way discussion implications. results section likely requires summary statistics, tables, graphs. aspects cross-referenced text associated details seen . section strictly relay results; , interested results , rather mean.section also typically include table/s coefficient estimates based modelling used explore data. Various features estimates discussed, differences models explained. may different subsets data considered separately. , graphs tables need plain language text accompany . rough guide amount text least equal amount space taken tables graphs. instance, full page used display table coefficient estimates, cross-referenced accompanied least full page text table.","code":""},{"path":"on-writing.html","id":"discussion","chapter":"5 On writing","heading":"5.3.7 Discussion","text":"discussion section may final section paper typically four five sub-sections.discussion section typically begin sub-section comprises one- two-paragraph summary done paper. followed two three sub-sections devoted key things learn world paper. instance, typically implications come modelling results. sub-sections main opportunity justify detail implications story told paper. Typically, sub-sections see newly introduced graphs tables, instead focused learn introduced earlier sections. may results discussed relation others found, differences attempted reconciled .Following sub-sections learn world, typically sub-section focused weaknesses done. concern aspects data used, approach, model. final sub-section typically paragraphs specify left learn, future work proceed.general, expect section take least twenty-five per cent total paper. instance, eight-page paper, expect least two pages discussion.","code":""},{"path":"on-writing.html","id":"brevity-typos-and-grammar","chapter":"5 On writing","heading":"5.3.8 Brevity, typos, and grammar","text":"Brevity important. Partly write reader, reader priorities. also writer focuses us consider important points , can best support , arguments weakest. Jean Chrétien, former Canadian Prime Minister, describes ‘[t]o allow get heart issue quickly, asked officials summarize documents two three pages attach rest materials background information. soon discovered problem didn’t really know talking .’ (Chrétien 2007, 105).experience unique Canada. instance, Oliver Letwin, former British Conservative Cabinet member, describes ‘huge amount terrible guff, huge, colossal, humongous length coming departments’ asked ‘one quarter length’ (Hughes Rutter 2016). found departments able accommodate request without losing anything important.experience also new. instance, Churchill asked brevity Second World War, saying ‘discipline setting real points concisely prove aid clearer thinking.’ letter Szilard Einstein FDR catalyst Manhattan Project two pages.experience also unique academia. instance, one foundations Amazon, one world’s largest companies, clear writing. Specifically, instead PowerPoint presentations, Jeff Bezos asked ‘[w]ell structured, narrative text… [] forces better thought better understanding ’s important , things related.’Zinsser (1976) goes describes ‘secret good writing’ ‘strip every sentence cleanest components.’ Every sentence simplified essence. every word contribute removed.Typos grammatical mistakes affect credibility claims. reader trust us use spell-checker, trust us use logistic regression? Microsoft Word Google Docs useful spell-checkers: copy/paste R Markdown, look red green lines, fix R Markdown.worried n-th degree grammatical content. Instead, interested grammar sentence structure occurs conversational language use (S. King 2000, 118). way develop comfort reading lot asking others read work also.Unnecessary words, typos, grammatical issues removed papers fanatical zeal.","code":""},{"path":"on-writing.html","id":"rules","chapter":"5 On writing","heading":"5.3.9 Rules","text":"variety authors established rules writing, including famously, Orwell (1946), reimagined Economist (2013). Fiske Kuriwaki (2021) list rules scientific papers. reimagining, focused telling stories data, :Focus reader needs. Everything else comment.Establish logical structure rely structure tell story.Write first draft quickly possible.Re-write extensively without favor.Aim concise direct. Remove many words possible.Using words precisely. Stock-markets rise fall, improve worsen.Use short sentence possible.Avoid jargon.Write though work front page newspaper. .","code":""},{"path":"on-writing.html","id":"exercises-and-tutorial-4","chapter":"5 On writing","heading":"5.4 Exercises and tutorial","text":"","code":""},{"path":"on-writing.html","id":"exercises-4","chapter":"5 On writing","heading":"5.4.1 Exercises","text":"According Introduction Zinsser (1976), whose picture hangs Zinsser’s office?\nCharlotte Bronte\nE. M. Forster\nE. B. White\nStephen King\nCharlotte BronteE. M. ForsterE. B. WhiteStephen KingAccording Chapter 2 Zinsser (1976), secret good writing?\nCorrect sentence structure grammar.\nuse long words, adverbs, passive voice.\nThorough planning.\nStrip every sentence cleanest components.\nCorrect sentence structure grammar.use long words, adverbs, passive voice.Thorough planning.Strip every sentence cleanest components.According Chapter 2 Zinsser (1976), must writer constantly ask?\ntrying say?\nwriting ?\ncan re-written?\nmatter?\ntrying say?writing ?can re-written?matter?two repeated words, instance Chapter 3, characterize advice Zinsser (1976)?\nRe-write, re-write.\nRemove, remove.\nSimplify, simplify.\nLess, less.\nRe-write, re-write.Remove, remove.Simplify, simplify.Less, less.According Chapter 5 Zinsser (1976), writer never say anything writing wouldn’t say ?\nPrivate\nPublic\nConversation\nSpeeches\nPrivatePublicConversationSpeechesAccording Chapter 6 Zinsser (1976), tools writer ?\nPapers\nWords\nParagraphs\nSentences\nPapersWordsParagraphsSentencesAccording G. King (2006), key task subheadings (pick one)?\nEnable reader randomly falls asleep keeps turning pages know .\nbroad sweeping reader impressed importance paper.\nUse acronyms integrate paper literature.\nEnable reader randomly falls asleep keeps turning pages know .broad sweeping reader impressed importance paper.Use acronyms integrate paper literature.According G. King (2006), maximum length abstract (pick one)?\nTwo hundred words.\nTwo hundred fifty words.\nOne hundred words.\nOne hundred fifty words.\nTwo hundred words.Two hundred fifty words.One hundred words.One hundred fifty words.According G. King (2006), paper, raw computer output (pick one)?\nCommented .\nincluded.\nIncluded.\nCommented .included.Included.According G. King (2006), standard error 0.05 following specificity coefficient silly (select apply)?\n2.7182818\n2.718282\n2.72\n2.7\n2.7183\n2.718\n3\n2.71828\n2.71828182.7182822.722.72.71832.71832.71828When try use ‘delete’ key (pick one)?\nwriting first draft.\nwriting second draft.\nwriting third draft.\n‘delete’ key always used.\nwriting first draft.writing second draft.writing third draft.‘delete’ key always used.long first draft take write five--ten-page paper (pick one)?\nOne hour\nOne day\nOne week\nOne month\nOne hourOne dayOne weekOne monthWhat key aspect re-drafting process (select apply)?\nGoing red pen remove unneeded words.\nPrinting paper reading physical copy.\nCutting pasting enhance flow.\nReading aloud.\nExchanging others.\nGoing red pen remove unneeded words.Printing paper reading physical copy.Cutting pasting enhance flow.Reading aloud.Exchanging others.three features good research question (write paragraph two)?challenges ‘data-first’ (write paragraph two)?challenges ‘question-first’ (write paragraph two)?counterfactual (pick one)?\n-statements happen.\n-statements happens.\nStatements either true false.\nStatements neither true false.\n-statements happen.-statements happens.Statements either true false.Statements neither true false.following best title (pick one)?\n“Problem Set 1”\n“Unemployment”\n“Examining England’s Unemployment (2010-2020)”\n“England’s Unemployment Increased 2010 2020”\n“Problem Set 1”“Unemployment”“Examining England’s Unemployment (2010-2020)”“England’s Unemployment Increased 2010 2020”following best title (pick one)?\n“Problem Set 2”\n“Standard errors”\n“standard errors small samples”\n“Problem Set 2”“Standard errors”“standard errors small samples”word/s can removed following sentence without substantially affecting meaning (select apply)? ‘Like many parents, children born, one first things wife regularly read stories .’\nfirst\nregularly\nstories\nfirstregularlystoriesPlease write new title either Barron et al. (2018) Fourcade Healy (2017).Please write new title first article list articles New Yorker read.Please write new title article list articles New Yorker read.Please write new four-sentence abstract Chambliss (1989)Please write new four-sentence abstract Doll Hill (1950) Student (1908) Kharecha Hansen (2013).Please write abstract first article list ‘miscellaneous’ articles read.Please write abstract article list ‘miscellaneous’ articles read.Using 1000-popular words English language – https://xkcd.com/simplewriter/ – re-write following retains original meaning:using data, try tell convincing story. may exciting predicting elections, banal increasing internet advertising click rates, serious finding cause disease, fun forecasting basketball games. case key elements .","code":""},{"path":"on-writing.html","id":"tutorial-4","chapter":"5 On writing","heading":"5.4.2 Tutorial","text":"Caro (2019, xii) writes least one thousand words almost every day. tutorial write every day week. day pick one papers specified required materials complete following tasks:Transcribe, writing word , entire introduction.(idea comes McPhee (2017, 186).) Re-write introduction five lines (10 per cent, whichever less) shorter.Transcribe, writing word , abstract.Re-write new, four-sentence, abstract paper.(idea comes comes Chelsea Parlett-Pelleriti.) Write second version new abstract using one thousand popular words English language: https://xkcd.com/simplewriter/.Detail three points way paper written likeDetail one point way paper written like.Please use R Markdown produce single PDF whole week. Make judicious use headings sub-headings structure submission. Submit PDF.","code":""},{"path":"static-communication.html","id":"static-communication","chapter":"6 Static communication","heading":"6 Static communication","text":"Required materialRead R Data Science, Chapter 28 ‘Graphics communication,’ (Wickham Grolemund 2017).Read Data Visualization: Practical Introduction, Chapters 3 ‘Make plot,’ 4 ‘Show right numbers,’ 5 ‘Graph tables, add labels, make notes,’ (Healy 2018).Read Testing Statistical Charts: Makes Good Graph?, (Vanderplas, Cook, Hofmann 2020).Read Data Feminism, Chapter 3 ‘Rational, Scientific, Objective Viewpoints Mythical, Imaginary, Impossible Standpoints,’ (D’Ignazio Klein 2020).Key concepts skillsKnowing importance showing reader actual dataset, close possible.Using variety different graph options, including bar charts, scatterplots, line plots, histograms.Knowing use tables show part dataset, communicate summary statistics, display regression results.Approaching maps type graph.Comfort geocoding places.Key librariesdatasauRus (Locke D’Agostino McGowan 2018)ggmap (Kahle Wickham 2013)kableExtra (Zhu 2020)knitr (Xie 2021)mapsmodelsummary (Arel-Bundock 2021a)opendatatoronto (Gelfand 2020)patchwork (Pedersen 2020)tidyverse (Wickham et al. 2019a)viridis (Garnier et al. 2021)WDI (Arel-Bundock 2021b)Key functionsggmap::get_googlemap()ggmap::get_stamenmap()ggmap::ggmap()ggplot2::coord_map()ggplot2::facet_wrap()ggplot2::geom_abline()ggplot2::geom_bar()ggplot2::geom_boxplot()ggplot2::geom_dotplot()ggplot2::geom_freqpoly()ggplot2::geom_histogram()ggplot2::geom_jitter()ggplot2::geom_line()ggplot2::geom_path()ggplot2::geom_point()ggplot2::geom_polygon()ggplot2::geom_smooth()ggplot2::geom_step()ggplot2::ggplot()ggplot2::ggsave()ggplot2::labeller()ggplot2::labs()ggplot2::map_data()ggplot2::scale_color_brewer()ggplot2::scale_colour_viridis_d()ggplot2::scale_fill_brewer()ggplot2::scale_fill_viridis()ggplot2::stat_qq()ggplot2::stat_qq_line()ggplot2::theme()ggplot2::theme_bw()ggplot2::theme_classic()ggplot2::theme_linedraw()ggplot2::theme_minimal()kableExtra::add_header_above()knitr::kable()lm()maps::map()modelsummary::datasummary()modelsummary::datasummary_balance()modelsummary::datasummary_correlation()modelsummary::datasummary_skim()modelsummary::modelsummary()WDI::WDI()WDI::WDIsearch()","code":""},{"path":"static-communication.html","id":"introduction-3","chapter":"6 Static communication","heading":"6.1 Introduction","text":"telling stories data, like data much work convincing reader. paper medium, data message. end, want try show reader data allowed us come understanding story. use graphs, tables, maps help achieve .critical task show actual data underpin analysis, close can. instance, dataset consists 2,500 responses survey, point paper expect graph contains 2,500 points. build graphs using ggplot2 (Wickham 2016). go variety different options including bar charts, scatterplots, line plots, histograms.contrast role graphs, show actual data, close possible, role tables typically show extract dataset convey various summary statistics. build tables using knitr (Xie 2021) kableExtra (Zhu 2020) initially, modelsummary (Arel-Bundock 2021a).Finally, cover maps variant graphs used show particular type data. build static maps using ggmap (Kahle Wickham 2013), obtained geocoded data need using tidygeocoder (Cambon Belanger 2021).","code":""},{"path":"static-communication.html","id":"graphs","chapter":"6 Static communication","heading":"6.2 Graphs","text":"Graphs critical aspect compelling stories told data.Graphs allow us explore data see overall patterns see detailed behavior; approach can compete revealing structure data thoroughly. Graphs allow us view complex mathematical models fitted data, allow us assess validity models.Cleveland (1994, 5)way, graphing data information coding process create glyph, purposeful mark, mean convey information audience. audience must decode glyph. success graph turns much information lost process. decoding critical aspect (Cleveland 1994, 221), means creating graphs audience. nothing else possible, important feature convey much actual data possible.see important begin using dataset ‘datasaurus_dozen’ datasauRus (Locke D’Agostino McGowan 2018). installing loading necessary packages, can take quick look dataset.can see dataset consists values ‘x’ ‘y,’ plotted x-axis y-axis, respectively. can see thirteen different values variable ‘dataset’ including: “dino,” “star,” “away,” “bullseye.” focus four generate summary statistics (Table 6.1).Table 6.1: Mean standard deviation four ‘datasaurus’ datasetsDespite similarities summary statistics, turns different ‘datasets’ actually different beasts graph actual data (Figure 6.1).\nFigure 6.1: Graph four ‘datasaurus’ datasets\nvariant famous ‘Anscombe’s Quartet.’ key takeaway important plot actual data rely summary statistics. ‘anscombe’ dataset built R.consists six observations four different datasets, x y values observation. need manipulate dataset pivot_longer() get ‘tidy format.’can first create summary statistics (Table 6.2) graph data (Figure 6.2). see importance graphing actual data, rather relying summary statistics.Table 6.2: Mean standard deviation Anscombe\nFigure 6.2: Recreation Anscombe’s Quartet\n","code":"\ninstall.packages('datasauRus')\nlibrary(tidyverse)\nlibrary(datasauRus)\n\nhead(datasaurus_dozen)\n#> # A tibble: 6 × 3\n#>   dataset     x     y\n#>   <chr>   <dbl> <dbl>\n#> 1 dino     55.4  97.2\n#> 2 dino     51.5  96.0\n#> 3 dino     46.2  94.5\n#> 4 dino     42.8  91.4\n#> 5 dino     40.8  88.3\n#> 6 dino     38.7  84.9\ndatasaurus_dozen |> \n  count(dataset)\n#> # A tibble: 13 × 2\n#>    dataset        n\n#>    <chr>      <int>\n#>  1 away         142\n#>  2 bullseye     142\n#>  3 circle       142\n#>  4 dino         142\n#>  5 dots         142\n#>  6 h_lines      142\n#>  7 high_lines   142\n#>  8 slant_down   142\n#>  9 slant_up     142\n#> 10 star         142\n#> 11 v_lines      142\n#> 12 wide_lines   142\n#> 13 x_shape      142\n# From Julia Silge: \n# https://juliasilge.com/blog/datasaurus-multiclass/\ndatasaurus_dozen |>\n  filter(dataset %in% c(\"dino\", \"star\", \"away\", \"bullseye\")) |>\n  group_by(dataset) |>\n  summarise(across(c(x, y),\n                   list(mean = mean,\n                        sd = sd)),\n            x_y_cor = cor(x, y)) |>\n  knitr::kable(\n    caption = \n      \"Mean and standard deviation for four 'datasaurus' datasets\",\n    col.names = c(\"Dataset\", \n                  \"x mean\", \n                  \"x sd\", \n                  \"y mean\", \n                  \"y sd\", \n                  \"correlation\"),\n    digits = 1,\n    booktabs = TRUE,\n    linesep = \"\"\n  )\ndatasaurus_dozen |> \n  filter(dataset %in% c(\"dino\", \"star\", \"away\", \"bullseye\")) |>\n  ggplot(aes(x=x, y=y, colour=dataset)) +\n  geom_point() +\n  theme_minimal() +\n  facet_wrap(vars(dataset), nrow = 2, ncol = 2) +\n  labs(colour = \"Dataset\")\nhead(anscombe)\n#>   x1 x2 x3 x4   y1   y2    y3   y4\n#> 1 10 10 10  8 8.04 9.14  7.46 6.58\n#> 2  8  8  8  8 6.95 8.14  6.77 5.76\n#> 3 13 13 13  8 7.58 8.74 12.74 7.71\n#> 4  9  9  9  8 8.81 8.77  7.11 8.84\n#> 5 11 11 11  8 8.33 9.26  7.81 8.47\n#> 6 14 14 14  8 9.96 8.10  8.84 7.04\n# From Nick Tierney: \n# https://www.njtierney.com/post/2020/06/01/tidy-anscombe/\n# Code from pivot_longer() vignette.\ntidy_anscombe <- \n  anscombe |>\n  pivot_longer(everything(),\n               names_to = c(\".value\", \"set\"),\n               names_pattern = \"(.)(.)\"\n               )\ntidy_anscombe |>\n  group_by(set) |>\n  summarise(across(c(x, y),\n                   list(mean = mean, sd = sd)),\n            x_y_cor = cor(x, y)) |>\n  knitr::kable(\n    caption = \"Mean and standard deviation for Anscombe\",\n    col.names = c(\"Dataset\", \n                  \"x mean\", \n                  \"x sd\", \n                  \"y mean\", \n                  \"y sd\", \n                  \"correlation\"),\n    digits = 1,\n    booktabs = TRUE,\n    linesep = \"\"\n  )\ntidy_anscombe |> \n  ggplot(aes(x = x, y = y, colour = set)) +\n  geom_point() +\n  theme_minimal() +\n  facet_wrap(vars(set), nrow = 2, ncol = 2) +\n  labs(colour = \"Dataset\")"},{"path":"static-communication.html","id":"bar-charts","chapter":"6 Static communication","heading":"6.2.1 Bar charts","text":"typically use bar chart categorical variable want focus . saw example Chapter 2 constructed graph number occupied beds. geom primarily use geom_bar(), many variants cater specific situations.use dataset 1997-2001 British Election Panel Study put together Fox Andersen (2006).dataset consists party person supports, along various demographic, economic, political variables. particular, age respondents. begin creating age-groups ages, making bar chart age-groups using geom_bar() (Figure 6.3).\nFigure 6.3: Distribution ages 1997-2001 British Election Panel Study\ndefault, geom_bar() created count number times age-group appears dataset. default ‘stat’ geom_bar() ‘count.’ saves us create statistic . already constructed count (instance, beps |> count(age)), also specify column values y-axis use stat = \"identity\".may also like consider different groupings data, instance, looking age-groups party respondent supports (Figure 6.4).\nFigure 6.4: Distribution ages, vote preference, 1997-2001 British Election Panel Study\ndefault different groups stacked, can placed side--side position = \"dodge\" (Figure 6.5).\nFigure 6.5: Distribution age-groups, vote preference, 1997-2001 British Election Panel Study\npoint, may like address general look graph. various themes built ggplot2. include theme_bw(), theme_classic(), theme_dark(), theme_minimal(). full list available ggplot2 cheatsheet. can use themes adding layer (Figure 6.6). can use patchwork (Pedersen 2020) bring together multiple graphs. assign graph name, use ‘+’ signal next , ‘/’ signal top, brackets precedence.\nFigure 6.6: Distribution age-groups, vote preference, 1997-2001 British Election Panel Study, illustrating different themes\ncan install themes packages, including ggthemes (Arnold 2021), hrbrthemes (Rudis 2020). can also build .default labels use dby ggplot2 name relevant variable, often useful add detail. add title caption point. caption can useful add information source dataset. title can useful graph going considered outside context paper. case graph included paper, need cross-reference graphs paper means included title within labs() unnecessary (Figure 6.7).\nFigure 6.7: Distribution age-groups, vote preference, 1997-2001 British Election Panel Study\nuse facets create ‘many little graphics variations single graphic’ (L. Wilkinson 2005, 219). especially useful want specifically compare across variable, already used color. instance, may interested explain vote, age gender (Figure 6.8).\nFigure 6.8: Distribution age-group gender, vote preference, 1997-2001 British Election Panel Study\nchange facet_wrap() wrap vertically instead horizontally dir = \"v\". Alternatively, specify number rows, say nrow = 2, number columns, say ncol = 2. Additionally, default, facets scales. enable facets different scales scales = \"free\", just x-axis scales = \"free_x\", just y-axis scales = \"free_y\" (Figure 6.9).\nFigure 6.9: Distribution age-group gender, vote preference, 1997-2001 British Election Panel Study\nFinally, can change labels facets using labeller() (Figure 6.10).\nFigure 6.10: Distribution age-group gender, vote preference, 1997-2001 British Election Panel Study\nvariety different ways change colors, many palettes available including RColorBrewer (Neuwirth 2014), specify scale_fill_brewer(), viridis (Garnier et al. 2021), specify scale_fill_viridis() particularly focused color-blind palettes (Figure 6.11).\nFigure 6.11: Distribution age-group vote preference, 1997-2001 British Election Panel Study\nDetails variety palettes available RColorBrewer viridis available help files. Many different palettes available, can also build . said, color something considered great deal care added increase amount information communicated (Cleveland 1994). Colors added graphs unnecessarily—say, must play role. Typically, role distinguish different groups, implies making colors dissimilar. Colors may also appropriate relationship color variable, instance making graph sales , say, mangoes raspberries, help reader colors yellow red, respectively (Franconeri et al. 2021, 121).","code":"\n# Vincent Arel Bundock provides access to this dataset.\nbeps <- \n  read_csv(\n    file = \n    \"https://vincentarelbundock.github.io/Rdatasets/csv/carData/BEPS.csv\"\n    )\nhead(beps)\n#> # A tibble: 6 × 11\n#>    ...1 vote     age economic.cond.n… economic.cond.h… Blair\n#>   <dbl> <chr>  <dbl>            <dbl>            <dbl> <dbl>\n#> 1     1 Liber…    43                3                3     4\n#> 2     2 Labour    36                4                4     4\n#> 3     3 Labour    35                4                4     5\n#> 4     4 Labour    24                4                2     2\n#> 5     5 Labour    41                2                2     1\n#> 6     6 Labour    47                3                4     4\n#> # … with 5 more variables: Hague <dbl>, Kennedy <dbl>,\n#> #   Europe <dbl>, political.knowledge <dbl>, gender <chr>\nbeps <- \n  beps |> \n  mutate(age_group = \n           case_when(age < 35 ~ \"<35\",\n                     age < 50 ~ \"35-49\",\n                     age < 65 ~ \"50-64\",\n                     age < 80 ~ \"65-79\",\n                     age < 100 ~ \"80-99\"\n                     ),\n         age_group = factor(age_group,\n                            levels = c(\"<35\",\n                                       \"35-49\",\n                                       \"50-64\",\n                                       \"65-79\",\n                                       \"80-99\"\n                                       )\n                            )\n         )\n\nbeps |>  \n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar()\nbeps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar()\nbeps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar(position = \"dodge\")\nlibrary(patchwork)\n\ntheme_bw <- \n  beps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar(position = \"dodge\") +\n  theme_bw()\n\ntheme_classic <- \n  beps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar(position = \"dodge\") +\n  theme_classic()\n\ntheme_dark <- \n  beps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar(position = \"dodge\") +\n  theme_dark()\n\ntheme_minimal <- \n  beps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar(position = \"dodge\") +\n  theme_minimal()\n\n(theme_bw + theme_classic) / (theme_dark + theme_minimal)\nbeps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group of respondent\",\n       y = \"Number of respondents\",\n       fill = \"Voted for\",\n       title = \"Distribution of age-groups, and vote preference, in\n       the 1997-2001 British Election Panel Study\",\n       caption = \"Source: 1997-2001 British Election Panel Study.\")\nbeps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group of respondent\",\n       y = \"Number of respondents\",\n       fill = \"Voted for\") +\n  facet_wrap(vars(gender))\nbeps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group of respondent\",\n       y = \"Number of respondents\",\n       fill = \"Voted for\") +\n  facet_wrap(vars(gender),\n             dir = \"v\",\n             scales = \"free\")\nnew_labels <- c(female = \"Female\", male = \"Male\")\n\nbeps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group of respondent\",\n       y = \"Number of respondents\",\n       fill = \"Voted for\") +\n  facet_wrap(vars(gender),\n             dir = \"v\",\n             scales = \"free\",\n             labeller = labeller(gender = new_labels))\nlibrary(viridis)\nlibrary(patchwork)\n\nRColorBrewerBrBG <- \n  beps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group of respondent\",\n       y = \"Number of respondents\",\n       fill = \"Voted for\") + \n  scale_fill_brewer(palette = \"Blues\")\n\nRColorBrewerSet2 <- \n  beps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group of respondent\",\n       y = \"Number of respondents\",\n       fill = \"Voted for\") +\n  scale_fill_brewer(palette = \"Set1\")\n\nviridis <- \n  beps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group of respondent\",\n       y = \"Number of respondents\",\n       fill = \"Voted for\") + \n  scale_fill_viridis(discrete = TRUE)\n\nviridismagma <- \n  beps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group of respondent\",\n       y = \"Number\",\n       fill = \"Voted for\") +\n   scale_fill_viridis(discrete = TRUE, \n                      option = \"magma\")\n\n(RColorBrewerBrBG + RColorBrewerSet2) /\n  (viridis + viridismagma)"},{"path":"static-communication.html","id":"scatterplots","chapter":"6 Static communication","heading":"6.2.2 Scatterplots","text":"often interested relationship two variables. can use scatterplots show information. Unless good reason move different option, scatterplot almost always best choice (Weissgerber et al. 2015). Indeed, ‘among forms statistical graphics, scatterplot may considered versatile generally useful invention entire history statistical graphics.’ (Friendly Wainer 2021, 121) illustrate scatterplots, use WDI (Arel-Bundock 2021b) download economic indicators World Bank, particular WDIsearch() find unique key need pass WDI() facilitate download.Oh, think good data ! Gross Domestic Product (GDP) ‘combines single figure, double counting, output (production) carried firms, non-profit institutions, government bodies households given country given period, regardless type goods services produced, provided production takes place within country’s economic territory’ (OECD (2014), p. 15). modern concept developed Simon Kuznets widely used reported. certain comfort definitive concrete single number describe something complicated entire economic activity country. crucial summary statistics. summary statistic, strength also weakness. single number necessarily loses information constituent components, distributional differences critical. highlights short term economic progress longer term improvements. ‘quantitative definiteness estimates makes easy forget dependence upon imperfect data consequently wide margins possible error totals components liable’ (Kuznets 1941, xxvi). Reliance one summary measure economic performance presents misguided picture country’s economy, also peoples.may like change names meaningful, keep columns need.get started can use geom_point() make scatterplot showing GDP growth inflation, country (Figure 6.12).\nFigure 6.12: Relationship inflation GDP growth Australia, Ethiopia, India, US\nbar charts, change theme, update labels (Figure 6.13), although , normally need caption title just use one.\nFigure 6.13: Relationship inflation GDP growth Australia, Ethiopia, India, US\nuse ‘color’ instead ‘fill’ using dots rather bars. also slightly affects change palette (Figure 6.14).\nFigure 6.14: Relationship inflation GDP growth Australia, Ethiopia, India, US\npoints scatterplot sometimes overlap. can address situation one two ways:Adding degree transparency dots ‘alpha’ (Figure 6.15). value ‘alpha’ can vary 0, fully transparent, 1, completely opaque.Adding small noise, slightly moves points, using geom_jitter() (Figure 6.16). default, movement uniform directions, can specify direction movement occurs ‘width’ ‘height.’ decision two options turns degree exact accuracy matters, number points.\nFigure 6.15: Relationship inflation GDP growth Australia, Ethiopia, India, US\n\nFigure 6.16: Relationship inflation GDP growth Australia, Ethiopia, India, US\ncommon use case scatterplot illustrate relationship two variables. can useful add line best fit using geom_smooth() (Figure 6.17). default, geom_smooth() use LOESS smoothing used datasets less 1,000 observations, can specify relationship using ‘method,’ change color ‘color’ remove standard errors ‘se.’ Using geom_smooth() adds layer graph, inherits aesthetics ggplot(). instance, initially one line country Figure 6.17. overwrite specifying particular color, third graph Figure 6.17.\nFigure 6.17: Relationship inflation GDP growth Australia, Ethiopia, India, US\n","code":"\ninstall.packages('WDI')\nlibrary(tidyverse)\nlibrary(WDI)\nWDIsearch(\"gdp growth\")\n#>      indicator             \n#> [1,] \"5.51.01.10.gdp\"      \n#> [2,] \"6.0.GDP_growth\"      \n#> [3,] \"NV.AGR.TOTL.ZG\"      \n#> [4,] \"NY.GDP.MKTP.KD.ZG\"   \n#> [5,] \"NY.GDP.MKTP.KN.87.ZG\"\n#>      name                                    \n#> [1,] \"Per capita GDP growth\"                 \n#> [2,] \"GDP growth (annual %)\"                 \n#> [3,] \"Real agricultural GDP growth rates (%)\"\n#> [4,] \"GDP growth (annual %)\"                 \n#> [5,] \"GDP growth (annual %)\"\nWDIsearch(\"inflation\")\n#>      indicator             \n#> [1,] \"FP.CPI.TOTL.ZG\"      \n#> [2,] \"FP.FPI.TOTL.ZG\"      \n#> [3,] \"FP.WPI.TOTL.ZG\"      \n#> [4,] \"NY.GDP.DEFL.87.ZG\"   \n#> [5,] \"NY.GDP.DEFL.KD.ZG\"   \n#> [6,] \"NY.GDP.DEFL.KD.ZG.AD\"\n#>      name                                               \n#> [1,] \"Inflation, consumer prices (annual %)\"            \n#> [2,] \"Inflation, food prices (annual %)\"                \n#> [3,] \"Inflation, wholesale prices (annual %)\"           \n#> [4,] \"Inflation, GDP deflator (annual %)\"               \n#> [5,] \"Inflation, GDP deflator (annual %)\"               \n#> [6,] \"Inflation, GDP deflator: linked series (annual %)\"\nWDIsearch(\"population, total\")\n#>           indicator                name \n#>       \"SP.POP.TOTL\" \"Population, total\"\nWDIsearch(\"Unemployment, total\")\n#>      indicator          \n#> [1,] \"SL.UEM.TOTL.NE.ZS\"\n#> [2,] \"SL.UEM.TOTL.ZS\"   \n#>      name                                                                 \n#> [1,] \"Unemployment, total (% of total labor force) (national estimate)\"   \n#> [2,] \"Unemployment, total (% of total labor force) (modeled ILO estimate)\"\nworld_bank_data <- \n  WDI(indicator = c(\"FP.CPI.TOTL.ZG\",\n                    \"NY.GDP.MKTP.KD.ZG\",\n                    \"SP.POP.TOTL\",\n                    \"SL.UEM.TOTL.NE.ZS\"\n                    ),\n      country = c(\"AU\", \"ET\", \"IN\", \"US\")\n      )\nworld_bank_data <- \n  world_bank_data |> \n  rename(inflation = FP.CPI.TOTL.ZG,\n         gdp_growth = NY.GDP.MKTP.KD.ZG,\n         population = SP.POP.TOTL,\n         unemployment_rate = SL.UEM.TOTL.NE.ZS\n         ) |> \n  select(-iso2c)\n\nhead(world_bank_data)\n#> # A tibble: 6 × 6\n#>   country    year inflation gdp_growth population\n#>   <chr>     <dbl>     <dbl>      <dbl>      <dbl>\n#> 1 Australia  1960     3.73       NA      10276477\n#> 2 Australia  1961     2.29        2.48   10483000\n#> 3 Australia  1962    -0.319       1.29   10742000\n#> 4 Australia  1963     0.641       6.21   10950000\n#> 5 Australia  1964     2.87        6.98   11167000\n#> 6 Australia  1965     3.41        5.98   11388000\n#> # … with 1 more variable: unemployment_rate <dbl>\nworld_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point()\nworld_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Inflation\",\n       y = \"GDP growth\",\n       color = \"Country\",\n       title = \"Relationship between inflation and GDP growth\",\n       caption = \"Data source: World Bank.\")\nlibrary(patchwork)\n\nRColorBrewerBrBG <-\n  world_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Inflation\",\n       y = \"GDP growth\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\") +\n  scale_color_brewer(palette = \"Blues\")\n\nRColorBrewerSet2 <- \n  world_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Inflation\",\n       y = \"GDP growth\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\") +\n  scale_color_brewer(palette = \"Set1\")\n\nviridis <- \n  world_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Inflation\",\n       y = \"GDP growth\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\") +\n  scale_colour_viridis_d()\n\nviridismagma <- \n  world_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Inflation\",\n       y = \"GDP growth\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\") +\n  scale_colour_viridis_d(option = \"magma\")\n\nRColorBrewerBrBG / \n  RColorBrewerSet2 /\n  viridis /\n  viridismagma\nworld_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point(alpha = 0.5) +\n  theme_minimal() +\n  labs(x = \"Inflation\",\n       y = \"GDP growth\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\")\nworld_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  theme_minimal() +\n  labs(x = \"Inflation\",\n       y = \"GDP growth\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\")\ndefaults <- \n  world_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  geom_smooth() +\n  theme_minimal() +\n  labs(x = \"Inflation\",\n       y = \"GDP growth\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\")\n\nstraightline <- \n  world_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  geom_smooth(method = lm, se = FALSE) +\n  theme_minimal() +\n  labs(x = \"Inflation\",\n       y = \"GDP growth\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\")\n\nonestraightline <- \n  world_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  geom_smooth(method = lm, color = \"black\", se = FALSE) +\n  theme_minimal() +\n  labs(x = \"Inflation\",\n       y = \"GDP growth\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\")\n\ndefaults / \n  straightline /\n  onestraightline"},{"path":"static-communication.html","id":"line-plots","chapter":"6 Static communication","heading":"6.2.3 Line plots","text":"can use line plot variables joined together, instance, economic time series. continue dataset World Bank focus US GDP growth using geom_line() (Figure 6.18).\nFigure 6.18: US GDP growth (1961-2020)\n, can adjust theme, say theme_minimal() labels labs() (Figure 6.19).\nFigure 6.19: US GDP growth (1961-2020)\ncan use slight variant geom_line(), geom_step() focus attention change year year (Figure 6.20).\nFigure 6.20: US GDP growth (1961-2020)\nPhillips curve name given plot relationship unemployment inflation time. inverse relationship sometimes found data, instance UK 1861 1957 (Phillips 1958). variety ways investigate including:Adding second line graph. instance, add inflation (Figure 6.21). may require use use pivot_longer() ensure data tidy format.Using geom_path() links values order appear dataset. Figure 6.22 show Phillips curve US 1960 2020. Figure 6.22 appear show clear relationship unemployment inflation.\nFigure 6.21: Unemployment inflation US (1960-2020)\n\nFigure 6.22: Phillips curve US (1960-2020)\n","code":"\nworld_bank_data |>\n  filter(country == \"United States\") |>\n  ggplot(mapping = aes(x = year, y = gdp_growth)) +\n  geom_line()\nworld_bank_data |>\n  filter(country == \"United States\") |>\n  ggplot(mapping = aes(x = year, y = gdp_growth)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Year\",\n       y = \"GDP growth\",\n       caption = \"Data source: World Bank.\")\nworld_bank_data |>\n  filter(country == \"United States\") |>\n  ggplot(mapping = aes(x = year, y = gdp_growth)) +\n  geom_step() +\n  theme_minimal() +\n  labs(x = \"Year\",\n       y = \"GDP growth\",\n       caption = \"Data source: World Bank.\")\nworld_bank_data |>\n  filter(country == \"United States\") |>\n  select(-population, -gdp_growth) |>\n  pivot_longer(cols = c(\"inflation\", \"unemployment_rate\"),\n               names_to = \"series\",\n               values_to = \"value\"\n               ) |>\n  ggplot(mapping = aes(x = year, y = value, color = series)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Year\",\n       y = \"Value\",\n       color = \"Economic indicator\",\n       caption = \"Data source: World Bank.\") +\n  scale_color_brewer(palette = \"Set1\", labels = c(\"Inflation\", \"Unemployment\")) +\n  theme(legend.position = \"bottom\")\nworld_bank_data |>\n  filter(country == \"United States\") |>\n  ggplot(mapping = aes(x = unemployment_rate, y = inflation)) +\n  geom_path() +\n  theme_minimal() +\n  labs(x = \"Unemployment rate\",\n       y = \"Inflation\",\n       caption = \"Data source: World Bank.\")"},{"path":"static-communication.html","id":"histograms","chapter":"6 Static communication","heading":"6.2.4 Histograms","text":"histogram useful show shape continuous variable works constructing counts number observations different subsets support, called ‘bins.’ Figure 6.23 examine distribution GDP Ethiopia.\nFigure 6.23: Distribution GDP Ethiopia (1960-2020)\ncan add theme labels (Figure 6.24.\nFigure 6.24: Distribution GDP Ethiopia (1960-2020)\nkey component determining shape histogram number bins. can specified one two ways (Figure 6.25):specifying number ‘bins’ include, orspecifying wide ‘binwidth.’\nFigure 6.25: Distribution GDP Ethiopia (1960-2020)\nhistogram smoothing data, number bins affects much smoothing occurs. two bins data smooth, lost great deal accuracy. specifically, ‘histogram estimator piecewise constant function height function proportional number observations bin’ (Wasserman 2005, 303). bins result biased estimator, many bins results estimator high variance. decision number bins, width, concerned trying balance bias variance. depend variety concerns including subject matter goal (Cleveland 1994, 135).Finally, can use ‘fill’ distinguish different types observations, can get quite messy. usually better give away showing distribution columns instead trace outline distribution, using geom_freqpoly() (Figure 6.26) build using dots geom_dotplot() (Figure 6.27) .\nFigure 6.26: Distribution GDP four countries (1960-2020)\n\nFigure 6.27: Distribution GDP four countries (1960-2020)\n","code":"\nworld_bank_data |> \n  filter(country == \"Ethiopia\") |> \n  ggplot(mapping = aes(x = gdp_growth)) +\n  geom_histogram()\nworld_bank_data |> \n  filter(country == \"Ethiopia\") |> \n  ggplot(mapping = aes(x = gdp_growth)) +\n  geom_histogram() +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       caption = \"Data source: World Bank.\")\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n\ntwobins <- \n  world_bank_data |> \n  filter(country == \"Ethiopia\") |> \n  ggplot(mapping = aes(x = gdp_growth)) +\n  geom_histogram(bins = 2) +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       caption = \"Data source: World Bank.\")\n\nfivebins <- \n  world_bank_data |> \n  filter(country == \"Ethiopia\") |> \n  ggplot(mapping = aes(x = gdp_growth)) +\n  geom_histogram(bins = 5) +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       caption = \"Data source: World Bank.\")\n\ntwentybins <- \n  world_bank_data |> \n  filter(country == \"Ethiopia\") |> \n  ggplot(mapping = aes(x = gdp_growth)) +\n  geom_histogram(bins = 20) +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       caption = \"Data source: World Bank.\")\n\nhalfbinwidth <- \n  world_bank_data |> \n  filter(country == \"Ethiopia\") |> \n  ggplot(mapping = aes(x = gdp_growth)) +\n  geom_histogram(binwidth = 0.5) +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       caption = \"Data source: World Bank.\")\n\ntwobinwidth <- \n  world_bank_data |> \n  filter(country == \"Ethiopia\") |> \n  ggplot(mapping = aes(x = gdp_growth)) +\n  geom_histogram(binwidth = 2) +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       caption = \"Data source: World Bank.\")\n\nfivebinwidth <- \n  world_bank_data |> \n  filter(country == \"Ethiopia\") |> \n  ggplot(mapping = aes(x = gdp_growth)) +\n  geom_histogram(binwidth = 5) +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       caption = \"Data source: World Bank.\")\n\n(twobins + fivebins + twentybins) / \n  (halfbinwidth + twobinwidth + fivebinwidth)\nworld_bank_data |> \n  ggplot(mapping = aes(x = gdp_growth, color = country)) +\n  geom_freqpoly() +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\") +\n  scale_color_brewer(palette = \"Set1\") \nworld_bank_data |> \n  ggplot(mapping = aes(x = gdp_growth, group = country, fill = country)) +\n  geom_dotplot(method = 'histodot', alpha = 0.4) +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       fill = \"Country\",\n       caption = \"Data source: World Bank.\") +\n  scale_color_brewer(palette = \"Set1\") "},{"path":"static-communication.html","id":"boxplots","chapter":"6 Static communication","heading":"6.2.5 Boxplots","text":"Boxplots almost never appropriate choice hide distribution data, rather show . Unless need compare summary statistics many variables , almost never used. boxplot can apply different distributions. see , consider simulated data beta distribution two types. One type data contains draws two beta distributions: one right skewed another left skewed. type data contains draws beta distribution skew.can first compare boxplots two series (Figure 6.28.\nFigure 6.28: Data drawn beta distributions different parameters\nplot actual data can see different (Figure 6.29).\nFigure 6.29: Data drawn beta distributions different parameters\nOne way forward, boxplot must included, include actual data layer top boxplot. instance, Figure 6.30 show distribution inflation across four countries.\nFigure 6.30: Distribution unemployment data four countries (1960-2020)\n","code":"\nset.seed(853)\n\nboth_left_and_right_skew <- \n  c(\n    rbeta(500, 5, 2),\n    rbeta(500, 2, 5)\n    )\n\nno_skew <- \n  rbeta(1000, 1, 1)\n\nbeta_distributions <- \n  tibble(\n    observation = c(both_left_and_right_skew, no_skew),\n    source = c(rep(\"Left and right skew\", 1000),\n               rep(\"No skew\", 1000)\n               )\n  )\nbeta_distributions |> \n  ggplot(aes(x = source, y = observation)) +\n  geom_boxplot() +\n  theme_classic()\nbeta_distributions |> \n  ggplot(aes(x = observation, color = source)) +\n  geom_freqpoly(binwidth = 0.05) +\n  theme_classic()\nworld_bank_data |> \n  ggplot(mapping = aes(x = country, y = inflation)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.3, width = 0.15, height = 0) +\n  theme_minimal() +\n  labs(x = \"Country\",\n       y = \"Inflation\",\n       caption = \"Data source: World Bank.\") +\n  scale_color_brewer(palette = \"Set1\") "},{"path":"static-communication.html","id":"tables","chapter":"6 Static communication","heading":"6.3 Tables","text":"Tables critical telling compelling story. Tables can communicate less information graph, can high fidelity. primarily use tables three ways:show actual dataset, use kable() knitr (Xie 2021), alongside kableExtra (Zhu 2020).communicate summary statistics, use modelsummary (Arel-Bundock 2021a).display regression results, also use modelsummary (Arel-Bundock 2021a).","code":""},{"path":"static-communication.html","id":"showing-part-of-a-dataset","chapter":"6 Static communication","heading":"6.3.1 Showing part of a dataset","text":"illustrate showing part dataset using kable() knitr drawing kableExtra enhancement. use World Bank dataset downloaded earlier.begin, can display first ten rows default kable() settings.order able cross-reference text, need add caption ‘caption.’ can also make column names information ‘col.names’ specify number digits displayed (Table 6.3).Table 6.3: First ten rows dataset economic indicators \nAustralia, Ethiopia, India, USWhen producing PDFs, ‘booktabs’ option makes host small changes default display results tables look better (Table 6.4). using ‘booktabs’ additionally specify ‘linesep’ otherwise kable() adds small space every five lines. (None show html output.)Table 6.4: First ten rows dataset economic indicators \nAustralia, Ethiopia, India, USTable 6.5: First ten rows dataset economic indicators \nAustralia, Ethiopia, India, USWe can specify alignment columns using character vector ‘l’ (left), ‘c’ (centre), ‘r’ (right) (Table 6.6). Additionally, can change formatting. instance, specify groupings numbers least one thousand using ‘format.args = list(big.mark = “,”).’Table 6.6: First ten rows dataset economic indicators \nAustralia, Ethiopia, India, USWe can use kableExtra (Zhu 2020) add extra functionality kable. instance, add row groups columns (Table 6.6).\nTable 6.7: First ten rows dataset economic indicators \nAustralia, Ethiopia, India, US\nAnother especially nice way build tables use gt (Iannone, Cheng, Schloerke 2020)., can add caption informative column labels (Table 6.8).Table 6.8: First ten rows dataset economic indicators \n    Australia, Ethiopia, India, US","code":"\nlibrary(knitr)\nhead(world_bank_data)\n#> # A tibble: 6 × 6\n#>   country    year inflation gdp_growth population\n#>   <chr>     <dbl>     <dbl>      <dbl>      <dbl>\n#> 1 Australia  1960     3.73       NA      10276477\n#> 2 Australia  1961     2.29        2.48   10483000\n#> 3 Australia  1962    -0.319       1.29   10742000\n#> 4 Australia  1963     0.641       6.21   10950000\n#> 5 Australia  1964     2.87        6.98   11167000\n#> 6 Australia  1965     3.41        5.98   11388000\n#> # … with 1 more variable: unemployment_rate <dbl>\nworld_bank_data |> \n  slice(1:10) |> \n  kable() \nworld_bank_data |> \n  slice(1:10) |> \n  kable(\n    caption = \"First ten rows of a dataset of economic indicators for \n    Australia, Ethiopia, India, and the US\",\n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \"Population\", \"Unemployment rate\"),\n    digits = 1\n  )\nworld_bank_data |> \n  slice(1:10) |> \n  kable(\n    caption = \"First ten rows of a dataset of economic indicators for \n    Australia, Ethiopia, India, and the US\",\n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \"Population\", \"Unemployment rate\"),\n    digits = 1,\n    booktabs = TRUE, \n    linesep = \"\"\n  )\nworld_bank_data |> \n  slice(1:10) |> \n  kable(\n    caption = \"First ten rows of a dataset of economic indicators for\n    Australia, Ethiopia, India, and the US\",\n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \"Population\", \"Unemployment rate\"),\n    digits = 1,\n    booktabs = TRUE\n  )\nworld_bank_data |> \n  slice(1:10) |> \n  mutate(year = as.factor(year)) |>\n  kable(\n    caption = \"First ten rows of a dataset of economic indicators for \n    Australia, Ethiopia, India, and the US\",\n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \n                  \"Population\", \"Unemployment rate\"),\n    digits = 1,\n    booktabs = TRUE, \n    linesep = \"\",\n    align = c('l', 'l', 'c', 'c', 'r', 'r'),\n    format.args = list(big.mark = \",\")\n  )\nlibrary(kableExtra)\n\nworld_bank_data |> \n  slice(1:10) |> \n  kable(\n    caption = \"First ten rows of a dataset of economic indicators for \n    Australia, Ethiopia, India, and the US\",\n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \n                  \"Population\", \"Unemployment rate\"),\n    digits = 1,\n    booktabs = TRUE, \n    linesep = \"\",\n    align = c('l', 'l', 'c', 'c', 'r', 'r'),\n  ) |> \n  add_header_above(c(\" \" = 2, \"Economic indicators\" = 4))\nlibrary(gt)\n\nworld_bank_data |> \n  slice(1:10) |> \n  gt() \nworld_bank_data |>\n  slice(1:10) |>\n  gt(\n    caption = \"First ten rows of a dataset of economic indicators for\n    Australia, Ethiopia, India, and the US\") |>\n  cols_label(\n      country = \"Country\",\n      year = \"Year\",\n      inflation = \"Inflation\",\n      gdp_growth = \"GDP growth\",\n      population = \"Population\",\n      unemployment_rate = \"Unemployment rate\"\n    )"},{"path":"static-communication.html","id":"communicating-summary-statistics","chapter":"6 Static communication","heading":"6.3.2 Communicating summary statistics","text":"can use datasummary() modelsummary create tables summary statistics dataset.default, datasummary() summarizes ‘numeric’ variables, can ask ‘categorical’ variables (Table 6.9). Additionally can add cross-references way kable(), , include title cross-reference name R chunk.\nTable 6.9: Summary categorical economic indicator\nvariables four countries\ncan create table shows correlation variables using datasummary_correlation() (Table 6.10).\nTable 6.10: Correlation economic indicator variables \nfour countries (Australia, Ethiopia, India, US)\ntypically need table descriptive statistics add paper (Table 6.11). contrasts Table 6.9 likely included paper. can add note source data using ‘notes.’\nTable 6.11: Descriptive statistics inflation GDP dataset\n","code":"\nlibrary(modelsummary)\n#> \n#> Attaching package: 'modelsummary'\n#> The following object is masked from 'package:gt':\n#> \n#>     escape_latex\n\nworld_bank_data |> \n  datasummary_skim()\nworld_bank_data |> \n  datasummary_skim(type = \"categorical\",\n                   title = \"Summary of categorical economic indicator \n                   variables for four countries\")\nworld_bank_data |> \n  datasummary_correlation(\n    title = \"Correlation between the economic indicator variables for \n    four countries (Australia, Ethiopia, India, and the US)\"\n    )\ndatasummary_balance(formula = ~country,\n                    data = world_bank_data,\n                    title = \"Descriptive statistics for the inflation and GDP dataset\",\n                    notes = \"Data source: World Bank.\")"},{"path":"static-communication.html","id":"display-regression-results","chapter":"6 Static communication","heading":"6.3.3 Display regression results","text":"Finally, one common reason needing table report regression results. using modelsummary() modelsummary (Arel-Bundock 2021a).can put variety different different models together (Table 6.12).\nTable 6.12: Explaining GDP function inflation\ncan adjust number significant digits (Table 6.13).\nTable 6.13: Two models GDP function inflation\n","code":"\nfirst_model <- lm(formula = gdp_growth ~ inflation, \n                  data = world_bank_data)\n\nmodelsummary(first_model)\nsecond_model <- lm(formula = gdp_growth ~ inflation + country, \n                  data = world_bank_data)\n\nthird_model <- lm(formula = gdp_growth ~ inflation + country + population, \n                  data = world_bank_data)\n\nmodelsummary(list(first_model, second_model, third_model),\n             title = \"Explaining GDP as a function of inflation\")\nmodelsummary(list(first_model, second_model, third_model),\n             fmt = 1,\n             title = \"Two models of GDP as a function of inflation\")"},{"path":"static-communication.html","id":"maps","chapter":"6 Static communication","heading":"6.4 Maps","text":"many ways maps can thought another type graph, x-axis latitude, y-axis longitude, outline background image. seen type set-used type set-, instance, ggplot2 setting, quite familiar.small complications, part straight-forward . first step get data. geographic data built ggplot2, additional information ‘world.cities’ dataset maps.information hand, can create map France shows larger cities. use geom_polygon() ggplot2 draw shapes connecting points within groups. coord_map() adjusts fact making 2D map represent world 3D.)often case R, many different ways get started creating static maps. seen can built using ggplot2, ggmap brings additional functionality (Kahle Wickham 2013).two essential components map:border background image (sometimes called tile); andsomething interest within border, top tile.ggmap, use open-source option tile, Stamen Maps. use plot points based latitude longitude.","code":"\nggplot() +\n  geom_polygon( # First draw an outline\n    data = some_data, \n    aes(x = latitude, \n        y = longitude,\n        group = group\n        )) +\n  geom_point( # Then add points of interest\n    data = some_other_data, \n    aes(x = latitude, \n        y = longitude)\n    )\nlibrary(maps)\n\nfrance <- map_data(map = \"france\")\n\nhead(france)\n#>       long      lat group order region subregion\n#> 1 2.557093 51.09752     1     1   Nord      <NA>\n#> 2 2.579995 51.00298     1     2   Nord      <NA>\n#> 3 2.609101 50.98545     1     3   Nord      <NA>\n#> 4 2.630782 50.95073     1     4   Nord      <NA>\n#> 5 2.625894 50.94116     1     5   Nord      <NA>\n#> 6 2.597699 50.91967     1     6   Nord      <NA>\n\nfrench_cities <- \n  world.cities |>\n  filter(country.etc == \"France\")\n\nhead(french_cities)\n#>              name country.etc    pop   lat long capital\n#> 1       Abbeville      France  26656 50.12 1.83       0\n#> 2         Acheres      France  23219 48.97 2.06       0\n#> 3            Agde      France  23477 43.33 3.46       0\n#> 4            Agen      France  34742 44.20 0.62       0\n#> 5 Aire-sur-la-Lys      France  10470 50.64 2.39       0\n#> 6 Aix-en-Provence      France 148622 43.53 5.44       0\nggplot() +\n  geom_polygon(data = france,\n               aes(x = long,\n                   y = lat,\n                   group = group),\n               fill = \"white\", \n               colour = \"grey\") +\n  coord_map() +\n  geom_point(aes(x = french_cities$long, \n                 y = french_cities$lat),\n             alpha = 0.3,\n             color = \"black\") +\n  theme_classic() +\n  labs(x = \"Longitude\",\n       y = \"Latitude\")"},{"path":"static-communication.html","id":"australian-polling-places","chapter":"6 Static communication","heading":"6.4.1 Australian polling places","text":"Australia people go specific locations, called booths, vote. booths latitudes longitudes can plot . One reason may like notice patterns geographies.get started need get tile. going use ggmap get tile Stamen Maps, builds OpenStreetMap (openstreetmap.org). main argument function specify bounding box. requires two latitudes - one top box one bottom box - two longitudes - one left box one right box. can useful use Google Maps, alternative, find values need. bounding box provides coordinates edges interested . case provided coordinates centered around Canberra, Australia, small city created purposes capital.defined bounding box, function get_stamenmap() get tiles area. number tiles needs get depends zoom, type tiles gets depends maptype. used black--white type map helpfile specifies others. point can map maps ggmap() plot tile! actively downloading tiles, needs internet connection.map can use ggmap() plot . Now want get data plot top tiles. just plot location polling places, based ‘division’ . available . Australian Electoral Commission (AEC) official government agency responsible elections Australia.dataset whole Australia, just going plot area around Canberra filter booths geographic (AEC various options people hospital, able get booth, etc, still ‘booths’ dataset).Now can use ggmap way plot underlying tiles, build using geom_point() add points interest.may like save map draw every time, can way graph, using ggsave().Finally, reason used Stamen Maps OpenStreetMap open source, also used Google Maps. requires first register credit card Google, specify key, low usage free. Using Google Maps, get_googlemap(), brings advantages get_stamenmap(), instance attempt find placename, rather needing specify bounding box.","code":"\nlibrary(ggmap)\n\nbbox <- c(left = 148.95, bottom = -35.5, right = 149.3, top = -35.1)\ncanberra_stamen_map <- get_stamenmap(bbox, zoom = 11, maptype = \"toner-lite\")\n\nggmap(canberra_stamen_map)\n# Read in the booths data for each year\nbooths <-\n  readr::read_csv(\n    \"https://results.aec.gov.au/24310/Website/Downloads/GeneralPollingPlacesDownload-24310.csv\",\n    skip = 1,\n    guess_max = 10000\n  )\n\nhead(booths)\n#> # A tibble: 6 × 15\n#>   State DivisionID DivisionNm PollingPlaceID\n#>   <chr>      <dbl> <chr>               <dbl>\n#> 1 ACT          318 Bean                93925\n#> 2 ACT          318 Bean                93927\n#> 3 ACT          318 Bean                11877\n#> 4 ACT          318 Bean                11452\n#> 5 ACT          318 Bean                 8761\n#> 6 ACT          318 Bean                 8763\n#> # … with 11 more variables: PollingPlaceTypeID <dbl>,\n#> #   PollingPlaceNm <chr>, PremisesNm <chr>,\n#> #   PremisesAddress1 <chr>, PremisesAddress2 <chr>,\n#> #   PremisesAddress3 <chr>, PremisesSuburb <chr>,\n#> #   PremisesStateAb <chr>, PremisesPostCode <chr>,\n#> #   Latitude <dbl>, Longitude <dbl>\n# Reduce the booths data to only rows with that have latitude and longitude\nbooths_reduced <-\n  booths |>\n  filter(State == \"ACT\") |> \n  select(PollingPlaceID, DivisionNm, Latitude, Longitude) |> \n  filter(!is.na(Longitude)) |> # Remove rows that do not have a geography\n  filter(Longitude < 165) # Remove Norfolk Island\nggmap(canberra_stamen_map,\n      extent = \"normal\",\n      maprange = FALSE) +\n  geom_point(data = booths_reduced,\n             aes(x = Longitude,\n                 y = Latitude,\n                 colour = DivisionNm),) +\n  scale_color_brewer(name = \"2019 Division\", palette = \"Set1\") +\n  coord_map(\n    projection = \"mercator\",\n    xlim = c(attr(map, \"bb\")$ll.lon, attr(map, \"bb\")$ur.lon),\n    ylim = c(attr(map, \"bb\")$ll.lat, attr(map, \"bb\")$ur.lat)\n  ) +\n  labs(x = \"Longitude\",\n       y = \"Latitude\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank())\nggsave(\"map.pdf\", width = 20, height = 10, units = \"cm\")"},{"path":"static-communication.html","id":"us-troop-deployment","chapter":"6 Static communication","heading":"6.4.2 US troop deployment","text":"Let us see another example static map, time using data US military deployments troopdata (Flynn 2021). can access data US overseas military bases back start Cold War using get_basedata().look locations US military bases : Germany, Japan, Australia. troopdata dataset already latitude longitude base. use item interest. first step define bounding box country.need get tiles using get_stamenmap() ggmap.finally, can bring together maps show US military bases Germany (Figure 6.31), Japan (Figure 6.32), Australia (Figure 6.33).\nFigure 6.31: Map US military bases Germany\n\nFigure 6.32: Map US military bases Japan\n\nFigure 6.33: Map US military bases Australia\n","code":"\ninstall.packages(\"troopdata\")\nlibrary(troopdata)\n\nbases <- get_basedata()\n\nhead(bases)\n#> # A tibble: 6 × 9\n#>   countryname ccode iso3c basename   lat   lon  base lilypad\n#>   <chr>       <dbl> <chr> <chr>    <dbl> <dbl> <dbl>   <dbl>\n#> 1 Afghanistan   700 AFG   Bagram …  34.9  69.3     1       0\n#> 2 Afghanistan   700 AFG   Kandaha…  31.5  65.8     1       0\n#> 3 Afghanistan   700 AFG   Mazar-e…  36.7  67.2     1       0\n#> 4 Afghanistan   700 AFG   Gardez    33.6  69.2     1       0\n#> 5 Afghanistan   700 AFG   Kabul     34.5  69.2     1       0\n#> 6 Afghanistan   700 AFG   Herat     34.3  62.2     1       0\n#> # … with 1 more variable: fundedsite <dbl>\nlibrary(ggmap)\n\n# Based on: https://data.humdata.org/dataset/bounding-boxes-for-countries\nbbox_germany <-\n  c(\n    left = 5.867,\n    bottom = 45.967,\n    right = 15.033,\n    top = 55.133\n  )\n\nbbox_japan <-\n  c(\n    left = 127,\n    bottom = 30,\n    right = 146,\n    top = 45\n  )\n\nbbox_australia <-\n  c(\n    left = 112.467,\n    bottom = -45,\n    right = 155,\n    top = -9.133\n  )\ngermany_stamen_map <-\n  get_stamenmap(bbox_germany, zoom = 6, maptype = \"toner-lite\")\n\njapan_stamen_map <-\n  get_stamenmap(bbox_japan, zoom = 6, maptype = \"toner-lite\")\n\naustralia_stamen_map <-\n  get_stamenmap(bbox_australia, zoom = 5, maptype = \"toner-lite\")\nggmap(germany_stamen_map) +\n  geom_point(data = bases,\n             aes(x = lon,\n                 y = lat)\n             ) +\n  labs(x = \"Longitude\",\n       y = \"Latitude\") +\n  theme_minimal() \nggmap(japan_stamen_map) +\n  geom_point(data = bases,\n             aes(x = lon,\n                 y = lat)\n             ) +\n  labs(x = \"Longitude\",\n       y = \"Latitude\") +\n  theme_minimal() \nggmap(australia_stamen_map) +\n  geom_point(data = bases,\n             aes(x = lon,\n                 y = lat)\n             ) +\n  labs(x = \"Longitude\",\n       y = \"Latitude\") +\n  theme_minimal() "},{"path":"static-communication.html","id":"geocoding","chapter":"6 Static communication","heading":"6.4.3 Geocoding","text":"point assumed already geocoded data, means latitude longitude. place names, ‘Canberra, Australia,’ ‘Ottawa, Canada,’ ‘Accra, Ghana,’ ‘Quito, Ecuador’ just names, actually inherently location. plot need get latitude longitude . process going names coordinates called geocoding.range options geocode data R, tidygeocoder especially useful (Cambon Belanger 2021). first need dataframe locations.can now plot label cities (Figure 6.34).\nFigure 6.34: Map Accra, Canberra, Ottawa, Quito geocoding obtain locations\n","code":"\nplace_names <-\n  tibble(\n    city = c('Canberra', 'Ottawa', 'Accra', 'Quito'),\n    country = c('Australia', 'Canada', 'Ghana', 'Ecuador')\n  )\n\nplace_names\n#> # A tibble: 4 × 2\n#>   city     country  \n#>   <chr>    <chr>    \n#> 1 Canberra Australia\n#> 2 Ottawa   Canada   \n#> 3 Accra    Ghana    \n#> 4 Quito    Ecuador\nlibrary(tidygeocoder)\n\nplace_names <-\n  geo(city = place_names$city,\n      country = place_names$country,\n      method = 'osm')\n\nplace_names\n#> # A tibble: 4 × 4\n#>   city     country       lat    long\n#>   <chr>    <chr>       <dbl>   <dbl>\n#> 1 Canberra Australia -35.3   149.   \n#> 2 Ottawa   Canada     45.4   -75.7  \n#> 3 Accra    Ghana       5.56   -0.201\n#> 4 Quito    Ecuador    -0.220 -78.5\nworld <- map_data(map = \"world\")\n\nggplot() +\n  geom_polygon(\n    data = world,\n    aes(x = long,\n        y = lat,\n        group = group),\n    fill = \"white\",\n    colour = \"grey\"\n  ) +\n  coord_map(ylim = c(47,-47)) +\n  geom_point(aes(x = place_names$long,\n                 y = place_names$lat),\n             color = \"black\") +\n  geom_text(aes(\n    x = place_names$long,\n    y = place_names$lat,\n    label = place_names$city\n  ),\n  nudge_y = -5,) +\n  theme_classic() +\n  labs(x = \"Longitude\",\n       y = \"Latitude\")"},{"path":"static-communication.html","id":"exercises-and-tutorial-5","chapter":"6 Static communication","heading":"6.5 Exercises and tutorial","text":"","code":""},{"path":"static-communication.html","id":"exercises-5","chapter":"6 Static communication","heading":"6.5.1 Exercises","text":"Assume tidyverse datasauRus installed loaded. outcome following code?\ndatasaurus_dozen |> filter(dataset == \"high_lines\") |> ggplot(aes(x=x, y=y)) + geom_point()\nFour vertical lines\nFive vertical lines\nThree vertical lines\nTwo vertical lines\nFour vertical linesFive vertical linesThree vertical linesTwo vertical linesAssume tidyverse ‘beps’ dataset installed loaded. change made following make bars different parties next rather top ?\nbeps |> ggplot(mapping = aes(x = age, fill = vote)) + geom_bar()\nposition = \"side_by_side\"\nposition = \"dodge\"\nposition = \"adjacent\"\nposition = \"closest\"\nposition = \"side_by_side\"position = \"dodge\"position = \"adjacent\"position = \"closest\"theme used remove solid lines along x y axes?\ntheme_minimal()\ntheme_classic()\ntheme_bw()\ntheme_dark()\ntheme_minimal()theme_classic()theme_bw()theme_dark()Assume tidyverse ‘beps’ dataset installed loaded. added ‘labs()’ change text legend?\nbeps |> ggplot(mapping = aes(x = age, fill = vote)) + geom_bar() + theme_minimal() + labs(x = \"Age respondent\", y = \"Number respondents\")\ncolor = \"Voted \"\nlegend = \"Voted \"\nscale = \"Voted \"\nfill = \"Voted \"\ncolor = \"Voted \"legend = \"Voted \"scale = \"Voted \"fill = \"Voted \"palette scale_colour_brewer() divergent?\n‘Accent’\n‘RdBu’\n‘GnBu’\n‘Set1’\n‘Accent’‘RdBu’‘GnBu’‘Set1’geom used make scatter plot?\ngeom_smooth()\ngeom_point()\ngeom_bar()\ngeom_dotplot()\ngeom_smooth()geom_point()geom_bar()geom_dotplot()result largest number bins?\ngeom_histogram(binwidth = 5)\ngeom_histogram(binwidth = 2)\ngeom_histogram(binwidth = 5)geom_histogram(binwidth = 2)dataset contains heights 100 birds one three different species. interested understanding distribution heights, paragraph two, please explain type graph used ?Assume dataset columns exist. code work? data |> ggplot(aes(x = col_one)) |> geom_point() (pick one)?\nYes\n\nYesNoWhich geom used plot categorical data (pick one)?\ngeom_bar()\ngeom_point()\ngeom_abline()\ngeom_boxplot()\ngeom_bar()geom_point()geom_abline()geom_boxplot()boxplots often inappropriate (pick one)?\nhide full distribution data.\nhard make.\nugly.\nmode clearly displayed.\nhide full distribution data.hard make.ugly.mode clearly displayed.following, , elements layered grammar graphics (Wickham 2010) (select apply)?\ndefault dataset set mappings variables aesthetics.\nOne layers, layer one geometric object, one statistical transformation, one position adjustment, optionally, one dataset set aesthetic mappings.\nColors enable reader understand main point.\ncoordinate system.\nfacet specification.\nOne scale aesthetic mapping used.\ndefault dataset set mappings variables aesthetics.One layers, layer one geometric object, one statistical transformation, one position adjustment, optionally, one dataset set aesthetic mappings.Colors enable reader understand main point.coordinate system.facet specification.One scale aesthetic mapping used.function modelsummary used create table descriptive statistics?\ndatasummary_descriptive()\ndatasummary_skim()\ndatasummary_crosstab()\ndatasummary_balance()\ndatasummary_descriptive()datasummary_skim()datasummary_crosstab()datasummary_balance()","code":""},{"path":"static-communication.html","id":"tutorial-5","chapter":"6 Static communication","heading":"6.5.2 Tutorial","text":"Using R Markdown, please create graph using ggplot2 map using ggmap add explanatory text accompany . sure include cross-references captions, etc. take one two pages ., graph, please reflect Vanderplas, Cook, Hofmann (2020) add paragraphs different options considered graph effective. (’ve now got least two pages graph ’ve likely written little.)finally, map, please reflect following quote Heather Krause: ‘maps show people aren’t invisible makers’ well Chapter 3 D’Ignazio Klein (2020) add paragraphs related . (, ’ve now got least two pages map ’ve likely written little.)Please submit PDF.","code":""},{"path":"interactive-communication.html","id":"interactive-communication","chapter":"7 Interactive communication","heading":"7 Interactive communication","text":"Required materialRead M-F-E-O: postcards + distill, (Presmanes Hill 2021a).\nRead Geocomputation R, Chapter 2 ‘Geographic data R,’ (Lovelace, Nowosad, Muenchow 2019).Read Mastering Shiny, Chapter 1 ‘first Shiny app,’ (Wickham 2021a).Read Still Can’t See American Slavery , (Bouie 2022).Key concepts skillsBuilding website using within R environment using: postcards (Kross 2021), distill (Allaire et al. 2021), blogdown (Xie, Dervieux, Hill 2021).Adding interaction maps, using leaflet (Cheng, Karambelkar, Xie 2021) mapdeck (Cooley 2020).Adding interaction graphs, using Shiny (Chang et al. 2021).Key librariesblogdown (Xie, Dervieux, Hill 2021)distill (Allaire et al. 2021)leaflet (Cheng, Karambelkar, Xie 2021)mapdeck (Cooley 2020)postcards (Kross 2021)tidyverse (Wickham et al. 2019a)usethis (Wickham Bryan 2020)Key functionsblogdown::serve_site()distill::create_article()leaflet::addCircleMarkers()leaflet::addLegend()leaflet::addMarkers()leaflet::addTiles()leaflet::leaflet()mapdeck:::add_scatterplot()mapdeck::mapdeck()mapdeck::mapdeck_style()shiny::fluidPage()shiny::mainPanel()shiny::plotOutput()shiny::renderPlot()shiny::shinyApp()usethis::edit_r_environ()usethis::use_git()usethis::use_github()","code":""},{"path":"interactive-communication.html","id":"introduction-4","chapter":"7 Interactive communication","heading":"7.1 Introduction","text":"Books papers primary mediums communication thousands years. rise computers, especially internet, recent decades, static approaches complemented interactive approaches. Fundamentally, internet making files available others. additionally allow something make available, need take variety additional aspects consideration.chapter begin covering create publish website. serves place host portfolio work. cover adding interaction maps graphs, two nicely lend .","code":""},{"path":"interactive-communication.html","id":"making-a-website","chapter":"7 Interactive communication","heading":"7.2 Making a website","text":"website critical part communication. instance, place make portfolio work publicly available. One way make website use blogdown (Xie, Dervieux, Hill 2021). blogdown package allows make websites, just blogs, notwithstanding name, largely within R Studio. builds ‘Hugo,’ popular general framework making websites. blogdown enables us freely quickly get website --running. easy add content time--time. integrates R Markdown, makes easy share work. blogdown brittle. dependent Hugo, features work today may work tomorrow. Also, owners Hugo templates can update time, without thought existing users. blogdown good option know , specific use-case, style, mind. two alternatives better starting points.first distill (Allaire et al. 2021). , R package wraps around another framework, case ‘Distill.’ contrast Hugo, Distill focused common needs data science, also maintained one group, may considered stable choice. said, default distill site little plain looking. , following Presmanes Hill (2021a), pair third option, postcards (Kross 2021).third option, one start , postcards (Kross 2021). tailored solution creates simple biographical websites look great. set-GitHub R Studio, literally possible postcards website online five minutes.","code":""},{"path":"interactive-communication.html","id":"postcards","chapter":"7 Interactive communication","heading":"7.2.1 Postcards","text":"Begin installing postcards, install.packages('postcards') creating new project website (‘File’ -> ‘New Project’ -> ‘New Directory’ -> ‘Postcards Website’). can pick name location project, select postcards theme. case, can start ‘trestles’ can changed later. Click option ‘Open new session’ create project.open new file can now build site clicking ‘Knit.’ result one-page website (Figure 7.1).\nFigure 7.1: Example website made postcards using ‘trestles’ theme\ncan now update basic content name, bio links, match (Figure 7.2).\nFigure 7.2: Example Trestles website updated details\ndetails personalized, can push GitHub act host website. default, GitHub try build site, want, need first add hidden file turn , running console:, assuming GitHub set-Chapter 4, can use usethis (Wickham Bryan 2020) get newly created project onto GitHub. use use_git() initialize Git repository, use_github() pushes GitHub.project GitHub. can use GitHub pages host : ‘Settings -> Pages’ change source ‘main’ ‘master,’ depending settings. GitHub let know address can share visit site.","code":"\nfile.create('.nojekyll')\nlibrary(usethis)\nuse_git()\nuse_github()"},{"path":"interactive-communication.html","id":"distill","chapter":"7 Interactive communication","heading":"7.2.2 Distill","text":"now use distill (Allaire et al. 2021) build additional infrastructure around postcards site, following Presmanes Hill (2021a). explore aspects distill make nice choice, mention trade-offs. First, install distill install.packages('distill'), , create new project website (‘File’ -> ‘New Project’ -> ‘New Directory’ -> ‘Distill Blog’).can pick name location project, set title. Select ‘Configure GitHub Pages’ also ‘Open new session.’ options can changed ex post. look something like Figure 7.3.\nFigure 7.3: Example settings setting distill\npoint can click ‘Build Website’ Build tab, see default website (Figure 7.4).\nFigure 7.4: Example default distill website\n, now need update reflect details. default ‘Distill Blog’ blog homepage. can change use postcards page homepage. First change name ‘index.Rmd’ ‘blog.Rmd’ create new ‘trestles’ page:trestles page open, need add following line yaml file: site: distill::distill_website. Figure 7.5 added line 16, can rebuild website.\nFigure 7.5: Updating yaml change homepage\ncan make changes default content earlier, instance, updating links, image, bio. advantage using distill now additional pages, just one-page website, also blog. default, ‘’ page, pages may useful, depending particular use-case, include: ‘research,’ ‘teaching,’ ‘talks,’ ‘projects,’ ‘software,’ ‘datasets.’ example, add edit page called ‘software’ using distill::create_article(file = 'software').create open R Markdown document. add website, open ’_site.yml’ add line ‘navbar’ (Figure 7.6. done can rebuild website, ‘software’ page added.\nFigure 7.6: Adding another page website\ncan continue process happy website. instance, may want add blog. follow pattern , ‘blog’ instead ‘software.’happy website, can push GitHub use GitHub Pages host , way postcards site.Using distill good option need multi-page website, still want fairly controlled environment. many options can changed, Presmanes Hill (2021a) good starting point, addition distill homepage: https://rstudio.github.io/distill/.said, distill opinionated. great option, want something little flexible blogdown might better option.","code":"\npostcards::create_postcard(file = \"index.Rmd\", template = \"trestles\")"},{"path":"interactive-communication.html","id":"blogdown","chapter":"7 Interactive communication","heading":"7.2.3 Blogdown","text":"Using blogdown (Xie, Dervieux, Hill 2021) work Google sites Squarespace. requires little knowledge using basic Wordpress site. need customize absolutely every aspect website, need everything ‘just ’ blogdown may good option. blogdown allows variety level expression possible distill. Presmanes Hill (2021b) Xie, Thomas, Presmanes Hill (2021) useful learning blogdown.First need install blogdown install.packages(\"blogdown\"). create new project website (‘File’ -> ‘New Project’ -> ‘New Directory’ -> ‘Website using blogdown’). point can set name location, also select ‘Open new session’ (Figure 7.7).\nFigure 7.7: Example settings setting blogdown\ncan click ‘Build Website’ ‘Build’ pane, extra step needed; need serve site blogdown:::serve_site(). , site show ‘Viewer’ pane (Figure 7.8).\nFigure 7.8: Serving default blogdown site\ndefault website now ‘served’ locally. means changes make reflected website see Viewer pane. see website web browser, click ‘Show new window’ top left Viewer. open website using address R Studio also provides.now want update content, starting ‘’ section. go ‘content -> .md’ modify add content. One nice aspect blogdown automatically reload content save, changes appear immediately modify aspects also. instance, change logo, adding square image ‘public/images/’ changing call ‘logo.png’ ‘config.yaml.’ happy , can make website public way postcards.One advantage using blogdown allows us use Hugo templates. provides large number beautifully crafted websites. pick theme go Hugo themes page: https://themes.gohugo.io. hundreds different themes. general, can made work blogdown, sometimes can bit hassle.One nice option Apéro: https://hugo-apero-docs.netlify.app. can specify use theme part creating new site (‘File’ -> ‘New Project’ -> ‘New Directory’ -> ‘Website using blogdown’). point, addition setting name location, can specify theme. Specifically, ‘Hugo theme’ field, can specify GitHub username repository, case ‘hugo-apero/apero’ (Figure 7.9).\nFigure 7.9: Using Apéro theme\n","code":""},{"path":"interactive-communication.html","id":"interactive-maps","chapter":"7 Interactive communication","heading":"7.3 Interactive maps","text":"nice thing interactive maps can let user decide interested . instance, case map, people interested , say, Toronto, others interested Chennai even Auckland. difficult present map focused , interactive map way allow users focus want.said, important cognizant build maps, broadly, done scale enable us able build maps. instance, regard Google, McQuire (2019) says:Google began life 1998 company famously dedicated organising vast amounts data Internet. last two decades ambitions changed crucial way. Extracting data words numbers physical world now merely stepping-stone towards apprehending organizing physical world data. Perhaps shift surprising moment become possible comprehend human identity form (genetic) ‘code.’ However, apprehending organizing world data current settings likely take us well beyond Heidegger’s ‘standing reserve’ modern technology enframed ‘nature’ productive resource. 21st century, stuff human life —genetics bodily appearances, mobility, gestures, speech, behaviour—progressively rendered productive resource can harvested continuously subject modulation time.mean use build interactive maps? course . important aware fact frontier, boundaries appropriate use still determined. Indeed, literal boundaries maps consistently determined updated. move digital maps, compared physical printed maps, means possible different users presented different realities. instance, ‘…Google routinely takes sides border disputes. Take, instance, representation border Ukraine Russia. Russia, Crimean Peninsula represented hard-line border Russian-controlled, whereas Ukrainians others see dotted-line border. strategically important peninsula claimed nations violently seized Russia 2014, one many skirmishes control’ Bensinger (2020).","code":""},{"path":"interactive-communication.html","id":"leaflet","chapter":"7 Interactive communication","heading":"7.3.1 Leaflet","text":"can use leaflet (Cheng, Karambelkar, Xie 2021) make interactive maps. essentials similar ggmap (Kahle Wickham 2013), many additional aspects beyond . can redo US military deployments map Chapter 6 used troopdata (Flynn 2021). advantage interactive map can plot bases allow user focus area want, comparison Chapter 6 just picked particular countries.way graph ggplot2 begins ggplot(), map leaflet begins leaflet(). can specify data, options width height. , add ‘layers’ way added ggplot2. first layer add tile, using addTiles(). case, default OpenStreeMap. add markers addMarkers() show location base (Figure 7.10).\nFigure 7.10: Interactive map US bases\ntwo new arguments, compared ggmap. first ‘popup,’ behavior occurs user clicks marker. case, name base provided. second ‘label,’ happens user hovers marker. case name country.can try another example, time amount spent building bases. introduce different type marker , circles. allow us use different colors outcomes type. four possible outcomes: “$100,000,” “$10,000,” “$1,000,” “$1,000 less” (Figure 7.11).\nFigure 7.11: Interactive map US bases colored circules indicate spend\n","code":"\nlibrary(leaflet)\nlibrary(tidyverse)\nlibrary(troopdata)\n\nbases <- get_basedata()\n\n# Some of the bases include unexpected characters which we need to address\nEncoding(bases$basename) <- 'latin1'\n\nleaflet(data = bases) |>\n  addTiles() |>  # Add default OpenStreetMap map tiles\n  addMarkers(lng = bases$lon, \n             lat = bases$lat, \n             popup = bases$basename,\n             label = bases$countryname)\nbuild <- \n  get_builddata(startyear = 2008, endyear = 2019) |>\n  filter(!is.na(lon)) |>\n  mutate(\n    cost = case_when(\n      spend_construction > 100000 ~ \"More than $100,000\",\n      spend_construction > 10000 ~ \"More than $10,000\",\n      spend_construction > 1000 ~ \"More than $1,000\",\n      TRUE ~ \"$1,000 or less\"\n      )\n    )\n\npal <-\n  colorFactor(\"Dark2\", domain = build$cost |> unique())\n\nleaflet() |>\n  addTiles() |>  # Add default OpenStreetMap map tiles\n  addCircleMarkers(\n    data = build,\n    lng = build$lon,\n    lat = build$lat,\n    color = pal(build$cost),\n    popup = paste(\n      \"<b>Location:<\/b>\",\n      as.character(build$location),\n      \"<br>\",\n      \"<b>Amount:<\/b>\",\n      as.character(build$spend_construction),\n      \"<br>\"\n    )\n  ) |>\n  addLegend(\n    \"bottomright\",\n    pal = pal,\n    values = build$cost |> unique(),\n    title = \"Type\",\n    opacity = 1\n  )"},{"path":"interactive-communication.html","id":"mapdeck","chapter":"7 Interactive communication","heading":"7.3.2 Mapdeck","text":"mapdeck (Cooley 2020) based WebGL. means web browser lot work us. enables us accomplish things mapdeck leaflet struggles , larger datasets.point used ‘stamen maps’ underlying tile, mapdeck uses ‘Mapbox’: https://www.mapbox.com/. requires registering account obtaining token. free needs done . token add R environment (details process covered Chapter 8) running usethis::edit_r_environ(), open text file. can add Mapbox secret token.save ‘.Renviron’ file, restart R (‘Session’ -> ‘Restart R’).obtained token, can create plot base spend data earlier (Figure 7.12).\nFigure 7.12: Interactive map US bases using Mapdeck\n","code":"\nMAPBOX_TOKEN = 'PUT_YOUR_MAPBOX_SECRET_HERE'\nlibrary(mapdeck)\n\nmapdeck(style = mapdeck_style('dark')\n        ) |>\n  add_scatterplot(\n    data = build, \n    lat = \"lat\", \n    lon = \"lon\", \n    layer_id = 'scatter_layer',\n    radius = 10,\n    radius_min_pixels = 5,\n    radius_max_pixels = 100,\n    tooltip = \"location\"\n  )"},{"path":"interactive-communication.html","id":"shiny","chapter":"7 Interactive communication","heading":"7.4 Shiny","text":"shiny (Chang et al. 2021) way making interactive web applications using R. fun, fiddly. going step one way take advantage shiny. quickly add interactivity graphs. return shiny detail Chapter 20.going make interactive graph based ‘babynames’ dataset babynames (Wickham 2019b). First, build static version (Figure 7.13).\nFigure 7.13: Popular baby names\ncan see popular boys names tend clustered, compared -popular girls names, may spread . However, one thing might interested effect ‘bins’ parameter shapes see. might like use interactivity explore different values.get started, create new shiny app (‘File -> New File -> Shiny Web App’). Give name, ‘not_my_first_shiny’ leave options default. new file ‘app.R’ open click ‘Run app’ see looks like.Now replace content file, ‘app.R,’ content , click ‘Run app’just build interactive graph number bins can changed. look like Figure 7.14.\nFigure 7.14: Example Shiny app user controls number bins\n","code":"\nlibrary(babynames)\nlibrary(tidyverse)\n\ntop_five_names_by_year <-\n  babynames |>\n  group_by(year, sex) |>\n  arrange(desc(n)) |>\n  slice_head(n = 5)\n\ntop_five_names_by_year |>\n  ggplot(aes(x = n, fill = sex)) +\n  geom_histogram(position = \"dodge\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(x = \"Babies with that name\",\n       y = \"Occurances\",\n       fill = \"Sex\")\nlibrary(shiny)\n\n# Define UI for application that draws a histogram\nui <- fluidPage(\n  # Application title\n  titlePanel(\"Count of names for five most popular names each year.\"),\n  \n  # Sidebar with a slider input for number of bins\n  sidebarLayout(sidebarPanel(\n    sliderInput(\n      inputId = \"number_of_bins\",\n      label = \"Number of bins:\",\n      min = 1,\n      max = 50,\n      value = 30\n    )\n  ),\n  \n  # Show a plot of the generated distribution\n  mainPanel(plotOutput(\"distPlot\")))\n)\n\n# Define server logic required to draw a histogram\nserver <- function(input, output) {\n  output$distPlot <- renderPlot({\n    # Draw the histogram with the specified number of bins\n    top_five_names_by_year |>\n      ggplot(aes(x = n, fill = sex)) +\n      geom_histogram(position = \"dodge\", bins = input$number_of_bins) +\n      theme_minimal() +\n      scale_fill_brewer(palette = \"Set1\") +\n      labs(x = \"Babies with that name\",\n           y = \"Occurances\",\n           fill = \"Sex\")\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)"},{"path":"interactive-communication.html","id":"exercises-and-tutorial-6","chapter":"7 Interactive communication","heading":"7.5 Exercises and tutorial","text":"","code":""},{"path":"interactive-communication.html","id":"exercises-6","chapter":"7 Interactive communication","heading":"7.5.1 Exercises","text":"Based Presmanes Hill (2021a), posts distill re-built automatically (pick one)?\n\nYes\nDepends settings\nNoYesDepends settingsBased Lovelace, Nowosad, Muenchow (2019), please explain paragraph two, difference vector data raster data context geographic data?Based Wickham (2021a), shiny uses:\nObject-oriented programming\nFunctional programming\nReactive programming\nObject-oriented programmingFunctional programmingReactive programmingIn paragraph two, important website?following packages use make website (select apply)?\ndistill\nblogdown\npostcards\nhugo\ndistillblogdownpostcardshugoLooking help file postcards, following themes use (select apply)?\njolla\njolla-blue\njolla-red\ntrestles\nmjolnir\nonofre\nsolana\njollajolla-bluejolla-redtrestlesmjolnironofresolanaWhich function use stop GitHub trying build site instead just serving (pick one)?\nfile.create('.nojekyll')\nfile.remove('.nojekyll')\nfile.create('.jekyll')\nfile.remove('.jekyll')\nfile.create('.nojekyll')file.remove('.nojekyll')file.create('.jekyll')file.remove('.jekyll')argument addMarkers() used specify behavior occurs marker clicked (pick one)?\nlayerId\nicon\npopup\nlabel\nlayerIdiconpopuplabel","code":""},{"path":"interactive-communication.html","id":"tutorial-6","chapter":"7 Interactive communication","heading":"7.5.2 Tutorial","text":"catalyst tutorial work Mauricio Vargas Sepúlveda (‘Pachá’) Andrew Whitby.Please obtain data ethnic origins number victims Auschwitz. use shiny create interactive graph interactive table. show number people murdered nationality/category allow user specify groups interested seeing data . Publish . , based themes brought Bouie (2022), discuss work least two pages. Submit PDF created using R Markdown, ensure contains link app GitHub repo contains code data.","code":""},{"path":"interactive-communication.html","id":"paper-1","chapter":"7 Interactive communication","heading":"7.5.3 Paper","text":"point, Paper Two (Appendix B.2) appropriate.","code":""},{"path":"gather-data.html","id":"gather-data","chapter":"8 Gather data","heading":"8 Gather data","text":"Required materialRead Working-Class Households Reading, (focus method approach, necessarily specific results) (Bowley 1913).Read Representative Method: Method Stratified Sampling Method Purposive Selection, Parts ‘Introduction,’ III ‘Different Aspects Representative Method,’ V ‘Conclusion’ Bowley’s discussion p. 607 - 610, (Neyman 1934).Read Turning History Data: Data Collection, Measurement, Inference HPE, (Cirone Spirling 2021).Read Two Regimes Prison Data Collection, (K. R. Johnson 2021).Key concepts skillsWhy conduct sampling two approaches: probability non-probability.Terminology concepts including: target population, sampling frame, sample, simple random sampling, systematic sampling, stratified sampling, cluster sampling.Ratio regression estimators.Non-probability sampling including convenience quota sampling also snowball respondent-driven sampling.Initial usage APIs, directly, including dealing semi-structured data, indirectly R Packages.Use R environments manage keys.Web scraping, especially reasonable use ethical concerns.Cleaning data unstructured data structured, tidy, data.Extracting data PDFs, able parsed image require OCR.Key librariesbabynames (Wickham 2019b)httr (Wickham 2019c)jsonlite (Ooms 2014)lubridate (Grolemund Wickham 2011)pdftools (Ooms 2018a)purrr (Henry Wickham 2020)rtweet (Kearney 2019)rvest (Wickham 2019d)spotifyr (Thompson et al. 2020)tesseract (Ooms 2018b)tidyverse (Wickham et al. 2019a)usethis (Wickham Bryan 2020)xml2 (Wickham, Hester, Ooms 2021)Key functionsdownload.file()dplyr::row_number()dplyr::slice_sample()factor()function()httr::GET()pdftools::pdf_text()purrr::walk2()rtweet::get_favorites()rtweet::get_friends()rtweet::get_timelines()rtweet::search_tweets()rvest::html_nodes()rvest::html_text()set.seed()spotifyr::get_artist_audio_features()sys.sleep()tesseract::ocr()usethis::edit_r_environ()","code":""},{"path":"gather-data.html","id":"introduction-5","chapter":"8 Gather data","heading":"8.1 Introduction","text":"think world telling stories , one difficult aspects reduce beautiful complexity dataset can use. need know giving . Often, interested understanding implications dataset, making forecasts based , using dataset make claims broader world. Regardless turn world data, ever sample data need. Statistics provides formal approaches use keep issues front mind.chapter first\nintroduce statistical notions around sampling provide framework use guide data gathering. \ngo variety approaches gathering data, including use APIs semi-structured data, JSON XML, web scraping, converting PDFs,\nusing optical character recognition, especially obtain text data.","code":""},{"path":"gather-data.html","id":"sampling-essentials","chapter":"8 Gather data","heading":"8.2 Sampling essentials","text":"Statistics heart telling stories data. Statisticians spent considerable time effort thinking properties various samples data enable us speak implications broader population.Let us say data. instance, particular toddler goes sleep 6:00pm every night. might interested know whether bedtime common among toddlers, unusual toddler. one toddler ability use bedtime speak toddlers limited.One approach talk friends also toddlers. talk friends friends. many friends, friends friends, ask can begin feel comfortable speaking underlying truth toddler bedtime?Wu Thompson (2020, 3) describe statistics ‘science collect analyze data draw statements conclusions unknown populations.’ ‘population’ refers infinite group can never know exactly, can use probability distributions random variables describe characteristics . Another way say statistics involves getting data trying say something sensible based .critical terminology includes:‘Target population’: collection items like speak.‘Sampling frame’: list items target population get data .‘Sample’: items sampling frame get data .target population finite set labelled items, size \\(N\\). instance, hypothetically add label books world: ‘Book 1,’ ‘Book 2,’ ‘Book 3,’ …, ‘Book \\(N\\).’ difference use term population , everyday usage. instance, one sometimes hears work census data say need worry sampling whole population country. conflation terms, sample gathered census population country.can difficult define target population. instance, say asked find consumption habits hipsters. can define target population? someone regularly eats avocado toast, never drunk bullet coffee, population? aspects might interested formally defined extent always commonly realized. instance, whether area classified rural often formally defined country’s statistical agency. aspects less clear. instance, classify someone ‘smoker?’ 15-year-old 100 cigarettes lifetime, need treat differently none. 90-year-old 100 cigarettes lifetime, likely different 90-year-old none? age, number cigarettes answers change?Consider want speak titles books ever written. target population books ever written. almost impossible us imagine get information title book written nineteenth century, author locked desk never told anyone . One sampling frame books Library Congress Online Catalog, another 25 million digitized Google (Somers 2017). finally, sample may tens thousands available Project Gutenberg, can access using gutenbergr (D. Robinson 2021).consider another example, consider wanting speak attitudes Brazilians live Germany. target population Brazilians live Germany. One possible source information Facebook case, sampling frame might Brazilians live Germany Facebook. sample might Brazilians live Germany Facebook can gather data . target population sampling frame different Brazilians live Germany Facebook. sampling frame different sample likely able gather data Brazilians live Germany Facebook.","code":""},{"path":"gather-data.html","id":"sampling-in-dublin-and-reading","chapter":"8 Gather data","heading":"8.2.1 Sampling in Dublin and Reading","text":"clearer, consider two examples: 1798 count number inhabitants Dublin, Ireland (Whitelaw 1905), 1912 count working-class households Reading, England (Bowley 1913).1798 Reverend James Whitelaw conducted survey Dublin, Ireland, count population. Whitelaw (1905) describes population estimates wide variation, instance estimated size London time ranged 128,570 300,000. Reverend Whitelaw expected Lord Mayor Dublin compel person charge house affix list inhabitants house door, Reverend Whitelaw simply use .Instead, found lists ‘frequently illegible, generally short actual number third, even one-half.’ instead recruited assistants, went door--door making counts. resulting estimates particularly informative (Figure 8.1). total population Dublin 1798 estimated 182,370.\nFigure 8.1: Extract results Reverend Whitelaw found 1798\nOne aspect worth noticing Reverend Whitelaw includes information class. difficult know determined, played large role data collection. Reverend Whitelaw describes houses ‘middle upper classes always contained individual competent task [making list].’ ‘among lower class, forms great mass population city, case different.’ difficult know Reverend Whitelaw known upper middle classes representing number, lower class . also difficult imagine Reverend Whitelaw going houses upper class counting number, assistants lower classes. always, issue defining target population difficult one, seems may slightly different approaches class.little one hundred years later, Bowley (1913) interested counting number working-class households Reading, England. Bowley selects sample using following procedure (Bowley 1913, 672):One building ten marked throughout local directory alphabetical order streets, making 1,950 . 300 marked shops, factories, institutions non-residential buildings, 300 found indexed among Principal Residents, marked. remaining 1,350 working-class houses, number volunteers set visit every one … []t decided take one house 20, rejecting incomplete information intermediate tenths. visitors instructed never substitute another house marked, however difficult proved get information, whatever type house.Bowley (1913) continues ended information 622 working-class households. , judged, based census 18,000 households Reading, Bowley (1913) applies ‘[t]multiplier twenty-one… sample data give estimates whole Reading.’ Bowley (1913) explains reasonableness estimates depends ‘proportion whole, magnitude, conditions random sampling secured, believed inquiry.’ Bowley , instance, able furnish information rent paid per week (Figure 8.2).\nFigure 8.2: Extract results Bowley found rent paid working-class Reading\n","code":""},{"path":"gather-data.html","id":"probabilistic-sampling","chapter":"8 Gather data","heading":"8.2.2 Probabilistic sampling","text":"identified target population sampling frame, need distinguish probability non-probability sampling, Neyman (1934) describes ‘random sampling’ ‘purposive selection’:‘Probability sampling’: Every unit sampling frame , known, chance sampled specific sample obtained randomly based chances. Note chances necessarily need unit.‘Non-probability sampling’: Units sampling frame sampled based convenience, quotas, judgement, non-random processes.Often difference probability non-probability sampling one degree. instance, often forcibly obtain data almost always aspect volunteering. Even penalties providing data, case completing census form many countries, difficult even government force people fill completely truthfully. One reason Randomized Control Trial revolution, discussed Chapter 9, needed due lack probability sampling. important aspect clear probability sampling role uncertainty. allows us make claims population, based sample, known amounts error. trade-probability sampling often expensive difficult.add specificity discussion, following Lohr (2019, 27) may help consider numbers 1 100 let us define target population. simple random sampling, every unit chance included. case 20 per cent. expect around 20 units sample, around 1 5 compared target population.systematic sampling, used Bowley (1913), proceed selecting value, say 5. randomly pick starting point units 1 5, say 3. include every fifth unit. starting point usually randomly selecting.consider population, typically grouping. may straight-forward country states, provinces, counties, statistical districts; university faculties departments; humans age-groups. stratified structure one can divide population mutually exclusive collectively exhaustive sub-populations, strata.use stratification help efficiency sampling balance survey. instance, population US around 335 million, 40 million California, Wyoming around half million. even survey 10,000 responses expect 15 responses Wyoming, make inference Wyoming difficult. use stratification ensure 200 responses 50 US states. use random sampling within state select person data gathered.case, stratify illustration, consider strata 10s, , 1 10 one stratum, 11 20 another, . use simple random sampling within strata select two units.finally, can also take advantage clusters may exist dataset. Like strata, clusters collectively exhaustive mutually exclusive. examples earlier, states, departments, age-groups remain valid clusters. However, intentions toward groups different. Specific, cluster sampling, intend collect data every cluster, whereas stratified sampling . stratified sampling look every stratum conduct simple random sampling within strata select sample. cluster sampling conduct simple random sampling select clusters interest. can either sample every unit selected clusters use simple random sampling, within selected clusters, select units. said, difference can become less clear practice, especially ex post.case, cluster illustration based 10s. use simple random sampling select two clusters use entire cluster.point can illustrate differences approaches (Figure 8.3).\nFigure 8.3: Illustrative example simple random sampling, systematic sampling, stratified sampling, cluster sampling numbers 1 100\nestablished sample, typically want use make claims population. Neyman (1934, 561) goes says ‘[o]bviously problem representative method par excellence problem statistical estimation. interested characteristics certain population, \\(\\pi\\), either impossible least difficult study detail, try estimate characteristics basing judgment sample.’particular, typically interested estimate population mean variance.Scaling can used interested using count sample imply total count population. saw Bowley (1913) ratio number households sample, compared number households known census, 21, information used scale sample.consider example, perhaps interested sum numbers 1 100. know samples size 20, need scaled five times (Table 8.1).Table 8.1: Sum numbers sample, implied sum populationThe actual sum population 5,050. can obtain using trick, attributed Euler, noticed sum 1 number can quickly obtained finding middle number multiplying one plus number. , case, 50*101. Alternatively can use R: sum(1:100).estimate population sum, based scaling, especially revealing. closest stratified sample, closely followed systematic sampling. Cluster sampling little 10 per cent , simple random sampling little away. get close, important sampling method gets many higher values possible. stratified systematic sampling, ensured unit larger numbers particularly well. performance cluster simple random sampling depend particular clusters, units, selected. case, stratified systematic sampling ensured estimate sum population, far away actual population sum.approach long history. instance, Adolphe Quetelet, nineteenth century astronomer, mathematician, statistician, sociologist proposed one. Stigler (1986, 163) describes 1826 Quetelet become involved statistical bureau, planning census. Quetelet argued births deaths well known, migration . proposed approach based counts specific geographies, scaled whole country. criticism plan focused difficulty selecting appropriate geographies, saw also example cluster sampling. criticism reasonable, even today, two hundred years later, something keep front mind, (Stigler 1986):[Quetelet] acutely aware infinite number factors affect quantities wished measure, lacked information tell indeed important. … reluctant group together homogenous, data reason believe … aware myriad potentially important factors, without knowing truly important effect may felt, often fear worst’…. [Quetelet] bring treat large regions homogeneous, [] think single rate applying large areaWe able scaling know population total, know , concerns around precision approach may use ratio estimator.Ratio estimators also long history. instance, 1802 used Pierre-Simon Laplace estimate total population France, based ratio number registered births, known throughout country, number inhabitants, know certain communes. calculated ratio three communes, scaled , based knowing number births across whole country produce estimate population France (Lohr 2019).particular, ratio estimator population parameter ratio two means. instance, may information number hours toddler sleeps overnight, \\(x\\), number hours parents sleep overnight \\(y\\) 30 day period.average :ratio estimate proportion sleep parent gets compared toddler :\\[\\hat{B} = \\frac{\\bar{y}}{\\bar{x}} = \\frac{4.9}{6.16} \\approx 0.8\\]acknowledging spectrum, much statistics developed based probability sampling. considerable amount modern sampling done using non-probability sampling. common approach use Facebook advertisements recruit panel respondents exchange compensation. panel group sent various surveys necessary. think moment implications . instance, type people likely respond advertisement? richest person world likely respond? especially young especially old people likely respond? cases, possible census. Nation-states typically one every five ten years. reason nation states —expensive, time-consuming, surprisingly, sometimes accurate may hope general need .","code":"\nlibrary(tidyverse)\n\nset.seed(853)\n\nillustrative_sampling <-\n  tibble(unit = 1:100,\n         simple_random_sampling = \n           sample(\n             x = c(\"Included\", \"Not included\"),\n             size = 100,\n             replace = TRUE,\n             prob = c(0.2, 0.8)\n             ))\n\nillustrative_sampling\n#> # A tibble: 100 × 2\n#>     unit simple_random_sampling\n#>    <int> <chr>                 \n#>  1     1 Not included          \n#>  2     2 Not included          \n#>  3     3 Not included          \n#>  4     4 Not included          \n#>  5     5 Not included          \n#>  6     6 Not included          \n#>  7     7 Not included          \n#>  8     8 Not included          \n#>  9     9 Not included          \n#> 10    10 Not included          \n#> # … with 90 more rows\nset.seed(853)\n\nstarting_point <- sample(x = c(1:5), \n                         size = 1)\n\nillustrative_sampling <-\n  illustrative_sampling |>\n  mutate(systematic_sampling = \n           if_else(row_number() %in% seq.int(from = starting_point, \n                                             to = 100, \n                                             by = 5), \n                   \"Included\", \n                   \"Not included\")\n         )\n\nillustrative_sampling\n#> # A tibble: 100 × 3\n#>     unit simple_random_sampling systematic_sampling\n#>    <int> <chr>                  <chr>              \n#>  1     1 Not included           Included           \n#>  2     2 Not included           Not included       \n#>  3     3 Not included           Not included       \n#>  4     4 Not included           Not included       \n#>  5     5 Not included           Not included       \n#>  6     6 Not included           Included           \n#>  7     7 Not included           Not included       \n#>  8     8 Not included           Not included       \n#>  9     9 Not included           Not included       \n#> 10    10 Not included           Not included       \n#> # … with 90 more rows\nset.seed(853)\n\nselected_within_strata <-\n  illustrative_sampling |>\n  mutate(strata = (row_number() - 1) %/% 10) |>\n  group_by(strata) |>\n  slice_sample(n = 2) |>\n  pull(unit)\n\nillustrative_sampling <-\n  illustrative_sampling |>\n  mutate(\n    stratified_sampling = if_else(\n      row_number() %in% selected_within_strata,\n      \"Included\",\n      \"Not included\"\n    )\n  )\n\nillustrative_sampling\n#> # A tibble: 100 × 4\n#>     unit simple_random_sa… systematic_samp… stratified_samp…\n#>    <int> <chr>             <chr>            <chr>           \n#>  1     1 Not included      Included         Included        \n#>  2     2 Not included      Not included     Not included    \n#>  3     3 Not included      Not included     Not included    \n#>  4     4 Not included      Not included     Not included    \n#>  5     5 Not included      Not included     Not included    \n#>  6     6 Not included      Included         Not included    \n#>  7     7 Not included      Not included     Not included    \n#>  8     8 Not included      Not included     Not included    \n#>  9     9 Not included      Not included     Included        \n#> 10    10 Not included      Not included     Not included    \n#> # … with 90 more rows\nset.seed(853)\n\nselected_clusters <- \n  sample(x = c(0:9),\n         size = 2)\n\nillustrative_sampling <-\n  illustrative_sampling |>\n  mutate(cluster = (row_number() - 1) %/% 10, \n         cluster_sampling = if_else(\n           cluster %in% selected_clusters,\n           \"Included\",\n           \"Not included\"\n           )\n         ) %>% \n  select(-cluster)\n\nillustrative_sampling\n#> # A tibble: 100 × 5\n#>     unit simple_random_sa… systematic_samp… stratified_samp…\n#>    <int> <chr>             <chr>            <chr>           \n#>  1     1 Not included      Included         Included        \n#>  2     2 Not included      Not included     Not included    \n#>  3     3 Not included      Not included     Not included    \n#>  4     4 Not included      Not included     Not included    \n#>  5     5 Not included      Not included     Not included    \n#>  6     6 Not included      Included         Not included    \n#>  7     7 Not included      Not included     Not included    \n#>  8     8 Not included      Not included     Not included    \n#>  9     9 Not included      Not included     Included        \n#> 10    10 Not included      Not included     Not included    \n#> # … with 90 more rows, and 1 more variable:\n#> #   cluster_sampling <chr>\nnew_labels <- c(simple_random_sampling = \"Simple random sampling\", \n                systematic_sampling = \"Systematic sampling\",\n                stratified_sampling = \"Stratified sampling\",\n                cluster_sampling = \"Cluster sampling\")\n\nillustrative_sampling_long <- \n  illustrative_sampling |>\n  pivot_longer(\n    cols = c(\n      simple_random_sampling,\n      systematic_sampling,\n      stratified_sampling,\n      cluster_sampling),\n    names_to = \"sampling_method\",\n    values_to = \"in_sample\"\n  ) |>\n  mutate(sampling_method = factor(sampling_method,\n                                  levels = c(\"simple_random_sampling\",\n                                             \"systematic_sampling\",\n                                             \"stratified_sampling\",\n                                             \"cluster_sampling\"))\n         ) \n\nillustrative_sampling_long |>\n  filter(in_sample == \"Included\") |>\n  ggplot(aes(x = unit, y = in_sample)) +\n  geom_point() +\n  facet_wrap(vars(sampling_method),\n             dir = \"v\",\n             ncol = 1,\n             labeller = labeller(sampling_method = new_labels)\n             ) +\n  theme_minimal() +\n  labs(x = \"Unit\",\n       y = \"Is included in sample\") +\n  theme(axis.text.y = element_blank())\nillustrative_sampling_long |>\n  filter(in_sample == \"Included\") |>\n  group_by(sampling_method) |>\n  summarize(sum_from_sample = sum(unit)) |>\n  mutate(scaled_by_five = sum_from_sample * 5) |>\n  knitr::kable(\n    caption = \"Sum of the numbers in each sample, and implied sum of population\",\n    col.names = c(\"Sampling method\", \"Sum of sample\", \"Implied population sum\"),\n    format.args = list(big.mark = \",\")\n  )\nset.seed(853)\n\nsleep <- \n  tibble(\n    toddler_sleep = sample(x = c(2:14), size = 30, replace = TRUE),\n    difference = sample(x = c(0:3), size = 30, replace = TRUE),\n    parent_sleep = toddler_sleep - difference\n  )\n\nsleep\n#> # A tibble: 30 × 3\n#>    toddler_sleep difference parent_sleep\n#>            <int>      <int>        <int>\n#>  1            10          1            9\n#>  2            11          0           11\n#>  3            14          2           12\n#>  4             2          2            0\n#>  5             6          1            5\n#>  6            14          2           12\n#>  7             3          0            3\n#>  8             5          2            3\n#>  9             4          3            1\n#> 10             4          1            3\n#> # … with 20 more rows\nsleep %>% \n  summarize(toddler_sleep_average = mean(toddler_sleep),\n            parent_sleep_average = mean(parent_sleep))\n#> # A tibble: 1 × 2\n#>   toddler_sleep_average parent_sleep_average\n#>                   <dbl>                <dbl>\n#> 1                  6.17                  4.9"},{"path":"gather-data.html","id":"non-probability-samples","chapter":"8 Gather data","heading":"8.2.3 Non-probability samples","text":"Non-probability samples important role play typically cheaper quicker obtain probability samples. , discussed, difference probability non-probability samples sometimes one degree, rather dichotomy. case, non-probability samples legitimate appropriate tasks provided one clear trade-offs ensure transparency (Baker et al. 2013).Convenience sampling involves gathering data sample easy access. instance, one often asks one’s friends family fill survey way testing wide-scale distribution. instead analyze sample, likely using convenience sampling.main issue convenience sampling unlikely able speak much broader population filled survey. also tricky ethical considerations, typically lack anonymity may bias results. hand, can useful cheaply get quick sense situation rolling sampling approaches likely broadly useful.Quota sampling occurs strata, use random sampling within strata select unit. instance, stratified US based state, instead ensuring everyone Wyoming chance chosen stratum, just picked people Jackson Hole. , advantages approach, especially terms speed cost, resulting sample likely biased various ways.saying goes, birds feather flock together. can take advantage sampling. Although Handcock Gile (2011) describe various uses , notoriously difficult define attribution multidisciplinary work, snowball sampling nicely defined Goodman (1961). Following Goodman (1961), conduct snowball sampling, first draw random sample sampling frame. asked name \\(k\\) others also sample population, initial draw, form ‘first stage.’ individual first stage similarly asked name \\(k\\) others also sample population, random draw first stage, form ‘second stage.’ need specified number stages, \\(s\\), also \\(k\\) ahead time.Respondent-driven sampling developed Heckathorn (1997) focus hidden populations, : 1) sampling frame 2) known sampling population negative effect. instance, imagine various countries difficult sample gay population abortions illegal. Respondent-driven sampling differs snowball sampling two ways: 1) addition compensation response, case snowball sampling, respondent-driven sampling typically also involves compensation recruiting others. 2) Respondents asked provide information others investigator, instead recruit study. Selection sample occurs sampling frame, instead networks already sample (M. J. Salganik Heckathorn 2004).established foundations sampling, remain front mind, turn describe approaches gathering data. largely represent convenience samples.","code":""},{"path":"gather-data.html","id":"apis","chapter":"8 Gather data","heading":"8.3 APIs","text":"everyday language, purposes, Application Programming Interface (API) situation someone set specific files computer can follow instructions get . instance, use gif Slack, Slack asks Giphy’s server appropriate gif, Giphy’s server gives gif Slack Slack inserts chat. way Slack Giphy interact determined Giphy’s API. strictly, API just application runs server access using HTTP protocol.focus using APIs gathering data. , focus, API website set-another computer able access, rather person. instance, go Google Maps: https://www.google.com/maps. scroll click drag center map Canberra, Australia. paste browser: https://www.google.com/maps/@-35.2812958,149.1248113,16z. just used Google Maps API, result map similar Figure 8.4.\nFigure 8.4: Example Google Maps, 29 January 2022\nadvantage using API data provider specifies exactly data willing provide, terms provide . terms may include aspects rate limits (.e. often can ask data), can data, instance, might allowed use commercial purposes, republish . Additionally, API provided specifically us use , less likely subject unexpected changes legal issues. ethically legally clear API available try use rather web scraping.now go case studies using APIs. first deal directly API using httr (Wickham 2019c). second access data Twitter using rtweet (Kearney 2019). third access data Spotify using spotifyr (Thompson et al. 2020).","code":""},{"path":"gather-data.html","id":"case-study-gathering-data-from-arxiv-nasa-and-dataverse","chapter":"8 Gather data","heading":"8.3.1 Case study: Gathering data from arXiv, NASA, and Dataverse","text":"use GET() httr (Wickham 2019c) obtain data API directly. try get specific data main argument ‘url.’ way, similar earlier Google Maps example. example, specific information interested map.case study use API provided arXiv: https://arxiv.org. arXiv online repository academic papers go peer-review, typically referred ‘pre-prints.’ installing loading httr, use GET() ask arXiv obtain information pre-print R. Alexander Alexander (2021).can use status_code() check whether received error server. assuming received something back server, can use content() display information. case received XML formatted data, can read using read_xml() xml2 (Wickham, Hester, Ooms 2021). XML semi-formatted structure, can useful start look using html_structure().might interested create dataset based extracting various aspects XML tree. instance, might interested look ‘entry,’ eighth item, particular obtain title URL, fourth ninth items, respectively, within entry.day NASA provides Astronomy Picture Day (APOD) APOD API. can use GET() obtain URL photo particular dates display .Examining returned data using content(), can see provided various fields, date, title, explanation, URL. can provide URL include_graphics() knitr display (Figure 8.5).\nFigure 8.5: Photo James Webb Space Telescope Earth another Tranquility Base obtained NASA APOD API\nFinally, another common API response semi-structured form JSON. can parse JSON jsonlite (Ooms 2014). Dataverse web application makes easier share dataset. can use API go query demonstration dataverse. instance might interested datasets related politics.can also look dataset using View(politics_datasets), allows us expand tree based interested even get code need focus different aspects hovering item clicking icon green arrow (Figure 8.6).\nFigure 8.6: Example hovering JSON element, ‘items,’ icon green arrow can clicked get code focus element\ntells us obtain dataset interest.","code":"\nlibrary(httr)\nlibrary(tidyverse)\nlibrary(xml2)\n\narxiv <-\n  GET(\"http://export.arxiv.org/api/query?id_list=2111.09299\")\n\nstatus_code(arxiv)\n#> [1] 200\ncontent(arxiv) |>\n  read_xml() |>\n  html_structure()\n#> <feed [xmlns]>\n#>   <link [href, rel, type]>\n#>   <title [type]>\n#>     {text}\n#>   <id>\n#>     {text}\n#>   <updated>\n#>     {text}\n#>   <totalResults [xmlns:opensearch]>\n#>     {text}\n#>   <startIndex [xmlns:opensearch]>\n#>     {text}\n#>   <itemsPerPage [xmlns:opensearch]>\n#>     {text}\n#>   <entry>\n#>     <id>\n#>       {text}\n#>     <updated>\n#>       {text}\n#>     <published>\n#>       {text}\n#>     <title>\n#>       {text}\n#>     <summary>\n#>       {text}\n#>     <author>\n#>       <name>\n#>         {text}\n#>     <author>\n#>       <name>\n#>         {text}\n#>     <comment [xmlns:arxiv]>\n#>       {text}\n#>     <link [href, rel, type]>\n#>     <link [title, href, rel, type]>\n#>     <primary_category [term, scheme, xmlns:arxiv]>\n#>     <category [term, scheme]>\ndata_from_arxiv <-\n  tibble(\n    title = content(arxiv) |>\n      read_xml() |>\n      xml_child(search = 8) |>\n      xml_child(search = 4) |>\n      xml_text(),\n    link = content(arxiv) |>\n      read_xml() |>\n      xml_child(search = 8) |>\n      xml_child(search = 9) |>\n      xml_attr(\"href\")\n  )\ndata_from_arxiv\n#> # A tibble: 1 × 2\n#>   title                                                link \n#>   <chr>                                                <chr>\n#> 1 \"The Increased Effect of Elections and Changing Pri… http…\nNASA_APOD_20211226 <-\n  GET(\"https://api.nasa.gov/planetary/apod?api_key=DEMO_KEY&date=2021-12-26\")\n\nNASA_APOD_20190719 <-\n  GET(\"https://api.nasa.gov/planetary/apod?api_key=DEMO_KEY&date=2019-07-19\")\ncontent(NASA_APOD_20211226)$date\n#> [1] \"2021-12-26\"\ncontent(NASA_APOD_20211226)$title\n#> [1] \"James Webb Space Telescope over Earth\"\ncontent(NASA_APOD_20211226)$explanation\n#> [1] \"There's a big new telescope in space. This one, the James Webb Space Telescope (JWST), not only has a mirror over five times larger than Hubble's in area, but can see better in infrared light. The featured picture shows JWST high above the Earth just after being released by the upper stage of an Ariane V rocket, launched yesterday from French Guiana. Over the next month, JWST will move out near the Sun-Earth L2 point where it will co-orbit the Sun with the Earth. During this time and for the next five months, JWST will unravel its segmented mirror and an array of sophisticated scientific instruments -- and test them. If all goes well, JWST will start examining galaxies across the universe and planets orbiting stars across our Milky Way Galaxy in the summer of 2022.   APOD Gallery: Webb Space Telescope Launch\"\ncontent(NASA_APOD_20211226)$url\n#> [1] \"https://apod.nasa.gov/apod/image/2112/JwstLaunch_Arianespace_1080.jpg\"\n\ncontent(NASA_APOD_20190719)$date\n#> [1] \"2019-07-19\"\ncontent(NASA_APOD_20190719)$title\n#> [1] \"Tranquility Base Panorama\"\ncontent(NASA_APOD_20190719)$explanation\n#> [1] \"On July 20, 1969 the Apollo 11 lunar module Eagle safely touched down on the Moon. It landed near the southwestern corner of the Moon's Mare Tranquillitatis at a landing site dubbed Tranquility Base. This panoramic view of Tranquility Base was constructed from the historic photos taken from the lunar surface. On the far left astronaut Neil Armstrong casts a long shadow with Sun is at his back and the Eagle resting about 60 meters away ( AS11-40-5961). He stands near the rim of 30 meter-diameter Little West crater seen here to the right ( AS11-40-5954). Also visible in the foreground is the top of the camera intended for taking stereo close-ups of the lunar surface.\"\ncontent(NASA_APOD_20190719)$url\n#> [1] \"https://apod.nasa.gov/apod/image/1907/apollo11TranquilitybasePan600h.jpg\"\nlibrary(jsonlite)\n\npolitics_datasets <- fromJSON(\"https://demo.dataverse.org/api/search?q=politics\")\nas_tibble(politics_datasets[[\"data\"]][[\"items\"]])\n#> # A tibble: 10 × 18\n#>    name      type  url   identifier description published_at\n#>    <chr>     <chr> <chr> <chr>      <chr>       <chr>       \n#>  1 China Ar… data… http… china-arc… Introducti… 2016-12-09T…\n#>  2 cc16_AK.… file  http… <NA>       Alaska sub… 2021-12-26T…\n#>  3 cc16_AL.… file  http… <NA>       <NA>        2021-12-26T…\n#>  4 cc16_AR.… file  http… <NA>       <NA>        2021-12-26T…\n#>  5 cc16_AZ.… file  http… <NA>       <NA>        2021-12-26T…\n#>  6 cc16_CA.… file  http… <NA>       <NA>        2021-12-26T…\n#>  7 cc16_CO.… file  http… <NA>       <NA>        2021-12-26T…\n#>  8 cc16_CT.… file  http… <NA>       <NA>        2021-12-26T…\n#>  9 cc16_DC.… file  http… <NA>       DC subset   2021-12-27T…\n#> 10 cc16_DE.… file  http… <NA>       <NA>        2021-12-26T…\n#> # … with 12 more variables: file_id <chr>, file_type <chr>,\n#> #   file_content_type <chr>, size_in_bytes <int>,\n#> #   md5 <chr>, checksum <df[,2]>, unf <chr>,\n#> #   file_persistent_id <chr>, dataset_name <chr>,\n#> #   dataset_id <chr>, dataset_persistent_id <chr>,\n#> #   dataset_citation <chr>"},{"path":"gather-data.html","id":"case-study-gathering-data-from-twitter","chapter":"8 Gather data","heading":"8.3.2 Case study: Gathering data from Twitter","text":"Twitter rich source text data. Twitter API way Twitter asks gather data. rtweet (Kearney 2019) built around API allows us interact ways similar using R package. Initially, can use Twitter API just regular Twitter account.Begin installing loading rtweet tidyverse. need authorize rtweet start process calling function package, instance get_favorites() return tibble user’s favorites. open browser, log regular Twitter account (Figure 8.7).\nFigure 8.7: rtweet authorisation page\napplication authorized, can use get_favorites() actually get favorites user save .look recent favorites, keeping mind may different depending accessed.can use search_tweets() search tweets particular topic. instance, look tweets using hashtag commonly associated R: ‘#rstats.’useful functions can used include get_friends() get accounts user follows, get_timelines() get user’s recent tweets. Registering developer enables access API functionality.using APIs, even wrapped R package, case rtweet, important read terms access provided. Twitter API docs surprisingly readable, developer policy especially clear: https://developer.twitter.com/en/developer-terms/policy. see easy violate terms API provider makes data available, consider saved tweets downloaded. push GitHub, possible may accidentally stored sensitive information happened tweets. Twitter also explicit asking use API especially careful sensitive information matching Twitter users -Twitter folks. , documentation around restricted uses clear readable: https://developer.twitter.com/en/developer-terms/--restricted-use-cases.","code":"\nlibrary(rtweet)\nlibrary(tidyverse)\nget_favorites(user = \"RohanAlexander\")\nrohans_favorites <- get_favorites(\"RohanAlexander\")\n\nsaveRDS(rohans_favorites, \"rohans_favorites.rds\")\nrohans_favorites |> \n  arrange(desc(created_at)) |> \n  slice(1:10) |> \n  select(screen_name, text)\n#> # A tibble: 10 × 2\n#>    screen_name text                                         \n#>    <chr>       <chr>                                        \n#>  1 EconAndrew  \"How much better are the investment opportun…\n#>  2 simonpcouch \"There's a new release of #rstats broom up o…\n#>  3 MineDogucu  \"🚨 New manuscript🚨\\n📕 Content and Computi…\n#>  4 reid_nancy  \"Latest issue. From the intro: \\\"... it has …\n#>  5 tjmahr      \"bathing is good, folks\"                     \n#>  6 andrewheiss \"finished hand washing that load in the bath…\n#>  7 monkmanmh   \"@CMastication https://t.co/3Eh0mLy44v\"      \n#>  8 eplusgg     \"Stares from Ontario https://t.co/swzYhaptF9\"\n#>  9 ryancbriggs \"Same. https://t.co/C9pNpXO0F9\"              \n#> 10 flynnpolsci \"I’m not great at coming up with assignments…\nrstats_tweets <- search_tweets(\n  q = \"#rstats\",\n  include_rts = FALSE\n)\n\nsaveRDS(rstats_tweets, \"rstats_tweets.rds\")\nrstats_tweets |> \n  select(screen_name, text) |> \n  head()\n#> # A tibble: 6 × 2\n#>   screen_name     text                                      \n#>   <chr>           <chr>                                     \n#> 1 SuccessAnalytiX \"The Science of Success \\n\\nhttps://t.co/…\n#> 2 babycoin_dev    \"BabyCoin (BABY)\\n\\nGUI wallet v2.05 =&gt…\n#> 3 rstatsdata      \"#rdata #rstats: Yield of 6 barley variet…\n#> 4 PDH_SciTechNews \"#Coding Arm Puts Security Architecture t…\n#> 5 PDH_SciTechNews \"#Coding Network Engineer: Skills, Roles …\n#> 6 PDH_SciTechNews \"#Coding CockroachDB Strengthens Change D…"},{"path":"gather-data.html","id":"case-study-gathering-data-from-spotify","chapter":"8 Gather data","heading":"8.3.3 Case study: Gathering data from Spotify","text":"final case study, use spotifyr (Thompson et al. 2020), wrapper around Spotify API. Install install.packages('spotifyr') load package.access Spotify API, need Spotify Developer Account: https://developer.spotify.com/dashboard/. require logging Spotify account accepting Developer Terms (Figure 8.8).\nFigure 8.8: Spotify Developer Account Terms agreement page\nContinuing registration process, case, ‘know’ building Spotify requires us use non-commercial agreement. use Spotify API need ‘Client ID’ ‘Client Secret.’ things want keep anyone details use developer account though us. One way keep details secret minimum hassle keep ‘System Environment.’ way, push GitHub included. (followed process without explanation Chapter 7 used mapdeck.) use usethis (Wickham Bryan 2020) modify System Environment. particular, file called ‘.Renviron’ open using edit_r_environ() add ‘Client ID’ ‘Client Secret’ .run edit_r_environ(), ‘.Renviron’ file open can add ‘Spotify Client ID’ ‘Client Secret.’ important use names, spotifyr look environment keys specific names.Save ‘.Renviron’ file, restart R (‘Session’ -> ‘Restart R’). can now use ‘Spotify Client ID’ ‘Client Secret’ needed. functions require details arguments work without explicitly specified . get save information Radiohead, English rock band, using get_artist_audio_features(). One required arguments authorization, set, default, look ‘.Renviron’ file, need specify .variety information available based songs. might interested see whether songs getting longer time (Figure 8.9).\nFigure 8.9: Length Radiohead song, time, gathered Spotify\nOne interesting variable provided Spotify song ‘valence.’ Spotify documentation describe measure 0 1 signals ‘musical positiveness’ track higher values positive. details available documentation: https://developer.spotify.com/documentation/web-api/reference/#/operations/get-audio-features. might interested compare valence time artists, instance, American rock band National, American singer Taylor Swift.First, need gather data.can bring together make graph (Figure 8.10). appears show Taylor Swift Radiohead largely maintained level valence overtime, National decreased .\nFigure 8.10: Comparing valence, time, Radiohead, Taylor Swift, National\namazing live world information available little effort cost. gathered data, lot done. instance, Pavlik (2019) uses expanded dataset classify musical genres Economist (2022) looks language associated music streaming Spotify. ability gather data enables us answer questions considered experimentally past, instance M. J. Salganik, Dodds, Watts (2006) use experimental data rather real data able access. time, worth thinking valence purporting represent. Little information available Spotify documentation created. doubtful one number can completely represent positive song .","code":"\nlibrary(spotifyr)\nlibrary(usethis)\n\nedit_r_environ()\nSPOTIFY_CLIENT_ID = 'PUT_YOUR_CLIENT_ID_HERE'\nSPOTIFY_CLIENT_SECRET = 'PUT_YOUR_SECRET_HERE'\nradiohead <- get_artist_audio_features('radiohead')\nsaveRDS(radiohead, \"radiohead.rds\")\nradiohead <- readRDS(\"radiohead.rds\")\nradiohead |> \n  select(artist_name, track_name, album_name) |> \n  head()\n#>   artist_name                    track_name   album_name\n#> 1   Radiohead Everything In Its Right Place KID A MNESIA\n#> 2   Radiohead                         Kid A KID A MNESIA\n#> 3   Radiohead           The National Anthem KID A MNESIA\n#> 4   Radiohead   How to Disappear Completely KID A MNESIA\n#> 5   Radiohead                   Treefingers KID A MNESIA\n#> 6   Radiohead                    Optimistic KID A MNESIA\nlibrary(lubridate)\n\nradiohead |> \n  mutate(album_release_date = ymd(album_release_date)) |> \n  ggplot(aes(x = album_release_date, y = duration_ms)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Album release date\",\n       y = \"Duration of song (ms)\"\n       ) \ntaylor_swift <- get_artist_audio_features('taylor swift')\nthe_national <- get_artist_audio_features('the national')\n\nsaveRDS(taylor_swift, \"taylor_swift.rds\")\nsaveRDS(the_national, \"the_national.rds\")\nthree_artists <-\n  rbind(taylor_swift, the_national, radiohead) |>\n  select(artist_name, album_release_date, valence) |>\n  mutate(album_release_date = ymd(album_release_date))\n\nthree_artists |>\n  ggplot(aes(x = album_release_date,\n             y = valence,\n             color = artist_name)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth() +\n  theme_minimal() +\n  labs(x = \"Album release date\",\n       y = \"Valence\",\n       color = \"Artist\") +\n  scale_color_brewer(palette = \"Set1\")"},{"path":"gather-data.html","id":"web-scraping","chapter":"8 Gather data","heading":"8.4 Web scraping","text":"Web scraping way get data websites. Rather going website using browser copy pasting, write code us. opens lot data us, hand, typically data made available purposes. means important respectful . generally illegal, specifics legality web scraping depend jurisdictions specifics , also important mindful . use rarely commercially competitive, particular concern conflict need work reproducible need respect terms service may disallow data republishing (Luscombe, Dick, Walby 2021). finally, web scraping imposes cost website host, important reduce extent possible.said, web scraping invaluable source data. typically datasets can created -product someone trying achieve another aim. instance, retailer may website products prices. created deliberately source data, can scrape create dataset. , following principles useful guide web scraping.Avoid . Try use API wherever possible.Abide desires. websites ‘robots.txt’ file contains information comfortable scrapers , instance ‘https://www.google.com/robots.txt.’Reduce impact.\nFirstly, slow scraper, instance, rather visit website every second, slow using sys.sleep(). need hundred files, just visit website times minute, running background overnight?\nSecondly, consider timing run scraper. instance, scraping retailer maybe set script run 10pm morning, fewer customers likely using site. Similarly, government website big monthly release, might polite avoid day.\nFirstly, slow scraper, instance, rather visit website every second, slow using sys.sleep(). need hundred files, just visit website times minute, running background overnight?Secondly, consider timing run scraper. instance, scraping retailer maybe set script run 10pm morning, fewer customers likely using site. Similarly, government website big monthly release, might polite avoid day.Take needed. instance, need scrape entire Wikipedia need names ten largest cities Croatia. reduces impact website, allows us easily justify actions.scrape . means save everything go re-collect data. Similarly, data, keep separate modify . course, need data time need go back, different needlessly re-scraping page.republish pages scraped. (contrasts datasets create .)Take ownership ask permission possible. minimum level scripts contact details . Depending circumstances, may worthwhile asking permission scrape.Web scraping possible taking advantage underlying structure webpage. use patterns HTML/CSS get data want. look underlying HTML/CSS can either:open browser, right-click, choose something like ‘Inspect’; orsave website open text editor rather browser.HTML/CSS markup language comprised matching tags. want text bold, use something like:Similarly, want list start end list, well item.scraping search tags.get started, can pretend obtained HTML website, want get name . can see name bold, want focus feature extract .use read_html() rvest (Wickham 2019d) read data.language used rvest look tags ‘node,’ focus bold nodes. default html_nodes() returns tags well. can focus text contain, html_text().","code":"<b>My bold text<\/b><ul>\n  <li>Learn webscraping<\/li>\n  <li>Do data science<\/li>\n  <li>Proft<\/li>\n<\/ul>\nwebsite_extract <- \"<p>Hi, I’m <b>Rohan<\/b> Alexander.<\/p>\"\n# install.packages(\"rvest\")\nlibrary(rvest)\n\nrohans_data <- read_html(website_extract)\n\nrohans_data\n#> {html_document}\n#> <html>\n#> [1] <body><p>Hi, I’m <b>Rohan<\/b> Alexander.<\/p><\/body>\nrohans_data |> \n  html_nodes(\"b\")\n#> {xml_nodeset (1)}\n#> [1] <b>Rohan<\/b>\n\nfirst_name <- \n  rohans_data |> \n  html_nodes(\"b\") |>\n  html_text()\n\nfirst_name\n#> [1] \"Rohan\""},{"path":"gather-data.html","id":"case-study-web-scraping-book-information","chapter":"8 Gather data","heading":"8.4.1 Case study: Web scraping book information","text":"case study scrape list books : https://rohanalexander.com/bookshelf.html. clean data look distribution first letters author surnames. slightly complicated example , underlying approach : download website, look nodes interest, extract information, clean .use rvest (Wickham 2019d) download website, navigate html find aspects interested . use tidyverse clean dataset. first need go website save local copy.Now need navigate HTML get aspects want, put sensible structure. start trying get data tibble quickly possible allow us easily use dplyr verbs tidyverse functions.get data tibble first need identify data interested using html tags. look website need focus list items (Figure 8.11). can look source, focusing particularly looking list (Figure 8.12).\nFigure 8.11: Books website displayed\n\nFigure 8.12: HTML top books website list books\ntag list item ‘li,’ can use focus list.now need clean data. First want separate title author using separate() clean author title columns.end need get rid ‘best .’Finally, make table distribution first letter names (Table 8.2).Table 8.2: Distribution first letter author names collection books","code":"\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(xml2)\n\nbooks_data <- read_html(\"https://rohanalexander.com/bookshelf.html\")\n\nwrite_html(books_data, \"raw_data.html\") \nbooks_data <- read_html(\"inputs/my_website/raw_data.html\")\nbooks_data\n#> {html_document}\n#> <html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"\" xml:lang=\"\">\n#> [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text ...\n#> [2] <body>\\n\\n<!--radix_placeholder_front_matter-->\\n\\n<s ...\ntext_data <- \n  books_data |>\n  html_nodes(\"li\") |>\n  html_text()\n\nall_books <- \n  tibble(books = text_data)\n\nhead(all_books)\n#> # A tibble: 6 × 1\n#>   books                     \n#>   <chr>                     \n#> 1 Academic                  \n#> 2 Non-fiction               \n#> 3 Fiction                   \n#> 4 Cookbooks                 \n#> 5 Want to buy               \n#> 6 Best books that I read in:\nall_books <-\n  all_books |>\n  slice(7:nrow(all_books)) |>\n  separate(books, into = c(\"author\", \"title\"), sep = \", ‘\")\n\nall_books <-\n  all_books |>\n  separate(title, into = c(\"title\", \"debris\"), sep = \"’.\") |>\n  select(-debris) |>\n  mutate(author = str_remove_all(author, \"^, \"),\n         author = str_squish(author),\n         title = str_remove(title, \"“\"),\n         title = str_remove(title, \"^-\")\n         )\n\nhead(all_books)\n#> # A tibble: 6 × 2\n#>   author                               title                \n#>   <chr>                                <chr>                \n#> 1 Bryant, John, and Junni L. Zhang     Bayesian Demographic…\n#> 2 Chan, Ngai Hang                      Time Series          \n#> 3 Clark, Greg                          The Son Also Rises   \n#> 4 Duflo, Esther                        Expérience, science …\n#> 5 Foster, Ghani, Jarmin, Kreuter, Lane Big Data and Social …\n#> 6 Francois Chollet with JJ Allaire     Deep Learning with R\nall_books <- \n  all_books |> \n  slice(1:142) |>\n  filter(author != \"‘150 Years of Stats Canada!’.\")\nall_books |> \n  mutate(\n    first_letter = str_sub(author, 1, 1)\n    ) |> \n  group_by(first_letter) |> \n  count() |>\n  knitr::kable(\n    caption = \"Distribution of first letter of author names in a collection of books\",\n    col.names = c(\"First letter\", \"Number of times\"),\n    booktabs = TRUE, \n    linesep = \"\"\n    )"},{"path":"gather-data.html","id":"case-study-web-scraping-uk-prime-ministers-from-wikipedia","chapter":"8 Gather data","heading":"8.4.2 Case study: Web scraping UK Prime Ministers from Wikipedia","text":"case study interested long UK prime ministers lived, based year born. scrape data Wikipedia using rvest (Wickham 2019d), clean , make graph. Every time scrape website things change. scrape largely bespoke, even can borrow code earlier projects. completely normal feel frustrated times. helps begin end mind.end, can start generating simulated data. Ideally, want table row prime minister, column name, column birth death years. still alive, death year can empty. know birth death years somewhere 1700 1990, death year larger birth year. Finally, also know years integers, names characters. , want something looks roughly like :One advantages generating simulated dataset working groups one person can start making graph, using simulated dataset, person gathers data. terms graph, aiming something like Figure 8.13.\nFigure 8.13: Sketch planned graph showing long UK prime ministers lived\nstarting question interest, long UK prime minister lived. , need identify source data plenty data sources births deaths prime minister, want one can trust, going scraping, want one structure . Wikipedia page UK prime ministers fits criteria: https://en.wikipedia.org/wiki/List_of_prime_ministers_of_the_United_Kingdom. popular page information likely correct, data available table.load rvest download page using read_html(). Saving locally provides us copy need reproducibility case website changes, also means keep visiting website. likely property, typically something necessarily redistributed.earlier case study looking patterns HTML can use help us get closer data want. iterative process requires lot trial error. Even simple examples take time.One tool may help SelectorGadget: https://rvest.tidyverse.org/articles/articles/selectorgadget.html. allows us pick choose elements want, gives us input give html_nodes() (Figure 8.14)\nFigure 8.14: Using Selector Gadget identify tag, 13 March 2020.\ncase blank lines need filter away.Now parsed data, need clean match wanted. particular want names column, well columns birth year death year. use separate() take advantage fact looks like dates distinguished brackets.Finally, need clean columns.dataset looks similar one said wanted start (Table 8.3).Table 8.3: UK Prime Ministers, old diedAt point like make graph illustrates long prime minister lived. still alive like run end, like color differently.","code":"\nlibrary(babynames)\n\nset.seed(853)\n\nsimulated_dataset <-\n  tibble(\n    prime_minister = sample(\n      x = babynames |> filter(prop > 0.01) |>\n        select(name) |> unique() |> unlist(),\n      size = 10,\n      replace = FALSE\n    ),\n    birth_year = sample(\n      x = c(1700:1990),\n      size = 10,\n      replace = TRUE\n    ),\n    years_lived = sample(\n      x = c(50:100),\n      size = 10,\n      replace = TRUE\n    ),\n    death_year = birth_year + years_lived\n  ) |>\n  select(prime_minister, birth_year, death_year, years_lived) |>\n  arrange(birth_year)\nlibrary(rvest)\nlibrary(tidyverse)\nraw_data <-\n  read_html(\"https://en.wikipedia.org/wiki/List_of_prime_ministers_of_the_United_Kingdom\")\nwrite_html(raw_data, \"pms.html\")\n# Read in our saved data\nraw_data <- read_html(\"pms.html\")\n# We can parse tags in order\nparse_data_selector_gadget <- \n  raw_data |> \n  html_nodes(\"td:nth-child(3)\") |> \n  html_text()\n\nhead(parse_data_selector_gadget)\n#> [1] \"\\nSir Robert Walpole(1676–1745)\\n\"                       \n#> [2] \"\\nSpencer Compton1st Earl of Wilmington(1673–1743)\\n\"    \n#> [3] \"\\nHenry Pelham(1694–1754)\\n\"                             \n#> [4] \"\\nThomas Pelham-Holles1st Duke of Newcastle(1693–1768)\\n\"\n#> [5] \"\\nWilliam Cavendish4th Duke of Devonshire(1720–1764)\\n\"  \n#> [6] \"\\nThomas Pelham-Holles1st Duke of Newcastle(1693–1768)\\n\"\nparsed_data <-\n  tibble(raw_text = parse_data_selector_gadget) |>\n  filter(raw_text != \"—\\n\") |>\n  filter(\n    !raw_text %in% c(\n      \"\\n1868\\n\",\n      \"\\n1874\\n\",\n      \"\\n1880\\n\",\n      \"\\n1885\\n\",\n      \"\\n1892\\n\",\n      \"\\n1979\\n\",\n      \"\\n1997\\n\",\n      \"\\n2010\\n\"\n    )\n  ) |>\n  filter(\n    !raw_text %in% c(\n      \"\\nNational Labour\\n\",\n      \"\\nWilliam Pulteney1st Earl of Bath(1684–1764)\\n\",\n      \"\\nJames Waldegrave2nd Earl Waldegrave(1715–1763)\\n\",\n      \"\\nEdward VII\\n\\n\\n1901–1910\\n\\n\", \n      \"\\nGeorge V\\n\\n\\n1910–1936\\n\\n\"\n    )\n  )\n\nhead(parsed_data)\n#> # A tibble: 6 × 1\n#>   raw_text                                                  \n#>   <chr>                                                     \n#> 1 \"\\nSir Robert Walpole(1676–1745)\\n\"                       \n#> 2 \"\\nSpencer Compton1st Earl of Wilmington(1673–1743)\\n\"    \n#> 3 \"\\nHenry Pelham(1694–1754)\\n\"                             \n#> 4 \"\\nThomas Pelham-Holles1st Duke of Newcastle(1693–1768)\\n\"\n#> 5 \"\\nWilliam Cavendish4th Duke of Devonshire(1720–1764)\\n\"  \n#> 6 \"\\nThomas Pelham-Holles1st Duke of Newcastle(1693–1768)\\n\"\ninitial_clean <- \n  parsed_data |> \n  mutate(raw_text = str_remove_all(raw_text, \"\\n\")) |>\n  separate(raw_text, \n            into = c(\"Name\", \"not_name\"), \n            sep = \"\\\\(\",\n            remove = FALSE) |> # The remove = FALSE option here means that we \n  # keep the original column that we are separating.\n  separate(not_name, \n            into = c(\"Date\", \"all_the_rest\"), \n            sep = \"\\\\)\",\n            remove = FALSE)\n\nhead(initial_clean)\n#> # A tibble: 6 × 5\n#>   raw_text                 Name  not_name Date  all_the_rest\n#>   <chr>                    <chr> <chr>    <chr> <chr>       \n#> 1 Sir Robert Walpole(1676… Sir … 1676–17… 1676… \"\"          \n#> 2 Spencer Compton1st Earl… Spen… 1673–17… 1673… \"\"          \n#> 3 Henry Pelham(1694–1754)  Henr… 1694–17… 1694… \"\"          \n#> 4 Thomas Pelham-Holles1st… Thom… 1693–17… 1693… \"\"          \n#> 5 William Cavendish4th Du… Will… 1720–17… 1720… \"\"          \n#> 6 Thomas Pelham-Holles1st… Thom… 1693–17… 1693… \"\"\ninitial_clean <- \n initial_clean |> \n  separate(col = Name, \n           into = c(\"Name\", \"Title\"),\n           sep = \"[[:digit:]]\",\n           extra = \"merge\",\n           fill = \"right\") |>\n  separate(col = Name, \n           into = c(\"Name\", \"Title\"),\n           sep = \"MP for\",\n           extra = \"merge\",\n           fill = \"right\") |>\n  mutate(Name = str_remove(Name, \"\\\\[b\\\\]\"))\n\nhead(initial_clean)\n#> # A tibble: 6 × 6\n#>   raw_text           Name  Title not_name Date  all_the_rest\n#>   <chr>              <chr> <chr> <chr>    <chr> <chr>       \n#> 1 Sir Robert Walpol… Sir … <NA>  1676–17… 1676… \"\"          \n#> 2 Spencer Compton1s… Spen… <NA>  1673–17… 1673… \"\"          \n#> 3 Henry Pelham(1694… Henr… <NA>  1694–17… 1694… \"\"          \n#> 4 Thomas Pelham-Hol… Thom… <NA>  1693–17… 1693… \"\"          \n#> 5 William Cavendish… Will… <NA>  1720–17… 1720… \"\"          \n#> 6 Thomas Pelham-Hol… Thom… <NA>  1693–17… 1693… \"\"\ncleaned_data <- \n  initial_clean |> \n  select(Name, Date) |> \n  separate(Date, into = c(\"Birth\", \"Died\"), sep = \"–\", remove = FALSE) |> # The \n  # PMs who have died have their birth and death years separated by a hyphen, but \n  # we need to be careful with the hyphen as it seems to be a slightly odd type of \n  # hyphen and we need to copy/paste it.\n  mutate(Birth = str_remove_all(Birth, \"born\"),\n         Birth = str_trim(Birth)\n         ) |> # Alive PMs have slightly different format\n  select(-Date) |> \n  mutate(Name = str_remove(Name, \"\\n\")) |> # Remove some html tags that remain\n  mutate_at(vars(Birth, Died), ~as.integer(.)) |> # Change birth and death to integers\n  mutate(Age_at_Death = Died - Birth) |>  # Add column of the number of years they lived\n  distinct() # Some of the PMs had two goes at it.\n\nhead(cleaned_data)\n#> # A tibble: 6 × 4\n#>   Name                 Birth  Died Age_at_Death\n#>   <chr>                <int> <int>        <int>\n#> 1 Sir Robert Walpole    1676  1745           69\n#> 2 Spencer Compton       1673  1743           70\n#> 3 Henry Pelham          1694  1754           60\n#> 4 Thomas Pelham-Holles  1693  1768           75\n#> 5 William Cavendish     1720  1764           44\n#> 6 John Stuart           1713  1792           79\ncleaned_data |> \n  knitr::kable(\n    caption = \"UK Prime Ministers, by how old they were when they died\",\n    col.names = c(\"Prime Minister\", \"Birth year\", \"Death year\", \"Age at death\"),\n    booktabs = TRUE, \n    linesep = \"\"\n    )\ncleaned_data |> \n  mutate(still_alive = if_else(is.na(Died), \"Yes\", \"No\"),\n         Died = if_else(is.na(Died), as.integer(2022), Died)) |> \n  mutate(Name = as_factor(Name)) |> \n  ggplot(aes(x = Birth, \n             xend = Died,\n             y = Name,\n             yend = Name, \n             color = still_alive)) +\n  geom_segment() +\n  labs(x = \"Year of birth\",\n       y = \"Prime minister\",\n       color = \"PM is alive\",\n       title = \"How long each UK Prime Minister lived, by year of birth\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")"},{"path":"gather-data.html","id":"case-study-downloading-multiple-files","chapter":"8 Gather data","heading":"8.4.3 Case study: Downloading multiple files","text":"Considering text data exciting opens lot different research questions. Many guides assume already nicely formatted text dataset, rarely actually case. case study download files different pages. already seen two examples web scraping, focused just one page, whereas often need many. focus iteration. use download.file() download, purrr (Henry Wickham 2020) apply function across multiple sites.Reserve Bank Australia (RBA) Australia’s central bank sets monetary policy. responsibility setting cash rate, interest rate used loans banks. interest rate especially important one, large impact interest rates economy. Four times year – February, May, August, November – RBA publishes statement monetary policy, available PDFs. example download four statements published 2021.First set-dataframe information need.can apply function download.files() fourThen can write function download file, let us know downloaded, wait polite amount time, go get next file.now apply function list URLs.result downloaded four PDFs saved computer. next section build discuss getting information PDFs.","code":"\nlibrary(tidyverse)\n\nstatements_of_interest <- \n  tibble(\n    address = c(\"https://www.rba.gov.au/publications/smp/2021/nov/pdf/00-overview.pdf\",\n                \"https://www.rba.gov.au/publications/smp/2021/aug/pdf/00-overview.pdf\",\n                \"https://www.rba.gov.au/publications/smp/2021/may/pdf/00-overview.pdf\",\n                \"https://www.rba.gov.au/publications/smp/2021/feb/pdf/00-overview.pdf\"\n                ),\n    local_save_name = c(\n      \"2021-11.pdf\",\n      \"2021-08.pdf\",\n      \"2021-05.pdf\",\n      \"2021-02.pdf\"\n    )\n  )\n\nstatements_of_interest\n#> # A tibble: 4 × 2\n#>   address                                    local_save_name\n#>   <chr>                                      <chr>          \n#> 1 https://www.rba.gov.au/publications/smp/2… 2021-11.pdf    \n#> 2 https://www.rba.gov.au/publications/smp/2… 2021-08.pdf    \n#> 3 https://www.rba.gov.au/publications/smp/2… 2021-05.pdf    \n#> 4 https://www.rba.gov.au/publications/smp/2… 2021-02.pdf\nvisit_download_and_wait <-\n  function(the_address_to_visit, where_to_save_it_locally) {\n    \n    download.file(url = the_address_to_visit,\n                  destfile = where_to_save_it_locally\n                  )\n\n    print(paste(\"Done with\", the_address_to_visit, \"at\", Sys.time()))\n    \n    Sys.sleep(sample(5:10, 1))\n  }\nwalk2(statements_of_interest$address,\n      statements_of_interest$local_save_name,\n      ~visit_download_and_wait(.x, .y))"},{"path":"gather-data.html","id":"pdfs","chapter":"8 Gather data","heading":"8.5 PDFs","text":"contrast API, PDF usually produced human rather computer consumption. nice thing PDFs static constant. nice make data available . trade-:overly useful larger-scale statistical analysis.know PDF put together know whether can trust .manipulate data get results interested .Indeed, sometimes governments publish data PDFs actually want us able analyze . able get data PDFs opens large number datasets.two important aspects keep mind approaching PDF mind extracting data :Begin end mind. Planning literally sketching want final dataset/graph/paper stops us wasting time keeps us focused.Start simple, iterate. quickest way make complicated model often first build simple model complicate . Start just trying get one page PDF working even just one line. iterate .start walking several examples go case study gather data US Total Fertility Rate, state.Figure 8.15 PDF consists just first sentence Jane Eyre taken Project Gutenberg (Bronte 1847).\nFigure 8.15: First sentence Jane Eyre\nassume saved ‘first_example.pdf,’ can pdftools (Ooms 2019a) get text one-page PDF R.can see PDF correctly read , character vector.now try slightly complicated example consists first paragraphs Jane Eyre (Figure 8.16). Also notice now chapter heading well.\nFigure 8.16: First paragraphs Jane Eyre\nuse function ., character vector. end line signaled ‘\\n,’ looks pretty good. Finally, consider first two pages.Notice first page first element character vector, second page second element. familiar rectangular data, try get format quickly possible. can use regular tidyverse functions deal .First want convert character vector tibble. point may like add page numbers well.want separate lines line observation. can looking ‘\\n’ remembering need escape backslash special character.","code":"\nlibrary(pdftools)\nlibrary(tidyverse)\n\nfirst_example <- pdf_text(\"first_example.pdf\")\n\nfirst_example\n\nclass(first_example)\nsecond_example <- pdftools::pdf_text(\"second_example.pdf\")\n\nsecond_example\n\nclass(second_example)\nthird_example <- pdftools::pdf_text(\"third_example.pdf\")\n\nthird_example\n\nclass(third_example)\njane_eyre <- tibble(raw_text = third_example,\n                    page_number = c(1:2))\njane_eyre <- \n  separate_rows(jane_eyre, raw_text, sep = \"\\\\n\", convert = FALSE)\njane_eyre\n#> # A tibble: 93 × 2\n#>    raw_text                                      page_number\n#>    <chr>                                               <int>\n#>  1 \"CHAPTER I\"                                             1\n#>  2 \"There was no possibility of taking a walk t…           1\n#>  3 \"leafless shrubbery an hour in the morning; …           1\n#>  4 \"company, dined early) the cold winter wind …           1\n#>  5 \"penetrating, that further out-door exercise…           1\n#>  6 \"\"                                                      1\n#>  7 \"I was glad of it: I never liked long walks,…           1\n#>  8 \"coming home in the raw twilight, with nippe…           1\n#>  9 \"chidings of Bessie, the nurse, and humbled …           1\n#> 10 \"Eliza, John, and Georgiana Reed.\"                      1\n#> # … with 83 more rows"},{"path":"gather-data.html","id":"case-study-gathering-data-on-the-us-total-fertility-rate","chapter":"8 Gather data","heading":"8.5.1 Case-study: Gathering data on the US Total Fertility Rate","text":"US Department Health Human Services Vital Statistics Report provides information total fertility rate (average number births per woman women experience current age-specific fertility rates throughout reproductive years) state nineteen years. US persists making data available PDFs, hinders research. can use approaches get data nice dataset.instance, case year 2000 table interested page 40 PDF available https://www.cdc.gov/nchs/data/nvsr/nvsr50/nvsr50_05.pdf. column interest labelled: “Total fertility rate” (Figure 8.17).\nFigure 8.17: Example Vital Statistics Report, 2000\nfirst step getting data PDF sketch eventually want. PDF typically contains lot information, handy clear need. helps keep focused, prevents scope creep, also helpful thinking data checks. literally write paper mind. case, needed table column state, year TFR (Figure 8.18).\nFigure 8.18: Planned dataset TFR year US state\n19 different PDFs, interested particular column particular table . Unfortunately, nothing magical coming. first step requires working link , page column name interest. end, looks like .first step get code works one . ’ll step code lot detail normal ’re going use pieces lot.choose year 2000. first download save PDF using download.file().read PDF character vector using pdf_text() pdftools. convert tibble, can use familiar verbs .Grab page interest (remembering page element character vector, hence row tibble).Now want separate rows.Now searching patterns can use. Let us look first ten lines content.get much better :dots separating states data.space columns.can now separate separate columns. First want match least two dots (remembering dot special character needs escaped).get expected warnings top bottom multiple dots. (Another option use pdf_data() allow us use location rather delimiters.)can now separate data based spaces. inconsistent number spaces, first squish example one space just one.looking fairly great. thing left clean .’re done year. Now want take pieces, put function run function 19 years.first part downloading 19 PDFs need. going build code used . code :modify need:iterate lines dataset contains CSVs (.e. says 1, want 1, 2, 3, etc.).filename, need iterate desired filenames (.e. year_2000, year_2001, year_2002, etc).like way little robust errors. instance, one URLs wrong internet drops like just move onto next PDF, warn us end missed one, stop. (really matter 19 files, easy find oneself thousands files).draw purrr (Henry Wickham 2020).take download.file() pass two arguments: .x .y. walk2() applies function inputs give , case URLs columns .x pdf_names column .y. Finally, safely() means failures just moves onto next file instead throwing error.now PDFs saved can move onto getting data .Now need get data PDFs. , going build code used . code (overly condensed) :first thing want iterate argument pdf_text(), number slice() also need change (work get page interested ).Two aspects hardcoded, may need updated. particular:separate works tables columns order; andthe slice (restricts data just states) works case.Finally, add year end, whereas need bring earlier process.start writing function go files, grab data, get page interest, expand rows. use pmap_dfr() purrr apply function PDFs output tibble.Now need clean state names filter .next step separate data get correct column . going separate based spaces cleaned .can now grab correct column.Finally, need convert case.run checks.particular want 51 states 19 years.done (Table 8.4)!Table 8.4: First ten rows dataset TFR US state, 2000-2019","code":"\nsummary_tfr_dataset |> \n  select(year, page, table, column_name, url) |> \n  gt()\ndownload.file(url = summary_tfr_dataset$url[1], \n              destfile = \"year_2000.pdf\")\n# INTERNAL\ndownload.file(url = summary_tfr_dataset$url[1], \n              destfile = \"inputs/pdfs/dhs/year_2000.pdf\")\nlibrary(pdftools)\ndhs_2000 <- pdf_text(\"year_2000.pdf\")\ndhs_2000 <- tibble(raw_data = dhs_2000)\n\nhead(dhs_2000)\n#> # A tibble: 6 × 1\n#>   raw_data                                                  \n#>   <chr>                                                     \n#> 1 \"Volume 50, Number 5                                     …\n#> 2 \"2   National Vital Statistics Report, Vol. 50, No. 5, Fe…\n#> 3 \"                                                        …\n#> 4 \"4   National Vital Statistics Report, Vol. 50, No. 5, Fe…\n#> 5 \"                                                        …\n#> 6 \"6   National Vital Statistics Report, Vol. 50, No. 5, Fe…\ndhs_2000 <- \n  dhs_2000 |> \n  slice(summary_tfr_dataset$page[1])\n\nhead(dhs_2000)\n#> # A tibble: 1 × 1\n#>   raw_data                                                  \n#>   <chr>                                                     \n#> 1 \"40 National Vital Statistics Report, Vol. 50, No. 5, Rev…\ndhs_2000 <- \n  dhs_2000 |> \n  separate_rows(raw_data, sep = \"\\\\n\", convert = FALSE)\n\nhead(dhs_2000)\n#> # A tibble: 6 × 1\n#>   raw_data                                                  \n#>   <chr>                                                     \n#> 1 \"40 National Vital Statistics Report, Vol. 50, No. 5, Rev…\n#> 2 \"\"                                                        \n#> 3 \"Table 10. Number of births, birth rates, fertility rates…\n#> 4 \"United States, each State and territory, 2000\"           \n#> 5 \"[By place of residence. Birth rates are live births per …\n#> 6 \"estimated in each area; total fertility rates are sums o…\ndhs_2000[13:22,]\n#> # A tibble: 10 × 1\n#>    raw_data                                                 \n#>    <chr>                                                    \n#>  1 \"                                  State                …\n#>  2 \"                                                       …\n#>  3 \"                                                       …\n#>  4 \"\"                                                       \n#>  5 \"\"                                                       \n#>  6 \"United States 1 .......................................…\n#>  7 \"\"                                                       \n#>  8 \"Alabama ...............................................…\n#>  9 \"Alaska ................................................…\n#> 10 \"Arizona ...............................................…\ndhs_2000 <- \n  dhs_2000 |> \n  separate(col = raw_data, \n           into = c(\"state\", \"data\"), \n           sep = \"\\\\.{2,}\", \n           remove = FALSE,\n           fill = \"right\"\n           )\n\nhead(dhs_2000)\n#> # A tibble: 6 × 3\n#>   raw_data                                       state data \n#>   <chr>                                          <chr> <chr>\n#> 1 \"40 National Vital Statistics Report, Vol. 50… \"40 … <NA> \n#> 2 \"\"                                             \"\"    <NA> \n#> 3 \"Table 10. Number of births, birth rates, fer… \"Tab… <NA> \n#> 4 \"United States, each State and territory, 200… \"Uni… <NA> \n#> 5 \"[By place of residence. Birth rates are live… \"[By… <NA> \n#> 6 \"estimated in each area; total fertility rate… \"est… <NA>\ndhs_2000 <- \n  dhs_2000 |>\n  mutate(data = str_squish(data)) |> \n  tidyr::separate(col = data, \n           into = c(\"number_of_births\", \n                    \"birth_rate\", \n                    \"fertility_rate\", \n                    \"TFR\", \n                    \"teen_births_all\", \n                    \"teen_births_15_17\", \n                    \"teen_births_18_19\"), \n           sep = \"\\\\s\", \n           remove = FALSE\n           )\n\nhead(dhs_2000)\n#> # A tibble: 6 × 10\n#>   raw_data           state data  number_of_births birth_rate\n#>   <chr>              <chr> <chr> <chr>            <chr>     \n#> 1 \"40 National Vita… \"40 … <NA>  <NA>             <NA>      \n#> 2 \"\"                 \"\"    <NA>  <NA>             <NA>      \n#> 3 \"Table 10. Number… \"Tab… <NA>  <NA>             <NA>      \n#> 4 \"United States, e… \"Uni… <NA>  <NA>             <NA>      \n#> 5 \"[By place of res… \"[By… <NA>  <NA>             <NA>      \n#> 6 \"estimated in eac… \"est… <NA>  <NA>             <NA>      \n#> # … with 5 more variables: fertility_rate <chr>, TFR <chr>,\n#> #   teen_births_all <chr>, teen_births_15_17 <chr>,\n#> #   teen_births_18_19 <chr>\ndhs_2000 <- \n  dhs_2000 |> \n  select(state, TFR) |> \n  slice(13:69) |> \n  mutate(year = 2000)\n\ndhs_2000\n#> # A tibble: 57 × 3\n#>    state                                         TFR    year\n#>    <chr>                                         <chr> <dbl>\n#>  1 \"                                  State    … <NA>   2000\n#>  2 \"                                           … <NA>   2000\n#>  3 \"                                           … <NA>   2000\n#>  4 \"\"                                            <NA>   2000\n#>  5 \"\"                                            <NA>   2000\n#>  6 \"United States 1 \"                            2,13…  2000\n#>  7 \"\"                                            <NA>   2000\n#>  8 \"Alabama \"                                    2,02…  2000\n#>  9 \"Alaska \"                                     2,43…  2000\n#> 10 \"Arizona \"                                    2,65…  2000\n#> # … with 47 more rows\ndownload.file(url = summary_tfr_dataset$url[1], destfile = \"year_2000.pdf\")\nlibrary(purrr)\n\nsummary_tfr_dataset <- \n  summary_tfr_dataset |> \n  mutate(pdf_name = paste0(\"dhs/year_\", year, \".pdf\"))\nwalk2(\n  summary_tfr_dataset$url,\n  summary_tfr_dataset$pdf_name,\n  safely( ~ download.file(.x , .y))\n)\ndhs_2000 <- pdftools::pdf_text(\"year_2000.pdf\")\n\ndhs_2000 <-\n  tibble(raw_data = dhs_2000) |>\n  slice(summary_tfr_dataset$page[1]) |>\n  separate_rows(raw_data, sep = \"\\\\n\", convert = FALSE) |>\n  separate(\n    col = raw_data,\n    into = c(\"state\", \"data\"),\n    sep = \"\\\\.{2,}\",\n    remove = FALSE\n  ) |>\n  mutate(data = str_squish(data)) |>\n  separate(\n    col = data,\n    into = c(\n      \"number_of_births\",\n      \"birth_rate\",\n      \"fertility_rate\",\n      \"TFR\",\n      \"teen_births_all\",\n      \"teen_births_15_17\",\n      \"teen_births_18_19\"\n    ),\n    sep = \"\\\\s\",\n    remove = FALSE\n  ) |>\n  select(state, TFR) |>\n  slice(13:69) |>\n  mutate(year = 2000)\n\ndhs_2000\nget_pdf_convert_to_tibble <- function(pdf_name, page, year){\n  \n  dhs_table_of_interest <- \n    tibble(raw_data = pdftools::pdf_text(pdf_name)) |> \n    slice(page) |> \n    separate_rows(raw_data, sep = \"\\\\n\", convert = FALSE) |> \n    separate(col = raw_data, \n             into = c(\"state\", \"data\"), \n             sep = \"[�|\\\\.]\\\\s+(?=[[:digit:]])\", \n             remove = FALSE) |> \n    mutate(\n      data = str_squish(data),\n      year_of_data = year)\n\n  print(paste(\"Done with\", year))\n  \n  return(dhs_table_of_interest)\n}\n\nraw_dhs_data <- purrr::pmap_dfr(summary_tfr_dataset |> select(pdf_name, page, year),\n                                get_pdf_convert_to_tibble)\n#> [1] \"Done with 2000\"\n#> [1] \"Done with 2001\"\n#> [1] \"Done with 2002\"\n#> [1] \"Done with 2003\"\n#> [1] \"Done with 2004\"\n#> [1] \"Done with 2005\"\n#> [1] \"Done with 2006\"\n#> [1] \"Done with 2007\"\n#> [1] \"Done with 2008\"\n#> [1] \"Done with 2009\"\n#> [1] \"Done with 2010\"\n#> [1] \"Done with 2011\"\n#> [1] \"Done with 2012\"\n#> [1] \"Done with 2013\"\n#> [1] \"Done with 2014\"\n#> [1] \"Done with 2015\"\n#> [1] \"Done with 2016\"\n#> [1] \"Done with 2016\"\n#> [1] \"Done with 2017\"\n#> [1] \"Done with 2017\"\n#> [1] \"Done with 2018\"\n\nhead(raw_dhs_data)\n#> # A tibble: 6 × 4\n#>   raw_data                          state data  year_of_data\n#>   <chr>                             <chr> <chr>        <dbl>\n#> 1 \"40 National Vital Statistics Re… \"40 … 50, …         2000\n#> 2 \"\"                                \"\"    <NA>          2000\n#> 3 \"Table 10. Number of births, bir… \"Tab… <NA>          2000\n#> 4 \"United States, each State and t… \"Uni… <NA>          2000\n#> 5 \"[By place of residence. Birth r… \"[By… <NA>          2000\n#> 6 \"estimated in each area; total f… \"est… <NA>          2000\nstates <- c(\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \n            \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \n            \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \n            \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \n            \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \n            \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \n            \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \n            \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \n            \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \n            \"Wyoming\", \"District of Columbia\")\n\nraw_dhs_data <- \n  raw_dhs_data |> \n  mutate(state = str_remove_all(state, \"\\\\.\"),\n         state = str_remove_all(state, \"�\"),\n         state = str_remove_all(state, \"\\u0008\"),\n         state = str_replace_all(state, \"United States 1\", \"United States\"),\n         state = str_replace_all(state, \"United States1\", \"United States\"),\n         state = str_replace_all(state, \"United States 2\", \"United States\"),\n         state = str_replace_all(state, \"United States2\", \"United States\"),\n         state = str_replace_all(state, \"United States²\", \"United States\"),\n         ) |> \n  mutate(state = str_squish(state)) |> \n  filter(state %in% states)\n\nhead(raw_dhs_data)\n#> # A tibble: 6 × 4\n#>   raw_data                          state data  year_of_data\n#>   <chr>                             <chr> <chr>        <dbl>\n#> 1 Alabama ........................… Alab… 63,2…         2000\n#> 2 Alaska .........................… Alas… 9,97…         2000\n#> 3 Arizona ........................… Ariz… 85,2…         2000\n#> 4 Arkansas .......................… Arka… 37,7…         2000\n#> 5 California .....................… Cali… 531,…         2000\n#> 6 Colorado .......................… Colo… 65,4…         2000\nraw_dhs_data <- \n  raw_dhs_data |> \n  mutate(data = str_remove_all(data, \"\\\\*\")) |> \n  separate(data, into = c(\"col_1\", \"col_2\", \"col_3\", \"col_4\", \"col_5\", \n                          \"col_6\", \"col_7\", \"col_8\", \"col_9\", \"col_10\"), \n           sep = \" \",\n           remove = FALSE)\nhead(raw_dhs_data)\n#> # A tibble: 6 × 14\n#>   raw_data   state data  col_1 col_2 col_3 col_4 col_5 col_6\n#>   <chr>      <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n#> 1 Alabama .… Alab… 63,2… 63,2… 14.4  65.0  2,02… 62.9  37.9 \n#> 2 Alaska ..… Alas… 9,97… 9,974 16.0  74.6  2,43… 42.4  23.6 \n#> 3 Arizona .… Ariz… 85,2… 85,2… 17.5  84.4  2,65… 69.1  41.1 \n#> 4 Arkansas … Arka… 37,7… 37,7… 14.7  69.1  2,14… 68.5  36.7 \n#> 5 Californi… Cali… 531,… 531,… 15.8  70.7  2,18… 48.5  28.6 \n#> 6 Colorado … Colo… 65,4… 65,4… 15.8  73.1  2,35… 49.2  28.6 \n#> # … with 5 more variables: col_7 <chr>, col_8 <chr>,\n#> #   col_9 <chr>, col_10 <chr>, year_of_data <dbl>\ntfr_data <- \n  raw_dhs_data |> \n  mutate(TFR = if_else(year_of_data < 2008, col_4, col_3)) |> \n  select(state, year_of_data, TFR) |> \n  rename(year = year_of_data)\nhead(tfr_data)\n#> # A tibble: 6 × 3\n#>   state       year TFR    \n#>   <chr>      <dbl> <chr>  \n#> 1 Alabama     2000 2,021.0\n#> 2 Alaska      2000 2,437.0\n#> 3 Arizona     2000 2,652.5\n#> 4 Arkansas    2000 2,140.0\n#> 5 California  2000 2,186.0\n#> 6 Colorado    2000 2,356.5\nhead(tfr_data)\n#> # A tibble: 6 × 3\n#>   state       year TFR    \n#>   <chr>      <dbl> <chr>  \n#> 1 Alabama     2000 2,021.0\n#> 2 Alaska      2000 2,437.0\n#> 3 Arizona     2000 2,652.5\n#> 4 Arkansas    2000 2,140.0\n#> 5 California  2000 2,186.0\n#> 6 Colorado    2000 2,356.5\n\ntfr_data <- \n  tfr_data |> \n  mutate(TFR = str_remove_all(TFR, \",\"),\n         TFR = as.numeric(TFR))\n\nhead(tfr_data)\n#> # A tibble: 6 × 3\n#>   state       year   TFR\n#>   <chr>      <dbl> <dbl>\n#> 1 Alabama     2000 2021 \n#> 2 Alaska      2000 2437 \n#> 3 Arizona     2000 2652.\n#> 4 Arkansas    2000 2140 \n#> 5 California  2000 2186 \n#> 6 Colorado    2000 2356.\ntfr_data$state %>% unique() %>% length() == 51\n#> [1] TRUE\n\ntfr_data$year %>% unique() %>% length() == 19\n#> [1] TRUE\ntfr_data |>\n  slice(1:10) |>\n  knitr:: kable(\n    caption = \"First ten rows of a dataset of TFR by US state, 2000-2019\",\n    col.names = c(\"State\", \"Year\", \"TFR\"),\n    digits = 0,\n    booktabs = TRUE, \n    linesep = \"\",\n    format.args = list(big.mark = \",\")\n  )"},{"path":"gather-data.html","id":"optical-character-recognition","chapter":"8 Gather data","heading":"8.5.2 Optical Character Recognition","text":"predicated PDF already ‘digitized.’ images? case need first use Optical Character Recognition (OCR) using tesseract (Ooms 2019c). R wrapper around Tesseract open-source OCR engine.Let us see example scan first page Jane Eyre (Figure 8.19).\nFigure 8.19: Scan first page Jane Eyre\n","code":"\nlibrary(tesseract)\n\ntext <- tesseract::ocr(here::here(\"jane_scan.png\"), engine = tesseract(\"eng\"))\ncat(text)"},{"path":"gather-data.html","id":"exercises-and-tutorial-7","chapter":"8 Gather data","heading":"8.6 Exercises and tutorial","text":"","code":""},{"path":"gather-data.html","id":"exercises-7","chapter":"8 Gather data","heading":"8.6.1 Exercises","text":"types probability sampling, circumstances might want implement (write two three pages)?substantial political polling ‘misses’ recent years (Trump Brexit come mind). extent think non-response bias cause (write page two, sure ground writing citations)?seems like lot businesses closed since pandemic. investigate , walk along blocks downtown count number businesses closed open. decide blocks walk, open map, start lake, pick every 10th street. type sampling (select )?\nCluster sampling.\nSystematic sampling.\nStratified sampling.\nSimple random sampling.\nConvenience sampling.\nCluster sampling.Systematic sampling.Stratified sampling.Simple random sampling.Convenience sampling.Please name reasons may wish use cluster sampling (select )?\nBalance responses.\nAdministrative convenience.\nEfficiency terms money.\nUnderlying systematic concerns.\nEstimation sub-populations.\nBalance responses.Administrative convenience.Efficiency terms money.Underlying systematic concerns.Estimation sub-populations.Please consider Beaumont, 2020, ‘probability surveys bound disappear production official statistics?’ reference paper, think probability surveys disappear, (please write paragraph two)?words, API (write paragraph two)?Find two APIs discuss use tell interesting stories (write paragraph two )?Find two APIs R packages written around . use tell interesting stories (write paragraph two)?main argument httr::GET() (pick one)?\n‘url’\n‘website’\n‘domain’\n‘location’\n‘url’‘website’‘domain’‘location’three reasons respectful getting scraping data websites (write paragraph two)?features website typically take advantage parse code (select apply)?\nHTML/CSS mark-.\nCookies.\nFacebook beacons.\nCode comments.\nHTML/CSS mark-.Cookies.Facebook beacons.Code comments.three advantages three disadvantages scraping compared using API (write paragraph two)?three delimiters useful trying bring order PDF read character vector (write paragraph two)?following, used part regular expression, match full stop (hint: see ‘strings’ cheat sheet) (pick one)?\n‘.’\n‘.’\n‘\\.’\n‘\\.’\n‘.’‘.’‘\\.’‘\\.’Name three reasons sketching want starting try extract data PDF (write paragraph two )?three checks might like use demographic data, number births country particular year (write paragraph two check)?three checks might like use economic data, GDP particular country particular year (write paragraph two check)?purrr package (select apply)?\nEnhances R’s functional programming toolkit.\nMakes loops easier code read.\nChecks consistency datasets.\nIdentifies issues data structures proposes replacements.\nEnhances R’s functional programming toolkit.Makes loops easier code read.Checks consistency datasets.Identifies issues data structures proposes replacements.functions purrr package (select apply)?\nmap()\nwalk()\nrun()\nsafely()\nmap()walk()run()safely()principles follow scraping (select apply)?\nAvoid possible\nFollow site’s guidance\nSlow \nUse scalpel axe.\nAvoid possibleFollow site’s guidanceSlow downUse scalpel axe.robots.txt file (pick one)?\ninstructions Frankenstein followed.\nNotes web scrapers follow scraping.\ninstructions Frankenstein followed.Notes web scrapers follow scraping.html tag item list (pick one)?\nli\nbody\nb\nem\nlibodybemWhich function use following text data: ‘rohan_alexander’ column called ‘names’ want split first name surname based underbar (pick one)?\nseparate()\nslice()\nspacing()\ntext_to_columns()\nseparate()slice()spacing()text_to_columns()","code":""},{"path":"gather-data.html","id":"tutorial-7","chapter":"8 Gather data","heading":"8.6.2 Tutorial","text":"Please redo web scraping example, one : Australia, Canada, India, New Zealand.Plan, gather, clean data, use create similar table one created . Write paragraphs findings. write paragraphs data source, gathered, went . took longer expected? become fun? differently next time ? submission least two pages likely .Please submit link PDF produced using R Markdown includes link GitHub repo.","code":""},{"path":"hunt-data.html","id":"hunt-data","chapter":"9 Hunt data","heading":"9 Hunt data","text":"Required materialRead Big tech testing , (Fry 2020).Read Inventing randomized double-blind trial: Nuremberg salt test 1835, (Stolberg 2006).Read Impact evaluation practice, Chapters 3 4, (Gertler et al. 2016).Read Statistics causal inference, Parts 1-3, (Holland 1986).Key concepts/skills/etcTreatment control groups.Internal external validity.Average treatment effect.Generating simulated datasets.Informed consent establishing need experiment./B testingPutting together surveys Google FormsKey librariestidyverse (Wickham et al. 2019a)Key functions/etccount()filter()ggplot()group_by()head()()if_else()left_join()length()mean()mutate()names()nrow()pivot_wider()read_csv()ref()rename()rnorm()rowwise()sample()scale_fill_brewer()seed()select()str_detect()sum()summarize()test()theme_classic()theme_minimal()tibble()ungroup()","code":""},{"path":"hunt-data.html","id":"experiments-and-randomized-controlled-trials","chapter":"9 Hunt data","heading":"9.1 Experiments and randomized controlled trials","text":"","code":""},{"path":"hunt-data.html","id":"introduction-6","chapter":"9 Hunt data","heading":"9.1.1 Introduction","text":"Ronald Fisher, twentieth century statistician, Francis Galton, nineteenth century statistician, intellectual grandfathers much work cover chapter. cases directly work, cases work built contributions. men believed eugenics, amongst things generally reprehensible.\nway art history must acknowledge, say Caravaggio murderer, also considering work influence, must statistics data sciences generally concern past, time try build better future.chapter experiments. situation can explicitly control vary interested . advantage identification clear. treatment group subject interested , control group . randomly split treatment. , end different must treatment. Unfortunately, life rarely smooth. Arguing similar treatment control groups tends carry indefinitely. ability speak whether measured effect treatment, affects ability speak effect treatment .chapter cover experiments, especially constructing treatment control groups, appropriately considering results. discuss aspects ethical behaviour experiments reference abhorrent Tuskegee Syphilis Study ECMO. go Oregon Health Insurance Experiment case study. turn /B testing, extensively used industry, consider case study based Upworthy data. Finally, go actually implementing survey using Google Forms.\n","code":""},{"path":"hunt-data.html","id":"motivation-and-notation","chapter":"9 Hunt data","heading":"9.1.2 Motivation and notation","text":"Professional sports big deal North America. Consider situation someone moves San Francisco 2014, soon moved Giants win World Series Golden State Warriors begin historic streak World Championships. move Chicago, immediately Cubs win World Series first time hundred years. move Massachusetts, Patriots win Super Bowl , , . finally, move Toronto, Raptors immediately win World Championship. city pay move, municipal funds better spent elsewhere?One way get answer run experiment. Make list North American cities major sports teams, roll dice send live year. enough lifetimes, work . fundamental issue live city live city. fundamental problem causal inference: person treated untreated. Experiments randomized controlled trials circumstances try randomly allocate treatment, belief everything else (least ignorable). framework use formalize situation Neyman-Rubin model (Holland 1986).treatment, \\(t\\), often binary variable either 0 1. 0 person, \\(\\), treated, say control group, 1 treated. typically outcome, \\(Y_i\\), interest person, binary, multinomial, continuous. instance, vote choice, case measure whether person : ‘Conservative’ ‘Conservative’; party support, say: ‘Conservative,’ ‘Liberal,’ ‘Democratic,’ ‘Green’; maybe probability support.treatment causal \\((Y_i|t=0) \\neq (Y_i|t=1)\\). say, outcome person \\(\\), given treated, different outcome given treated. treat control one individual one time, know treatment caused change outcome, factor explain . fundamental problem causal inference treat control one individual one time. want know effect treatment, need compare counterfactual. counterfactual happened individual treated. turns , means one way think causal inference missing data problem, missing counterfactual.compared treatment control one individual, instead compare average two groups—treated . looking estimate counterfactual group level impossibility individual level. Making trade-allows us move forward comes cost certainty. must instead rely randomization, probabilities, expectations.usually consider default effect look evidence cause us change mind. interested happening groups, turn expectations, notions probability express . Hence, make claims talk, average. Maybe wearing fun socks really make lucky day, average, across group, probably case.worth pointing just interested average effect. may consider median, variance, whatever. Nonetheless, interested average effect, one way proceed :divide dataset two—treated treated—binary effect column;sum column, divide length column; andthen look ratio.estimator, touched Chapter 5, way putting together guess something interest. estimand thing interest, case average effect, estimate whatever guess turns . can simulate data illustrate situation.broadly, tell causal stories need bring together theory detailed knowledge interested (Cunningham 2021, 4). Chapter 8 discussed gathering data observed world. chapter going active turning world data need. researcher decide measure , need define interested . active participants data generating process. , want use data, researchers must go hunt , like.","code":"\nlibrary(tidyverse)\n\nset.seed(853)\n\ntreatment_control <- \n  tibble(\n    binary_effect = sample(x = c(0, 1), size = 10, replace = TRUE)\n  )\n\ntreatment_control\n#> # A tibble: 10 × 1\n#>    binary_effect\n#>            <dbl>\n#>  1             0\n#>  2             1\n#>  3             1\n#>  4             0\n#>  5             0\n#>  6             0\n#>  7             0\n#>  8             0\n#>  9             1\n#> 10             1\nestimate <-\n  sum(treatment_control$binary_effect) / length(treatment_control$binary_effect)\nestimate\n#> [1] 0.4"},{"path":"hunt-data.html","id":"randomization","chapter":"9 Hunt data","heading":"9.1.3 Randomization","text":"Correlation can enough settings, order able make forecasts things change, circumstances slightly different need understand causation. key counterfactual: happened absence treatment. Ideally, keep everything else constant, randomly divide world two groups, treat one . can pretty confident difference two groups due treatment. reason population randomly select two groups , two groups (provided big enough) characteristics population. Randomized controlled trials (RCTs) /B testing attempt get us close ‘gold standard’ can hope. , others Athey Imbens (2017), use language like gold standard refer approaches, mean imply perfect. Just can better options.hope able find treatment control groups , treatment. means establishing control group critical , establish counterfactual. might worried , say, underlying trends, one issue --comparison, selection bias, occur allow self-selection. Either issues result biased estimators. use randomization go way addressing .get started, simulate population, randomly sample . set half population likes blue, half likes white. , someone likes blue almost surely prefer dogs, like white almost surely prefer cats. approach heavily using simulation critical part workflow advocated book. know roughly outcomes analysis simulated data. Whereas go straight analyzing real data know unexpected outcomes due analysis errors, actual results. Another good reason useful take approach simulation working teams analysis can get started data collection cleaning completed. simulation also help collection cleaning team think tests run data.now construct frame, assuming frame contains 80 per cent population.now, set aside dog cat preferences focus creating treatment control groups basis favorite color .look mean two groups, can see proportions prefer blue white similar specified (Table 9.1).Table 9.1: Proportion treatment control group prefer blue whiteWe randomized based favorite color. also find took dog cat preferences along time ‘representative’ share people prefer dogs cats. happen randomized variables? Let’s start looking dataset (Table 9.2).Table 9.2: Proportion treatment control group prefer dogs catsIt exciting representative share ‘unobservables.’ case, ‘observe’ —illustrate point—select . get variables correlated. break several ways discuss. also assumes large enough groups. instance, considered specific dog breeds, instead dogs entity, may find situation. check two groups look see can identify difference two groups based observables. case looked mean, look aspects well.brings us Analysis Variation (ANOVA). ANOVA introduced Fisher working statistical problems agriculture. less unexpected may seem historically agricultural research closely tied statistical innovation. mention ANOVA importance historically, variant linear regression cover detail Chapter 15. , general, usually use ANOVA day--day. nothing wrong right circumstances. hundred years old number modern use-case best option small.case, approach ANOVA expectation groups distribution conduct using aov(). case, fail reject default hypothesis samples .","code":"\nset.seed(853)\n\nnumber_of_people <- 5000\n\npopulation <-\n  tibble(\n    person = c(1:number_of_people),\n    favorite_color = sample(\n      x = c(\"Blue\", \"White\"),\n      size  = number_of_people,\n      replace = TRUE\n    )\n  ) |>\n  mutate(\n    prefers_dogs_to_cats =\n      if_else(favorite_color == \"Blue\", \"Yes\", \"No\"),\n    noise = sample(1:10, size = 1),\n    prefers_dogs_to_cats =\n      if_else(\n        noise <= 8,\n        prefers_dogs_to_cats,\n        sample(c(\"Yes\", \"No\"), \n               size = 1)\n        )\n  ) |> \n  select(-noise)\n\n\npopulation\n#> # A tibble: 5,000 × 3\n#>    person favorite_color prefers_dogs_to_cats\n#>     <int> <chr>          <chr>               \n#>  1      1 Blue           Yes                 \n#>  2      2 White          No                  \n#>  3      3 White          No                  \n#>  4      4 Blue           Yes                 \n#>  5      5 Blue           Yes                 \n#>  6      6 Blue           Yes                 \n#>  7      7 Blue           Yes                 \n#>  8      8 Blue           Yes                 \n#>  9      9 White          No                  \n#> 10     10 White          No                  \n#> # … with 4,990 more rows\n\npopulation |>\n  group_by(favorite_color) |> \n  count()\n#> # A tibble: 2 × 2\n#> # Groups:   favorite_color [2]\n#>   favorite_color     n\n#>   <chr>          <int>\n#> 1 Blue            2547\n#> 2 White           2453\nset.seed(853)\n\nframe <-\n  population |>\n  mutate(\n    in_frame = sample(\n      x = c(0, 1),\n      size  = number_of_people,\n      replace = TRUE,\n      prob = c(0.2, 0.8)\n  )) |>\n  filter(in_frame == 1)\n\nframe |>\n  group_by(favorite_color) |> \n  count()\n#> # A tibble: 2 × 2\n#> # Groups:   favorite_color [2]\n#>   favorite_color     n\n#>   <chr>          <int>\n#> 1 Blue            2023\n#> 2 White           1980\nset.seed(853)\n\nsample <-\n  frame |>\n  select(-prefers_dogs_to_cats) |>\n  mutate(group = sample(\n    x = c(\"Treatment\", \"Control\"),\n    size  = nrow(frame),\n    replace = TRUE\n  ))\nsample |>\n  group_by(group, favorite_color) |>\n  count() |>\n  ungroup() |>\n  group_by(group) |>\n  mutate(prop = n / sum(n)) |>\n  knitr::kable(\n    caption = \"Proportion of the treatment and control group that prefer blue or white\",\n    col.names = c(\"Group\", \"Preferred color\", \"Number\", \"Proportion\"),\n    digits = 2,\n    booktabs = TRUE,\n    linesep = \"\"\n  )\nsample |> \n  left_join(frame |> select(person, prefers_dogs_to_cats), \n            by = \"person\") |>\n  group_by(group, prefers_dogs_to_cats) |> \n  count() |>\n  ungroup() |>\n  group_by(group) |>\n  mutate(prop = n / sum(n)) |>\n  knitr::kable(\n    caption = \"Proportion of the treatment and control group that prefer dogs or cats\",\n    col.names = c(\"Group\", \"Prefers dogs to cats\", \"Number\", \"Proportion\"),\n    digits = 2,\n    booktabs = TRUE,\n    linesep = \"\"\n  )"},{"path":"hunt-data.html","id":"treatment-and-control","chapter":"9 Hunt data","heading":"9.1.4 Treatment and control","text":"treated control groups ways remain way, treatment, internal validity, say control work counterfactual results can speak difference groups study. Internal validity means estimates effect treatment speaking treatment aspect. mean can use results make claims happened experiment.group applied randomization representative broader population, experimental set-fairly similar outside conditions, external validity. mean difference find just apply experiment, also broader population. External validity means can use experiment make claims happen outside experiment. randomization allowed happen.means need randomization twice. Firstly, group subject experiment, secondly, treatment control. think randomization, extent matter?interested effect treated. may charge different prices, continuous treatment variable, compare different colors website, discrete treatment variable. Either way, need make sure groups otherwise . can convinced ? One way ignore treatment variable examine variables, looking whether can detect difference groups based variables. instance, conducting experiment website, groups roughly similar terms , say:Microsoft Apple users?Safari, Chrome, Firefox users?Mobile desktop users?Users certain locations?, groups representative broader population? threats validity claims.done properly, treatment truly independent, can estimate average treatment effect (ATE). binary treatment variable setting :\\[\\mbox{ATE} = \\mathbb{E}[Y|t=1] - \\mathbb{E}[Y|t=0].\\], difference treated group, \\(t = 1\\), control group, \\(t = 0\\), measured expected value outcome, \\(Y\\). ATE becomes difference two expectations.illustrate concept, first simulate data shows difference one treatment control groups.can see difference, simulated one, two groups Figure (Figure 9.1). can compute average groups difference see also get back result put (Table 9.3).\nFigure 9.1: Simulated data showing difference treatment control groups\nTable 9.3: Average difference treatment control groups data simulated average difference oneUnfortunately, often difference simulated data reality. instance, experiment run long otherwise people may treated many times, become inured treatment; short otherwise measure longer term outcomes. ‘representative’ sample across every facet population, , treatment control different. Practical difficulties may make difficult follow certain groups end biased collection. questions explore working real experimental data include:participants selected frame consideration?selected treatment? hope done randomly, term applied variety situations. Additionally, early ‘success’ can lead pressure treat everyone, especially medical settings.treatment assessed?extent random allocation ethical fair? argue shortages mean reasonable randomly allocate, may depend linear benefits . may also difficult establish definitions, power imbalance making decisions treated considered.Bias issues end world. need think carefully. well-known example, Abraham Wald, twentieth century Hungarian mathematician, given data planes came back Britain shot WW2. question place armor. One option place bullet holes. Wald recognized selection effect —planes made back examined—holes necessarily need armor. Arguably armor better placed bullet holes.instance, results survey difficulty university course differ students completed course surveyed, dropped ? work try make dataset good possible, may possible use model control bias. instance, variable correlated say, attrition, added model either -, interaction. Similarly, correlation individuals. instance, ‘hidden variable’ know meant individuals correlated, use ‘wider’ standard errors. needs done carefully discuss Chapter 16. said, issues can anticipated, can better change experiment. instance, perhaps possibly stratify hidden variable.","code":"\nset.seed(853)\n\nate_example <- tibble(person = c(1:1000),\n                      was_treated = sample(\n                        x = c(\"Yes\", \"No\"),\n                        size  = 1000,\n                        replace = TRUE\n                      ))\n\n# Make outcome a bit more likely if treated.\nate_example <-\n  ate_example |>\n  rowwise() |>\n  mutate(outcome = if_else(\n    was_treated == \"No\",\n    rnorm(n = 1, mean = 5, sd = 1),\n    rnorm(n = 1, mean = 6, sd = 1)\n  ))\nate_example |>\n  ggplot(aes(x = outcome,\n             fill = was_treated)) +\n  geom_histogram(position = \"dodge\",\n                 binwidth = 0.2) +\n  theme_minimal() +\n  labs(x = \"Outcome\",\n       y = \"Number of people\",\n       fill = \"Person was treated\") +\n  scale_fill_brewer(palette = \"Set1\")\nate_example |>\n  group_by(was_treated) |>\n  summarize(mean = mean(outcome)) |>\n  pivot_wider(names_from = was_treated, values_from = mean) |>\n  mutate(difference = Yes - No) |>\n  knitr::kable(\n    caption = \"Average difference between the treatment and control groups for data simulated to have an average difference of one\",\n    col.names = c(\"Average for treated\", \"Average for not treated\", \"Difference\"),\n    digits = 2,\n    booktabs = TRUE,\n    linesep = \"\"\n  )"},{"path":"hunt-data.html","id":"fishers-tea-party","chapter":"9 Hunt data","heading":"9.1.5 Fisher’s tea party","text":"Fisher introduced experiment designed see person can distinguish cup tea milk added first, last. begin preparing eight cups tea: four milk added first four milk added last. randomize order eight cups. tell taster, call ‘Ian,’ experimental set-: eight cups tea, four type, given cups tea random order, task group two groups.One nice aspects experiment can . things careful practice, including : quantities milk tea consistent; groups marked way taster see; order randomized.Another nice aspect experiment can calculate chance Ian able randomly get groupings correct. decide groupings likely occurred random, need calculate probability happen. First, count number successes four chosen. Fisher (1935, 14) says : \\({8 \\choose 4} = \\frac{8!}{4!(8-4)!}=70\\) possible outcomes.asking Ian group cups, identify , two ways perfectly correct. either correctly identify ones milk-first (one outcome 70) correctly identify ones tea-first (one outcome 70). means probability event : \\(\\frac{2}{70} \\approx 0.028\\) 3 per cent.Fisher (1935, 15) makes clear, now becomes judgement call. need consider weight evidence require accept groupings occur chance Ian well-aware . need decide evidence takes us convinced. possible evidence dissuade us view held coming experiment, say, difference milk-first tea-first, point experiment? expect Ian got completely right, accept able tell difference.almost perfect? chance, 16 ways person ‘--one.’ Either Ian thinks one cup milk-first tea-first—, \\({4 \\choose 1} = 4\\), four ways happen—thinks one cup tea-first milk-first—, , \\({4 \\choose 1}\\) = 4, four ways happen. outcomes independent, probability \\(\\frac{4\\times 4}{70} \\approx 0.228\\). . Given almost 23 per cent chance --one just randomly grouping teacups, outcome probably convince us Ian can tell difference tea-first milk-first.looking , order claim something experimentally demonstrable results just shown , instead know features experiment result reliably found (Fisher 1935, 16). looking thoroughly interrogate data experiments, think precisely analysis methods using. Rather searching meaning constellations stars, want make easy possible others reproduce work. way conclusions stand chance holding long-term.","code":""},{"path":"hunt-data.html","id":"informed-consent-and-the-need-for-an-experiment","chapter":"9 Hunt data","heading":"9.1.6 Informed consent and the need for an experiment","text":"One foundations ethical experimental practice informed consent ensuring experiment actually needed. now detail two cases human life potentially lost due issues. One issue experiments medical settings weight evidence measured lost lives. Ethical practice experiments develops many people may unnecessarily lost life due experiments. Two cases dramatically informed practice Tuskegee Syphilis Study ECMO.Tuskegee Syphilis Study infamous medical trial began 1932. part experiment 400 Black Americans syphilis, control group without, given appropriate treatment, even told syphilis (case treatment group), well standard treatment syphilis established widely available sometime mid-1940s early 1950s (Brandt 1978; Alsan Wanamaker 2018). Like treatment group, control group also given non-effective drugs. financially-poor Black Americans US South identified offered compensation including ‘hot meals, guise treatment, burial payments’ (Alsan Wanamaker 2018). men actually treated syphilis (Brandt 1978; Alsan Wanamaker 2018). men told part experiment (Brandt 1978; Alsan Wanamaker 2018). , extensive work undertaken ensure men receive treatment anywhere including writing local doctors, local health department, , incredibly, men drafted told immediately get treatment, draft board complied request men excluded treatment (Brandt 1978, 25). time study stopped 1972, half men deceased many deaths syphilis-related causes (Alsan Wanamaker 2018).effect Tuskegee Syphilis Study felt just men study, broadly. Alsan Wanamaker (2018) found associated decrease life expectancy age 45 1.5 years Black men. response US established requirements Institutional Review Boards President Clinton made formal apology 1997. Brandt (1978, 27) says:retrospect Tuskegee Study revealed pathology racism pathology syphilis; nature scientific inquiry nature disease process… [T]notion science value-free discipline must rejected. need greater vigilance assessing specific ways social values attitudes affect professional behavior clearly indicated.Turning evaluation extracorporeal membrane oxygenation (ECMO), J. H. Ware (1989) describes viewed ECMO possible treatment persistent pulmonary hypertension newborns (PPHN). enrolled 19 patients used conventional medical therapy ten , ECMO nine . found six ten control group survived treatment group survived. J. H. Ware (1989) used randomized consent whereby parents infants randomly selected treated ECMO asked consent.J. H. Ware (1989) concerned ‘equipoise,’ refer situation genuine uncertainty whether treatment effective existing procedures. note medical settings even initial equipoise undermined treatment found effective early study. J. H. Ware (1989) describe results first 19 patients, randomization stopped ECMO used. recruiters treating patients initially told randomization stopped. decided complete allocation ECMO continue ‘either 28th survivor 4th death observed.’ 19 20 additional patients survived ECMO trial terminated. , actual result experiment divided two phases: first randomized use ECMO, second ECMO used.One approach settings ‘randomized play--winner’ rule following Wei Durham (1978). Treatment still randomized, weight probability shifts successful treatment make treatment likely stopping rule. Berry (1989) argues stopping rule case J. H. Ware (1989) occurred study started need study equipoise never existed. Berry (1989) re-visit literature mentioned J. H. Ware (1989) find extensive evidence ECMO known effective. Berry (1989) points almost never complete consensus one almost always argue existence equipoise even face substantial weight evidence. Berry (1989) criticizes J. H. Ware (1989) use randomized consent potential may different outcomes infants subject conventional therapy parents known options. instead, Berry (1989) argues need comprehensive patient registries, enabling analysis large datasets.Tuskegee Syphilis Study ECMO may seem quite far present circumstances, Monica Alexander, Assistant Professor, University Toronto explains may illegal exact research days, mean unethical research still happen. see time machine learning applications health areas. meant explicitly discriminate meant get consent, mean implicitly discriminate without type buy-. instance, Obermeyer et al. (2019) describes US health care systems use algorithms score severity sick patient . show score, ‘Black patients considerably sicker White patients, evidenced signs uncontrolled illnesses’ Black patients scored way White patients, receive considerably help now. find discrimination occurs algorithm based health care costs, rather sickness. access healthcare unequally distributed Black White patients, algorithm, however inadvertently, perpetuates racial bias.","code":""},{"path":"hunt-data.html","id":"case-study-the-oregon-health-insurance-experiment","chapter":"9 Hunt data","heading":"9.1.7 Case study: The Oregon Health Insurance Experiment","text":"US, unlike many developed countries, basic health insurance necessarily available residents even low incomes. Oregon Health Insurance Experiment involved low-income adults Oregon, state north-west US, 2008 2010 (Finkelstein et al. 2012).Oregon funded 10,000 places state-run Medicaid program, provides health insurance people low incomes. lottery used allocate places judged fair expected, correctly turned , demand places exceed supply. People month sign enter draw. lottery used determine 89,824 individuals signed allowed apply Medicaid.draws conducted six-month period selected opportunity sign . 35,169 individuals selected (household actually won draw given opportunity) 30 per cent completed paperwork eligible (typically earned much). insurance lasted indefinitely. random allocation insurance allowed researchers understand effect health insurance.reason random allocation important usually possible compare without insurance type people sign get health insurance differ . decision ‘confounded’ variables results selection effect.opportunity apply health insurance randomly allocated, researchers able evaluate health earnings received health insurance compare . used administrative data, hospital discharge data, credit reports matched 68.5 per cent lottery participants, mortality records, uncommon. Interestingly collection data fairly restrained included survey conducted via mail.specifics important, say Chapter 15, use statistical model analyze results (Finkelstein et al. 2012):\\[\\begin{equation}\ny_{ihj} = \\beta_0 + \\beta_1\\mbox{Lottery} + X_{ih}\\beta+2 + V_{ih}\\beta_3 + \\epsilon_{ihj} \\tag{9.1}\n\\end{equation}\\]Equation (9.1) explains various \\(j\\) outcomes (health) individual \\(\\) household \\(h\\) function indicator variable whether household \\(h\\) selected lottery. Hence, ‘(t)coefficient Lottery, \\(\\beta_1\\), main coefficient interest, gives average difference (adjusted) means treatment group (lottery winners) control group (selected lottery).’complete specification Equation (9.1), \\(X_{ih}\\) set variables correlated probability treated. adjust impact certain extent. example number individuals household. finally, \\(V_{ih}\\) set variables correlated lottery. variables include demographics, hospital discharge lottery draw.found earlier studies Brook et al. (1984), Finkelstein et al. (2012) found , treatment group 25 per cent likely insurance control group. treatment group used health care including primary preventive care well hospitalizations lower --pocket medical expenditures. generally, treatment group reported better physical mental health.","code":""},{"path":"hunt-data.html","id":"ab-testing","chapter":"9 Hunt data","heading":"9.2 A/B testing","text":"past decade probably seen experiments ever run several orders magnitude extensive use /B testing websites. Large tech companies typically extensive infrastructure experiments, term /B tests comparison two groups: one gets treatment either gets treatment B see change (M. Salganik 2018, 185). Every time online probably subject tens, hundreds, potentially thousands, different /B tests. use apps like TikTok run tens thousands. , heart, still just surveys result data need analysed, several interesting features discuss.instance, Kohavi, Tang, Xu (2020, 3) discusses example Microsoft’s search engine Bing increased amount content displayed ads. change triggered alert usually signaled bug billing. bug, instead case revenue increase 12 per cent, around $100 million annually US, without significant trade-measured.use term /B test strictly refer situation primarily implementing experiment technology stack something primarily internet, instance change website similar. heart just experiments, /B testing range specific concerns. something different tens thousands small experiments time, compared normal experimental set-conducting one experiment course months. Additionally, tech firms distinct cultures can difficult shift toward experimental set-. Sometimes can easier experiment delivering, delaying, change decided create control group rather treatment group (M. Salganik 2018, 188). Often difficult aspect /B testing conducting experiments generally, statistics, ’s politics.first aspect concern delivery /B test (Kohavi, Tang, Xu 2020, 153–61). case experiment, usually clear delivered. instance, may person come doctor’s clinic inject either drug placebo. case /B testing, less obvious. instance, run ‘server-side,’ meaning make change website, ‘client-side,’ meaning change app. decision affects ability conduct experiment gather data .case effect conducting experiment, relatively easy normal update website time. means small changes can easily implemented experiment conducted server-side. case client-side implementation app, conducting experiment becomes bigger deal. instance, release may need go app store, usually happen time. Instead, need part regular release cycle. also selection concern users update app possibility different regularly update app.Turning effect delivery decision ability gather data experiment. , server-side less big deal get data anyway part user interacting website. case app, user may use app offline limited data upload, requires data transmission protocol caching, affect user experience, especially phones place limits various aspects.effect need plan. instance, results unlikely available day change app, whereas likely available day change website. , may need consider results context different devices platforms, potentially using, say, multilevel regression covered Chapter 15.second aspect concern ‘instrumentation’ method measurement (Kohavi, Tang, Xu 2020, 162 - 165). conduct traditional experiment might, instance, ask respondents fill survey. usually done /B testing. One approach put cookie user’s device, different users clear different rates. Another approach use beacon, forcing user download tiny image server, know completed action. instance, commonly used approach know user opened email. practical concerns around beacon loads, instance, main content loads user experience may worse, sample may biased.third aspect concern randomizing (Kohavi, Tang, Xu 2020, 162 - 165). case traditional experiments, usually clear often person, sometimes various groups people. case /B testing can less clear. instance, randomizing page, session, user?think , let us consider color. instance, say interested whether change logo red blue homepage. randomizing page level, user goes page website, back homepage logo back red. randomizing session level, blue use website time, close come back red. Finally, randomizing user level possibly always red one used, always blue another.extent matters depends trade-consistency importance. instance, /B testing product prices consistency likely feature. /B testing background colors consistency might important. hand, /B testing position log-button might important move around much one user, users might matter less.Interestingly, /B testing, traditional experiments, concerned treatment control groups , treatment. case traditional experiments, satisfy making conducting analysis basis data experiment conducted. usually can weird treat control groups. case /B testing, pace experimentation allows us randomly create treatment control groups, check, subject treatment group treatment, groups . instance, show group website, expect outcomes across two groups. found different outcomes know may randomization issue (Taddy 2019, 129).One interesting aspects /B testing usually running desperately care specific outcome, feeds measure care . instance, care whether website quite-dark-blue slightly-dark-blue? Probably , probably care lot company share price. picking best blue comes cost share price? example bit contrived, let us pretend work food delivery app concerned driver retention. Say /B tests find drivers always likely retained can deliver food customer faster. finding faster better, driver retention, always. one way achieve faster deliveries, put food hot box maintain food’s temperature. Something like might save 30 seconds, significant 10-15 minute delivery. Unfortunately, although decide encourage basis /B tests designed optimize driver-retention, decision likely make customer experience worse. customers receive cold food, meant hot, may stop using app, ultimately bad business.trade-may obvious run driver experiment look customer complaints. possible small team exposed tickets, larger team may . Ensuring /B tests resulting false optimization especially important. something typically worry normal experiments.","code":""},{"path":"hunt-data.html","id":"case-study-upworthy","chapter":"9 Hunt data","heading":"9.2.1 Case study: Upworthy","text":"trouble much /B testing done firms typically datasets can use. Matias et al. (2019) provide access dataset /B tests Upworthy, clickbait media website used /B testing optimize content. Fitts (2014) provides background information Upworthy. datasets /B tests available: https://osf.io/jd64p/.can look dataset looks like, get sense looking names extract.also useful look documentation dataset. describes structure dataset, packages within tests. package collection headlines images shown randomly different visitors website, part test. test can include many packages. row dataset package test part specified ‘clickability_test_id’ column.variety variables. focus :‘created_at,’‘clickability_test_id’ can create comparison groups,‘headline,’‘impressions’ number people saw package, ‘clicks’ number clicked package.Within batch tests, interested effect varied headlines impressions clicks.focus text contained headlines, look whether headlines asked question got clicks . want remove effect different images focus tests image. identify whether headline asks question, search question mark. Although complicated constructions use, enough get started.every test, every picture, want know whether asking question affected number clicks.find general, question headline may slightly decrease number clicks headline, although effect appear large (Figure 9.2).\nFigure 9.2: Comparison average number clicks headline contains question mark \n","code":"\nupworthy <- read_csv(\"https://osf.io/vy8mj/download\")\nupworthy |> \n  names()\n#>  [1] \"...1\"                 \"created_at\"          \n#>  [3] \"updated_at\"           \"clickability_test_id\"\n#>  [5] \"excerpt\"              \"headline\"            \n#>  [7] \"lede\"                 \"slug\"                \n#>  [9] \"eyecatcher_id\"        \"impressions\"         \n#> [11] \"clicks\"               \"significance\"        \n#> [13] \"first_place\"          \"winner\"              \n#> [15] \"share_text\"           \"square\"              \n#> [17] \"test_week\"\n\nupworthy |> \n  head()\n#> # A tibble: 6 × 17\n#>    ...1 created_at          updated_at         \n#>   <dbl> <dttm>              <dttm>             \n#> 1     0 2014-11-20 06:43:16 2016-04-02 16:33:38\n#> 2     1 2014-11-20 06:43:44 2016-04-02 16:25:54\n#> 3     2 2014-11-20 06:44:59 2016-04-02 16:25:54\n#> 4     3 2014-11-20 06:54:36 2016-04-02 16:25:54\n#> 5     4 2014-11-20 06:54:57 2016-04-02 16:31:45\n#> 6     5 2014-11-20 06:55:07 2016-04-02 16:25:54\n#> # … with 14 more variables: clickability_test_id <chr>,\n#> #   excerpt <chr>, headline <chr>, lede <chr>, slug <chr>,\n#> #   eyecatcher_id <chr>, impressions <dbl>, clicks <dbl>,\n#> #   significance <dbl>, first_place <lgl>, winner <lgl>,\n#> #   share_text <chr>, square <chr>, test_week <dbl>\nupworthy_restricted <- \n  upworthy |> \n  select(created_at, clickability_test_id, headline, impressions, clicks)\n\nhead(upworthy_restricted)\n#> # A tibble: 6 × 5\n#>   created_at          clickability_tes… headline impressions\n#>   <dttm>              <chr>             <chr>          <dbl>\n#> 1 2014-11-20 06:43:16 546d88fb84ad38b2… They're…        3052\n#> 2 2014-11-20 06:43:44 546d88fb84ad38b2… They're…        3033\n#> 3 2014-11-20 06:44:59 546d88fb84ad38b2… They're…        3092\n#> 4 2014-11-20 06:54:36 546d902c26714c6c… This Is…        3526\n#> 5 2014-11-20 06:54:57 546d902c26714c6c… This Is…        3506\n#> 6 2014-11-20 06:55:07 546d902c26714c6c… This Is…        3380\n#> # … with 1 more variable: clicks <dbl>\nupworthy_restricted <-\n  upworthy_restricted |>\n  mutate(asks_question = str_detect(string = headline, pattern = \"\\\\?\"))\n\nupworthy_restricted |> \n  count(asks_question)\n#> # A tibble: 2 × 2\n#>   asks_question     n\n#>   <lgl>         <int>\n#> 1 FALSE         19130\n#> 2 TRUE           3536\nto_question_or_not_to_question <- \n  upworthy_restricted |> \n  group_by(clickability_test_id, asks_question) |> \n  summarize(ave_clicks = mean(clicks)) |> \n  ungroup()\n#> `summarise()` has grouped output by 'clickability_test_id'.\n#> You can override using the `.groups` argument.\n\nlook_at_differences <- \n  to_question_or_not_to_question |> \n  pivot_wider(id_cols = clickability_test_id,\n              names_from = asks_question,\n              values_from = ave_clicks) |> \n  rename(ave_clicks_not_question = `FALSE`,\n         ave_clicks_is_question = `TRUE`) |> \n  filter(!is.na(ave_clicks_not_question)) |>\n  filter(!is.na(ave_clicks_is_question)) |> \n  mutate(difference_in_clicks = ave_clicks_is_question - ave_clicks_not_question)\n\nlook_at_differences$difference_in_clicks |> mean()\n#> [1] -4.890435"},{"path":"hunt-data.html","id":"implementing-surveys","chapter":"9 Hunt data","heading":"9.3 Implementing surveys","text":"many ways implement surveys. instance, dedicated survey platforms Survey Monkey Qualtrics. general, focus platforms putting together survey form expect already contact details sample interest. platforms, Mechanical Turk Prolific, focus providing audience, can ask audience just take survey. useful, usually comes higher costs. Finally, platforms Facebook also provide ability run survey. One especially common approach, free, use Google Forms.create survey Google Forms, sign Google Account, go Google Drive, click ‘New’ ‘Google Form.’ default, form largely empty (Figure 9.3), add title description.\nFigure 9.3: default view new Google Form created contains many empty fields\ndefault, multiple-choice question included, can update content clicking question field. Helpfully, often suggestions can help provide options. can make question required toggling (Figure 9.4).\nFigure 9.4: Updating multiple-choice question included default\ncan add another question, clicking plus circle around , select different types question, instance, ‘Short answer,’ ‘Checkboxes,’ ‘Linear scale’ (Figure 9.5). can especially useful use ‘Short answer’ aspect name email address, checkboxes linear scale understand preferences.\nFigure 9.5: Different options questions include short answer, checkboxes, linear scale\nhappy survey, make like preview , clicking icon looks like eye. checking way, can click ‘Send.’ Usually especially useful use second option, send via link, can handy shorten URL (Figure 9.6).\nFigure 9.6: variety ways share survey, one helpful one get link short URL\nshare survey, results accrue ‘Responses’ tab can especially useful create spreadsheet view responses, clicking ‘Sheets’ icon. collected enough responses can turn ‘Accepting responds’ (Figure 9.7)\nFigure 9.7: Responses show alongside survey can helpful add separate spreadsheet\n","code":""},{"path":"hunt-data.html","id":"exercises-and-tutorial-8","chapter":"9 Hunt data","heading":"9.4 Exercises and tutorial","text":"","code":""},{"path":"hunt-data.html","id":"exercises-8","chapter":"9 Hunt data","heading":"9.4.1 Exercises","text":"words, role randomization constructing counterfactual (write two three paragraphs)?external validity (pick one)?\nFindings experiment hold setting.\nFindings experiment hold outside setting.\nFindings experiment repeated many times.\nFindings experiment code data available.\nFindings experiment hold setting.Findings experiment hold outside setting.Findings experiment repeated many times.Findings experiment code data available.internal validity (pick one)?\nFindings experiment hold setting.\nFindings experiment hold outside setting.\nFindings experiment repeated many times.\nFindings experiment code data available.\nFindings experiment hold setting.Findings experiment hold outside setting.Findings experiment repeated many times.Findings experiment code data available.dataset named ‘netflix_data,’ columns ‘person’ ‘tv_show’ ‘hours,’ (person character class uniqueID every person, tv_show character class name tv show, hours double expressing number hours person watched tv show). please write code randomly assign people one two groups? data looks like :context randomization, stratification mean (write paragraph two)?check randomization done appropriately (write two three paragraphs)?Identify three companies conduct /B testing commercially write one paragraph work trade-offs involved.Pretend work junior analyst large consulting firm. , pretend consulting firm taken contract put together facial recognition model Canada Border Services Agency’s Inland Enforcement branch. Taking page two, please discuss thoughts matter. ?estimate (pick one)?\nrule calculating estimate given quantity based observed data.\nquantity interest.\nresult.\nUnknown numbers determine statistical model.\nrule calculating estimate given quantity based observed data.quantity interest.result.Unknown numbers determine statistical model.estimator (pick one)?\nrule calculating estimate given quantity based observed data.\nquantity interest.\nresult.\nUnknown numbers determine statistical model.\nrule calculating estimate given quantity based observed data.quantity interest.result.Unknown numbers determine statistical model.estimand (pick one)?\nrule calculating estimate given quantity based observed data.\nquantity interest.\nresult.\nUnknown numbers determine statistical model.\nrule calculating estimate given quantity based observed data.quantity interest.result.Unknown numbers determine statistical model.parameter (pick one)?\nrule calculating estimate given quantity based observed data.\nquantity interest.\nresult.\nUnknown numbers determine statistical model.\nrule calculating estimate given quantity based observed data.quantity interest.result.Unknown numbers determine statistical model.J. Ware (1989, 298) mentions ‘randomized play winner design.’ ?J. Ware (1989, 299) mentions ‘adaptive randomization.’ , words?J. Ware (1989, 299) mentions ‘randomized-consent.’ continues ‘attractive setting standard approach informed consent require parents infants near death approached give informed consent invasive surgical procedure , instances, administered. familiar agonizing experience child neonatal intensive care unit can appreciate process obtaining informed consent frightening stressful parents.’ extent agree position, especially given, Ware (1989), p. 305, mentions ‘need withhold information study parents infants receiving CMT?’J. Ware (1989, 300) mentions ‘equipoise.’ words, please define discuss , using example experience.power (statistical context)?","code":"\nlibrary(tidyverse)\nnetflix_data <- \n  tibble(person = c(\"Rohan\", \"Rohan\", \"Monica\", \"Monica\", \"Monica\", \n                    \"Patricia\", \"Patricia\", \"Helen\"),\n         tv_show = c(\"Broadchurch\", \"Duty-Shame\", \"Broadchurch\", \"Duty-Shame\", \n                     \"Shetland\", \"Broadchurch\", \"Shetland\", \"Duty-Shame\"),\n         hours = c(6.8, 8.0, 0.8, 9.2, 3.2, 4.0, 0.2, 10.2)\n         )"},{"path":"hunt-data.html","id":"tutorial-8","chapter":"9 Hunt data","heading":"9.4.2 Tutorial","text":"Please build website using postcards (Kross 2021). Add Google Analytics. Deploy using Netlify. Change aspect website, add different tracker, push new branch. use Netlify conduct /B test. Write one--two page paper found.","code":""},{"path":"farm-data.html","id":"farm-data","chapter":"10 Farm data","heading":"10 Farm data","text":"Required materialRead Atlas AI, Chapter 3 ‘Data,’ (Crawford 2021).Read Guide Census Population, 2016, Chapter 10 ‘Data quality assessment,’ (Statistics Canada 2017).Key concepts skillsObtain data censuses datasets provided governments.Key librariescancensus (von Bergmann, Shkolnik, Jacobs 2021)canlang (T. Timbers 2020)tidyverse (Wickham et al. 2019a)Key functionscancensus::get_census()cancensus::list_census_datasets()cancensus::list_census_regions()cancensus::list_census_vectors()cancensus::set_api_key()canlang::can_langcanlang::region_langdplyr::left_join()dplyr::mutate()dplyr::select()dplyr::slice_max()dplyr::slice_min()readr::read_csv()","code":""},{"path":"farm-data.html","id":"introduction-7","chapter":"10 Farm data","heading":"10.1 Introduction","text":"variety sources data produced purposes used datasets. One thinks especially censuses. Whitby (2020, 30–31) provides enthralling overview, describing earliest censuses written suggestions China’s Yellow River valley, used just purposes taxation conscription. Whitby (2020) also highlights links censuses religion, quoting Book Luke ‘days Caesar Augustus issued decree census taken entire Roman world,’ led David Mary travelling Bethlehem.Taxation substantial motivator censuses. Jones (1953) describes census records survive ‘probably engraved late third early fourth century .D., Diocletian colleagues successors known active carrying censuses serve basis new system taxation.’ detailed records sort abused. instance, Luebke Milton (1994) say ‘(t)Nazi regime gathered information two relatively conventional tools modern administration: national census police registration.’Another source data deliberately put together dataset include economic conditions unemployment, inflation, GDP. Interestingly, Rockoff (2019) describes economic statistics actually developed federal government, even though federal governments typically eventually took role. Typically, sources data put together governments. powers state behind enables thorough way datasets , similarly bring specific perspective.Another, similarly, large established source data long-running large surveys. conducted regular basis, usually directly conducted government, usually funded, one way another, government. instance, often think electoral surveys, Canadian Election Study, run association every federal election since 1965, similarly British Election Study associated every general election since 1964.Finally, large push toward open data government. term become contentious occurred practice, underlying principle—government make available data —undeniable.\nchapter cover datasets, term ‘farmed data.’ typically fairly nicely put together work collecting, preparing cleaning datasets typically done. also, usually, conducted known release cycle. instance, developed countries release unemployment inflation dataset monthly basis, GDP quarterly basis, census every five ten years.datasets always useful, developed time much analysis conducted without use scripts programming languages. cottage industry R package development sprung around making easier get datasets R. chapter cover especially useful.important recognize data neutral. Thinking clearly included dataset, systematically excluded, critical. Crawford (2021, 121) says:way data understood, captured, classified, named fundamentally act world-making containment…. myth data collection benevolent practice… obscured operations power, protecting profit avoiding responsibility consequences.","code":""},{"path":"farm-data.html","id":"censuses","chapter":"10 Farm data","heading":"10.2 Censuses","text":"","code":""},{"path":"farm-data.html","id":"canada","chapter":"10 Farm data","heading":"10.2.1 Canada","text":"first census Canada conducted 1666. 3,215 inhabitants counted, census asked age, sex, marital status, occupation (Statistics Canada 2017). association Confederation, 1867 decennial census required political representatives allocated new Parliament. Regular censuses occurred since , recent 2021.can explore data languages spoken Canada 2016 Census using canlang (T. Timbers 2020). package yet available CRAN, install GitHub, using devtools (Wickham, Hester, Chang 2020).start ‘can_lang’ dataset, provides number Canadians use language 214 languages.can quickly see top-10 common languages mother tongue.combine two datasets together ‘region_lang’ ‘region_data,’ see five -common languages differ largest region, Toronto, smallest, Belleville.can see considerable difference proportions, little 50 per cent Toronto English mother tongue, case around 90 per cent Belleville.general, data Canadian censuses easily available countries. Statistics Canada, government agency responsible census official statistics freely provides Individuals File 2016 census Public Use Microdata File (PUMF), response request. 2.7 per cent sample 2016 census, PUMF provides limited detail.Another way access data Canadian census use cancensus (von Bergmann, Shkolnik, Jacobs 2021). package can installed CRAN. requires API key, can requested creating account going ‘edit profile.’ package helper function set_api_key(\"ADD_YOUR_API_KEY_HERE\", install = TRUE) makes easier add API key ‘.Renviron’ file, way Chapter 8.can use get_census() get census data. need specify census interest, variety arguments. instance, get data 2016 census Ontario, largest Canadian province population.Data 1996, 2001, 2006, 2011, 2016 censuses available, list_census_datasets() provides metadata need provide get_census() access . Data available based variety regions, list_census_regions() provides metadata need. finally, list_census_vectors() provides metadata variables available.","code":"\ninstall.packages(\"devtools\")\ndevtools::install_github(\"ttimbers/canlang\")\nlibrary(tidyverse)\nlibrary(canlang)\n\ncan_lang\n#> # A tibble: 214 × 6\n#>    category language mother_tongue most_at_home most_at_work\n#>    <chr>    <chr>            <dbl>        <dbl>        <dbl>\n#>  1 Aborigi… Aborigi…           590          235           30\n#>  2 Non-Off… Afrikaa…         10260         4785           85\n#>  3 Non-Off… Afro-As…          1150          445           10\n#>  4 Non-Off… Akan (T…         13460         5985           25\n#>  5 Non-Off… Albanian         26895        13135          345\n#>  6 Aborigi… Algonqu…            45           10            0\n#>  7 Aborigi… Algonqu…          1260          370           40\n#>  8 Non-Off… America…          2685         3020         1145\n#>  9 Non-Off… Amharic          22465        12785          200\n#> 10 Non-Off… Arabic          419890       223535         5585\n#> # … with 204 more rows, and 1 more variable:\n#> #   lang_known <dbl>\ncan_lang |>\n  slice_max(mother_tongue, n = 10) |>\n  select(language, mother_tongue)\n#> # A tibble: 10 × 2\n#>    language                     mother_tongue\n#>    <chr>                                <dbl>\n#>  1 English                           19460850\n#>  2 French                             7166700\n#>  3 Mandarin                            592040\n#>  4 Cantonese                           565270\n#>  5 Punjabi (Panjabi)                   501680\n#>  6 Spanish                             458850\n#>  7 Tagalog (Pilipino, Filipino)        431385\n#>  8 Arabic                              419890\n#>  9 German                              384040\n#> 10 Italian                             375635\nregion_lang |>\n  left_join(region_data, by = \"region\") |>\n  slice_max(c(population)) |>\n  slice_max(mother_tongue, n = 5) |>\n  select(region, language, mother_tongue, population) |>\n  mutate(prop = mother_tongue / population)\n#> # A tibble: 5 × 5\n#>   region  language          mother_tongue population   prop\n#>   <chr>   <chr>                     <dbl>      <dbl>  <dbl>\n#> 1 Toronto English                 3061820    5928040 0.516 \n#> 2 Toronto Cantonese                247710    5928040 0.0418\n#> 3 Toronto Mandarin                 227085    5928040 0.0383\n#> 4 Toronto Punjabi (Panjabi)        171225    5928040 0.0289\n#> 5 Toronto Italian                  151415    5928040 0.0255\n\nregion_lang |>\n  left_join(region_data, by = \"region\") |>\n  slice_min(c(population)) |>\n  slice_max(mother_tongue, n = 5) |>\n  select(region, language, mother_tongue, population) |>\n  mutate(prop = mother_tongue / population)\n#> # A tibble: 5 × 5\n#>   region     language mother_tongue population    prop\n#>   <chr>      <chr>            <dbl>      <dbl>   <dbl>\n#> 1 Belleville English          93655     103472 0.905  \n#> 2 Belleville French            2675     103472 0.0259 \n#> 3 Belleville German             635     103472 0.00614\n#> 4 Belleville Dutch              600     103472 0.00580\n#> 5 Belleville Spanish            350     103472 0.00338library(tidyverse)\nlibrary(cancensus)\n\nontario_population <- \n  get_census(dataset = \"CA16\",\n             level = \"Regions\",\n             vectors = \"v_CA16_1\", \n             regions = list(PR=c('35')\n                            )\n             )\n#> \nDownloading: 120 B     \nDownloading: 120 B     \nDownloading: 120 B     \nDownloading: 120 B     \nDownloading: 120 B     \nDownloading: 120 B\n\nontario_population\n#> # A tibble: 0 × 0"},{"path":"farm-data.html","id":"usa","chapter":"10 Farm data","heading":"10.2.2 USA","text":"","code":""},{"path":"farm-data.html","id":"american-community-survey","chapter":"10 Farm data","heading":"10.2.2.1 American Community Survey","text":"requirement US Census included US Constitution, decent, though clunky, access provided. US envious situation usually better approach going national statistical agency IPUMS. IPUMS provides access wide range datasets, including international census microdata. specific case US, American Community Survey (ACS) survey comparable questions asked many censuses, available annual basis, compared census quite --date time data available. ends millions responses year. Although ACS smaller census, advantage available timely basis. access ACS IPUMS.Go IPUMS, ‘IPUMS USA,’ select ‘get data’ (Figure 10.1).\nFigure 10.1: IPUMS homepage, IPUMS USA shown top left box\ninterested sample, go ‘SELECT SAMPLE,’ un-select ‘Default sample year’ instead select ‘2019 ACS’ ‘SUBMIT SAMPLE SELECTIONS’ (Figure 10.2).\nFigure 10.2: Selecting sample IPUMS USA specifying interest 2019 ACS\nmight interested data based state. begin looking ‘HOUSEHOLD’ variables selecting ‘GEOGRAPHIC’ (Figure 10.3).\nFigure 10.3: Specifying interested state\nadd ‘STATEICP’ ‘cart’ clicking plus, turn tick (Figure 10.4).\nFigure 10.4: Adding STATEICP cart\nmight interested data ‘PERSON’ basis, instance, ‘DEMOGRAPHIC’ variables ‘AGE,’ add cart. Still ‘PERSON’ basis, might interested ‘INCOME,’ instance, ‘Total personal income’ ‘INCTOT’ add cart (Figure 10.5).\nFigure 10.5: Adding additional demographic variables available individual basis\ndone, can ‘VIEW CART,’ ‘CREATE DATA EXTRACT’ (Figure 10.6). point two aspects likely want change:Change ‘DATA FORMAT’ dat csv (Figure 10.7).Customize sample size likely need three million responses, just change , say, 500,000 (Figure 10.8).\nFigure 10.6: Beginning checkout process\n\nFigure 10.7: Specifying interested CSV files\n\nFigure 10.8: Reducing sample size three million responses half million\nFinally, want include descriptive name extract, instance, ‘2022-02-06: Income based state age,’ specifies date made extract extract. can ‘SUBMIT EXTRACT.’asked log create account, able submit request. IPUMS email extract available, can download read R usual way. critical cite dataset use (Ruggles et al. 2021).Incredibly, full count, entire census, data available IPUMS US censuses conducted : 1850, 1860, 1870, 1880, 1900, 1910, 1920, 1930, 1940. 1890 census records destroyed due fire 1921. 1 per cent samples available years, 1990. ACS data available 2000.","code":"\nlibrary(tidyverse)\nipums_extract <- read_csv(\"usa_00010.csv\")\n\nipums_extract#> # A tibble: 6 × 4\n#>    YEAR STATEICP   AGE INCTOT\n#>   <dbl>    <dbl> <dbl>  <dbl>\n#> 1  2019       41    39   9000\n#> 2  2019       41    35   9300\n#> 3  2019       41    39  60000\n#> 4  2019       41    32  14400\n#> 5  2019       41    21      0\n#> 6  2019       41    61  11100"},{"path":"farm-data.html","id":"exercises-and-tutorial-9","chapter":"10 Farm data","heading":"10.3 Exercises and tutorial","text":"","code":""},{"path":"farm-data.html","id":"exercises-9","chapter":"10 Farm data","heading":"10.3.1 Exercises","text":"Please identify three sources data interested describe available (please include link code)?Please focus one sources. steps go order get dataset can analyzed R?Let’s say take job RBC (Canadian bank) already quantitative data use. questions explore deciding whether data useful ?Write three points (welcome use dot points) government data may especially useful?Please pick government interest find inflation statistics. extent know data gathered?reference Chen et al. (2019) Martinez (2019) extent think can trust government statistics? Please mention least three governments answer.2021 census Canada asked, firstly, ‘person’s sex birth? Sex refers sex assigned birth. Male/Female,’ ‘person’s gender? Refers current gender may different sex assigned birth may different indicated legal documents. Male/Female/please specify person’s gender (space typed handwritten answer).’ reference Statistics Canada (2020), please discuss extent think appropriate way census proceeded. welcome discuss case different country familiar .Pretend conducted survey everyone Canada, asked age, sex, gender. friend claims need worry uncertainty ‘whole population.’ friend right wrong, ?","code":""},{"path":"farm-data.html","id":"tutorial-9","chapter":"10 Farm data","heading":"10.3.2 Tutorial","text":"Use IPUMS access ACS. Download data interest write two--three page paper analyzing .","code":""},{"path":"farm-data.html","id":"paper-2","chapter":"10 Farm data","heading":"10.3.3 Paper","text":"point, Paper Three (Appendix B.3) appropriate.","code":""},{"path":"cleaning-and-preparing-data.html","id":"cleaning-and-preparing-data","chapter":"11 Cleaning and preparing data","heading":"11 Cleaning and preparing data","text":"Required materialRead Data Feminism, Chapter 5 ‘Unicorns, Janitors, Ninjas, Wizards, Rock Stars,’ (D’Ignazio Klein 2020).Read R Data Science, Chapter 12 ‘Tidy data,’ (Wickham Grolemund 2017).Read Gave Four Good Pollsters Raw Data. Four Different Results, (Cohn 2016).Read Column Names Contracts, (Riederer 2020)\n\nKey concepts skillsPlanning end-point simulating dataset like end , key elements cleaning preparing data.Begin small sample dataset, write code fix , iterate generalize additional tranches.Develop series tests checks dataset pass features dataset clear.especially concerned class variables, clear names, values variable expected given .Key librariesconvo (Riederer 2022)janitor (Firke 2020)pointblank (Iannone Vargas 2022)purrr (Henry Wickham 2020)stringr (Wickham 2019e)tidyr (Wickham 2021c)tidyverse (Wickham 2017)Key functionsdplyr::count()dplyr::mutate()dplyr::select()janitor::clean_names()stringr::str_replace_all()stringr::str_trim()tidyr::pivot_longer()tidyr::separate()tidyr::separate_rows()","code":""},{"path":"cleaning-and-preparing-data.html","id":"introduction-8","chapter":"11 Cleaning and preparing data","heading":"11.1 Introduction","text":"“Well, Lyndon, may right may every bit intelligent say,” said Rayburn, “’d feel whole lot better just one run sheriff .”Sam Rayburn’s reaction Lyndon Johnson’s enthusiasm Kennedy’s incoming cabinet, quoted Halberstam (1972, 41).earlier chapters done data cleaning preparation, chapter put place formal approaches. large extent, role data cleaning preparation great, people can trust understand data, cleaned . , paradoxically, often cleaning preparation often trust least. point every data science workflow, modelling get hands dirty data cleaning. clean prepare data make lot different decisions, many important.long time, data cleaning preparation largely overlooked. now realize mistake. difficult trust results disciplines apply statistics. reproducibility crisis, started psychology now extended many fields physical social sciences, brought light issues p-value ‘hacking,’ researcher degrees freedom, file-drawer issues, even data results fabrication (Gelman Loken 2013). Steps now put place address . However, relatively little focus data gathering, cleaning, preparation aspects applied statistics, despite evidence decisions made steps greatly affect statistical results (Huntington-Klein et al. 2020). chapter focus issues.statistical practices underpin data science correct robust applied simulated datasets, data science typically conducted types datasets. instance, data scientists interested ‘messy, unfiltered, possibly unclean data—tainted heteroskedasticity, complex dependence missingness patterns—recently avoided polite conversations traditional statisticians’ (Craiu 2019).Big data resolve issue, may even exacerbate , instance ‘without taking data quality account, population inferences Big Data subject Big Data Paradox: data, surer fool ’ (Meng 2018). important note issues found much applied statistics research necessarily associated researcher quality, biases (Silberzahn et al. 2018). Instead, result environment within data science conducted. chapter aims give tools explicitly think work.Gelman Vehtari (2020) writing important statistical ideas past 50 years say enabled new ways thinking data analysis brought tent statistics, approaches ‘considered matter taste philosophy.’ focus data cleaning preparation chapter analogous, insofar, represents codification, bringing inside tent, aspects typically, incorrectly, considered taste rather statistics.workflow advocate :Save raw data.Plan end state.Execute plan tiny sample.Write tests documentation.Iterate plan.Generalize execution.Update tests documentation.need variety skills effective, stuff statistical sciences. approach needed combination dogged sensible. Perfect much enemy good enough comes data cleaning. specific, better 90 per cent data cleaned prepared, start exploring , deciding whether worth effort clean prepare remaining 10 per cent remainder likely take awful lot time effort.data regardless whether obtained hunting, gathering, farming, issues critical approaches can deal variety issues, importantly, understand might affect modelling (Van den Broeck et al. 2005). clean data analyze data. process forces us make choices value results (Au 2020).","code":""},{"path":"cleaning-and-preparing-data.html","id":"workflow","chapter":"11 Cleaning and preparing data","heading":"11.2 Workflow","text":"","code":""},{"path":"cleaning-and-preparing-data.html","id":"save-a-copy-of-the-raw-data","chapter":"11 Cleaning and preparing data","heading":"11.2.1 Save a copy of the raw data","text":"first step save raw data separate, local, folder. important save raw data, extent possible, establishes foundation reproducibility (Wilson et al. 2017). obtaining data third-party, government website, control whether continue host data, whether update , address available. also want reduce burden impose servers, saving local copy.locally saved raw data must maintain state, modify . begin clean prepare , instead create another dataset. Maintaining initial, raw, state dataset, using scripts create dataset interested analyzing, ensures entire workflow reproducible.","code":""},{"path":"cleaning-and-preparing-data.html","id":"begin-with-an-end-in-mind","chapter":"11 Cleaning and preparing data","heading":"11.2.2 Begin with an end in mind","text":"Planning end state forcing begin end mind important variety reasons. scraping data, helps us proactive scope-creep, data cleaning additionally forces us really think want final dataset look like.first step sketch dataset interested . key features sketch aspects names columns, class, possible range values. instance, might interested populations US states. case sketch might look like Figure 11.1.CHANGE \nFigure 11.1: Planned dataset US states populations\ncase, sketch forces us decide whether want full names abbreviations state names, population measured millions. process sketching end-point forced us make decisions early , clear desired end state.implement using code simulate data. , process forces us think reasonable values look like dataset literally forced decide functions use. Thinking carefully membership column , instance column meant ‘gender’ values ‘male,’ ‘female,’ ‘,’ ‘unknown’ may expected, number ‘1,000’ likely unexpected. also forces us explicit variable names assign outputs functions variable. instance, simulate data population data.purpose, data cleaning preparation, bring raw data close plan. Ideally, plan desired end-state dataset ‘tidy data,’ introduced Chapter 3.","code":"\nlibrary(tidyverse)\n\nset.seed(853)\n\nsimulated_tfr <- \n  tibble(\n    state = state.name,\n    population = runif(n = 50, min = 0, max = 50) |> round(digits = 2)\n  )\n\nsimulated_tfr\n#> # A tibble: 50 × 2\n#>    state       population\n#>    <chr>            <dbl>\n#>  1 Alabama          18.0 \n#>  2 Alaska            6.01\n#>  3 Arizona          24.2 \n#>  4 Arkansas         15.8 \n#>  5 California        1.87\n#>  6 Colorado         20.2 \n#>  7 Connecticut       6.54\n#>  8 Delaware         12.1 \n#>  9 Florida           7.9 \n#> 10 Georgia           9.44\n#> # … with 40 more rows"},{"path":"cleaning-and-preparing-data.html","id":"start-small","chapter":"11 Cleaning and preparing data","heading":"11.2.3 Start small","text":"thoroughly planned can turn raw data dealing . Usually, regardless raw data look like, want manipulate rectangular dataset quickly possible. allows us use family dplyr verbs tidyverse approaches. instance, let us assume starting .txt file.first step look regularities dataset. wanting end tabular data, means need type delimiter distinguish different columns. Ideally might features comma, semicolon, tab, double space, line break.worse cases may regular feature dataset can take advantage . instance, sometimes various text repeated.case, although traditional delimiter can use regularity ‘State ’ ’ population ’ get need. difficult case line breaks.One way approach take advantage different classes values looking . instance, case, know US states, 50 possible options, use existence delimiter. also use fact population number , split based space followed number.now go process converting last example tidy data using tidyr (Wickham 2021c),","code":"Alabama, 5\nAlaska, 0.7\nArizona, 7\nArkansas, 3\nCalifornia, 40State is Alabama and population is 5 million.\nState is Alaska and population is 0.7 million.\nState is Arizona and population is 7 million.\nState is Arkansas and population is 3 million.\nState is California and population is 40 million.Alabama 5 Alaska 0.7 Arizona 7 Arkansas 3 California 40\nraw_data <-\n  c('Alabama 5 Alaska 0.7 Arizona 7 Arkansas 3 California 40')\n\ndata_as_tibble <-\n  tibble(raw = raw_data)\n\ntidy_data <-\n  data_as_tibble |>\n  separate(col = raw,\n           into = letters[1:5],\n           sep = \"(?<=[[:digit:]]) \") |>\n  pivot_longer(cols = letters[1:5],\n               names_to = \"drop_me\",\n               values_to = \"separate_me\") |>\n  separate(col = separate_me,\n           into = c('state', 'population'),\n           sep = \" (?=[[:digit:]])\") |>\n  select(-drop_me)\n\ntidy_data\n#> # A tibble: 5 × 2\n#>   state      population\n#>   <chr>      <chr>     \n#> 1 Alabama    5         \n#> 2 Alaska     0.7       \n#> 3 Arizona    7         \n#> 4 Arkansas   3         \n#> 5 California 40"},{"path":"cleaning-and-preparing-data.html","id":"write-tests-and-documentation.","chapter":"11 Cleaning and preparing data","heading":"11.2.4 Write tests and documentation.","text":"established rectangular dataset, albeit messy one, begin look classes . necessarily want fix classes point, can result us losing data. look class see , compare simulated dataset see needs get . note columns different.changing class going onto bespoke issues, deal common issues class. common issues :Commas punctuation, denomination signs columns numeric.Inconsistent formatting dates, ‘December’ ‘Dec’ ‘12.’Unexpected characters, especially unicode, may display consistently.Typically, want fix anything immediately obvious. instance, remove commas used group digits currencies. However, situation typically quickly become dire. need look membership group, triage fix. probably make decision triage based likely largest impact. usually means starting counts, sorting descending order, dealing come.tests membership passed, finally can change class, run tests . adapting idea software development approach unit testing. Tests crucial enable us understand whether software (case data) fit purpose (Wilson 2021).Let us run example collection strings, slightly wrong. type output typical OCR, often gets way , quite., first want get rectangular dataset.now need decide errors going fix. help us decide important, create count.common element correct one, great. next one - ‘PatricIa’ - looks like ‘’ incorrectly capitalized, one - ‘8atricia’ - distinguished ‘8’ instead ‘P.’ Let us quickly fix issues redo count.Already much better 60 per cent values correct, compared earlier 30 per cent. two obvious errors - ‘Ptricia’ ‘Patncia’ - first missing ‘’ second ‘n’ ‘ri’ . , can quickly update fix .achieved 80 per cent fix much effort. two remaining issues subtle. first - ‘Patric1a’ - occurred ‘’ incorrectly coded ‘1.’ fonts show , others difficult see. common issue, especially OCR, something aware . second - ‘Patricia’ - similarly subtle occurring trailing space. , trailing leading spaces common issue can address str_trim(). fix two remaining issues entries corrected.tests head example. know hoping ‘Patricia.’ can start document test well. One way look see values ‘Patricia’ exist dataset.can make things little imposing stopping code execution condition met stopifnot(). use define condition like met. implement type check throughout code. instance, expected certain number rows dataset, certain column various properties, integer, factor.can use stopifnot() ensure script working expected goes . Another way especially usedAnother way write tests dataset use testthat (Wickham 2011). Although developed testing packages, can use functionality test datasets. instance, can use expect_length() check length dataset use expect_equal() check content.tests pass nothing happens, tests fail script stop.","code":"\nmessy_string <-\n  c('Patricia, Ptricia, PatricIa, Patncia, PatricIa, Patricia, Patricia, Patric1a, Patricia , 8atricia')\n\nmessy_string\n#> [1] \"Patricia, Ptricia, PatricIa, Patncia, PatricIa, Patricia, Patricia, Patric1a, Patricia , 8atricia\"\nmessy_data <- \n  tibble(names = messy_string) |> \n  separate_rows(names, sep = \", \") \n\nmessy_data\n#> # A tibble: 10 × 1\n#>    names      \n#>    <chr>      \n#>  1 \"Patricia\" \n#>  2 \"Ptricia\"  \n#>  3 \"PatricIa\" \n#>  4 \"Patncia\"  \n#>  5 \"PatricIa\" \n#>  6 \"Patricia\" \n#>  7 \"Patricia\" \n#>  8 \"Patric1a\" \n#>  9 \"Patricia \"\n#> 10 \"8atricia\"\nmessy_data |> \n  count(names, sort = TRUE)\n#> # A tibble: 7 × 2\n#>   names           n\n#>   <chr>       <int>\n#> 1 \"Patricia\"      3\n#> 2 \"PatricIa\"      2\n#> 3 \"8atricia\"      1\n#> 4 \"Patncia\"       1\n#> 5 \"Patric1a\"      1\n#> 6 \"Patricia \"     1\n#> 7 \"Ptricia\"       1\nmessy_data <- \n  messy_data |> \n  mutate(names = str_replace_all(names, 'PatricIa', 'Patricia'),\n         names = str_replace_all(names, '8atricia', 'Patricia')\n         )\n\nmessy_data |> \n  count(names, sort = TRUE)\n#> # A tibble: 5 × 2\n#>   names           n\n#>   <chr>       <int>\n#> 1 \"Patricia\"      6\n#> 2 \"Patncia\"       1\n#> 3 \"Patric1a\"      1\n#> 4 \"Patricia \"     1\n#> 5 \"Ptricia\"       1\nmessy_data <- \n  messy_data |> \n  mutate(names = str_replace_all(names, 'Ptricia', 'Patricia'),\n         names = str_replace_all(names, 'Patncia', 'Patricia')\n         )\n\nmessy_data |> \n  count(names, sort = TRUE)\n#> # A tibble: 3 × 2\n#>   names           n\n#>   <chr>       <int>\n#> 1 \"Patricia\"      8\n#> 2 \"Patric1a\"      1\n#> 3 \"Patricia \"     1\ncleaned_data <- \n  messy_data |> \n  mutate(names = str_replace_all(names, 'Patric1a', 'Patricia'),\n         names = str_trim(names, side = c(\"right\"))\n         )\n\ncleaned_data |> \n  count(names, sort = TRUE)\n#> # A tibble: 1 × 2\n#>   names        n\n#>   <chr>    <int>\n#> 1 Patricia    10\ncheck_me <- \n  cleaned_data |> \n  filter(names != \"Patricia\")\n\nif (nrow(check_me) > 0) {\n  print(\"Still have values that are not Patricia!\")\n}\nstopifnot(nrow(check_me) == 0)\nlibrary(testthat)\n\nexpect_length(check_me, 1)\nexpect_equal(class(cleaned_data$names), \"character\")\nexpect_equal(unique(cleaned_data$names), \"Patricia\")"},{"path":"cleaning-and-preparing-data.html","id":"iterate-generalize-and-update","chapter":"11 Cleaning and preparing data","heading":"11.2.5 Iterate, generalize and update","text":"now iterate plan. recent case, started 10 entries. reason increase 100 even 1,000. may need generalize cleaning procedures tests. eventually start dataset sort order.","code":""},{"path":"cleaning-and-preparing-data.html","id":"case-study-kenya-census","chapter":"11 Cleaning and preparing data","heading":"11.3 Case study: Kenya census","text":"","code":""},{"path":"cleaning-and-preparing-data.html","id":"gather-and-clean-data","chapter":"11 Cleaning and preparing data","heading":"11.3.1 Gather and clean data","text":"make clear, let us gather, clean, prepare data 2019 Kenyan census. distribution population age, sex, administrative unit 2019 Kenyan census can downloaded . format PDF makes easy look particular result, overly useful want model data. order able , need convert PDF Kenyan census results counts, age sex, county sub-county, tidy dataset can analyzed. use janitor (Firke 2020), pdftools (Ooms 2019b), tidyverse (Wickham et al. 2019b), stringi (Gagolewski 2020).can download1 read PDF 2019 Kenyan census. PDF want read content R, pdf_text() pdftools (Ooms 2019b) useful. works well many recently produced PDFs content text can extract. PDF image, pdf_text() work. Instead, PDF first need go OCR, covered Chapter 8.can see example page PDF 2019 Kenyan census (Figure 11.2).\nFigure 11.2: Example page 2019 Kenyan census\nfirst challenge get dataset format can easily manipulate. consider page PDF extract relevant parts. , first write function, apply page.now function need page PDF. use map_dfr() purrr (Henry Wickham 2020) apply function page, combine outputs one tibble.got rectangular format, now need clean dataset make useful.first step make numbers actual numbers, rather characters. can convert type, need remove anything number otherwise cell converted NA. first identify values numbers can remove .use janitor , worthwhile least first looking going sometimes odd stuff janitor (packages) deal , way want. case, Kenyan government used Excel similar, converted two entries dates. just took numbers column 23 15 , inspecting column can use Excel reverse process enter correct values 4,923 4,611, respectively.identified everything needs removed, can actual removal convert character column numbers integers.next thing clean areas. know 47 counties Kenya, large number sub-counties. Kenyan government purports provide list pages 19 22 PDF (document pages 7 10). list complete, minor issues deal later. case, first need fix inconsistencies.Kenya 47 counties, sub-counties. PDF arranged county data sub-counties, without designating . can use names, certain extent, handful cases, sub-county name county, need first fix .PDF made-three tables. can first get names counties based final two tables reconcile get list counties.hoped, 47 . can add flag based names, need deal sub-counties share name. based page, looking deciding county page sub-county page.Now can add flag whether area county, adjust ones troublesome,dealt areas, can deal ages. First, need fix clear errors.census done work putting together age-groups us, want make easy just focus counts single-year-age. add flag type age : age group, “ages 0 5,” single age, “1.”moment, age character variable. decision make . want character variable (graph properly), want numeric, total 100+ . now, just make factor, least able nicely graphed.","code":"\nlibrary(janitor)\nlibrary(pdftools)\nlibrary(purrr)\nlibrary(tidyverse)\nlibrary(stringi)\ndownload.file(\n  \"https://www.knbs.or.ke/download/2019-kenya-population-and-housing-census-volume-iii-distribution-of-population-by-age-sex-and-administrative-units/?wpdmdl=5729&refresh=620561f1ce3ad1644519921\", \n  \"2019_Kenya_census.pdf\",\n  mode=\"wb\")\n\nall_content <- pdf_text(\"2019_Kenya_census.pdf\")\n# The function is going to take an input of a page\nget_data <- function(i){\n  # i = 467\n  # Just look at the page of interest\n  # Based on Bob Rudis: https://stackoverflow.com/a/47793617\n  just_page_i <- stri_split_lines(all_content[[i]])[[1]] \n  \n  just_page_i <- just_page_i[just_page_i != \"\"]\n  \n  # Grab the name of the location\n  area <- just_page_i[3] |> str_squish()\n  area <- str_to_title(area)\n  \n  # Grab the type of table\n  type_of_table <- just_page_i[2] |> str_squish()\n  \n  # Get rid of the top matter\n  # Manually for now, but could create some rules if needed\n  just_page_i_no_header <- just_page_i[5:length(just_page_i)] \n  \n  # Get rid of the bottom matter\n  # Manually for now, but could create some rules if needed\n  just_page_i_no_header_no_footer <- just_page_i_no_header[1:62] \n  \n  # Convert into a tibble\n  demography_data <- tibble(all = just_page_i_no_header_no_footer)\n  \n  # Split columns\n  demography_data <-\n    demography_data |>\n    mutate(all = str_squish(all)) |> # Any space more than two spaces is reduced\n    mutate(all = str_replace(all, \"10 -14\", \"10-14\")) |> # One specific issue\n    mutate(all = str_replace(all, \"Not Stated\", \"NotStated\")) |> # And another\n    separate(col = all,\n             into = c(\"age\", \"male\", \"female\", \"total\", \"age_2\", \"male_2\", \"female_2\", \"total_2\"),\n             sep = \" \", # Works fine because the tables are nicely laid out\n             remove = TRUE,\n             fill = \"right\",\n             extra = \"drop\"\n    )\n  \n  # They are side by side at the moment, need to append to bottom\n  demography_data_long <-\n    rbind(demography_data |> select(age, male, female, total),\n          demography_data |>\n            select(age_2, male_2, female_2, total_2) |>\n            rename(age = age_2, male = male_2, female = female_2, total = total_2)\n    )\n  \n  # There is one row of NAs, so remove it\n  demography_data_long <- \n    demography_data_long |> \n    remove_empty(which = c(\"rows\"))\n  \n  # Add the area and the page\n  demography_data_long$area <- area\n  demography_data_long$table <- type_of_table\n  demography_data_long$page <- i\n  \n  rm(just_page_i,\n     i,\n     area,\n     type_of_table,\n     just_page_i_no_header,\n     just_page_i_no_header_no_footer,\n     demography_data)\n  \n  return(demography_data_long)\n}\n# Run through each relevant page and get the data\npages <- c(30:513)\nall_tables <- map_dfr(pages, get_data)\nrm(pages, get_data)\nall_tables\n#> # A tibble: 59,532 × 7\n#>    age   male    female  total     area    table        page\n#>    <chr> <chr>   <chr>   <chr>     <chr>   <chr>       <int>\n#>  1 Total 610,257 598,046 1,208,303 Mombasa Table 2.3:…    30\n#>  2 0     15,111  15,009  30,120    Mombasa Table 2.3:…    30\n#>  3 1     15,805  15,308  31,113    Mombasa Table 2.3:…    30\n#>  4 2     15,088  14,837  29,925    Mombasa Table 2.3:…    30\n#>  5 3     14,660  14,031  28,691    Mombasa Table 2.3:…    30\n#>  6 4     14,061  13,993  28,054    Mombasa Table 2.3:…    30\n#>  7 0-4   74,725  73,178  147,903   Mombasa Table 2.3:…    30\n#>  8 5     13,851  14,023  27,874    Mombasa Table 2.3:…    30\n#>  9 6     12,889  13,216  26,105    Mombasa Table 2.3:…    30\n#> 10 7     13,268  13,203  26,471    Mombasa Table 2.3:…    30\n#> # … with 59,522 more rows\n# Need to convert male, female, and total to integers\n# First find the characters that should not be in there\nall_tables |> \n  select(male, female, total) |>\n  mutate_all(~str_remove_all(., \"[:digit:]\")) |> \n  mutate_all(~str_remove_all(., \",\")) |>\n  mutate_all(~str_remove_all(., \"_\")) |>\n  mutate_all(~str_remove_all(., \"-\")) |> \n  distinct()\n#> # A tibble: 3 × 3\n#>   male  female total\n#>   <chr> <chr>  <chr>\n#> 1 \"\"    \"\"     \"\"   \n#> 2 \"Aug\" \"\"     \"\"   \n#> 3 \"Jun\" \"\"     \"\"\n\n# We clearly need to remove \",\", \"_\", and \"-\". \n# This also highlights a few issues on p. 185 that need to be manually adjusted\n# https://twitter.com/RohanAlexander/status/1244337583016022018\nall_tables$male[all_tables$male == \"23-Jun\"] <- 4923\nall_tables$male[all_tables$male == \"15-Aug\"] <- 4611\nall_tables <-\n  all_tables |>\n  mutate_at(vars(male, female, total), ~str_remove_all(., \",\")) |>\n  mutate_at(vars(male, female, total), ~str_replace(., \"_\", \"0\")) |>\n  mutate_at(vars(male, female, total), ~str_replace(., \"-\", \"0\")) |>\n  mutate_at(vars(male, female, total), ~as.integer(.))\n\nall_tables\n#> # A tibble: 59,532 × 7\n#>    age     male female   total area    table            page\n#>    <chr>  <int>  <int>   <int> <chr>   <chr>           <int>\n#>  1 Total 610257 598046 1208303 Mombasa Table 2.3: Dis…    30\n#>  2 0      15111  15009   30120 Mombasa Table 2.3: Dis…    30\n#>  3 1      15805  15308   31113 Mombasa Table 2.3: Dis…    30\n#>  4 2      15088  14837   29925 Mombasa Table 2.3: Dis…    30\n#>  5 3      14660  14031   28691 Mombasa Table 2.3: Dis…    30\n#>  6 4      14061  13993   28054 Mombasa Table 2.3: Dis…    30\n#>  7 0-4    74725  73178  147903 Mombasa Table 2.3: Dis…    30\n#>  8 5      13851  14023   27874 Mombasa Table 2.3: Dis…    30\n#>  9 6      12889  13216   26105 Mombasa Table 2.3: Dis…    30\n#> 10 7      13268  13203   26471 Mombasa Table 2.3: Dis…    30\n#> # … with 59,522 more rows\n# Fix some area names\nall_tables$area[all_tables$area == \"Taita/ Taveta\"] <- \"Taita/Taveta\"\nall_tables$area[all_tables$area == \"Elgeyo/ Marakwet\"] <- \"Elgeyo/Marakwet\"\nall_tables$area[all_tables$area == \"Nairobi City\"] <- \"Nairobi\"\nall_tables$table |> \n  table()\n#> \n#> Table 2.3: Distribution of Population by Age, Sex*, County and Sub- County \n#>                                                                      48216 \n#>       Table 2.4a: Distribution of Rural Population by Age, Sex* and County \n#>                                                                       5535 \n#>       Table 2.4b: Distribution of Urban Population by Age, Sex* and County \n#>                                                                       5781\nlist_counties <- \n  all_tables |> \n  filter(table %in% c(\"Table 2.4a: Distribution of Rural Population by Age, Sex* and County\",\n                      \"Table 2.4b: Distribution of Urban Population by Age, Sex* and County\")\n         ) |> \n  select(area) |> \n  distinct()\n\nlist_counties\n#> # A tibble: 47 × 1\n#>    area        \n#>    <chr>       \n#>  1 Kwale       \n#>  2 Kilifi      \n#>  3 Tana River  \n#>  4 Lamu        \n#>  5 Taita/Taveta\n#>  6 Garissa     \n#>  7 Wajir       \n#>  8 Mandera     \n#>  9 Marsabit    \n#> 10 Isiolo      \n#> # … with 37 more rows\nall_tables |> \n  filter(table == \"Table 2.3: Distribution of Population by Age, Sex*, County and Sub- County\") |> \n  filter(area %in% c(\"Busia\",\n                     \"Garissa\",\n                     \"Homa Bay\",\n                     \"Isiolo\",\n                     \"Kiambu\",\n                     \"Machakos\",\n                     \"Makueni\",\n                     \"Samburu\",\n                     \"Siaya\",\n                     \"Tana River\",\n                     \"Vihiga\",\n                     \"West Pokot\")\n         ) |> \n  select(area, page) |> \n  distinct()\n#> # A tibble: 24 × 2\n#>    area        page\n#>    <chr>      <int>\n#>  1 Samburu       42\n#>  2 Tana River    53\n#>  3 Tana River    56\n#>  4 Garissa       65\n#>  5 Garissa       69\n#>  6 Isiolo        98\n#>  7 Isiolo       100\n#>  8 Machakos     149\n#>  9 Machakos     154\n#> 10 Makueni      159\n#> # … with 14 more rows\nall_tables <- \n  all_tables |> \n  mutate(area_type = if_else(area %in% list_counties$area, \"county\", \"sub-county\"))\n\nall_tables <- \n  all_tables |> \n  mutate(area_type = case_when(\n    area == \"Samburu\" & page == 42 ~ \"sub-county\",\n    area == \"Tana River\" & page == 56 ~ \"sub-county\",\n    area == \"Garissa\" & page == 69 ~ \"sub-county\",\n    area == \"Isiolo\" & page == 100 ~ \"sub-county\",\n    area == \"Machakos\" & page == 154 ~ \"sub-county\",\n    area == \"Makueni\" & page == 164 ~ \"sub-county\",\n    area == \"Kiambu\" & page == 213 ~ \"sub-county\",\n    area == \"West Pokot\" & page == 233 ~ \"sub-county\",\n    area == \"Vihiga\" & page == 333 ~ \"sub-county\",\n    area == \"Busia\" & page == 353 ~ \"sub-county\",\n    area == \"Siaya\" & page == 360 ~ \"sub-county\",\n    area == \"Homa Bay\" & page == 375 ~ \"sub-county\",\n    TRUE ~ area_type\n    )\n  )\n\nrm(list_counties)\n\nall_tables\n#> # A tibble: 59,532 × 8\n#>    age     male female   total area    table  page area_type\n#>    <chr>  <int>  <int>   <int> <chr>   <chr> <int> <chr>    \n#>  1 Total 610257 598046 1208303 Mombasa Tabl…    30 county   \n#>  2 0      15111  15009   30120 Mombasa Tabl…    30 county   \n#>  3 1      15805  15308   31113 Mombasa Tabl…    30 county   \n#>  4 2      15088  14837   29925 Mombasa Tabl…    30 county   \n#>  5 3      14660  14031   28691 Mombasa Tabl…    30 county   \n#>  6 4      14061  13993   28054 Mombasa Tabl…    30 county   \n#>  7 0-4    74725  73178  147903 Mombasa Tabl…    30 county   \n#>  8 5      13851  14023   27874 Mombasa Tabl…    30 county   \n#>  9 6      12889  13216   26105 Mombasa Tabl…    30 county   \n#> 10 7      13268  13203   26471 Mombasa Tabl…    30 county   \n#> # … with 59,522 more rows\ntable(all_tables$age) |> head()\n#> \n#>     0   0-4     1    10 10-14 10-19 \n#>   484   484   484   484   482     1\nunique(all_tables$age) |> head()\n#> [1] \"Total\" \"0\"     \"1\"     \"2\"     \"3\"     \"4\"\n\n# Looks like there should be 484, so need to follow up on some:\nall_tables$age[all_tables$age == \"NotStated\"] <- \"Not Stated\"\nall_tables$age[all_tables$age == \"43594\"] <- \"5-9\"\nall_tables$age[all_tables$age == \"43752\"] <- \"10-14\"\nall_tables$age[all_tables$age == \"9-14\"] <- \"5-9\"\nall_tables$age[all_tables$age == \"10-19\"] <- \"10-14\"\nall_tables$age_type <-\n  if_else(str_detect(all_tables$age, c(\"-\")), \"age-group\", \"single-year\")\nall_tables$age_type <-\n  if_else(str_detect(all_tables$age, c(\"Total\")),\n          \"age-group\",\n          all_tables$age_type)\nall_tables$age <- as_factor(all_tables$age)"},{"path":"cleaning-and-preparing-data.html","id":"check-data","chapter":"11 Cleaning and preparing data","heading":"11.3.2 Check data","text":"gathered cleaned data, like run checks. Given format data, can check ‘total’ sum ‘male’ ‘female.’ (prefer use different groupings, Kenyan government collected makes available.)can adjust one looks wrong.Kenyan census provides different tables total county sub-county; within county, number urban area county, number urban area county. counties urban count, like make sure sum rural urban counts equals total count. requires pivoting data long wide.First, construct different tables three.constructed constituent parts, can join based age, area, whether county.can now check sum rural urban total.just , difference 1, just move .Finally, want check single age counts sum age-groups.Mt. Kenya Forest, Aberdare Forest, Kakamega Forest slightly dodgy. seem documentation, looks like Kenyan government apportioned various countries. understandable, unlikely big deal, , , just move .","code":"\nfollow_up <- \n  all_tables |> \n  mutate(check_sum = male + female,\n         totals_match = if_else(total == check_sum, 1, 0)\n         ) |> \n  filter(totals_match == 0)\n# There is just one that looks wrong\nall_tables$male[all_tables$age == \"10\" & all_tables$page == 187] <- as.integer(1)\n\nrm(follow_up)\n# Table 2.3\ntable_2_3 <- all_tables |> \n  filter(table == \"Table 2.3: Distribution of Population by Age, Sex*, County and Sub- County\")\ntable_2_4a <- all_tables |> \n  filter(table == \"Table 2.4a: Distribution of Rural Population by Age, Sex* and County\")\ntable_2_4b <- all_tables |> \n  filter(table == \"Table 2.4b: Distribution of Urban Population by Age, Sex* and County\")\nboth_2_4s <-\n  full_join(\n    table_2_4a,\n    table_2_4b,\n    by = c(\"age\", \"area\", \"area_type\"),\n    suffix = c(\"_rural\", \"_urban\")\n  )\n\nall <-\n  full_join(\n    table_2_3,\n    both_2_4s,\n    by = c(\"age\", \"area\", \"area_type\"),\n    suffix = c(\"_all\", \"_\")\n  )\n\nall <-\n  all |>\n  mutate(\n    page = glue::glue(\n      'Total from p. {page}, rural from p. {page_rural}, urban from p. {page_urban}'\n    )\n  ) |>\n  select(\n    -page,\n    -page_rural,\n    -page_urban,-table,\n    -table_rural,\n    -table_urban,-age_type_rural,\n    -age_type_urban\n  )\n\nrm(both_2_4s, table_2_3, table_2_4a, table_2_4b)\nfollow_up <- \n  all |> \n  mutate(total_from_bits = total_rural + total_urban,\n         check_total_is_rural_plus_urban = if_else(total == total_from_bits, 1, 0),\n         total_from_bits - total) |> \n  filter(check_total_is_rural_plus_urban == 0)\n\nhead(follow_up)\n#> # A tibble: 3 × 16\n#>   age          male female  total area   area_type age_type \n#>   <fct>       <int>  <int>  <int> <chr>  <chr>     <chr>    \n#> 1 Not Stated     31     10     41 Nakuru county    single-y…\n#> 2 Total      434287 441379 875666 Bomet  county    age-group\n#> 3 Not Stated      3      2      5 Bomet  county    single-y…\n#> # … with 9 more variables: male_rural <int>,\n#> #   female_rural <int>, total_rural <int>,\n#> #   male_urban <int>, female_urban <int>,\n#> #   total_urban <int>, total_from_bits <int>,\n#> #   check_total_is_rural_plus_urban <dbl>,\n#> #   `total_from_bits - total` <int>\nrm(follow_up)\nfollow_up <- \n  all |> \n  mutate(groups = case_when(age %in% c(\"0\", \"1\", \"2\", \"3\", \"4\", \"0-4\") ~ \"0-4\",\n                            age %in% c(\"5\", \"6\", \"7\", \"8\", \"9\", \"5-9\") ~ \"5-9\",\n                            age %in% c(\"10\", \"11\", \"12\", \"13\", \"14\", \"10-14\") ~ \"10-14\",\n                            age %in% c(\"15\", \"16\", \"17\", \"18\", \"19\", \"15-19\") ~ \"15-19\",\n                            age %in% c(\"20\", \"21\", \"22\", \"23\", \"24\", \"20-24\") ~ \"20-24\",\n                            age %in% c(\"25\", \"26\", \"27\", \"28\", \"29\", \"25-29\") ~ \"25-29\",\n                            age %in% c(\"30\", \"31\", \"32\", \"33\", \"34\", \"30-34\") ~ \"30-34\",\n                            age %in% c(\"35\", \"36\", \"37\", \"38\", \"39\", \"35-39\") ~ \"35-39\",\n                            age %in% c(\"40\", \"41\", \"42\", \"43\", \"44\", \"40-44\") ~ \"40-44\",\n                            age %in% c(\"45\", \"46\", \"47\", \"48\", \"49\", \"45-49\") ~ \"45-49\",\n                            age %in% c(\"50\", \"51\", \"52\", \"53\", \"54\", \"50-54\") ~ \"50-54\",\n                            age %in% c(\"55\", \"56\", \"57\", \"58\", \"59\", \"55-59\") ~ \"55-59\",\n                            age %in% c(\"60\", \"61\", \"62\", \"63\", \"64\", \"60-64\") ~ \"60-64\",\n                            age %in% c(\"65\", \"66\", \"67\", \"68\", \"69\", \"65-69\") ~ \"65-69\",\n                            age %in% c(\"70\", \"71\", \"72\", \"73\", \"74\", \"70-74\") ~ \"70-74\",\n                            age %in% c(\"75\", \"76\", \"77\", \"78\", \"79\", \"75-79\") ~ \"75-79\",\n                            age %in% c(\"80\", \"81\", \"82\", \"83\", \"84\", \"80-84\") ~ \"80-84\",\n                            age %in% c(\"85\", \"86\", \"87\", \"88\", \"89\", \"85-89\") ~ \"85-89\",\n                            age %in% c(\"90\", \"91\", \"92\", \"93\", \"94\", \"90-94\") ~ \"90-94\",\n                            age %in% c(\"95\", \"96\", \"97\", \"98\", \"99\", \"95-99\") ~ \"95-99\",\n                            TRUE ~ \"Other\")\n         ) |> \n  group_by(area_type, area, groups) |> \n  mutate(group_sum = sum(total, na.rm = FALSE),\n         group_sum = group_sum / 2,\n         difference = total - group_sum) |> \n  ungroup() |> \n  filter(age == groups) |> \n  filter(total != group_sum) \n\nhead(follow_up)\n#> # A tibble: 6 × 16\n#>   age    male female total area           area_type age_type\n#>   <fct> <int>  <int> <int> <chr>          <chr>     <chr>   \n#> 1 0-4       1      5     6 Mt. Kenya For… sub-coun… age-gro…\n#> 2 5-9       1      2     3 Mt. Kenya For… sub-coun… age-gro…\n#> 3 10-14     6      0     6 Mt. Kenya For… sub-coun… age-gro…\n#> 4 15-19     9      1    10 Mt. Kenya For… sub-coun… age-gro…\n#> 5 20-24    21      4    25 Mt. Kenya For… sub-coun… age-gro…\n#> 6 25-29    59      9    68 Mt. Kenya For… sub-coun… age-gro…\n#> # … with 9 more variables: male_rural <int>,\n#> #   female_rural <int>, total_rural <int>,\n#> #   male_urban <int>, female_urban <int>,\n#> #   total_urban <int>, groups <chr>, group_sum <dbl>,\n#> #   difference <dbl>\n\nrm(follow_up)"},{"path":"cleaning-and-preparing-data.html","id":"tidy-up","chapter":"11 Cleaning and preparing data","heading":"11.3.3 Tidy-up","text":"Now confident everything looking good, can just convert tidy format. make easier work .original purpose cleaning dataset make table M. Alexander Alkema (2021). Just bring together, make graph single-year counts, gender, Nairobi (Figure 11.3).\nFigure 11.3: Distribution age gender Nairobi 2019, based Kenyan census\nvariety features clear Figure 11.3, including age-heaping, slight difference ratio male-female birth, substantial difference ages 15 25.","code":"\nall <-\n  all |>\n  rename(male_total = male,\n         female_total = female,\n         total_total = total) |>\n  pivot_longer(\n    cols = c(\n      male_total,\n      female_total,\n      total_total,\n      male_rural,\n      female_rural,\n      total_rural,\n      male_urban,\n      female_urban,\n      total_urban\n    ),\n    names_to = \"type\",\n    values_to = \"number\"\n  ) |>\n  separate(\n    col = type,\n    into = c(\"gender\", \"part_of_area\"),\n    sep = \"_\"\n  ) |>\n  select(area, area_type, part_of_area, age, age_type, gender, number)\n\nhead(all)\n#> # A tibble: 6 × 7\n#>   area  area_type part_of_area age   age_type gender  number\n#>   <chr> <chr>     <chr>        <fct> <chr>    <chr>    <int>\n#> 1 Momb… county    total        Total age-gro… male    610257\n#> 2 Momb… county    total        Total age-gro… female  598046\n#> 3 Momb… county    total        Total age-gro… total  1208303\n#> 4 Momb… county    rural        Total age-gro… male        NA\n#> 5 Momb… county    rural        Total age-gro… female      NA\n#> 6 Momb… county    rural        Total age-gro… total       NA\nmonicas_dataset <- \n  all |> \n  filter(area_type == \"county\") |> \n  filter(part_of_area == \"total\") |>\n  filter(age_type == \"single-year\") |> \n  select(area, age, gender, number)\n\nhead(monicas_dataset)\n#> # A tibble: 6 × 4\n#>   area    age   gender number\n#>   <chr>   <fct> <chr>   <int>\n#> 1 Mombasa 0     male    15111\n#> 2 Mombasa 0     female  15009\n#> 3 Mombasa 0     total   30120\n#> 4 Mombasa 1     male    15805\n#> 5 Mombasa 1     female  15308\n#> 6 Mombasa 1     total   31113\nmonicas_dataset |>\n  filter(area == \"Nairobi\") |>\n  filter(gender != \"total\") |>\n  ggplot(aes(x = age, y = number, fill = gender)) +\n  geom_col(aes(x = age, y = number, fill = gender), position = \"dodge\") +\n  scale_y_continuous(labels = scales::comma) +\n  scale_x_discrete(breaks = c(seq(from = 0, to = 99, by = 5), \"100+\")) +\n  theme_classic()+\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(y = \"Number\",\n       x = \"Age\",\n       fill = \"Gender\",\n       caption = \"Data source: 2019 Kenya Census\")"},{"path":"cleaning-and-preparing-data.html","id":"checks-and-tests","chapter":"11 Cleaning and preparing data","heading":"11.4 Checks and tests","text":"Robert Caro, biographer Lyndon Johnson, spent years tracking everyone connected 36th President United States. went far live Texas Hill Country three years better understand LBJ . heard story LBJ used run Senate senator, ran route multiple times try understand LBJ running. Caro eventually understood ran route sun rising, just LBJ done, sun hits Senate Rotunda particularly inspiring way (Caro 2019). background work enabled uncover aspects one else knew. instance, turns LBJ almost surely stole first election win Texas Senator (Caro 2019). need understand data extent. must turn every page go every extreme.idea negative space well established design. refers surrounds subject. Sometimes negative space used effect, instance logo FedEx, American logistics company, negative space E x creates arrow. similar way, want cognizant data , data . worried data somehow meaning, potentially even extent changing conclusions. cleaning data, looking anomalies. interested values , also opposite situation—values missing . four tools use identify situations: graphs, counts, green/red conditions, targets.","code":""},{"path":"cleaning-and-preparing-data.html","id":"graphs-1","chapter":"11 Cleaning and preparing data","heading":"11.4.1 Graphs","text":"Graphs invaluable tool cleaning data, show point dataset, relation points. especially useful identifying value belong. instance, value expected numerical, still character plot warning displayed.Graphs especially useful numerical data, still useful text categorical data. Let us pretend situation interested person’s age, youth survey. following data:graph clearly shows unexpected value 150. likely explanation data incorrectly entered trailing 0, 15. can fix , document , redo graph, see everything seems reasonable now.","code":"\nraw_data <- \n  tibble(ages = c(11, 17, 22, 13, 21, 16, 16, 6, 16, 11, 150))\n\nraw_data |> \n  ggplot(aes(y = ages, x = 0)) +\n  geom_point()"},{"path":"cleaning-and-preparing-data.html","id":"counts","chapter":"11 Cleaning and preparing data","heading":"11.4.2 Counts","text":"want focus getting data right. interested counts unique values. Hopefully majority data concentrated common counts. can also useful invert , see especially uncommon. extent want deal depends need. Ultimately, time fix one getting additional observations, potentially even just one! Counts especially useful text categorical data, can helpful numerical well.Let us see example.use count clearly identifies spend time - changing ‘Australie’ ‘Australia’ almost double amount usable data.","code":"\nraw_data <- \n  tibble(country = c('Australie', 'Austrelia', 'Australie', 'Australie', 'Aeustralia', 'Austraia', 'Australia', 'Australia', 'Australia', 'Australia'\n                  )\n         )\n\nraw_data |> \n  count(country, sort = TRUE)\n#> # A tibble: 5 × 2\n#>   country        n\n#>   <chr>      <int>\n#> 1 Australia      4\n#> 2 Australie      3\n#> 3 Aeustralia     1\n#> 4 Austraia       1\n#> 5 Austrelia      1"},{"path":"cleaning-and-preparing-data.html","id":"gono-go","chapter":"11 Cleaning and preparing data","heading":"11.4.3 Go/no-go","text":"things important require cleaned dataset . go/-go conditions. typically come experience, expert knowledge, planning simulation exercises. example may negative numbers age column, ages 140.specifically require condition met. examples include:cross-country analysis, list country names know dataset useful. -go conditions : 1) values list dataset, , vice versa; 2) countries expected .concrete example, let us consider analysis five largest counties Kenya: ‘Nairobi,’ ‘Kiambu,’ ‘Nakuru,’ ‘Kakamega,’ ‘Bungoma.’ Let us create array first.begin following dataset.Based count know fix two numbers obvious fixes.point can use go/-go conditions decide whether finished .clear still cleaning !may also find similar conditions experts experience particular field.","code":"\ncorrect_counties <- c('Nairobi', 'Kiambu', 'Nakuru', 'Kakamega', 'Bungoma')\ntop_five_kenya <- \n  tibble(county = c('Nairobi', 'Nairob1', 'Nakuru', 'Kakamega', 'Nakuru', \n                      'Kiambu', 'Kiambru', 'Kabamega', 'Bun8oma', 'Bungoma')\n  )\n\ntop_five_kenya |> \n  count(county, sort = TRUE)\n#> # A tibble: 9 × 2\n#>   county       n\n#>   <chr>    <int>\n#> 1 Nakuru       2\n#> 2 Bun8oma      1\n#> 3 Bungoma      1\n#> 4 Kabamega     1\n#> 5 Kakamega     1\n#> 6 Kiambru      1\n#> 7 Kiambu       1\n#> 8 Nairob1      1\n#> 9 Nairobi      1\ntop_five_kenya <- \n  top_five_kenya |> \n  mutate(county = str_replace_all(county, 'Nairob1', 'Nairobi'),\n         county = str_replace_all(county, 'Bun8oma', 'Nairobi')\n  )\n\ntop_five_kenya |> \n  count(county, sort = TRUE)\n#> # A tibble: 7 × 2\n#>   county       n\n#>   <chr>    <int>\n#> 1 Nairobi      3\n#> 2 Nakuru       2\n#> 3 Bungoma      1\n#> 4 Kabamega     1\n#> 5 Kakamega     1\n#> 6 Kiambru      1\n#> 7 Kiambu       1\ntop_five_kenya$county |> unique()\n#> [1] \"Nairobi\"  \"Nakuru\"   \"Kakamega\" \"Kiambu\"   \"Kiambru\" \n#> [6] \"Kabamega\" \"Bungoma\"\n\nif(all(top_five_kenya$county |> unique() == top_five_kenya)) {\n  \"Oh no\"\n}\nif(all(top_five_kenya==top_five_kenya$county |> unique()) ) {\n  \"Oh no\"\n}"},{"path":"cleaning-and-preparing-data.html","id":"class-1","chapter":"11 Cleaning and preparing data","heading":"11.4.4 Class","text":"often said American society obsessed money, British society obsessed class. case data cleaning preparation need British. Explicit checks class variables essential. Accidentally assigning wrong class variable can large effect subsequent analysis. particular:check whether value number factor; andcheck dates correctly formatted.understand important clear whether value number factor, consider following situation:Let us start ‘group’ integer look logistic regression.Now can try factor. interpretation variable completely different.Another critical aspect check dates. particular want try make following format: YYYY-MM-DD. course differences opinion appropriate date format broader world, reasonable people can differ whether 1 July 2010 July 1, 2020, better, YYYY-MM-DD format generally appropriate data.","code":"\nsome_data <- \n  tibble(response = c(1, 1, 0, 1, 0, 1, 1, 0, 0),\n         group = c(1, 2, 1, 1, 2, 3, 1, 2, 3)) |> \n  mutate(group_as_integer = as.integer(group),\n         group_as_factor = as.factor(group),\n         )\nlm(response~group_as_integer, data = some_data) |> \n  summary()\n#> \n#> Call:\n#> lm(formula = response ~ group_as_integer, data = some_data)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#>  -0.68  -0.52   0.32   0.32   0.64 \n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)        0.8400     0.4495   1.869    0.104\n#> group_as_integer  -0.1600     0.2313  -0.692    0.511\n#> \n#> Residual standard error: 0.5451 on 7 degrees of freedom\n#> Multiple R-squared:  0.064,  Adjusted R-squared:  -0.06971 \n#> F-statistic: 0.4786 on 1 and 7 DF,  p-value: 0.5113\nlm(response~group_as_factor, data = some_data) |> \n  summary()\n#> \n#> Call:\n#> lm(formula = response ~ group_as_factor, data = some_data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -0.7500 -0.3333  0.2500  0.2500  0.6667 \n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)        0.7500     0.2826   2.654   0.0378 *\n#> group_as_factor2  -0.4167     0.4317  -0.965   0.3717  \n#> group_as_factor3  -0.2500     0.4895  -0.511   0.6278  \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.5652 on 6 degrees of freedom\n#> Multiple R-squared:  0.1375, Adjusted R-squared:  -0.15 \n#> F-statistic: 0.4783 on 2 and 6 DF,  p-value: 0.6416"},{"path":"cleaning-and-preparing-data.html","id":"naming-things","chapter":"11 Cleaning and preparing data","heading":"11.5 Naming things","text":"improved scanning software developed identified gene name errors 30.9% (3,436/11,117) articles supplementary Excel gene lists; figure significantly higher previously estimated. due gene names converted just dates floating-point numbers, also internal date format (five-digit numbers).Abeysooriya et al. (2021)Names matter. land much book written today named Toronto, within country named Canada, long time known Turtle Island. common, days people sometimes still refer Turtle Island. tells us something , use name Canada tells something us. big rock center Australia. long time, called Uluru, known Ayers Rock. Today dual name combines , choice name use tells someone something . Even British Royal Family recognize power names. 1917 changed House Saxe-Coburg Gotha House Windsor, due feeling former Germanic given World War ongoing. Names matter everyday life. matter data science .importance names, ignoring existing claims re-naming clear cases, see data science well. need careful name datasets, variables, functions. tendency, days, call variable ‘gender’ even though may male female, want say word ‘sex.’ Tukey (1962) essentially defines today call data science, popularized folks computer science 2010s ignored, either deliberately ignorance, came . past ten years characteristic renaming concepts well-established fields computer science recently expanded . instance, use binary variables regression, sometimes called ‘dummy variables,’ called one-hot encoding computer science. Like fashions, one pass also. recently saw 1980s early 2010s economics. Economists described ‘queen social sciences’ self-described imperialistic (Lazear 2000). now recognizing costs imperialism social sciences, future look back count cost computer science imperialism data science. key area study ever terra nullius, nobody’s land. important recognize, adopt, use existing names, practices.Names give places meaning, ignoring existing names, ignore come us. Kimmerer (2012, 34) describes ‘Tahawus Algonquin name Mount Marcy, highest peak Adirondacks. ’s called Mount March commemorate governor never set foot wild slopes.’ continues ‘[w]hen call place name transformed wilderness homeland.’ talking regard physical places, true function names, variable names dataset names. use gender instead sex want say sex front others, ignore preferences provided data.addition respecting nature data, names need satisfy two additional considerations:need machine readable, andthey need human readable.Machine readable names easier standard meet, usually means avoiding spaces special characters. space can replaced underbar. Usually, special characters just removed can inconsistent different computers languages. names also unique within dataset, unique within collection datasets unless particular column deliberately used key join different datasets.especially useful function use get closer machine readable names janitor::clean_names() janitor package (Firke 2020). deals issues mentioned well others. can see example.Human readable names require additional layer. need consider cultures may interpret names using. also need consider different experience levels subsequent users dataset may . terms experience programming statistics, also experience similar datasets. instance, column ‘flag’ often used signal column contains data needs followed treated carefully way. experienced analyst know , beginner . Try use meaningful names wherever possible (Lin, Ali, Wilson 2020). found shorter names may take longer comprehend (Hofmeister, Siegmund, Holt 2017), often useful avoid abbreviations possible.One interesting feature R certain cases partial matching names possible. instance:behavior possible within tidyverse, instance data.frame replaced tibble code. Partial matching almost never used. makes difficult understand code break, others come fresh.Riederer (2020) advises using column names contracts, establishing controlled vocabulary column names. way, define set words can use column names. controlled vocabulary Riederer (2020) column start abbreviation class, something specific pertains , various details. instance, Kenyan data example earlier following column names: “area,” “age,” “gender,” “number.” use column names contracts, : “chr_area,” “fctr_group_age,” “chr_group_gender,” “int_group_count.”can use pointblank (Iannone Vargas 2022) set-tests us.\n    &marker;chr_area\n  ✓————\n    &marker;chr_group_gender\n  ✓————\n    &marker;fctr_group_age\n  ✓————\n    &marker;int_group_count\n  ✓————\n    &marker;chr_group_gender\n  male, female, total✓————","code":"\nbad_names_good_names <- \n  tibble(\n    'First' = c(1),\n    'second name has spaces' = c(1),\n    'weird#symbol' = c(1),\n    'InCoNsIsTaNtCaPs' = c(1)\n  )\n\nbad_names_good_names\n#> # A tibble: 1 × 4\n#>   First `second name has s…` `weird#symbol` InCoNsIsTaNtCaPs\n#>   <dbl>                <dbl>          <dbl>            <dbl>\n#> 1     1                    1              1                1\n\nbad_names_good_names <- \n  bad_names_good_names |> \n  janitor::clean_names()\n  \nbad_names_good_names\n#> # A tibble: 1 × 4\n#>   first second_name_has_s… weird_number_sy… in_co_ns_is_ta_…\n#>   <dbl>              <dbl>            <dbl>            <dbl>\n#> 1     1                  1                1                1\nnever_use_partial_matching <- \n  data.frame(\n    my_first_name = c(1, 2),\n    another_name = c(\"wow\", \"great\")\n  )\n\nnever_use_partial_matching$my_first_name\n#> [1] 1 2\nnever_use_partial_matching$my\n#> [1] 1 2\ncolumn_names_as_contracts <- \n  monicas_dataset |> \n  rename(\n    \"chr_area\" = \"area\",\n    \"fctr_group_age\" = \"age\",\n    \"chr_group_gender\" = \"gender\",\n    \"int_group_count\" = \"number\"\n  )\nlibrary(pointblank)\n\nagent <-\n  create_agent(tbl = column_names_as_contracts) |>\n  col_is_character(columns = vars(chr_area, chr_group_gender)) |>\n  col_is_factor(columns = vars(fctr_group_age)) |>\n  col_is_integer(columns = vars(int_group_count)) |>\n  col_vals_in_set(columns = chr_group_gender,\n                  set = c(\"male\", \"female\", \"total\")) |>\n  interrogate()\n\nagent"},{"path":"cleaning-and-preparing-data.html","id":"exercises-and-tutorial-10","chapter":"11 Cleaning and preparing data","heading":"11.6 Exercises and tutorial","text":"","code":""},{"path":"cleaning-and-preparing-data.html","id":"exercises-10","chapter":"11 Cleaning and preparing data","heading":"11.6.1 Exercises","text":"following example tidy data?dealing ages likely class variable? [Select apply.]\ninteger\nmatrix\nnumeric\nfactor\nintegermatrixnumericfactor","code":"\ntibble(name = c('Anne', 'Bethany', 'Stephen', 'William'),\n       age_group = c('18-29', '30-44', '45-60', '60+'),\n       )\n#> # A tibble: 4 × 2\n#>   name    age_group\n#>   <chr>   <chr>    \n#> 1 Anne    18-29    \n#> 2 Bethany 30-44    \n#> 3 Stephen 45-60    \n#> 4 William 60+"},{"path":"cleaning-and-preparing-data.html","id":"tutorial-10","chapter":"11 Cleaning and preparing data","heading":"11.6.2 Tutorial","text":"regard Jordan (2019), D’Ignazio Klein (2020), Chapter 6, Au (2020), relevant work, extent think let data speak ? [Please write page two.]","code":""},{"path":"storing-and-retrieving-data.html","id":"storing-and-retrieving-data","chapter":"12 Storing and retrieving data","heading":"12 Storing and retrieving data","text":"Required materialRead Datasheets datasets, (Gebru et al. 2021).Key concepts skillsFAIR principles data sharing management.Sharing data using GitHubCreating R data packagesDepositing dataData documentation especially datasheets datasets","code":""},{"path":"storing-and-retrieving-data.html","id":"introduction-9","chapter":"12 Storing and retrieving data","heading":"12.1 Introduction","text":"put together dataset, important part responsible storing appropriately enabling easy retrieval. certainly possible especially concerned , entire careers based storage retrieval data, certain extent, baseline onerous. can get dataset computer, much way . confirming someone else can retrieve use , puts us much .said, FAIR principles useful come think formally data sharing management. (M. D. Wilkinson et al. 2016):Findable. one, unchanging, identifier dataset dataset high-quality descriptions explanations.Accessible. Standardized approaches can used retrieve data, open free, possibly authentication, metadata persists even dataset removed.Interoperable. dataset metadata use broadly applicable language, vocabulary.Reusable. plenty description dataset usage conditions made clear along provenance.important recognize just dataset FAIR, necessarily unbiased representation world. FAIR reflects whether dataset appropriately available, whether appropriate.chapter consider plan organize datasets meet essential requirements. large extent put place make life easier come back use dataset later. go putting dataset GitHub, building R packages data, finally depositing various archives. Finally, documentation, particular focus datasheets.","code":""},{"path":"storing-and-retrieving-data.html","id":"plan-3","chapter":"12 Storing and retrieving data","heading":"12.2 Plan","text":"storage retrieval information long history. especially connected libraries existed since antiquity established protocols deciding information store discard, well retrieval. One defining aspects libraries deliberate curation organization. cataloging system ensures books similar topics located close , typically also deliberate plans ensuring collection --date.Vannevar Bush defines ‘memex’ 1945 device used store books, records communications, way supplements memory (Bush 1945). key indexing, linking together items. can see concept echoed Tim Berners-Lee proposal hypertext (Berners-Lee 1989), led World Wide Web. way resources identified. transported Internet, using HTTP.fundamental, storing retrieving data. instance, make various files computer available others. internet famously brittle, considering storage retrieval datasets want consider especially, long important data stored (Michener 2015). instance, want data available decade widely available becomes important store data open persistent formats, CSVs (Hart et al. 2016). just using data part intermediate step, raw data, scripts create , might fine worry much considerations.Storing raw data important many cases raw data revealed hinted fraud (Simonsohn 2013). Shared data also enhances credibility work, enabling others verify , can lead generation new knowledge others use answer different questions (Christensen, Freese, Miguel 2019). Finally, research shares data may highly cited (Christensen et al. 2019).","code":""},{"path":"storing-and-retrieving-data.html","id":"github-1","chapter":"12 Storing and retrieving data","heading":"12.3 GitHub","text":"easiest place store datasets GitHub already built workflow. instance, push, dataset becomes available. One great benefit , set-workspace appropriately, likely store raw data, tidy data, well scripts needed transform one .example stored data, can access ‘raw_data.csv’ file ‘starter_folder.’ get file pass read_csv() navigate file GitHub, click ‘Raw.’One issue dataset much documentation. can store retrieve dataset easily way, lacks much explanation, formal dictionary, aspects license bring dataset closer aligning FAIR principles.","code":"\nlibrary(tidyverse)\n\nstarter_data <- read_csv(\"https://raw.githubusercontent.com/RohanAlexander/starter_folder/main/inputs/data/raw_data.csv\")\n\nstarter_data\n#> # A tibble: 1 × 3\n#>   first_col second_col third_col\n#>   <chr>     <chr>      <chr>    \n#> 1 some      raw        data"},{"path":"storing-and-retrieving-data.html","id":"r-packages-for-data","chapter":"12 Storing and retrieving data","heading":"12.4 R Packages for data","text":"point largely used R packages code, although seen focused sharing data, instance, Flynn (2021). can build R Package dataset add GitHub. make easy store retrieve can obtain dataset loading package. first R package build. Chapter 20, return R packages use deploy models.get started, create new package (‘File’ -> ‘New project’ -> ‘New Directory’ -> ‘R Package’). Give package name, ‘favcolordata’ select ‘Open new session.’ Create new folder called ‘data.’ simulate dataset include.point largely trying use CSV files datasets. include data R package, save dataset particular format, ‘rda,’ using save().create R file ‘data.R’ ‘R’ folder. R file contain documentation using roxygen2 comments, start #', follow documentation troopdata closely.Finally, add README provides summary someone coming project outside.can go ‘Build’ tab ‘Install Restart.’ happens, package ‘favcolordata,’ loaded data can accessed using ‘color_data.’ push GitHub, anyone able install package using devtools (Wickham, Hester, Chang 2020) use dataset.addressed many issues faced earlier. instance, included README data dictionary sorts terms descriptions added. try put package onto CRAN, might face issues. instance, maximum size package 5MB quickly come . also largely forced users use R, considerable benefits , may like language agnostic (N. J. Tierney Ram 2020).","code":"\nlibrary(tidyverse)\n\nset.seed(853)\n\ncolor_data <-\n    tibble(\n        name =\n            c(\"Edward\",\n              \"Helen\",\n              \"Hugo\",\n              \"Ian\",\n              \"Monica\",\n              \"Myles\",\n              \"Patricia\",\n              \"Roger\",\n              \"Rohan\",\n              \"Ruth\"\n            ),\n        fav_color =\n            sample(\n                x = c(\"Black\", \"White\", \"Rainbow\"),\n                size = 10,\n                replace = TRUE\n            )\n    )\nsave(color_data, file=\"data/color_data.rda\")\n#' Favorite color of various people data\n#'\n#' @description \\code{favcolordata} returns a dataframe of the favorite color of various people.\n#' \n#' @return Returns a dataframe of the favorite color of various people.\n#' \n#' @docType data\n#'\n#' @usage data(color_data)\n#'\n#' @format An dataframe of individual-level observations with the following variables: \n#'\n#' \\describe{\n#' \\item{\\code{name}}{A character vector of individual names.}\n#' \\item{\\code{fav_color}}{A character vector of one of: black, white, rainbow.}\n#' }\n#'\n#' @keywords datasets\n#'\n#' @source \\url{https://www.tellingstorieswithdata.com/storing-and-retrieving-data.html}\n#'\n\"color_data\"\ndevtools::install_github(\"RohanAlexander/favcolordata\")\n\nlibrary(favcolordata)\n\ncolor_data"},{"path":"storing-and-retrieving-data.html","id":"depositing-data","chapter":"12 Storing and retrieving data","heading":"12.5 Depositing data","text":"possible dataset cited available GitHub R package, becomes likely dataset deposited somewhere. number reasons , one seems bit formal. Zenodo Open Science Framework (OSF) two commonly used. instance, C. Carleton (2021) use Zenodo share dataset analysis supporting W. C. Carleton, Campbell, Collard (2021). Similarly Michael et al. (2021) use Zenodo share dataset underpins Geuenich et al. (2021).\nAnother option dataverse, Harvard Dataverse, common requirement journal publications. One nice aspect can use dataverse (Kuriwaki, Beasley, Leeper 2022) retrieve dataset part reproducible workflow.","code":""},{"path":"storing-and-retrieving-data.html","id":"documentation","chapter":"12 Storing and retrieving data","heading":"12.6 Documentation","text":"Datasheets (Gebru et al. 2021) increasingly critical aspect data science. Datasheets basically nutrition labels datasets. process creating enables us think carefully feed model. importantly, enable others better understand fed model. One important task going back putting together datasheets datasets widely used. instance, researchers went back wrote datasheet one popular datasets computer science, found around 30 per cent data duplicated (Bandy Vincent 2021).Instead telling us unhealthy various foods , datasheet tells us things like:put dataset together?paid dataset created?complete dataset?fields present, equally present, particular observations?Sometimes done lot work create dataset. case, may like publish share , instance Biderman, Bicheno, Gao (2022) Bandy Vincent (2021). typically datasheet might live appendix paper, included file adjacent dataset.put together datasheet dataset underpins R. Alexander Hodgetts (2021). text questions directly comes Gebru et al. (2021). create datasheets dataset, especially dataset put together , possible answer questions simply “Unknown.”MotivationFor purpose dataset created? specific task mind? specific gap needed filled? Please provide description.\ndataset created enable analysis Australian politicians. unable find publicly available dataset structured format biographical political information Australian politicians needed modelling.\ndataset created enable analysis Australian politicians. unable find publicly available dataset structured format biographical political information Australian politicians needed modelling.created dataset (example, team, research group) behalf entity (example, company, institution, organization)?\nRohan Alexander, working Australian National University University Toronto\nRohan Alexander, working Australian National University University TorontoWho funded creation dataset? associated grant, please provide name grantor grant name number.\ndirect funding received project, Rohan received salary University Toronto.\ndirect funding received project, Rohan received salary University Toronto.comments?\n.\n.CompositionWhat instances comprise dataset represent (example, documents, photos, people, countries)? multiple types instances (example, movies, users, ratings; people interactions ; nodes edges)? Please provide description.\nrow main dataset individual, link datasets row refers various information person.\nrow main dataset individual, link datasets row refers various information person.many instances total (type, appropriate)?\nlittle 1700.\nlittle 1700.dataset contain possible instances sample (necessarily random) instances larger set? dataset sample, larger set? sample representative larger set (example, geographic coverage)? , please describe representativeness validated/verified. representative larger set, please describe (example, cover diverse range instances, instances withheld unavailable).\nindividuals elected appointed Australian Federal Parliament dataset.\nindividuals elected appointed Australian Federal Parliament dataset.data instance consist ? “Raw” data (example, unprocessed text images) features? either case, please provide description.\ninstance consists biographical information birthdate, political information, political party membership.\ninstance consists biographical information birthdate, political information, political party membership.label target associated instance? , please provide description.\nYes unique key comprising surname year birth, individuals needing additional demarcation.\nYes unique key comprising surname year birth, individuals needing additional demarcation.information missing individual instances? , please provide description, explaining information missing (example, unavailable). include intentionally removed information, might include, example, redacted text.\nBirthdate available cases, especially earlier dataset.\nBirthdate available cases, especially earlier dataset.relationships individual instances made explicit (example, users’ movie ratings, social network links)? , please describe relationships made explicit.\nYes, uniqueID.\nYes, uniqueID.recommended data splits (example, training, development/validation, testing)? , please provide description splits, explaining rationale behind .\n.\n.errors, sources noise, redundancies dataset? , please provide description.\nuncertainty cabinet ministries. instance, different sources differ. also little bit uncertainty birthdates.\nuncertainty cabinet ministries. instance, different sources differ. also little bit uncertainty birthdates.dataset self-contained, link otherwise rely external resources (example, websites, tweets, datasets)? links relies external resources, ) guarantees exist, remain constant, time; b) official archival versions complete dataset (, including external resources existed time dataset created); c) restrictions (example, licenses, fees) associated external resources might apply dataset consumer? Please provide descriptions external resources restrictions associated , well links access points, appropriate.\nSelf-contained.\nSelf-contained.dataset contain data might considered confidential (example, data protected legal privilege doctor-patient confidentiality, data includes content individuals’ non-public communications)? , please provide description.\n, data gathered public sources.\n, data gathered public sources.dataset contain data , viewed directly, might offensive, insulting, threatening, might otherwise cause anxiety? , please describe .\n.\n.dataset identify sub-populations (example, age, gender)? , please describe subpopulations identified provide description respective distributions within dataset.\nYes, age gender.\nYes, age gender.possible identify individuals (, one natural persons), either directly indirectly (, combination data) dataset? , please describe .\nYes, individuals identified name.\nYes, individuals identified name.dataset contain data might considered sensitive way (example, data reveals race ethnic origins, sexual orientations, religious beliefs, political opinions union memberships, locations; financial health data; biometric genetic data; forms government identification, social security numbers; criminal history)? , please provide description.\ndataset contains sensitive information, political membership, however public knowledge federal politicians.\ndataset contains sensitive information, political membership, however public knowledge federal politicians.comments?\n.\n.Collection processHow data associated instance acquired? data directly observable (example, raw text, movie ratings), reported subjects (example, survey responses), indirectly inferred/derived data (example, part--speech tags, model-based guesses age language)? data reported subjects indirectly inferred/derived data, data validated/verified? , please describe .\ndata gathered Australian Parliamentary Handbook first instance, augmented information parliaments, especially Victoria New South Wales, Wikipedia.\ndata gathered Australian Parliamentary Handbook first instance, augmented information parliaments, especially Victoria New South Wales, Wikipedia.mechanisms procedures used collect data (example, hardware apparatuses sensors, manual human curation, software programs, software APIs)? mechanisms procedures validated?\nScraping parsing using R.\nScraping parsing using R.dataset sample larger set, sampling strategy (example, deterministic, probabilistic specific sampling probabilities)?\ndataset sample.\ndataset sample.involved data collection process (example, students, crowdworkers, contractors) compensated (example, much crowdworkers paid)?\nRohan Alexander. Paid post-doc assistant professor, although tied specific project.\nRohan Alexander. Paid post-doc assistant professor, although tied specific project.timeframe data collected? timeframe match creation timeframe data associated instances (example, recent crawl old news articles)? , please describe timeframe data associated instances created.\nThree years, updated time time.\nThree years, updated time time.ethical review processes conducted (example, institutional review board)? , please provide description review processes, including outcomes, well link access point supporting documentation.\n.\n.collect data individuals question directly, obtain via third parties sources (example, websites)?\nThird parties almost cases.\nThird parties almost cases.individuals question notified data collection? , please describe (show screenshots information) notice provided, provide link access point , otherwise reproduce, exact language notification .\n.\n.individuals question consent collection use data? , please describe (show screenshots information) consent requested provided, provide link access point , otherwise reproduce, exact language individuals consented.\n.\n.consent obtained, consenting individuals provided mechanism revoke consent future certain uses? , please provide description, well link access point mechanism (appropriate).\nConsent obtained.\nConsent obtained.analysis potential impact dataset use data subjects (example, data protection impact analysis) conducted? , please provide description analysis, including outcomes, well link access point supporting documentation.\n.\n.comments?\n.\n.Preprocessing/cleaning/labelingWas preprocessing/cleaning/labeling data done (example, discretization bucketing, tokenization, part--speech tagging, SIFT feature extraction, removal instances, processing missing values)? , please provide description. , may skip remaining questions section.\nYes cleaning data done.\nYes cleaning data done.“raw” data saved addition preprocessed/cleaned/labeled data (example, support unanticipated future uses)? , please provide link access point “raw” data.\ngeneral, . scripts got data parliamentary handbook CSV available. scripts go Wikipedia check things available.\ngeneral, . scripts got data parliamentary handbook CSV available. scripts go Wikipedia check things available.software used preprocess/clean/label data available? , please provide link access point.\nR used.\nR used.comments?\n\nNoUsesHas dataset used tasks already? , please provide description.\nYes, papers Australian politics, instance, https://arxiv.org/abs/2111.09299.\nYes, papers Australian politics, instance, https://arxiv.org/abs/2111.09299.repository links papers systems use dataset? , please provide link access point.\n\nNoWhat () tasks dataset used ?\nLinking elections interesting.\nLinking elections interesting.anything composition dataset way collected preprocessed/cleaned/labeled might impact future uses? example, anything dataset consumer might need know avoid uses result unfair treatment individuals groups (example, stereotyping, quality service issues) risks harms (example, legal risks, financial harms)? , please provide description. anything dataset consumer mitigate risks harms?\n.\n.tasks dataset used? , please provide description.\n.\n.comments?\n.\n.DistributionWill dataset distributed third parties outside entity (example, company, institution, organization) behalf dataset created? , please provide description.\ndataset available GitHub.\ndataset available GitHub.dataset distributed (example, tarball website, API, GitHub)? dataset digital object identifier (DOI)?\nGitHub now, eventually deposit.\nGitHub now, eventually deposit.dataset distributed?\ndataset available now.\ndataset available now.dataset distributed copyright intellectual property (IP) license, /applicable terms use (ToU)? , please describe license / ToU, provide link access point , otherwise reproduce, relevant licensing terms ToU, well fees associated restrictions.\n. MIT license.\n. MIT license.third parties imposed IP-based restrictions data associated instances? , please describe restrictions, provide link access point , otherwise reproduce, relevant licensing terms, well fees associated restrictions.\nNone known.\nNone known.export controls regulatory restrictions apply dataset individual instances? , please describe restrictions, provide link access point , otherwise reproduce, supporting documentation.\nNone known.\nNone known.comments?\n.\n.MaintenanceWho supporting/hosting/maintaining dataset?\nRohan Alexander\nRohan AlexanderHow can owner/curator/manager dataset contacted (example, email address)?\nrohan.alexander@utoronto\nrohan.alexander@utorontoIs erratum? , please provide link access point.\n, dataset just updated.\n, dataset just updated.dataset updated (example, correct labeling errors, add new instances, delete instances)? , please describe often, , updates communicated dataset consumers (example, mailing list, GitHub)?\nYes, roughly quarterly.\nYes, roughly quarterly.dataset relates people, applicable limits retention data associated instances (example, individuals question told data retained fixed period time deleted)? , please describe limits explain enforced.\n.\n.older versions dataset continue supported/hosted/maintained? , please describe . , please describe obsolescence communicated dataset consumers.\ndataset just updated. Although history available GitHub.\ndataset just updated. Although history available GitHub.others want extend/augment/build /contribute dataset, mechanism ? , please provide description. contributions validated/verified? , please describe . , ? process communicating/distributing contributions dataset consumers? , please provide description.\nPull request GitHub.\nPull request GitHub.comments?\n\n","code":""},{"path":"storing-and-retrieving-data.html","id":"exercises-and-tutorial-11","chapter":"12 Storing and retrieving data","heading":"12.7 Exercises and tutorial","text":"","code":""},{"path":"storing-and-retrieving-data.html","id":"exercises-11","chapter":"12 Storing and retrieving data","heading":"12.7.1 Exercises","text":"According Gebru et al. (2021), datasheet document dataset’s (please select apply):\ncomposition.\nrecommended uses.\nmotivation.\ncollection process.\ncomposition.recommended uses.motivation.collection process.Following M. D. Wilkinson et al. (2016), following FAIR principles (please select apply)?\nFindable.\nApproachable.\nInteroperable.\nReusable.\nIntegrated.\nFungible.\nReduced.\nAccessible.\nFindable.Approachable.Interoperable.Reusable.Integrated.Fungible.Reduced.Accessible.Please create R package simulated dataset, push GitHub, submit link.","code":""},{"path":"storing-and-retrieving-data.html","id":"tutorial-11","chapter":"12 Storing and retrieving data","heading":"12.7.2 Tutorial","text":"Please identify important dataset datasheet (Gebru et al. 2021). reminder, datasheets accompany datasets document ‘motivation, composition, collection process, recommended uses,’ among aspects. Please put together datasheet dataset. welcome use template starting point. datasheet completely contained GitHub repository. Please submit PDF.","code":""},{"path":"disseminating-and-protecting-data.html","id":"disseminating-and-protecting-data","chapter":"13 Disseminating and protecting data","heading":"13 Disseminating and protecting data","text":"STATUS: construction.Required materialKey concepts skillsKey librariesKey functions","code":""},{"path":"disseminating-and-protecting-data.html","id":"introduction-10","chapter":"13 Disseminating and protecting data","heading":"13.1 Introduction","text":"","code":""},{"path":"disseminating-and-protecting-data.html","id":"what-is-personally-identifying-information","chapter":"13 Disseminating and protecting data","heading":"13.2 What is personally identifying information","text":"Zook et al. (2017)","code":""},{"path":"disseminating-and-protecting-data.html","id":"hashing-and-salting","chapter":"13 Disseminating and protecting data","heading":"13.3 Hashing and salting","text":"","code":""},{"path":"disseminating-and-protecting-data.html","id":"gdpr-and-hipaa","chapter":"13 Disseminating and protecting data","heading":"13.4 GDPR and HIPAA","text":"","code":""},{"path":"disseminating-and-protecting-data.html","id":"making-fake-data-to-distribute-when-you-cant-share-the-real-stuff","chapter":"13 Disseminating and protecting data","heading":"13.5 Making fake data to distribute when you can’t share the real stuff","text":"","code":""},{"path":"disseminating-and-protecting-data.html","id":"differential-privacy","chapter":"13 Disseminating and protecting data","heading":"13.6 Differential privacy","text":"Kenny et al. (2021)Ruggles et al. (2019)Suriyakumar et al. (2020)","code":""},{"path":"disseminating-and-protecting-data.html","id":"exercises-and-tutorial-12","chapter":"13 Disseminating and protecting data","heading":"13.7 Exercises and tutorial","text":"","code":""},{"path":"disseminating-and-protecting-data.html","id":"exercises-12","chapter":"13 Disseminating and protecting data","heading":"13.7.1 Exercises","text":"","code":""},{"path":"disseminating-and-protecting-data.html","id":"tutorial-12","chapter":"13 Disseminating and protecting data","heading":"13.7.2 Tutorial","text":"","code":""},{"path":"disseminating-and-protecting-data.html","id":"paper-3","chapter":"13 Disseminating and protecting data","heading":"13.7.3 Paper","text":"point, Paper Four (Appendix B.4) appropriate.","code":""},{"path":"exploratory-data-analysis.html","id":"exploratory-data-analysis","chapter":"14 Exploratory data analysis","heading":"14 Exploratory data analysis","text":"STATUS: construction.Required readingBarocas, Solon, Danah Boyd, 2017, ‘Engaging ethics data science practice,’ Communications ACM, 60.11 (2017): 23-25.DiCiccio, Thomas J., Mary E. Thompson, 2004, ‘Conversation Donald . S. Fraser,’ Statistical Science, 19 (2) pp. 370-386, https://utstat.toronto.edu/craiu/DonFraser_SSInterview.pdf.Jordan, Michael , 2019, ‘AI - revolution hasn’t started yet,’ Harvard Data Science Review, 1 July, https://hdsr.mitpress.mit.edu/pub/wot7mkc1.Tukey, John W., 1961, ‘Future Data Analysis,’ annals mathematical statistics, Part 1 ‘General Considerations,’ https://projecteuclid.org/journals/annals--mathematical-statistics/volume-33/issue-1/-Future--Data-Analysis/10.1214/aoms/1177704711.full.Wickham, Hadley, Garrett Grolemund, 2017, R Data Science, Chapters 3 7, https://r4ds..co.nz/.Key concepts/skills/etcQuickly coming terms new dataset constructing graphs tables.Understanding issues features dataset may affect modelling decisions.Thinking missing values outliers.Key librariesbroomggrepelherejanitorlubridateopendatatorontotidymodelstidyversevisdatKey functions/etcaugment()clean_names()coord_flip()count()distinct()facet_grid()facet_wrap()geom_bar()geom_col()geom_density()geom_histogram()geom_line()geom_point()geom_smooth()geom_text_repel()get_dupes()glance()if_else()ifelse()initial_split()left_join()mutate()mutate_all()names()ncol()nrow()pivot_wider()scale_color_brewer()scale_fill_brewer()scale_x_log10()scale_y_log10()str_detect()str_extract()str_remove()str_split()str_starts()summarise()summarise_all()theme_classic()theme_minimal()vis_dat()vis_miss()","code":""},{"path":"exploratory-data-analysis.html","id":"introduction-11","chapter":"14 Exploratory data analysis","heading":"14.1 Introduction","text":"future data analysis can involve great progress, overcoming real difficulties, provision great service fields science technology. ? remains us, willingness take rocky road real problems preference smooth road unreal assumptions, arbitrary criteria, abstract results without real attachments. challenge?Tukey (1962, 64).Exploratory data analysis never finished, just die. active process exploring becoming familiar data. Like farmer hands earth, need know every contour aspect data. need know changes, shows, hides, limits. Exploratory data analysis unstructured process .said, exploratory data analysis (EDA) something ends final paper. means end inform entire paper, especially data section, ’s typically something belongs final draft. best way proceed make separate .Rmd add code brief notes go. Don’t delete previous code, just add . run time, ’ll useful notebook captures exploration. document collaborators guide subsequent modelling .EDA draws everything know analyst. Every tool fair game considered. Look raw data, make tables, plots, summary statistics, make models. key iterate, move quickly perfectly, come understand data.chapter working real data many issues can understand main characteristics potential issues. use opendatatoronto package (Gelfand 2020), among sources. lot options EDA (Staniak Biecek 2019) focus .","code":""},{"path":"exploratory-data-analysis.html","id":"case-study---ttc-subway-delays","chapter":"14 Exploratory data analysis","heading":"14.2 Case study - TTC subway delays","text":"section written Monica Alexander.","code":""},{"path":"exploratory-data-analysis.html","id":"introduction-12","chapter":"14 Exploratory data analysis","heading":"14.2.1 Introduction","text":"opendatatoronto package (Gelfand 2020) provides interface data available Open Data Portal provided City Toronto. going use take quick look subway delays. ’re additionally going especially draw tidyverse (Wickham et al. 2019a), well ggrepel (Slowikowski 2021), janitor (Firke 2020), lubridate (Grolemund Wickham 2011), visdat (N. Tierney 2017) packages.","code":"\nlibrary(opendatatoronto)\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(visdat)"},{"path":"exploratory-data-analysis.html","id":"gather-the-data","chapter":"14 Exploratory data analysis","heading":"14.2.2 Gather the data","text":"begin , use opendatatoronto::list_packages() look datasets available.’ll download data TTC subway delays 2019. multiple files 2019 need get make one big dataframe.Let’s also download delay code readme, reference. ’d probably want save ‘inputs’ folder, raw data underpin analysis.dataset bunch interesting variables. can refer readme descriptions. outcome interest min_delay, give delay minutes.Next ’re going highlight tools might useful getting used new dataset. ’s one way explore, ’s important keep mind:variables look like (type, values, distribution, etc);surprising (outliers etc); andwhat end goal (, might understanding factors associated delays, e.g. stations, time year, time day, etc).data analysis project, turns data issues, surprising values, missing data etc, ’s important document anything found subsequent steps assumptions made moving onto data analysis modeling.always:Start end mind.lazy possible.","code":"\nall_data <- opendatatoronto::list_packages(limit = 500)\nall_data\n#> # A tibble: 425 × 11\n#>    title         id    topics civic_issues publisher excerpt\n#>    <chr>         <chr> <chr>  <chr>        <chr>     <chr>  \n#>  1 EarlyON Chil… 2619… Commu… Poverty red… Children… \"Early…\n#>  2 Short Term R… 2ab2… Permi… Affordable … Municipa… \"This …\n#>  3 Polls conduc… 7bce… City … <NA>         City Cle… \"Polls…\n#>  4 COVID-19 Imm… d3f2… Health <NA>         Toronto … \"This …\n#>  5 COVID-19 Tes… cd61… Health <NA>         Toronto … \"This …\n#>  6 Rain Gauge L… f293… Locat… Climate cha… Toronto … \"This …\n#>  7 Street Furni… 99b1… City … <NA>         Transpor… \"Infor…\n#>  8 Automated Sp… a154… Trans… Mobility     Transpor… \"This …\n#>  9 BodySafe      c405… City … <NA>         Toronto … \"This …\n#> 10 Street Furni… 1db3… City … Mobility     Transpor… \"Trans…\n#> # … with 415 more rows, and 5 more variables:\n#> #   dataset_category <chr>, num_resources <int>,\n#> #   formats <chr>, refresh_rate <chr>,\n#> #   last_refreshed <date>\n# We know this number based on the 'id' of the interest.\nttc_resources <- \n  list_package_resources(\"996cfe8d-fb35-40ce-b569-698d51fc683b\")\n\nttc_resources <- \n  ttc_resources %>% \n  mutate(year = str_extract(name, \"201.?\"))\n\ndelay_2019_ids <- \n  ttc_resources %>% \n  filter(year==2019) %>% \n  select(id) %>% \n  pull()\n\ndelay_2019 <- c()\n\nfor(i in 1:length(delay_2019_ids)) {\n  delay_2019 <- bind_rows(delay_2019, get_resource(delay_2019_ids[i]))\n}\n\n# make the column names nicer to work with\ndelay_2019 <- clean_names(delay_2019)\ndelay_codes <- get_resource(\"fece136b-224a-412a-b191-8d31eb00491e\")\n#> New names:\n#> * `` -> ...1\n#> * `CODE DESCRIPTION` -> `CODE DESCRIPTION...3`\n#> * `` -> ...4\n#> * `` -> ...5\n#> * `CODE DESCRIPTION` -> `CODE DESCRIPTION...7`\n\ndelay_data_codebook <- get_resource(\"54247e39-5a7d-40db-a137-82b2a9ab0708\")\nhead(delay_2019)\n#> # A tibble: 6 × 10\n#>   date                time  day     station  code  min_delay\n#>   <dttm>              <chr> <chr>   <chr>    <chr>     <dbl>\n#> 1 2019-01-01 00:00:00 01:08 Tuesday YORK MI… PUSI          0\n#> 2 2019-01-01 00:00:00 02:14 Tuesday ST ANDR… PUMST         0\n#> 3 2019-01-01 00:00:00 02:16 Tuesday JANE ST… TUSC          0\n#> 4 2019-01-01 00:00:00 02:27 Tuesday BLOOR S… SUO           0\n#> 5 2019-01-01 00:00:00 03:03 Tuesday DUPONT … MUATC        11\n#> 6 2019-01-01 00:00:00 03:08 Tuesday EGLINTO… EUATC        11\n#> # … with 4 more variables: min_gap <dbl>, bound <chr>,\n#> #   line <chr>, vehicle <dbl>"},{"path":"exploratory-data-analysis.html","id":"sanity-checks","chapter":"14 Exploratory data analysis","heading":"14.2.3 Sanity Checks","text":"need check variables say . aren’t, natural next question issues. recode , even remove ? ’s important distinguish factors characters, well factors numerics.instance, look column claims days week using unique().Another function ’s useful table().Let’s now check lines.looks like many issues , obvious re-code, . drop ? ’s absolute right answer , depends ’re using data , need aware issues data.","code":"\nunique(delay_2019$day)\n#> [1] \"Tuesday\"   \"Wednesday\" \"Thursday\"  \"Friday\"   \n#> [5] \"Saturday\"  \"Sunday\"    \"Monday\"\ntable(delay_2019$day)\n#> \n#>    Friday    Monday  Saturday    Sunday  Thursday   Tuesday \n#>      2979      2970      2238      1978      3116      2939 \n#> Wednesday \n#>      3002\nunique(delay_2019$line)\n#>  [1] \"YU\"                     \"BD\"                    \n#>  [3] \"YU/BD\"                  \"SHP\"                   \n#>  [5] \"SRT\"                    NA                      \n#>  [7] \"YUS\"                    \"B/D\"                   \n#>  [9] \"BD LINE\"                \"999\"                   \n#> [11] \"YU/ BD\"                 \"YU & BD\"               \n#> [13] \"BD/YU\"                  \"YU\\\\BD\"                \n#> [15] \"46 MARTIN GROVE\"        \"RT\"                    \n#> [17] \"BLOOR-DANFORTH\"         \"YU / BD\"               \n#> [19] \"134 PROGRESS\"           \"YU - BD\"               \n#> [21] \"985 SHEPPARD EAST EXPR\" \"22 COXWELL\"            \n#> [23] \"100 FLEMINGDON PARK\"    \"YU LINE\""},{"path":"exploratory-data-analysis.html","id":"missing-values","chapter":"14 Exploratory data analysis","heading":"14.2.4 Missing values","text":"Exploring missing data course , main point presence (lack thereof) haunt analysis. Insert joke ghostbusters .get started look known-unknowns, NAs variable.visdat package (N. Tierney 2017) also useful , particularly see missing values distributed.known-unknowns, interested whether missing random. want , ideally, show data happened just drop . course, unlikely case, looking see systematic data missing.","code":"\ndelay_2019 %>% \n  summarise_all(list(~sum(is.na(.))))\n#> # A tibble: 1 × 10\n#>    date  time   day station  code min_delay min_gap bound\n#>   <int> <int> <int>   <int> <int>     <int>   <int> <int>\n#> 1     0     0     0       0     0         0       0  4380\n#> # … with 2 more variables: line <int>, vehicle <int>\nvis_dat(delay_2019)\n#> Warning: `gather_()` was deprecated in tidyr 1.2.0.\n#> Please use `gather()` instead.\n#> This warning is displayed once every 8 hours.\n#> Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\nvis_miss(delay_2019)"},{"path":"exploratory-data-analysis.html","id":"duplicate-rows","chapter":"14 Exploratory data analysis","heading":"14.2.5 Duplicate rows","text":"Sometime data happen duplicated. didn’t notice analysis wrong ways ’d able consistently expect. variety ways look duplicated rows, janitor::get_dupes() function janitor package (Firke 2020) especially useful.delays dataset quite duplicates. , ’re interested whether something systematic going . Remembering ’re trying quickly come terms dataset, one way forward flag issue come back explore later, just remove now.","code":"\njanitor::get_dupes(delay_2019)\n#> No variable names specified - using all columns.\n#> # A tibble: 158 × 11\n#>    date                time  day     station code  min_delay\n#>    <dttm>              <chr> <chr>   <chr>   <chr>     <dbl>\n#>  1 2019-01-01 00:00:00 08:18 Tuesday DONLAN… MUESA         5\n#>  2 2019-01-01 00:00:00 08:18 Tuesday DONLAN… MUESA         5\n#>  3 2019-02-01 00:00:00 05:51 Friday  SCARB … MRTO         10\n#>  4 2019-02-01 00:00:00 05:51 Friday  SCARB … MRTO         10\n#>  5 2019-02-01 00:00:00 06:45 Friday  MIDLAN… MRWEA         3\n#>  6 2019-02-01 00:00:00 06:45 Friday  MIDLAN… MRWEA         3\n#>  7 2019-02-01 00:00:00 06:55 Friday  LAWREN… ERDO          0\n#>  8 2019-02-01 00:00:00 06:55 Friday  LAWREN… ERDO          0\n#>  9 2019-02-01 00:00:00 07:16 Friday  MCCOWA… MRWEA         5\n#> 10 2019-02-01 00:00:00 07:16 Friday  MCCOWA… MRWEA         5\n#> # … with 148 more rows, and 5 more variables:\n#> #   min_gap <dbl>, bound <chr>, line <chr>, vehicle <dbl>,\n#> #   dupe_count <int>\ndelay_2019 <- \n  delay_2019 %>% \n  distinct()"},{"path":"exploratory-data-analysis.html","id":"visualizing-distributions","chapter":"14 Exploratory data analysis","heading":"14.2.6 Visualizing distributions","text":"need see data raw form understand , histograms, barplots, density plots friends . ’re looking beauty , ’re looking get look data quickly possible.Let’s look one outcome interest: ‘min_delay.’ First let’s just look histogram data.Somewhat concerningly evidence outliers (given large x-axis). variety ways focus going , quick way plot logged scale (remembering ’d expect values 0 drop away).initial exploration hinting outlying delay time, let’s take look largest delays. need join dataset ‘delay_codes’ dataset see delay , requires wrangling slightly different codes.can see 455 minute delay due ‘Rail Related Problem’ seems much outlier.","code":"\n## Removing the observations that have non-standardized lines\ndelay_2019 <- \n  delay_2019 %>% \n  filter(line %in% c(\"BD\", \"YU\", \"SHP\", \"SRT\"))\n\nggplot(data = delay_2019) + \n  geom_histogram(aes(x = min_delay))\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\nggplot(data = delay_2019) + \n  geom_histogram(aes(x = min_delay)) + \n  scale_x_log10()\n#> Warning: Transformation introduced infinite values in\n#> continuous x-axis\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n#> Warning: Removed 11944 rows containing non-finite values\n#> (stat_bin).\ndelay_2019 <- \n  delay_2019 %>% \n  left_join(delay_codes %>% \n              rename(code = `SUB RMENU CODE`, code_desc = `CODE DESCRIPTION...3`) %>%\n              select(code, code_desc)\n            )\n#> Joining, by = \"code\"\n\ndelay_2019 <- \n  delay_2019 %>%\n  mutate(code_srt = ifelse(line==\"SRT\", code, \"NA\")) %>% \n  left_join(delay_codes %>% \n              rename(code_srt = `SRT RMENU CODE`, code_desc_srt = `CODE DESCRIPTION...7`) %>%\n              select(code_srt, code_desc_srt)) %>% \n  mutate(code = ifelse(code_srt==\"NA\", code, code_srt),\n         code_desc = ifelse(is.na(code_desc_srt), code_desc, code_desc_srt)) %>% \n  select(-code_srt, -code_desc_srt)\n#> Joining, by = \"code_srt\"\ndelay_2019 %>% \n  left_join(delay_codes %>% \n              rename(code = `SUB RMENU CODE`, code_desc = `CODE DESCRIPTION...3`) %>%\n              select(code, code_desc)) %>% \n  arrange(-min_delay) %>% \n  select(date, time, station, line, min_delay, code, code_desc)\n#> Joining, by = c(\"code\", \"code_desc\")\n#> # A tibble: 18,697 × 7\n#>    date                time  station   line  min_delay code \n#>    <dttm>              <chr> <chr>     <chr>     <dbl> <chr>\n#>  1 2019-06-25 00:00:00 18:48 WILSON T… YU          455 PUTR \n#>  2 2019-02-12 00:00:00 20:28 LAWRENCE… SRT         284 MRWEA\n#>  3 2019-06-05 00:00:00 12:42 UNION TO… YU          250 MUPLA\n#>  4 2019-10-22 00:00:00 14:22 LAWRENCE… YU          228 PUTS \n#>  5 2019-09-26 00:00:00 11:38 YORK MIL… YU          193 MUPR1\n#>  6 2019-06-08 00:00:00 08:51 SPADINA … BD          180 MUPLB\n#>  7 2019-12-02 00:00:00 06:59 DUNDAS W… BD          176 MUPLB\n#>  8 2019-01-29 00:00:00 05:46 VICTORIA… BD          174 MUWEA\n#>  9 2019-02-22 00:00:00 17:32 ELLESMER… SRT         168 PRW  \n#> 10 2019-02-10 00:00:00 07:53 BAYVIEW … SHP         165 PUSI \n#> # … with 18,687 more rows, and 1 more variable:\n#> #   code_desc <chr>"},{"path":"exploratory-data-analysis.html","id":"groups-and-small-counts","chapter":"14 Exploratory data analysis","heading":"14.2.7 Groups and small counts","text":"Another thing ’re looking various groupings data, especially sub-groups may end small numbers observations (analysis easily influenced ). quick way group data variable interest, instance ‘line,’ using colour.plot uses density can look distributions comparably, also aware differences frequency. case, ’ll see SHP SRT much smaller counts.group second variable can useful use facets. ’re little fiddly initially, get used , ’re quick powerful.aside, station names mess. try quickly bring little order chaos just taking just first word (, first two starts ‘ST’).can now plot top five stations mean delay.","code":"\nggplot(data = delay_2019) + \n  geom_histogram(aes(x = min_delay, y = ..density.., fill = line), \n                 position = 'dodge', \n                 bins = 10) + \n  scale_x_log10()\n#> Warning: Transformation introduced infinite values in\n#> continuous x-axis\n#> Warning: Removed 11944 rows containing non-finite values\n#> (stat_bin).\nggplot(data = delay_2019) + \n  geom_histogram(aes(x = min_delay, fill = line), \n                 position = 'dodge', \n                 bins = 10) + \n  scale_x_log10()\n#> Warning: Transformation introduced infinite values in\n#> continuous x-axis\n#> Warning: Removed 11944 rows containing non-finite values\n#> (stat_bin).\nggplot(data = delay_2019) + \n  geom_density(aes(x = min_delay, color = day), \n               bw = .08) + \n  scale_x_log10() + \n  facet_grid(~line)\n#> Warning: Transformation introduced infinite values in\n#> continuous x-axis\n#> Warning: Removed 11944 rows containing non-finite values\n#> (stat_density).\ndelay_2019 <- \n  delay_2019 %>% \n  mutate(station_clean = ifelse(str_starts(station, \"ST\"), word(station, 1,2), word(station, 1)))\ndelay_2019 %>% \n  group_by(line, station_clean) %>% \n  summarise(mean_delay = mean(min_delay), n_obs = n()) %>% \n  filter(n_obs>1) %>% \n  arrange(line, -mean_delay) %>% \n  slice(1:5) %>% \n  ggplot(aes(station_clean, mean_delay)) + \n    geom_col() + \n    coord_flip() + \n    facet_wrap(~line, scales = \"free_y\")\n#> `summarise()` has grouped output by 'line'. You can override\n#> using the `.groups` argument."},{"path":"exploratory-data-analysis.html","id":"visualizing-time-series","chapter":"14 Exploratory data analysis","heading":"14.2.8 Visualizing time series","text":"Dates pain work . always go wrong give issues, critical aspect explore EDA. daily plot data , well, messy (can check ). Instead, let’s look week see ’s seasonality. lubridate package (Grolemund Wickham 2011) lot helpful functions deal date variables. ’s essentially indispensable.get started, let’s look mean delay (delayed zero minutes).Now let’s look proportion delays greater 10 minutes., ’s important clear . charts tables analyse place final report, allowing become comfortable data. , additionally making notes plot table go, noting warnings implications aspects return .","code":"\ndelay_2019 %>% \n  filter(min_delay>0) %>% \n  mutate(week = week(date)) %>% \n  group_by(week, line) %>% \n  summarise(mean_delay = mean(min_delay)) %>% \n  ggplot(aes(week, mean_delay, color = line)) + \n    geom_point() + \n    geom_smooth() + \n    facet_grid(~line)\n#> `summarise()` has grouped output by 'week'. You can override using the `.groups` argument.\n#> `geom_smooth()` using method = 'loess' and formula 'y ~ x'\ndelay_2019 %>% \n  mutate(week = week(date)) %>% \n  group_by(week, line) %>% \n  summarise(prop_delay = sum(min_delay>10)/n()) %>% \n  ggplot(aes(week, prop_delay, color = line)) + \n    geom_point() + \n    geom_smooth() + \n    facet_grid(~line)\n#> `summarise()` has grouped output by 'week'. You can override using the `.groups` argument.\n#> `geom_smooth()` using method = 'loess' and formula 'y ~ x'"},{"path":"exploratory-data-analysis.html","id":"visualizing-relationships","chapter":"14 Exploratory data analysis","heading":"14.2.9 Visualizing relationships","text":"also interested looking relationship two variables. Scatter plots especially useful continuous variables, good precursor modeling. saw little ‘mean_delay’ ‘week.’relationship categorical variables takes work, also, instance, look top five reasons delay station. particular, may interested whether differ, difference modelled.","code":"\ndelay_2019 %>%\n  ggplot(aes(x = min_delay, y = min_gap)) +\n  geom_point() +\n  scale_x_log10() +\n  scale_y_log10()\n#> Warning: Transformation introduced infinite values in\n#> continuous x-axis\n#> Warning: Transformation introduced infinite values in\n#> continuous y-axis\ndelay_2019 %>%\n  group_by(line, code_desc) %>%\n  summarise(mean_delay = mean(min_delay)) %>%\n  arrange(-mean_delay) %>%\n  slice(1:5) %>%\n  ggplot(aes(x = code_desc,\n             y = mean_delay)) +\n  geom_col() + \n  facet_wrap(vars(line), \n             scales = \"free_y\",\n             nrow = 4) +\n  coord_flip()\n#> `summarise()` has grouped output by 'line'. You can override\n#> using the `.groups` argument."},{"path":"exploratory-data-analysis.html","id":"principal-components-analysis","chapter":"14 Exploratory data analysis","heading":"14.2.10 Principal components analysis","text":"Principal components analysis (PCA) another powerful exploratory tool. allows pick potential clusters /outliers can help inform model building. see let’s quick (imperfect) example looking types delays station.delay categories bit mess, ’s hundreds . simple start, remembering task come terms dataset quickly possible, let’s just take first word.Let’s also just restrict analysis causes happen least 50 times 2019. PCA, dataframe also needs switched wide format.Now can quickly PCA.can plot first two principal components, add labels outlying stations.also plot factor loadings. se evidence perhaps one public, compared another operator.","code":"\ndelay_2019 <- delay_2019 %>% \n  mutate(code_red = case_when(\n    str_starts(code_desc, \"No\") ~ word(code_desc, 1, 2),\n    str_starts(code_desc, \"Operator\") ~ word(code_desc, 1,2),\n    TRUE ~ word(code_desc,1))\n         )\n\ndwide <- delay_2019 %>% \n  group_by(line, station_clean) %>% \n  mutate(n_obs = n()) %>% \n  filter(n_obs>1) %>% \n  group_by(code_red) %>% \n  mutate(tot_delay = n()) %>% \n  arrange(tot_delay) %>% \n  filter(tot_delay>50) %>% \n  group_by(line, station_clean, code_red) %>% \n  summarise(n_delay = n()) %>% \n  pivot_wider(names_from = code_red, values_from = n_delay) %>% \n  mutate_all(.funs = funs(ifelse(is.na(.), 0, .)))\n#> `summarise()` has grouped output by 'line', 'station_clean'. You can override using the `.groups` argument.\n#> `mutate_all()` ignored the following grouping variables:\n#> Columns `line`, `station_clean`\n#> Use `mutate_at(df, vars(-group_cols()), myoperation)` to silence the message.\n#> Warning: `funs()` was deprecated in dplyr 0.8.0.\n#> Please use a list of either functions or lambdas: \n#> \n#>   # Simple named list: \n#>   list(mean = mean, median = median)\n#> \n#>   # Auto named with `tibble::lst()`: \n#>   tibble::lst(mean, median)\n#> \n#>   # Using lambdas\n#>   list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n#> This warning is displayed once every 8 hours.\n#> Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\ndelay_pca <- prcomp(dwide[,3:ncol(dwide)])\n\ndf_out <- as_tibble(delay_pca$x)\ndf_out <- bind_cols(dwide %>% select(line, station_clean), df_out)\nhead(df_out)\n#> # A tibble: 6 × 40\n#> # Groups:   line, station_clean [6]\n#>   line  station_clean    PC1     PC2    PC3    PC4    PC5\n#>   <chr> <chr>          <dbl>   <dbl>  <dbl>  <dbl>  <dbl>\n#> 1 BD    BATHURST        6.50   26.9   -2.71 -10.8  -8.40 \n#> 2 BD    BAY            24.8     7.63  -2.19  -7.05  0.714\n#> 3 BD    BLOOR         -62.4  -112.    57.3  -23.4  -5.09 \n#> 4 BD    BROADVIEW      -6.60   28.1   -1.06 -14.0  -6.49 \n#> 5 BD    CASTLE         23.8    11.8   -1.31  -7.93 -3.62 \n#> 6 BD    CHESTER        24.6    -1.87 -18.6    2.75  1.85 \n#> # … with 33 more variables: PC6 <dbl>, PC7 <dbl>,\n#> #   PC8 <dbl>, PC9 <dbl>, PC10 <dbl>, PC11 <dbl>,\n#> #   PC12 <dbl>, PC13 <dbl>, PC14 <dbl>, PC15 <dbl>,\n#> #   PC16 <dbl>, PC17 <dbl>, PC18 <dbl>, PC19 <dbl>,\n#> #   PC20 <dbl>, PC21 <dbl>, PC22 <dbl>, PC23 <dbl>,\n#> #   PC24 <dbl>, PC25 <dbl>, PC26 <dbl>, PC27 <dbl>,\n#> #   PC28 <dbl>, PC29 <dbl>, PC30 <dbl>, PC31 <dbl>, …\nggplot(df_out,aes(x=PC1,y=PC2,color=line )) + \n  geom_point() + \n  geom_text_repel(data = df_out %>% filter(PC2>100|PC1<100*-1), \n                  aes(label = station_clean)\n                  )\ndf_out_r <- as_tibble(delay_pca$rotation)\ndf_out_r$feature <- colnames(dwide[,3:ncol(dwide)])\n\ndf_out_r\n#> # A tibble: 38 × 39\n#>         PC1      PC2        PC3       PC4      PC5      PC6\n#>       <dbl>    <dbl>      <dbl>     <dbl>    <dbl>    <dbl>\n#>  1 -0.0412   0.0638   0.0133    -0.0467    0.0246   0.0184 \n#>  2 -0.0332  -0.00469 -0.0414    -0.00751   0.0201  -0.0122 \n#>  3 -0.135    0.207    0.0237    -0.144     0.135   -0.0381 \n#>  4 -0.0652   0.0475  -0.0443    -0.0251   -0.00139 -0.0748 \n#>  5 -0.00443  0.00878 -0.0000499 -0.000830  0.00967  0.00954\n#>  6 -0.0268  -0.00722 -0.00439    0.000534 -0.0151  -0.0125 \n#>  7 -0.0813   0.0960  -0.0462     0.0479   -0.0978  -0.0365 \n#>  8 -0.0117   0.0135   0.00548   -0.0294    0.0125   0.0377 \n#>  9 -0.516    0.655   -0.0177    -0.162    -0.221   -0.287  \n#> 10 -0.151    0.0826   0.0548     0.352    -0.397    0.281  \n#> # … with 28 more rows, and 33 more variables: PC7 <dbl>,\n#> #   PC8 <dbl>, PC9 <dbl>, PC10 <dbl>, PC11 <dbl>,\n#> #   PC12 <dbl>, PC13 <dbl>, PC14 <dbl>, PC15 <dbl>,\n#> #   PC16 <dbl>, PC17 <dbl>, PC18 <dbl>, PC19 <dbl>,\n#> #   PC20 <dbl>, PC21 <dbl>, PC22 <dbl>, PC23 <dbl>,\n#> #   PC24 <dbl>, PC25 <dbl>, PC26 <dbl>, PC27 <dbl>,\n#> #   PC28 <dbl>, PC29 <dbl>, PC30 <dbl>, PC31 <dbl>, …\n\nggplot(df_out_r,aes(x=PC1,y=PC2,label=feature )) + geom_text_repel()\n#> Warning: ggrepel: 29 unlabeled data points (too many\n#> overlaps). Consider increasing max.overlaps"},{"path":"exploratory-data-analysis.html","id":"case-study---opinions-about-a-casino-in-toronto","chapter":"14 Exploratory data analysis","heading":"14.3 Case study - Opinions about a casino in Toronto","text":"written Michael Chong.","code":""},{"path":"exploratory-data-analysis.html","id":"data-preparation","chapter":"14 Exploratory data analysis","heading":"14.3.1 Data preparation","text":"use opendatatoronto package . See previous case study deeper explanation code works.dataset ’m extracting results survey 2012 regarding establishment casino Toronto. info available following link. analysis, ’ll hoping address question: demographic (age/gender) groups likely supportive new casino Toronto?object casino_resource isn’t quite useable yet, ’s (inconveniently) stored list 2 data frames:just return object, can see second list item empty, just want keep first one:, let’s keep first item indexing list double square brackets:Let’s check first couple rows dataframe looks like. default, head() returns first 6 rows:Unfortunately column names aren’t informative. simplicity, ’ll use ‘.pdf’ questionnaire accompanies dataset Toronto Open Data website. Alternatively, get parse ‘readme’ R package. ’s link questionnaire.Question 1 indicates level support casino Toronto. ’ll use response variable.Concerning potential predictor variables, questions ask respondents opinions different aspects potential casino development, aren’t particularly useful towards cause. demographic variables Age Gender, let’s choose .’m also going rename columns resulting data frame columns ‘opinion,’ ‘age,’ ‘gender.’","code":"\n# Get the data\ncasino_resource <- \n  search_packages(\"casino survey\")%>%\n  list_package_resources() %>%\n  filter(name == \"toronto-casino-survey-results\") %>%\n  get_resource()\n#> New names:\n#> * `` -> ...93\n#> * `` -> ...94\nhead(casino_resource)\n#> $tblSurvey\n#> # A tibble: 17,766 × 94\n#>    SurveyID Q1_A   Q1_B1 Q1_B2 Q1_B3 Q2_A  Q2_B  Q3_A  Q3_B \n#>       <dbl> <chr>  <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n#>  1        1 Stron… Do n… Do n… Do n… Does… \"As … Not … Very…\n#>  2        2 Stron… Econ… Jobs  Arts… Fits… \"Cos… Very… Very…\n#>  3        3 Stron… Ther… If t… <NA>  Fits… \"Big… Very… Very…\n#>  4        4 Somew… beli… mone… evid… Does… \"My … Very… Very…\n#>  5        5 Neutr… Like… Conc… <NA>  Neut… \"Aga… Very… Very…\n#>  6        6 Stron… have… <NA>  <NA>  Does… \"Tor… Not … Not …\n#>  7        7 Stron… The … Peop… We s… Does… \"#3 … Not … Not …\n#>  8        8 Stron… It w… Mora… <NA>  Does… \"Cas… Very… Very…\n#>  9        9 Stron… It's… traf… heal… Does… \"No … Not … Very…\n#> 10       10 Stron… Toro… Avoi… Prov… Fits… \"Tor… Very… Very…\n#> # … with 17,756 more rows, and 85 more variables:\n#> #   Q3_C <chr>, Q3_D <chr>, Q3_E <chr>, Q3_F <chr>,\n#> #   Q3_G <chr>, Q3_H <chr>, Q3_I <chr>, Q3_J <chr>,\n#> #   Q3_K <chr>, Q3_L <chr>, Q3_M <chr>, Q3_N <chr>,\n#> #   Q3_O <chr>, Q3_P <chr>, Q3_Q <chr>, Q3_Q_Other <chr>,\n#> #   Q3_Comments <chr>, Q4_A <chr>, Q5 <chr>, Q6 <chr>,\n#> #   Q6_Comments <chr>, Q7_A_StandAlone <chr>, …\n#> \n#> $Sheet1\n#> # A tibble: 0 × 0\n# Check what kind of object the casino_resource object is\nclass(casino_resource)\n#> [1] \"list\"\ncasino_resource\n#> $tblSurvey\n#> # A tibble: 17,766 × 94\n#>    SurveyID Q1_A   Q1_B1 Q1_B2 Q1_B3 Q2_A  Q2_B  Q3_A  Q3_B \n#>       <dbl> <chr>  <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n#>  1        1 Stron… Do n… Do n… Do n… Does… \"As … Not … Very…\n#>  2        2 Stron… Econ… Jobs  Arts… Fits… \"Cos… Very… Very…\n#>  3        3 Stron… Ther… If t… <NA>  Fits… \"Big… Very… Very…\n#>  4        4 Somew… beli… mone… evid… Does… \"My … Very… Very…\n#>  5        5 Neutr… Like… Conc… <NA>  Neut… \"Aga… Very… Very…\n#>  6        6 Stron… have… <NA>  <NA>  Does… \"Tor… Not … Not …\n#>  7        7 Stron… The … Peop… We s… Does… \"#3 … Not … Not …\n#>  8        8 Stron… It w… Mora… <NA>  Does… \"Cas… Very… Very…\n#>  9        9 Stron… It's… traf… heal… Does… \"No … Not … Very…\n#> 10       10 Stron… Toro… Avoi… Prov… Fits… \"Tor… Very… Very…\n#> # … with 17,756 more rows, and 85 more variables:\n#> #   Q3_C <chr>, Q3_D <chr>, Q3_E <chr>, Q3_F <chr>,\n#> #   Q3_G <chr>, Q3_H <chr>, Q3_I <chr>, Q3_J <chr>,\n#> #   Q3_K <chr>, Q3_L <chr>, Q3_M <chr>, Q3_N <chr>,\n#> #   Q3_O <chr>, Q3_P <chr>, Q3_Q <chr>, Q3_Q_Other <chr>,\n#> #   Q3_Comments <chr>, Q4_A <chr>, Q5 <chr>, Q6 <chr>,\n#> #   Q6_Comments <chr>, Q7_A_StandAlone <chr>, …\n#> \n#> $Sheet1\n#> # A tibble: 0 × 0\ncasino_data <- casino_resource[[1]]\nhead(casino_data) \n#> # A tibble: 6 × 94\n#>   SurveyID Q1_A    Q1_B1 Q1_B2 Q1_B3 Q2_A  Q2_B  Q3_A  Q3_B \n#>      <dbl> <chr>   <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n#> 1        1 Strong… Do n… Do n… Do n… Does… \"As … Not … Very…\n#> 2        2 Strong… Econ… Jobs  Arts… Fits… \"Cos… Very… Very…\n#> 3        3 Strong… Ther… If t… <NA>  Fits… \"Big… Very… Very…\n#> 4        4 Somewh… beli… mone… evid… Does… \"My … Very… Very…\n#> 5        5 Neutra… Like… Conc… <NA>  Neut… \"Aga… Very… Very…\n#> 6        6 Strong… have… <NA>  <NA>  Does… \"Tor… Not … Not …\n#> # … with 85 more variables: Q3_C <chr>, Q3_D <chr>,\n#> #   Q3_E <chr>, Q3_F <chr>, Q3_G <chr>, Q3_H <chr>,\n#> #   Q3_I <chr>, Q3_J <chr>, Q3_K <chr>, Q3_L <chr>,\n#> #   Q3_M <chr>, Q3_N <chr>, Q3_O <chr>, Q3_P <chr>,\n#> #   Q3_Q <chr>, Q3_Q_Other <chr>, Q3_Comments <chr>,\n#> #   Q4_A <chr>, Q5 <chr>, Q6 <chr>, Q6_Comments <chr>,\n#> #   Q7_A_StandAlone <chr>, Q7_A_Integrated <chr>, …\n# Narrow down the dataframe to our variables of interest\ncasino_data <- \n  casino_data %>%\n  select(Q1_A, Age, Gender) %>%\n  rename(opinion = Q1_A, age = Age, gender = Gender)\n\n# Look at first couple rows:\nhead(casino_data)\n#> # A tibble: 6 × 3\n#>   opinion                   age   gender\n#>   <chr>                     <chr> <chr> \n#> 1 Strongly Opposed          25-34 Male  \n#> 2 Strongly in Favour        35-44 Female\n#> 3 Strongly in Favour        55-64 Male  \n#> 4 Somewhat Opposed          25-34 Male  \n#> 5 Neutral or Mixed Feelings 25-34 Female\n#> 6 Strongly Opposed          45-54 Female"},{"path":"exploratory-data-analysis.html","id":"some-visual-exploration-and-more-cleanup-of-course","chapter":"14 Exploratory data analysis","heading":"14.3.2 Some visual exploration (and more cleanup, of course)","text":"Let’s first quick exploration get feel ’s going data. ’ll first calculate proportions casino support age-gender combination:notes:use geom_col() make bar chart,facet_grid() modifies plot plot panels correspond certain values discrete variables (case, “facet” age gender). helpful case interested distribution opinions changes age gender.things note:x-axis labels order sense monotone order increasing/decreasing supportthere NA values opinion, age, gender, well “Prefer disclose” responses","code":"\n# Calculate proportions\ncasino_summary <- casino_data %>%\n  group_by(age, gender, opinion) %>%\n  summarise(n = n()) %>% # Count the number in each group and response\n  group_by(age, gender) %>%\n  mutate(prop = n/sum(n)) # Calculate proportions within each group\n#> `summarise()` has grouped output by 'age', 'gender'. You can\n#> override using the `.groups` argument.\nggplot(casino_summary) +\n  geom_col(aes(x = opinion, y = prop)) + # Specify a histogram of opinion responses\n  facet_grid(age~gender) + #Facet by age and gender\n  theme(axis.text.x = element_text(angle = 90)) # Rotate the x-axis labels to be readable"},{"path":"exploratory-data-analysis.html","id":"getting-the-data-into-a-more-model-suitable-format","chapter":"14 Exploratory data analysis","heading":"14.3.3 Getting the data into a more model-suitable format","text":"First need get rid responses aren’t suitable. simplicity ’ll assume NA values “Prefer disclose” responses occur randomly, remove dataset (note reality assumption might hold might want careful). Let’s check many rows original dataset:Now let’s dplyr::filter() accordingly omit responses don’t want. case ’re unfamiliar, ’m going make use :.na(), returns TRUE argument NA,! operator, flips TRUE FALSE. instance, !.na(x) return TRUE x NA, want keep.Let’s check many rows data ’re left :Now need convert response variable binary. clean first problem (response variables order), might well take opportunity convert format suitable model. logistic regression, like response variable binary, case 5 possible categories ranging “Strongly Opposed” “Strongly Favour.” ’ll recategorize new ‘supportive_or_not’ variable follows.‘supportive = 1’ “Strongly Favour” “Somewhat Favour”‘supportive = 0’ “Neutral Mixed Feelings,” “Somewhat Opposed,” “Strongly Opposed”dplyr::mutate() function, creates new columns (possibly functions existing columns), dplyr::case_when(), provides way assign values conditional -statements. syntax little strange. LHS ~ “” condition, RHS tilde value return. example, ‘x == 0 ~ 3’ return 3 ‘x’ 0.Another commonly used operator %% operator, checks whether something element vector, instance, e.g.:1 %% c(1, 3, 4) returns TRUE2 %% c(1, 3, 4) returns FALSENow need convert age numeric variable. Age survey given age groups. Let’s instead map numeric variable can easily talk trends age. ’ll map youngest age 1, :Now let’s make plot , new processed data:can sort see difference distribution different panels. formalize , can run logistic regression.","code":"\n# nrow() returns the number of rows in a dataframe:\nnrow(casino_data)\n#> [1] 17766\ncasino_data <- casino_data %>%\n  # Only keep rows with non-NA:\n  filter(!is.na(opinion), !is.na(age), !is.na(gender)) %>%\n  # Only keep rows where age and gender are disclosed:\n  filter(age != \"Prefer not to disclose\", gender != \"Prefer not to disclose\")\nnrow(casino_data)\n#> [1] 13658\n# Store possible opinions in vectors\nyes_opinions <- c(\"Strongly in Favour\", \"Somewhat in Favour\")\nno_opinions <- c(\"Neutral or Mixed Feelings\", \"Somewhat Opposed\", \"Strongly Opposed\")\n\n# Create `supportive` column:\ncasino_data <- \n  casino_data %>%\n  mutate(supportive = case_when(\n    opinion %in% yes_opinions ~ TRUE, # Assign TRUE\n    opinion %in% no_opinions ~ FALSE  # Assign FALSE\n  ))\ncasino_data <- \n  casino_data %>%\n  mutate(age_group = case_when(\n    age == \"Under 15\" ~ 0,\n    age == \"15-24\" ~ 1,\n    age == \"25-34\" ~ 2,\n    age == \"35-44\" ~ 3,\n    age == \"45-54\" ~ 4,\n    age == \"55-64\" ~ 5,\n    age == \"65 or older\" ~ 6\n  ))\ncasino_summary2 <- \n  casino_data %>%\n  group_by(age_group, gender, supportive) %>%\n  summarise(n = n()) %>% # Count the number in each group and response\n  group_by(age_group, gender) %>%\n  mutate(prop = n/sum(n)) # Calculate proportions within each group\n#> `summarise()` has grouped output by 'age_group', 'gender'.\n#> You can override using the `.groups` argument.\n\nggplot(casino_summary2) +\n  facet_grid(age_group ~ gender) +\n  geom_col(aes(x = supportive, y = prop)) "},{"path":"exploratory-data-analysis.html","id":"logistic-regression","chapter":"14 Exploratory data analysis","heading":"14.3.4 Logistic Regression","text":"Now, ’re set feed regression. can glm(), allows us fit generalized linear models.use family = \"binomial\" specify logistic regression, formula supportive ~ age_group + gender, indicates supportive (binary) response variable since ’s LHS, age_group gender predictor variables.can take look results running GLM using summary():Interpretation can little tricky. important things note results:Firstly, numeric age group variable. Remember coded age_group numbers 1 5. ’ve used age groups instead age, careful phrase conclusion. coefficient estimate corresponds effect moving unit age group scale (e.g. 25-34 age group 35-44 age group), rather 1 year age (e.g. age 28 29).Secondly, results log-odds ratios. effect estimates log-odds scale. means effect -0.07983 age_group interpreted : ‘unit increase age_group, estimate 0.07983 decrease log-odds supportive casino.’exponentiate coefficient estimate make least little easier interpret. number get interpreted factor odds.(cleaner) interpretation : ‘odds individuals gender pro-casino predicted change factor 0.9232733 unit increase age_group’Finally, baseline category. First, note categorical variables, gender coefficients relative “baseline” category. value gender doesn’t appear table, Female, implicitly used baseline gender category. Technical note: variable stored character class, glm() choose alphabetically first value use baseline., interpretation genderMale coefficient : ‘odds male individual supporting casino 2.0144778 times higher female individual age_group.’can make estimates variety ways. First ’ll look manually.Using formula found James et al. (2017, 4.3.3), can make estimates individual certain characteristics. Suppose wanted predict probability supporting Toronto casino individual 36 identified transgender. :age_group takes value 3, since age group 35-44 coded 3,genderTransgendered takes value 1First, let’s extract coefficient estimates vector using coefficients():Since vector labeled, can index using square brackets names. instance:first let’s evaluate exponent term \\(e^{\\beta_0 + \\cdots + \\beta_p X_p}\\):Now evaluate expression gives probability casino support:works, stream-lined ways. Thankfully R comes convenient function make prediction estimates glm(). using predict() function. First, need make dataframe relevant variables values ’re interested predicting. ’ll use values :dataframe looks like :feed predict() function, along glm object. get probability, need specify type = \"response\".matches probability got manually, yay!","code":"\ncasino_glm <- glm(supportive ~ age_group + gender, data = casino_data, family = \"binomial\")\nsummary(casino_glm)\n#> \n#> Call:\n#> glm(formula = supportive ~ age_group + gender, family = \"binomial\", \n#>     data = casino_data)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -1.0107  -0.8888  -0.6804   1.4249   1.8822  \n#> \n#> Coefficients:\n#>                     Estimate Std. Error z value Pr(>|z|)\n#> (Intercept)         -1.10594    0.05863 -18.862  < 2e-16\n#> age_group           -0.07983    0.01376  -5.801 6.59e-09\n#> genderMale           0.70036    0.04027  17.390  < 2e-16\n#> genderTransgendered  0.69023    0.39276   1.757   0.0789\n#>                        \n#> (Intercept)         ***\n#> age_group           ***\n#> genderMale          ***\n#> genderTransgendered .  \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 16010  on 13657  degrees of freedom\n#> Residual deviance: 15653  on 13654  degrees of freedom\n#> AIC: 15661\n#> \n#> Number of Fisher Scoring iterations: 4\nexp(-0.07983)\n#> [1] 0.9232733\nexp(0.70036)\n#> [1] 2.014478\ncoefs <- coefficients(casino_glm)\ncoefs\n#>         (Intercept)           age_group          genderMale \n#>         -1.10593925         -0.07983372          0.70036199 \n#> genderTransgendered \n#>          0.69022910\ncoefs[\"age_group\"]\n#>   age_group \n#> -0.07983372\nexp_term <- exp(coefs[\"(Intercept)\"] + coefs[\"age_group\"]*3 + coefs[\"genderTransgendered\"]*1)\n# The unname() command just takes off the label that it \"inherited\" from the coefs vector.\n# (don't worry about it, doesn't affect any functionality)\nunname(exp_term / (1 + exp_term))\n#> [1] 0.3418161\nprediction_df <- data.frame(age_group = 3, gender = \"Transgendered\")\nprediction_df\n#>   age_group        gender\n#> 1         3 Transgendered\npredict(casino_glm, newdata = prediction_df, type = \"response\")\n#>         1 \n#> 0.3418161"},{"path":"exploratory-data-analysis.html","id":"case-study---airbnb-listing-in-toronto","chapter":"14 Exploratory data analysis","heading":"14.4 Case study - Airbnb listing in Toronto","text":"","code":""},{"path":"exploratory-data-analysis.html","id":"essentials","chapter":"14 Exploratory data analysis","heading":"14.4.1 Essentials","text":"case study look Airbnb listings Toronto.","code":""},{"path":"exploratory-data-analysis.html","id":"set-up","chapter":"14 Exploratory data analysis","heading":"14.4.2 Set up","text":"","code":"\nlibrary(broom) # Helps with model outputs etc\nlibrary(here) # Helps with specifying path names\nlibrary(janitor) # Helps with initial data cleaning and pretty tables\nlibrary(tidymodels) # Help with modelling\nlibrary(tidyverse) \nlibrary(visdat) # Helps check missing values"},{"path":"exploratory-data-analysis.html","id":"get-data","chapter":"14 Exploratory data analysis","heading":"14.4.3 Get data","text":"dataset Inside Airbnb (Cox 2021). package help (Müller 2017).can give read_csv() link dataset download . helps reproducibility source clear. , link change time, longer-term reproducibility, well wanting minimise effect Inside Airbnb servers, suggest also save local copy data use . (original data , make public without first getting written permission.)","code":"\n# For reproducibility\n# airbnb_data_reduced <- read_csv(\"http://data.insideairbnb.com/canada/on/toronto/2021-01-02/data/listings.csv.gz\", guess_max = 20000)\n# write_csv(airbnb_data_reduced, \"week_6/data/airbnb_toronto_2019-12-07.csv\")\n\n# For actual work\nairbnb_data <- read_csv(here::here(\"dont_push/airbnb_toronto_2021_january-listings.csv\"), guess_max = 20000)\n\n# The guess_max option in read_csv helps us avoid having to specify the column types. Usually read_csv takes a best guess at the column types based on the first few rows. But sometimes those first ones are misleading and so guess_max forces it to look at a larger number of rows to try to work out what is going on."},{"path":"exploratory-data-analysis.html","id":"clean-data","chapter":"14 Exploratory data analysis","heading":"14.4.4 Clean data","text":"enormous number columns, ’ll just select .might like brief look dataset see anything weird going . bunch ways .things jump :character variables probably numerics dates/times: host_response_time, price, weekly_price, monthly_price, cleaning_fee.Weekly monthly price missing overwhelming number observations.Roughly fifth observations missing review score seems like correlation review-type variables.two variants neighbourhood name.NAs host_is_superhost.reviews seem really skewed.someone 328 properties Airbnb.","code":"\nnames(airbnb_data) %>% length()\n#> [1] 74\n\nairbnb_data_selected <- \n  airbnb_data %>% \n  select(host_id, \n         host_since, \n         host_response_time, \n         host_is_superhost, \n         host_listings_count,\n         host_total_listings_count,\n         host_neighbourhood, \n         host_listings_count, \n         neighbourhood_cleansed, \n         room_type, \n         bathrooms, \n         bedrooms, \n         price, \n         number_of_reviews, \n         has_availability, \n         review_scores_rating, \n         review_scores_accuracy, \n         review_scores_cleanliness, \n         review_scores_checkin, \n         review_scores_communication, \n         review_scores_location, \n         review_scores_value\n         )"},{"path":"exploratory-data-analysis.html","id":"price","chapter":"14 Exploratory data analysis","heading":"14.4.4.1 Price","text":"First need convert numeric. common problem, need little careful doesn’t just convert NAs. case just force price data numeric go NA lot characters R doesn’t know convert, e.g. numeric ‘$?’ need remove characters first.Now can look prices.outliers. Let’s zoom prices $1,000.Let’s look detail price $4,999.Let’s look distribution prices ‘reasonable’ range, Monica full professor, defined nightly price less $1,000.Interestingly looks like bunching prices, possible around numbers ending zero nine? Let’s just zoom prices $90 $210, interest, change bins smaller.","code":"\nairbnb_data_selected$price %>% head()\n#> [1] \"$469.00\" \"$96.00\"  \"$64.00\"  \"$70.00\"  \"$45.00\" \n#> [6] \"$127.00\"\n\n# First work out what is going on\nairbnb_data_selected$price %>% str_split(\"\") %>% unlist() %>% unique()\n#>  [1] \"$\" \"4\" \"6\" \"9\" \".\" \"0\" \"7\" \"5\" \"1\" \"2\" \"3\" \"8\" \",\"\n# It's clear that '$' needs to go. The only odd thing is ',' so take a look at those:\nairbnb_data_selected %>% \n  select(price) %>% \n  filter(str_detect(price, \",\"))\n#> # A tibble: 145 × 1\n#>    price    \n#>    <chr>    \n#>  1 $1,724.00\n#>  2 $1,000.00\n#>  3 $1,100.00\n#>  4 $1,450.00\n#>  5 $1,019.00\n#>  6 $1,000.00\n#>  7 $1,300.00\n#>  8 $2,142.00\n#>  9 $2,000.00\n#> 10 $1,200.00\n#> # … with 135 more rows\n# It's clear that the data is just nicely formatted, but we need to remove the comma:\nairbnb_data_selected <- \n  airbnb_data_selected %>% \n  mutate(price = str_remove(price, \"\\\\$\"),\n         price = str_remove(price, \",\"),\n         price = as.integer(price)\n         )\n# Look at distribution of price\nairbnb_data_selected %>%\n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 10) +\n  theme_classic() +\n  labs(x = \"Price per night\",\n       y = \"Number of properties\")\n#  We use bins with a width of 10, so that's going to aggregate prices into 10s so that we don't get overwhelmed with bars.\n# Look at distribution of high prices\nairbnb_data_selected %>%\n  filter(price > 1000) %>% \n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 10) +\n  theme_classic() +\n  labs(x = \"Price per night\",\n       y = \"Number of properties\")\nairbnb_data_selected %>%\n  filter(price > 4999)\n#> # A tibble: 11 × 21\n#>      host_id host_since host_response_time host_is_superhost\n#>        <dbl> <date>     <chr>              <lgl>            \n#>  1   9310264 2013-10-08 N/A                FALSE            \n#>  2  99076885 2016-10-10 N/A                FALSE            \n#>  3 119693302 2017-03-07 N/A                FALSE            \n#>  4 147240941 2017-08-22 N/A                FALSE            \n#>  5 174625477 2018-02-21 N/A                FALSE            \n#>  6  70349386 2016-05-04 N/A                FALSE            \n#>  7 215966560 2018-09-17 N/A                FALSE            \n#>  8  12931053 2014-03-08 within a few hours TRUE             \n#>  9 116137780 2017-02-12 N/A                FALSE            \n#> 10 113826425 2017-01-29 within a few hours FALSE            \n#> 11 184607983 2018-04-16 a few days or more FALSE            \n#> # … with 17 more variables: host_listings_count <dbl>,\n#> #   host_total_listings_count <dbl>,\n#> #   host_neighbourhood <chr>, neighbourhood_cleansed <chr>,\n#> #   room_type <chr>, bathrooms <lgl>, bedrooms <dbl>,\n#> #   price <int>, number_of_reviews <dbl>,\n#> #   has_availability <lgl>, review_scores_rating <dbl>,\n#> #   review_scores_accuracy <dbl>, …\n# It's pretty clear that there is something odd going on with some of these, but some of them seem legit.\nairbnb_data_selected %>%\n  filter(price < 1000) %>% \n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 10) +\n  theme_classic() +\n  labs(x = \"Price per night\",\n       y = \"Number of properties\")\n# Look at distribution of price again\nairbnb_data_selected %>%\n  filter(price > 90) %>% \n  filter(price < 210) %>% \n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 1) +\n  theme_classic() +\n  labs(x = \"Price per night\",\n       y = \"Number of properties\")"},{"path":"exploratory-data-analysis.html","id":"superhosts","chapter":"14 Exploratory data analysis","heading":"14.4.4.2 Superhosts","text":"Airbnb says :Superhosts experienced hosts provide shining example hosts, extraordinary experiences guests.host reaches Superhost status, badge superhost badge automatically appear listing profile help identify .check Superhosts’ activity four times year, ensure program highlights people dedicated providing outstanding hospitality.First ’ll look NAs host_is_superhost.285 ’s clear something odd going - maybe host removed listing similar?’ll also want create binary variable . ’s true/false moment, fine modelling, handful situations ’ll easier 0/1.","code":"\nairbnb_data_selected %>%\n  filter(is.na(host_is_superhost))\n#> # A tibble: 11 × 21\n#>      host_id host_since host_response_time host_is_superhost\n#>        <dbl> <date>     <chr>              <lgl>            \n#>  1  23472830 NA         <NA>               NA               \n#>  2  31675651 NA         <NA>               NA               \n#>  3  75779190 NA         <NA>               NA               \n#>  4  47614482 NA         <NA>               NA               \n#>  5 201103629 NA         <NA>               NA               \n#>  6 111820893 NA         <NA>               NA               \n#>  7  23472830 NA         <NA>               NA               \n#>  8 196269219 NA         <NA>               NA               \n#>  9  23472830 NA         <NA>               NA               \n#> 10 266594170 NA         <NA>               NA               \n#> 11 118516038 NA         <NA>               NA               \n#> # … with 17 more variables: host_listings_count <dbl>,\n#> #   host_total_listings_count <dbl>,\n#> #   host_neighbourhood <chr>, neighbourhood_cleansed <chr>,\n#> #   room_type <chr>, bathrooms <lgl>, bedrooms <dbl>,\n#> #   price <int>, number_of_reviews <dbl>,\n#> #   has_availability <lgl>, review_scores_rating <dbl>,\n#> #   review_scores_accuracy <dbl>, …\nairbnb_data_selected <- \n  airbnb_data_selected %>%\n  mutate(host_is_superhost_binary = case_when(\n    host_is_superhost == TRUE ~ 1,\n    host_is_superhost == FALSE ~ 0,\n    TRUE ~ 999\n    )\n  )\nairbnb_data_selected$host_is_superhost_binary[airbnb_data_selected$host_is_superhost_binary == 999] <- NA"},{"path":"exploratory-data-analysis.html","id":"reviews","chapter":"14 Exploratory data analysis","heading":"14.4.4.3 Reviews","text":"Airbnb says :addition written reviews, guests can submit overall star rating set category star ratings stay.Hosts can view star ratings Progress page, Reviews. Hosts using professional hosting tools can find reviews quality details Performance page, Quality.Guests can give ratings :Overall experience. Overall, stay?Cleanliness. guests feel space clean tidy?Accuracy. accurately listing page represent space? example, guests able find --date info photos listing description.Value. guest feel listing provided good value price?Communication. well communicate stay? Guests often care host responds quickly, reliably, frequently messages questions.Check-. smoothly check-go?Location. guests feel neighbourhood? may mean ’s accurate description proximity access transportation, shopping centres, city centre, etc., description includes special considerations, like noise, family safety.Amenities. guests feel amenities available stay? Guests often care amenities listed available, working, good condition.category, hosts able see often get 5 stars, guests rated nearby hosts, , cases, tips help improve listing.number stars displayed top listing page aggregate primary scores guests given listing. bottom listing page, ’s aggregate category rating. host needs receive star ratings least 3 guests aggregate score appears.TODO: don’t understand review scores constructed. Airbnb says ’s star rating, converting 10 point scale, similarly, constructing overall one, seems 100? ’s lot clumping around 20, 40, 60, 80, 100 - averaging five-star scale rebasing 100?Now ’ll deal NAs review_scores_rating. one complicated lot .Now see ’s just don’t reviews.’s clear almost cases don’t review yet don’t enough reviews. However, ’s large proportion total - almost fifth properties don’t reviews (hence NA review_scores_rating).can use vis_miss visdat package (N. Tierney 2017) make sure components review missing. NAs driven Airbnb requirement least three reviews expect missing.looks pretty convincing almost cases, different variants reviews missing. let’s just focus main review.’s pretty clear almost reviews 80. Let’s just zoom 60 80 range check distribution looks like range.","code":"\nairbnb_data_selected %>%\n  filter(is.na(review_scores_rating))\n#> # A tibble: 4,368 × 22\n#>    host_id host_since host_response_time host_is_superhost\n#>      <dbl> <date>     <chr>              <lgl>            \n#>  1   48239 2009-10-25 N/A                FALSE            \n#>  2  187320 2010-08-01 within a few hours TRUE             \n#>  3  188183 2010-08-01 a few days or more FALSE            \n#>  4  187320 2010-08-01 within a few hours TRUE             \n#>  5  304551 2010-11-29 within an hour     TRUE             \n#>  6  545074 2011-04-29 N/A                FALSE            \n#>  7 1210571 2011-09-26 N/A                FALSE            \n#>  8 1411076 2011-11-15 N/A                FALSE            \n#>  9 1409872 2011-11-15 N/A                FALSE            \n#> 10 1664812 2012-01-28 N/A                FALSE            \n#> # … with 4,358 more rows, and 18 more variables:\n#> #   host_listings_count <dbl>,\n#> #   host_total_listings_count <dbl>,\n#> #   host_neighbourhood <chr>, neighbourhood_cleansed <chr>,\n#> #   room_type <chr>, bathrooms <lgl>, bedrooms <dbl>,\n#> #   price <int>, number_of_reviews <dbl>,\n#> #   has_availability <lgl>, review_scores_rating <dbl>, …\nairbnb_data_selected %>%\n  filter(is.na(review_scores_rating)) %>% \n  select(number_of_reviews) %>% \n  table()\n#> .\n#>    0    1    2    3    4 \n#> 4105  227   23   10    3\n# We'll just check whether this is the same for all of the different variants of reviews\nairbnb_data_selected %>% \n  select(review_scores_rating, \n         review_scores_accuracy, \n         review_scores_cleanliness, \n         review_scores_checkin, \n         review_scores_communication, \n         review_scores_location, \n         review_scores_value) %>% \n  vis_miss()\nairbnb_data_selected %>%\n  filter(!is.na(review_scores_rating)) %>% \n  ggplot(aes(x = review_scores_rating)) +\n  geom_histogram(binwidth = 1) +\n  theme_classic() +\n  labs(x = \"Average review score\",\n       y = \"Number of properties\")\nairbnb_data_selected %>%\n  filter(!is.na(review_scores_rating)) %>% \n  filter(review_scores_rating > 60) %>%\n  filter(review_scores_rating < 80) %>% \n  ggplot(aes(x = review_scores_rating)) +\n  geom_histogram(binwidth = 1) +\n  theme_classic() +\n  labs(x = \"Average review score\",\n       y = \"Number of properties\")"},{"path":"exploratory-data-analysis.html","id":"response-time","chapter":"14 Exploratory data analysis","heading":"14.4.4.4 Response time","text":"Airbnb says :Hosts 24 hours officially accept decline reservation requests. ’ll updated email status request.half reservation requests accepted within one hour received. vast majority hosts reply within 12 hours.host confirms request, payment processed collected Airbnb full. host declines request request expires, don’t process payment.TODO: don’t understand can get response time NA? must related variable.Looking now response time:Interestingly seems like looks like ‘NAs’ host_response_time variable coded proper NAs, instead treated another category. ’ll recode actual NAs.clearly issues NAs. probably want filter away example ’s just quick example, awful lot (20 per cent) ’ll quick look relation review score.seem awful lot overall review 100. also awful lot review score NA.","code":"\ntable(airbnb_data_selected$host_response_time)\n#> \n#> a few days or more                N/A       within a day \n#>                816               8469               1235 \n#> within a few hours     within an hour \n#>               2062               5672\nairbnb_data_selected$host_response_time[airbnb_data_selected$host_response_time == \"N/A\"] <- NA\nairbnb_data_selected %>% \n  filter(is.na(host_response_time)) %>% \n  ggplot(aes(x = review_scores_rating)) +\n  geom_histogram(binwidth = 1)\n#> Warning: Removed 2590 rows containing non-finite values\n#> (stat_bin).\nairbnb_data_selected %>% \n  filter(is.na(host_response_time)) %>%\n  filter(is.na(review_scores_rating))\n#> # A tibble: 2,590 × 22\n#>    host_id host_since host_response_time host_is_superhost\n#>      <dbl> <date>     <chr>              <lgl>            \n#>  1   48239 2009-10-25 <NA>               FALSE            \n#>  2  545074 2011-04-29 <NA>               FALSE            \n#>  3 1210571 2011-09-26 <NA>               FALSE            \n#>  4 1411076 2011-11-15 <NA>               FALSE            \n#>  5 1409872 2011-11-15 <NA>               FALSE            \n#>  6 1664812 2012-01-28 <NA>               FALSE            \n#>  7 1828773 2012-02-28 <NA>               FALSE            \n#>  8 1923052 2012-03-14 <NA>               FALSE            \n#>  9 2432916 2012-05-22 <NA>               FALSE            \n#> 10 2577688 2012-06-07 <NA>               FALSE            \n#> # … with 2,580 more rows, and 18 more variables:\n#> #   host_listings_count <dbl>,\n#> #   host_total_listings_count <dbl>,\n#> #   host_neighbourhood <chr>, neighbourhood_cleansed <chr>,\n#> #   room_type <chr>, bathrooms <lgl>, bedrooms <dbl>,\n#> #   price <int>, number_of_reviews <dbl>,\n#> #   has_availability <lgl>, review_scores_rating <dbl>, …"},{"path":"exploratory-data-analysis.html","id":"host-number-of-listings","chapter":"14 Exploratory data analysis","heading":"14.4.4.5 Host number of listings","text":"two versions variable telling many properties host Airbnb, start just check whether ’s difference.none dataset can just remove one column now quick look one.large number somewhere 2-10 properties range, usual long tail. number 0 listings unexpected worth following . bunch NA ’ll need deal .’s nothing immediately jumps odd people zero listings, must something going .Based dataset, ’s third way looking number properties someone ’s look number times unique ID occurs.makes clear many multiple properties listed.","code":"\nairbnb_data_selected %>% \n  mutate(listings_count_is_same = if_else(host_listings_count == host_total_listings_count, 1, 0)) %>% \n  filter(listings_count_is_same == 0)\n#> # A tibble: 0 × 23\n#> # … with 23 variables: host_id <dbl>, host_since <date>,\n#> #   host_response_time <chr>, host_is_superhost <lgl>,\n#> #   host_listings_count <dbl>,\n#> #   host_total_listings_count <dbl>,\n#> #   host_neighbourhood <chr>, neighbourhood_cleansed <chr>,\n#> #   room_type <chr>, bathrooms <lgl>, bedrooms <dbl>,\n#> #   price <int>, number_of_reviews <dbl>, …\nairbnb_data_selected <- \n  airbnb_data_selected %>% \n  select(-host_listings_count)\n\nairbnb_data_selected %>% \n  count(host_total_listings_count)\n#> # A tibble: 49 × 2\n#>    host_total_listings_count     n\n#>                        <dbl> <int>\n#>  1                         0  2128\n#>  2                         1  7662\n#>  3                         2  2476\n#>  4                         3  1384\n#>  5                         4   802\n#>  6                         5   478\n#>  7                         6   385\n#>  8                         7   270\n#>  9                         8   291\n#> 10                         9   170\n#> # … with 39 more rows\nairbnb_data_selected %>% \n  filter(host_total_listings_count == 0) %>% \n  head()\n#> # A tibble: 6 × 21\n#>   host_id host_since host_response_time host_is_superhost\n#>     <dbl> <date>     <chr>              <lgl>            \n#> 1  140602 2010-06-08 <NA>               FALSE            \n#> 2 1024550 2011-08-26 <NA>               FALSE            \n#> 3 2647656 2012-06-15 <NA>               FALSE            \n#> 4 3783106 2012-10-06 within an hour     FALSE            \n#> 5 3814089 2012-10-09 within an hour     FALSE            \n#> 6 3827668 2012-10-10 within a day       FALSE            \n#> # … with 17 more variables:\n#> #   host_total_listings_count <dbl>,\n#> #   host_neighbourhood <chr>, neighbourhood_cleansed <chr>,\n#> #   room_type <chr>, bathrooms <lgl>, bedrooms <dbl>,\n#> #   price <int>, number_of_reviews <dbl>,\n#> #   has_availability <lgl>, review_scores_rating <dbl>,\n#> #   review_scores_accuracy <dbl>, …\nairbnb_data_selected %>% \n  count(host_id) %>% \n  arrange(-n) %>% \n  head()\n#> # A tibble: 6 × 2\n#>     host_id     n\n#>       <dbl> <int>\n#> 1  10202618    74\n#> 2   1919294    63\n#> 3 152088065    59\n#> 4 293274089    56\n#> 5    785826    54\n#> 6    846505    46"},{"path":"exploratory-data-analysis.html","id":"decisions","chapter":"14 Exploratory data analysis","heading":"14.4.4.6 Decisions","text":"purpose document just give quick introduction using real-world data, ’ll just remove anything annoying, ’re using research ’d need justify decisions /possibly make different ones.Get rid prices $999.Get rid anyone NA whether super host.Get rid anyone NA main review score - removes roughly 20 per cent observations.Get rid anyone main review score less 70.Get rid anyone NA response time - removes roughly another 20 per cent observations.TODO: don’t next step ’ve already got rid point. ’s something systematic going come back look .Get rid anyone NA number properties.Get rid anyone 100 review_scores_rating.keep people one property:","code":"\nairbnb_data_filtered <- \n  airbnb_data_selected %>% \n  filter(price < 1000)\ndim(airbnb_data_filtered)\n#> [1] 18120    21\n# Just remove host_is_superhost NAs for now.\nairbnb_data_filtered <- \n  airbnb_data_filtered %>%\n  filter(!is.na(host_is_superhost))\ndim(airbnb_data_filtered)\n#> [1] 18109    21\n# We'll just get rid of them for now, but this is probably something that deserves more attention - possibly in an appendix or similar.\nairbnb_data_filtered <- \n  airbnb_data_filtered %>%\n  filter(!is.na(review_scores_rating))\n# There are still some where the rest of the reviews are missing even though there is a main review score\n# There seem to be an awful lot that have an overall review of 100. Does that make sense?\ndim(airbnb_data_filtered)\n#> [1] 13801    21\n# We'll just get rid of them for now, but this is probably something that deserves more attention - possibly in an appendix or similar.\nairbnb_data_filtered <- \n  airbnb_data_filtered %>%\n  filter(review_scores_rating > 69)\n# There are still some where the rest of the reviews are missing even though there is a main review score\n# There seem to be an awful lot that have an overall review of 100. Does that make sense?\ndim(airbnb_data_filtered)\n#> [1] 13471    21\nairbnb_data_filtered <- \n  airbnb_data_filtered %>% \n  filter(!is.na(host_response_time))\ndim(airbnb_data_filtered)\n#> [1] 7763   21\nairbnb_data_filtered <- \n  airbnb_data_filtered %>% \n  filter(!is.na(host_total_listings_count))\ndim(airbnb_data_filtered)\n#> [1] 7763   21\nairbnb_data_filtered <- \n  airbnb_data_filtered %>% \n  filter(review_scores_rating != 100)\ndim(airbnb_data_filtered)\n#> [1] 5648   21\nairbnb_data_filtered <- \n  airbnb_data_filtered %>% \n  add_count(host_id) %>% \n  filter(n == 1) %>% \n  select(-n)\ndim(airbnb_data_filtered)\n#> [1] 2304   21"},{"path":"exploratory-data-analysis.html","id":"explore-data","chapter":"14 Exploratory data analysis","heading":"14.4.5 Explore data","text":"might like make graphs see can relationships jump . aspects come mind looking prices reviews super hosts, number properties neighbourhood.","code":""},{"path":"exploratory-data-analysis.html","id":"price-and-reviews","chapter":"14 Exploratory data analysis","heading":"14.4.5.1 Price and reviews","text":"Look relationship price reviews, whether super-host.","code":"\n# Look at both price and reviews\nairbnb_data_filtered %>%\n  ggplot(aes(x = price, y = review_scores_rating, color = host_is_superhost)) +\n  geom_point(size = 1, alpha = 0.1) + # Make the points smaller and more transparent as they overlap considerably.\n  theme_classic() +\n  labs(x = \"Price per night\",\n       y = \"Average review score\",\n       color = \"Super host\") + # Probably should recode this to more meaningful than TRUE/FALSE.\n  scale_color_brewer(palette = \"Set1\")"},{"path":"exploratory-data-analysis.html","id":"superhost-and-response-time","chapter":"14 Exploratory data analysis","heading":"14.4.5.2 Superhost and response-time","text":"One aspects may make someone super host quickly respond enquiries. One imagine superhost involves quickly saying yes enquiries. Let’s look data. First, want look possible values superhost response times.Fortunately, looks like removed reviews rows removed NAs whether super host, go back look may need check . tabyl function within janitor package (Firke, 2020) list NAs , case don’t trust , another way check try filter just NAs.Now let’s look response time.vast majority respond within hour.Finally, can look cross-tab.someone doesn’t respond within hour ’s unlikely super host.","code":"\nairbnb_data_filtered %>% \n  tabyl(host_is_superhost) %>% \n  adorn_totals(\"row\") %>% \n  adorn_pct_formatting()\n#>  host_is_superhost    n percent\n#>              FALSE 1224   53.1%\n#>               TRUE 1080   46.9%\n#>              Total 2304  100.0%\nairbnb_data_filtered %>% \n  filter(is.na(host_is_superhost))\n#> # A tibble: 0 × 21\n#> # … with 21 variables: host_id <dbl>, host_since <date>,\n#> #   host_response_time <chr>, host_is_superhost <lgl>,\n#> #   host_total_listings_count <dbl>,\n#> #   host_neighbourhood <chr>, neighbourhood_cleansed <chr>,\n#> #   room_type <chr>, bathrooms <lgl>, bedrooms <dbl>,\n#> #   price <int>, number_of_reviews <dbl>,\n#> #   has_availability <lgl>, review_scores_rating <dbl>, …\nairbnb_data_filtered %>% \n  tabyl(host_response_time) %>% \n  adorn_totals(\"row\") %>% \n  adorn_pct_formatting()\n#>  host_response_time    n percent\n#>  a few days or more  167    7.2%\n#>        within a day  378   16.4%\n#>  within a few hours  519   22.5%\n#>      within an hour 1240   53.8%\n#>               Total 2304  100.0%\nairbnb_data_filtered %>% \n  tabyl(host_response_time, host_is_superhost) %>% \n  adorn_percentages(\"row\") %>%\n  adorn_pct_formatting(digits = 0) %>%\n  adorn_ns() %>% \n  adorn_title()\n#>                     host_is_superhost          \n#>  host_response_time             FALSE      TRUE\n#>  a few days or more         88% (147) 12%  (20)\n#>        within a day         67% (254) 33% (124)\n#>  within a few hours         51% (266) 49% (253)\n#>      within an hour         45% (557) 55% (683)"},{"path":"exploratory-data-analysis.html","id":"neighbourhood","chapter":"14 Exploratory data analysis","heading":"14.4.5.3 Neighbourhood","text":"Finally, let’s look neighbourhood. data provider attempted clean neighbourhood variable us, ’ll just use now. analysis properly, ’d need check whether ’d made mistakes.","code":"\n# We expect something in the order of 100 to 150 neighbourhoods, with the top ten accounting for a large majority of listings.\nairbnb_data_filtered %>% \n  tabyl(neighbourhood_cleansed) %>% \n  adorn_totals(\"row\") %>% \n  adorn_pct_formatting() %>% \n  nrow()\n#> [1] 140\nairbnb_data_filtered %>% \n  tabyl(neighbourhood_cleansed) %>% \n  adorn_pct_formatting() %>% \n  arrange(-n) %>% \n  filter(n > 100) %>% \n  adorn_totals(\"row\") %>% \n  head()\n#>             neighbourhood_cleansed   n percent\n#>  Waterfront Communities-The Island 409   17.8%\n#>                            Niagara 101    4.4%\n#>                              Total 510       -"},{"path":"exploratory-data-analysis.html","id":"model-data","chapter":"14 Exploratory data analysis","heading":"14.4.6 Model data","text":"now run models dataset. first split data test/training groups, using functions tidymodels package (Kuhn Wickham 2020) (like tidyverse package (Wickham et al. 2019a) package packages).","code":"\nset.seed(853)\n\nairbnb_data_filtered_split <- \n  airbnb_data_filtered %>%\n  initial_split(prop = 3/4)\n\nairbnb_train <- training(airbnb_data_filtered_split)\nairbnb_test <- testing(airbnb_data_filtered_split)\n\nrm(airbnb_data_filtered_split)"},{"path":"exploratory-data-analysis.html","id":"logistic-regression-1","chapter":"14 Exploratory data analysis","heading":"14.4.6.1 Logistic regression","text":"may like look whether can forecast whether someone super host, factors go explaining . dependent variable binary, good opportunity look logistic regression. expect better reviews associated faster responses higher reviews. Specifically, model estimate :\\[\\mbox{Prob(super host} = 1) = \\beta_0 + \\beta_1 \\mbox{Response time} + \\beta_2 \\mbox{Reviews} + \\epsilon\\]estimate model using glm R language (R Core Team 2021).can quick look results, instance, summary function. also use tidy glance broom package (D. Robinson, Hayes, Couch 2020). details , broom functions tibbles, means can easily deal within tidy framework.might like look model predicts, compared whether person actually super host. can variety ways, one way use augment broom package (D. Robinson, Hayes, Couch 2020). add prediction associated uncertainty data. every row probability model estimating superhost. ultimately, need binary forecast. bunch different options, one just say model estimates probability 0.5 bin superhost, .can look far model . bunch ways , one look probability model given person.can look model probabilities change based average review score, average time respond.nice thing graph illustrates nicely effect host average response time , say, ‘within hour’ compared ‘within hours.’can focus model terms raw classification using confusionMatrix caret package (Kuhn, 2020). also gives bunch diagnostics (help file explains ). general, suggest isn’t best model ’s ever existed.case, point ’ve looking model done training set. ’s also relevant test set. , bunch ways , one use augment function, include newdata argument.expect performance slightly worse test set. ’s actually fairly similar.compare test training sets terms forecasts.","code":"\nlogistic_reg_superhost_response_review <- glm(host_is_superhost ~ \n                                                host_response_time + \n                                                review_scores_rating,\n                                              data = airbnb_train,\n                                              family = binomial\n                                              )\nsummary(logistic_reg_superhost_response_review)\n#> \n#> Call:\n#> glm(formula = host_is_superhost ~ host_response_time + review_scores_rating, \n#>     family = binomial, data = airbnb_train)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -1.9297  -0.8873  -0.0416   0.8310   4.0657  \n#> \n#> Coefficients:\n#>                                      Estimate Std. Error\n#> (Intercept)                          -40.5578     2.4208\n#> host_response_timewithin a day         1.5351     0.3466\n#> host_response_timewithin a few hours   1.9520     0.3360\n#> host_response_timewithin an hour       2.2883     0.3240\n#> review_scores_rating                   0.4037     0.0248\n#>                                      z value Pr(>|z|)    \n#> (Intercept)                          -16.754  < 2e-16 ***\n#> host_response_timewithin a day         4.429 9.46e-06 ***\n#> host_response_timewithin a few hours   5.809 6.28e-09 ***\n#> host_response_timewithin an hour       7.062 1.65e-12 ***\n#> review_scores_rating                  16.276  < 2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 2392.2  on 1727  degrees of freedom\n#> Residual deviance: 1750.0  on 1723  degrees of freedom\n#> AIC: 1760\n#> \n#> Number of Fisher Scoring iterations: 6\ntidy(logistic_reg_superhost_response_review)\n#> # A tibble: 5 × 5\n#>   term                 estimate std.error statistic  p.value\n#>   <chr>                   <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)           -40.6      2.42      -16.8  5.31e-63\n#> 2 host_response_timew…    1.54     0.347       4.43 9.46e- 6\n#> 3 host_response_timew…    1.95     0.336       5.81 6.28e- 9\n#> 4 host_response_timew…    2.29     0.324       7.06 1.65e-12\n#> 5 review_scores_rating    0.404    0.0248     16.3  1.45e-59\nglance(logistic_reg_superhost_response_review)\n#> # A tibble: 1 × 8\n#>   null.deviance df.null logLik   AIC   BIC deviance\n#>           <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>\n#> 1         2392.    1727  -875. 1760. 1787.    1750.\n#> # … with 2 more variables: df.residual <int>, nobs <int>\nairbnb_data_filtered_logistic_fit_train <- \n  augment(logistic_reg_superhost_response_review, \n          data = airbnb_train %>% select(host_is_superhost, \n                                         host_is_superhost_binary,\n                                         host_response_time,\n                                         review_scores_rating\n                                         ),\n          type.predict = \"response\") %>% # We use the \"response\" option here so that the function does the work of worrying about the exponential and log odds for us. Our result will be a probability.\n  select(-.hat, -.sigma, -.cooksd, -.std.resid) %>% \n  mutate(predict_host_is_superhost = if_else(.fitted > 0.5, 1, 0), # How do things change if we change the 0.5 cutoff?\n         host_is_superhost_binary = as.factor(host_is_superhost_binary),\n         predict_host_is_superhost_binary = as.factor(predict_host_is_superhost)\n         )\nairbnb_data_filtered_logistic_fit_train %>% \n  ggplot(aes(x = .fitted, fill = host_is_superhost_binary)) +\n  geom_histogram(binwidth = 0.05, position = \"dodge\") +\n  theme_classic() +\n  labs(x = \"Estimated probability that host is super host\",\n       y = \"Number\",\n       fill = \"Host is super host\") +\n  scale_fill_brewer(palette = \"Set1\")\nggplot(airbnb_data_filtered_logistic_fit_train, \n       aes(x = review_scores_rating, \n           y = .fitted, \n           color = host_response_time)) +\n  geom_line() +\n  geom_point() +\n  labs(x = \"Average review score\",\n       y = \"Predicted probability of being a superhost\",\n       color = \"Host response time\") +\n  theme_classic() +\n  scale_color_brewer(palette = \"Set1\")\ncaret::confusionMatrix(data = airbnb_data_filtered_logistic_fit_train$predict_host_is_superhost_binary,\n                       reference = airbnb_data_filtered_logistic_fit_train$host_is_superhost_binary)\n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction   0   1\n#>          0 619 124\n#>          1 283 702\n#>                                           \n#>                Accuracy : 0.7645          \n#>                  95% CI : (0.7437, 0.7843)\n#>     No Information Rate : 0.522           \n#>     P-Value [Acc > NIR] : < 2.2e-16       \n#>                                           \n#>                   Kappa : 0.5318          \n#>                                           \n#>  Mcnemar's Test P-Value : 4.811e-15       \n#>                                           \n#>             Sensitivity : 0.6863          \n#>             Specificity : 0.8499          \n#>          Pos Pred Value : 0.8331          \n#>          Neg Pred Value : 0.7127          \n#>              Prevalence : 0.5220          \n#>          Detection Rate : 0.3582          \n#>    Detection Prevalence : 0.4300          \n#>       Balanced Accuracy : 0.7681          \n#>                                           \n#>        'Positive' Class : 0               \n#> \nairbnb_data_filtered_logistic_fit_test <- \n  augment(logistic_reg_superhost_response_review, \n          data = airbnb_train %>% select(host_is_superhost, \n                                         host_is_superhost_binary,\n                                         host_response_time,\n                                         review_scores_rating\n                                         ),\n          newdata = airbnb_test %>% select(host_is_superhost, \n                                         host_is_superhost_binary,\n                                         host_response_time,\n                                         review_scores_rating\n                                         ), # I'm selecting just because the\n          # dataset is quite wide, and so this makes it easier to look at.\n          type.predict = \"response\") %>% \n  mutate(predict_host_is_superhost = if_else(.fitted > 0.5, 1, 0), \n         host_is_superhost_binary = as.factor(host_is_superhost_binary),\n         predict_host_is_superhost_binary = as.factor(predict_host_is_superhost)\n         )\ncaret::confusionMatrix(data = airbnb_data_filtered_logistic_fit_test$predict_host_is_superhost_binary,\n                       reference = airbnb_data_filtered_logistic_fit_test$host_is_superhost_binary)\n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction   0   1\n#>          0 197  36\n#>          1 125 218\n#>                                           \n#>                Accuracy : 0.7205          \n#>                  95% CI : (0.6819, 0.7568)\n#>     No Information Rate : 0.559           \n#>     P-Value [Acc > NIR] : 1.018e-15       \n#>                                           \n#>                   Kappa : 0.4533          \n#>                                           \n#>  Mcnemar's Test P-Value : 4.052e-12       \n#>                                           \n#>             Sensitivity : 0.6118          \n#>             Specificity : 0.8583          \n#>          Pos Pred Value : 0.8455          \n#>          Neg Pred Value : 0.6356          \n#>              Prevalence : 0.5590          \n#>          Detection Rate : 0.3420          \n#>    Detection Prevalence : 0.4045          \n#>       Balanced Accuracy : 0.7350          \n#>                                           \n#>        'Positive' Class : 0               \n#> \ntraining <- airbnb_data_filtered_logistic_fit_train %>% \n  select(host_is_superhost_binary, .fitted) %>% \n  mutate(type = \"Training set\")\n\ntest <- airbnb_data_filtered_logistic_fit_test %>% \n  select(host_is_superhost_binary, .fitted) %>% \n  mutate(type = \"Test set\")\n\nboth <- rbind(training, test)\nrm(training, test)\n\nboth %>% \n  ggplot(aes(x = .fitted, \n             fill = host_is_superhost_binary)) +\n  geom_histogram(binwidth = 0.05, position = \"dodge\") +\n  theme_minimal() +\n  labs(x = \"Estimated probability that host is super host\",\n       y = \"Number\",\n       fill = \"Host is super host\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  facet_wrap(vars(type),\n             nrow = 2,\n             scales = \"free_y\")"},{"path":"exploratory-data-analysis.html","id":"exercises-and-tutorial-13","chapter":"14 Exploratory data analysis","heading":"14.5 Exercises and tutorial","text":"","code":""},{"path":"exploratory-data-analysis.html","id":"exercises-13","chapter":"14 Exploratory data analysis","heading":"14.5.1 Exercises","text":"words exploratory data analysis (difficult, please write one nuanced paragraph)?Tukey’s words, exploratory data analysis (please write one paragraph)?Tukey (please write paragraph two)?Tukey’s link DoSS (hint: advisor someone’s PhD - person)?Can identify female equivalent Tukey (historians statistics) may overlooked?dataset called ‘my_data,’ two columns: ‘first_col’ ‘second_col,’ please write rough R code generate graph (type graph doesn’t matter).Consider dataset 500 rows 3 columns, 1,500 cells. 100 cells missing data least one columns, remove whole row dataset try run analysis data , procedure? dataset 10,000 rows instead, number missing cells?Please note three ways identifying unusual values.difference categorical continuous variable?difference factor integer variable?can think systematically excluded dataset?Using opendatatoronto package, download data mayoral campaign contributions 2014. (note: 2014 file get get_resource, just keep sheet relates Mayor election).\nClean data format (fixing parsing issue standardizing column names using janitor)\nSummarize variables dataset. missing values, , worried ? every variable format ? , create new variable(s) right format.\nVisually explore distribution values contributions. contributions notable outliers? share similar characteristic(s)? may useful plot distribution contributions without outliers get better sense majority data.\nList top five candidates categories: 1) total contributions; 2) mean contribution; 3) number contributions.\nRepeat process, without contributions candidates .\nmany contributors gave money one candidate?\nClean data format (fixing parsing issue standardizing column names using janitor)Summarize variables dataset. missing values, , worried ? every variable format ? , create new variable(s) right format.Visually explore distribution values contributions. contributions notable outliers? share similar characteristic(s)? may useful plot distribution contributions without outliers get better sense majority data.List top five candidates categories: 1) total contributions; 2) mean contribution; 3) number contributions.Repeat process, without contributions candidates .many contributors gave money one candidate?Name three geoms produce graphs bars ggplot().Consider dataset 10,000 observations 27 variables. observation, least one missing variable. Please discuss, paragraph two, steps take understand going .Known missing data, leave holes dataset. data never collected? Please look McClelland, Alexander, 2019, ‘“Lock Whore ”: Legal Violence Flows Information Precipitating Personal Violence People Criminalised HIV-Related Crimes Canada,’ European Journal Risk Regulation, 10 (1), pp. 132-147. look Policing Pandemic - https://www.policingthepandemic.ca/. Look gathered dataset took put together. dataset ? missing ? affect results? might similar biases enter datasets used read ?","code":""},{"path":"exploratory-data-analysis.html","id":"tutorial-13","chapter":"14 Exploratory data analysis","heading":"14.5.2 Tutorial","text":"","code":""},{"path":"ijalm.html","id":"ijalm","chapter":"15 It’s Just A Linear Model","heading":"15 It’s Just A Linear Model","text":"STATUS: construction.Required readingGreenland, Sander, Stephen J. Senn, Kenneth J. Rothman, John B. Carlin, Charles Poole, Steven N. Goodman, Douglas G. Altman, 2016, ‘Statistical tests, P values, confidence intervals, power: guide misinterpretations,’ European journal epidemiology, 31, . 4, pp. 337-350.James, Gareth, Daniela Witten, Trevor Hastie Robert Tibshirani, 2017, Introduction Statistical Learning Applications R, 1st Edition, Chapters 3 4.1-4.3., https://www.statlearning.com.Obermeyer, Z., Powers, B., Vogeli, C., & Sendhill, M., 2019, ‘Dissecting racial bias algorithm used manage health populations,’ Science, (366): 447-453.Wickham, Hadley, Garrett Grolemund, 2017, R Data Science, Chapter 23, https://r4ds..co.nz/.Zook M, Barocas S, boyd d, Crawford K, Keller E, Gangadharan SP, et al. (2017) ‘Ten simple rules responsible big data research,’ PLoS Comput Biol 13(3): e1005399. https://doi.org/10.1371/journal.pcbi.1005399Recommended readingAngrist, Joshua D., Jörn-Steffen Pischke, 2008, Mostly harmless econometrics: empiricist’s companion, Princeton University Press, Chapter 3.4.3.Cunningham, Scott, Causal Inference: Mixtape, Chapter 2, Yale University Press, https://mixtape.scunning.com.ElHabr, Tony, 2019, ‘Bayesian Approach Ranking English Premier League Teams (using R),’ https://tonyelhabr.rbind.io/post/bayesian-statistics-english-premier-league/.Ioannidis, John PA, 2005, ‘published research findings false,’ PLoS medicine, 2, . 8, e124.Pavlik, Kaylin, 2018, ‘Exploring Relationship Dog Names Breeds,’ https://www.kaylinpavlik.com/dog-names-tfidf/.Pavlik, Kaylin, 2019, ‘Understanding + classifying genres using Spotify audio features,’ https://www.kaylinpavlik.com/classifying-songs-genres/.Silge, Julia, 2019, ‘Modeling salary gender tech industry,’ https://juliasilge.com/blog/salary-gender/.Silge, Julia, 2019, ‘Opioid prescribing habits Texas,’ https://juliasilge.com/blog/texas-opioids/.Silge, Julia, 2019, ‘Tidymodels,’ https://juliasilge.com/blog/intro-tidymodels/.Silge, Julia, 2020, ‘#TidyTuesday hotel bookings recipes,’ https://juliasilge.com/blog/hotels-recipes/.Silge, Julia, 2020, ‘Hyperparameter tuning #TidyTuesday food consumption,’ https://juliasilge.com/blog/food-hyperparameter-tune/.Taddy, Matt, 2019, Business Data Science, Chapters 2 4.Wasserstein, Ronald L. Nicole . Lazar, 2016, ‘ASA Statement p-Values: Context, Process, Purpose,’ American Statistician, 70:2, 129-133, DOI: 10.1080/00031305.2016.1154108.Fun readingChellel, Kit, 2018, ‘Gambler Cracked Horse-Racing Code,’ Bloomberg Businessweek, 3 May, https://www.bloomberg.com/news/features/2018-05-03/-gambler--cracked--horse-racing-code.Key concepts/skills/etcSimple multiple linear regression.Logistic Poisson regression.key role uncertainty.Threats validity inferencesOverfitting.Key librariesbroomhuxtablerstanarmtidymodelstidyverseKey functionsbroom::augment()broom::glance()broom::tidy()glm()huxtable::huxreg()lm()parsnip::fit()parsnip::linear_reg()parsnip::logistic_reg()parsnip::set_engine()poissonreg::poisson_reg()rnorm()rpois()rsample::initial_split()rsample::testing()rsample::training()sample()set.seed()summary()QuizPlease write linear relationship response variable, Y, predictor, X. intercept term? slope term? adding hat indicate?least squares criterion? Similarly, RSS trying run least squares regression?statistical bias?three variables: Snow, Temperature, Wind, please write R code fit simple linear regression explain Snow function Temperature Wind. think another explanatory variable - daily stock market returns - model?According Greenland et al. (2016), p-values test (pick one)?\nassumptions data generated (entire model), just targeted hypothesis supposed test (null hypothesis).\nWhether hypothesis targeted testing true .\ndichotomy whereby results can declared ‘statistically significant.’\nassumptions data generated (entire model), just targeted hypothesis supposed test (null hypothesis).Whether hypothesis targeted testing true .dichotomy whereby results can declared ‘statistically significant.’According Greenland et al. (2016), p-value may small (select )?\ntargeted hypothesis false.\nstudy protocols violated.\nselected presentation based small size.\ntargeted hypothesis false.study protocols violated.selected presentation based small size.According Obermeyer et al. (2019), racial bias occur algorithm used guide health decisions US (pick one)?\nalgorithm uses health costs proxy health needs.\nalgorithm trained Reddit data.\nalgorithm uses health costs proxy health needs.algorithm trained Reddit data.use logistic regression (pick one)?\nContinuous dependent variable.\nBinary dependent variable.\nCount dependent variable.\nContinuous dependent variable.Binary dependent variable.Count dependent variable.interested studying voting intentions recent US presidential election vary individual’s income. set logistic regression model study relationship. study, one possible dependent variable (pick one)?\nWhether respondent US citizen (yes/)\nrespondent’s personal income (high/low)\nWhether respondent going vote Trump (yes/)\nrespondent voted 2016 (Trump/Clinton)\nWhether respondent US citizen (yes/)respondent’s personal income (high/low)Whether respondent going vote Trump (yes/)respondent voted 2016 (Trump/Clinton)interested studying voting intentions recent US presidential election vary individual’s income. set logistic regression model study relationship. study, one possible dependent variable (pick one)?\nrace respondent (white/white)\nrespondent’s marital status (married/)\nWhether respondent registered vote (yes/)\nWhether respondent going vote Biden (yes/)\nrace respondent (white/white)respondent’s marital status (married/)Whether respondent registered vote (yes/)Whether respondent going vote Biden (yes/)Please explain p-value , using term (.e. ‘p-value’) words amongst 1,000 common English language according XKCD Simple Writer - https://xkcd.com/simplewriter/. (Please write one two paragraphs.)mean Poisson distribution equal ?\nMedian.\nStandard deviation.\nVariance.\nMedian.Standard deviation.Variance.","code":""},{"path":"ijalm.html","id":"overview","chapter":"15 It’s Just A Linear Model","heading":"15.1 Overview","text":"Words! Mere words! terrible ! clear, vivid, cruel! One escape . yet subtle magic ! seemed able give plastic form formless things, music sweet viol lute. Mere words! anything real words?Oscar Wilde, Picture Dorian Gray.Regression sort . Regression indeed oracle, cruel one. speaks riddles delights punishing us asking bad questions.McElreath (2020, 162).Linear models around long time, least since Galton many others (eugenicists) used linear regression earnest. generalized linear model framework came , formal sense, 70s seminal folks Nelder Wedderburn (Nelder Wedderburn 1972). idea generalized linear models broaden types outcomes allowed. ’re still modelling things linear function, ’re constrained outcome normally distributed. outcome can anything exponential family. , well, generalization generalized linear models generalized additive models ’re generalizing anything outcome, instead structure explanatory side, . ’re still explaining dependent variable additive function bits, bits can functions. framework, way, came 90s, Hastie Tibshirani (Hastie Tibshirani 1990) (fun fact, Tibshirani stats masters Toronto, professor 1985 1998!).’s important recognise build models discovering ‘truth.’ using model help us explore understand data . one best model, just useful models help us learn something data hence, hopefully, something world data generated. Ben Rhodes, Obama staffer, titled White House memoirs ‘World : Memoir Obama White House.’ use models, similarly trying understand world, second part title makes clear, enormous constraints perspective. way ’d expect Rhodes advocate Australian, Canadian, even US Republican, perspective world, ’s silly expect one model universal.use models understand world. poke, push, test . build rejoice beauty, seek understand limits ultimately destroy . process important, process allows us better understand world. McElreath (2020, 19) talks small large worlds, saying ‘()ll statistical modeling two frames: small world model large world hope deploy model .’ extent model trained experiences straight, cis, men, speak world ? ’s worthless, ’s also unimpeachable. extent model teach us data ? extent data reflect world like draw conclusions? Keep questions front mind.Much statistics developed vacuum. ’s reasonable developed situations X, Y Z. original statisticians literally able randomise order fields planting literally worked agricultural stations (CITE). However, almost subsequent applications properties. often teach undergraduates science proceeds (ADD POINTS NULL HYPOTHESIS POPPER). believe ’s works, bridge sell . Scientists react incentives. dabble, guess, test, follow guesses backfill. apply grant funding things last time (know ’ll work) spend money conduct things. fine. ’s world traditional null hypothesis holds, means p-values power lose meaning. need understand ‘old world,’ also need sophisticated enough understand need move away .chapter … called ‘’s Just Linear Model’ famous quote Professor Daniela Witten, identifies far can get linear models huge extent underpin statistics.","code":""},{"path":"ijalm.html","id":"simple-linear-regression","chapter":"15 It’s Just A Linear Model","heading":"15.2 Simple linear regression","text":"\nFigure 15.1: Oh .\nSource: Mijke Rhemtulla, 3 March 2020.","code":""},{"path":"ijalm.html","id":"overview-1","chapter":"15 It’s Just A Linear Model","heading":"15.2.1 Overview","text":"two continuous variables use simple linear regression. based Normal (also ‘Gaussian’) distribution. Pitman (1993, 94) ‘normal distribution mean \\(\\mu\\) standard deviation \\(\\sigma\\) distribution x-axis defined areas normal curve parameters. equation normal curve parameters \\(\\mu\\) \\(\\sigma\\), can written :\n\\[y = \\frac{1}{\\sqrt{2\\pi\\sigma}}e^{-\\frac{1}{2}z^2},\\]\n\\(z = (x - \\mu)/\\sigma\\) measures number standard deviations mean \\(\\mu\\) number \\(x\\).’R can simulate \\(n\\) data points Normal distribution rnorm().take draws get expected shape.use simple linear regression, assume relationship characterised variables parameters, difference, often denoted \\(\\epsilon\\), expectation reality normally distributed.two variables, \\(Y\\) \\(X\\), characterise relationship :\n\\[Y \\sim \\beta_0 + \\beta_1 X.\\]two coefficients/parameters: ‘intercept’ \\(\\beta_0\\), ‘slope’ \\(\\beta_1\\). saying \\(Y\\) value, \\(\\beta_0\\), even \\(X\\) 0, \\(Y\\) change \\(\\beta_1\\) units every one unit change \\(X\\). language use ‘X regressed Y.’may take relationship data relationship order estimate coefficients particular values :\n\\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x.\\]hats used indicate estimated values. saying linear regression assume \\(x\\) doubles \\(y\\) also double. Linear regressions considers average dependent variable changes based independent variables.want focus data, ’ll make example concrete, generating data discussing everything context . example looking someone’s time running five kilometers, compared time running marathon.set-may like use \\(x\\), five-kilometer time, produce estimates \\(y\\), marathon time. involve also estimating values \\(\\beta_0\\) \\(\\beta_1\\), hat .estimate coefficients? Even impose linear relationship lot options (many straight lines can fit piece paper?). clearly fits great.One way may define great impose close possible \\(x\\) \\(y\\) combinations know. lot candidates define ‘close possible,’ one minimise sum least squares. produce estimates \\(\\hat{y}\\) based estimates \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\), given \\(x\\), work ‘wrong,’ every point \\(\\), :\n\\[e_i = y_i - \\hat{y}_i.\\]residual sum squares (RSS) requires summing across points:\n\\[\\mbox{RSS} = e^2_1+ e^2_2 +\\dots + e^2_n.\\]\nresults one ‘linear best-fit’ line, worth thinking assumptions decisions took get us point.least squares criterion want values \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\) result smallest RSS.","code":"\nrnorm(n = 20, mean = 0, sd = 1)\n#>  [1] -0.0749815081 -0.4270510215  0.3267231244 -1.1277805316\n#>  [5]  0.4642146787  0.5837224189  0.5181465856 -1.1978346549\n#>  [9] -0.0002279032  1.0468397672 -1.5353983294 -0.4777300597\n#> [13] -1.1593712738  0.2904819650 -1.3684571610 -0.8993383047\n#> [17] -1.1195471355 -0.3640948245  0.9199040244  0.2078188889\nlibrary(tidyverse)\nset.seed(853)\ntibble(\n  number_of_draws = c(\n    rep.int(x = \"2 draws\", times = 2),\n    rep.int(x = \"5 draws\", times = 5),\n    rep.int(x = \"10 draws\", times = 10),\n    rep.int(x = \"50 draws\", times = 50),\n    rep.int(x = \"100 draws\", times = 100),\n    rep.int(x = \"500 draws\", times = 500),\n    rep.int(x = \"1,000 draws\", times = 1000),\n    rep.int(x = \"10,000 draws\", times = 10000),\n    rep.int(x = \"100,000 draws\", times = 100000)),\n  draws = c(\n    rnorm(n = 2, mean = 0, sd = 1),\n    rnorm(n = 5, mean = 0, sd = 1),\n    rnorm(n = 10, mean = 0, sd = 1),\n    rnorm(n = 50, mean = 0, sd = 1),\n    rnorm(n = 100, mean = 0, sd = 1),\n    rnorm(n = 500, mean = 0, sd = 1),\n    rnorm(n = 1000, mean = 0, sd = 1),\n    rnorm(n = 10000, mean = 0, sd = 1),\n    rnorm(n = 100000, mean = 0, sd = 1))\n  ) %>% \n  mutate(number_of_draws = as_factor(number_of_draws)) %>% \n  ggplot(aes(x = draws)) +\n  geom_density() +\n  theme_classic() +\n  facet_wrap(vars(number_of_draws),\n             scales = \"free_y\") +\n  labs(x = 'Draw',\n       y = 'Density')\nset.seed(853)\nnumber_of_observations <- 100\nrunning_data <- \n  tibble(five_km_time = rnorm(number_of_observations, 20, 3),\n         noise = rnorm(number_of_observations, 0, 10),\n         marathon_time = five_km_time * 8.4 + noise,\n         was_raining = sample(c(\"Yes\", \"No\"), \n                              size = number_of_observations,\n                              replace = TRUE, \n                              prob = c(0.2, 0.8)) \n         )\n\nrunning_data %>% \n  ggplot(aes(x = five_km_time, y = marathon_time)) +\n  geom_point() +\n  labs(x = \"Five-kilometer time (minutes)\",\n       y = \"Marathon time (minutes)\") +\n  theme_classic()\nrunning_data %>% \n  ggplot(aes(x = five_km_time, y = marathon_time)) +\n  geom_point() + \n  geom_smooth(method = \"lm\", \n              se = FALSE, \n              color = \"black\", \n              linetype = \"dashed\",\n              formula = 'y ~ x') +\n  labs(x = \"Five-kilometer time (minutes)\",\n       y = \"Marathon time (minutes)\") +\n  theme_classic()"},{"path":"ijalm.html","id":"implementation-in-base-r","chapter":"15 It’s Just A Linear Model","heading":"15.2.2 Implementation in base R","text":"Within R, main function linear regression lm. included base R, don’t need call packages, moment, call bunch packages surround lm within environment familiar . specify relationship dependent variable first, ~, independent variables. Finally, specify dataset (pipe usual).general, assign object:see result regression can call summary().first part result tells us regression called, information residuals, estimated coefficients. finally useful diagnostics.considering relationship \\(X\\) \\(Y\\), : \\(Y = f(X) + \\epsilon\\). going say function, \\(f()\\), linear relationship :\n\\[\\hat{Y} = \\beta_0 + \\beta_1 X + \\epsilon.\\]‘true’ relationship \\(X\\) \\(Y\\), don’t know . can use sample data try estimate . understanding depends sample, every possible sample, get slightly different relationship (measured coefficients).\\(\\epsilon\\) measure error - model know? ’s going plenty model doesn’t know, hope error depend \\(X\\), error normally distributed.intercept marathon time expect five-kilometer time 0 minutes. Hopefully example illustrates need carefully interpret intercept coefficient! coefficient five-kilometer run time shows expect marathon time change five-kilometer run time changed one unit. case ’s 8.4, makes sense seeing marathon roughly many times longer five-kilometer run.","code":"\nlm(y ~ x, data = dataset)\nrunning_data_first_model <- \n  lm(marathon_time ~ five_km_time, \n     data = running_data)\nsummary(running_data_first_model)\n#> \n#> Call:\n#> lm(formula = marathon_time ~ five_km_time, data = running_data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -24.763  -5.686   0.722   6.650  16.707 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)    0.4114     6.0610   0.068    0.946    \n#> five_km_time   8.3617     0.3058  27.343   <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 8.474 on 98 degrees of freedom\n#> Multiple R-squared:  0.8841, Adjusted R-squared:  0.8829 \n#> F-statistic: 747.6 on 1 and 98 DF,  p-value: < 2.2e-16"},{"path":"ijalm.html","id":"tidy-up-with-broom","chapter":"15 It’s Just A Linear Model","heading":"15.2.3 Tidy up with broom","text":"nothing wrong base approach, want introduce broom package provide us outputs tidy framework (D. Robinson, Hayes, Couch 2020). three key functions:broom::tidy(): Gives coefficient estimates tidy output.broom::glance(): Gives diagnostics.broom::augment(): Adds forecast values, hence, residuals, dataset.Notice results fairly similar base summary function.now make plots residuals.say estimate unbiased, trying say even though sample estimate might high, another sample estimate might low, eventually lot data estimate population. (pro hockey player may sometimes shoot right net, sometimes left net, ’d hope average ’d right middle net). words James et al. (2017), ‘unbiased estimator systematically - -estimate true parameter.’want try speak ‘true’ relationship, need try capture much think understanding depends particular sample analyse. standard error comes . tells us estimate compared actual.standard errors, can compute confidence interval. 95 per cent confidence interval means 0.95 probability interval happens contain population parameter (typically unknown).bunch different tests can use understand model performing given data. One quick way look whole bunch different aspects use performance package (Lüdecke et al. 2020).","code":"\nlibrary(broom)\ntidy(running_data_first_model)\n#> # A tibble: 2 × 5\n#>   term         estimate std.error statistic  p.value\n#>   <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)     0.411     6.06     0.0679 9.46e- 1\n#> 2 five_km_time    8.36      0.306   27.3    1.17e-47\nglance(running_data_first_model)\n#> # A tibble: 1 × 12\n#>   r.squared adj.r.squared sigma statistic  p.value    df\n#>       <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>\n#> 1     0.884         0.883  8.47      748. 1.17e-47     1\n#> # … with 6 more variables: logLik <dbl>, AIC <dbl>,\n#> #   BIC <dbl>, deviance <dbl>, df.residual <int>,\n#> #   nobs <int>\nrunning_data <- \n  augment(running_data_first_model,\n          data = running_data)\nhead(running_data)\n#> # A tibble: 6 × 10\n#>   five_km_time  noise marathon_time was_raining .fitted\n#>          <dbl>  <dbl>         <dbl> <chr>         <dbl>\n#> 1         18.9 -3.73           155. No             159.\n#> 2         19.9  8.42           175. No             167.\n#> 3         14.7  4.32           127. No             123.\n#> 4         16.6 -2.74           137. No             139.\n#> 5         17.0 -4.89           138. No             142.\n#> 6         25.3  0.648          213. No             212.\n#> # … with 5 more variables: .resid <dbl>, .hat <dbl>,\n#> #   .sigma <dbl>, .cooksd <dbl>, .std.resid <dbl>\nggplot(running_data, \n       aes(x = .resid)) + \n  geom_histogram(binwidth = 1) +\n  theme_classic() +\n  labs(y = \"Number of occurrences\",\n       x = \"Residuals\")\n\nggplot(running_data, aes(five_km_time, .resid)) + \n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dotted\", color = \"grey\") +\n  theme_classic() +\n  labs(y = \"Residuals\",\n       x = \"Five-kilometer time (minutes)\")\nrunning_data %>% \n  ggplot(aes(x = five_km_time, y = marathon_time)) +\n  geom_point() + \n  geom_smooth(method = \"lm\", \n              se = TRUE, \n              color = \"black\", \n              linetype = \"dashed\",\n              formula = 'y ~ x') +\n  labs(x = \"Five-kilometer time (minutes)\",\n       y = \"Marathon time (minutes)\") +\n  theme_classic()\nlibrary(performance)\nperformance::check_model(running_data_first_model)"},{"path":"ijalm.html","id":"testing-hypothesis","chapter":"15 It’s Just A Linear Model","heading":"15.2.4 Testing hypothesis","text":"Now interval can say 95 per cent probability contains true population parameter can test claims. instance, null hypothesis relationship \\(X\\) \\(Y\\) (.e. \\(\\beta_1 = 0\\)), compared alternative hypothesis relationship \\(X\\) \\(Y\\) (.e. \\(\\beta_1 \\neq 0\\)).need know whether estimate \\(\\beta_1\\), \\(\\hat{\\beta}_1\\), ‘far enough’ away zero us comfortable claiming \\(\\beta_1 \\neq 0\\). far ‘far enough?’ confident estimate \\(\\beta_1\\) wouldn’t far, substantial. depends bunch things, essentially standard error \\(\\hat{\\beta}_1\\).compare standard error \\(\\hat{\\beta}_1\\) get t-statistic:\n\\[t = \\frac{\\hat{\\beta}_1 - 0}{\\mbox{SE}(\\hat{\\beta}_1)}.\\]\ncompare t-statistic t-distribution compute probability getting absolute t-statistic larger one, \\(\\beta_1 = 0\\). p-value. small p-value means unlikely observe association due chance wasn’t relationship.","code":""},{"path":"ijalm.html","id":"on-p-values","chapter":"15 It’s Just A Linear Model","heading":"15.2.5 On p-values","text":"p-value specific subtle concept. easy abuse. main issue embodies, assumes correct, every assumption model. Greenland et al. (2016, 339): ‘p-value probability chosen test statistic least large observed value every model assumption correct, including test hypothesis.’ provide background language used case ’re unfamiliar, test hypothesis typically ‘null hypothesis,’ ‘test statistic’ ‘distance data model prediction’ (Greenland et al. 2016).following quote (minor edits consistency ) summarises situation:true smaller p-value, unusual data every single assumption correct; small p-value tell us assumption incorrect. example, p-value may small targeted hypothesis false; may instead (addition) small study protocols violated, selected presentation based small size. Conversely, large p-value indicates data unusual model, imply model aspect (targeted hypothesis) correct; may instead (addition) large () study protocols violated, selected presentation based large size.general definition p-value may help one understand statistical tests tell us much less many think : p-value tell us whether hypothesis targeted testing true ; says nothing specifically related hypothesis unless can completely assured every assumption used computation correct—assurance lacking far many studies.Greenland et al. (2016, 339).nothing inherently wrong using p-values, important use sophisticated thoughtful ways.Typically one application ’s easy see abuse p-values power analysis. Gelman Hill (2007, 438) say, ‘[s]ample size never large enough…. problem… [w]e just emphasizing , just never enough money, perceived needs increase resources, inferential needs increase sample size.’ Power refers probability incorrectly failing reject null hypothesis. Imai (2017, 303) says:use power analysis order formalize degree informativeness data hypothesis tests. power statistical hypothesis test defined one minus probability type II error:power = 1-P(type II error)vacuum, ’d like high power can achieve either really big effect sizes, larger number observations.","code":""},{"path":"ijalm.html","id":"multiple-linear-regression","chapter":"15 It’s Just A Linear Model","heading":"15.3 Multiple linear regression","text":"point ’ve just considered one explanatory variable. ’ll usually one. One approach run separate regressions explanatory variable. compared separate linear regressions , adding explanatory variables allows us better understanding intercept accounts interaction. Often results quite different.slightly counterintuitive result common many real life situations. Consider absurd example illustrate point. Running regression shark attacks versus ice cream sales data collected given beach community period time show positive relationship, similar seen sales newspapers. course one (yet) suggested ice creams banned beaches reduce shark attacks. reality, higher temperatures cause people visit beach, turn results ice cream sales shark attacks. multiple regression attacks versus ice cream sales temperature reveals , intuition implies, former predictor longer significant adjusting temperature.(James et al. 2017, 74).may also like consider variables inherent ordering. instance, pregnant . two options can use binary variable 0 1. two levels use combination binary variables, ‘missing’ outcome (baseline) gets pushed onto intercept.languages may need explicitly construct dummy variables, R designed language statistical programming, lot work fairly forgiving. instance, column character values two values: c(\"Monica\", \"Rohan\", \"Rohan\", \"Monica\", \"Monica\", \"Rohan\"), used independent variable usual regression set R treat dummy variable.result probably isn’t surprising look plot data.addition wanting include additional explanatory variables may think related one another. instance, wanting explain amount snowfall Toronto, may interested humidity temperature, two variables may also interact. can using * instead + specify model R. interact variables, almost always also include individual variables well (Figure 15.2).\nFigure 15.2: Don’t leave main effects interactive model\nSource: Kai Arzheimer, 16 February 2020.","code":"\nrunning_data_rain_model <- \n  lm(marathon_time ~ five_km_time + was_raining, \n     data = running_data)\nsummary(running_data_rain_model)\n#> \n#> Call:\n#> lm(formula = marathon_time ~ five_km_time + was_raining, data = running_data)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -24.6239  -5.5806   0.8377   6.7636  16.8671 \n#> \n#> Coefficients:\n#>                Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)      0.1430     6.1476   0.023    0.981    \n#> five_km_time     8.3689     0.3081  27.166   <2e-16 ***\n#> was_rainingYes   0.7043     2.2220   0.317    0.752    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 8.513 on 97 degrees of freedom\n#> Multiple R-squared:  0.8842, Adjusted R-squared:  0.8818 \n#> F-statistic: 370.4 on 2 and 97 DF,  p-value: < 2.2e-16\nrunning_data %>%\n  ggplot(aes(x = five_km_time, y = marathon_time, color = was_raining)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\", linetype = \"dashed\") +\n  labs(x = \"Five-kilometer time (minutes)\",\n       y = \"Marathon time (minutes)\",\n       color = \"Was raining\") +\n  theme_classic() +\n  scale_color_brewer(palette = \"Set1\")\n#> `geom_smooth()` using formula 'y ~ x'"},{"path":"ijalm.html","id":"threats-to-validity-and-aspects-to-think-about","chapter":"15 It’s Just A Linear Model","heading":"15.3.1 Threats to validity and aspects to think about","text":"variety weaknesses aspects discuss use linear regression. quick list includes (James et al. 2017, 92):Non-linearity response-predictor relationships.Correlation error terms.Non-constant variance error terms.Outliers.High-leverage points.CollinearityThese also aspects discuss use linear regression. Including plots tends handy illustrate points. aspects may consider discussing include (James et al. 2017, 75):least one predictors \\(X_1, X_2, \\dots, X_p\\) useful predicting response?predictors help explain \\(Y\\), subset predictors useful?well model fit data?Given set predictor values, response value predict, accurate prediction?","code":""},{"path":"ijalm.html","id":"more-credible-outputs","chapter":"15 It’s Just A Linear Model","heading":"15.3.2 More credible outputs","text":"Finally, creating beautiful graphs tables may want regression output look just nice. variety packages R automatically format regression outputs. One particularly nice huxtable (Hugh-Jones 2020).Table 15.1:  ","code":"\nlibrary(huxtable)\nhuxreg(running_data_first_model, running_data_rain_model)"},{"path":"ijalm.html","id":"implementation-in-tidymodels","chapter":"15 It’s Just A Linear Model","heading":"15.3.3 Implementation in tidymodels","text":"reason went trouble simple regression often want fit bunch models. One way copy/paste code bunch times. ’s nothing wrong . ’s way people get started, may want take approach scales easily. also need think carefully -fitting, able evaluate models.tidymodels package (Kuhn Wickham 2020) cool kids using days. ’s attempt bring order chaos different modelling packages R. (attempts past ’ve crashed burned, hopefully time different.) issue let’s say want run simple linear regression run random forest. language ’d use code models fairly different. tidymodels package latest attempt bring coherent grammar . ’s also package packages.’ll create test training datasets.81 points training set, 19 test set 100 total.can make datasets test training samples.look dataset made can see ’s got fewer rows. reached outcome something like:","code":"\nset.seed(853)\nlibrary(tidymodels)\n\nrunning_data_split <- rsample::initial_split(running_data, prop = 0.80)\nrunning_data_split\n#> <Analysis/Assess/Total>\n#> <80/20/100>\nrunning_data_train <- rsample::training(running_data_split)\nrunning_data_test  <-  rsample::testing(running_data_split)\nrunning_data <- \n  running_data %>% \n  mutate(magic_number = sample(x = c(1:nrow(running_data)), size = nrow(running_data), replace = FALSE))\n\nrunning_data_test <- \n  running_data %>% \n  filter(magic_number <= 20)\n\nrunning_data_train <- \n  running_data %>% \n  filter(magic_number > 20)\nfirst_go <- \n  parsnip::linear_reg() %>%\n  parsnip::set_engine(engine = \"lm\") %>% \n  parsnip::fit(marathon_time ~ five_km_time + was_raining, \n               data = running_data_train\n               )"},{"path":"ijalm.html","id":"implementation-in-rstanarm","chapter":"15 It’s Just A Linear Model","heading":"15.3.4 Implementation in rstanarm","text":"tidymodels package fine specific types tasks. instance machine learning chances interested forecasting. ’s kind thing tidymodels really built . want equivalent firepower explanatory modelling one option use Bayesian approaches directly. Yes, can use Bayesian models within tidymodels ecosystem, start move away ---box solutions, becomes important start understand going hood.variety ways getting started, essentially need probabilistic programming language. one specifically designed sort thing, comparison R, designed general statistical computing. use Stan notes within context familiar R environment. interface Stan using rstanarm package (Goodrich et al. 2020).","code":"\nlibrary(rstanarm)\n\nfirst_go_in_rstanarm <-\n  stan_lm(\n    marathon_time ~ five_km_time + was_raining, \n    data = running_data,\n    prior = NULL,\n    seed = 853\n  )\nfirst_go_in_rstanarm\n#> stan_lm\n#>  family:       gaussian [identity]\n#>  formula:      marathon_time ~ five_km_time + was_raining\n#>  observations: 100\n#>  predictors:   3\n#> ------\n#>                Median MAD_SD\n#> (Intercept)    0.4    6.0   \n#> five_km_time   8.4    0.3   \n#> was_rainingYes 0.7    2.2   \n#> \n#> Auxiliary parameter(s):\n#>               Median MAD_SD\n#> R2            0.9    0.0   \n#> log-fit_ratio 0.0    0.0   \n#> sigma         8.6    0.6   \n#> \n#> ------\n#> * For help interpreting the printed output see ?print.stanreg\n#> * For info on the priors used see ?prior_summary.stanreg"},{"path":"ijalm.html","id":"logistic-regression-2","chapter":"15 It’s Just A Linear Model","heading":"15.4 Logistic regression","text":"","code":""},{"path":"ijalm.html","id":"overview-2","chapter":"15 It’s Just A Linear Model","heading":"15.4.1 Overview","text":"steal joke someone, ‘’s AI ’re fundraising, machine learning ’re hiring, logistic regression ’re implementing.’dependent variable binary outcome, 0 1, instead linear regression may like use logistic regression. Although binary outcome may sound limiting, lot circumstances outcome either naturally falls situation, can adjusted (e.g. voter supports liberals liberals).reason use logistic regression ’ll modelling probability bounded 0 1. Whereas linear regression may end values outside . practice usually fine start linear regression move logistic regression build confidence.said, logistic regression, Daniella Witten teaches us, just linear model!","code":""},{"path":"ijalm.html","id":"implementation-in-base","chapter":"15 It’s Just A Linear Model","heading":"15.4.2 Implementation in base","text":"’d like consider slightly interesting example, dataset pearl jewellery, Australian retailer Paspaley.case ’ll model whether jewellery made white yellow gold, based price year (Figure 15.3).\nFigure 15.3: Examining type gold jewellery made .\ngraph suggests filter price higher $100,000.linear regression, logistic regression built R, glm function. case, ’ll try work jewellery white gold. Although strictly necessary particular function, ’ll change binary, 1 white gold 0 .One reason logistic regression can bit pain initially coefficients take bit work interpret. particular, estimate price -3.170e-06. odds. odds white gold decrease -3.170e-06 price increases. can model make forecasts terms probability, asking .Table 15.2:  ","code":"\npaspaley_dataset <- read_csv(\"https://raw.githubusercontent.com/RohanAlexander/paspaley/master/outputs/data/cleaned_dataset.csv\")\n#> Rows: 1289 Columns: 13\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (10): product, name, description, availability, sku,...\n#> dbl  (2): price, year\n#> lgl  (1): keshi\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npaspaley_dataset$metal %>% table()\n#> .\n#>       Other    Platinum   Rose gold  White gold Yellow gold \n#>         134          23          89         475         568\npaspaley_logistic_dataset <- \n  paspaley_dataset %>% \n  filter(metal %in% c('White gold', 'Yellow gold')) %>% \n  select(metal, price, year)\npaspaley_logistic_dataset <- \n  paspaley_logistic_dataset %>% \n  filter(price < 100000)\npaspaley_logistic_dataset <- \n  paspaley_logistic_dataset %>% \n  mutate(is_white_gold = if_else(metal == \"White gold\", 1, 0))\n\nwhite_gold_model <- \n  glm(is_white_gold ~ price + year, \n    data = paspaley_logistic_dataset, \n    family = 'binomial')\n\nsummary(white_gold_model)\n#> \n#> Call:\n#> glm(formula = is_white_gold ~ price + year, family = \"binomial\", \n#>     data = paspaley_logistic_dataset)\n#> \n#> Deviance Residuals: \n#>    Min      1Q  Median      3Q     Max  \n#> -1.250  -1.103  -1.015   1.247   1.353  \n#> \n#> Coefficients:\n#>               Estimate Std. Error z value Pr(>|z|)  \n#> (Intercept)  2.087e+02  8.674e+01   2.406   0.0161 *\n#> price        3.832e-06  5.405e-06   0.709   0.4783  \n#> year        -1.035e-01  4.296e-02  -2.408   0.0160 *\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 1411.6  on 1023  degrees of freedom\n#> Residual deviance: 1405.5  on 1021  degrees of freedom\n#> AIC: 1411.5\n#> \n#> Number of Fisher Scoring iterations: 4\npaspaley_logistic_dataset <- \n  broom::augment(white_gold_model,\n          data = paspaley_logistic_dataset,\n          type.predict = \"response\")\nhead(paspaley_logistic_dataset)"},{"path":"ijalm.html","id":"implementation-in-tidymodels-1","chapter":"15 It’s Just A Linear Model","heading":"15.4.3 Implementation in tidymodels","text":"can use tidymodels run wanted. case, need factor.","code":"\nset.seed(853)\n\npaspaley_logistic_dataset <- \n  paspaley_logistic_dataset %>% \n  mutate(is_white_gold = as_factor(is_white_gold))\n\npaspaley_logistic_dataset_split <- rsample::initial_split(paspaley_logistic_dataset, prop = 0.80)\npaspaley_logistic_dataset_train <- rsample::training(paspaley_logistic_dataset_split)\npaspaley_logistic_dataset_test  <-  rsample::testing(paspaley_logistic_dataset_split)\n\nwhite_gold_model_tidymodels <-\n  parsnip::logistic_reg(mode = \"classification\") %>%\n  parsnip::set_engine(\"glm\") %>%\n  fit(is_white_gold ~ price + year, \n      data = paspaley_logistic_dataset_train)\n\nwhite_gold_model_tidymodels\n#> parsnip model object\n#> \n#> Fit time:  4ms \n#> \n#> Call:  stats::glm(formula = is_white_gold ~ price + year, family = stats::binomial, \n#>     data = data)\n#> \n#> Coefficients:\n#> (Intercept)        price         year  \n#>   1.832e+02    5.245e-06   -9.082e-02  \n#> \n#> Degrees of Freedom: 818 Total (i.e. Null);  816 Residual\n#> Null Deviance:       1130 \n#> Residual Deviance: 1125  AIC: 1131"},{"path":"ijalm.html","id":"implementation-in-rstanarm-1","chapter":"15 It’s Just A Linear Model","heading":"15.4.4 Implementation in rstanarm","text":"","code":"\npaspaley_in_rstanarm <-\n  rstanarm::stan_glm(\n    is_white_gold ~ price + year,\n    data = paspaley_logistic_dataset,\n    family = binomial(link = \"logit\"),\n    prior = NULL,\n    seed = 853\n  )"},{"path":"ijalm.html","id":"poisson-regression","chapter":"15 It’s Just A Linear Model","heading":"15.5 Poisson regression","text":"","code":""},{"path":"ijalm.html","id":"overview-3","chapter":"15 It’s Just A Linear Model","heading":"15.5.1 Overview","text":"count data, use Poisson distribution. Pitman (1993, 121) ’Poisson distribution parameter \\(\\mu\\) Poisson (\\(\\mu\\)) distribution distribution probabilities \\(P_{\\mu}(k)\\) \\({0, 1, 2, ...}\\) defined :\n\\[P_{\\mu}(k) = e^{-\\mu}\\mu^k/k!\\mbox{, }k=0,1,2,...\\]\ncan simulate \\(n\\) data points Poisson distribution rpois() \\(\\lambda\\) mean variance.\\(\\lambda\\) parameter governs shape distribution.instance, look number + grades awarded university course given term course count.","code":"\nrpois(n = 20, lambda = 3)\n#>  [1] 2 2 3 4 2 2 4 2 4 1 3 2 3 3 2 2 0 1 2 1\nset.seed(853)\nnumber_of_each <- 1000\ntibble(lambda = c(rep(0, number_of_each), rep(1, number_of_each), rep(2, number_of_each), rep(5, number_of_each), rep(10, number_of_each)),\n       draw = c(rpois(n = number_of_each, lambda = 0), rpois(n = number_of_each, lambda = 1), rpois(n = number_of_each, lambda = 2), rpois(n = number_of_each, lambda = 5), rpois(n = number_of_each, lambda = 10))) %>% \n  ggplot(aes(x = draw)) +\n  geom_density() +\n  facet_wrap(vars(lambda)) +\n  theme_classic()\nset.seed(853)\ncount_of_A_plus <- \n  tibble( \n    # https://stackoverflow.com/questions/1439513/creating-a-sequential-list-of-letters-with-r\n    department = c(rep.int(\"1\", 26), rep.int(\"2\", 26)),\n    course = c(paste0(\"DEP_1_\", letters), paste0(\"DEP_2_\", letters)),\n    number_of_A_plus = c(sample(c(1:10), \n                              size = 26,\n                              replace = TRUE),\n                         sample(c(1:50), \n                              size = 26,\n                              replace = TRUE)\n    )\n  )"},{"path":"ijalm.html","id":"implementation-in-base-1","chapter":"15 It’s Just A Linear Model","heading":"15.5.2 Implementation in base","text":"","code":"\ngrades_model <- \n  glm(number_of_A_plus ~ department, \n    data = count_of_A_plus, \n    family = 'poisson')\n\nsummary(grades_model)\n#> \n#> Call:\n#> glm(formula = number_of_A_plus ~ department, family = \"poisson\", \n#>     data = count_of_A_plus)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -6.7386  -1.2102  -0.2515   1.3292   3.9520  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  1.44238    0.09535   15.13   <2e-16 ***\n#> department2  1.85345    0.10254   18.07   <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for poisson family taken to be 1)\n#> \n#>     Null deviance: 816.08  on 51  degrees of freedom\n#> Residual deviance: 334.57  on 50  degrees of freedom\n#> AIC: 545.38\n#> \n#> Number of Fisher Scoring iterations: 5"},{"path":"ijalm.html","id":"implementation-in-tidymodels-2","chapter":"15 It’s Just A Linear Model","heading":"15.5.3 Implementation in tidymodels","text":"can use tidymodels run wanted although first need install helper package poissonreg.","code":"\n# install.packages(\"poissonreg\")\n\nset.seed(853)\n\ncount_of_A_plus_split <- rsample::initial_split(count_of_A_plus, prop = 0.80)\ncount_of_A_plus_train <- rsample::training(count_of_A_plus_split)\ncount_of_A_plus_test  <-  rsample::testing(count_of_A_plus_split)\n\na_plus_model_tidymodels <-\n  poissonreg::poisson_reg(mode = \"regression\") %>%\n  parsnip::set_engine(\"glm\") %>%\n  parsnip::fit(number_of_A_plus ~ department, \n      data = count_of_A_plus_train)\n\na_plus_model_tidymodels\n#> parsnip model object\n#> \n#> Fit time:  2ms \n#> \n#> Call:  stats::glm(formula = number_of_A_plus ~ department, family = stats::poisson, \n#>     data = data)\n#> \n#> Coefficients:\n#> (Intercept)  department2  \n#>       1.488        1.867  \n#> \n#> Degrees of Freedom: 40 Total (i.e. Null);  39 Residual\n#> Null Deviance:       618.6 \n#> Residual Deviance: 210.1     AIC: 380.4"},{"path":"ijalm.html","id":"exercises-and-tutorial-14","chapter":"15 It’s Just A Linear Model","heading":"15.6 Exercises and tutorial","text":"","code":""},{"path":"ijalm.html","id":"exercises-14","chapter":"15 It’s Just A Linear Model","heading":"15.6.1 Exercises","text":"","code":""},{"path":"ijalm.html","id":"tutorial-14","chapter":"15 It’s Just A Linear Model","heading":"15.6.2 Tutorial","text":"","code":""},{"path":"ijalm.html","id":"paper-4","chapter":"15 It’s Just A Linear Model","heading":"15.6.3 Paper","text":"point, Paper Four (Appendix B.4) appropriate.","code":""},{"path":"causality.html","id":"causality","chapter":"16 Causality from observational data","heading":"16 Causality from observational data","text":"STATUS: construction.TODO: Replace arm matching https://kosukeimai.github.io/MatchIt/index.htmlRequired readingAngelucci, Charles, Julia Cagé, 2019, ‘Newspapers times low advertising revenues,’ American Economic Journal: Microeconomics, vol. 11, . 3, pp. 319-364, DOI: 10.1257/mic.20170306, available : https://www.aeaweb.org/articles?id=10.1257/mic.20170306.Better Evaluation, ‘Regression Discontinuity,’ https://www.betterevaluation.org/en/evaluation-options/regressiondiscontinuityDagan, Noa, Noam Barda, Eldad Kepten, Oren Miron, Shay Perchik, Mark . Katz, Miguel . Hernán, Marc Lipsitch, Ben Reis, Ran D. Balicer, 2021, ‘BNT162b2 mRNA Covid-19 vaccine nationwide mass vaccination setting,’ New England Journal Medicine, 24 February, https://www.nejm.org/doi/full/10.1056/NEJMoa2101765.Eggers, Andrew C., Anthony Fowler, Jens Hainmueller, Andrew B. Hall, James M. Snyder Jr, 2015, ‘validity regression discontinuity design estimating electoral effects: New evidence 40,000 close races,’ American Journal Political Science, 59 (1), pp. 259-274Gelman, Andrew, 2019, ‘Another Regression Discontinuity Disaster can learn ,’ 25 June, https://statmodeling.stat.columbia.edu/2019/06/25/another-regression-discontinuity-disaster---can--learn--/.Gelman, Andrew, Jennifer Hill Aki Vehtari, 2020, Regression Stories, Cambridge University Press, Chs 18 - 21.Gertler, Paul, Sebastian Martinez, Patrick Premand, Laura Rawlings, Christel Vermeersch, ‘Impact Evaluation Practice,’ Chapter 5 - 8.McElreath, Richard, 2020, Statistical Rethinking, 2nd Edition, CRC Press, Ch 14.Meng, Xiao-Li, 2021, ‘Values Data, Data Science, Data Scientists?’ Harvard Data Science Review, https://doi.org/10.1162/99608f92.ee717cf7, https://hdsr.mitpress.mit.edu/pub/bj2dfcwg/release/2.Riederer, Emily, 2021, ‘Causal design patterns data analysts,’ 30 January, https://emilyriederer.netlify.app/post/causal-design-patterns/Sekhon, Jasjeet Rocio Titiunik, 2016, ‘Understanding Regression Discontinuity Designs Observational Studies,’ Observational Studies 2 (2016) 174-182, http://sekhon.berkeley.edu/papers/SekhonTitiunik2016-OS.pdf.Wong, Jeffrey, Colin McFarland, 2020, ‘Computational Causal Inference Netflix,’ Netflix Technology Blog, 11 Aug, https://netflixtechblog.com/computational-causal-inference--netflix-293591691c62.Required viewingGelman, Andrew, 2020 ‘100 Stories Causal Inference,’ 4 August, https://www.youtube.com/watch?v=jnI5KI843Lk.King, Gary, 2020, ‘Research Designs,’ Lectures Quantitative Social Science Methods 1, https://youtu./SBwPLwVOb7s.Kuriwaki, Shiro, 2020, ‘Difference--Differences Estimation R (parts 1 2),’ 18 April, https://vimeo.com/409267138 https://vimeo.com/409267190.Kuriwaki, Shiro, 2020, ‘Instrumental variables R,’ 11 April, https://vimeo.com/406629459.Kuriwaki, Shiro, 2020, ‘Regression Discontinuity R (parts 1 2),’ 25 March, https://vimeo.com/400826628 https://vimeo.com/400826660.Oostrom, Tamar, 2021, ‘Funding Clinical Trials Reported Drug Efficacy,’ 2 March, https://youtu./DdnpWS9Km5U.Riederer, Emily, 2021, ‘Observational Causal Inference,’ Toronto Data Workshop, 15 February, https://youtu./VP3BBZ7poc0.Recommended readingAlexander, Monica, Polimis, Kivan, Zagheni, Emilio, 2019,’ impact Hurricane Maria -migration Puerto Rico: Evidence Facebook data’, Population Development Review. (Example using diff--diff measure effect Hurricane Maria.)Alexander, Rohan, Zachary Ward, 2018, ‘Age arrival assimilation age mass migration,’ Journal Economic History, 78, . 3, 904-937. (Example used differences brothers estimate effect education.)Angrist, Joshua D., Jörn-Steffen Pischke, 2008, Mostly harmless econometrics: empiricist’s companion, Princeton University Press, Chapter 4.Angrist, Joshua D., Jörn-Steffen Pischke, 2008, Mostly harmless econometrics: empiricist’s companion, Princeton University Press, Chapter 6.Angrist, Joshua D., Jörn-Steffen Pischke, 2008, Mostly harmless econometrics: empiricist’s companion, Princeton University Press, Chapters 3.3.2 5.Austin, Peter C., 2011, ‘Introduction Propensity Score Methods Reducing Effects Confounding Observational Studies,’ Multivariate Behavioral Research, vol. 46, . 3, pp.399-424. (Broad overview propensity score matching, nice discussion comparison randomised controlled trials.)Baker, Andrew, 2019, ‘Difference--Differences Methodology,’ 25 September, https://andrewcbaker.netlify.app/2019/09/25/difference--differences-methodology/.Coppock, Alenxader, Donald P. Green, 2016, ‘Voting Habit Forming? New Evidence Experiments Regression Discontinuities,’ American Journal Political Science, Volume 60, Issue 4, pp. 1044-1062, available : https://onlinelibrary.wiley.com/doi/abs/10.1111/ajps.12210. (code data.)Cunningham, Scott, ‘Causal Inference: Mixtape,’ Chapter ‘Instrumental variables,’ http://www.scunning.com/causalinference_norap.pdf.Cunningham, Scott, ‘Causal Inference: Mixtape,’ chapter ‘Regression discontinuity,’ http://www.scunning.com/causalinference_norap.pdf.Cunningham, Scott, Causal Inference: Mixtape, chapters ‘Matching subclassifications’ ‘Differences--differences,’ http://www.scunning.com/causalinference_norap.pdf. (well-written notes diff--diff.)Dell, Melissa, Pablo Querubin, 2018, ‘Nation Building Foreign Intervention: Evidence Discontinuities Military Strategies,’ Quarterly Journal Economics, Volume 133, Issue 2, pp. 701–764, https://doi.org/10.1093/qje/qjx037.Evans, David, 2013, ‘Regression Discontinuity Porn,’ World Bank Blogs, 16 November, https://blogs.worldbank.org/impactevaluations/regression-discontinuity-porn.Gelman, Andrew, 2019, ‘Another Regression Discontinuity Disaster can learn ,’ Statistical Modeling, Causal Inference, Social Science, 25 June, https://statmodeling.stat.columbia.edu/2019/06/25/another-regression-discontinuity-disaster---can--learn--/.Gelman, Andrew, Guido Imbens, 2019, “high-order polynomials used regression discontinuity designs,” Journal Business & Economic Statistics, 37, pp. 447-456.Gelman, Andrew, Jennifer Hill, 2007, Data Analysis Using Regression Muiltilevel/Hierarchical Models, Chapter 10, pp. 207-215.Grogger, Jeffrey, Andreas Steinmayr, Joachim Winter, 2020, ‘Wage Penalty Regional Accents,’ NBER Working Paper . 26719.Harris, Rich, Mlacki Migliozzi Niraj Chokshi, ‘13,000 Missing Flights: Global Consequences Coronavirus,’ New York Times, 21 February 2020. freely available (make account): https://www.nytimes.com/interactive/2020/02/21/business/coronavirus-airline-travel.html.Imai, Kosuke, 2017, Quantitative Social Science: Introduction, Princeton University Press, Ch 2.5.Imbens, Guido W., Thomas Lemieux, 2008, ‘Regression discontinuity designs: guide practice,’ Journal Econometrics, vol. 142, . 2, pp. 615-635.King, Gary, Richard Nielsen, 2019, ‘Propensity Scores Used Matching,’ Political Analysis. (Academic paper limits propensity score matching. Propensity score matching big thing 90s everyone knew weaknesses died . Lately, resurgence CS/ML folks using without thinking King Nielsen wrote nice paper flaws. mean, can’t say weren’t warned.)Myllyvirta, Lauri, 2020, ‘Analysis: Coronavirus temporarily reduced China’s CO2 emissions quarter,’ Carbon Brief, 19 February, https://www.carbonbrief.org/analysis-coronavirus--temporarily-reduced-chinas-co2-emissions---quarter.Saeed, Sahar, Erica E. M. Moodie, Erin C. Strumpf, Marina B. Klein, 2019, ‘Evaluating impact health policies: using difference--differences approach,’ International Journal Public Health, 64, pp. 637–642, https://doi.org/10.1007/s00038-018-1195-2.Taddy, Matt, 2019, Business Data Science, Chapter 5, pp. 146-162.Tang, John, 2015, ‘Pollution havens trade toxic chemicals: evidence U.S. trade flows,’ Ecological Economics, vol. 112, pp. 150-160. (Example using diff--diff estimate pollution.)Travis, D.J., Carleton, .M. Lauritsen, R.G., 2004. ‘Regional variations US diurnal temperature range 11–14 September 2001 aircraft groundings: Evidence jet contrail influence climate,’ Journal climate, 17(5), pp.1123-1134.Travis, David J., Andrew M. Carleton, Ryan G. Lauritsen. “Contrails reduce daily temperature range.” Nature, 418, . 6898 (2002): 601-601.Valencia Caicedo, Felipe. ‘mission: Human capital transmission, economic persistence, culture South America.’ Quarterly Journal Economics 134.1 (2019): 507-556. (Data available : Valencia Caicedo, Felipe, 2018, “Replication Data : ‘Mission: Human Capital Transmission, Economic Persistence, Culture South America’,” https://doi.org/10.7910/DVN/ML1155, Harvard Dataverse, V1.).Zinovyeva, Natalia Maryna Tverdostup, 2019, ‘women earn slightly husbands hard find?’ 10 June, https://blogs.lse.ac.uk/businessreview/2019/06/10/--women--earn-slightly----husbands-hard--find/.Key concepts/skills/etcEssential matching methods.Weaknesses matching.Difference--differences.Identifying opportunities instrumental variables.Implementing instrumental variables.Challenges validity instrumental variables.Reading foreign data.Difference differences.Replicating work.Displaying multiple regression results.Discussing results.Generating simulated data.Understanding regression discontinuity implementing manually using packages.Appreciating threats validity regression discontinuity.Key librariesbroomtidyverseestimatrtidyversehavenhuxtablescalestidyversebroomrdrobusttidyverseKey functions/etctidy()lm()iv_robust()dollar_format()hux_reg()lm()mutate_at()read_dta()lm()tidy()rdrobust()()QuizSharla Gelfand ‘(s)haring two #rstats functions days - one know love, one ’s new !’ Please go Sharla’s GitHub page: https://github.com/sharlagelfand/twofunctionsmostdays. Please find package mentions never used. Please find relevant website package. Please describe package context useful .Sharla Gelfand ‘(s)haring two #rstats functions days - one know love, one ’s new !’ Please go Sharla’s GitHub page: https://github.com/sharlagelfand/twofunctionsmostdays. Please find function mentions never used. Please look help file function. Please detail arguments function, context useful .propensity score matching? matching people, features like match ? sort ethical questions collecting storing information raise ?Putting one side, ethical issues, statistical weaknesses propensity score matching?key assumption using diff--diff?Please read fascinating article Markup car insurance algorithms: https://themarkup.org/allstates-algorithm/2020/02/25/car-insurance-suckers-list. Please read article tell think. may wish focus ethical, legal, social, statistical, , aspects.Please go GitHub page related fascinating article Markup car insurance algorithms: https://github.com/-markup/investigation-allstates-algorithm. great work? improved?fundamental features regression discontinuity design?conditions needed order RDD able used?Can think situation life RDD may useful?threats validity RDD estimates?Please look performance package: https://easystats.github.io/performance/index.html. features package may useful work?think using COVID-19 RDD setting? Statistically? Ethically?Please read reproduce main findings Eggers, Fowler, Hainmueller, Hall, Snyder, 2015.instrumental variable?circumstances instrumental variables might useful?conditions must instrumental variables satisfy?early instrumental variable authors?Can please think explain application instrumental variables life?key assumption difference--differences\nParallel trends.\nHeteroscedasticity.\nParallel trends.Heteroscedasticity.’re using regression discontinuity, whare aspects aware think really hard (select apply)?\ncut-free manipulation?\nforcing function continuous?\nextent functional form driving estimate?\ndifferent fitted lines affect results?\ncut-free manipulation?forcing function continuous?extent functional form driving estimate?different fitted lines affect results?main reason Oostrom (2021) finds outcome RCT can depend funding (pick one)?\nPublication bias\nExplicit manipulation\nSpecialisation\nLarger number arms\nPublication biasExplicit manipulationSpecialisationLarger number armsWhat key coefficient interest Angelucci Cagé, 2019 (pick one)?\n\\(\\beta_0\\)\n\\(\\beta_1\\)\n\\(\\lambda\\)\n\\(\\gamma\\)\n\\(\\beta_0\\)\\(\\beta_1\\)\\(\\lambda\\)\\(\\gamma\\)instrumental variable (please pick apply):\nCorrelated treatment variable.\ncorrelated outcome.\nHeteroskedastic.\nCorrelated treatment variable.correlated outcome.Heteroskedastic.two candidates invented instrumental variables?\nSewall Wright\nPhilip G. Wright\nSewall Cunningham\nPhilip G. Cunningham\nSewall WrightPhilip G. WrightSewall CunninghamPhilip G. CunninghamWhat two main assumptions instrumental variables?\nExclusion Restriction.\nRelevance.\nIgnorability.\nRandomization.\nExclusion Restriction.Relevance.Ignorability.Randomization.According Meng, 2021, ‘Data science can persuade via…’ (pick apply):\ncareful establishment evidence fair-minded high-quality data collection\nprocessing analysis\nhonest interpretation communication findings\nlarge sample sizes\ncareful establishment evidence fair-minded high-quality data collectionprocessing analysisthe honest interpretation communication findingslarge sample sizesAccording Reiderer, 2021, ‘disjoint treated untreated groups partitioned sharp cut-’ method use measure local treatment effect juncture groups (pick one)?\nregression discontinuity\nmatching\ndifference--differences\nevent study methods\nregression discontinuitymatchingdifference--differencesevent study methodsAccording Reiderer, 2021, ‘Causal inference requires investment ’ (pick apply):\ndata management\ndomain knowledge\nprobabilistic reasoning\ndata science\ndata managementdomain knowledgeprobabilistic reasoningdata scienceI Australian 30-39 year old male living Toronto one child PhD. following think match closely (please explain paragraph two)?\nAustralian 30-39 year old male living Toronto one child bachelors degree\nCanadian 30-39 year old male living Toronto one child PhD\nAustralian 30-39 year old male living Ottawa one child PhD\nCanadian 18-29 year old male living Toronto one child PhD\nAustralian 30-39 year old male living Toronto one child bachelors degreeA Canadian 30-39 year old male living Toronto one child PhDAn Australian 30-39 year old male living Ottawa one child PhDA Canadian 18-29 year old male living Toronto one child PhDIn disdainful tone (jokes, love DAGs), DAG (words please)?confounder (please select one answer)?\nvariable, z, causes x y, x also causes y.\nvariable, z, caused x y, x also causes y.\nvariable, z, causes y caused x, x also causes y.\nvariable, z, causes x y, x also causes y.variable, z, caused x y, x also causes y.variable, z, causes y caused x, x also causes y.mediator (please select one answer)?\nvariable, z, causes y caused x, x also causes y.\nvariable, z, causes x y, x also causes y.\nvariable, z, caused x y, x also causes y.\nvariable, z, causes y caused x, x also causes y.variable, z, causes x y, x also causes y.variable, z, caused x y, x also causes y.collider (please select one answer)?\nvariable, z, causes x y, x also causes y.\nvariable, z, causes y caused x, x also causes y.\nvariable, z, caused x y, x also causes y.\nvariable, z, causes x y, x also causes y.variable, z, causes y caused x, x also causes y.variable, z, caused x y, x also causes y.Please talk brief example may want careful checking Simpson’s paradox.Please talk brief example may want careful checking Berkson’s paradox.According McElreath (2020, 162) ‘Regression sort . Regression indeed oracle, cruel one. speaks riddles delights punishing us …’ (please select one answer)?\novercomplicating models.\nasking bad questions.\nusing bad data.\novercomplicating models.asking bad questions.using bad data.model fits small large world important , ?Kahneman, Sibony, Sunstein (2021) authors, including Nobel Prize winner Daniel Kahneman, say ‘… correlation imply causation, causation imply correlation. causal link, find correlation.’ reference Cunningham (2021Chapter 1), right wrong, ?","code":""},{"path":"causality.html","id":"introduction-13","chapter":"16 Causality from observational data","heading":"16.1 Introduction","text":"Life grand can conduct experiments able speak causality. can run survey - can’t run experiment? begin discussion circumstances methods allow nonetheless speak causality. use (relatively) simple methods, sophisticated, well-developed, ways (cf, much done days) applied statistics draw variety social sciences including economics, political science.Following publication Dagan et al. (2021), one authors tweeted (slight edits formatting):’ve just confirmed effectiveness Pfizer-BioNTech vaccine outside randomized trials. Yes, great news, let’s talk methodological issues arise using observational data estimate vaccine effectiveness.critical concern observational studies vaccine effectiveness confounding: Suppose people get vaccinated , average, lower risk infection/disease don’t get vaccinated. , even vaccine useless, ’d look beneficial.adjust confounding: start identifying potential confounders. example: Age (vaccination campaigns prioritize older people older people likely develop severe disease). choose valid adjustment method. paper, matched age. age adjustment, know residual confounding? one way go : know previous randomized trial vaccine effect first days. check whether matching age suffices replicate finding. , doesn’t. matching age (sex), curves infection start diverge day 0, indicates vaccinated lower risk infection unvaccinated. Conclusion: adjustment age sex insufficient.learned match COVID-19 risk factors, e.g., location, comorbidities, healthcare use… high-quality data Clalit Research Institute, part health services organization covers >50% Israeli population. example, vaccinated 76 year-old Arab male specific neighborhood received 4 influenza vaccines last 5 years 2 comorbidities matched unvaccinated Arab male neighborhood, aged 76-77, 3-4 influenza vaccines 2 comorbidities. matching risk factors, curves infection start diverge day ~12, expected vaccinated unvaccinated comparable risk infection. Using “negative control,” provide evidence large residual confounding.good illustration randomized trials observational studies complement better efficient #causalinference. First, randomized trial conducted estimate effectiveness vaccine prevent symptomatic infection, … trial’s estimates severe disease specific age groups imprecise. Second, observational analysis emulates #targettrial (order magnitude greater) confirms vaccine’s effectiveness severe disease different age groups. However… observational study needs trial’s findings benchmark guide data analysis strengthen quality causal inference.Randomized trials & Observational studies working together. best worlds. Let’s keep pandemic. luxury able think issues colleagues Noa Dagan, Noam Barda, Marc Lipsitch, Ben Reis, Ran D. Balicer. hope experience helpful researchers around world use observational data estimate vaccine effectiveness.Miguel Hernán, 24 Februrary 2021.chapter . can nonetheless comfortable making causal statements, even can’t run /B tests RCTs. Indeed, circumstances may actually prefer run run observational-based approaches addition . cover three major methods popular use days: difference--differences; regression discontinuity; instrumental variables.","code":""},{"path":"causality.html","id":"dags-and-trying-not-to-be-tricked-by-the-data","chapter":"16 Causality from observational data","heading":"16.2 DAGs and trying not to be tricked by the data","text":"","code":""},{"path":"causality.html","id":"dags-and-confounding","chapter":"16 Causality from observational data","heading":"16.2.1 DAGs and confounding","text":"discussing causality can help specific mean. ’s easy get caught data tricks . ’s important think really hard. One framework help become popular recently use directed acyclic graph (DAG), essentially just fancy name flow diagram. DAG involves drawing arrows variables indicating relationship . use DiagrammeR package draw (Iannone 2020), provides quite lot control (Figure 16.1). However can little finicky ’re just looking something really quickly ggdag package can useful (Barrett 2021b). code draw DAGs draws heavily Igelström (2020).\nFigure 16.1: Using DAG illustrate perceived relationships\nexample, claim \\(x\\) causes \\(y\\). build another situation less clear. find \\(x\\) \\(y\\) confusing, change fruits (Figure 16.2).\nFigure 16.2: Carrot confounder\ncase think \\(apple\\) causes \\(banana\\). ’s also clear \\(carrot\\) causes \\(banana\\), \\(carrot\\) also causes \\(apple\\). relationship ‘backdoor path,’ create spurious correlation analysis. may think changes \\(apple\\) causing changes \\(banana\\), ’s actually \\(carrot\\) changing hence variable called ‘confounder.’excellent discussion Hernan Robins (2020, 83):Suppose investigator conducted observational study answer causal question “one’s looking sky make pedestrians look ?” found association first pedestrian’s looking second one’s looking . However, also found pedestrians tend look hear thunderous noise . Thus unclear making second pedestrian look , first pedestrian’s looking thunderous noise? concluded effect one’s looking confounded presence thunderous noise.randomized experiments treatment assigned flip coin, observational studies treatment (e.g., person’s looking ) may determined many factors (e.g., thunderous noise). factors affect risk developing outcome (e.g., another person’s looking ), effects factors become entangled effect treatment. say confounding, just form lack exchangeability treated untreated. Confounding often viewed main shortcoming observational studies. presence confounding, old adage “association causation” holds even study population arbitrarily large.interested causal effects need adjust \\(carrot\\), \\(thunder\\) one way include regression. However, validity requires number assumptions. particular, Gelman Hill (2007, 169) warns us estimate correspond average causal effect sample : 1) include ‘confounding covariates’; 2) ‘model correct.’ Putting one side second requirement, focusing first, don’t observe confounder, can’t adjust . role domain experts, experience, theory, really add lot analysis.might similar situation, think \\(apple\\) causes \\(banana\\), time \\(apple\\) also causes \\(carrot\\), causes \\(banana\\) (Figure 16.3).\nFigure 16.3: Carrot mediator\ncase, \\(carrot\\) called ‘mediator’ ’d like adjust , affect estimate effect \\(apple\\) \\(banana\\).Finally, might yet another similar situation, think \\(apple\\) causes \\(banana\\), time \\(apple\\) \\(banana\\) cause \\(carrot\\) (Figure 16.4).\nFigure 16.4: Carrot collider\ncase, \\(carrot\\) called ‘collider’ condition create misleading relationship.’ve circling around point , ’s time address . create DAG - nothing create . means need think really carefully situation. ’s one thing see something DAG something . ’s another know ’s . McElreath (2020, 180) describes haunted DAGs.DAGs become fashionable. course helpful, just tool help think really deeply situation. McElreath (2020, 162) says ‘Regression sort . Regression indeed oracle, cruel one. speaks riddles delights punishing us asking bad questions.’ true DAGs, methods cover notes.","code":"\nlibrary(DiagrammeR)\nDiagrammeR::grViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape = plaintext]\n    x\n    y\n  edge [minlen = 2, arrowhead = vee]\n    x->y\n  { rank = same; x; y }\n}\n\")\nDiagrammeR::grViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape = plaintext]\n    Apple\n    Banana\n    Carrot\n  edge [minlen = 2, arrowhead = vee]\n    Apple->Banana\n    Carrot->Apple\n    Carrot->Banana\n  { rank = same; Apple; Banana }\n}\n\")\nDiagrammeR::grViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape = plaintext]\n    Apple\n    Banana\n    Carrot\n  edge [minlen = 2, arrowhead = vee]\n    Apple->Banana\n    Apple->Carrot\n    Carrot->Banana\n  { rank = same; Apple; Banana }\n}\n\")\nDiagrammeR::grViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape = plaintext]\n    Apple\n    Banana\n    Carrot\n  edge [minlen = 2, arrowhead = vee]\n    Apple->Banana\n    Apple->Carrot\n    Banana->Carrot\n  { rank = same; Apple; Banana }\n}\n\")"},{"path":"causality.html","id":"selection-and-measurement-bias","chapter":"16 Causality from observational data","heading":"16.2.2 Selection and measurement bias","text":"Selection bias occurs outcomes dependent ‘process individuals selected analysis’ (Hernan Robins 2020, 99). going able see DAG (mean one draw DAG shows , need know order draw DAG mean), many default diagnostics. One way go things /testing experimental settings, comparing sample general characteristics, instance age-group, gender, education. fundamental point, Dr Jill Sheppard says, ‘people respond surveys weird,’ generalises whatever method ’re using gather data. Using Facebook ads? People click Facebook ads weird. Going door knocking? People answer door weird. Call people phone? Literally answers phone anymore.pernicious aspect selection bias, pervades every aspect analysis. Even sample starts perfectly representative, may become selected time. instance, survey panels used political polling need updated time time folks don’t get anything stop responding - people may still vote need polled.Another bias aware measurement bias, ‘association treatment outcome weakened strengthened result process study data measured’ (Hernan Robins 2020, 113). implicit definition Hernan Robins (2020), important systematic. instance, ask people person income likely get different answers ask phone via online form.","code":""},{"path":"causality.html","id":"two-common-paradoxes","chapter":"16 Causality from observational data","heading":"16.2.3 Two common paradoxes","text":"two situations data can trick common ’d like explicitly go . Simpson’s paradox, Berkson’s paradox. Keep situations back mind times dealing data.Simpson’s paradox occurs estimate relationship subsets data, different relationship consider entire dataset (Simpson 1951). instance, may positive relationship undergraduate grades performance graduate school statistics economics considering department individually. undergraduate grades tended higher statistics economics graduate school performance tended opposite, may actually find negative relationship undergraduate grades performance graduate school.see let’s simulate data.Berkson’s paradox occurs estimate relationship based dataset , dataset selected relationship different general dataset (Berkson 1946). instance, dataset professional cyclists find relationship VO2 max chance winning bike race. dataset general population find enormous relationship VO2 max chance winning bike race. professional dataset just selected relationship disappears - can’t become professional unless high VO2 max.see let’s simulate data.","code":"\nset.seed(853)\nnumber_in_each <- 1000\nstatistics <- tibble(undergrad = runif(n = number_in_each, min = 0.7, max = 0.9),\n                     noise = rnorm(n = number_in_each, 0, sd = 0.1),\n                     grad = undergrad + noise,\n                     type = \"Statistics\")\neconomics <- tibble(undergrad = runif(n = number_in_each, min = 0.6, max = 0.8),\n                    noise = rnorm(n = number_in_each, 0, sd = 0.1),\n                    grad = undergrad + noise + 0.3,\n                    type = \"Economics\")\nboth = rbind(statistics, economics)\n\nboth %>% \n  ggplot(aes(x = undergrad, y = grad)) +\n  geom_point(aes(color = type), alpha = 0.1) +\n  geom_smooth(aes(color = type), method = 'lm', formula = 'y ~ x') +\n  geom_smooth(method = 'lm', formula = 'y ~ x', color = 'black') +\n  labs(x = \"Undergraduate results\",\n       y = \"Graduate results\",\n       color = \"Type\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\nset.seed(853)\nnumber_of_pros <- 100\nnumber_of_public <- 1000\nprofessionals <- \n  tibble(VO2 = runif(n = number_of_pros, min = 0.7, max = 0.9),\n         chance_of_winning = runif(n = number_of_pros, min = 0.7, max = 0.9),\n         type = \"Professionals\")\ngeneral_public <- \n  tibble(VO2 = runif(n = number_of_public, min = 0.6, max = 0.8),\n         noise = rnorm(n = number_of_public, 0, sd = 0.03),\n         chance_of_winning = VO2 + noise + 0.1,\n         type = \"Public\") %>% \n  select(-noise)\nboth = rbind(professionals, general_public)\n\nboth %>% \n  ggplot(aes(x = VO2, y = chance_of_winning)) +\n  geom_point(aes(color = type), alpha = 0.1) +\n  geom_smooth(aes(color = type), method = 'lm', formula = 'y ~ x') +\n  geom_smooth(method = 'lm', formula = 'y ~ x', color = 'black') +\n  labs(x = \"VO2 max\",\n       y = \"Chance of winning a bike race\",\n       color = \"Type\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")"},{"path":"causality.html","id":"difference-in-differences","chapter":"16 Causality from observational data","heading":"16.3 Difference in differences","text":"","code":""},{"path":"causality.html","id":"matching-and-difference-in-differences","chapter":"16 Causality from observational data","heading":"16.3.1 Matching and difference-in-differences","text":"","code":""},{"path":"causality.html","id":"introduction-14","chapter":"16 Causality from observational data","heading":"16.3.1.1 Introduction","text":"ideal situation able conduct experiment rarely possible data science setting. Can really reasonably expect Netflix allow us change prices. even , let us , , ? , rarely can explicitly create treatment control groups. Finally, experiments really expensive potentially unethical. Instead, need make . Rather counterfactual coming us randomisation, hence us knowing two treatment, try identify groups similar treatment, hence differences can attributed treatment. practice, tend even differences two groups treat. Provided pre-treatment differences satisfy assumptions (basically consistent, expect consistency continue absence treatment) – ‘parallel trends’ assumption – can look difference differences effect treatment. One lovely aspects difference differences analysis can using fairly straight-forward quantitative methods - linear regression dummy variable needed convincing job.","code":""},{"path":"causality.html","id":"motivation","chapter":"16 Causality from observational data","heading":"16.3.1.2 Motivation","text":"Consider us wanting know effect new tennis racket serve speed. One way test measure difference Roger Federer’s serve speed without tennis racket mine tennis racket. Sure, ’d find difference know much attribute tennis racket? Another way consider difference serve speed without tennis racket serve speed tennis racket. serves just getting faster naturally time? Instead, let’s combine two look difference differences!world measure Federer’s serve compare serve without new racket. measure Federer’s serve measure serve new racket. difference differences estimate effect new racket.sorts assumptions jump going make order analysis appropriate?something else may affected , Roger affect serve speed? Probably.likely Roger Federer trajectory serve speed improvement? Probably . ‘parallel trends’ assumption, dominates discussion difference differences analysis. Finally, likely variance serve speeds ? Probably .might powerful? don’t need treatment control group treatment. just need good idea differ.","code":""},{"path":"causality.html","id":"simulated-example","chapter":"16 Causality from observational data","heading":"16.3.1.3 Simulated example","text":"Let’s generate data.Let’s make graph.simple example, manually, getting average difference differences.Let’s use OLS analysis. general regression equation :\n\\[Y_{,t} = \\beta_0 + \\beta_1\\mbox{Treatment group dummy}_i + \\beta_2\\mbox{Time dummy}_t + \\beta_3(\\mbox{Treatment group dummy} \\times\\mbox{Time dummy})_{,t} + \\epsilon_{,t}\\]use * regression automatically includes separate aspects well interaction. ’s estimate \\(\\beta_3\\) interest.Fortunately, estimates !","code":"\nlibrary(broom)\nlibrary(tidyverse)\n\nset.seed(853)\n\ndiff_in_diff_example_data <- tibble(person = rep(c(1:1000), times = 2),\n                       time = c(rep(0, times = 1000), rep(1, times = 1000)),\n                       treatment_group = rep(sample(x = 0:1, size  = 1000, replace = TRUE), times = 2)\n                       )\n## We want to make the outcome slightly more likely if they were treated than if not.\ndiff_in_diff_example_data <- \n  diff_in_diff_example_data %>% \n  rowwise() %>% \n  mutate(serve_speed = case_when(\n    time == 0 & treatment_group == 0 ~ rnorm(n = 1, mean = 5, sd = 1),\n    time == 1 & treatment_group == 0 ~ rnorm(n = 1, mean = 6, sd = 1),\n    time == 0 & treatment_group == 1 ~ rnorm(n = 1, mean = 8, sd = 1),\n    time == 1 & treatment_group == 1 ~ rnorm(n = 1, mean = 14, sd = 1),\n    )\n    )\n\nhead(diff_in_diff_example_data)\n#> # A tibble: 6 × 4\n#> # Rowwise: \n#>   person  time treatment_group serve_speed\n#>    <int> <dbl>           <int>       <dbl>\n#> 1      1     0               0        4.43\n#> 2      2     0               1        6.96\n#> 3      3     0               1        7.77\n#> 4      4     0               0        5.31\n#> 5      5     0               0        4.09\n#> 6      6     0               0        4.85\ndiff_in_diff_example_data$treatment_group <- as.factor(diff_in_diff_example_data$treatment_group)\ndiff_in_diff_example_data$time <- as.factor(diff_in_diff_example_data$time)\n\ndiff_in_diff_example_data %>% \n  ggplot(aes(x = time,\n             y = serve_speed,\n             color = treatment_group)) +\n  geom_point() +\n  geom_line(aes(group = person), alpha = 0.2) +\n  theme_minimal() +\n  labs(x = \"Time period\",\n       y = \"Serve speed\",\n       color = \"Person got a new racket\") +\n  scale_color_brewer(palette = \"Set1\")\naverage_differences <- \n  diff_in_diff_example_data %>% \n  pivot_wider(names_from = time,\n              values_from = serve_speed,\n              names_prefix = \"time_\") %>% \n  mutate(difference = time_1 - time_0) %>% \n  group_by(treatment_group) %>% \n  summarise(average_difference = mean(difference))\n\naverage_differences$average_difference[2] - average_differences$average_difference[1]\n#> [1] 5.058414\ndiff_in_diff_example_regression <- lm(serve_speed ~ treatment_group*time, \n                         data = diff_in_diff_example_data)\n\ntidy(diff_in_diff_example_regression)\n#> # A tibble: 4 × 5\n#>   term                 estimate std.error statistic  p.value\n#>   <chr>                   <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)              4.97    0.0428     116.  0       \n#> 2 treatment_group1         3.03    0.0622      48.7 0       \n#> 3 time1                    1.01    0.0605      16.6 2.97e-58\n#> 4 treatment_group1:ti…     5.06    0.0880      57.5 0"},{"path":"causality.html","id":"assumptions","chapter":"16 Causality from observational data","heading":"16.3.1.4 Assumptions","text":"want use difference differences, need satisfy assumptions. three touched earlier, want focus one: ‘parallel trends’ assumption. parallel trends assumption haunts everything diff--diff analysis can never prove , can just convinced .see can never prove , consider example want know effect new stadium professional sports team’s wins/loses. consider two teams: Warriors Raptors. Warriors changed stadiums start 2019-20 season (Raptors ), consider four time periods: 2016-17 season, 2017-18 season, 2018-19 season, finally compare performance one moved, 2019-20 season. Raptors act counterfactual. means assume relationship Warriors Raptors, absence new stadium, continued change consistent way. can never know certain. present sufficient evidence assuage concerns reader may .variety reasons, worth tougher normal requirements around evidence take convince effect.four main ‘threats validity’ using difference differences address (Cunningham, 2020, pp. 272–277):Non-parallel trends. treatment control groups may based differences. can difficult convincingly argue parallel trends. case, maybe try find another factor consider model may adjust . may require difference difference differences (earlier example, perhaps add San Francisco 49ers broad geographic area Warriors). maybe re-think analysis see can make different control group. Adding additional earlier time periods may help may introduce issues (see third point).Compositional differences. concern working repeated cross-sections. composition cross-sections change? instance, work Tik Tok app rapidly growing want look effect change. initial cross-section, may mostly young people, subsequent cross-section, may older people demographics app usage change. Hence results may just age-effect, effect change interested .Long-term effects vs. reliability. discussed last chapter, trade-length analysis run. run analysis longer opportunity factors affect results. also increased chance someone treated treated. , hand, can difficult convincingly argue short-term results continue long-term.Functional form dependence. less issue outcomes similar, different functional form may responsible aspects results.","code":""},{"path":"causality.html","id":"matching","chapter":"16 Causality from observational data","heading":"16.3.1.5 Matching","text":"section draws material Gelman Hill, 2007, pp. 207-212.Difference differences powerful analysis framework. learnt began see opportunities implement everywhere. can tough identify appropriate treatment control groups. Alexander Ward, 2018, compare migrant brothers - one education different country, education US. really best match?may able match based observable variables. instance, age-group education. two different times compare smoking rates 18-year-olds one city smoking rates 18-year-olds another city. fine, fairly coarse. know differences 18-year-olds, even terms variables commonly observe, say gender education. One way deal may create sub-groups: 18-year-old males high school education, etc. sample sizes likely quickly become small. deal continuous variables? also, difference 18-year-old 19-year-old really different? Shouldn’t also compare ?One way proceed consider nearest neighbour approach. limited concern uncertainty approach. also issue large number variables end high-dimension graph. leads us propensity score matching.Propensity score matching involves assigning probability observation. construct probability based observation’s values independent variables, values treatment. probability best guess probability observation treated, regardless whether treated . instance, 18-year-old males treated 19-year-old males , much difference 18-year-old males 19-year-old males assigned probability fairly similar. can compare outcomes observations similar propensity scores.One advantage propensity score matching allows us easily consider many independent variables , can constructed using logistic regression.Let’s generate data illustrate propensity score matching. Let’s pretend work Amazon. going treat individuals free-shipping see happens average purchase.Now need add probability treated free shipping, depends variables. Younger, higher-income, male Toronto make slightly likely.Finally, need measure person’s average spend. want free shipping slightly higher without.Now construct logistic regression model ‘explains’ whether person treated function variables think explain .now add forecast dataset.Now use forecast create matches. variety ways . moment ’ll step code , worked example small number possibilities, can just manually.every person actually treated (given free shipping) want untreated person considered similar (based propensity score) possible.’re going use matching function arm package. finds closest ones treated, one treated.Now reduce dataset just matched. 371 treated, expect dataset 742 observations.Finally, can examine ‘effect’ treated average spend ‘usual’ way.Table 16.1:  cover propensity score matching widely used. Hence, need know use . People think ’s weird didn’t, way cover ANOVA people think ’s weird entire experimental design course didn’t cover even though modern ways looking differences two means. time need know flaws propensity score matching. now discuss .Matching. Propensity score matching match unobserved variables. may fine class-room setting, realistic settings likely cause issues.Modelling. results tend specific model used. King Nielsen, 2019, discuss thoroughly.Statistically. using data twice.","code":"\nlibrary(tidyverse)\nsample_size <- 10000\nset.seed(853)\n\namazon_purchase_data <-\n  tibble(\n    unique_person_id = c(1:sample_size),\n    age = runif(n = sample_size,\n                min = 18,\n                max = 100),\n    city = sample(\n      x = c(\"Toronto\", \"Montreal\", \"Calgary\"),\n      size = sample_size,\n      replace = TRUE\n      ),\n    gender = sample(\n      x = c(\"Female\", \"Male\", \"Other/decline\"),\n      size = sample_size,\n      replace = TRUE,\n      prob = c(0.49, 0.47, 0.02)\n      ),\n    income = rlnorm(n = sample_size,\n                    meanlog = 0.5, \n                    sdlog = 1)\n    )\namazon_purchase_data <-\n  amazon_purchase_data %>% \n  mutate(age_num = case_when(\n           age < 30 ~ 3,\n           age < 50 ~ 2,\n           age < 70 ~ 1,\n           TRUE ~ 0),\n         city_num = case_when(\n           city == \"Toronto\" ~ 3,\n           city == \"Montreal\" ~ 2,\n           city == \"Calgary\" ~ 1,\n           TRUE ~ 0),\n         gender_num = case_when(\n           gender == \"Male\" ~ 3,\n           gender == \"Female\" ~ 2,\n           gender == \"Other/decline\" ~ 1,\n           TRUE ~ 0),\n         income_num = case_when(\n           income > 3 ~ 3,\n           income > 2 ~ 2,\n           income > 1 ~ 1,\n           TRUE ~ 0)\n         ) %>% \n  rowwise() %>% \n  mutate(sum_num = sum(age_num, city_num, gender_num, income_num),\n         softmax_prob = exp(sum_num)/exp(12),\n         free_shipping = sample(\n           x = c(0:1),\n           size = 1,\n           replace = TRUE,\n           prob = c(1-softmax_prob, softmax_prob)\n           )\n         ) %>% \n  ungroup()\n\namazon_purchase_data <-\n  amazon_purchase_data %>% \n  dplyr::select(-age_num, -city_num, -gender_num, -income_num, -sum_num, -softmax_prob)\namazon_purchase_data <-\n  amazon_purchase_data %>% \n  mutate(mean_spend = if_else(free_shipping == 1, 60, 50)) %>% \n  rowwise() %>% \n  mutate(average_spend = rnorm(1, mean_spend, sd = 5)\n    ) %>% \n  ungroup() %>% \n  dplyr::select(-mean_spend)\n\n## Fix the class on some\namazon_purchase_data <-\n  amazon_purchase_data %>% \n  mutate_at(vars(city, gender, free_shipping), ~as.factor(.)) ## Change some to factors\ntable(amazon_purchase_data$free_shipping)\n#> \n#>    0    1 \n#> 9629  371\n\nhead(amazon_purchase_data)\n#> # A tibble: 6 × 7\n#>   unique_person_id   age city    gender income free_shipping\n#>              <int> <dbl> <fct>   <fct>   <dbl> <fct>        \n#> 1                1  47.5 Calgary Female  1.72  0            \n#> 2                2  27.8 Montre… Male    1.54  0            \n#> 3                3  57.7 Toronto Female  3.16  0            \n#> 4                4  43.9 Toronto Male    0.636 0            \n#> 5                5  21.1 Toronto Female  1.43  0            \n#> 6                6  51.1 Calgary Male    1.18  0            \n#> # … with 1 more variable: average_spend <dbl>\npropensity_score <- glm(free_shipping ~ age + city + gender + income, \n                        family = binomial,\n                        data = amazon_purchase_data)\namazon_purchase_data <- \n  augment(propensity_score, \n          data = amazon_purchase_data,\n          type.predict = \"response\") %>% \n  dplyr::select(-.resid, -.std.resid, -.hat, -.sigma, -.cooksd) \namazon_purchase_data <- \n  amazon_purchase_data %>% \n  arrange(.fitted, free_shipping)\namazon_purchase_data$treated <- if_else(amazon_purchase_data$free_shipping == 0, 0, 1)\namazon_purchase_data$treated <- as.integer(amazon_purchase_data$treated)\n\nmatches <- arm::matching(z = amazon_purchase_data$treated, score = amazon_purchase_data$.fitted)\n\namazon_purchase_data <- cbind(amazon_purchase_data, matches)\namazon_purchase_data_matched <- \n  amazon_purchase_data %>% \n  filter(match.ind != 0) %>% \n  dplyr::select(-match.ind, -pairs, -treated)\n\nhead(amazon_purchase_data_matched)\n#>   unique_person_id      age     city gender     income\n#> 1             5710 81.15636 Montreal Female 0.67505625\n#> 2             9458 97.04859 Montreal Female 9.49752179\n#> 3             6428 83.21262  Calgary   Male 0.05851482\n#> 4             2022 98.97504 Montreal   Male 1.66683768\n#> 5             9824 64.61936  Calgary Female 3.35263989\n#> 6             1272 97.09546  Toronto Female 0.71813784\n#>   free_shipping average_spend     .fitted cnts\n#> 1             0      47.36258 0.001375987    1\n#> 2             1      61.15317 0.001376161    1\n#> 3             0      49.90080 0.001560150    1\n#> 4             1      57.75673 0.001560418    1\n#> 5             1      64.69709 0.002207195    1\n#> 6             0      56.64754 0.002207514    1\npropensity_score_regression <- lm(average_spend ~ age + city + gender + income + free_shipping, \n                                  data = amazon_purchase_data_matched)\nhuxtable::huxreg(propensity_score_regression)"},{"path":"causality.html","id":"case-study---lower-advertising-revenue-reduced-french-newspaper-prices-between-1960-and-1974","chapter":"16 Causality from observational data","heading":"16.4 Case study - Lower advertising revenue reduced French newspaper prices between 1960 and 1974","text":"","code":""},{"path":"causality.html","id":"introduction-15","chapter":"16 Causality from observational data","heading":"16.4.1 Introduction","text":"case study introduce Angelucci Cagé, 2019, replicate main findings. Angelucci Cagé, 2019, paper difference differences used examine effect reduction advertising revenues newspapers’ content prices. create dataset ‘French newspapers 1960 1974.’ ‘perform difference--differences analysis’ exploit ‘introduction advertising television’ change ‘affected national newspapers severely local ones.’ ‘find robust evidence decrease amount journalistic-intensive content produced subscription price.’order conduct analysis use dataset provide alongside paper. dataset available : https://www.openicpsr.org/openicpsr/project/116438/version/V1/view. available download registration. dataset Stata data format, use haven package read (Wickham Miller, 2019).","code":"\nlibrary(here)\nlibrary(haven)\nlibrary(huxtable)\n#> \n#> Attaching package: 'huxtable'\n#> The following objects are masked from 'package:ggdag':\n#> \n#>     label, label<-\n#> The following object is masked from 'package:dplyr':\n#> \n#>     add_rownames\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     theme_grey\nlibrary(scales)\n#> \n#> Attaching package: 'scales'\n#> The following object is masked from 'package:huxtable':\n#> \n#>     number_format\n#> The following object is masked from 'package:purrr':\n#> \n#>     discard\n#> The following object is masked from 'package:readr':\n#> \n#>     col_factor\nlibrary(tidyverse)"},{"path":"causality.html","id":"background-1","chapter":"16 Causality from observational data","heading":"16.4.2 Background","text":"Newspapers trouble. can probably think local newspaper closed recently pressure brought internet. issue isn’t new. television started, similar concerns. paper, Angelucci Cagé use introduction television advertising France, announced 1967, examine effect decreased advertising revenue newspapers.reason important allows us disentangle competing effects. instance, newspapers becoming redundant can longer charge high prices ads consumers prefer get news ways? fewer journalists needed smartphones technology mean can productive? Angelucci Cagé look advertising revenue features, new advertising platform arrives, case television advertising.","code":""},{"path":"causality.html","id":"data-1","chapter":"16 Causality from observational data","heading":"16.4.3 Data","text":"() dataset contains annual data local national newspapers 1960 1974, well detailed information television content. 1967, French government announced relax long-standing regulations prohibited television advertising. provide evidence reform can plausibly interpreted exogenous negative shock advertising side newspaper industry… []t likely introduction television advertising constituted direct shock advertising side newspaper industry indirect shock reader side… (O)ur empirical setting constitutes unique opportunity isolate consequences decrease newspapers’ advertising revenues choices regarding size newsroom, amount information produce, prices charge sides market.authors’ argue national newspapers affected television advertising change, local newspapers . national newspapers treatment group local newspapers control group.dataset can read using read_dta(), function within haven package reading Stata dta files. equivalent read_csv().1,196 observations dataset 52 variables. authors interested 1960-1974 time period around 100 newspapers. 14 national newspapers beginning period 12 end.just want replicate main results, don’t need variables. just select() ones interested change class() needed.can now look main variables interest national (Figure 16.5) local daily newspapers (Figure 16.6).\nFigure 16.5: Angelucci Cagé, 2019, summary statistics: national daily newspapers\nSource: Angelucci Cagé, 2019, p. 333.\nFigure 16.6: Angelucci Cagé, 2019, summary statistics: local daily newspapers\nSource: Angelucci Cagé, 2019, p. 334.Please read section paper see describe dataset.interested change 1967 onward.","code":"\nnewspapers <- read_dta(here::here(\"inputs/data/116438-V1/data/dta/Angelucci_Cage_AEJMicro_dataset.dta\"))\n\ndim(newspapers)\n#> [1] 1196   52\nnewspapers <- \n  newspapers %>% \n  dplyr::select(year, id_news, after_national, local, national, ## Diff in diff variables\n         ra_cst, qtotal, ads_p4_cst, ads_s, ## Advertising side dependents\n         ps_cst, po_cst, qtotal, qs_s, rs_cst) %>% #Reader side dependents\n  mutate(ra_cst_div_qtotal = ra_cst / qtotal) %>% ## An advertising side dependents needs to be built\n  mutate_at(vars(id_news, after_national, local, national), ~as.factor(.)) %>% ## Change some to factors\n  mutate(year = as.integer(year))\nnewspapers %>% \n  mutate(type = if_else(local == 1, \"Local\", \"National\")) %>% \n  ggplot(aes(x = year, y = ra_cst)) +\n  geom_point(alpha = 0.5) +\n  scale_y_continuous(labels = dollar_format(prefix=\"$\", suffix = \"M\", scale = 0.000001)) +\n  labs(x = \"Year\",\n       y = \"Advertising revenue\") +\n  facet_wrap(vars(type),\n               nrow = 2) +\n  theme_classic() +\n  geom_vline(xintercept = 1966.5, linetype = \"dashed\")"},{"path":"causality.html","id":"model-1","chapter":"16 Causality from observational data","heading":"16.4.4 Model","text":"model interested estimating :\n\\[\\mbox{ln}(y_{n,t}) = \\beta_0 + \\beta_1(\\mbox{National dummy}\\times\\mbox{1967 onward dummy}) + \\lambda_n + \\gamma_y + \\epsilon.\\]\n\\(\\lambda_n\\) fixed effect newspaper, \\(\\gamma_y\\) fixed effect year. just use regular linear regression, different dependent variables. \\(\\beta_1\\) coefficient interested .","code":""},{"path":"causality.html","id":"results-1","chapter":"16 Causality from observational data","heading":"16.4.5 Results","text":"can run models using lm().Looking advertising-side variables.Table 16.2:  Similarly, can look reader-side variables.Table 16.3:  ","code":"\n## Advertising side\nad_revenue <- lm(log(ra_cst) ~ after_national + id_news + year, data = newspapers)\nad_revenue_div_circulation <- lm(log(ra_cst_div_qtotal) ~ after_national + id_news + year, data = newspapers)\nad_price <- lm(log(ads_p4_cst) ~ after_national + id_news + year, data = newspapers)\nad_space <- lm(log(ads_s) ~ after_national + id_news + year, data = newspapers)\n\n## Consumer side\nsubscription_price <- lm(log(ps_cst) ~ after_national + id_news + year, data = newspapers)\nunit_price <- lm(log(po_cst) ~ after_national + id_news + year, data = newspapers)\ncirculation <- lm(log(qtotal) ~ after_national + id_news + year, data = newspapers)\nshare_of_sub <- lm(log(qs_s) ~ after_national + id_news + year, data = newspapers)\nrevenue_from_sales <- lm(log(rs_cst) ~ after_national + id_news + year, data = newspapers)\nomit_me <- c(\"(Intercept)\", \"id_news3\", \"id_news6\", \"id_news7\", \"id_news13\", \n             \"id_news16\", \"id_news25\", \"id_news28\", \"id_news34\", \"id_news38\", \n             \"id_news44\", \"id_news48\", \"id_news51\", \"id_news53\", \"id_news54\", \n             \"id_news57\", \"id_news60\", \"id_news62\", \"id_news66\", \"id_news67\", \n             \"id_news70\", \"id_news71\", \"id_news72\", \"id_news80\", \"id_news82\", \n             \"id_news88\", \"id_news95\", \"id_news97\", \"id_news98\", \"id_news103\", \n             \"id_news105\", \"id_news106\", \"id_news118\", \"id_news119\", \"id_news127\", \n             \"id_news136\", \"id_news138\", \"id_news148\", \"id_news151\", \"id_news153\", \n             \"id_news154\", \"id_news157\", \"id_news158\", \"id_news161\", \"id_news163\", \n             \"id_news167\", \"id_news169\", \"id_news179\", \"id_news184\", \"id_news185\", \n             \"id_news187\", \"id_news196\", \"id_news206\", \"id_news210\", \"id_news212\", \n             \"id_news213\", \"id_news224\", \"id_news225\", \"id_news234\", \"id_news236\", \n             \"id_news245\", \"id_news247\", \"id_news310\", \"id_news452\", \"id_news467\", \n             \"id_news469\", \"id_news480\", \"id_news20040\", \"id_news20345\", \n             \"id_news20346\", \"id_news20347\", \"id_news20352\", \"id_news20354\", \n             \"id_news21006\", \"id_news21025\", \"id_news21173\", \"id_news21176\", \n             \"id_news33718\", \"id_news34689\", \"id_news73\")\n\nhuxreg(\"Ad. rev.\" = ad_revenue, \n       \"Ad rev. div. circ.\" = ad_revenue_div_circulation, \n       \"Ad price\" = ad_price, \n       \"Ad space\" = ad_space,\n        omit_coefs = omit_me, \n        number_format = 2\n        )\nomit_me <- c(\"(Intercept)\", \"id_news3\", \"id_news6\", \"id_news7\", \"id_news13\", \n             \"id_news16\", \"id_news25\", \"id_news28\", \"id_news34\", \"id_news38\", \n             \"id_news44\", \"id_news48\", \"id_news51\", \"id_news53\", \"id_news54\", \n             \"id_news57\", \"id_news60\", \"id_news62\", \"id_news66\", \"id_news67\", \n             \"id_news70\", \"id_news71\", \"id_news72\", \"id_news80\", \"id_news82\", \n             \"id_news88\", \"id_news95\", \"id_news97\", \"id_news98\", \"id_news103\", \n             \"id_news105\", \"id_news106\", \"id_news118\", \"id_news119\", \"id_news127\", \n             \"id_news136\", \"id_news138\", \"id_news148\", \"id_news151\", \"id_news153\", \n             \"id_news154\", \"id_news157\", \"id_news158\", \"id_news161\", \"id_news163\", \n             \"id_news167\", \"id_news169\", \"id_news179\", \"id_news184\", \"id_news185\", \n             \"id_news187\", \"id_news196\", \"id_news206\", \"id_news210\", \"id_news212\", \n             \"id_news213\", \"id_news224\", \"id_news225\", \"id_news234\", \"id_news236\", \n             \"id_news245\", \"id_news247\", \"id_news310\", \"id_news452\", \"id_news467\", \n             \"id_news469\", \"id_news480\", \"id_news20040\", \"id_news20345\", \n             \"id_news20346\", \"id_news20347\", \"id_news20352\", \"id_news20354\", \n             \"id_news21006\", \"id_news21025\", \"id_news21173\", \"id_news21176\", \n             \"id_news33718\", \"id_news34689\", \"id_news73\")\n\nhuxreg(\"Subscription price\" = subscription_price, \n       \"Unit price\" = unit_price, \n       \"Circulation\" = circulation, \n       \"Share of sub\" = share_of_sub,\n       \"Revenue from sales\" = revenue_from_sales,\n       omit_coefs = omit_me, \n       number_format = 2\n       )"},{"path":"causality.html","id":"other-points","chapter":"16 Causality from observational data","heading":"16.4.6 Other points","text":"certainly find many cases appears difference 1967 onward.general, able obtain results similar Angelucci Cagé, 2019. spent time, probably replicate findings perfectly. Isn’t great! else ?Parallel trends: Notice wonderful way test ‘parallel trends’ assumption pp. 350-351.Discussion: Look wonderful discussion (pp. 353-358) interpretation, external validity, robustness.–>","code":""},{"path":"causality.html","id":"case-study---funding-of-clinical-trials-and-reported-drug-efficacy","chapter":"16 Causality from observational data","heading":"16.5 Case study - Funding of Clinical Trials and Reported Drug Efficacy","text":"Oostrom (2021) looks clinical trials drugs. days, course, know lot may ever wished , clinical trials. one thing (think) know , well, clinical. mean, doesn’t matter actual trial, outcome . Oostrom (2021) says isn’t true.way background, clinical trials needed drug can approved. Oostrom (2021) finds pharmaceutical firms sponsor clinical trial, ‘drug appears 0.15 standard deviations effective trial sponsored drug’s manufacturer, compared drug trial without drug manufacturer’s involvement.’ exploiting fact often ‘exact sets drugs often compared different randomized control trials conducted parties different financial interests.’main finding (Oostrom 2021, 2):Utilizing dozens drug combinations across hundreds clinical trials, estimate drug appears 36 percent effective (0.15 standard deviations base 0.42) trial sponsored drug’s manufacturing marketing firm, compared drug, evaluated comparators, without drug manufacturer’s involvement. medical literature, measure efficacy, case antidepressants, share patients respond medication , case schizophrenia, average decline symptoms.might happen? Oostrom (2021) looks variety different options, grouped happen trial happen trial ‘publication bias.’ finds ‘publication bias can explain much half sponsorship effect. Incorporating data unpublished clinical trials, find sponsored trials less likely publish non-positive results drugs.’Oostrom (2021) focuses antidepressant antipsychotic drugs allows obtain dataset trials. ‘arm’ trial refers ‘unit randomization occurs. Arms often unique drugs occasionally refer unique drug dosage combinations.’ (Oostrom 2021, 9).Summary statistics provided summary table (Figure 16.7) (approach common economics, great idea hides distribution data - better plot raw data.)\nFigure 16.7: Summary statistics Ooostrom\nmodel :\\[y_{ij} = \\alpha + \\beta \\mbox{ Sponsor}_{ij} + X_{ij}\\gamma + G_{d(),s(j)} +\\epsilon_{ij}\\]\\(y_{ij}\\) efficacy arm \\(\\) trial \\(j\\). main coefficient interest \\(\\beta\\) based whether \\(\\mbox{Sponsor}_{ij}\\). outcome relative placebo arm trial, least effective arm.‘Table 3.3’ paper actually reason included case student. sounds odd ’ve read millions papers unclear results. ‘Table 3.3’ (republished Figure 16.8) beautiful ’ll allow speak .\nFigure 16.8: Results\npaper available : https://www.tamaroostrom.com/research ’d recommend brief read.","code":""},{"path":"causality.html","id":"regression-discontinuity-design","chapter":"16 Causality from observational data","heading":"16.6 Regression discontinuity design","text":"","code":""},{"path":"causality.html","id":"introduction-16","chapter":"16 Causality from observational data","heading":"16.6.1 Introduction","text":"Regression discontinuity design (RDD) popular way get causality continuous variable cut-offs determine treatment. difference student gets 79 per cent student gets 80 per cent? Probably much, one gets -, gets B+, seeing transcript affect gets job affect income. case percentage ‘forcing variable’ cut-- ‘threshold.’ treatment determined forcing variable need control variable. , seemingly arbitrary cut-offs can seen time. Hence, ‘explosion’ use regression discontinuity design (Figure 16.9).Please note ’ve followed terminology Taddy, 2019. Gelman Hill, 2007, others use slightly different terminology. instance, Cunningham refers forcing function running variable. doesn’t matter use long consistent. terminology familiar please feel free use , share !\nFigure 16.9: explosion regression discontinuity designs recent years.\nSource: John Holbein, 13 February 2020.key assumptions :cut-‘known, precise free manipulation’ (Cunningham, 2020, p. 163).forcing function continuous means can say people either side threshold , happening just fall either side threshold.","code":""},{"path":"causality.html","id":"simulated-example-1","chapter":"16 Causality from observational data","heading":"16.6.2 Simulated example","text":"Let’s generate data.Table 15.2:  Let’s make graph.can use dummy variable linear regression estimate effect (’re hoping ’s 2 imposed.)Table 16.4:  various caveats estimate ’ll get later, essentials .great thing regression discontinuity can almost good RCT. instance, (thank John Holbein pointer) Bloom, Bell, Reiman (2020) compare randomized trials RDDs find RCTs compare favourably.","code":"\nlibrary(broom)\nlibrary(tidyverse)\n\nset.seed(853)\n\nnumber_of_observation <- 1000\n\nrdd_example_data <- tibble(person = c(1:number_of_observation),\n                           grade = runif(number_of_observation, min = 78, max = 82),\n                           income = rnorm(number_of_observation, 10, 1)\n                           )\n\n## We want to make income more likely to be higher if they are have a grade over 80\nrdd_example_data <- \n  rdd_example_data %>% \n  mutate(income = if_else(grade > 80, income + 2, income))\n\nhead(rdd_example_data)\nrdd_example_data %>% \n  ggplot(aes(x = grade,\n             y = income)) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(data = rdd_example_data %>% filter(grade < 80), \n              method='lm',\n              color = \"black\") +\n  geom_smooth(data = rdd_example_data %>% filter(grade >= 80), \n              method='lm',\n              color = \"black\") +\n  theme_minimal() +\n  labs(x = \"Grade\",\n       y = \"Income ($)\")\n#> `geom_smooth()` using formula 'y ~ x'\n#> `geom_smooth()` using formula 'y ~ x'\nrdd_example_data <- \n  rdd_example_data %>% \n  mutate(grade_80_and_over = if_else(grade < 80, 0, 1)) \n\nlm(income ~ grade + grade_80_and_over, data = rdd_example_data) %>% \n  tidy()"},{"path":"causality.html","id":"different-slopes","chapter":"16 Causality from observational data","heading":"16.6.2.1 Different slopes","text":"Figure 16.10 shows example different slopes.\nFigure 16.10: Effect minimum unit pricing alcohol Scotland.\nSource: John Burn-Murdoch, 7 February 2020.","code":""},{"path":"causality.html","id":"overlap","chapter":"16 Causality from observational data","heading":"16.6.3 Overlap","text":"randomised control trial /B testing section, randomised assignment treatment, imposed control treatment groups treatment. moved difference--differences, assumed common trend treated control groups. allowed groups different, ‘difference ’ differences. Finally, considered matching, said even control treatment groups seemed quite different able match treated group similar ways, apart fact treated.regression discontinuity consider slightly different setting - two groups completely different terms forcing variable - either side threshold. overlap . know threshold believe either side essentially matched. Let’s consider 2019 NBA Eastern Conference Semifinals - Toronto Philadelphia. Game 1: Raptors win 108-95; Game 2: 76ers win 94-89; Game 3: 76ers win 116-95; Game 4: Raptors win 101-96; Game 5: Raptors win 125-89; Game 6: 76ers win 112-101; finally, Game 7: Raptors win 92-90, ball win bouncing rim four times. really much difference teams (Figure 16.11)?\nFigure 16.11: took four bounces go , different teams…?\nSource: Stan Behal / Postmedia Network.","code":""},{"path":"causality.html","id":"examples","chapter":"16 Causality from observational data","heading":"16.6.4 Examples","text":"difference--differences, learnt , began see opportunities implement everywhere. Frankly, find lot easier think legitimate examples using regression discontinuity difference--differences. , risk mentioning yet another movie 1990s none seen, think RDD, first thought often Sliding Doors (Figure 16.12).\nFigure 16.12: Nobody expects Spanish Inquisition.\nSource: Mlotek, Haley, 2018, ‘Almosts -ifs ’Sliding Doors’’, Ringer, 24 April, freely available : https://www.theringer.com/movies/2018/4/24/17261506/sliding-doors-20th-anniversary.movie great soundtrack help propel Gwyneth Paltrow super-stardom, features iconic moment Paltrow’s character, Helen, arrives tube station point movie splits two. one version just makes train, arrives home find boyfriend cheating ; another just misses train doesn’t find boyfriend.’d say, spoiler alert, movie released 1998, … course, ‘threshold’ turns important. world gets train leaves boyfriend, cuts hair, changes everything life. world misses train doesn’t. least initially. , can’t say better Ashley Fetters:end Sliding Doors, “bad” version Helen’s life elides right “good” version; even “bad” version, philandering !@#$%^& boyfriend eventually gets found dumped, true love eventually gets met-cute, MVP friend comes . According Sliding Doors philosophy, words, even lives take fluky, chaotic detours, ultimately good-hearted people find , bad boyfriends home-wreckers world get comeuppance. ’s freak turn events allows cheating boyfriend just keep cheating, well-meaning, morally upright soulmates just keep floating around universe unacquainted.Fetters, Ashley, 2018, ‘Think Lot: Sliding Doors Sliding Doors,’ Cut, 9 April, freely available : https://www.thecut.com/2018/04/-think----lot--sliding-doors--sliding-doors.html.’m getting -track , point , seem though ‘threshold,’ seems though ’s continuity!Let’s see legitimate implementations regression discontinuity. (thank Ryan Edwards pointing .)","code":""},{"path":"causality.html","id":"elections","chapter":"16 Causality from observational data","heading":"16.6.4.1 Elections","text":"Elections common area application regression discontinuity election close arguably ’s much difference candidates. plenty examples regression discontinuity elections setting, one recent one George, Siddharth Eapen, 2019, ‘Like Father, Like Son? Effect Political Dynasties Economic Development,’ freely available : https://www.dropbox.com/s/orhvh3n03wd9ybl/sid_JMP_dynasties_latestdraft.pdf?dl=0.paper George interested political dynasties. child politician likely elected child politician, happen also similarly skilled politics? Regression discontinuity can help close election, can look differences places someone narrowly won similar someone narrowly lost.George, 2019, case examines:descendant effects using close elections regression discontinuity (RD) design. focus close races dynastic descendants (.e. direct relatives former officeholders) non-dynasts, compare places descendant narrowly won descendant narrowly lost. elections, descendants non-dynasts similar demographic political characteristics, win similar places similar rates. Nevertheless, find negative economic effects descendant narrowly wins. Villages represented descendant lower asset ownership public good provision electoral term: households less likely live brick house basic amenities like refrigerator, mobile phone, vehicle. Moreover, voters assess descendants perform worse office. additional standard deviation exposure descendants lowers village’s wealth rank 12pp.model George, 2019, estimates (p. 19:\n\\[y_i = \\alpha_{\\mbox{district}} + \\beta \\times \\mbox{Years descendant rule}_i + f(\\mbox{Descendant margin}) + \\gamma X_i + \\epsilon_{,t}.\\]\nmodel, \\(y_i\\) various development outcomes village \\(\\); \\(\\mbox{Years descendant rule}_i\\) number years dynastic descendant represented village \\(\\) national state parliament; \\(\\mbox{Descendant margin}\\) vote share difference dynastic descendant non-dynast; \\(\\gamma X_i\\) vector village-level adjustments.George, 2019, conducts whole bunch tests validity regression discontinuity design (p. 19). critical order results believed. lot different results one shown Figure 16.13.\nFigure 16.13: George, 2019, descendant effects identified using close elections RD design (p. 41).\n","code":""},{"path":"causality.html","id":"economic-development","chapter":"16 Causality from observational data","heading":"16.6.4.2 Economic development","text":"One issues considering economic development place typically either subject treatment . However, sometimes regression discontinuity allows us compare areas just barely treated just barely .One recent paper Esteban Mendez-Chacon Diana Van Patten, 2020, ‘Multinationals, monopsony local development: Evidence United Fruit Company’ available : https://www.dianavanpatten.com/. interested effect United Fruit Company (UFCo), given land Costa Rica 1889 1984. given roughly 4 per cent national territory around 4500 acres. key land assignment redrawn 1904 based river hence re-assignment essentially random regard determinants growth point. compare areas assigned UFCo . find:find firm positive persistent effect living standards. Regions within UFCo 26 per cent less likely poor 1973 nearby counterfactual locations, 63 per cent gap closing following three decades. Company documents explain key concern time attract maintain sizable workforce, induced firm invest heavily local amenities likely account result.model :\n\\[y_{,g,t} = \\gamma\\mbox{UFCo}_g + f(\\mbox{geographic location}_g) + X_{,g,t}\\beta + X_g\\Gamma + \\alpha_t + \\epsilon_{,g,t}.\\]\nmodel, \\(y_{,g,t}\\) development outcome household \\(\\) census-block \\(g\\) year \\(t\\); \\(\\gamma\\mbox{UFCo}_g\\) indicator variable whether census-block UFCo area ; \\(f(\\mbox{geographic location}_g)\\) function latitude longitude adjust geographic area; \\(X_{,g,t}\\) covariates household \\(\\); \\(X_g\\) geographic characteristics census-block; \\(\\alpha_t\\) year fixed effect., lot different results one shown Figure 16.14.\nFigure 16.14: George, 2020, UFCo effect probability poor (p. 17).\n","code":""},{"path":"causality.html","id":"implementation","chapter":"16 Causality from observational data","heading":"16.6.5 Implementation","text":"Although fairly conceptually similar work done past, wanting use regression discontinuity work might like consider specialised package. package rdrobust one recommendation, although others available try interested. (rdd package go-, seems taken CRAN recently. use RDD, maybe just follow see comes back one pretty nice.)Let’s look example using rdrobust.","code":"\nlibrary(rdrobust)\nrdrobust(y = rdd_example_data$income, \n         x = rdd_example_data$grade, \n         c = 80, h = 2, all = TRUE) %>% \n  summary()\n#> Call: rdrobust\n#> \n#> Number of Obs.                 1000\n#> BW type                      Manual\n#> Kernel                   Triangular\n#> VCE method                       NN\n#> \n#> Number of Obs.                  497          503\n#> Eff. Number of Obs.             497          503\n#> Order est. (p)                    1            1\n#> Order bias  (q)                   2            2\n#> BW est. (h)                   2.000        2.000\n#> BW bias (b)                   2.000        2.000\n#> rho (h/b)                     1.000        1.000\n#> Unique Obs.                     497          503\n#> \n#> =============================================================================\n#>         Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]       \n#> =============================================================================\n#>   Conventional     1.974     0.143    13.783     0.000     [1.693 , 2.255]     \n#> Bias-Corrected     1.977     0.143    13.805     0.000     [1.696 , 2.258]     \n#>         Robust     1.977     0.211     9.374     0.000     [1.564 , 2.390]     \n#> ============================================================================="},{"path":"causality.html","id":"fuzzy-rdd","chapter":"16 Causality from observational data","heading":"16.6.6 Fuzzy RDD","text":"examples point ‘sharp’ RDD. , threshold strict. However, reality, often boundary little less strict. instance, consider drinking age. Although legal drinking age, say 19. looked number people drank, ’s likely increase years leading age. Perhaps went Australia drinking age 18 drank. perhaps snuck bar 17, etc.sharp RDD setting, know value forcing function know outcome. instance, get grade 80 know got -, got grade 79 know got B+. fuzzy RDD known probability. can say Canadian 19-year-old likely drunk alcohol Canadian 18 year old, number Canadian 18-year-olds drunk alcohol zero.may possible deal fuzzy RDD settings appropriate choice model data. may also possible deal using instrumental variables, cover next section.","code":""},{"path":"causality.html","id":"threats-to-validity","chapter":"16 Causality from observational data","heading":"16.6.7 Threats to validity","text":"continuity assumption fairly important, test based counterfactual. Instead need convince people . Ways include:Using test/train set-.Trying different specifications (careful results don’t broadly persist just consider linear quadratic functions).Considering different subsets data.Consider different windows.-front uncertainty intervals, especially graphs.Discuss assuage concerns possibility omitted variables.threshold also important. instance, actual shift non-linear relationship?want ‘sharp’ effect possible, thresholds known, gamed. instance, lot evidence people run certain marathon times, know people aim certain grades. Similarly, side, lot easier instructor just give justify Bs. One way look consider ‘balanced’ sample either side threshold. using histograms appropriate bins, instance Figure 16.15, E. J. Allen et al. (2017).\nFigure 16.15: Bunching around marathon times.\nneed really think possible effect decision around choice model. see consider difference linear polynomial.","code":"\nsome_data <- \n  tibble(outcome = rnorm(n = 100, mean = 1, sd = 1),\n         running_variable = c(1:100),\n         location = \"before\")\n\nsome_more_data <- \n  tibble(outcome = rnorm(n = 100, mean = 2, sd = 1),\n         running_variable = c(101:200),\n         location = \"after\")\n\nboth <- \n  rbind(some_data, some_more_data)\n\nboth %>% \n  ggplot(aes(x = running_variable, y = outcome, color = location)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(formula = y~x, method = 'lm')\n  \nboth %>% \n  ggplot(aes(x = running_variable, y = outcome, color = location)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(formula = y ~ poly(x, 3), method = 'lm')"},{"path":"causality.html","id":"weaknesses","chapter":"16 Causality from observational data","heading":"16.6.8 Weaknesses","text":"External validity may hard - think -/B+ example - think findings generalise B-/C+?important responses close cut-. even whole bunch B- + students, don’t really help much. Hence need lot data.lot freedom researcher, open science best practice becomes vital.","code":""},{"path":"causality.html","id":"case-study---stiers-hooghe-and-dassonneville-2020","chapter":"16 Causality from observational data","heading":"16.7 Case study - Stiers, Hooghe, and Dassonneville, 2020","text":"Paper: Stiers, D., Hooghe, M. Dassonneville, R., 2020. Voting 16: lowering voting age lead political engagement? Evidence quasi-experiment city Ghent (Belgium). Political Science Research Methods, pp.1-8. Available : https://www.cambridge.org/core/journals/political-science-research--methods/article/voting--16--lowering--voting-age-lead---political-engagement-evidence---quasiexperiment---city--ghent-belgium/172A2D9B75ECB66E98C9680787F302AD#fndtn-informationData: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/J1FQW9","code":""},{"path":"causality.html","id":"case-study---caughey-and-sekhon.-2011","chapter":"16 Causality from observational data","heading":"16.8 Case study - Caughey, and Sekhon., 2011","text":"Paper: Caughey, Devin, Jasjeet S. Sekhon. “Elections regression discontinuity design: Lessons close US house races, 1942–2008.” Political Analysis 19.4 (2011): 385-408. Available : https://www.cambridge.org/core/journals/political-analysis/article/elections---regression-discontinuity-design-lessons--close-us-house-races-19422008/E5A69927D29BE682E012CAE9BFD8AEB7Data: https://dataverse.harvard.edu/dataset.xhtml?persistentId=hdl:1902.1/16357&version=1.0","code":""},{"path":"causality.html","id":"instrumental-variables","chapter":"16 Causality from observational data","heading":"16.9 Instrumental variables","text":"","code":""},{"path":"causality.html","id":"introduction-17","chapter":"16 Causality from observational data","heading":"16.9.1 Introduction","text":"Instrumental variables (IV) approach can handy type treatment control going , lot correlation variables possibly don’t variable actually measures interested . adjusting observables enough create good estimate. Instead find variable - eponymous instrumental variable - :correlated treatment variable, butnot correlated outcome.solves problem way instrumental variable can effect treatment variable, able adjust understanding effect treatment variable appropriately. trade-instrumental variables must satisfy bunch different assumptions, , frankly, difficult identify ex ante. Nonetheless, able use powerful tool speaking causality.canonical instrumental variables example smoking. days know smoking causes cancer. smoking correlated lot variables, instance, education, actually education causes cancer. RCTs may possible, likely troublesome terms speed ethics, instead look variable correlated smoking, , , lung cancer. case, look tax rates, policy responses, cigarettes. tax rates cigarettes correlated number cigarettes smoked, correlated lung cancer, impact cigarette smoking, can assess effect cigarettes smoked lung cancer.implement instrumental variables first regress tax rates cigarette smoking get coefficient instrumental variable, (separate regression) regress tax rates lung cancer get coefficient instrumental variable. estimate ratio coefficients. (Gelman Hill 2007, 219) describe ratio ‘Wald estimate.’Following language (Gelman Hill 2007, 216) use instrumental variables make variety assumptions including:Ignorability instrument.Correlation instrumental variable treatment variable.Monotonicity.Exclusion restriction.summarise exactly instrumental variables , better recommend first pages ‘Instrumental Variables’ chapter Cunningham (2021), key paragraph particular (way background, Cunningham explained impossible randomly allocate ‘clean’ ‘dirty’ water randomised controlled trial continues…):Snow need way trick data allocation clean dirty water people associated determinants cholera mortality, hygiene poverty. just need someone something making treatment assignment .Fortunately Snow, rest London, someone something existed. London 1800s, many different water companies serving different areas city. served one company. Several took water Thames, heavily polluted sewage. service areas companies much higher rates cholera. Chelsea water company exception, exceptionally good filtration system. ’s Snow major insight. 1849, Lambeth water company moved intake point upstream along Thames, main sewage discharge point, giving customers purer water. Southwark Vauxhall water company, hand, left intake point downstream sewage discharged. Insofar kinds people company serviced approximately , comparing cholera rates two houses experiment Snow desperately needed test hypothesis.","code":""},{"path":"causality.html","id":"history","chapter":"16 Causality from observational data","heading":"16.9.2 History","text":"history instrumental variables rare statistical mystery, Stock Trebbi (2003) provide brief overview. method first published Wright (1928). book effect tariffs animal vegetable oil. might instrumental variables important book tariffs animal vegetable oil? fundamental problem effect tariffs depends supply demand. know prices quantities, don’t know driving effect. can use instrumental variables pin causality.gets interesting, becomes something mystery, instrumental variables discussion Appendix B. made major statistical break-hide appendix? , Philip G. Wright, book’s author, son Sewall Wright, considerable expertise statistics specific method used Appendix B. Hence mystery Appendix B - Philip Sewall write ? Cunningham (2021) Stock Trebbi (2003) go detail, balance feel likely Philip actually author work.","code":""},{"path":"causality.html","id":"simulated-example-2","chapter":"16 Causality from observational data","heading":"16.9.3 Simulated example","text":"Let’s generate data. explore simulation related canonical example health status, smoking, tax rates. looking explain healthy someone based amount smoke, via tax rate smoking. going generate different tax rates provinces. understanding tax rate cigarettes now pretty much provinces, fairly recent. ’ll pretend Alberta low tax, Nova Scotia high tax.reminder, simulating data illustrative purposes, need impose answer want. actually use instrumental variables reversing process.Now need relate number cigarettes someone smoked health. ’ll model health status draw normal distribution, either high low mean depending whether person smokes.Now need relationship cigarettes province (illustration, provinces different tax rates).Table 16.5:  Now can look data.Finally, can use tax rate instrumental variable estimate effect smoking health.find, luckily, smoke health likely worse don’t smoke.Equivalently, can think instrumental variables two-stage regression context.","code":"\nlibrary(broom)\nlibrary(tidyverse)\n\nset.seed(853)\n\nnumber_of_observation <- 10000\n\niv_example_data <- tibble(person = c(1:number_of_observation),\n                          smoker = sample(x = c(0:1),\n                                          size = number_of_observation, \n                                          replace = TRUE)\n                          )\niv_example_data <- \n  iv_example_data %>% \n  mutate(health = if_else(smoker == 0,\n                          rnorm(n = n(), mean = 1, sd = 1),\n                          rnorm(n = n(), mean = 0, sd = 1)\n                          )\n         )\n## So health will be one standard deviation higher for people who don't or barely smoke.\niv_example_data <- \n  iv_example_data %>% \n  rowwise() %>% \n  mutate(province = case_when(smoker == 0 ~ sample(x = c(\"Nova Scotia\", \"Alberta\"),\n                                                                       size = 1, \n                                                                       replace = FALSE, \n                                                                       prob = c(1/2, 1/2)),\n                              smoker == 1 ~ sample(x = c(\"Nova Scotia\", \"Alberta\"),\n                                                                       size = 1, \n                                                                       replace = FALSE, \n                                                                       prob = c(1/4, 3/4)))) %>% \n  ungroup()\n\niv_example_data <- \n  iv_example_data %>% \n  mutate(tax = case_when(province == \"Alberta\" ~ 0.3,\n                         province == \"Nova Scotia\" ~ 0.5,\n                         TRUE ~ 9999999\n  )\n  )\n\niv_example_data$tax %>% table()\n#> .\n#>  0.3  0.5 \n#> 6206 3794\n\nhead(iv_example_data)\niv_example_data %>% \n  mutate(smoker = as_factor(smoker)) %>% \n  ggplot(aes(x = health, fill = smoker)) +\n  geom_histogram(position = \"dodge\", binwidth = 0.2) +\n  theme_minimal() +\n  labs(x = \"Health rating\",\n       y = \"Number of people\",\n       fill = \"Smoker\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  facet_wrap(vars(province))\nhealth_on_tax <- lm(health ~ tax, data = iv_example_data)\nsmoker_on_tax <- lm(smoker ~ tax, data = iv_example_data)\n\ncoef(health_on_tax)[\"tax\"] / coef(smoker_on_tax)[\"tax\"]\n#>        tax \n#> -0.8554502\nfirst_stage <- lm(smoker ~ tax, data = iv_example_data)\nhealth_hat <- first_stage$fitted.values\nsecond_stage <- lm(health ~ health_hat, data = iv_example_data)\n\nsummary(second_stage)\n#> \n#> Call:\n#> lm(formula = health ~ health_hat, data = iv_example_data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.9867 -0.7600  0.0068  0.7709  4.3293 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  0.91632    0.04479   20.46   <2e-16 ***\n#> health_hat  -0.85545    0.08911   -9.60   <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.112 on 9998 degrees of freedom\n#> Multiple R-squared:  0.009134,   Adjusted R-squared:  0.009034 \n#> F-statistic: 92.16 on 1 and 9998 DF,  p-value: < 2.2e-16"},{"path":"causality.html","id":"implementation-1","chapter":"16 Causality from observational data","heading":"16.9.4 Implementation","text":"regression discontinuity, although possible use existing functions, might worth looking specialised packages. Instrumental variables moving pieces, specialised package can help keep everything organised, additionally, standard errors need adjusted specialised packages make easier. package estimatr recommendation, although others available try interested. estimatr package team DeclareDesign.Let’s look example using iv_robust().","code":"\nlibrary(estimatr)\niv_robust(health ~ smoker | tax, data = iv_example_data) %>% \n  summary()\n#> \n#> Call:\n#> iv_robust(formula = health ~ smoker | tax, data = iv_example_data)\n#> \n#> Standard error type:  HC2 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value   Pr(>|t|) CI Lower\n#> (Intercept)   0.9163    0.04057   22.59 3.163e-110   0.8368\n#> smoker       -0.8555    0.08047  -10.63  2.981e-26  -1.0132\n#>             CI Upper   DF\n#> (Intercept)   0.9958 9998\n#> smoker       -0.6977 9998\n#> \n#> Multiple R-squared:  0.1971 ,    Adjusted R-squared:  0.197 \n#> F-statistic:   113 on 1 and 9998 DF,  p-value: < 2.2e-16"},{"path":"causality.html","id":"assumptions-1","chapter":"16 Causality from observational data","heading":"16.9.5 Assumptions","text":"discussed earlier, variety assumptions made using instrumental variables. two important :Exclusion Restriction. assumption instrumental variable affects dependent variable independent variable interest.Relevance. must actually relationship instrumental variable independent variable.typically trade-two. plenty variables thatWhen thinking potential instrumental variables Cunningham (2021), p. 211, puts brilliantly:, let’s say think good instrument. might defend someone else? necessary sufficient condition instrument can satisfy exclusion restriction people confused tell instrument’s relationship outcome. Let explain. one going confused tell think family size reduce female labor supply. don’t need Becker model convince women children probably work less fewer children. ’s common sense. , think told mothers whose first two children gender worked less whose children balanced sex ratio? probably give confused look. gender composition children whether woman works?doesn’t – matters, fact, people whose first two children gender decide third child. brings us back original point – people buy family size can cause women work less, ’re confused say women work less first two kids gender. point two children’s gender induces people larger families otherwise, person\n“gets ,” might excellent instrument.Relevance can tested using regression tests correlation. exclusion restriction tested. need present evidence convincing arguments. Cunningham (2021) p. 225 says ‘Instruments certain ridiculousness [.] , know good instrument instrument doesn’t seem relevant explaining outcome interest ’s exclusion restriction implies.’","code":""},{"path":"causality.html","id":"conclusion","chapter":"16 Causality from observational data","heading":"16.9.6 Conclusion","text":"Instrumental variables useful approach one can obtain causal estimates even without explicit randomisation. Finding instrumental variables used bit white whale, especially academia. However, leave final (hopefully motivating) word Taddy (2019), p. 162:final point importance IV models analysis, note inside firm—especially inside modern technology firm—explicitly randomised instruments everywhere…. often case decision-makers want understand effects policies randomised rather downstream things AB tested. example, suppose algorithm used predict creditworthiness potential borrowers assign loans. Even process loan assignment never randomised, parameters machine learning algorithms used score credit AB tested, experiments can used instruments loan assignment treatment. ‘upstream randomisation’ extremely common IV analysis key tool causal inference setting.’","code":""},{"path":"causality.html","id":"case-study---effect-of-police-on-crime","chapter":"16 Causality from observational data","heading":"16.10 Case study - Effect of Police on Crime","text":"","code":""},{"path":"causality.html","id":"overview-4","chapter":"16 Causality from observational data","heading":"16.10.1 Overview","text":"’ll use example Levitt (2002) looks effect police crime. interesting might think, police associated lower crime. , actually opposite, crime causes police hired - many police hypothetical country crime need? Hence need find sort instrumental variable affects crime relationship number police (, , related crime), yet also correlated police numbers. Levitt (2002) suggests number firefighters city.Levitt (2002) argues firefighters appropriate instrument, ‘(f)actors power public sector unions, citizen tastes government services, affirmative action initiatives, mayor’s desire provide spoils might expected jointly influence number firefighters police.’ Levitt (2002) also argues relevance assumption met showing ‘changes number police officers firefighters within city highly correlated time.’terms satisfying exclusion restriction, Levitt (2002) argues number firefighters ‘direct impact crime.’ However, may common factors, Levitt (2002) adjusts regression.","code":""},{"path":"causality.html","id":"data-2","chapter":"16 Causality from observational data","heading":"16.10.2 Data","text":"dataset based 122 US cities 1975 1995. Summary statistics provided Figure 16.16.\nFigure 16.16: Summary statistics Levitt 2002.\nSource: Levitt (2002) p. 1,246.","code":""},{"path":"causality.html","id":"model-2","chapter":"16 Causality from observational data","heading":"16.10.3 Model","text":"first stage Levitt (2002) looks police function firefighters, bunch adjustment variables:\n\\[\\ln(\\mbox{Police}_{ct}) = \\gamma \\ln(\\mbox{Fire}_{ct}) + X'_{ct}\\Gamma + \\lambda_t + \\phi_c + \\epsilon_{ct}.\\]\nimportant part police firefighters numbers per capita basis. bunch adjustment variables \\(X\\) includes things like state prisoners per capita, unemployment rate, etc, well year dummy variables fixed-effects city.established relationship police firefights, Levitt (2002) can use estimates number police, based number firefighters, explain crime rates:\n\\[\\Delta\\ln(\\mbox{Crime}_{ct}) = \\beta_1 \\ln(\\mbox{Police}_{ct-1}) + X'_{ct}\\Gamma + \\Theta_c + \\mu_{ct}.\\]typical way present instrumental variable results show stages. Figure 16.17 shows relationship police firefighters.\nFigure 16.17: relationship firefighters, police crime.\nSource: Levitt (2002) p. 1,247.Figure 16.18 shows relationship police crime, IV results ones interest.\nFigure 16.18: impact police crime.\nSource: Levitt (2002) p. 1,248.","code":""},{"path":"causality.html","id":"discussion-1","chapter":"16 Causality from observational data","heading":"16.10.4 Discussion","text":"key finding Levitt (2002) negative effect number police amount crime.variety points want raise regard paper. come across little negative, mostly just paper 2002, reading today, standards changed.’s fairly remarkable reliant various model specifications results . results bounce around fair bit ’s just ones reported. Chances bunch results reported, interest see impact.note, fairly limited model validation. probably something aware days, seems likely fair degree -fitting .Levitt (2002) actually response, another researcher, McCrary (2002), found issues original paper: Levitt (1997). Levitt appears quite decent , jarring see Levitt thanked McCrary (2002) providing ‘data computer code.’ Levitt decent providing data code? code unintelligible? ways nice see far come - author similar paper days forced make code data available part paper, wouldn’t ask . reinforces importance open data reproducible science.","code":""},{"path":"causality.html","id":"exercises-and-tutorial-15","chapter":"16 Causality from observational data","heading":"16.11 Exercises and tutorial","text":"","code":""},{"path":"causality.html","id":"exercises-15","chapter":"16 Causality from observational data","heading":"16.11.1 Exercises","text":"","code":""},{"path":"causality.html","id":"tutorial-15","chapter":"16 Causality from observational data","heading":"16.11.2 Tutorial","text":"","code":""},{"path":"mrp.html","id":"mrp","chapter":"17 Multilevel regression with post-stratification","heading":"17 Multilevel regression with post-stratification","text":"STATUS: construction.Required materialRead Analyzing name changes marriage using non-representative survey, (M. Alexander 2019c).Read Chapter 17 Regression Stories, (Gelman, Hill, Vehtari 2020).Read introduction multilevel regression post-stratification estimating constituency opinion, (Hanretty 2020).Read Introduction Estimating State Public Opinion Multi-Level Regression Poststratification using R, (Kastellec, Lax, Phillips 2016).Read Using sex gender survey adjustment, (Kennedy et al. 2020).Read MRP rstanarm, (Kennedy Gabry 2020).Read Know population know model: Using model-based regression poststratification generalize findings beyond observed sample, (Kennedy Gelman 2020).Read Forecasting elections non-representative polls, (W. Wang et al. 2015).Read Chapter 17 Sampling Theory Practice, (Wu Thompson 2020).Watch Statistical Models Election Outcomes, (Gelman 2020).Listen Episode 248: Democrats irrational? (David Shor), (Galef 2020).Recommended readingArnold, Jeffrey B., 2018, ‘Simon Jackman’s Bayesian Model Examples Stan,’ Ch 13, 7 May, https://jrnold.github.io/bugs-examples--stan/campaign.html.Cohn, Nate, 2016, ‘Gave Four Good Pollsters Raw Data. Four Different Results,’ New York Times, Upshot, 20 September, https://www.nytimes.com/interactive/2016/09/20/upshot/-error--polling-world-rarely-talks-.html.Edelman, M., Vittert, L., & Meng, X.-L., 2021, ‘Interview Murray Edelman History Exit Poll,’ Harvard Data Science Review, https://doi.org/10.1162/99608f92.3a25cd24 https://hdsr.mitpress.mit.edu/pub/fekmqbv4/release/2.Gelman, Andrew, Julia Azari, 2017, ‘19 things learned 2016 election,’ Statistics Public Policy, 4 (1), pp. 1-10.Gelman, Andrew, Jessica Hullman, Christopher Wlezien, 2020, ‘Information, incentives, goals election forecasts,’ 8 September, available : http://www.stat.columbia.edu/~gelman/research/unpublished/forecast_incentives3.pdfGelman, Andrew, Merlin Heidemanns, Elliott Morris, 2020, ‘2020 US POTUS model,’ Economist, freely available: https://github.com/TheEconomist/us-potus-model.Ghitza, Yair, Andrew Gelman, 2013, ‘Deep Interactions MRP: Election Turnout Voting Patterns Among Small Electoral Subgroups,’ American Journal Political Science, 57 (3), pp. 762-776.Ghitza, Yair, Andrew Gelman, 2020, ‘Voter Registration Databases MRP: Toward Use Large-Scale Databases Public Opinion Research,’ Political Analysis, pp. 1-25.Imai, Kosuke, 2017, Quantitative Social Science: Introduction, Princeton University Press, Ch 4.1, 5.3.Jackman, Simon, 2005, ‘Pooling polls election campaign,’ Australian Journal Political Science, 40 (4), pp. 499-517.Jackman, Simon, Shaun Ratcliff Luke Mansillo, 2019, ‘Small area estimates public opinion: Model-assisted post-stratification data voter advice applications,’ 4 January, https://www.cambridge.org/core/membership/services/aop-file-manager/file/5c2f6ebb7cf9ee1118d11c0a/APMM-2019-Simon-Jackman.pdf.Lauderdale, Ben, Delia Bailey, Jack Blumenau, Doug Rivers, 2020, ‘Model-based pre-election polling national sub-national outcomes US UK,’ International Journal Forecasting, 36 (2), pp. 399-413.Leigh, Andrew, Justin Wolfers, 2006, ‘Competing approaches forecasting elections: Economic models, opinion polling prediction markets,’ Economic Record, 82 (258), pp.325-340.Nickerson, David W., Todd Rogers, 2014, ‘Political campaigns big data,’ Journal Economic Perspectives, 28 (2), pp. 51-74.Shirani-Mehr, Houshmand, David Rothschild, Sharad Goel, Andrew Gelman, 2018, ‘Disentangling bias variance election polls,’ Journal American Statistical Association, 113 (522), pp. 607-614.Recommended viewingJackman, Simon, 2020, ‘triumph quants?: Model-based poll aggregation election forecasting,’ Ihaka Lecture Series, https://youtu./MvGYsKIsLFs.Key librariesbrmsbroomgtsummaryhavenlabelledlme4modelsummaryrstanarmtidybayestidyverse","code":""},{"path":"mrp.html","id":"introduction-18","chapter":"17 Multilevel regression with post-stratification","heading":"17.1 Introduction","text":"[Presidential election ] 2016 largest analytics failure US political history.David Shor, 13 August 2020Multilevel regression post-stratification (MRP) popular way adjust non-representative samples better analyse opinion survey responses.2 uses regression model relate individual-level survey responses various characteristics rebuilds sample better match population. way MRP can allow better understanding responses, also allow us analyse data may otherwise unusable. However, can challenge get started MRP terminology may unfamiliar, data requirements can onerous.Let’s say biased survey. Maybe conducted survey computers academic seminar, folks post-graduate degrees likely -represented. nonetheless interested making claims population. Let’s say found 37.5 per cent respondents prefer Macs. One way forward just ignore bias say ‘37.5 per cent people prefer Macs.’ Another way say, well 50 per cent respondents post-graduate degree prefer Macs, without post-graduate degree, 25 per cent prefer Macs. knew proportion broader population post-graduate degree, let’s assume 10 per cent, conduct re-weighting, post-stratification, follows: \\(0.5 * 0.1 + 0.25 * 0.9 = 0.275\\), estimate 27.5 per cent people prefer Macs. MRP third approach, uses model help re-weighting. use logistic regression estimate relationship preferring Macs highest educational attainment survey. apply relationship population dataset.MRP handy approach dealing survey data. Hanretty (2020) puts well says ‘MRP used alternatives either poor expensive.’ Essentially, trains model based survey, applies trained model another dataset. two main, related, advantages:can allow us ‘re-weight’ way includes uncertainty front--mind isn’t hamstrung small samples. alternative way deal small sample either go gather data throw away.can allow us use broad surveys speak subsets. Hanretty (2020) says ‘poor alternative [using MRP] simply splitting large sample (much) smaller geographic subsamples. poor alternative guarantee sample representative national level representative broken smaller groups.’practical perspective, tends less expensive collect non-probability samples benefits able use types data. said, magic-bullet laws statistics still apply. larger uncertainty around estimates still subject usual biases. Lauren Kennedy points , ‘MRP traditionally used probability surveys potential non-probability surveys, ’re sure limitations moment.’ ’s exciting area research academia industry.workflow need MRP straight-forward, details tiny decisions made step can become overwhelming. point need keep mind trying create relationship two datasets using statistical model, need establish similarity two datasets terms variables levels. steps :gather prepare survey dataset, thinking needed coherence post-stratification dataset;gather prepare post-stratification dataset thinking needed coherence survey dataset;model variable interest survey using independent variables levels available survey post-stratification datasets;apply model post-stratification data.notes, begin simulating situation pretend know features population. move famous example MRP used survey data Xbox platform exit poll data forecast 2012 US election. move examples Australian political situation. discuss features aware conducting MRP.","code":""},{"path":"mrp.html","id":"simulation---toddler-bedtimes","chapter":"17 Multilevel regression with post-stratification","heading":"17.2 Simulation - Toddler bedtimes","text":"","code":""},{"path":"mrp.html","id":"construct-a-population","chapter":"17 Multilevel regression with post-stratification","heading":"17.2.1 Construct a population","text":"get started simulate data population various properties, take biased sample, conduct MRP demonstrate can get properties back. going two ‘explanatory variables’ - age-group toilet-trained - one dependent variable - bedtime. Bed-time increase age-group increases, later children toilet-trained, compared . clear, example ‘know’ ‘true’ features population, isn’t something occurs use real data - just help understand MRP . ’re going rely heavily tidyverse package (Wickham et al. 2019a).point, Figure 17.1 provides invaluable advice (thank Mahfouz).\nFigure 17.1: Bernie ask us ?\n, always, dataset, first try plot better understand going (million points, ’ll just grab first 1,000 plots nicely).can also work ‘truth’ information interested (remembering ’d never actually know move away simulated examples).","code":"\n# Uncomment this (by deleting the #) if you need to install the packages\n# install.packages('tidyverse')\nlibrary(tidyverse)\n\n# This helps reproducibility\n# It makes it more likely that you're able to get the same random numbers as in this example.\nset.seed(853)\n\n# One million people in our population.\nsize_of_population <- 1000000\n\npopulation_for_mrp_example <- \n  tibble(age_group = sample(x = c(1:3), # Draw from any of 1, 2, 3.\n                            size = size_of_population,\n                            replace = TRUE # After you draw a number, allow that number to be drawn again.\n                            ),\n         toilet_trained = sample(x = c(0, 1),\n                                 size = size_of_population,\n                                 replace = TRUE\n                                 ),\n         noise = rnorm(size_of_population, mean = 0, sd = 1), \n         bed_time = 5 + 0.5 * age_group + 1 * toilet_trained + noise, # Make bedtime a linear function of the variables that we just generated and a intercept (no special reason for that intercept to be five; it could be anything).\n         ) %>% \n  select(-noise) %>% \n  mutate(age_group = as_factor(age_group),\n         toilet_trained = as_factor(toilet_trained)\n         )\n\npopulation_for_mrp_example %>% \n  head()\n#> # A tibble: 6 × 3\n#>   age_group toilet_trained bed_time\n#>   <fct>     <fct>             <dbl>\n#> 1 1         0                  5.74\n#> 2 2         1                  6.48\n#> 3 1         0                  6.53\n#> 4 1         1                  5.39\n#> 5 1         1                  8.40\n#> 6 3         0                  6.54\npopulation_for_mrp_example %>% \n  slice(1:1000) %>% \n  ggplot(aes(x = age_group, y = bed_time)) +\n  geom_jitter(aes(color = toilet_trained), \n              alpha = 0.4, \n              width = 0.1, \n              height = 0) +\n  labs(x = \"Age-group\",\n       y = \"Bed time\",\n       color = \"Toilet trained\") +\n  theme_classic() +\n  scale_color_brewer(palette = \"Set1\")\npopulation_for_mrp_example_summarised <- \n  population_for_mrp_example %>% \n  group_by(age_group, toilet_trained) %>% \n  summarise(median_bed_time = median(bed_time)) \n\npopulation_for_mrp_example_summarised %>% \n  knitr::kable(digits = 2,\n               col.names = c(\"Age-group\", \"Is toilet trained\", \"Average bed time\"))"},{"path":"mrp.html","id":"get-a-biased-sample-from-it","chapter":"17 Multilevel regression with post-stratification","heading":"17.2.2 Get a biased sample from it","text":"Now want pretend survey biased sample. ’ll allow -samples children younger toilet-trained. instance, perhaps gathered sample based records paediatrician, ’s likely see biased sample children. interested knowing proportion children toilet-trained various age-groups.can plot also.’s pretty clear sample different bedtime overall population, let’s just exercise look median, age toilet-trained status.","code":"\n# Thanks to Monica Alexander\nset.seed(853)\n\n# Add a weight for each 'type' (has to sum to one)\npopulation_for_mrp_example <- \n  population_for_mrp_example %>% \n  mutate(weight = \n           case_when(toilet_trained == 0 & age_group == 1 ~ 0.7,\n                     toilet_trained == 0 ~ 0.1,\n                     age_group %in% c(1, 2, 3) ~ 0.2\n                     ),\n         id = 1:n()\n         )\n\nget_these <- \n  sample(\n    x = population_for_mrp_example$id,\n    size = 1000,\n    prob = population_for_mrp_example$weight\n    )\n\nsample_for_mrp_example <- \n  population_for_mrp_example %>% \n  filter(id %in% get_these) %>% \n  select(-weight, -id)\n\n# Clean up\npoststratification_dataset <- \n  population_for_mrp_example %>% \n  select(-weight, -id)\nsample_for_mrp_example %>% \n  mutate(toilet_trained = as_factor(toilet_trained)) %>% \n  ggplot(aes(x = age_group, y = bed_time)) +\n  geom_jitter(aes(color = toilet_trained), alpha = 0.4, width = 0.1, height = 0) +\n  labs(x = \"Age-group\",\n       y = \"Bed time\",\n       color = \"Toilet trained\") +\n  theme_classic() +\n  scale_color_brewer(palette = \"Set1\")\nsample_for_mrp_example_summarized <- \n  sample_for_mrp_example %>% \n  group_by(age_group, toilet_trained) %>% \n  summarise(median_bed_time = median(bed_time))\n\nsample_for_mrp_example_summarized %>% \n  knitr::kable(digits = 2,\n               col.names = c(\"Age-group\", \"Is toilet trained\", \"Average bed time\"))"},{"path":"mrp.html","id":"model-the-sample","chapter":"17 Multilevel regression with post-stratification","heading":"17.2.3 Model the sample","text":"quickly train model based (biased) survey. ’ll use modelsummary (Arel-Bundock 2021a) format estimates.‘multilevel regression’ part MRP (although isn’t really multilevel model just keep things simple now).","code":"\nlibrary(modelsummary)\n\nmrp_example_model <- \n  sample_for_mrp_example %>% \n  lm(bed_time ~ age_group + toilet_trained, data = .)\n\nmrp_example_model %>% \n  modelsummary::modelsummary(fmt = 2)"},{"path":"mrp.html","id":"get-a-post-stratification-dataset","chapter":"17 Multilevel regression with post-stratification","heading":"17.2.4 Get a post-stratification dataset","text":"Now use post-stratification dataset get estimates number cell. typically use larger dataset may closely reflection population. US popular choice ACS, countries typically use census.simulation example, ’ll just take 10 per cent sample population use post-stratification dataset.ideal world individual-level data post-stratification dataset (’s case ). world can apply model individual. likely situation, reality, just counts groups, ’re going try construct estimate group.","code":"\nset.seed(853)\n\npoststratification_dataset <- \n  population_for_mrp_example %>% \n  slice(1:100000) %>% \n  select(-bed_time)\n\npoststratification_dataset %>% \n  head()\n#> # A tibble: 6 × 4\n#>   age_group toilet_trained weight    id\n#>   <fct>     <fct>           <dbl> <int>\n#> 1 1         0                 0.7     1\n#> 2 2         1                 0.2     2\n#> 3 1         0                 0.7     3\n#> 4 1         1                 0.2     4\n#> 5 1         1                 0.2     5\n#> 6 3         0                 0.1     6\npoststratification_dataset_grouped <- \n  poststratification_dataset %>% \n  group_by(age_group, toilet_trained) %>% \n  count()\n\npoststratification_dataset_grouped %>% \n  head()\n#> # A tibble: 6 × 3\n#> # Groups:   age_group, toilet_trained [6]\n#>   age_group toilet_trained     n\n#>   <fct>     <fct>          <int>\n#> 1 1         0              16766\n#> 2 1         1              16649\n#> 3 2         0              16801\n#> 4 2         1              16617\n#> 5 3         0              16625\n#> 6 3         1              16542"},{"path":"mrp.html","id":"post-stratify-our-model-estimates","chapter":"17 Multilevel regression with post-stratification","heading":"17.2.5 Post-stratify our model estimates","text":"Now create estimate group, add confidence intervals.point can look MRP estimates (circles) along confidence intervals, compare raw estimates data (squares). case, know truth, can also compare known truth (triangles) (’s something can normally).","code":"\npoststratification_dataset_grouped <- \n  mrp_example_model %>% \n  predict(newdata = poststratification_dataset_grouped, interval = \"confidence\") %>% \n  as_tibble() %>% \n  cbind(poststratification_dataset_grouped, .) # The dot modifies the behaviour of the pipe; it pipes to the dot instead of the first argument as normal.\npoststratification_dataset_grouped %>% \n  ggplot(aes(x = age_group, y = fit)) +\n  geom_point(data = population_for_mrp_example_summarised,\n             aes(x = age_group, y = median_bed_time, color = toilet_trained), \n             shape = 17) +\n  geom_point(data = sample_for_mrp_example_summarized,\n             aes(x = age_group, y = median_bed_time, color = toilet_trained), \n             shape = 15) +\n  geom_pointrange(aes(ymin=lwr, ymax=upr, color = toilet_trained)) +\n  labs(x = \"Age-group\",\n       y = \"Bed time\",\n       color = \"Toilet trained\") +\n  theme_classic() +\n  scale_color_brewer(palette = \"Set1\")"},{"path":"mrp.html","id":"case-study---xbox-paper","chapter":"17 Multilevel regression with post-stratification","heading":"17.3 Case study - Xbox paper","text":"","code":""},{"path":"mrp.html","id":"overview-5","chapter":"17 Multilevel regression with post-stratification","heading":"17.3.1 Overview","text":"One famous MRP example W. Wang et al. (2015). used data Xbox gaming platform forecast 2012 US Presidential Election.Key facts set-:Data opt-poll available Xbox gaming platform 45 days leading 2012 US presidential election (Obama Romney).day three five questions, including voter intention: ‘election held today, vote ?’Respondents allowed answer per day.First-time respondents asked provide information , including sex, race, age, education, state, party ID, political ideology, voted 2008 presidential election.total, 750,148 interviews conducted, 345,858 unique respondents - 30,000 completed five polls.Young men dominate Xbox population: 18--29-year-olds comprise 65 per cent Xbox dataset, compared 19 per cent exit poll; men make 93 per cent Xbox sample 47 per cent electorate.","code":""},{"path":"mrp.html","id":"model-3","chapter":"17 Multilevel regression with post-stratification","heading":"17.3.2 Model","text":"Given structure US electorate, use two-stage modelling approach. details don’t really matter much, essentially model likely respondent vote Obama, given various information state, education, sex, etc:\\[\nPr\\left(Y_i = \\mbox{Obama} | Y_i\\\\{\\mbox{Obama, Romney}\\}\\right) = \\mbox{logit}^{-1}(\\alpha_0 + \\alpha_1(\\mbox{state last vote share}) + \\alpha_{j[]}^{\\mbox{state}} + \\alpha_{j[]}^{\\mbox{edu}} + \\alpha_{j[]}^{\\mbox{sex}} + ...)\n\\]run R using glmer() ‘lme4’ (Bates et al. 2015).","code":""},{"path":"mrp.html","id":"post-stratify","chapter":"17 Multilevel regression with post-stratification","heading":"17.3.3 Post-stratify","text":"trained model considers effect various independent variables support candidates, now post-stratify, ‘cell-level estimates weighted proportion electorate cell aggregated appropriate level (.e., state national).’means need cross-tabulated population data. general, census worked, one large surveys available US, difficulty variables need available cross-tab basis. , use exit polls (viable option countries).make state-specific estimates post-stratifying features state (Figure 17.2).\nFigure 17.2: Post-stratified estimates state based Xbox survey MRP\nSimilarly, can examine demographic-differences (Figure 17.3).\nFigure 17.3: Post-stratified estimates demographic basis based Xbox survey MRP\nFinally, convert estimates electoral college estimates (Figure 17.4).\nFigure 17.4: Post-stratified estimates electoral college outcomes based Xbox survey MRP\n","code":""},{"path":"mrp.html","id":"simulation---australian-voting","chapter":"17 Multilevel regression with post-stratification","heading":"17.4 Simulation - Australian voting","text":"","code":""},{"path":"mrp.html","id":"overview-6","chapter":"17 Multilevel regression with post-stratification","heading":"17.4.1 Overview","text":"reminder, workflow use :read poll;model poll;read post-stratification data; andapply model post-stratification data.earlier example, didn’t really much modelling step, despite name ‘multilevel modelling post-stratification,’ didn’t actually use multilevel model. ’s nothing says use multilevel model, lot situations circumstances ’s likely worse. clear, means although individual-level data, grouping individuals ’ll take advantage . instance, case trying model elections, usually districts/divisions/electorates/ridings/etc exist within provinces/states likely make sense , least, include coefficient adjusts intercept province.section ’re simulate another dataset fit different models . ’re going draw Australian elections set-. Australia parliamentary system, 151 seats parliament, one electorate. electorates grouped within six states two territories. two major parties - Australian Labor Party (ALP) Liberal Party (LP). Somewhat confusingly, Liberal party actually conservative, right-wing party, Labor party progressive, left-wing, party.","code":""},{"path":"mrp.html","id":"construct-a-survey","chapter":"17 Multilevel regression with post-stratification","heading":"17.4.2 Construct a survey","text":"move us slightly closer reality, going simulate survey (rather sample population earlier) post-stratify using real data. dependent variable ‘supports_ALP,’ binary variable - either 0 1. ’ll just start three independent variables :‘gender,’ either ‘female’ ‘male’ (available Australian Bureau Statistics);‘age_group,’ one four groups: ‘ages 18 29,’ ‘ages 30 44,’ ‘ages 45 59,’ ‘ages 60 plus’;‘state,’ one eight integers: 1 - 8 (inclusive).point, ’s worth briefly discussing role sex gender survey research, following Kennedy et al. (2020). Sex based biological attributes, gender socially constructed. likely interested effect gender dependent variable. Moving away non-binary concept gender, terms official statistics, something happened recently. researcher one problems insisting binary , Kennedy et al. (2020, 2) say ‘…measuring gender simply two categories, failure capture unique experiences identify either male female, whose gender align sex classification.’ researcher variety ways proceeding, Kennedy et al. (2020) discuss based : ethics, accuracy, practicality, flexibility. However, ‘single good solution can applied situations. Instead, important recognize compromise ethical concerns, statistical concerns, appropriate decision reflective ’ [p. 16]. important consideration ensure appropriate ‘respect consideration survey respondent.’Finally, want survey -sample females, ’ll just get rid 300 males.","code":"\nlibrary(tidyverse)\nset.seed(853)\n\nsize_of_sample_for_australian_polling <- 2000\n\nsample_for_australian_polling <- \n  tibble(age_group = \n           sample(x = c(0:3), \n                  size = size_of_sample_for_australian_polling,\n                  replace = TRUE\n                  ),\n         gender = \n           sample(x = c(0:1),\n                  size = size_of_sample_for_australian_polling,\n                  replace = TRUE\n                  ),\n         state = \n           sample(x = c(1:8),\n                  size = size_of_sample_for_australian_polling,\n                  replace = TRUE\n                  ),\n         noise = rnorm(size_of_sample_for_australian_polling, mean = 0, sd = 1), \n         support_alp = 1 + 0.5 * age_group + 0.5 * gender + 0.01 * state + noise\n         ) \n\n# Normalize the outcome variable\nsample_for_australian_polling <- \n  sample_for_australian_polling %>% \n  mutate(support_alp = \n           if_else(support_alp > median(support_alp, na.rm = TRUE), \n                   'Supports ALP', \n                   'Does not')\n         )\n\n# Clean up the simulated data\nsample_for_australian_polling <- \n  sample_for_australian_polling %>% \n  mutate(\n    age_group = case_when(\n      age_group == 0 ~ 'Ages 18 to 29',\n      age_group == 1 ~ 'Ages 30 to 44',\n      age_group == 2 ~ 'Ages 45 to 59',\n      age_group == 3 ~ 'Ages 60 plus',\n      TRUE ~ 'Problem'\n      ),\n    gender = case_when(\n      gender == 0 ~ 'Male',\n      gender == 1 ~ 'Female',\n      TRUE ~ 'Problem'\n      ),\n    state = case_when(\n      state == 1 ~ 'Queensland',\n      state == 2 ~ 'New South Wales',\n      state == 3 ~ 'Australian Capital Territory',\n      state == 4 ~ 'Victoria',\n      state == 5 ~ 'Tasmania',\n      state == 6 ~ 'Northern Territory',\n      state == 7 ~ 'South Australia',\n      state == 8 ~ 'Western Australia',\n      TRUE ~ 'Problem'\n      ),\n    \n    ) %>% \n  select(-noise)\n\n# Tidy the class\nsample_for_australian_polling <- \n  sample_for_australian_polling %>% \n  mutate(across(c(age_group, gender, state, support_alp), as_factor))\n\nsample_for_australian_polling %>%   \n  head()\n#> # A tibble: 6 × 4\n#>   age_group     gender state           support_alp \n#>   <fct>         <fct>  <fct>           <fct>       \n#> 1 Ages 18 to 29 Female South Australia Supports ALP\n#> 2 Ages 60 plus  Male   South Australia Supports ALP\n#> 3 Ages 30 to 44 Male   Victoria        Does not    \n#> 4 Ages 18 to 29 Male   Tasmania        Does not    \n#> 5 Ages 18 to 29 Female Victoria        Does not    \n#> 6 Ages 18 to 29 Male   Queensland      Supports ALP\nsample_for_australian_polling <- \n  sample_for_australian_polling %>% \n  arrange(gender) %>% \n  slice(1:1700)"},{"path":"mrp.html","id":"model-the-survey","chapter":"17 Multilevel regression with post-stratification","heading":"17.4.3 Model the survey","text":"polling data generated make males older people less likely vote ALP; females younger people likely vote Labor Party. Females -sampled. , ALP skew dataset. ’re going use gtsummary package quickly make summary table (Sjoberg et al. 2021).\n          1\n          \n           \n          n (%)\n          Now ’d like see can get results back (find females less likely males vote Australian Labor Party people less likely vote Australian Labor Party get older). model :ADD MODEL.model says probability person, \\(j\\), vote Australian Labor Party depends gender age-group. Based simulated data, like older age-groups less likely vote Australian Labor Party males less likely vote Australian Labor Party.Essentially ’ve got inputs back. dependent variable binary, used logistic regression results little difficult interpret.","code":"\nlibrary(gtsummary)\n\nsample_for_australian_polling %>% \n  gtsummary::tbl_summary()\nalp_support <- \n  glm(support_alp ~ gender + age_group + state, \n      data = sample_for_australian_polling,\n      family = \"binomial\"\n      )\n\nalp_support %>% \n  modelsummary::modelsummary(fmt = 2, exponentiate = TRUE)"},{"path":"mrp.html","id":"post-stratify-1","chapter":"17 Multilevel regression with post-stratification","heading":"17.4.4 Post-stratify","text":"Now ’d like see can use found poll get estimate state based demographic features.First read real demographic data, state basis, ABS.point, ’ve got decision make need variables survey post-stratification dataset, state abbreviations used, survey, full names used. ’ll change post-stratification dataset survey data already modelled.’re just going rough forecasts. gender age-group want relevant coefficient example data can construct estimates.now post-stratified estimates state model fair weaknesses. instance, small cell counts going problematic. approach ignores uncertainty, now something working can complicate .","code":"\npost_strat_census_data <- \n  read_csv(\"outputs/data/census_data.csv\")\n\nhead(post_strat_census_data)\n#> # A tibble: 6 × 5\n#>   state gender age_group  number cell_prop_of_division_total\n#>   <chr> <chr>  <chr>       <dbl>                       <dbl>\n#> 1 ACT   Female ages18to29  34683                       0.125\n#> 2 ACT   Female ages30to44  42980                       0.155\n#> 3 ACT   Female ages45to59  33769                       0.122\n#> 4 ACT   Female ages60plus  30322                       0.109\n#> 5 ACT   Male   ages18to29  34163                       0.123\n#> 6 ACT   Male   ages30to44  41288                       0.149\npost_strat_census_data <- \n  post_strat_census_data %>% \n  mutate(\n    state = \n      case_when(\n        state == 'ACT' ~ 'Australian Capital Territory',\n        state == 'NSW' ~ 'New South Wales',\n        state == 'NT' ~ 'Northern Territory',\n        state == 'QLD' ~ 'Queensland',\n        state == 'SA' ~ 'South Australia',\n        state == 'TAS' ~ 'Tasmania',\n        state == 'VIC' ~ 'Victoria',\n        state == 'WA' ~ 'Western Australia',\n        TRUE ~ \"Problem\"\n      ),\n    age_group = \n      case_when(\n        age_group == 'ages18to29' ~ 'Ages 18 to 29',\n        age_group == 'ages30to44' ~ 'Ages 30 to 44',\n        age_group == 'ages45to59' ~ 'Ages 45 to 59',\n        age_group == 'ages60plus' ~ 'Ages 60 plus',\n        TRUE ~ \"Problem\"\n      )\n  )\npost_strat_census_data <- \n  alp_support %>% \n  predict(newdata = post_strat_census_data, type = 'response', se.fit = TRUE) %>% \n  as_tibble() %>% \n  cbind(post_strat_census_data, .)\n\npost_strat_census_data %>% \n  mutate(alp_predict_prop = fit*cell_prop_of_division_total) %>% \n  group_by(state) %>% \n  summarise(alp_predict = sum(alp_predict_prop))\n#> # A tibble: 8 × 2\n#>   state                        alp_predict\n#>   <chr>                              <dbl>\n#> 1 Australian Capital Territory       0.551\n#> 2 New South Wales                    0.487\n#> 3 Northern Territory                 0.546\n#> 4 Queensland                         0.491\n#> 5 South Australia                    0.403\n#> 6 Tasmania                           0.429\n#> 7 Victoria                           0.521\n#> 8 Western Australia                  0.460"},{"path":"mrp.html","id":"improving-the-model","chapter":"17 Multilevel regression with post-stratification","heading":"17.4.5 Improving the model","text":"’d like address major issues approach, specifically able deal small cell counts, also taking better account uncertainty. dealing survey data, prediction intervals something similar critical, ’s appropriate report central estimates. ’ll use broad approach , just improve model. ’re going change Bayesian model use rstanarm package (Goodrich et al. 2020).Now, using basic model , Bayesian setting., ’d like estimate state based demographic features. ’re just going rough forecasts. gender age-group want relevant coefficient example data can construct estimates (code Monica Alexander). ’re going use tidybayes (Kay 2020).now post-stratified estimates division. new Bayesian approach enable us think deeply uncertainty. complicate variety ways including adding coefficients (remember ’d need get new cell counts), adding layers.One interesting aspect multilevel approach allow us deal small cell counts borrowing information cells. Even remove , say, 18--29-year-old, male respondents Tasmania model still provide estimates. pooling, effect young, male, Tasmanians partially determined cells respondents.many interesting aspects may like communicate others. instance, may like show model affecting results. can make graph compares raw estimate model estimate.Similarly, may like plot distribution coefficients.3","code":"\nlibrary(rstanarm)\n\nimproved_alp_support <- \n  rstanarm::stan_glm(support_alp ~ gender + age_group + state,\n                     data = sample_for_australian_polling,\n                     family = binomial(link = \"logit\"),\n                     prior = normal(0, 1), \n                     prior_intercept = normal(0, 1),\n                     cores = 2, \n                     seed = 12345)\nlibrary(tidybayes)\n\npost_stratified_estimates <- \n  improved_alp_support %>% \n  tidybayes::add_fitted_draws(newdata = post_strat_census_data) %>% \n  rename(alp_predict = .value) %>% \n  mutate(alp_predict_prop = alp_predict*cell_prop_of_division_total) %>% \n  group_by(state, .draw) %>% \n  summarise(alp_predict = sum(alp_predict_prop)) %>% \n  group_by(state) %>% \n  summarise(mean = mean(alp_predict), \n            lower = quantile(alp_predict, 0.025), \n            upper = quantile(alp_predict, 0.975))\n\npost_stratified_estimates\n#> # A tibble: 8 × 4\n#>   state                         mean lower upper\n#>   <chr>                        <dbl> <dbl> <dbl>\n#> 1 Australian Capital Territory 0.550 0.494 0.604\n#> 2 New South Wales              0.486 0.429 0.544\n#> 3 Northern Territory           0.544 0.483 0.607\n#> 4 Queensland                   0.491 0.432 0.548\n#> 5 South Australia              0.412 0.361 0.464\n#> 6 Tasmania                     0.429 0.372 0.487\n#> 7 Victoria                     0.519 0.453 0.583\n#> 8 Western Australia            0.460 0.401 0.520\npost_stratified_estimates %>% \n  ggplot(aes(y = mean, x = forcats::fct_inorder(state), color = \"MRP estimate\")) + \n  geom_point() +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0) + \n  labs(y = \"Proportion ALP support\",\n       x = \"State\") + \n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\") +\n  theme(legend.title = element_blank()) +\n  coord_flip()\n# tidybayes::get_variables(improved_alp_support)\n# improved_alp_support %>%\n#   gather_draws(genderMale) %>%\n#   ungroup() %>%\n#   # mutate(coefficient = stringr::str_replace_all(.variable, c(\"b_\" = \"\"))) %>%\n#   mutate(coefficient = forcats::fct_recode(coefficient,\n#                                            Intercept = \"Intercept\",\n#                                            `Is male` = \"genderMale\",\n#                                            `Age 30-44` = \"age_groupages30to44\",\n#                                            `Age 45-59` = \"age_groupages45to59\",\n#                                            `Age 60+` = \"age_groupages60plus\"\n#                                            )) %>% \n# \n# # both %>% \n#   ggplot(aes(y=fct_rev(coefficient), x = .value)) + \n#   ggridges::geom_density_ridges2(aes(height = ..density..),\n#                                  rel_min_height = 0.01, \n#                                  stat = \"density\",\n#                                  scale=1.5) +\n#   xlab(\"Distribution of estimate\") +\n#   ylab(\"Coefficient\") +\n#   scale_fill_brewer(name = \"Dataset: \", palette = \"Set1\") +\n#   theme_minimal() +\n#   theme(panel.grid.major = element_blank(),\n#         panel.grid.minor = element_blank()) +\n#   theme(legend.position = \"bottom\")"},{"path":"mrp.html","id":"forecasting-the-2020-us-election","chapter":"17 Multilevel regression with post-stratification","heading":"17.5 Forecasting the 2020 US election","text":"US election lot features unique US, model going build going fairly generic , largely generalization earlier model Australian election. One good thing forecasting US election lot data around. case can use survey data Democracy Fund Voter Study Group.4 conducted polling lead-US election make publicly available registration. use Integrated Public Use Microdata Series (IPUMS), access 2018 American Community Survey (ACS) post-stratification dataset. use state, age-group, gender, education explanatory variables.","code":""},{"path":"mrp.html","id":"survey-data","chapter":"17 Multilevel regression with post-stratification","heading":"17.5.1 Survey data","text":"first step need actually get survey data. Go website:https://www.voterstudygroup.org ’re looking ‘Nationscape’ button along lines ‘Get latest Nationscape data.’ get dataset, need fill form, process email . real person side form, request take days.get access ’ll want download .dta files. Nationscape conducted many surveys many files. filename reference date, ‘ns20200625’ refers 25 June 2020, one ’ll use . (data mine share, ’ll refer)can read ‘.dta’ files using haven package (Wickham Miller 2020). ’ve based code written Alen Mitrovski, Xiaoyan Yang, Matthew Wankiewicz, available : https://github.com/matthewwankiewicz/US_election_forecast.’ve seen, one difficult aspects MRP ensuring consistency datasets. case, need work make variables consistent.","code":"\nlibrary(haven)\nlibrary(tidyverse)\n\nraw_nationscape_data <- \n  read_dta(here::here(\"dont_push/ns20200625.dta\"))\n\n# The Stata format separates labels so reunite those\nraw_nationscape_data <- \n  labelled::to_factor(raw_nationscape_data)\n\n# Just keep relevant variables\nnationscape_data <- \n  raw_nationscape_data %>% \n  select(vote_2020,\n         gender,\n         education,\n         state,\n         age)\n\n# For simplicity, remove anyone undecided or planning to vote for someone other than Biden/Trump and make vote a binary variable: 1 for Biden, 0 for Trump.\nnationscape_data <- \n  nationscape_data %>% \n  filter(vote_2020 == \"Joe Biden\" | vote_2020 == \"Donald Trump\") %>% \n  mutate(vote_biden = if_else(vote_2020 == \"Joe Biden\", 1, 0)) %>% \n  select(-vote_2020)\n\n# Create the dependent variables by grouping the existing variables\nnationscape_data <- \n  nationscape_data %>% \n  mutate(\n    age_group = case_when( # case_when works in order and exits when there's a match\n      age <= 29 ~ 'age_18-29',\n      age <= 44 ~ 'age_30-44',\n      age <= 59 ~ 'age_45-59',\n      age >= 60 ~ 'age_60_or_more',\n      TRUE ~ 'Trouble'\n      ),\n    gender = case_when(\n      gender == \"Female\" ~ 'female',\n      gender == \"Male\" ~ 'male',\n      TRUE ~ 'Trouble'\n      ),\n    education_level = case_when(\n      education == \"3rd Grade or less\" ~ \"High school or less\",\n      education == \"Middle School - Grades 4 - 8\" ~ \"High school or less\",\n      education == \"Completed some high school\" ~ \"High school or less\",\n      education == \"High school graduate\" ~ \"High school or less\",\n      education == \"Other post high school vocational training\" ~ \"Some post secondary\",\n      education == \"Completed some college, but no degree\" ~ \"Some post secondary\",\n      education == \"Associate Degree\" ~ \"Post secondary or higher\",\n      education == \"College Degree (such as B.A., B.S.)\" ~ \"Post secondary or higher\",\n      education == \"Completed some graduate, but no degree\" ~ \"Post secondary or higher\",\n      education == \"Masters degree\" ~ \"Graduate degree\",\n      education == \"Doctorate degree\" ~ \"Graduate degree\",\n      TRUE ~ 'Trouble'\n      )\n    ) %>% \n  select(-education, -age)\n\ntests <- \n  nationscape_data %>% \n  mutate(test = stringr::str_detect(age_group, 'Trouble'),\n         test = if_else(test == TRUE, TRUE, \n                        stringr::str_detect(education_level, 'Trouble')),\n         test = if_else(test == TRUE, TRUE, \n                        stringr::str_detect(gender, 'Trouble'))\n         ) %>% \n  filter(test == TRUE)\n\nif(nrow(tests) != 0) {\n  print(\"Check nationscape_data\")\n  } else {\n    rm(tests)\n    }\n\nnationscape_data %>% \n  head()\n#> # A tibble: 6 × 5\n#>   gender state vote_biden age_group      education_level         \n#>   <chr>  <chr>      <dbl> <chr>          <chr>                   \n#> 1 female WI             0 age_45-59      Post secondary or higher\n#> 2 female VA             0 age_45-59      Post secondary or higher\n#> 3 female TX             0 age_60_or_more High school or less     \n#> 4 female WA             0 age_45-59      High school or less     \n#> 5 female MA             1 age_18-29      Some post secondary     \n#> 6 female TX             1 age_30-44      Some post secondary\n# This code is very directly from Alen Mitrovski, Xiaoyan Yang, and Matthew Wankiewicz.\n# Format state names so the whole state name is written out, to match IPUMS data\nstates_names_and_abbrevs <- \n  tibble(stateicp = state.name, state = state.abb)\n\nnationscape_data <-\n  nationscape_data %>%\n  left_join(states_names_and_abbrevs)\n\nrm(states_names_and_abbrevs)\n\n# Make lowercase to match IPUMS data\nnationscape_data <- \n  nationscape_data %>% \n  mutate(stateicp = tolower(stateicp))\n\n# Replace NAs with DC\nnationscape_data$stateicp <- \n  replace_na(nationscape_data$stateicp, \"district of columbia\")\n\n# Tidy the class\nnationscape_data <- \n  nationscape_data %>% \n  mutate(across(c(gender, stateicp, education_level, age_group), as_factor))\n\n# Save data\nwrite_csv(nationscape_data, \"outputs/data/polling_data.csv\")\n\nnationscape_data %>% \n  head()\n#> # A tibble: 6 × 6\n#>   gender state vote_biden age_group      education_level          stateicp     \n#>   <fct>  <chr>      <dbl> <fct>          <fct>                    <fct>        \n#> 1 female WI             0 age_45-59      Post secondary or higher wisconsin    \n#> 2 female VA             0 age_45-59      Post secondary or higher virginia     \n#> 3 female TX             0 age_60_or_more High school or less      texas        \n#> 4 female WA             0 age_45-59      High school or less      washington   \n#> 5 female MA             1 age_18-29      Some post secondary      massachusetts\n#> 6 female TX             1 age_30-44      Some post secondary      texas"},{"path":"mrp.html","id":"post-stratification-data","chapter":"17 Multilevel regression with post-stratification","heading":"17.5.2 Post-stratification data","text":"lot options dataset post-stratify various considerations. dataset better quality (however defined), likely larger. strictly data perspective, best choice probably something like Cooperative Congressional Election Study (CCES), however whatever reason released election ’s reasonable choice. W. Wang et al. (2015) use exit poll data, ’s available election.countries ’d stuck using census, course quite large, likely --date. Luckily US opportunity use American Community Survey (ACS) asks analogous questions census, conducted every month, course year, end million responses. case ’re going access ACS IPUMS.go IPUMS website - https://ipums.org - ’re looking something like IPUMS USA ‘get data.’ Create account, need . ’ll take process. account, go ‘Select Samples’ de-select everything apart 2019 ACS. need get variables ’re interested . household want ‘STATEICP,’ person want ‘SEX,’ ‘AGE,’ ‘EDUC.’ everything selected, ‘view cart,’ want careful change ‘data format’ ‘.dta’ (’s nothing wrong formats, ’ve just already got code earlier deal type). Briefly just check many rows columns ’re requesting. around million rows, around ten twenty columns. ’s much 300MB maybe just see ’ve accidently selected something don’t need. Submit request within day, get email saying data can downloaded. take 30 minutes , don’t get email within day check size dataset, customize sample size reduce size initially.case let’s tidy data.dataset individual level. ’ll create counts sub-cell, proportions state.Now ’d like add proportions state.","code":"\n# Again, closely following code from Alen Mitrovski, Xiaoyan Yang, and Matthew Wankiewicz.\nlibrary(haven)\nlibrary(tidyverse)\n\nraw_poststrat_data <- \n  read_dta(here::here(\"dont_push/usa_00004.dta\"))\n\n# The Stata format separates labels so reunite those\nraw_poststrat_data <- \n  labelled::to_factor(raw_poststrat_data)\nhead(raw_poststrat_data)\n#> # A tibble: 6 × 28\n#>   year  sample serial cbserial  hhwt cluster region stateicp strata gq    pernum\n#>   <fct> <fct>   <dbl>    <dbl> <dbl>   <dbl> <fct>  <fct>     <dbl> <fct>  <dbl>\n#> 1 2018  2018 …      2  2.02e12 392.  2.02e12 east … alabama  190001 othe…      1\n#> 2 2018  2018 …      7  2.02e12  94.1 2.02e12 east … alabama   40001 grou…      1\n#> 3 2018  2018 …     13  2.02e12  83.7 2.02e12 east … alabama  130301 othe…      1\n#> 4 2018  2018 …     18  2.02e12  57.5 2.02e12 east … alabama  100001 grou…      1\n#> 5 2018  2018 …     23  2.02e12 157.  2.02e12 east … alabama  190001 grou…      1\n#> 6 2018  2018 …     28  2.02e12 157.  2.02e12 east … alabama  220001 othe…      1\n#> # … with 17 more variables: perwt <dbl>, sex <fct>, age <fct>, marst <fct>,\n#> #   race <fct>, raced <fct>, hispan <fct>, hispand <fct>, bpl <fct>,\n#> #   bpld <fct>, citizen <fct>, educ <fct>, educd <fct>, empstat <fct>,\n#> #   empstatd <fct>, labforce <fct>, inctot <dbl>\n\nraw_poststrat_data$age <- as.numeric(raw_poststrat_data$age)\n\npoststrat_data <- \n  raw_poststrat_data %>% \n  filter(inctot < 9999999) %>% \n  filter(age >= 18) %>% \n  mutate(gender = sex) %>% \n  mutate(\n    age_group = case_when( # case_when works in order and exits when there's a match\n      age <= 29 ~ 'age_18-29',\n      age <= 44 ~ 'age_30-44',\n      age <= 59 ~ 'age_45-59',\n      age >= 60 ~ 'age_60_or_more',\n      TRUE ~ 'Trouble'\n      ),\n    education_level = case_when(\n      educd == \"nursery school, preschool\" ~ \"High school or less\",\n      educd == \"kindergarten\" ~ \"High school or less\",\n      educd == \"grade 1\" ~ \"High school or less\",\n      educd == \"grade 2\" ~ \"High school or less\",\n      educd == \"grade 3\" ~ \"High school or less\",\n      educd == \"grade 4\" ~ \"High school or less\",\n      educd == \"grade 5\" ~ \"High school or less\",\n      educd == \"grade 6\" ~ \"High school or less\",\n      educd == \"grade 7\" ~ \"High school or less\",\n      educd == \"grade 8\" ~ \"High school or less\",\n      educd == \"grade 9\" ~ \"High school or less\",\n      educd == \"grade 10\" ~ \"High school or less\",\n      educd == \"grade 11\" ~ \"High school or less\",\n      educd == \"12th grade, no diploma\" ~ \"High school or less\",\n      educd == \"regular high school diploma\" ~ \"High school or less\",\n      educd == \"ged or alternative credential\" ~ \"High school or less\",\n      educd == \"some college, but less than 1 year\" ~ \"Some post secondary\",\n      educd == \"1 or more years of college credit, no degree\" ~ \"Some post secondary\",\n      educd == \"associate's degree, type not specified\" ~ \"Post secondary or higher\",\n      educd == \"bachelor's degree\" ~ \"Post secondary or higher\",\n      educd == \"master's degree\" ~ \"Graduate degree\",\n      educd == \"professional degree beyond a bachelor's degree\" ~ \"Graduate degree\",\n      educd == \"doctoral degree\" ~ \"Graduate degree\",\n      educd == \"no schooling completed\" ~ \"High school or less\",\n      TRUE ~ 'Trouble'\n      )\n    )\n\n# Just keep relevant variables\npoststrat_data <- \n  poststrat_data %>% \n  select(gender,\n         age_group,\n         education_level,\n         stateicp)\n\n# Tidy the class\npoststrat_data <- \n  poststrat_data %>% \n  mutate(across(c(gender, stateicp, education_level, age_group), as_factor))\n\n# Save data\nwrite_csv(poststrat_data, \"outputs/data/us_poststrat.csv\")\n\npoststrat_data %>% \n  head()\n#> # A tibble: 6 × 4\n#>   gender age_group      education_level     stateicp\n#>   <fct>  <fct>          <fct>               <fct>   \n#> 1 female age_18-29      Some post secondary alabama \n#> 2 female age_60_or_more Some post secondary alabama \n#> 3 male   age_45-59      Some post secondary alabama \n#> 4 male   age_30-44      High school or less alabama \n#> 5 female age_60_or_more High school or less alabama \n#> 6 male   age_30-44      High school or less alabama\npoststrat_data_cells <- \n  poststrat_data %>% \n  group_by(stateicp, gender, age_group, education_level) %>% \n  count()\npoststrat_data_cells <- \n  poststrat_data_cells %>% \n  group_by(stateicp) %>% \n  mutate(prop = n/sum(n)) %>% \n  ungroup()\n\npoststrat_data_cells %>% head()\n#> # A tibble: 6 × 6\n#>   stateicp    gender age_group      education_level              n    prop\n#>   <fct>       <fct>  <fct>          <fct>                    <int>   <dbl>\n#> 1 connecticut male   age_18-29      Some post secondary        149 0.0260 \n#> 2 connecticut male   age_18-29      High school or less        232 0.0404 \n#> 3 connecticut male   age_18-29      Post secondary or higher    96 0.0167 \n#> 4 connecticut male   age_18-29      Graduate degree             25 0.00436\n#> 5 connecticut male   age_60_or_more Some post secondary        142 0.0248 \n#> 6 connecticut male   age_60_or_more High school or less        371 0.0647"},{"path":"mrp.html","id":"model-4","chapter":"17 Multilevel regression with post-stratification","heading":"17.5.3 Model","text":"’re going use logistic regression estimate model binary support Biden explained gender, age-group, education-level, state. ’re going Bayesian framework using rstanarm (Goodrich et al. 2020). variety reasons using rstanarm , main one Stan pre-compiled eases computer set-issues may otherwise . great resource implementing MRP rstanarm Kennedy Gabry (2020).variety options ’ve largely unthinkingly set, exploring effect good idea, now can just quick look model.","code":"\nlibrary(rstanarm)\n\nus_election_model <- \n  rstanarm::stan_glmer(vote_biden ~ gender + age_group + (1 | stateicp) + education_level,\n                       data = nationscape_data,\n                       family = binomial(link = \"logit\"),\n                       prior = normal(0, 1), \n                       prior_intercept = normal(0, 1),\n                       cores = 2, \n                       seed = 853)\nmodelsummary::get_estimates(us_election_model)\n#>                                      term effect    estimate conf.level\n#> 1                             (Intercept)  fixed  0.27591651       0.95\n#> 2                              gendermale  fixed -0.54094841       0.95\n#> 3                 age_groupage_60_or_more  fixed  0.04886803       0.95\n#> 4                      age_groupage_18-29  fixed  0.87817445       0.95\n#> 5                      age_groupage_30-44  fixed  0.12455081       0.95\n#> 6      education_levelHigh school or less  fixed -0.35080948       0.95\n#> 7      education_levelSome post secondary  fixed -0.14970941       0.95\n#> 8          education_levelGraduate degree  fixed -0.21988079       0.95\n#> 9 Sigma[stateicp:(Intercept),(Intercept)] random  0.08002654       0.95\n#>      conf.low    conf.high      pd rope.percentage      rhat      ess\n#> 1  0.10322566  0.447878764 0.99850       0.1281242 1.0002399 2262.704\n#> 2 -0.65227570 -0.430121742 1.00000       0.0000000 0.9996776 5264.121\n#> 3 -0.10847716  0.200831286 0.72600       0.9765851 0.9998468 3726.472\n#> 4  0.69830955  1.061899702 1.00000       0.0000000 0.9997188 3931.709\n#> 5 -0.02530517  0.284994549 0.94125       0.7729545 0.9995061 3567.152\n#> 6 -0.51262507 -0.204222132 1.00000       0.0000000 0.9997706 3700.321\n#> 7 -0.29927232  0.006135831 0.96625       0.6771902 1.0005471 4090.628\n#> 8 -0.38172115 -0.036764485 0.99100       0.3249145 0.9999286 4589.040\n#> 9  0.02912381  0.160211514 1.00000       1.0000000 1.0031942 1284.436\n#>   prior.distribution prior.location prior.scale\n#> 1             normal              0           1\n#> 2             normal              0           1\n#> 3             normal              0           1\n#> 4             normal              0           1\n#> 5             normal              0           1\n#> 6             normal              0           1\n#> 7             normal              0           1\n#> 8             normal              0           1\n#> 9               <NA>             NA          NA\n# The default usage of modelsummary requires statistics that we don't have.\n# Uncomment the following line if you want to look at what is available and specify your own:\n# modelsummary::get_estimates(us_election_model)\nmodelsummary::modelsummary(us_election_model,\n                            statistic = c('conf.low', 'conf.high')\n                            )"},{"path":"mrp.html","id":"post-stratify-2","chapter":"17 Multilevel regression with post-stratification","heading":"17.5.4 Post-stratify","text":"can look estimates, like.","code":"\nbiden_support_by_state <- \n  us_election_model %>%\n  tidybayes::add_fitted_draws(newdata=poststrat_data_cells) %>%\n  rename(support_biden_predict = .value) %>% \n  mutate(support_biden_predict_prop = support_biden_predict*prop) %>% \n  group_by(stateicp, .draw) %>% \n  summarise(support_biden_predict = sum(support_biden_predict_prop)) %>% \n  group_by(stateicp) %>% \n  summarise(mean = mean(support_biden_predict), \n            lower = quantile(support_biden_predict, 0.025), \n            upper = quantile(support_biden_predict, 0.975))\nbiden_support_by_state %>% \n  ggplot(aes(y = mean, x = stateicp, color = \"MRP estimate\")) + \n  geom_point() +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0) + \n  geom_point(data = \n               nationscape_data %>% \n               group_by(stateicp, vote_biden) %>%\n               summarise(n = n()) %>% \n               group_by(stateicp) %>% \n               mutate(prop = n/sum(n)) %>% \n               filter(vote_biden==1), \n             aes(y = prop, x = stateicp, color = 'Nationscape raw data')) +\n  geom_hline(yintercept = 0.5, linetype = 'dashed') +\n  labs(x = 'State',\n       y = 'Estimated proportion support for Biden',\n       color = 'Source') +\n  theme_classic() +\n  scale_color_brewer(palette = 'Set1') +\n  coord_flip()"},{"path":"mrp.html","id":"concluding-remarks-and-next-steps","chapter":"17 Multilevel regression with post-stratification","heading":"17.6 Concluding remarks and next steps","text":"general, MRP good way accomplish specific aims, ’s without trade-offs. good quality survey, may way speak disaggregated aspects . concerned uncertainty good way think . biased survey ’s great place start, ’s panacea.’s lot work ’s done , ’s plenty scope exciting work variety approaches:statistical perspective, lot work terms thinking survey design modelling approaches interact extent underestimating uncertainty. ’m also interested thinking implications small samples uncertainty post-stratification dataset. ’s awful lot terms thinking appropriate model use, even evaluate ‘appropriate’ means ? statistical interests, probably go next Gao et al. (2021) well pretty much anything Yajuan Si, Si (2020) starting point.’s lot done sociology perspective terms survey responses can better design surveys, knowing going used MRP putting respect respondents first.political science perspective, just little idea conditions stable preferences relationships required MRP accurate, understanding relates uncertainty survey design. political science interests, natural next step go Lauderdale et al. (2020) Ghitza Gelman (2020).Economists might interested think use MRP better understand inflation unemployment rates local levels.statistical software side things, really need develop better packages around .’s information side things, ’m excited MRP. best store protect datasets, yet retain ability correspond ? put levels together way meaningful? extent people appreciate uncertainty estimates can better communicate estimates?generally, pretty much use MRP anywhere samples. Determining conditions actually , work whole generations.","code":""},{"path":"mrp.html","id":"exercises-and-tutorial-16","chapter":"17 Multilevel regression with post-stratification","heading":"17.7 Exercises and tutorial","text":"","code":""},{"path":"mrp.html","id":"exercises-16","chapter":"17 Multilevel regression with post-stratification","heading":"17.7.1 Exercises","text":"Mum asked ’ve learning term. decide tell multilevel regression post-stratification (MRP). Please explain MRP . Mum university-education, necessarily taken statistics, need explain technical terms use. [Please write two three paragraphs; strong answers clear strengths weaknesses.]respect W. Wang et al. (2015): paper interesting? like paper? wish better? extent can reproduce paper? [Please write one two paragraphs aspect.]respect W. Wang et al. (2015), feature mention election forecasts need?\nExplainable.\nAccurate.\nCost-effective.\nRelevant.\nTimely.\nExplainable.Accurate.Cost-effective.Relevant.Timely.respect W. Wang et al. (2015), weakness MRP?\nDetailed data requirement.\nAllows use biased data.\nExpensive conduct.\nDetailed data requirement.Allows use biased data.Expensive conduct.respect W. Wang et al. (2015), concerning Xbox sample?\nNon-representative.\nSmall sample size.\nMultiple responses respondent.\nNon-representative.Small sample size.Multiple responses respondent.interested studying voting intentions 2020 US presidential election vary individual’s income. set logistic regression model study relationship. study, possible independent variables : [Please check apply.]\nWhether respondent registered vote (yes/).\nWhether respondent going vote Biden (yes/).\nrace respondent (white/white).\nrespondent’s marital status (married/).\nWhether respondent registered vote (yes/).Whether respondent going vote Biden (yes/).race respondent (white/white).respondent’s marital status (married/).Please think Cohn (2016) type exercise carried ? think different groups, even background level quantitative sophistication, different estimates even use data? [Please write paragraph two aspect.]think multilevel regression post-stratification, key assumptions making? [Please write one two paragraphs aspect.]train model based survey, post-stratify using 2020 ACS dataset. practical considerations may contend ? [Please write paragraph least three considerations.]similar manner Ghitza Gelman (2020) pretend ’ve got access US voter file record private company. train model 2020 US CCES, post-stratify , individual-basis, based voter file.\nplease put-together datasheet voter file dataset following Gebru et al. (2021)? reminder, datasheets accompany datasets document ‘motivation, composition, collection process, recommended uses,’ among aspects.\nalso please put together model card model, following Mitchell et al. (2019)? reminder, model cards deliberately straight-forward one- two-page documents report aspects : model details; intended use; metrics; training data; ethical considerations; well caveats recommendations (Mitchell et al. 2019).\nplease discuss three ethical aspects around features using model? [Please write paragraph two point.]\nplease detail protections put place terms dataset, model, predictions?\nplease put-together datasheet voter file dataset following Gebru et al. (2021)? reminder, datasheets accompany datasets document ‘motivation, composition, collection process, recommended uses,’ among aspects.also please put together model card model, following Mitchell et al. (2019)? reminder, model cards deliberately straight-forward one- two-page documents report aspects : model details; intended use; metrics; training data; ethical considerations; well caveats recommendations (Mitchell et al. 2019).please discuss three ethical aspects around features using model? [Please write paragraph two point.]please detail protections put place terms dataset, model, predictions?model output lm() called ‘my_model_output’ can use modelsummary display output (assume package loaded) [please select apply]?\nmodelsummary::modelsummary(my_model_output)\nmodelsummary(my_model_output)\nmy_model_output %>% modelsummary()\nmy_model_output %>% modelsummary(statistic = NULL)\nmodelsummary::modelsummary(my_model_output)modelsummary(my_model_output)my_model_output %>% modelsummary()my_model_output %>% modelsummary(statistic = NULL)following examples linear models [please select apply]?\nlm(y ~ x_1 + x_2 + x_3, data = my_data)\nlm(y ~ x_1 + x_2^2 + x_3, data = my_data)\nlm(y ~ x_1 * x_2 + x_3, data = my_data)\nlm(y ~ x_1 + x_1^2 + x_2 + x_3, data = my_data)\nlm(y ~ x_1 + x_2 + x_3, data = my_data)lm(y ~ x_1 + x_2^2 + x_3, data = my_data)lm(y ~ x_1 * x_2 + x_3, data = my_data)lm(y ~ x_1 + x_1^2 + x_2 + x_3, data = my_data)Consider situation survey dataset age-groups: 18-29; 30-44; 45- 60; 60+. post-stratification dataset age-groups: 18-24; 25-29; 30-34; 35-39; 40-44; 45-49; 50-54; 55-59; 60+. approach take bringing together? [Please write paragraph.]Consider situation survey dataset age-groups: 18-29; 30-44; 45- 60; 60+. time post-stratification dataset age-groups: 18-34; 35-49; 50-64; 65+. approach take bringing together? [Please write paragraph.]Please consider Kennedy et al. (2020). statistical facets considering survey focused gender, post-stratification survey ? [Please check apply.]\nImpute non-male female\nEstimate gender using auxiliary information\nImpute population\nImpute sample values\nModel population distribution using auxiliary data\nRemove non-binary respondents\nRemove respondents\nAssume population distribution\nImpute non-male femaleEstimate gender using auxiliary informationImpute populationImpute sample valuesModel population distribution using auxiliary dataRemove non-binary respondentsRemove respondentsAssume population distributionPlease consider Kennedy et al. (2020). ethical facets considering survey focused gender, post-stratification survey ? [Please check apply.]\nImpute non-male female\nEstimate gender using auxiliary information\nImpute population\nImpute sample values\nModel population distribution using auxiliary data\nRemove non-binary respondents\nRemove respondents\nAssume population distribution\nImpute non-male femaleEstimate gender using auxiliary informationImpute populationImpute sample valuesModel population distribution using auxiliary dataRemove non-binary respondentsRemove respondentsAssume population distributionPlease consider Kennedy et al. (2020). define ethics?\nRespecting perspectives dignity individual survey respondents.\nGenerating estimates general population subpopulations interest.\nUsing complicated procedures serve useful function.\nRespecting perspectives dignity individual survey respondents.Generating estimates general population subpopulations interest.Using complicated procedures serve useful function.","code":""},{"path":"mrp.html","id":"tutorial-16","chapter":"17 Multilevel regression with post-stratification","heading":"17.7.2 Tutorial","text":"","code":""},{"path":"mrp.html","id":"paper-5","chapter":"17 Multilevel regression with post-stratification","heading":"17.7.3 Paper","text":"point, Paper Five (Appendix B.5) appropriate.","code":""},{"path":"text-as-data.html","id":"text-as-data","chapter":"18 Text as data","heading":"18 Text as data","text":"STATUS: construction.Required readingHvitfeldt, Emil, Julia Silge, 2021, Supervised Machine Learning Text Analysis R, Chapters 2, 5, 6, 7, https://smltar.com.Silge, Julia, David Robinson, 2017, Text Mining R, https://www.tidytextmining.com.Required viewingRecommended readingAmaka, Ofunne, Amber Thomas, 2020, ‘Naked Truth: names 6,816 complexion products can reveal bias beauty,’ Pudding, March, https://pudding.cool/2021/03/foundation-names/.Key concepts/skills/etcKey librariesKey functions/etcQuiz","code":""},{"path":"text-as-data.html","id":"introduction-19","chapter":"18 Text as data","heading":"18.1 Introduction","text":"Text can thought unwieldy, generally","code":""},{"path":"text-as-data.html","id":"lasso-regression","chapter":"18 Text as data","heading":"18.2 Lasso regression","text":"subsection, much code used, directly draws Julia Silge’s notes, particular: https://juliasilge.com/blog/tidy-text-classification/ (Silge 2018).One nice aspects text can adapt existing methods use input. going use variation logistics regression, along text inputs, forecast. want learn Lasso regression, consider taking Arik’s course summer, dive machine learning using Python.section going two different text inputs, train model sample text , try use model forecast text training set. Although arbitrary example, imagine many real-world applications. instance, work Twitter may want know tweet likely written bot, human. similarly, imagine work political party - may like know email likely email campaign organised group, individual.First need get data. Julia Silge’s example, nicely, uses book text input. Seeing jointly appointed Faculty Information, seems especially nice. wonderful thing R package - gutenbergr - makes easy get text Project Gutenberg R. key function gutenberg_download(), needs key book want. ’ll consider Jane Eyre Alice’s Adventures Wonderland, keys 1260 11, respectively.One great things dataset tibble. can just work familiar skills. package lot functionality, ’d encourage look package’s website: https://github.com/ropensci/gutenbergr. line book read different row dataset. Notice downloaded two books , added title. two books one . can see looking summary statistics.looks like Jane Eyre much longer Alice Wonderland, isn’t surprise read . don’t want step Digital Humanities much, don’t know anything , looking things like broader context books written, books written similar times, likely fascinating area.’ll just get rid blank linesThere’s still overwhelming amount Jane Eyre . ’ll just sample Jane Eyre make equal.’s bunch issues , instance, whole Alice, random bits Jane, nonetheless let’s continue ’ll try something moment.Now want get sample text book. use lines distinguish samples. use counter add line number.now want separate words. ’ll just use tidytext, focus modelling, bunch alternatives one especially good one quanteda package, specifically, tokens() function.Notice removed word wasn’t used 10 times. Nonetheless still lot unique words. (didn’t require word used author least 10 times end 6,000 words.)reason relevant independent variables. may used something less 10 explanatory variables, case going 585 , need model can handle .However, mentioned , going rows essentially just one word. allow , might also nice give model least words work .’ll create test/training split, load tidymodels.Now need create document-term matrix.independent variables sorted, now need binary dependent variable, whether book Alice Wonderland Jane Eyre.Now can run model.Perhaps unsurprisingly, mention Alice ’s likely Alice Wonderland mention Jane ’s likely Jane Eyre.","code":"\nlibrary(gutenbergr)\nalice_and_jane <- gutenbergr::gutenberg_download(c(1260, 11), meta_fields = \"title\")\n\n# Save the dataset so that we don't need to overwhelm the servers each time\nwrite_csv(alice_and_jane, \"inputs/books/alice_and_jane.csv\")\n\nhead(alice_and_jane)\nlibrary(gutenbergr)\n\nalice_and_jane <- read_csv(\"inputs/books/alice_and_jane.csv\")\n\nhead(alice_and_jane)\n#> # A tibble: 6 × 3\n#>   gutenberg_id text                               title                         \n#>          <dbl> <chr>                              <chr>                         \n#> 1           11 ALICE'S ADVENTURES IN WONDERLAND   Alice's Adventures in Wonderl…\n#> 2           11 <NA>                               Alice's Adventures in Wonderl…\n#> 3           11 Lewis Carroll                      Alice's Adventures in Wonderl…\n#> 4           11 <NA>                               Alice's Adventures in Wonderl…\n#> 5           11 THE MILLENNIUM FULCRUM EDITION 3.0 Alice's Adventures in Wonderl…\n#> 6           11 <NA>                               Alice's Adventures in Wonderl…\ntable(alice_and_jane$title)\n#> \n#> Alice's Adventures in Wonderland      Jane Eyre: An Autobiography \n#>                             3339                            20659\nlibrary(janitor)\n# TODO There's a way to do this within janitor, but I forget, need to look it up.\nalice_and_jane <- \n  alice_and_jane %>% \n  mutate(blank_line = if_else(text == \"\", 1, 0)) %>% \n  filter(blank_line == 0) %>% \n  select(-blank_line)\n\ntable(alice_and_jane$title)\n#> \n#> Alice's Adventures in Wonderland      Jane Eyre: An Autobiography \n#>                             2481                            16395\nset.seed(853)\n\nalice_and_jane$rows <- c(1:nrow(alice_and_jane))\nsample_from_me <- alice_and_jane %>% filter(title == \"Jane Eyre: An Autobiography\")\nkeep_me <- sample(x = sample_from_me$rows, size = 2481, replace = FALSE)\n\nalice_and_jane <- \n  alice_and_jane %>% \n  filter(title == \"Alice's Adventures in Wonderland\" | rows %in% keep_me) %>% \n  select(-rows)\n\ntable(alice_and_jane$title)\n#> \n#> Alice's Adventures in Wonderland      Jane Eyre: An Autobiography \n#>                             2481                             2481\nalice_and_jane <- \n  alice_and_jane %>% \n  group_by(title) %>% \n  mutate(line_number = paste(gutenberg_id, row_number(), sep = \"_\")) %>% \n  ungroup()\nlibrary(tidytext)\nalice_and_jane_by_word <- \n  alice_and_jane %>% \n  unnest_tokens(word, text) %>%\n  group_by(word) %>%\n  filter(n() > 10) %>%\n  ungroup()\nalice_and_jane_by_word$word %>% unique() %>% length()\n#> [1] 585\nalice_and_jane_by_word <- \n  alice_and_jane_by_word %>% \n  group_by(title, line_number) %>% \n  mutate(number_of_words_in_line = n()) %>% \n  ungroup() %>% \n  filter(number_of_words_in_line > 2) %>% \n  select(-number_of_words_in_line)\nlibrary(tidymodels)\n\nset.seed(853)\n\nalice_and_jane_by_word_split <- \n  alice_and_jane_by_word %>%\n  select(title, line_number) %>% \n  distinct() %>% \n  initial_split(prop = 3/4, strata = title)\n\n# alice_and_jane_by_word_train <- training(alice_and_jane_by_word_split) %>% select(line_number)\n# alice_and_jane_by_word_test <- testing(alice_and_jane_by_word_split)\n# \n# rm(alice_and_jane_by_word_split)\nalice_and_jane_dtm_training <- \n  alice_and_jane_by_word %>% \n  count(line_number, word) %>% \n  inner_join(training(alice_and_jane_by_word_split) %>% select(line_number)) %>% \n  cast_dtm(term = word, document = line_number, value = n)\n\ndim(alice_and_jane_dtm_training)\n#> [1] 3413  585\n\nresponse <- \n  data.frame(id = dimnames(alice_and_jane_dtm_training)[[1]]) %>% \n  separate(id, into = c(\"book\", \"line\", sep = \"_\")) %>% \n  mutate(is_alice = if_else(book == 11, 1, 0)) \n  \n\npredictor <- alice_and_jane_dtm_training[] %>% as.matrix()\nlibrary(glmnet)\n\nmodel <- cv.glmnet(x = predictor,\n                   y = response$is_alice,\n                   family = \"binomial\",\n                   keep = TRUE\n                   )\n\nsave(model, file = \"outputs/models/alice_vs_jane.rda\")\nload(\"outputs/models/alice_vs_jane.rda\")\nlibrary(glmnet)\nlibrary(broom)\n\ncoefs <- model$glmnet.fit %>%\n  tidy() %>%\n  filter(lambda == model$lambda.1se)\n\ncoefs %>% head()\n#> # A tibble: 6 × 5\n#>   term         step estimate  lambda dev.ratio\n#>   <chr>       <dbl>    <dbl>   <dbl>     <dbl>\n#> 1 (Intercept)    36 -0.335   0.00597     0.562\n#> 2 in             36 -0.144   0.00597     0.562\n#> 3 she            36  0.390   0.00597     0.562\n#> 4 so             36  0.00249 0.00597     0.562\n#> 5 a              36 -0.117   0.00597     0.562\n#> 6 about          36  0.279   0.00597     0.562\ncoefs %>%\n  group_by(estimate > 0) %>%\n  top_n(10, abs(estimate)) %>%\n  ungroup() %>%\n  ggplot(aes(fct_reorder(term, estimate), estimate, fill = estimate > 0)) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  coord_flip() +\n  theme_minimal() +\n  labs(x = \"Coefficient\",\n       y = \"Word\") +\n  scale_fill_brewer(palette = \"Set1\")"},{"path":"text-as-data.html","id":"topic-models","chapter":"18 Text as data","heading":"18.3 Topic models","text":"version notes previously circulated part R. Alexander Alexander (2021).","code":""},{"path":"text-as-data.html","id":"overview-7","chapter":"18 Text as data","heading":"18.3.1 Overview","text":"Sometimes statement want know . Sometimes easy, don’t always titles statements, even , sometimes titles define topics well-defined consistent way. One way get consistent estimates topics statement use topic models. many variants, one way use latent Dirichlet allocation (LDA) method Blei, Ng, Jordan (2003), implemented R package ‘topicmodels’ Grün Hornik (2011).key assumption behind LDA method statement, ‘document,’ made person decides topics like talk document, chooses words, ‘terms,’ appropriate topics. topic thought collection terms, document collection topics. topics specified ex ante; outcome method. Terms necessarily unique particular topic, document one topic. provides flexibility approaches strict word count method. goal words found documents group define topics.","code":""},{"path":"text-as-data.html","id":"document-generation-process","chapter":"18 Text as data","heading":"18.3.2 Document generation process","text":"LDA method considers statement result process person first chooses topics want speak . choosing topics, person chooses appropriate words use topics. generally, LDA topic model works considering document generated probability distribution topics. instance, five topics two documents, first document may comprised mostly first topics; document may mostly final topics (Figure 18.1).\nFigure 18.1: Probability distributions topics\nSimilarly, topic considered probability distribution terms. choose terms used document speaker picks terms topic appropriate proportion. instance, ten terms, one topic defined giving weight terms related immigration; topic may give weight terms related economy (Figure 18.2).\nFigure 18.2: Probability distributions terms\nFollowing Blei Lafferty (2009), Blei (2012) Griffiths Steyvers (2004), process document generated formally considered :\\(1, 2, \\dots, k, \\dots, K\\) topics vocabulary consists \\(1, 2, \\dots, V\\) terms. topic, decide terms topic uses randomly drawing distributions terms. distribution terms \\(k\\)th topic \\(\\beta_k\\). Typically topic small number terms Dirichlet distribution hyperparameter \\(0<\\eta<1\\) used: \\(\\beta_k \\sim \\mbox{Dirichlet}(\\eta)\\).5 Strictly, \\(\\eta\\) actually vector hyperparameters, one \\(K\\), practice tend value.Decide topics document cover randomly drawing distributions \\(K\\) topics \\(1, 2, \\dots, d, \\dots, D\\) documents. topic distributions \\(d\\)th document \\(\\theta_d\\), \\(\\theta_{d,k}\\) topic distribution topic \\(k\\) document \\(d\\). , Dirichlet distribution hyperparameter \\(0<\\alpha<1\\) used usually document cover handful topics: \\(\\theta_d \\sim \\mbox{Dirichlet}(\\alpha)\\). , strictly \\(\\alpha\\) vector length \\(K\\) hyperparameters, practice usually value.\\(1, 2, \\dots, n, \\dots, N\\) terms \\(d\\)th document, choose \\(n\\)th term, \\(w_{d, n}\\):\nRandomly choose topic term \\(n\\), document \\(d\\), \\(z_{d,n}\\), multinomial distribution topics document, \\(z_{d,n} \\sim \\mbox{Multinomial}(\\theta_d)\\).\nRandomly choose term relevant multinomial distribution terms topic, \\(w_{d,n} \\sim \\mbox{Multinomial}(\\beta_{z_{d,n}})\\).\nRandomly choose topic term \\(n\\), document \\(d\\), \\(z_{d,n}\\), multinomial distribution topics document, \\(z_{d,n} \\sim \\mbox{Multinomial}(\\theta_d)\\).Randomly choose term relevant multinomial distribution terms topic, \\(w_{d,n} \\sim \\mbox{Multinomial}(\\beta_{z_{d,n}})\\).Given set-, joint distribution variables (Blei (2012), p.6):\n\\[p(\\beta_{1:K}, \\theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}) = \\prod^{K}_{=1}p(\\beta_i) \\prod^{D}_{d=1}p(\\theta_d) \\left(\\prod^N_{n=1}p(z_{d,n}|\\theta_d)p\\left(w_{d,n}|\\beta_{1:K},z_{d,n}\\right) \\right).\\]Based document generation process analysis problem, discussed next section, compute posterior \\(\\beta_{1:K}\\) \\(\\theta_{1:D}\\), given \\(w_{1:D, 1:N}\\). intractable directly, can approximated (Griffiths Steyvers (2004) Blei (2012)).","code":""},{"path":"text-as-data.html","id":"analysis-process","chapter":"18 Text as data","heading":"18.3.3 Analysis process","text":"documents created, analyse. term usage document, \\(w_{1:D, 1:N}\\), observed, topics hidden, ‘latent.’ know topics document, terms defined topics. , know probability distributions Figures 18.1 18.2. sense trying reverse document generation process – terms like discover topics.earlier process around documents generated assumed observe terms document, can obtain estimates topics (Steyvers Griffiths (2006)). outcomes LDA process probability distributions define topics. term given probability member particular topic, document given probability particular topic. , trying calculate posterior distribution topics given terms observed document (Blei (2012), p.7):\n\\[p(\\beta_{1:K}, \\theta_{1:D}, z_{1:D, 1:N} | w_{1:D, 1:N}) = \\frac{p\\left(\\beta_{1:K}, \\theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}\\right)}{p(w_{1:D, 1:N})}.\\]initial practical step implementing LDA given corpus documents remove ‘stop words.’ words common, don’t typically help define topics. general list stop words : “”; “’s”; “able”; “”; “”… also remove punctuation capitalisation. documents need transformed document-term-matrix. essentially table column number times term appears document.dataset ready, R package ‘topicmodels’ Grün Hornik (2011) can used implement LDA approximate posterior. using Gibbs sampling variational expectation-maximization algorithm. Following Steyvers Griffiths (2006) Darling (2011), Gibbs sampling process attempts find topic particular term particular document, given topics terms documents. Broadly, first assigning every term every document random topic, specified Dirichlet priors \\(\\alpha = \\frac{50}{K}\\) \\(\\eta = 0.1\\) (Steyvers Griffiths (2006) recommends \\(\\eta = 0.01\\)), \\(\\alpha\\) refers distribution topics \\(\\eta\\) refers distribution terms (Grün Hornik (2011), p.7). selects particular term particular document assigns new topic based conditional distribution topics terms documents taken given (Grün Hornik (2011), p.6):\n\\[p(z_{d, n}=k | w_{1:D, 1:N}, z'_{d, n}) \\propto \\frac{\\lambda'_{n\\rightarrow k}+\\eta}{\\lambda'_{.\\rightarrow k}+V\\eta} \\frac{\\lambda'^{(d)}_{n\\rightarrow k}+\\alpha}{\\lambda'^{(d)}_{-}+K\\alpha} \\]\n\\(z'_{d, n}\\) refers topic assignments; \\(\\lambda'_{n\\rightarrow k}\\) count many times term assigned topic \\(k\\); \\(\\lambda'_{.\\rightarrow k}\\) count many times term assigned topic \\(k\\); \\(\\lambda'^{(d)}_{n\\rightarrow k}\\) count many times term assigned topic \\(k\\) particular document; \\(\\lambda'^{(d)}_{-}\\) count many times term assigned document. \\(z_{d,n}\\) estimated, estimates distribution words topics topics documents can backed .conditional distribution assigns topics depending often term assigned topic previously, common topic document (Steyvers Griffiths (2006)). initial random allocation topics means results early passes corpus document poor, given enough time algorithm converges appropriate estimate.","code":""},{"path":"text-as-data.html","id":"warnings-and-extensions","chapter":"18 Text as data","heading":"18.3.4 Warnings and extensions","text":"choice number topics, k, affects results, must specified priori. strong reason particular number, can used. Otherwise, one way choose appropriate number use test training set process. Essentially, means running process variety possible values k picking appropriate value performs well.One weakness LDA method considers ‘bag words’ order words matter (Blei (2012)). possible extend model reduce impact bag--words assumption add conditionality word order. Additionally, alternatives Dirichlet distribution can used extend model allow correlation. instance, Hansard topics related army may expected commonly found topics related navy, less commonly topics related banking.","code":""},{"path":"text-as-data.html","id":"word-embedding","chapter":"18 Text as data","heading":"18.4 Word embedding","text":"","code":""},{"path":"text-as-data.html","id":"conclusion-1","chapter":"18 Text as data","heading":"18.5 Conclusion","text":"Using text data exciting quantity variety text available us. general, dealing text datasets messy. lot cleaning preparation typically required. Often text datasets large. , workflow place, work reproducible way, simulating data first, clearly communicating findings becomes critical, keep everything organised mind. Nonetheless, exciting area, encourage regularly use text analysis possible.terms next steps two, related, concerns: data analysis.terms data many places get large amounts text data relatively easily, including:r package rtweets makes easy get Twitter data (although typically going looking forward start using , rather able look back). Plenty people U T work Twitter data including Jia Xue iSchool, Ludovic Rheault political science.inside Airbnb dataset used earlier provides text reviews.’ve seen gutenbergr package already notes, provides easy access text Project Gutenberg.’ve seen scraping Wikipedia, going bit may find better use package, instance WikipediR.terms analysis:Start going tidytext book, tidytext, lot nice explanations, code, examples.worthwhile working Quanteda package quanteda tutorials.Finally, consider packages text2vec, spacyr.","code":""},{"path":"text-as-data.html","id":"exercises-and-tutorial-17","chapter":"18 Text as data","heading":"18.6 Exercises and tutorial","text":"","code":""},{"path":"text-as-data.html","id":"exercises-17","chapter":"18 Text as data","heading":"18.6.1 Exercises","text":"","code":""},{"path":"text-as-data.html","id":"tutorial-17","chapter":"18 Text as data","heading":"18.6.2 Tutorial","text":"","code":""},{"path":"using-the-cloud.html","id":"using-the-cloud","chapter":"19 Using the cloud","heading":"19 Using the cloud","text":"STATUS: construction.Required readingRecommended readingEdmondson, Mark, 2020, ‘googleComputeEngineR documentation,’ version 0.3.0.9000, freely available : https://cloudyr.github.io/googleComputeEngineR/.McDermott, Grant R., 2020, ‘Cloud computing Google Compute Engine,’ Data Science Economists, freely available : https://raw.githack.com/uo-ec607/lectures/master/14-gce/14-gce.html.Morris, Mitzi, 2020, ‘Stan Notebooks Cloud,’ freely available : https://mc-stan.org/users/documentation/case-studies/jupyter_colab_notebooks_2020.html.Key concepts/skills/etcBenefits/costs cloud.Getting started cloud.Starting virtual machines R Studio.Stopping virtual machines.","code":""},{"path":"using-the-cloud.html","id":"introduction-20","chapter":"19 Using the cloud","heading":"19.1 Introduction","text":"Cloud benefits:\n- Costs can reduced, easily amortized.\n- Can scale need.\n- Many platforms already sorted e.g. R Studio just works.stole someone can’t remember , cloud another name ‘someone else’s computer.’ ’s . Nonetheless, learning use someone else’s computer can great number reasons including:Scalability: can quite expensive buy new computer, especially need run something every now , using someone else’s computer, can just rent hours days.Portability: can shift analysis workflow laptop cloud, suggests likely good things terms reproducibility portability. least, code capable running laptop cloud.Set--forget: something take , can great worry laptop’s fan running overnight, partner/baby/pet/housemate/etc accidently closing computer, able watch Netflix computer.use cloud running code ‘virtual machine.’ part larger bunch computers designed act like computer specific features. instance may specify virtual machine 8 GB RAM, 128 storage, 4 CPUs. VM act like computer specifications. cost use cloud options increases based specifications virtual machine choose.downsides:Cost: cloud options cheap, rarely free. (free options, tend powerful, end pay get computer better laptop.) give idea cost, use AWS, typically end spending five ten dollars couple days. ’s fairly cheap, ’s nothing. ’s also pretty easy accidently forget something run unexpected bill, especially initially.Public: pretty easy make mistakes accidently make everything public.Time: takes time get set-comfortable cloud.notes going introduce cloud starting options pretty much anyone can () take advantage : Google Colab; moving general cloud options including Google Compute Engine, AWS, Azure, may useful cases. want get job industry, advice pretty much every speaker industry Toronto Data Workshop learn least one cloud options. instance, Munich Re Azure shop, Receptiviti uses AWS, etc.","code":""},{"path":"using-the-cloud.html","id":"google-colab","chapter":"19 Using the cloud","heading":"19.2 Google Colab","text":"Google Colab similar R Studio Cloud, set-allow just log get started. case, need Google account. ’s better R Studio resources put development can use GPUs, hand designed Python, can use R, ’s really focused .get started need tell Google Colab want use R. can using : https://colab.research.google.com/notebook#create=true&language=r.point Jupyter notebook open run R. (R Markdown document.) can install packages normal, e.g. install.packages(\"tidyverse\"), call package e.g. library(tidyverse).Google Colab good option good reason using broader capabilities . want go deeper Morris reading bunch options can explore, Morris puts ‘Colab gateway drug - large-scale processing pipelines ’ll need move Google Cloud Platform one competitors AWS, Azure, etc.’ now.","code":""},{"path":"using-the-cloud.html","id":"aws","chapter":"19 Using the cloud","heading":"19.3 AWS","text":"Amazon Web Services cloud service Amazon. get started need AWS Developer account can create : https://aws.amazon.com/developer/.created account, need select region computer access located. , want “Launch virtual machine” (EC2).first step choose Amazon Machine Image (AMI). provides details computer using. instance, local computer may MacBook running Catalina. Helpfully, Louis Aslett provides bunch already set - http://www.louisaslett.com/RStudio_AMI/. can either select code region registered , can click link. benefit AMI set-specifically R Studio, however trade-little -dated, compiled May 2019.next step can choose powerful computer . free tier fairly basic computer, can choose better ones need . point can pretty much just launch instance. start using AWS seriously look different security settings.instance now running. can go pasting ‘public DNS’ browser. username ‘rstudio’ password instance ID.R Studio running, exciting. first thing probably change default password using instructions instance.don’t need install, say, tidyverse, instead can just call library keep going. can see list packages installed installed.packages(). instance, rstan already installed. can use GPUs want.Perhaps important able start AWS instance able stop (don’t get billed). free tier pretty great, need turn . stop instance, AWS instances page, select , ‘Actions -> Instance State -> Terminate.’","code":""},{"path":"using-the-cloud.html","id":"google-compute-engine","chapter":"19 Using the cloud","heading":"19.4 Google Compute Engine","text":"main R package related Google Compute Engine seems : googleComputeEngineR.reading Grant McDermott pretty good walk-.","code":""},{"path":"using-the-cloud.html","id":"azure","chapter":"19 Using the cloud","heading":"19.5 Azure","text":"bunch R packages related Azure : https://github.com/Azure/AzureR.","code":""},{"path":"using-the-cloud.html","id":"exercises-and-tutorial-18","chapter":"19 Using the cloud","heading":"19.6 Exercises and tutorial","text":"","code":""},{"path":"using-the-cloud.html","id":"exercises-18","chapter":"19 Using the cloud","heading":"19.6.1 Exercises","text":"","code":""},{"path":"using-the-cloud.html","id":"tutorial-18","chapter":"19 Using the cloud","heading":"19.6.2 Tutorial","text":"","code":""},{"path":"deploying-models.html","id":"deploying-models","chapter":"20 Deploying models","heading":"20 Deploying models","text":"STATUS: construction.Required readingChip Huyen, 2020, ‘Machine learning going real-time,’ 27 December, https://huyenchip.com/2020/12/27/real-time-machine-learning.html.Required viewingBlair, James, 2019, ‘Democratizing R Plumber APIs,’ RStudio Conference, 24 January, https://www.rstudio.com/resources/rstudioconf-2019/democratizing-r--plumber-apis/.Nolis, Heather, Jacqueline Nolis, ‘’re hitting R million times day made talk ,’ RStudio Conference, 30 January, https://www.rstudio.com/resources/rstudioconf-2020/-re-hitting-r--million-times--day---made--talk--/.Recommended readingKey concepts/skills/etcPutting models production requires different set skills building model. need familiarity cloud provider, APIs, course modelling. biggest difficulty, , getting things set-.Key librariesplumbershinyKey functionsQuiz","code":""},{"path":"deploying-models.html","id":"introduction-21","chapter":"20 Deploying models","heading":"20.1 Introduction","text":"key troupe R ’s production. ’m convince one way another, however section go bunch different tools allow lot R wanted. topics cover :SQL databases.Docker.Plumber APIs models.ShinyPackagesThe general idea need know whole workflow. point, ’ve able scrape data website, bring order chaos, make charts, appropriately model , write . academic settings enough. many industry settings ’re going want use model something. instance, set-website allows model used generate insurance quote given several inputs.One way deploy model use Shiny, seen examples earlier notes. enables individual use model. doesn’t really scale well. instance, wanted sell model forecasts businesses, might way like users interact results. general problem want model results available machines want make APIs.","code":""},{"path":"deploying-models.html","id":"packages","chapter":"20 Deploying models","heading":"20.2 Packages","text":"point ’ve largely using R Packages things us. However, another way loadedDoSS Toolkit","code":""},{"path":"deploying-models.html","id":"shiny-1","chapter":"20 Deploying models","heading":"20.3 Shiny","text":"","code":""},{"path":"deploying-models.html","id":"plumber-and-model-apis","chapter":"20 Deploying models","heading":"20.4 Plumber and model APIs","text":"","code":""},{"path":"deploying-models.html","id":"hello-toronto","chapter":"20 Deploying models","heading":"20.4.1 Hello Toronto","text":"general idea behind plumber package (Schloerke Allen 2021) can train model make available via API can call want forecast. ’s pretty great.Just get something working, let’s make function returns ‘Hello Toronto’ regardless output. Open new R file, add following, save ‘plumber.R’ (may need install plumber package ’ve done yet).saved, top right editor get button ‘Run API.’ Click , API load. ’ll ‘Swagger’ application, provides GUI around API. Expand GET method, clik ‘Try ’ ‘Execute.’ response body, get ‘Toronto.’closely reflect fact API designed computers, can copy/paste ‘request HTML’ browser return ‘Hello Toronto.’","code":"\nlibrary(plumber)\n\n#* @get /print_toronto\nprint_toronto <- function() {\n  result <- \"Hello Toronto\"\n  return(result)\n}"},{"path":"deploying-models.html","id":"local-model","chapter":"20 Deploying models","heading":"20.4.2 Local model","text":"Now, ’re going update API serves model output, given input. ’re going follow Buhr (2017) fairly closely.point, ’d recommend starting new R Project. get started, let’s simulate data train model . case ’re interested forecasting long baby may sleep overnight, given know long slept afternoon nap.Let’s now use tidymodels quickly make dodgy model.point, model. One difference might used ’ve saved model ‘.rds’ file. going read .Now model want put file use API access, called ‘plumber.R.’ also want file sets API, called ‘server.R.’ make R script called ‘server.R’ add following content:‘plumber.R’ add following content:, save ‘plumber.R’ file option ‘Run API.’ Click can try API locally way .","code":"\nlibrary(tidyverse)\nset.seed(853)\n\nnumber_of_observations <- 1000\n\nbaby_sleep <- \n  tibble(afternoon_nap_length = rnorm(number_of_observations, 120, 5) %>% abs(),\n         noise = rnorm(number_of_observations, 0, 120),\n         night_sleep_length = afternoon_nap_length * 4 + noise,\n         )\n\nbaby_sleep %>% \n  ggplot(aes(x = afternoon_nap_length, y = night_sleep_length)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Baby's afternoon nap length (minutes)\",\n       y = \"Baby's overnight sleep length (minutes)\") +\n  theme_classic()\nset.seed(853)\nlibrary(tidymodels)\n\nbaby_sleep_split <- rsample::initial_split(baby_sleep, prop = 0.80)\nbaby_sleep_train <- rsample::training(baby_sleep_split)\nbaby_sleep_test <- rsample::testing(baby_sleep_split)\n\nmodel <- \n  parsnip::linear_reg() %>%\n  parsnip::set_engine(engine = \"lm\") %>% \n  parsnip::fit(night_sleep_length ~ afternoon_nap_length, \n               data = baby_sleep_train\n               )\n\nwrite_rds(x = model, file = \"baby_sleep.rds\")\nlibrary(plumber)\n\nserve_model <- plumb(\"plumber.R\")\nserve_model$run(port = 8000)\nlibrary(plumber)\nlibrary(tidyverse)\n\nmodel <- readRDS(\"baby_sleep.rds\")\n\nversion_number <- \"0.0.1\"\n\nvariables <- \n  list(\n    afternoon_nap_length = \"A value in minutes, likely between 0 and 240.\",\n    night_sleep_length = \"A forecast, in minutes, likely between 0 and 1000.\"\n  )\n\n#* @param afternoon_nap_length\n#* @get /survival\npredict_sleep <- function(afternoon_nap_length=0) {\n  afternoon_nap_length = as.integer(afternoon_nap_length)\n  \n  payload <- data.frame(afternoon_nap_length=afternoon_nap_length)\n  \n  prediction <- predict(model, payload)\n\n  result <- list(\n    input = list(payload),\n    response = list(\"estimated_night_sleep\" = prediction),\n    status = 200,\n    model_version = version_number)\n\n  return(result)\n}"},{"path":"deploying-models.html","id":"cloud-model","chapter":"20 Deploying models","heading":"20.4.3 Cloud model","text":"point, ’ve got API working machine, really want get working computer API can accessed anyone. going use DigitalOcean - https://www.digitalocean.com. charged service, create account, come $100 credit, enough get started.set-process pain take time, need . Install two additional packages assist :plumberDeploy (J. Allen 2021).analogsea (Chamberlain, Wickham, Chang 2021).Now need connect local computer DigitalOcean account. Get started :Now need authenticate connection done using SSH public key. can using:first time may useful visual process, case follow instructions : https://docs.digitalocean.com/products/droplets/-/add-ssh-keys/-account/. want ‘.pub’ file computer. copy public key aspect file, add SSH keys section account security settings. key local computer can check using:, take validate. DigitalOcean calls every computer start ‘droplet.’ start three computers, ’ll started three droplets. can check droplets running using:everything set-properly, print information droplets associated account (point, probably none).create droplet, run:’ll get asked SSH passphrase ’ll just set-bunch things. ’re going need install whole bunch things onto droplet:finally set-(’ll seriously take 30 min ) can deploy API!","code":"\ninstall.packages(\"plumberDeploy\")\nremotes::install_github(\"sckott/analogsea\")\nanalogsea::account()\nanalogsea::key_create()\nssh::ssh_key_info()\nanalogsea::droplets()\nid <- plumberDeploy::do_provision(example = FALSE)\nanalogsea::install_r_package(droplet = id, c(\"plumber\", \n                                             \"remotes\", \n                                             \"here\"))\nanalogsea::debian_apt_get_install(id, \"libssl-dev\", \n                                  \"libsodium-dev\", \n                                  \"libcurl4-openssl-dev\")\nanalogsea::debian_apt_get_install(id, \n                                  \"libxml2-dev\")\n\nanalogsea::install_r_package(id, c(\"config\",\n                                   \"httr\",\n                                   \"urltools\",\n                                   \"plumber\"))\n\nanalogsea::install_r_package(id, c(\"xml2\"))\nanalogsea::install_r_package(id, c(\"tidyverse\"))\n\nanalogsea::install_r_package(id, c(\"tidymodels\"))\nplumberDeploy::do_deploy_api(droplet = id, \n                             path = \"example\", \n                             localPath = getwd(), \n                             port = 8000, \n                             docs = TRUE, \n                             overwrite=TRUE)"},{"path":"deploying-models.html","id":"exercises-and-tutorial-19","chapter":"20 Deploying models","heading":"20.5 Exercises and tutorial","text":"","code":""},{"path":"deploying-models.html","id":"exercises-19","chapter":"20 Deploying models","heading":"20.5.1 Exercises","text":"","code":""},{"path":"deploying-models.html","id":"tutorial-19","chapter":"20 Deploying models","heading":"20.5.2 Tutorial","text":"","code":""},{"path":"efficiency.html","id":"efficiency","chapter":"21 Efficiency","heading":"21 Efficiency","text":"STATUS: construction.Required readingRequired viewingRecommended readingKey concepts/skills/etcKey librariesKey functionsQuiz","code":""},{"path":"efficiency.html","id":"introduction-22","chapter":"21 Efficiency","heading":"21.1 Introduction","text":"","code":""},{"path":"efficiency.html","id":"data-efficiency","chapter":"21 Efficiency","heading":"21.2 Data efficiency","text":"","code":""},{"path":"efficiency.html","id":"sql","chapter":"21 Efficiency","heading":"21.2.1 SQL","text":"","code":""},{"path":"efficiency.html","id":"feather","chapter":"21 Efficiency","heading":"21.2.2 Feather","text":"","code":""},{"path":"efficiency.html","id":"code-efficiency","chapter":"21 Efficiency","heading":"21.3 Code efficiency","text":"large, worrying performance waste time. part far better , just pushing things cloud, letting run reasonable time, using time worry aspects pipeline. However, eventually becomes unfeasible. , something takes day run just becomes pain. rarely common area obvious performance gains. Instead need learn measure cut.fast valuable ’s mostly able iterate fast code running fast. find speed code completes bottle neck shard. throw machines . shard . throw machines .","code":""},{"path":"efficiency.html","id":"code-refactoring","chapter":"21 Efficiency","heading":"21.4 Code refactoring","text":"baby examples, focused data science, along lines : https://indrajeetpatil.github.io/Refactoring-ggstatsplot/refactoring-ggstatsplot#1Start example bad code, gets fixed.","code":""},{"path":"efficiency.html","id":"measure","chapter":"21 Efficiency","heading":"21.4.1 Measure","text":"Using tic() tic().Measuring","code":""},{"path":"efficiency.html","id":"experimental-efficiency","chapter":"21 Efficiency","heading":"21.5 Experimental efficiency","text":"Multi-armed bandit","code":""},{"path":"efficiency.html","id":"other-languages","chapter":"21 Efficiency","heading":"21.6 Other languages","text":"","code":""},{"path":"efficiency.html","id":"python","chapter":"21 Efficiency","heading":"21.6.1 Python","text":"","code":""},{"path":"efficiency.html","id":"julia","chapter":"21 Efficiency","heading":"21.6.2 Julia","text":"","code":""},{"path":"efficiency.html","id":"exercises-and-tutorial-20","chapter":"21 Efficiency","heading":"21.7 Exercises and tutorial","text":"","code":""},{"path":"efficiency.html","id":"exercises-20","chapter":"21 Efficiency","heading":"21.7.1 Exercises","text":"","code":""},{"path":"efficiency.html","id":"tutorial-20","chapter":"21 Efficiency","heading":"21.7.2 Tutorial","text":"","code":""},{"path":"concludingremarks.html","id":"concludingremarks","chapter":"22 Concluding remarks","heading":"22 Concluding remarks","text":"STATUS: construction.Required materialWatch Wrong ! 30+ Years Statistical Mistakes, (Gelman 2021).","code":""},{"path":"concludingremarks.html","id":"concluding-remarks","chapter":"22 Concluding remarks","heading":"22.1 Concluding remarks","text":"old saying, something along lines ‘may live interesting times.’ sure every generation feels , sure live interesting times. book, tried convey essentials think allow contribute. just getting started.35 ‘data science didn’t exist undergraduate’ generation. little decade data science gone something barely existed defining part academia industry. imply ? may imply one just making decisions optimize data science looks like right now, also happen. ’s little difficult, ’s also one things makes data science exciting. might mean choices like:taking courses fundamentals, just fashionable applications;reading books, just whatever trending; andtrying intersection least different areas, rather hyper-specialized.just someone likes play data using R. decade ago wouldn’t fit particular department. lucky days space data science someone like . nice thing now call data science ’s space well.Data science needs diversity. Data science needs intelligence enthusiasm. needs room, able make contributions. live interesting times ’s just exciting time enthusiastic data. can’t wait see build.","code":""},{"path":"concludingremarks.html","id":"some-issues","chapter":"22 Concluding remarks","heading":"22.2 Some issues","text":"write unit tests data science?UPDATE add functional tests stuffOne thing working real computer scientists taught importance unit tests. Basically just means writing small checks heads time. Like column purports year, ’s unlikely ’s character, ’s unlikely ’s integer larger 2500, ’s unlikely ’s negative integer. know , writing unit tests us write .case ’s obvious unit test looks like. generally, often little idea results look like ’re running well. approach taken add simulation—simulate reasonable results, write unit tests based , bring real data bear adjust necessary. really think need extensive work area current state---art lacking.happened machine learning revolution?don’t understand happened promised machine learning revolution social sciences. Specifically, yet see convincing application machine learning methods designed prediction social sciences problem care understanding. like either see evidence definitive thesis can’t happen. current situation untenable folks, especially fields historically female, made feel inferior even though results worse.think power?someone learnt statistics economists, now partly statistics department, think everyone learn statistics statisticians. isn’t anything economists, conversations statistics department statistical methods used different departments.think problem people outside statistics, treat statistics recipe follow various steps comes cake. regard ‘power’—turns bunch instructions one bothered check—turned oven temperature without checking 180C, ’s fine whatever mess came accepted people evaluating cake didn’t know needed check temperature appropriately set. (ditching analogy right now).know, issue power related broader discussion p-values, basically one taught properly, require changing awful lot teach statistics .e. moving away recipe approach., specific issue people think statistics recipe followed. think ’s trained especially social sciences like political science economics, ’s rewarded. ’s methods . Instead, statistics collection different instruments let us look data certain way. think need revolution , metaphorical tucking one’s shirt.","code":""},{"path":"concludingremarks.html","id":"next-steps","chapter":"22 Concluding remarks","heading":"22.3 Next steps","text":"book covered much ground, toward end , butler Stevens told novel Remains Day (Ishiguro 1989):evening’s best part day. ’ve done day’s work. Now can put feet enjoy .Chances aspects want explore , building foundation established. , accomplished set .new data science start book, next step backfill skipped , recommend T.-. Timbers, Campbell, Lee (2022). , learn R terms data science going Wickham Grolemund (2017). deepen understanding R , go next Wickham (2019a).interested learning causality start Cunningham (2021) Huntington-Klein (2021).interested learn statistics begin McElreath (2020), backfill . . Johnson, Ott, Dogucu (2022) solidify foundation Gelman et al. (2014). probably also backfill fundamentals around probability, starting Wasserman (2005).one next natural step interested learning statistical (come called machine) learning ’s James et al. (2017) followed Friedman, Tibshirani, Hastie (2009).interested sampling next book turn Lohr (2019). deepen understanding surveys experiments, go next Gerber Green (2012) combination Kohavi, Tang, Xu (2020).developing better data visualization skills, begin turning Healy (2018), , develop strong foundations, L. Wilkinson (2005). writing, best turn inward. Force write publish everyday month. . get better. said, useful books, including Caro (2019) S. King (2000).often hear phrase let data speak. Hopefully point understand never happens. can acknowledge ones using data tell stories, strive seek make worthy.","code":""},{"path":"concludingremarks.html","id":"exercises-and-tutorial-21","chapter":"22 Concluding remarks","heading":"22.4 Exercises and tutorial","text":"","code":""},{"path":"concludingremarks.html","id":"exercises-21","chapter":"22 Concluding remarks","heading":"22.4.1 Exercises","text":"","code":""},{"path":"concludingremarks.html","id":"tutorial-21","chapter":"22 Concluding remarks","heading":"22.4.2 Tutorial","text":"","code":""},{"path":"concludingremarks.html","id":"paper-6","chapter":"22 Concluding remarks","heading":"22.4.3 Paper","text":"point, Final Paper (Appendix B.7) appropriate.","code":""},{"path":"oh-you-think-and-shoulders-and-datasets.html","id":"oh-you-think-and-shoulders-and-datasets","chapter":"A Oh you think and shoulders and datasets","heading":"A Oh you think and shoulders and datasets","text":"just holding place content developed.","code":""},{"path":"oh-you-think-and-shoulders-and-datasets.html","id":"oh-you-think-we-have-good-data-on-that","chapter":"A Oh you think and shoulders and datasets","heading":"A.1 Oh, you think we have good data on that!","text":"Chapter 8:Oh, think good data ! City boundaries. constitues ‘Atlanta?’ Different definitions - metro, X, Y. (also issue countries boundaries changing time)Chapter 10:Oh, think good data ! One representation reality commonplace, chess. chess board (see Figure X - add photo chess board) 8 x 8 board alternating black white squares. squares denonated unique combination letter (-G) number (1-8). piece unique abbreviation, instance pawns X, knights Y. game recorded player noting move. way entire game can recreated. 2021 World Championship contested Magnus Carlsen Ian Nepomniachtchi. Figure X shows score sheet Game 6. variety reasons game particularly noteworthy, one uncharactertic mistakes Carlsen Nepomniachtchi made. instance, Move 32 Carlsen exploit opportunity; Move 36 different move provided Nepomniachtchi promising endgame (CITATION). One reason may players point game little time remaining—decide moves quickly. sense representation provided game sheet. ‘correct’ representation happened game, necessarily happened.Oh, think good data ! Migration.Oh, think good data ! Weather stationsOh, think good data ! Olympics events. decides scoring?. timing?Oh, think good data ! Personality scores. Myers Briggs Big 5 generally.Oh, think good data ! Cause deathOh, think good data ! Timing","code":""},{"path":"oh-you-think-and-shoulders-and-datasets.html","id":"shoulders-of-giants","chapter":"A Oh you think and shoulders and datasets","heading":"A.2 Shoulders of giants","text":"Chapter 2:Shoulders giants Robert Gentleman Ross IhakaChapter 3:Shoulders giants Hadley WickhamChapter 5:Shoulders giants Xiao-Li MengChapter 8:Shoulders giants Leo GoodmanChapter 9:Shoulders giants Donald B. RubinShoulders giants Marcella AlsanChapter 10:Shoulders giants Barbara BailarChapter 12:Shoulders giants Timnit GebruChapter 13:Shoulders giants Katherine WallmanChapter 14Shoulders giants John TukeyChapter 15:Shoulders giants Daniela WittenChapter 15:Shoulders giants Nancy ReidChapter 15:Shoulders giants Rob TibshiraniChapter 16:Shoulders giants Evelyn KitagawaChapter 17:Shoulders giants Andrew GelmanElizabeth ScottGertrude Mary CoxSimon KuznetsStella Cunliffe","code":""},{"path":"oh-you-think-and-shoulders-and-datasets.html","id":"possible-datasets","chapter":"A Oh you think and shoulders and datasets","heading":"A.3 Possible datasets","text":"https://som.yale.edu/faculty-research/-centers/international-center-finance/dataAlex cooksonDavid Andrew’s bookTidycensushttps://stat.ethz.ch/R-manual/R-patched/library/datasets/html/00Index.htmlhttps://www.historicalstatistics.org/https://data.cityofberkeley.info/browse?limitTo=datasets&utf8https://data.gov.hk/en-datasets/category/educationhttps://data.rijksmuseum.nl/object-metadata/download/World bank https://data.worldbank.org/ eg development indicatorsSouth sea bubbleOECDAer R package paper?CESrPaspaleyCanlangFred - api?538The Economisthttps://pds.nasa.gov/datasearch/subscription-service/SS-Release.shtmlhttps://github.com/BuzzFeedNews/nics-firearm-background-checksThe markupTom CardosoList APIs: https://bookdown.org/paul/apis_for_social_scientists/","code":""},{"path":"papers.html","id":"papers","chapter":"B Papers","heading":"B Papers","text":"","code":""},{"path":"papers.html","id":"paper-one","chapter":"B Papers","heading":"B.1 Paper One","text":"","code":""},{"path":"papers.html","id":"task","chapter":"B Papers","heading":"B.1.1 Task","text":"Working individually entirely reproducible way, please find dataset interest Open Data Toronto write short paper telling story data.Find dataset interest Open Data Toronto download reproducible way using opendatatoronto (Gelfand 2020).Create folder appropriate sub-folders, add GitHub, prepare PDF using R Markdown sections (welcome use starter folder): title, author, date, abstract, introduction, data, references.data section thoroughly precisely discuss source data bias brings (ethical, statistical, otherwise). Comprehensively describe summarize data using text least one graph one table. Graphs must made ggplot2 (Wickham 2016) tables must made knitr (Xie 2021) gt (Iannone, Cheng, Schloerke 2020). Graphs must show actual data, close possible, summary statistics. Make sure cross-reference graphs tables.Add references using bib file. sure reference R R packages use, well dataset. Check referenced everything. Strong submissions draw related literature also reference . various options R Markdown references style; just pick one used .Go back write introduction. two three paragraphs. last paragraph set remainder paper.Add abstract. three four sentences. abstract longer four sentences, need think lot whether long. may fine (always exceptions) probably good reason. abstract must tell reader top-level finding. one thing learn world paper?Add descriptive title.Additional points:sign school paper.Add link GitHub repo using footnote.Check GitHub repo well-organized, add informative README. Comment code. Make sure got least one R script , addition, R Markdown file.Pull together PDF check paper well-written able understood average reader , say, FiveThirtyEight. means allowed use mathematical notation, must explain plain language. statistical concepts terminology must explained. reader someone university education, necessarily someone understands p-value .Check evidence class assignment.Via Quercus, submit PDF.","code":""},{"path":"papers.html","id":"checks","chapter":"B Papers","heading":"B.1.2 Checks","text":"Check included R code raw R output final PDF.Check although probably code R Markdown, make sure least one R script ‘scripts’ folder.Check thoroughly commented code directly creates PDF. ‘knit html’ save PDF. ‘knit Word’ save PDFCheck graphs, tables, text extremely clear, comparable quality FiveThirtyEight.Check date updated.Check entire workflow entirely reproducible.Check typos.","code":""},{"path":"papers.html","id":"faq","chapter":"B Papers","heading":"B.1.3 FAQ","text":"Can use dataset Kaggle instead? , done hard work .use code download dataset, can just manually download ? , entire workflow needs reproducible. Please fix download problem pick different dataset.much write? students submit something two--six-page range, . precise thorough.data apartment blocks/NBA/League Legends ’s ethical bias aspect, ? Please re-read readings better understand bias ethics. really think something, might worth picking different dataset.Can use Python? . already know Python doesn’t hurt learn another language.need cite R, don’t need cite Word? R free statistical programming language academic origins, appropriate acknowledge work others. also important reproducibility.","code":""},{"path":"papers.html","id":"rubric","chapter":"B Papers","heading":"B.1.4 Rubric","text":"","code":""},{"path":"papers.html","id":"previous-examples","chapter":"B Papers","heading":"B.1.5 Previous examples","text":"examples papers well include:2021: Amy Farrow, Morgaine Westin, Rachel Lam.2022: Adam Labas, Alicia Yang, Alyssa Schleifer, Ethan Sansom, Hudson Yuen, Jack McKay, Roy Chan, Thomas D’Onofrio, William Gerecke.","code":""},{"path":"papers.html","id":"paper-two","chapter":"B Papers","heading":"B.2 Paper Two","text":"","code":""},{"path":"papers.html","id":"task-1","chapter":"B Papers","heading":"B.2.1 Task","text":"Working part team three people, please pick paper interest , code data available, published anytime since 2019, American Economic Association journal. journals : ‘American Economic Review,’ ‘AER: Insights,’ ‘AEJ: Applied Economics,’ ‘AEJ: Economic Policy,’ ‘AEJ: Macroeconomics,’ ‘AEJ: Microeconomics,’ ‘Journal Economic Literature,’ ‘Journal Economic Perspectives,’ ‘AEA Papers & Proceedings.’Following Guide Accelerating Computational Reproducibility Social Sciences, please complete replication6 least three graphs, tables, combination, paper, using Social Science Reproduction Platform. Note DOI replication.Working entirely reproducible way conduct reproduction based two three aspects paper, write short paper .\nCreate well-organized folder appropriate sub-folders, add GitHub, prepare PDF using R Markdown sections (welcome use starter folder): title, author, date, abstract, introduction, data, results, discussion, references.\naspects focus paper aspects replicated, need . Follow direction paper, make . means ask slightly different question, answer question slightly different way, still use dataset.\nInclude DOI replication paper link GitHub repo underpins paper.\ndiscussion include three four sub-sections focus point find interesting, another sub-section weaknesses paper next steps paper.\ndiscussion section, relevant section, please sure discuss ethics bias, reference relevant literature.\npaper well-written, draw relevant literature, explain technical concepts. Pitch educated, non-specialist, audience.\nUse appendices supporting, critical, material.\nCode entirely reproducible, well-documented, readable.\nCreate well-organized folder appropriate sub-folders, add GitHub, prepare PDF using R Markdown sections (welcome use starter folder): title, author, date, abstract, introduction, data, results, discussion, references.aspects focus paper aspects replicated, need . Follow direction paper, make . means ask slightly different question, answer question slightly different way, still use dataset.Include DOI replication paper link GitHub repo underpins paper.discussion include three four sub-sections focus point find interesting, another sub-section weaknesses paper next steps paper.discussion section, relevant section, please sure discuss ethics bias, reference relevant literature.paper well-written, draw relevant literature, explain technical concepts. Pitch educated, non-specialist, audience.Use appendices supporting, critical, material.Code entirely reproducible, well-documented, readable.Submit PDF paper.evidence class assignment.","code":""},{"path":"papers.html","id":"checks-1","chapter":"B Papers","heading":"B.2.2 Checks","text":"Check :paper just copy/pasted code original paper, instead used foundation work .paper link associated GitHub repository DOI Social Science Reproduction Platform replication conducted.referenced everything, including R. Strong submissions draw related literature discussion (sections) sure also reference . style references matter, provided consistent.","code":""},{"path":"papers.html","id":"faq-1","chapter":"B Papers","heading":"B.2.3 FAQ","text":"much write? students submit something 10--15-page range, . precise thorough.focus model result? , likely best stay away point, instead focus tables graphs summary explanatory statistics.paper choose language R? replication reproduction code R. need translate code R replication. reproduction work, also R.Can work ? Yes.","code":""},{"path":"papers.html","id":"rubric-1","chapter":"B Papers","heading":"B.2.4 Rubric","text":"","code":""},{"path":"papers.html","id":"previous-examples-1","chapter":"B Papers","heading":"B.2.5 Previous examples","text":"examples papers well past include : Amy Farrow, Laura Cline, Hong Shi, Jia Jia Ji.","code":""},{"path":"papers.html","id":"paper-three","chapter":"B Papers","heading":"B.3 Paper Three","text":"","code":""},{"path":"papers.html","id":"task-2","chapter":"B Papers","heading":"B.3.1 Task","text":"Working teams one three people, entirely reproducible way, please pick one following: Australian General Social Survey, Canadian General Social Survey, European Social Survey, German General Social Survey, US General Social Survey.Focus one aspect survey, obtain data, use tell story.\nCreate well-organized folder appropriate sub-folders, add GitHub, prepare PDF using R Markdown sections (welcome use starter folder): title, author, date, abstract, introduction, data, results, discussion, appendix containing survey, references.\naddition conveying sense dataset interest, data section include, limited :\ndetailed discussion survey methodology, key features, strengths, weaknesses. Including find people take survey; population, frame, sample ; sampling approach took trade-offs may ; non-response; cost.\ndiscussion questionnaire: good bad ?\ndiscussion methodology.\nbecomes detailed push appendix.\n\ndiscussion include three four sub-sections focus point find interesting, another sub-section weaknesses paper next steps paper.\ndiscussion section, relevant section, please sure discuss ethics bias, reference relevant literature.\nUse appendices supporting, critical, material.\nappendix, please include survey use augment general social survey choice. survey distributed manner general social survey, needs stand independently. survey put together using survey platform, link included appendix, well details survey. purpose survey gain additional information topic focus paper, beyond gathered general social survey.\nCode entirely reproducible, well-documented, readable.\nCreate well-organized folder appropriate sub-folders, add GitHub, prepare PDF using R Markdown sections (welcome use starter folder): title, author, date, abstract, introduction, data, results, discussion, appendix containing survey, references.addition conveying sense dataset interest, data section include, limited :\ndetailed discussion survey methodology, key features, strengths, weaknesses. Including find people take survey; population, frame, sample ; sampling approach took trade-offs may ; non-response; cost.\ndiscussion questionnaire: good bad ?\ndiscussion methodology.\nbecomes detailed push appendix.\ndetailed discussion survey methodology, key features, strengths, weaknesses. Including find people take survey; population, frame, sample ; sampling approach took trade-offs may ; non-response; cost.discussion questionnaire: good bad ?discussion methodology.becomes detailed push appendix.discussion include three four sub-sections focus point find interesting, another sub-section weaknesses paper next steps paper.discussion section, relevant section, please sure discuss ethics bias, reference relevant literature.Use appendices supporting, critical, material.appendix, please include survey use augment general social survey choice. survey distributed manner general social survey, needs stand independently. survey put together using survey platform, link included appendix, well details survey. purpose survey gain additional information topic focus paper, beyond gathered general social survey.Code entirely reproducible, well-documented, readable.Submit PDF paper.paper well-written, draw relevant literature, explain technical concepts. Pitch educated, non-specialist, audience. use survey, sampling, observational, statistical terminology, need explain . work flow easy follow understand. communicate well, anyone university level able read report relay back methodology, overall results, findings, weaknesses, next steps without confusion.evidence class assignment.","code":""},{"path":"papers.html","id":"checks-2","chapter":"B Papers","heading":"B.3.2 Checks","text":"Check :appendix included link survey included details .","code":""},{"path":"papers.html","id":"faq-2","chapter":"B Papers","heading":"B.3.3 FAQ","text":"focus ? may focus year, aspect, geography reasonable given focus constraints general social survey interested . Please consider year topics interested together, years surveys tend focus particular topics.\n\n","code":""},{"path":"papers.html","id":"rubric-2","chapter":"B Papers","heading":"B.3.4 Rubric","text":"","code":""},{"path":"papers.html","id":"previous-examples-2","chapter":"B Papers","heading":"B.3.5 Previous examples","text":"examples papers well past include :","code":""},{"path":"papers.html","id":"paper-four","chapter":"B Papers","heading":"B.4 Paper Four","text":"","code":""},{"path":"papers.html","id":"task-3","chapter":"B Papers","heading":"B.4.1 Task","text":"Paper data cleaning preparation: include data sheet.","code":""},{"path":"papers.html","id":"paper-five","chapter":"B Papers","heading":"B.5 Paper Five","text":"","code":""},{"path":"papers.html","id":"task-4","chapter":"B Papers","heading":"B.5.1 Task","text":"Paper causal inference.Paper causal inference.must include DAG (probably model section).must include DAG (probably model section).","code":""},{"path":"papers.html","id":"paper-six","chapter":"B Papers","heading":"B.6 Paper Six","text":"","code":""},{"path":"papers.html","id":"task-5","chapter":"B Papers","heading":"B.6.1 Task","text":"primary goal paper predict overall popular vote 2020 American presidential election using multilevel regression post-stratification.","code":""},{"path":"papers.html","id":"recommended-steps","chapter":"B Papers","heading":"B.6.2 Recommended steps","text":"expect work part group three people. suggested split work based 3-person group, just suggestions.Individual-level survey data:\nRequest access Democracy Fund + UCLA Nationscape ‘Full Data Set’: https://www.voterstudygroup.org/publication/nationscape-data-set. take day two. Please start early.\nGiven expense collecting data, privilege access , don’t properly cite dataset get zero problem set.\naccess pick survey interest. use “ns20200102.dta” example (number may different).\nlarge file share. push GitHub (use .gitignore file - see : https://carpentries-incubator.github.io/git-Rstudio-course/02-ignore/index.html).\nUse example R code get started preparing dataset, go cleaning preparing based need.\nMake graphs tables survey data write beautiful sentences paragraphs explaining everything.\nRequest access Democracy Fund + UCLA Nationscape ‘Full Data Set’: https://www.voterstudygroup.org/publication/nationscape-data-set. take day two. Please start early.Given expense collecting data, privilege access , don’t properly cite dataset get zero problem set.access pick survey interest. use “ns20200102.dta” example (number may different).large file share. push GitHub (use .gitignore file - see : https://carpentries-incubator.github.io/git-Rstudio-course/02-ignore/index.html).Use example R code get started preparing dataset, go cleaning preparing based need.Make graphs tables survey data write beautiful sentences paragraphs explaining everything.Post-stratification data:\nuse American Community Surveys (ACS).\nPlease create account IPUMS: https://usa.ipums.org/usa/index.shtml\nwant 2018 1-year ACS. need select variables. depend want model survey data, options include: REGION, STATEICP, AGE, SEX, MARST, RACE, HISPAN, BPL, CITIZEN, EDUC, LABFORCE, INCTOT. look around see interested , remembering need establish correspondence survey.\nDownload relevant post-stratification data (probably easiest change data format .dta). , can take time. Please start early.\nlarge file share. push GitHub (use .gitignore file - see : https://carpentries-incubator.github.io/git-Rstudio-course/02-ignore/index.html).\nGiven expense collecting data, privilege access , don’t properly cite dataset get zero problem set.\nClean prepare post-stratification dataset.\nRemember need cell counts sub-populations model. See examples readings.\nuse American Community Surveys (ACS).Please create account IPUMS: https://usa.ipums.org/usa/index.shtmlYou want 2018 1-year ACS. need select variables. depend want model survey data, options include: REGION, STATEICP, AGE, SEX, MARST, RACE, HISPAN, BPL, CITIZEN, EDUC, LABFORCE, INCTOT. look around see interested , remembering need establish correspondence survey.Download relevant post-stratification data (probably easiest change data format .dta). , can take time. Please start early.large file share. push GitHub (use .gitignore file - see : https://carpentries-incubator.github.io/git-Rstudio-course/02-ignore/index.html).Given expense collecting data, privilege access , don’t properly cite dataset get zero problem set.Clean prepare post-stratification dataset.Remember need cell counts sub-populations model. See examples readings.(may efficient start simulated data waiting real data) Modelling.\nwant explain vote intention based variety explanatory variables. Construct vote intention variable binary (either ‘supports Trump’ ‘supports Biden’).\nwelcome use lm() need explain nuances decision model section (Hint: start : https://statmodeling.stat.columbia.edu/2020/01/10/linear--logistic-regression--binary-outcomes/).\nsaid, probably use logistic regression possible . don’t know start look (increasing levels complexity) glm(), lme4::glmer(), brms::brm(). examples readings.\nThink deeply model fit, diagnostics, similar things need order convince someone model appropriate.\nflexibility model use, (hence cells need create next). general, cells better, may want fewer cells simplicity writing process ensure decent sample cell.\nApply trained model post-stratification dataset make best estimate election result can. specifics depend modelling approach likely involve predict(), add_predicted_draws(), similar. See examples readings. primarily interested distribution forecast overall Presidential popular vote, explanatory variables affect . great submissions go beyond . Also, ’re taking statistics course, just gave central estimate nothing else, great.\nCreate beautiful graphs tables model results.\nCreate wonderful paragraphs talking explaining everything.\nwant explain vote intention based variety explanatory variables. Construct vote intention variable binary (either ‘supports Trump’ ‘supports Biden’).welcome use lm() need explain nuances decision model section (Hint: start : https://statmodeling.stat.columbia.edu/2020/01/10/linear--logistic-regression--binary-outcomes/).said, probably use logistic regression possible . don’t know start look (increasing levels complexity) glm(), lme4::glmer(), brms::brm(). examples readings.Think deeply model fit, diagnostics, similar things need order convince someone model appropriate.flexibility model use, (hence cells need create next). general, cells better, may want fewer cells simplicity writing process ensure decent sample cell.Apply trained model post-stratification dataset make best estimate election result can. specifics depend modelling approach likely involve predict(), add_predicted_draws(), similar. See examples readings. primarily interested distribution forecast overall Presidential popular vote, explanatory variables affect . great submissions go beyond . Also, ’re taking statistics course, just gave central estimate nothing else, great.Create beautiful graphs tables model results.Create wonderful paragraphs talking explaining everything.(, probably efficient start simulated data/results waiting)\nWrite .\nUsing R Markdown, please write thorough paper analysis compile PDF.\npaper must well-written, draw relevant literature, show statistical skills explaining statistical concepts draw .\npaper must following sections: title, name/s, date, abstract keywords, introduction, data, model, results, discussion, references.\npaper may use appendices supporting, critical, material.\ndiscussion needs substantial. instance, paper 10 pages long discussion least 2.5 pages. discussion, paper must include subsections weaknesses, next steps - must proportion.\nreport must provide link GitHub repo contains everything (apart raw data git ignored share). code must entirely reproducible, documented, readable. repo must well-organized appropriately use folders README files.\ngraphs tables must high standard, well formatted, report ready. clean digestible. Furthermore, label describe table/figure.\ndiscuss datasets (data section) (remember least two datasets discuss) make sure discuss (least):\nkey features, strengths, weaknesses generally.\nsurvey questionnaire - good bad ?\ndiscussion methodology including find people take survey; population, frame, sample ; sampling approach took trade-offs may ; non-response; cost.\njust issues strong submissions consider. Show knowledge. becomes detailed push footnotes appendix.\n\ndataset section probably appropriate place include explanation post-stratification (non-statistical language) strengths weaknesses , although discussion may fit naturally another section. Regardless, sure justify inclusion explanatory variable.\ndiscuss model (model section), must extremely careful spell statistical model using, defining explaining aspect important. (Bayesian model, discussion priors regularization almost always important.) mention software used run model. clear model convergence, model checks, diagnostic issues, although may push details appendix depending detailed get. sampling survey aspects discussed assert modelling decisions make? can convince reader neither overfit underfit data? , becomes detailed push details footnotes appendix.\npresent model results, graphs, figures, etc, results section. section strictly relay results. must include text explaining summary statistics similar. However, interpretation results conclusions drawn results left discussion section.\ndiscussion focus model results, time interpreting , explaining mean. Put context. learn world understood model results? caveats apply? extent model represent small world large world (use language McElreath, Ch 2)? weaknesses opportunities future work? going win election? confident forecast? small large distribution? mean? confident certain states? certain explanatory variables carry weight others? Etc.\nCheck referenced everything. Strong submissions draw related literature discussion (sections) sure also reference . style references matter, must consistent.\ndon’t cite R get zero problem set.\nteam, via Quercus, submit PDF paper. , paper must link associated GitHub repo. must include R Markdown file produced PDF repo.\nRMarkdown file must exactly produce PDF. Don’t edit manually ex post - isn’t reproducible.\nWrite .Using R Markdown, please write thorough paper analysis compile PDF.paper must well-written, draw relevant literature, show statistical skills explaining statistical concepts draw .paper must following sections: title, name/s, date, abstract keywords, introduction, data, model, results, discussion, references.paper may use appendices supporting, critical, material.discussion needs substantial. instance, paper 10 pages long discussion least 2.5 pages. discussion, paper must include subsections weaknesses, next steps - must proportion.report must provide link GitHub repo contains everything (apart raw data git ignored share). code must entirely reproducible, documented, readable. repo must well-organized appropriately use folders README files.graphs tables must high standard, well formatted, report ready. clean digestible. Furthermore, label describe table/figure.discuss datasets (data section) (remember least two datasets discuss) make sure discuss (least):\nkey features, strengths, weaknesses generally.\nsurvey questionnaire - good bad ?\ndiscussion methodology including find people take survey; population, frame, sample ; sampling approach took trade-offs may ; non-response; cost.\njust issues strong submissions consider. Show knowledge. becomes detailed push footnotes appendix.\nkey features, strengths, weaknesses generally.survey questionnaire - good bad ?discussion methodology including find people take survey; population, frame, sample ; sampling approach took trade-offs may ; non-response; cost.just issues strong submissions consider. Show knowledge. becomes detailed push footnotes appendix.dataset section probably appropriate place include explanation post-stratification (non-statistical language) strengths weaknesses , although discussion may fit naturally another section. Regardless, sure justify inclusion explanatory variable.discuss model (model section), must extremely careful spell statistical model using, defining explaining aspect important. (Bayesian model, discussion priors regularization almost always important.) mention software used run model. clear model convergence, model checks, diagnostic issues, although may push details appendix depending detailed get. sampling survey aspects discussed assert modelling decisions make? can convince reader neither overfit underfit data? , becomes detailed push details footnotes appendix.present model results, graphs, figures, etc, results section. section strictly relay results. must include text explaining summary statistics similar. However, interpretation results conclusions drawn results left discussion section.discussion focus model results, time interpreting , explaining mean. Put context. learn world understood model results? caveats apply? extent model represent small world large world (use language McElreath, Ch 2)? weaknesses opportunities future work? going win election? confident forecast? small large distribution? mean? confident certain states? certain explanatory variables carry weight others? Etc.Check referenced everything. Strong submissions draw related literature discussion (sections) sure also reference . style references matter, must consistent.don’t cite R get zero problem set.team, via Quercus, submit PDF paper. , paper must link associated GitHub repo. must include R Markdown file produced PDF repo.\nRMarkdown file must exactly produce PDF. Don’t edit manually ex post - isn’t reproducible.good way work team split work, one person section. people sections rely data (analysis graphs) just simulate waiting person putting together data finish. recommended split , works .","code":""},{"path":"papers.html","id":"checks-3","chapter":"B Papers","heading":"B.6.3 Checks","text":"expected submission well written able understood average reader say 538. means allowed use mathematical notation, must able explain plain English. Similarly, can (hint: ) use survey, sampling, observational, statistical terminology, need explain . average person doesn’t know p-value confidence interval . need explain plain language first time use . work flow easy follow understand. communicate well, anyone university level able read report relay back methodology, overall results, findings, weaknesses, next steps without confusion.recommended (informally) proofread one another’s work - exchange papers another group?Everyone team receives mark.evidence class assignment.","code":""},{"path":"papers.html","id":"faq-3","chapter":"B Papers","heading":"B.6.4 FAQ","text":"","code":""},{"path":"papers.html","id":"rubric-3","chapter":"B Papers","heading":"B.6.5 Rubric","text":"","code":""},{"path":"papers.html","id":"previous-examples-3","chapter":"B Papers","heading":"B.6.6 Previous examples","text":"","code":""},{"path":"papers.html","id":"final-paper","chapter":"B Papers","heading":"B.7 Final paper","text":"","code":""},{"path":"papers.html","id":"task-6","chapter":"B Papers","heading":"B.7.1 Task","text":"Please work individually. Write paper involves original research tell story data.","code":""},{"path":"papers.html","id":"guidance","chapter":"B Papers","heading":"B.7.2 Guidance","text":"Please work individually.various options topics (pick one):\nDevelop research question interest obtain create relevant dataset. option involves developing research question based interests, background, expertise. encourage take option, please discuss plans .\nreproduction, sure use paper foundation rather end--.\nDevelop research question interest obtain create relevant dataset. option involves developing research question based interests, background, expertise. encourage take option, please discuss plans .reproduction, sure use paper foundation rather end--.Everything must entirely reproducible.paper must following sections:\nTitle, date, author, keywords, abstract, introduction, data, model, results, discussion, appendix (optional, supporting, critical, material), reference list.\nmust also include enhancement, either contained linked appendix.\nTitle, date, author, keywords, abstract, introduction, data, model, results, discussion, appendix (optional, supporting, critical, material), reference list.must also include enhancement, either contained linked appendix.paper must well-written, draw relevant literature, show statistical skills explaining statistical concepts.discussion needs substantial. instance, paper 10 pages long discussion least 2.5 pages. discussion, paper must include subsections weaknesses, next steps - must proportion.report must provide link GitHub repo contains everything (apart raw data git ignored share). code must entirely reproducible, documented, readable. repo must well-organized appropriately use folders README files.","code":""},{"path":"papers.html","id":"peer-review-submission","chapter":"B Papers","heading":"B.7.3 Peer review submission","text":"expectations paper high. ’m excited read submit. help achieve standard, initial ‘submission’ can get comments feedback final, actual, submission.Submit initial materials peer-review.\nindividual, via Quercus, submit PDF rough draft Quercus.\nminimum must include:\ntop-matter (title, author (can use pseudonym want), date, keywords, abstract) completely filled .\nfully written Introduction section.\nindividual, via Quercus, submit PDF rough draft Quercus.minimum must include:top-matter (title, author (can use pseudonym want), date, keywords, abstract) completely filled .fully written Introduction section.sections must present paper, don’t filled (e.g. must ‘Data’ heading, don’t need content section).clear - fine later change aspect submit checkpoint.awarded one percentage point just submitting draft meets minimum.point get feedback work (make sure least started thinking project) welcome (, possible) include sections wish get feedback .extensions granted submission since following submission dependent date.","code":""},{"path":"papers.html","id":"conduct-peer-review","chapter":"B Papers","heading":"B.7.4 Conduct peer-review","text":"individual, randomly assigned handful rough drafts provide feedback. three days provide feedback peers.provide feedback one peer receive one percentage point, provide feedback two peers receive two percentage points, provide feedback three () peers receive full three percentage points.feedback must include least five comments (meaningful/useful bullet points). must well-written thoughtful.extensions granted submission since following submission dependent date.Please remember providing feedback help colleagues. comments professional kind. challenging receive criticism. Please remember goal help peers advance writing/analysis. feedback inappropriate standard receive 0.","code":""},{"path":"papers.html","id":"checks-4","chapter":"B Papers","heading":"B.7.5 Checks","text":"TBD","code":""},{"path":"papers.html","id":"faq-4","chapter":"B Papers","heading":"B.7.6 FAQ","text":"Can work part team? . important work entirely . really need work show job applications etc.much write? students submit something 10--16-pages main content, additional pages devoted appendices, . precise thorough.submit initial paper order peer-review? Yes.","code":""},{"path":"papers.html","id":"rubric-4","chapter":"B Papers","heading":"B.7.7 Rubric","text":"","code":""},{"path":"papers.html","id":"previous-examples-4","chapter":"B Papers","heading":"B.7.8 Previous examples","text":"examples papers well past include : Rachael Lam.","code":""},{"path":"references-1.html","id":"references-1","chapter":"References","heading":"References","text":"","code":""}]
