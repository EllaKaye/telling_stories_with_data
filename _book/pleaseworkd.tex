% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
]{krantz}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
  \setmonofont[]{inconsolata}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Telling Stories With Data},
  pdfauthor={Rohan Alexander},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=Blue,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.61,0.61,0.61}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.14,0.14,0.14}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.43,0.43,0.43}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage[bf,singlelinecheck=off]{caption}
\usepackage[scale=.8]{sourcecodepro}

\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\renewenvironment{quote}{\begin{VF}}{\end{VF}}
\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}

\usepackage{makeidx}
\makeindex

\urlstyle{tt}

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\frontmatter
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{siunitx}
\newcolumntype{d}{S[input-symbols = ()]}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage{hhline}
\usepackage{calc}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage{hyperref}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Telling Stories With Data}
\author{Rohan Alexander}
\date{26 January, 2022}

\begin{document}
\maketitle

% you may need to leave a few empty pages before the dedication page

%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage
\thispagestyle{empty}

\begin{center}
For Mum and Dad
%\includegraphics{images/dedication.pdf}
\end{center}

\setlength{\abovedisplayskip}{-5pt}
\setlength{\abovedisplayshortskip}{-5pt}

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoftables
\listoffigures
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}


\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/tellingstorieswithdatapainting} \caption{Telling stories with data}\label{fig:unnamed-chunk-1}
\end{figure}

This book will help you tell stories with data. It establishes a foundation on which you can build and share knowledge about an aspect of the world of interest to you based on data that you observe. Telling stories in small groups around a fire played a critical role in the development of humans and society \citep{wiessner2014embers}. Today our stories, based on data, can influence millions.

In this book we will explore, prod, push, manipulate, knead, and ultimately, try to understand the implications of data. A variety of features drive the choices in this book.

The motto of the university from which I took my PhD is \emph{naturam primum cognoscere rerum} or roughly `learn the first nature of things'. But the original quote continues \emph{temporis aeterni quoniam}, or roughly `for eternal time'. We will do both of these things. I focus on tools, approaches, and workflows that enable you to establish lasting and reproducible knowledge.

When I talk of data in this book, it will typically be related to humans. Humans will be at the centre of most of our stories, and we will tell social, cultural, and economic stories. In particular, throughout this book I will draw attention to inequity both in social phenomena and in data. Most data analysis reflects the world as it is. Many of the least well-off face a double burden in this regard: not only are they disadvantaged, but the extent is more difficult to measure. Respecting those whose data are in our dataset is a primary concern, and so is thinking of those who are systematically not in our dataset.

While data are often specific to various contexts and disciplines, the approaches used to understand them tend to be similar. Data are also increasingly global with resources and opportunities available from a variety of sources. Hence, I draw on examples from many disciplines and geographies.

To become knowledge, our findings must be communicated to, understood, and trusted by other people. Scientific and economic progress can only be made by building on the work of others. And this is only possible if we can understand what they did. Similarly, if we are to create knowledge about the world, then we must enable others to understand precisely what we did, what we found, and how we went about our tasks. As such, in this book I will be particularly prescriptive about communication and reproducibility.

Improving the quality of quantitative work is an enormous challenge, yet it is the challenge of our time. Data are all around us, but there is little enduring knowledge being created. This book hopes to contribute, in some small way, to changing that.

\hypertarget{audience-and-assumed-background}{%
\section*{Audience and assumed background}\label{audience-and-assumed-background}}


The typical person reading this book has some familiarity with first-year statistics, for instance they have run a regression. But it is not targeted at a particular level, instead providing aspects relevant to almost any quantitative course. I have taught from this book at high school, undergraduate, and graduate levels. Everyone has unique needs, but hopefully some aspect of this book speaks to you.

This book especially complements \emph{Statistical Rethinking} \citep{citemcelreath}, \emph{R for Data Science} \citep{r4ds}, \emph{An Introduction to Statistical Learning} \citep{islr}, \emph{Causal Inference: The Mixtape} \citep{Cunningham2021}, and \emph{Building Software Together} \citep{buildingsoftwaretogether}. For instance, after this book you may be interested in learning more about Bayesian statistics, data science, statistical learning, causal inference, and building software.

All of that said, some of the most successful students have been those with no quantitative or coding background. Enthusiasm and interest have taken folks far. If you have those, then don't worry about too much else.

\hypertarget{structure-and-content}{%
\section*{Structure and content}\label{structure-and-content}}


This book is structured around six parts: I) Foundations, II) Communication, III) Acquisition, IV) Preparation, V) Modelling, and VI) Enrichment.

Part I -- Foundations -- begins with Chapter \ref{telling-stories-with-data}, which provides an overview of what I am trying to achieve with this book and why you should read it. Chapter \ref{drinking-from-a-fire-hose} provides some worked examples. The intention of these is that you can experience the full workflow recommended in this book without worrying too much about the specifics of what is happening. That workflow is: plan, simulate, acquire, model, and communicate. It is normal to not follow everything in this chapter, but you should go through it, typing out and executing the code yourself. If you only have time to read one chapter of this book, then I recommend that one. Chapter \ref{r-essentials} goes through some essential tasks in R, which is the statistical programming language used in this book. It is more of a reference chapter, and you may find yourself returning to it from time to time. And Chapter \ref{reproducible-workflows} introduces some key tools for reproducibility used in the workflow that I advocate. These are things like using the command line, R Markdown, R Projects, Git and GitHub, and using R in practice.

Part II -- Communication -- considers three types of communication: written, static, and interactive. Chapter \ref{on-writing} details the features that quantitative writing should have and how to go about writing a crisp, technical, paper. Static communication in Chapter \ref{static-communication} introduces features like graphs, tables, and maps. Interactive communication in Chapter \ref{interactive-communication} covers aspects such as websites, web applications, and maps that can be manipulated.

Part III -- Acquisition -- focuses on three aspects: gathering data, hunting data, and farming data. Gathering data in Chapter \ref{gather-data} covers things like using Application Programming Interface (APIs), scraping data, getting data from PDFs, and Optical Character Recognition (OCR). The idea is that data are available, but not necessarily designed to be datasets, and that we must go and get them. Hunting data in Chapter \ref{hunt-data} covers aspects where more is expected of us. For instance, we may need to conduct an experiment, run an A/B test, or do some surveys. Finally, farming data in Chapter \ref{farm-data} covers datasets that are explicitly provided for us to use as data, for instance censuses and other government statistics. These are typically clean, pre-packaged datasets.

Part IV -- Preparation -- covers how to respectfully transform raw data into something that can be explored and shared. Chapter \ref{cleaning-and-preparing-data} begins by detailing some principles to follow when approaching the task of cleaning and preparing data, and then goes through specific steps to take and checks to implement. Chapter \ref{storing-and-retrieving-data} focuses on methods of storing and retrieving those datasets, including the use of R packages. Chapter \ref{disseminating-and-protecting-data} discusses considerations and steps to take when wanting to disseminate datasets as broadly as possible, while at the same time respecting those whose data they are based on.

Part V -- Modelling -- begins with exploratory data analysis in Chapter \ref{exploratory-data-analysis}. This is the critical process of coming to understand the nature of a dataset, but not something that typically finds itself into the final product. In Chapter \ref{ijalm} the use of statistical models to explore data is introduced. Chapter \ref{causality} is the first of three applications of modelling. It focuses on attempts to make causal claims from observational data and covers approaches such as difference-in-differences, regression discontinuity, and instrumental variables. Chapter \ref{mrp} is the second of the modelling applications chapters and focuses on multilevel regression with post-stratification where we use a statistical model to adjust a sample for known biases. Chapter \ref{text-as-data} is the third and final modelling application and is focused on text-as-data.

Part VI -- Enrichment -- introduces various next steps that would improve aspects of the workflow and approaches introduced in previous chapters. Chapter \ref{using-the-cloud} which goes through moving away from your own computer and toward using the cloud. Chapter \ref{deploying-models} discusses deploying models through the use of packages, web applications, and APIs. Chapter \ref{efficiency} discusses various alternatives to the storage of data including feather and SQL; and also covers some ways to improve the performance of your code. Finally, Chapter \ref{concludingremarks} offers some concluding remarks, details some open problems, and suggests some next steps.

\hypertarget{pedagogy-and-key-features}{%
\section*{Pedagogy and key features}\label{pedagogy-and-key-features}}


You have to do the work. You should actively go through material and code yourself. As \citet{stephenking} says `{[}a{]}mateurs sit and wait for inspiration, the rest of us just get up and go to work'. Do not passively read this book. My role is best described by \citet[p.~2-3]{hamming1996}:

\begin{quote}
I am, as it were, only a coach. I cannot run the mile for you; at best I can discuss styles and criticize yours. You know you must run the mile if the athletics course is to be of benefit to you---hence you must think carefully about what you hear and read in this book if it is to be effective in changing you---which must obviously be the purpose\ldots{}
\end{quote}

This book is structured around a dense 12-week course. It provides enough material for advanced readers to be challenged, while establishing a core that all readers should master. Typically courses cover the material through to Chapter \ref{ijalm}, and then pick another couple of chapters that are of particular interest.

From as early as Chapter \ref{drinking-from-a-fire-hose} you will have a workflow---plan, simulate, acquire, model, and communicate---allowing you to tell a convincing story with data. In each subsequent chapter you will add aspects and depth to this workflow that will allow you to speak with increasing sophistication and credibility. As this workflow expands it addresses the skills that are typically sought in industry. For instance, features such as: communication, ethics, reproducibility, research question development, data collection, data cleaning, data protection and dissemination, exploratory data analysis, statistical modelling, and scaling.

One of the defining aspects of this book is that ethics and inequity concerns are integrated throughout, rather than being clustered in one, easily ignorable, chapter. These aspects are critical, yet it can be difficult to immediately see their value, hence their tight integration.

This book is also designed to enable you to build a portfolio of work that you could show to a potential employer. If you want an industry job, then this is arguably the most important thing that you should be doing. \citet[p.~55]{robinsonnolis2020} describe how a portfolio is a collection of projects that show what you can do and is something that can help be successful in a job search.

In the novel \emph{The Last Samurai} \citep[ p.~326]{helendewitt}, a character says:

\begin{quote}
{[}A{]} scholar should be able to look at any word in a passage and instantly think of another passage where it occurred; \ldots{} {[}so a{]} text was like a pack of icebergs each word a snowy peak with a huge frozen mass of cross-references beneath the surface.
\end{quote}

In an analogous way, this book not only provides you with text and instruction that is self-contained, but also helps develop critical masses of knowledge on which expertise is built. No chapter positions itself as the last word, instead they are written in relation to other work.

Each chapter has the following features:

\begin{itemize}
\tightlist
\item
  A list of required materials that you should go through before you read that chapter. To be clear, you should first read that material and then return to this book. Each chapter also contains recommended materials for those who are particularly interested in the topic and want a starting place for further exploration.
\item
  A summary of the key concepts and skills that are developed in that chapter. Technical chapters additionally contain a list of the main packages and functions that are used in the chapter. The combination of these features acts as a checklist for your learning, and you should return to them after completing the chapter.
\item
  A series of short exercises that you should complete after going through the required materials, but before going through the chapter, to test your knowledge. After completing the chapter, you should go back through the exercises to make sure that you understand each aspect.
\item
  One or two tutorial questions are included at the end of each chapter to further encourage you to actively engage with the material. You could consider forming small groups to discuss your answers to these questions.
\end{itemize}

Some chapters additionally feature:

\begin{itemize}
\tightlist
\item
  A section called `Oh, you think we have good data on that!' which focuses on a particular setting, such as cause of death, in which it is often assumed that there is unimpeachable and unambiguous data but the reality tends to be quite far from that.
\item
  A section called `Shoulders of giants', which focuses on some of those who created the intellectual foundation on which we build.
\end{itemize}

Finally, a set of six papers is included in the appendix. If you write these, you will be conducting original research on a topic that is of interest to you. Although open-ended research may be new to you, the extent to which you are able to: develop your own questions, use quantitative methods to explore them, and communicates your findings, is the measure of the success of this book.

\hypertarget{software-information-and-conventions}{%
\section*{Software information and conventions}\label{software-information-and-conventions}}


The software that I use in this book is R \citep{citeR}. This language was chosen because it is open source, widely used, general enough to cover the entire workflow, yet specific enough to have plenty of well-developed features. I do not assume that you have used R before, and so another reason for selecting R for this book is the community of R users. The community is especially welcoming of new-comers and there is a lot of complementary beginner-friendly material available. There is an R package, \texttt{DoSSToolkit} \citep{alexander2021introduction}, that contains \texttt{learnr} modules \citep{citelearnr}. This may be useful if you are newer to R and are especially complementary to this book.

If you don't have a programming language, then R is a great one to start with. If you have a preferred programming language already, then it wouldn't hurt to pick up R as well. That said, if you have a good reason to prefer another open-source programming language (for instance you use Python daily at work) then you may wish to stick with that. However, all examples in this book are in R.

Please download R and R Studio onto your own computer. You can download R for free here: \url{http://cran.utstat.utoronto.ca/}, and you can download R Studio Desktop for free here: \url{https://rstudio.com/products/rstudio/download/\#download}.
Please also create an account on R Studio Cloud: \url{https://rstudio.cloud/}. This will allow you to run R in the cloud.

Packages are in typewriter text, for instance, \texttt{tidyverse}, while functions are also in typewriter text, but include brackets, for instance \texttt{dplyr::filter()}.

\hypertarget{acknowledgments}{%
\section*{Acknowledgments}\label{acknowledgments}}


Many people generously gave code, data, examples, guidance, opportunities, thoughts, and time, that helped develop this book.

Thank you to David Grubbs and the team at CRC Press for taking a chance on me and providing invaluable support.

Thank you to Michael Chong and Sharla Gelfand for greatly helping to shape some of the approaches I advocate. However, both do much more than that and contribute in an enormous way to the spirit of generosity that characterises the R community.

Thank you to Kelly Lyons for her support, guidance, mentorship, and friendship. Every day she demonstrates what an academic should be, and more broadly, what we should all aspire to be as a person.

Thank you to Greg Wilson for providing a structure to think about teaching, for being the catalyst for this book, and for helpful comments on drafts. Every day he provides an example of how to contribute to the intellectual community.

Thank you to an anonymous reviewer and Isabella Ghement, who thoroughly went through an early draft of this book and provided detailed feedback that improved this book.

Thank you to Hareem Naveed for helpful feedback and encouragement. Her industry experience was an invaluable resource as I grappled with questions of coverage and focus.

Thank you to Leslie Root, who came up with the idea around `Oh, you think we have good data on that!'.

Thank you to my PhD supervisory panel John Tang, Martine Mariotti, Tim Hatton, and Zach Ward who gave me the freedom to explore the intellectual space that was of interest to me, the support to follow through on those interests, and the guidance to ensure that it all resulted in something tangible.

Thank you to Elle Côtè for enabling this book to be written.

This book has greatly benefited from the notes and teaching materials of others that are freely available online, especially: Chris Bail, Scott Cunningham, Andrew Heiss, Lisa Lendway, Grant McDermott, Nathan Matias, David Mimno, and Ed Rubin. Thank you to these folks. The changed norm of academics making their materials freely available online is a great one and one that I hope the free online version of this book helps contribute to.

Thank you to those students who contributed substantially to the development of this book, including: A Mahfouz, Faria Khandaker, Keli Chiu, Paul Hodgetts, and Thomas William Rosenthal. I discussed most aspects of this book with them, and while they made specific contributions, they also changed and sharpened the way that I thought about almost everything covered here. Paul additionally made the art for this book.

Thank you to those students who identified specific improvements, including: Aaron Miller, Amy Farrow, Cesar Villarreal Guzman, Flavia López, Hong Shi, Laura Cline, Lorena Almaraz De La Garza, Mounica Thanam, Reem Alasadi, Wijdan Tariq, and Yang Wu.

Finally, thank you to Monica Alexander. Without you I would not have written a book; I would not have even thought it possible. Thank you for your inestimable help with writing this book, providing the base on which it builds (remember in the library showing me many times how to get certain rows in R!), giving me the time that I needed to write, encouragement when it turned out that writing a book just meant endlessly re-writing that which was perfect the day before, reading everything in this book many times, providing perfect hydration in the form of coffee or cocktails as appropriate, and much more.

You can contact me at: \href{mailto:rohan.alexander@utoronto.ca}{\nolinkurl{rohan.alexander@utoronto.ca}}.

\begin{flushright}
Rohan Alexander\\
Toronto, Canada
\end{flushright}

\hypertarget{about-the-author}{%
\chapter*{About the author}\label{about-the-author}}


Rohan Alexander is an assistant professor at the University of Toronto in Information and Statistical Sciences. He is also the assistant director of CANSSI Ontario, a senior fellow at Massey College, a faculty affiliate at the Schwartz Reisman Institute for Technology and Society, and a co-lead of the Data Sciences Institute Thematic Program in Reproducibility. He holds a PhD in Economics from the Australian National University where he was supervised by John Tang (chair), Martine Mariotti, Tim Hatton, and Zach Ward.

He is interested in using statistical models to try to understand the world. And particularly how we get the data that go into those models; whose data are systematically missing; how we clean, prepare, and tidy data before they are modelled; the effects of all this on the implications of our models; and how we can reproducibly share the totality of this process. He tries to develop students that are skilled not only in using statistical methods across various disciplines, but also appreciate their limitations, and think deeply about the broader contexts of their work.

He enjoys teaching and aims to help students from a wide range of backgrounds learn how to use data to tell convincing stories. He teaches in both the Faculty of Information and the Department of Statistical Sciences at both undergraduate and graduate levels. He is a RStudio Certified Tidyverse Trainer.

He is married to Monica Alexander and they have two children. He probably spends too much money on books, and certainly too much time at libraries. If you have any book recommendations of your own, then he'd love to hear them.

\mainmatter

\hypertarget{part-foundations}{%
\part{Foundations}\label{part-foundations}}

\hypertarget{telling-stories-with-data}{%
\chapter{Telling stories with data}\label{telling-stories-with-data}}

\textbf{Required material}

\begin{itemize}
\tightlist
\item
  Read \emph{Counting the Countless}, \citep{keyes2019}.
\item
  Watch \emph{Data Science Ethics in 6 Minutes}, \citep{register2020}.
\end{itemize}

\hypertarget{on-telling-stories}{%
\section{On telling stories}\label{on-telling-stories}}

Like many parents, when our children were born, one of the first things that my wife and I did regularly was read stories to them. In doing so we carried on a tradition that has occurred for millennia. Myths, fables, and fairy tales can be seen and heard all around us. Not only are they entertaining but they enable us to learn something about the world. While \emph{The Very Hungry Caterpillar} \citep{veryhungry} may seem quite far from the world of dealing with data, there are similarities. Both are trying to tell a story and teaching us something about the world.

When using data, we try to tell a convincing story. It may be as exciting as predicting elections, as banal as increasing internet advertising click rates, as serious as finding the cause of a disease, or as fun as forecasting basketball games. In any case the key elements are the same. The English author, E. M. Forster, described the aspects common to all novels as: story, people, plot, fantasy, prophecy, pattern, and rhythm \citep{forster}. Similarly, when we tell stories with data, there are common concerns, regardless of the setting:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is the dataset? Who generated the dataset and why?
\item
  What is the process that underpins the dataset? Given that process, what is missing from the dataset or been poorly measured? Could other datasets have been generated, and if so, how different could they have been to the one that we have?
\item
  What is the dataset trying to say, and how can we let it say this? What else could it say? How do we decide between these?
\item
  What are we hoping others will see from this dataset, and how can we convince them of this? How much work must we do to convince them? To what extent can we share how we came to believe this?
\item
  Who is affected by the processes and outcomes related to this dataset? To what extent are they represented in the dataset, and have they been part of conducting the analysis?
\end{enumerate}

In the past, certain elements of telling stories with data were easier. For instance, experimental design has a long and robust tradition within agricultural and medical sciences, physics, and chemistry. Student's t-distribution was identified in the early 1900s by a chemist, William Sealy Gosset, who was working at Guinness, a beer manufacturer, and needed to assess the quality of the beer \citep{boland1984biographical}! It would have been possible for him to randomly sample the beer and change one aspect at a time.

Many of the fundamentals of the statistical methods that we use today were developed in such settings. There, it was typically possible to establish control groups and randomize; and in these settings there were fewer ethical concerns. A story told with the resulting data is likely to be fairly convincing.

Unfortunately, little of this applies these days, given the diversity of settings to which statistical methods are applied. On the other hand, we have many advantages. For instance, we have well-developed statistical techniques, easier access to large datasets, and open-source statistical languages such as R \citep{citeR}. But the difficulty of conducting traditional experiments means that we must also turn to other aspects to tell a convincing story.

\hypertarget{workflow-components}{%
\section{Workflow components}\label{workflow-components}}

There are five core components to the workflow needed to tell stories with data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Plan} and sketch an endpoint.
\item
  \textbf{Simulate} some reasonable data and consider that.
\item
  \textbf{Acquire} and prepare the real data.
\item
  \textbf{Explore} and understand that dataset.
\item
  \textbf{Share} what we did and what we found.
\end{enumerate}

We begin by \textbf{planning and sketching an endpoint} because this ensures that we think carefully about where we want to go. It forces us to deeply consider our situation, acts to keep us focused and efficient, and helps reduce scope creep. In \emph{Alice's Adventures in Wonderland} \citep{citealice}, Alice asks the Cheshire Cat which way she should go. The Cheshire Cat replies by asking where Alice would like to go. And when Alice replies that she does not mind, so long as she gets somewhere, the Cheshire Cat says then the direction does not matter because you will always get somewhere if you `walk long enough'. The issue, in our case, is that we typically cannot afford to walk aimlessly for long. While it may be that the endpoint needs to change, it is important that this is a deliberate, reasoned, decision. And that is only possible given an initial target. There is no need to spend too much time on this to get a lot of value from it. Often five minutes with paper and pen, is enough.

The next step is to \textbf{simulate data} because that forces us into the details. It helps with cleaning and preparing the dataset because it focuses us on the classes in the dataset and the distribution of the values that we expect. For instance, if we were interested in the effect of age-groups on political preferences, then we may expect that our age-group column would be a factor, with four possible values: `18-29', `30-44', `45-59', `60+'. The process of simulation provides us with clear features that our real dataset should satisfy. We could use these features to define tests that would guide our data cleaning and preparation. For instance, we could check our real dataset for age-groups that are not one of those four values. When those tests pass, we could be confident that our age-group column only contains values that we expect.

Simulating data is also important when we turn to the statistical modelling stage. When we are at that stage, we are concerned with whether the model reflects what is in the dataset. The issue is that if we go straight to modelling the real dataset, then we do not know whether we have a problem with our model. We simulate data so that we precisely know the underlying data generation process. We then apply the model to that simulated dataset. When we get out what we put in, then we know that our model is performing appropriately, and can turn to the real dataset. Without that initial application to simulated data, it would be more difficult to have confidence in our model.

Simulation is often cheap---almost free given modern computing resources and statistical programming languages---and fast. It provides `an intimate feeling for the situation' \citep[p.~239]{hamming1996}. The way to proceed is to start with a simulation that just contains the essentials, get that working, and to then complicate it.

\textbf{Acquiring and preparing the data} that we are interested in is an often-overlooked stage of the workflow. This is surprising because it can be one of the most difficult stages and requires many decisions to be made. This is increasingly the subject of research. For instance, it has been found that decisions made during this stage greatly affect statistical results \citep{huntington2021influence}.

At this stage of the workflow, it is common to feel a little overwhelmed. Typically, the data we can acquire leave us a little scared. There may be too little of it, in which case we worry about how we are going to be able to make our statistical machinery work. Alternatively, we may have the other problem and be worried about how we can even begin to deal with such a large amount of data.

\begin{quote}
Perhaps all the dragons in our lives are princesses who are only waiting to see us act, just once, with beauty and courage. Perhaps everything that frightens us is, in its deepest essence, something helpless that wants our love.

\citet{rilke}
\end{quote}

Developing comfort in this stage of the workflow unlocks the rest of it. The dataset that is needed to tell a convincing story is in there, but we need to iteratively remove everything that is not the data that we need, and to then shape that which is.

After we have a dataset, we then want to \textbf{explore and understand } certain relationships in that dataset. The use of statistical models to understand the implications of our data is not free of bias, nor are they `truth'; they do what we tell them to do. Within a workflow to tell stories with data, statistical models are tools and approaches that we use to explore our dataset, in the same way that we may use graphs and tables. They are not something that will provide us with a definitive result but will enable us to understand the dataset more clearly in a particular way.

By the time we get to this step in the workflow, to a large extent, the model will reflect the decisions that were made in earlier stages, especially acquisition and cleaning, as much as it reflects any type of underlying process. Sophisticated modelers know that their statistical models are like the bit of the iceberg above the surface; they build on, and are only possible due to, the majority that is underneath, in this case, the data. But when an expert at the whole workflow uses modelling, they recognize that the results that are obtained are additionally due to choices about whose data matters, decisions about how to measure and record the data, and other aspects that reflect the world as it is, well before that data is available to their specific workflow.

Finally, we must \textbf{share} what we did and what we found, at as high a fidelity as is possible. Talking about knowledge that only you have, does not make you knowledgeable, and that includes knowledge that only `past you' has. When communicating, we need to be clear about what decisions we made, why we made them, our findings, and the weaknesses of our approach. We are aiming to uncover something important (otherwise, why bother) so we write down everything, in the first instance, although this written communication may be supplemented with other forms of communication later. There are so many decisions that we must make in this workflow that we want to be sure that we are open about the entire thing---start to finish. This means much more than just the statistical modelling and creation of the graphs and tables, but everything. Without this, stories based on data do not have any credibility.

The world is not a rational meritocracy where everything is carefully and judiciously evaluated. Instead, we use shortcuts, hacks, and heuristics, based on our experience. Unclear communication will render even the best work moot, because it will not be thoroughly engaged with. While there is a minimum when it comes to communication, there is no upper limit to how impressive it can be. When it is the culmination of a thought-out workflow, at best, it obtains a certain \emph{sprezzatura}, or studied carelessness. Achieving such mastery, is the work of years.

\hypertarget{telling-stories-with-data-1}{%
\section{Telling stories with data}\label{telling-stories-with-data-1}}

A compelling story based on data can likely be told in around ten-to-twenty pages. Much less than this, and it is likely too light on some of the details. And while it is easy to write much more, often some reflection enables succinctness or for multiple stories to be separated. The best stories are typically based on research and independent learning.

It is possible to tell convincing stories even when it is not possible to conduct traditional experiments. These approaches do not rely on `big data'---which is not a panacea \citep{meng2018statistical}---but instead on better using the data that are available. A blend of theory and application, combined with practical skills, a sophisticated workflow, and an appreciation for what one does not know, is often enough to create lasting knowledge.

The best stories based on data tend to be multi-disciplinary. They take from whatever field they need to, but almost always draw on: statistics, data visualization, computer science, experimental design, economics, engineering, and information science (to name a few). As such, an end-to-end workflow requires a blend of skills from these areas. The best way to learn these skills is to use real-world data to conduct research projects where you:

\begin{itemize}
\tightlist
\item
  obtain and clean relevant datasets;
\item
  develop research questions;
\item
  use statistical techniques to explore those questions; and
\item
  communicate in a meaningful way.
\end{itemize}

The key elements of telling convincing stories with data are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Communication.
\item
  Ethics.
\item
  Reproducibility.
\item
  Questions.
\item
  Measurement.
\item
  Data collection.
\item
  Data cleaning.
\item
  Exploratory data analysis.
\item
  Modelling.
\item
  Scaling.
\end{enumerate}

These elements are the foundation on which the workflow are built (Figure \ref{fig:iceberg}).

\begin{figure}

{\centering \includegraphics[width=0.85\linewidth]{/Users/rohanalexander/Documents/book/figures/IMG_1820} 

}

\caption{The workflow builds on various elements}\label{fig:iceberg}
\end{figure}

This is a lot to master, but \textbf{communication} is the most important. Simple analysis, communicated well, is more valuable than complicated analysis communicated poorly. This is because the latter cannot be understood or trusted by others. A lack of clear communication sometimes reflects a failure by the researcher to understand what is going on, or even what they are doing. And so, while the level of the analysis should match the dataset, instruments, task, and skillset, when a trade-off is required between clarity and complication, it can be sensible to err on the side of clarity.

Clear communication means writing in plain language, with the help of tables, graphs, and technical terms, in a way that brings the audience along with you. It means setting out what was done and why, as well as what was found. The minimum hurdle is doing this in a way that enables another person to independently do what you did and find what you found. One challenge is that as you immerse yourself in the data, it can be difficult to remember what it was like when you first came to it. But that is where most of your audience will be coming from. Learning to provide an appropriate level of nuance and detail is especially difficult but is made easier by trying to write for the audience's benefit.

Active consideration of \textbf{ethics} is needed because the dataset likely concerns humans. This means considering things like: who is in the dataset, who is missing, and why? To what extent will our story perpetuate the past? And is this something that ought to happen? Even if the dataset does not concern humans, the story is likely being put together by humans, and we affect almost everything else. This means we have a moral responsibility to use data ethically, with concern for environmental impact, and inequity.

There are many definitions of ethics, but when it comes to telling stories with data, at a minimum it means considering the full context of the dataset \citep{datafeminism2020}. In jurisprudence, a textual approach to law means literally considering the words of the law as they are printed, while a purposive approach means laws are interpreted within a broader context. An ethical approach to telling stories with data means adopting the latter approach, and considering the social, cultural, historical, and political forces that shape our world, and hence our data \citep{crawford}.

\textbf{Reproducibility} is required to create lasting knowledge about the world. It means that everything that was done---all of it, end-to-end---can be independently redone. Ideally, autonomous end-to-end reproducibility is possible; anyone can get the code, data, and environment, to verify everything that was done. Unfettered access to code is almost always possible. While that is the default for data also, it is not always reasonable. For instance, studies in psychology may have small, personally identifying, samples. One way forward is to openly share simulated data with similar properties, along with defining a process by which the real data could be accessed, given appropriate \emph{bona fides}.

Curiosity provides internal motivation to explore a dataset, and associated process, to a proper extent. \textbf{Questions} tend to beget questions, and these usually improve and refine as the process of coming to understand a dataset carries on. In contrast to the stock Popperian approach to hypothesis testing often taught, questions are typically developed through a continuous and evolving process \citep{franklin2005exploratory}. It can be difficult to find an initial question. Selecting an area of interest can help, as can sketching a broad claim with the intent of evolving it into a specific question, and finally, bringing together two different areas.

Developing a comfort and ease in the messiness of real-world data means getting to ask new questions each time the data update. And knowing a dataset in detail tends to surface unexpected groupings or values that you can then work with subject-area experts to understand. Becoming a bit of a `mongrel' by developing a base of knowledge across a variety of areas is especially valuable, as is becoming comfortable with the possibility of initially asking dumb questions.

\textbf{Measurement} and \textbf{data collection} are about deciding how our world will become data. They are challenging. The world is so vibrant that it is difficult to reduce it to something that is possible to consistently measure and collect. Take, for instance, someone's height. We can, probably, all agree that we should take our shoes off before we measure height. But our height changes over the course of the day. And measuring someone's height with a tape measure will give different results to using a laser. If we are comparing heights between people or over time, it therefore becomes important to measure at the same time each day, using the same method. But that quickly becomes unfeasible.

Most of the questions we are interested in will use data that are more complicated than height. How do we measure how sad someone is? How do we measure pain? Who decides what we will measure and how we will measure it? There is a certain arrogance required to think that we can reduce the world to a value and then compare these. Ultimately, we must, but it is difficult to consistently define what is to be measured. This process is not value-free. The only way to reasonably come to terms with this brutal reduction is to deeply understand, and respect what we are measuring and collecting. What is the central essence, and what can be stripped away?

Pablo Picasso, the twentieth century Spanish painter, has a series of drawings where he depicts the outline of an animal using only one line (Figure \ref{fig:lumpthedog}). Despite their simplicity, we recognize which animal is being depicted---the drawing is sufficient to tell the animal is a dog, not a cat. Could this be used to determine whether the dog is sick? Probably not. We would likely want a more detailed drawing. The decision as to which features should be measured and collected, and which to ignore, turns on context and purpose.

\begin{figure}

{\centering \includegraphics[width=0.85\linewidth]{/Users/rohanalexander/Documents/book/figures/lump} 

}

\caption{This drawing is clearly a dog, even though it is just one line}\label{fig:lumpthedog}
\end{figure}

\textbf{Data cleaning and preparation} is a critical part of using data. We need to massage the data available to us into a dataset that we can use. This requires making a lot of decisions. The data cleaning and preparation stage is critical, and worthy of as much attention and care as any other.

Consider a survey that collected information about gender using four options: `man', `woman', `prefer not to say', `other', where `other' dissolved into an open textbox. When we come to that dataset, we are likely to find that most responses are either `man' or `woman'. We need to decide what to do about `prefer not to say'. If we drop it from our dataset, then we are actively ignoring these respondents. If we do not drop it, then it makes our analysis more complicated. Similarly, we need to decide how to deal with the open text responses. Again, we could drop these responses, but this ignores the experiences of some of our respondents. Another option is to merge this with `prefer not to say', but that shows a disregard for our respondents, because they specifically did not choose that option.

There is no easy, nor always-correct, choice in many data cleaning and preparation situations. It depends on context and purpose. Data cleaning and preparation involves making many choices like this, and so it is vital to record every step so that others can understand what was done and why. Data never speak for themselves; they are the dummies of the ventriloquists that cleaned and prepared them.

The process of coming to understand the look and feel of a dataset is termed \textbf{exploratory data analysis} (EDA). This is an open-ended process. We need to understand the shape of our dataset before we can formally model it. The process of EDA is an iterative one that involves producing summary statistics, graphs, tables, and sometimes even some modelling. It is a process that never formally finishes and requires a variety of skills.

It is difficult to delineate where EDA ends and formal statistical modelling begins, especially when considering how beliefs and understanding develop \citep{Hullman2021Designing}. But at its core, `EDA starts from the data', and involves immersing ourselves in it \citep{Cook2021Foundation}. EDA is not something that is typically explicitly part of our final story. But it has a central role in how we come to understand the story we are telling. And so, it is critical that all the steps taken during EDA are recorded and shared.

\textbf{Statistical modelling} has a long and robust history. Our knowledge of statistics has been built over hundreds of years. Statistics is not a series of dry theorems and proofs but is instead a way of exploring the world. It is analogous to `a knowledge of foreign languages or of algebra: it may prove of use at any time under any circumstances' \citep[p.~4]{bowley}. A statistical model is not a recipe to be blindly followed in an if-this-then-that way but is instead a way of understanding data \citep{islr}. Modelling is usually required to infer statistical patterns from data. More formally, `statistical inference, or ``learning'' as it is called in computer science, is the process of using data to infer the distribution that generated the data' \citep[ p.~87]{wasserman}.

Statistical significance is not the same as scientific significance, and we are realizing the cost of what has been the dominant paradigm. It is rarely appropriate to put our data through some arbitrary pass/fail statistical test. Instead, the proper use for statistical modelling is as a kind of echolocation. We listen to what comes back to us from the model, to help learn about the shape of the world, while recognizing that it is only one representation of the world.

The use of statistical programming languages, such as R, enables us to rapidly \textbf{scale} our work. This refers to both inputs and outputs. It is basically just as easy to consider 10 observations as 1,000, or even 1,000,000. This enables us to more quickly see the extent to which our stories apply. It is also the case that our outputs can be consumed as easily by one person as by 10, or 100. Using an Application Programming Interface (API) it is even possible for our stories to be considered many thousands of times each second.

\hypertarget{how-do-our-worlds-become-data}{%
\section{How do our worlds become data?}\label{how-do-our-worlds-become-data}}

\begin{quote}
There is the famous story by Eddington about some people who went fishing in the sea with a net. Upon examining the size of the fish they had caught, they decided there was a minimum size to the fish in the sea! Their conclusion arose from the tool used and not from reality.

\citet[p.~177]{hamming1996}
\end{quote}

To a certain extent we are wasting our time. We have a perfect model of the world---it is the world! But it is too complicated. If we knew perfectly how everything was affected by the uncountable factors that influence it, then we could perfectly forecast a coin toss, a dice roll, and every other seemingly random process each time. But we cannot. Instead, we must simplify things to that which is plausibly measurable, and it is that which we define as data. Our data are a simplification of the messy, complex, world from which they were generated.

There are different approximations of `plausibly measurable'. Hence, datasets are always the result of choices. We must decide whether they are nonetheless reasonable for the task at hand. We use statistical models to help us think deeply about, explore, and hopefully come to better understand, our data.

Much of statistics is focused on considering, thoroughly, the data that we have. And that was appropriate for when our data were predominately agricultural, astronomical, or from the physical sciences. But with the rise of data science, mostly because of the value of its application to datasets generated by humans, we must also actively consider what is not in our dataset. Who is systematically missing from our dataset? Whose data does not fit nicely into the approach that we are using and are hence is being inappropriately simplified? If the process of the world becoming data necessarily involves abstraction and simplification, then we need to be clear about the points at which we can reasonably simplify, and those which would be inappropriate, recognizing that this will be application specific.

The process of our world becoming data necessarily involves measurement. Paradoxically, often those that do the measurement and are deeply immersed in the details have less trust in the data than those that are removed from it. Even seemingly clear tasks, such as measuring distance, defining boundaries, and counting populations, are surprisingly difficult in practice. Turning our world into data requires many decisions and imposes much error. Among many other considerations, we need to decide what will be measured, how accurately we will do this, and who will be doing the measurement.

\begin{quote}
\textbf{Oh, you think we have good data on that!} An important example of how something seemingly simple quickly becomes difficult is maternal mortality. That refers to the number of women who die while pregnant, or soon after a termination, from a cause related to the pregnancy or its management \citep{matmortality}. It is difficult but critical to turn the tragedy of such a death into cause-specific data because that helps mitigate future deaths. Some countries have well-developed civil registration and vital statistics (CRVS). These collect data about every death. But many countries do not have a CRVS and so not every death is recorded. Even if a death is recorded, defining a cause of death may be difficult, especially when there is a lack of qualified medical personal or equipment. Maternal mortality is especially difficult because there are typically many causes. Some CRVS have a checkbox on the form to specify whether the death should be counted as maternal mortality. But even some developed countries have only recently adopted this. For instance, it was only introduced in the US in 2003, and even in 2015 Alabama, California, and West Virginia had not adopted the standard question \citep{macdorman2018failure}.
\end{quote}

We typically use various instruments to turn the world into data. In astronomy, the development of better telescopes, and eventually satellites and probes, enabled new understanding of other worlds. Similarly, we have new instruments for turning our own world into data being developed each day. Where once a census was a generational-defining event, now we have regular surveys, transactions data available by the second, and almost all interactions on the internet become data of some kind. The development of such instruments has enabled exciting new stories.

Our world imperfectly becomes data. If we are to nonetheless use data to learn about the world, then we need to actively seek to understand the ways they are imperfect and the implications of those imperfections.

\hypertarget{what-is-data-science-and-how-should-we-use-it-to-learn-about-the-world}{%
\section{What is data science and how should we use it to learn about the world}\label{what-is-data-science-and-how-should-we-use-it-to-learn-about-the-world}}

There is no agreed definition of data science, but a lot of people have tried. For instance, \citet{r4ds} say it is `\ldots an exciting discipline that allows you to turn raw data into understanding, insight, and knowledge.' Similarly, \citet{leekandpeng} say it is `\ldots the process of formulating a quantitative question that can be answered with data, collecting and cleaning the data, analyzing the data, and communicating the answer to the question to a relevant audience.' \citet{moderndatascience} say it is `\ldots the science of extracting meaningful information from data'. \citet{craiu2019hiring} argues that the lack of certainty as to what data science is might not matter because `\ldots who can really say what makes someone a poet or a scientist?' He goes on to broadly say that a data scientist is `\ldots someone with a data driven research agenda, who adheres to or aspires to using a principled implementation of statistical methods and uses efficient computation skills.'

In any case, alongside specific, technical, definitions, there is value in having a simple definition, even if we lose a bit of specificity. Probability is often informally defined as `counting things' \citep[p.~10]{citemcelreath}. In a similar informal sense, data science can be defined as something like: humans measuring stuff, typically related to other humans, and using sophisticated averaging to explain and predict.

That may sound a touch cute, but Francis Edgeworth, the nineteenth century statistician and economist, considered statistics to be the science `of those Means which are presented by social phenomena', so it is in good company \citep{edgeworth1885methods}. In any case, one feature of this definition is that it does not treat data as \emph{terra nullius}, or nobody's land. Statisticians tend to see data as the result of some process that we can never know, but that we try to use data to come to understand. Many statisticians care deeply about data and measurement, but there are many cases in statistics where data kind of just appear; they belong to no one. But that is never actually the case.

Data is generated, and then must be gathered, cleaned, and prepared, and these decisions matter. Every dataset is \emph{sui generis}, or a class by itself, and so when you come to know one dataset well, you just know one dataset, not all datasets.

Much of data science focuses on the `science', but it is important to also focus on `data'. And that is another feature of that cutesy definition of data science. A lot of data scientists are generalists, who are interested in a broad range of problems. Often, the thing that unites these is the need to gather, clean, and prepare messy data. And often it is the specifics of those data that requires the most time, that updates most often, and that are worthy of our most full attention.

\citet{Jordan2019Artificial} describes being in a medical office and being given some probability, based on prenatal screening, that his child, then a fetus, had Down syndrome. By way of background, one can test to know for sure, but that test comes with the risk of the fetus not surviving, so this initial screening probability matters. \citet{Jordan2019Artificial} found those probabilities were being determined based on a study done a decade earlier in the UK. The issue was that in the ensuing 10 years, imaging technology had improved so the test was not expecting such high-resolution images and there had been a subsequent (false) increase in Down syndrome diagnoses when the images improved. There was no problem with the science, it was the data.

\begin{quote}
\textbf{Shoulders of giants} Dr Michael Jordan is Pehong Chen Distinguished Professor at the University of California, Berkeley. After taking a PhD in Cognitive Science from University of California, San Diego, in 1985, he was appointed as an assistant professor at MIT, being promoted to full professor in 1997, and in 1998 he moved to Berkeley. One area of his research is statistical machine learning. One particularly important paper is \citet{Blei2003latent}, which enables text to be grouped together to define topics.
\end{quote}

It is not just the `science' bit that is hard, it is the `data' bit as well. For instance, researchers went back and examined one of the most popular text datasets in computer science, and they found that around 30 per cent of the data were inappropriately duplicated \citep{bandy2021addressing}. There is an entire field---linguistics---that specializes in these types of datasets, and inappropriate use of data is one of the dangers of any one field being hegemonic. The strength of data science is that it brings together folks with a variety of backgrounds and training to the task of learning about some dataset. It is not constrained by what was done in the past. But this means that we must go out of our way to show respect for those who do not come from our own tradition, but who are nonetheless as similarly interested in a dataset as we are. Data science is multi-disciplinary and increasingly critical; hence it must reflect our world. There is a pressing need a diversity of backgrounds, of approaches, and of disciplines in data science.

Our world is messy, and so are our data. To successfully tell stories with data you need to become comfortable with the fact that the process will be difficult. Hannah Fry, the British mathematician, describes spending six months rewriting code before it solved her problem \citep{hannahfryft}. You need to learn to stick with it. You also need to countenance failure, and you do this by developing resilience and having intrinsic motivation. The world of data is about considering possibilities and probabilities, and learning to make trade-offs between them. There is almost never anything that we know for certain, and there is no perfect analysis.

Ultimately, we are all just telling stories with data, but these stories are increasingly among the most important in the world.

\hypertarget{exercises-and-tutorial}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial}}

\hypertarget{exercises}{%
\subsection{Exercises}\label{exercises}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  According to \citet{register2020} data decisions affect (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Real people.
  \item
    No one.
  \item
    Those in the training set.
  \item
    Those in the test set.
  \end{enumerate}
\item
  What is data science (in your own words)?
\item
  According to \citet{keyes2019} what is perhaps a more accurate definition of data science (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    The inhumane reduction of humanity down to what can be counted.
  \item
    The quantitative analysis of large amounts of data for the purpose of decision-making.
  \item
    Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from many structural and unstructured data.
  \end{enumerate}
\item
  Imagine that you have a job in which including `race' and/or sexuality as explanatory variables improves the performance of your model. What types of issues would you consider when deciding whether to include these variable in production (in your own words)?
\item
  Re-order the following steps of the workflow to be correct:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Simulate.
  \item
    Acquire.
  \item
    Share.
  \item
    Plan.
  \item
    Explore.
  \end{enumerate}
\item
  According to \citet{crawford}, which of the following forces shape our world, and hence our data (select all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Political.
  \item
    Historical.
  \item
    Cultural.
  \item
    Social.
  \end{enumerate}
\item
  What is required to tell convincing stories (select all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Sophisticated workflow.
  \item
    Practical skills.
  \item
    Big data.
  \item
    Humility about one's own knowledge.
  \item
    Theory and application.
  \end{enumerate}
\item
  Why is ethics a key element of telling convincing stories (in your own words)?
\end{enumerate}

\hypertarget{tutorial}{%
\subsection{Tutorial}\label{tutorial}}

The purpose of this tutorial is to clarify in your mind the difficulty of measurement, even of seemingly simple things, and hence the likelihood of measurement issues in more complicated areas.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Please obtain some seeds for a fast-growing plant such as radishes, mustard greens, or arugula. Plant the seeds and measure how much soil you used. Water them and measure the water you used. Each day take a note of any changes. More generally, measure and record as much as you can. Note your thoughts about the difficulty of measurement. Eventually your seeds will sprout, and you should measure how big they are. We will return to use the data that you put together.
\item
  While you are waiting for the seeds to sprout, and for one week only, please measure the length of your hair daily. Write a one-or-two-page paper about what you found and what you learned about the difficulty of measurement.
\end{enumerate}

\hypertarget{drinking-from-a-fire-hose}{%
\chapter{Drinking from a fire hose}\label{drinking-from-a-fire-hose}}

\textbf{Required material}

\begin{itemize}
\tightlist
\item
  Read \emph{Data science as an atomic habit}, \citep{citeBarrett}
\item
  Read \emph{This is how AI bias really happens---and why it's so hard to fix}, \citep{hao2019}
\item
  Read \emph{The mundanity of excellence: An ethnographic report on stratification and Olympic swimmers}, \citep{chambliss1989mundanity}
\end{itemize}

\textbf{Key concepts and skills}

\begin{itemize}
\tightlist
\item
  The statistical programming language R enables us to tell interesting stories using data. It is a language like any other, and the path to mastery can be slow.
\item
  The framework that we use to approach projects is: plan, simulate, gather, explore, and share.
\item
  The way to learn R is to start with a small project and break down what is required to achieve it into tiny steps, look at other people's code, and draw on that to achieve each step. Complete that project and move onto the next project. Each project you will get a little better.
\item
  The key is to start actively working regularly.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{ggplot2} \citep{citeggplot}
\item
  \texttt{janitor} \citep{janitor}
\item
  \texttt{opendatatoronto} \citep{citeSharla}
\item
  \texttt{tidyr} \citep{citetidyr}
\item
  \texttt{tidyverse} \citep{Wickham2017}
\end{itemize}

\textbf{Key functions}

\begin{itemize}
\tightlist
\item
  \texttt{\textless{}-} `assign'
\item
  \texttt{\textbar{}\textgreater{}} `pipe'
\item
  \texttt{+} `add'
\item
  \texttt{c()}
\item
  \texttt{citation()}
\item
  \texttt{class()}
\item
  \texttt{dplyr::arrange()}
\item
  \texttt{dplyr::filter()}
\item
  \texttt{dplyr::mutate()}
\item
  \texttt{dplyr::recode()}
\item
  \texttt{dplyr::rename()}
\item
  \texttt{dplyr::select()}
\item
  \texttt{dplyr::summarize()}
\item
  \texttt{ggplot2::geom\_bar()}
\item
  \texttt{ggplot2::geom\_point()}
\item
  \texttt{ggplot2::ggplot()}
\item
  \texttt{head()}
\item
  \texttt{janitor::clean\_names()}
\item
  \texttt{library()}
\item
  \texttt{names()}
\item
  \texttt{readr::read\_csv()}
\item
  \texttt{readr::write\_csv()}
\item
  \texttt{rep()}
\item
  \texttt{rpois()}
\item
  \texttt{runif()}
\item
  \texttt{sample()}
\item
  \texttt{set.seed()}
\item
  \texttt{stringr::str\_remove()}
\item
  \texttt{sum()}
\item
  \texttt{tail()}
\item
  \texttt{tidyr::separate()}
\end{itemize}

\hypertarget{hello-world}{%
\section{Hello, World!}\label{hello-world}}

The way to start, is to start. In this chapter we go through three complete examples of the workflow advocated in this book. This means we will: plan, simulate, acquire, explore, and share. If you are new to R, then some of the code may be a bit unfamiliar to you. If you are new to statistics, then some of the concepts may be unfamiliar. Do not worry. It will all soon become familiar.
The only way to learn how to tell stories, is to start telling stories yourself. This means that you should try to get these examples working. Do the sketches yourself, type everything out yourself (using R Studio Cloud if you are new to R and do not have it installed on your own computer), and execute it all. It is important, and normal, to realize that it will be challenging at the start.

\begin{quote}
Whenever you're learning a new tool, for a long time, you're going to suck\ldots{} But the good news is that is typical; that's something that happens to everyone, and it's only temporary.

Hadley Wickham as quoted by \citet{citeBarrett}.
\end{quote}

You will be guided thoroughly here. Hopefully by experiencing the power of telling stories with data, you will feel empowered to stick with it.

To get started, go to R Studio Cloud -- \url{https://rstudio.cloud/} -- and create an account. As we are not doing anything too involved the free version will be fine for now. Once you have an account and log in, then it should look something like Figure \ref{fig:second}.

\begin{figure}
\includegraphics[width=1\linewidth]{/Users/rohanalexander/Documents/book/figures/01-03_r_essentials/02} \caption{Opening R Studio Cloud for the first time}\label{fig:second}
\end{figure}

(You will be in `Your Workspace', and you will not have an `Example Workspace'.) From here you should start a `New Project'. You can give the project a name by clicking on `Untitled Project' and replacing it.

We will now go through three worked examples: Canadian elections, Toronto homelessness, and neonatal mortality. These examples build increasing complexity, but from the first one, we will be telling a story with data.

\hypertarget{canadian-elections}{%
\section{Canadian elections}\label{canadian-elections}}

Canada is a parliamentary democracy with 338 seats in the House of Commons, which is the lower house and that from which government is formed. There are two major parties -- `Liberal' and `Conservative' -- three minor parties -- `Bloc Québécois', `New Democratic', and `Green' -- and many smaller parties and independents. In this example we will create a graph of the number of seats that each party won in the 2019 Federal Election.

\hypertarget{plan}{%
\subsection{Plan}\label{plan}}

For this example, we need to plan two aspects. The first is what the dataset that we need will look like, and the second is what the final graph will look like.

The basic requirement for the dataset is that it has the name of the seat (sometimes called a `riding' in Canada) and the party of the person elected. So, a quick sketch of the dataset that we would need could look something like Figure \ref{fig:canadaexampledata}.

\begin{figure}

{\centering \includegraphics[width=0.4\linewidth]{/Users/rohanalexander/Documents/book/figures/IMG_1815} 

}

\caption{Quick sketch of a dataset that could be useful for analysing Canadian elections}\label{fig:canadaexampledata}
\end{figure}

We also need to plan the graph that we are interested in. Given we want to display the number of seats that each party won, a quick sketch of what we might aim for is Figure \ref{fig:canadaexampletable}.

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{/Users/rohanalexander/Documents/book/figures/IMG_1814} 

}

\caption{Quick sketch of a possible graph of the number of ridings won by each party}\label{fig:canadaexampletable}
\end{figure}

\hypertarget{simulate}{%
\subsection{Simulate}\label{simulate}}

We now simulate some data, to bring some specificity to our sketches.

To get started, within R Studio Cloud, make a new R Markdown file (`File' -\textgreater{} `New File' -\textgreater{} `R Markdown'). When you do this, you will be asked to install some packages, which you should agree to. For this example, we will put everything into this one R Markdown document. You should save it as `canadian\_elections.Rmd' (`File' -\textgreater{} `Save As\ldots{}')

In the R Markdown document create a new R code chunk (`Code' -\textgreater{} `Insert Chunk') and add preamble documentation that explains:

\begin{itemize}
\tightlist
\item
  the purpose of the document;
\item
  the author and contact details;
\item
  when the file was written or last updated; and
\item
  pre-requisites that the file relies on.
\end{itemize}

In R, lines that start with `\#' are comments. This means that they are not run as code by R, but are instead designed to be read by humans. Each line of this preamble should start with a `\#'. Also make it clear that this is the preamble section by surrounding that with `\#\#\#\#'.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Preamble \#\#\#\#}
\CommentTok{\# Purpose: Read in data from the 2019 Canadian Election and make a}
\CommentTok{\# graph of the number of ridings each party won.}
\CommentTok{\# Author: Rohan Alexander}
\CommentTok{\# Email: rohan.alexander@utoronto.ca}
\CommentTok{\# Date: 1 January 2022}
\CommentTok{\# Prerequisites: Need to know where to get Canadian elections data.}
\end{Highlighting}
\end{Shaded}

After this we need to set-up the workspace. This involves installing and loading any packages that will be needed. A package only needs to be installed once for each computer, but needs to be loaded each time it is to be used. In this case we are going to use \texttt{tidyverse} \citep{Wickham2017}, \texttt{janitor} \citep{janitor}, and \texttt{tidyr} \citep{citetidyr}. They will need to be installed because this is the first time they are being used, and then each will need to be loaded.

An example of installing the packages follows (excessive comments have been added to be clear about what is going on; in general, this level of commenting is unnecessary). Run this code by clicking the small green arrow associated with the R code chunk.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Workspace set{-}up \#\#\#\#}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{) }\CommentTok{\# Only need to do this once per computer}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"janitor"}\NormalTok{) }\CommentTok{\# Only need to do this once per computer}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"tidyr"}\NormalTok{) }\CommentTok{\# Only need to do this once per computer}
\end{Highlighting}
\end{Shaded}

Now that the packages are installed, they need to be loaded. As that installation step only needs to be done once per computer, that code can be commented out so that it is not accidentally run.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Workspace set{-}up \#\#\#\#}
\CommentTok{\# install.packages("tidyverse") \# Only need to do this once per computer}
\CommentTok{\# install.packages("janitor") \# Only need to do this once per computer}
\CommentTok{\# install.packages("tidyr") \# Only need to do this once per computer}

\FunctionTok{library}\NormalTok{(tidyverse) }\CommentTok{\# A collection of data{-}related packages}
\FunctionTok{library}\NormalTok{(janitor) }\CommentTok{\# Helps clean datasets}
\FunctionTok{library}\NormalTok{(tidyr) }\CommentTok{\# Helps make tidy datasets}
\end{Highlighting}
\end{Shaded}

All packages contain a help file that provides information about them and their functions. It can be accessed by appending a question mark before the package name and then running that code. For instance \texttt{?tidyverse}.

To simulate our data, we need to create a dataset with two columns: `Riding' and `Party', and some values for each. In the case of `Riding' reasonable values would be a name of one of the 338 Canadian ridings. In the case of `Party' reasonable values would be one of the following six: `Liberal', `Conservative', `Bloc Québécois', `New Democratic', `Green', `Other'. Again, this code can be run by clicking the small green arrow associated with the R code chunk.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_data }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}
    \CommentTok{\# Use 1 through to 338 to represent each riding}
    \StringTok{\textquotesingle{}Riding\textquotesingle{}} \OtherTok{=} \DecValTok{1}\SpecialCharTok{:}\DecValTok{338}\NormalTok{,}
    \CommentTok{\# Randomly choose one of six options, with replacement, 338 times}
    \StringTok{\textquotesingle{}Party\textquotesingle{}} \OtherTok{=} \FunctionTok{sample}\NormalTok{(}
      \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}
        \StringTok{\textquotesingle{}Liberal\textquotesingle{}}\NormalTok{,}
        \StringTok{\textquotesingle{}Conservative\textquotesingle{}}\NormalTok{,}
        \StringTok{\textquotesingle{}Bloc Québécois\textquotesingle{}}\NormalTok{,}
        \StringTok{\textquotesingle{}New Democratic\textquotesingle{}}\NormalTok{,}
        \StringTok{\textquotesingle{}Green\textquotesingle{}}\NormalTok{,}
        \StringTok{\textquotesingle{}Other\textquotesingle{}}
\NormalTok{      ),}
      \AttributeTok{size =} \DecValTok{338}\NormalTok{,}
      \AttributeTok{replace =} \ConstantTok{TRUE}
\NormalTok{    ))}

\NormalTok{simulated\_data}
\CommentTok{\#\textgreater{} \# A tibble: 338 x 2}
\CommentTok{\#\textgreater{}    Riding Party         }
\CommentTok{\#\textgreater{}     \textless{}int\textgreater{} \textless{}chr\textgreater{}         }
\CommentTok{\#\textgreater{}  1      1 Conservative  }
\CommentTok{\#\textgreater{}  2      2 New Democratic}
\CommentTok{\#\textgreater{}  3      3 Liberal       }
\CommentTok{\#\textgreater{}  4      4 Bloc Québécois}
\CommentTok{\#\textgreater{}  5      5 New Democratic}
\CommentTok{\#\textgreater{}  6      6 Liberal       }
\CommentTok{\#\textgreater{}  7      7 Bloc Québécois}
\CommentTok{\#\textgreater{}  8      8 Other         }
\CommentTok{\#\textgreater{}  9      9 Other         }
\CommentTok{\#\textgreater{} 10     10 Liberal       }
\CommentTok{\#\textgreater{} \# ... with 328 more rows}
\end{Highlighting}
\end{Shaded}

\hypertarget{acquire}{%
\subsection{Acquire}\label{acquire}}

Now we want to get the actual data. The data we need is from Elections Canada, which is the non-partisan agency that organizes Canadian Federal elections. We can pass a website to \texttt{read\_csv()} from the \texttt{readr} package \citep{citereadr}. We do not need to explicitly load the \texttt{readr} package because it is part of the \texttt{tidyverse}. The `\textless-' or `assignment operator' is allocating the output of \texttt{read\_csv()} to an object called `raw\_elections\_data'.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Read in the data \#\#\#\#}
\NormalTok{raw\_elections\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}
    \AttributeTok{file =}
      \StringTok{"https://www.elections.ca/res/rep/off/ovr2019app/51/data\_donnees/table\_tableau11.csv"}\NormalTok{,}
    \AttributeTok{show\_col\_types =} \ConstantTok{FALSE}
\NormalTok{    ) }

\CommentTok{\# We have read the data from the Elections Canada website. We may like}
\CommentTok{\# to save it in case something happens or they move it. }
\FunctionTok{write\_csv}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ raw\_elections\_data, }
  \AttributeTok{file =} \StringTok{"canadian\_voting.csv"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

We can take a quick look at the dataset using \texttt{head()} which will show the first six rows, and \texttt{tail()} which will show the last six rows.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(raw\_elections\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 13}
\CommentTok{\#\textgreater{}   Province   \textasciigrave{}Electoral Distri\textasciitilde{} \textasciigrave{}Electoral Distr\textasciitilde{} Population}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}      \textless{}chr\textgreater{}                          \textless{}dbl\textgreater{}      \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Newfoundl\textasciitilde{} Avalon                         10001      86494}
\CommentTok{\#\textgreater{} 2 Newfoundl\textasciitilde{} Bonavista{-}{-}Burin{-}\textasciitilde{}             10002      74116}
\CommentTok{\#\textgreater{} 3 Newfoundl\textasciitilde{} Coast of Bays{-}{-}Ce\textasciitilde{}             10003      77680}
\CommentTok{\#\textgreater{} 4 Newfoundl\textasciitilde{} Labrador                       10004      27197}
\CommentTok{\#\textgreater{} 5 Newfoundl\textasciitilde{} Long Range Mounta\textasciitilde{}             10005      86553}
\CommentTok{\#\textgreater{} 6 Newfoundl\textasciitilde{} St. John\textquotesingle{}s East/S\textasciitilde{}             10006      85697}
\CommentTok{\#\textgreater{} \# ... with 9 more variables: Electors/Électeurs \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   Polling Stations/Bureaux de scrutin \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   Valid Ballots/Bulletins valides \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   Percentage of Valid Ballots /Pourcentage des bulletins valides \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   Rejected Ballots/Bulletins rejetés \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   Percentage of Rejected Ballots /Pourcentage des bulletins rejetés \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   Total Ballots Cast/Total des bulletins déposés \textless{}dbl\textgreater{}, ...}
\FunctionTok{tail}\NormalTok{(raw\_elections\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 13}
\CommentTok{\#\textgreater{}   Province   \textasciigrave{}Electoral Distri\textasciitilde{} \textasciigrave{}Electoral Distr\textasciitilde{} Population}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}      \textless{}chr\textgreater{}                          \textless{}dbl\textgreater{}      \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 British C\textasciitilde{} Vancouver South/V\textasciitilde{}             59040     102927}
\CommentTok{\#\textgreater{} 2 British C\textasciitilde{} Victoria                       59041     117133}
\CommentTok{\#\textgreater{} 3 British C\textasciitilde{} West Vancouver{-}{-}S\textasciitilde{}             59042     119113}
\CommentTok{\#\textgreater{} 4 Yukon      Yukon                          60001      35874}
\CommentTok{\#\textgreater{} 5 Northwest\textasciitilde{} Northwest Territo\textasciitilde{}             61001      41786}
\CommentTok{\#\textgreater{} 6 Nunavut    Nunavut                        62001      35944}
\CommentTok{\#\textgreater{} \# ... with 9 more variables: Electors/Électeurs \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   Polling Stations/Bureaux de scrutin \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   Valid Ballots/Bulletins valides \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   Percentage of Valid Ballots /Pourcentage des bulletins valides \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   Rejected Ballots/Bulletins rejetés \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   Percentage of Rejected Ballots /Pourcentage des bulletins rejetés \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   Total Ballots Cast/Total des bulletins déposés \textless{}dbl\textgreater{}, ...}
\end{Highlighting}
\end{Shaded}

We need to clean the data so that we can use it. We are trying to make it similar to the dataset that we thought we wanted in the planning stage. While it is fine to move away from the plan, this needs to be a deliberate, reasoned, decision. After reading in the dataset that we saved, the first thing that we will do is adjust the names to make them easier to type. Removing the spaces helps to type column names. We will do this using \texttt{clean\_names()} from \texttt{janitor} \citep{janitor} which changes the names into `snake\_case'.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Basic cleaning \#\#\#\#}
\NormalTok{raw\_elections\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"canadian\_voting.csv"}\NormalTok{,}
           \AttributeTok{show\_col\_types =} \ConstantTok{FALSE}
\NormalTok{           )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Make the names easier to type}
\NormalTok{cleaned\_elections\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{clean\_names}\NormalTok{(raw\_elections\_data)}

\CommentTok{\# Have a look at the first six rows}
\FunctionTok{head}\NormalTok{(cleaned\_elections\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 13}
\CommentTok{\#\textgreater{}   province   electoral\_distric\textasciitilde{} electoral\_distri\textasciitilde{} population}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}      \textless{}chr\textgreater{}                          \textless{}dbl\textgreater{}      \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Newfoundl\textasciitilde{} Avalon                         10001      86494}
\CommentTok{\#\textgreater{} 2 Newfoundl\textasciitilde{} Bonavista{-}{-}Burin{-}\textasciitilde{}             10002      74116}
\CommentTok{\#\textgreater{} 3 Newfoundl\textasciitilde{} Coast of Bays{-}{-}Ce\textasciitilde{}             10003      77680}
\CommentTok{\#\textgreater{} 4 Newfoundl\textasciitilde{} Labrador                       10004      27197}
\CommentTok{\#\textgreater{} 5 Newfoundl\textasciitilde{} Long Range Mounta\textasciitilde{}             10005      86553}
\CommentTok{\#\textgreater{} 6 Newfoundl\textasciitilde{} St. John\textquotesingle{}s East/S\textasciitilde{}             10006      85697}
\CommentTok{\#\textgreater{} \# ... with 9 more variables: electors\_electeurs \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   polling\_stations\_bureaux\_de\_scrutin \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   valid\_ballots\_bulletins\_valides \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   percentage\_of\_valid\_ballots\_pourcentage\_des\_bulletins\_valides \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   rejected\_ballots\_bulletins\_rejetes \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   percentage\_of\_rejected\_ballots\_pourcentage\_des\_bulletins\_rejetes \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   total\_ballots\_cast\_total\_des\_bulletins\_deposes \textless{}dbl\textgreater{}, ...}
\end{Highlighting}
\end{Shaded}

The names are faster to type because R Studio will auto-complete them. To do this, we begin typing the name of a column and then use `tab' to auto-complete it.

There are many columns in the dataset, and we are primarily interested in two: `electoral\_district\_name\_nom\_de\_circonscription', and `elected\_candidate\_candidat\_elu'. We can choose certain columns of interest using \texttt{select()} from \texttt{dplyr} \citep{citedplyr} which we loaded as part of the \texttt{tidyverse}. The `pipe operator', \texttt{\textbar{}\textgreater{}}, pushes the output of one line to be the first input of the function on the next line.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_elections\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  cleaned\_elections\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \CommentTok{\# Select only certain columns}
  \FunctionTok{select}\NormalTok{(electoral\_district\_name\_nom\_de\_circonscription,}
\NormalTok{         elected\_candidate\_candidat\_elu}
\NormalTok{         )}

\CommentTok{\# Have a look at the first six rows}
\FunctionTok{head}\NormalTok{(cleaned\_elections\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 2}
\CommentTok{\#\textgreater{}   electoral\_district\_name\_no\textasciitilde{} elected\_candidate\_candidat\_elu}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}                       \textless{}chr\textgreater{}                         }
\CommentTok{\#\textgreater{} 1 Avalon                      McDonald, Kenneth Liberal/Lib\textasciitilde{}}
\CommentTok{\#\textgreater{} 2 Bonavista{-}{-}Burin{-}{-}Trinity   Rogers, Churence Liberal/Libé\textasciitilde{}}
\CommentTok{\#\textgreater{} 3 Coast of Bays{-}{-}Central{-}{-}No\textasciitilde{} Simms, Scott Liberal/Libéral  }
\CommentTok{\#\textgreater{} 4 Labrador                    Jones, Yvonne Liberal/Libéral }
\CommentTok{\#\textgreater{} 5 Long Range Mountains        Hutchings, Gudie Liberal/Libé\textasciitilde{}}
\CommentTok{\#\textgreater{} 6 St. John\textquotesingle{}s East/St. John\textquotesingle{}s\textasciitilde{} Harris, Jack NDP{-}New Democrat\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

Some of the names of the columns are still quite long because they have both English and French in them. We can look at the names of the columns with \texttt{names()}. And we can change the names using \texttt{rename()} from \texttt{dplyr} \citep{citedplyr}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(cleaned\_elections\_data)}
\CommentTok{\#\textgreater{} [1] "electoral\_district\_name\_nom\_de\_circonscription"}
\CommentTok{\#\textgreater{} [2] "elected\_candidate\_candidat\_elu"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_elections\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  cleaned\_elections\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{rename}\NormalTok{(}
    \AttributeTok{riding =}\NormalTok{ electoral\_district\_name\_nom\_de\_circonscription,}
    \AttributeTok{elected\_candidate =}\NormalTok{ elected\_candidate\_candidat\_elu}
\NormalTok{    )}

\FunctionTok{head}\NormalTok{(cleaned\_elections\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 2}
\CommentTok{\#\textgreater{}   riding                             elected\_candidate      }
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}                              \textless{}chr\textgreater{}                  }
\CommentTok{\#\textgreater{} 1 Avalon                             McDonald, Kenneth Libe\textasciitilde{}}
\CommentTok{\#\textgreater{} 2 Bonavista{-}{-}Burin{-}{-}Trinity          Rogers, Churence Liber\textasciitilde{}}
\CommentTok{\#\textgreater{} 3 Coast of Bays{-}{-}Central{-}{-}Notre Dame Simms, Scott Liberal/L\textasciitilde{}}
\CommentTok{\#\textgreater{} 4 Labrador                           Jones, Yvonne Liberal/\textasciitilde{}}
\CommentTok{\#\textgreater{} 5 Long Range Mountains               Hutchings, Gudie Liber\textasciitilde{}}
\CommentTok{\#\textgreater{} 6 St. John\textquotesingle{}s East/St. John\textquotesingle{}s{-}Est     Harris, Jack NDP{-}New D\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

We will now look at this dataset, and the `elected\_candidate' column in particular.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(cleaned\_elections\_data}\SpecialCharTok{$}\NormalTok{elected\_candidate)}
\CommentTok{\#\textgreater{} [1] "McDonald, Kenneth Liberal/Libéral"                                   }
\CommentTok{\#\textgreater{} [2] "Rogers, Churence Liberal/Libéral"                                    }
\CommentTok{\#\textgreater{} [3] "Simms, Scott Liberal/Libéral"                                        }
\CommentTok{\#\textgreater{} [4] "Jones, Yvonne Liberal/Libéral"                                       }
\CommentTok{\#\textgreater{} [5] "Hutchings, Gudie Liberal/Libéral"                                    }
\CommentTok{\#\textgreater{} [6] "Harris, Jack NDP{-}New Democratic Party/NPD{-}Nouveau Parti démocratique"}
\end{Highlighting}
\end{Shaded}

We can see that we have the surname of the elected candidate, followed by a comma, followed by their first name, followed by a space, followed by the name of the party in both English and French, separated by a slash. We can break-up this column into its pieces using \texttt{separate()} from \texttt{tidyr} \citep{citetidyr}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_elections\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  cleaned\_elections\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \CommentTok{\# Separate the column into two based on the slash}
  \FunctionTok{separate}\NormalTok{(}\AttributeTok{col =}\NormalTok{ elected\_candidate,}
           \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}other\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}party\textquotesingle{}}\NormalTok{),}
           \AttributeTok{sep =} \StringTok{\textquotesingle{}/\textquotesingle{}}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \CommentTok{\# Remove the \textquotesingle{}other\textquotesingle{} column}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{other)}

\FunctionTok{head}\NormalTok{(cleaned\_elections\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 2}
\CommentTok{\#\textgreater{}   riding                             party                  }
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}                              \textless{}chr\textgreater{}                  }
\CommentTok{\#\textgreater{} 1 Avalon                             Libéral                }
\CommentTok{\#\textgreater{} 2 Bonavista{-}{-}Burin{-}{-}Trinity          Libéral                }
\CommentTok{\#\textgreater{} 3 Coast of Bays{-}{-}Central{-}{-}Notre Dame Libéral                }
\CommentTok{\#\textgreater{} 4 Labrador                           Libéral                }
\CommentTok{\#\textgreater{} 5 Long Range Mountains               Libéral                }
\CommentTok{\#\textgreater{} 6 St. John\textquotesingle{}s East/St. John\textquotesingle{}s{-}Est     NPD{-}Nouveau Parti démo\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

Finally we want to change the party names from French to English to match what we simulated, using \texttt{recode()} from \texttt{dplyr} \citep{citedplyr}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_elections\_data }\OtherTok{\textless{}{-}}
\NormalTok{  cleaned\_elections\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{party =}
      \FunctionTok{recode}\NormalTok{(}
\NormalTok{        party,}
        \StringTok{\textquotesingle{}Conservateur\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}Conservative\textquotesingle{}}\NormalTok{,}
        \StringTok{\textquotesingle{}Indépendant(e)\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}Other\textquotesingle{}}\NormalTok{,}
        \StringTok{\textquotesingle{}Libéral\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}Liberal\textquotesingle{}}\NormalTok{,}
        \StringTok{\textquotesingle{}NPD{-}Nouveau Parti démocratique\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}New Democratic\textquotesingle{}}\NormalTok{,}
        \StringTok{\textquotesingle{}Parti Vert\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}Green\textquotesingle{}}
\NormalTok{      )}
\NormalTok{  )}

\FunctionTok{head}\NormalTok{(cleaned\_elections\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 2}
\CommentTok{\#\textgreater{}   riding                             party         }
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}                              \textless{}chr\textgreater{}         }
\CommentTok{\#\textgreater{} 1 Avalon                             Liberal       }
\CommentTok{\#\textgreater{} 2 Bonavista{-}{-}Burin{-}{-}Trinity          Liberal       }
\CommentTok{\#\textgreater{} 3 Coast of Bays{-}{-}Central{-}{-}Notre Dame Liberal       }
\CommentTok{\#\textgreater{} 4 Labrador                           Liberal       }
\CommentTok{\#\textgreater{} 5 Long Range Mountains               Liberal       }
\CommentTok{\#\textgreater{} 6 St. John\textquotesingle{}s East/St. John\textquotesingle{}s{-}Est     New Democratic}
\end{Highlighting}
\end{Shaded}

Our data now matches our plan (Figure \ref{fig:canadaexampledata}) pretty well. For every electoral district we have the party of the person that won it.

Having now nicely cleaned the dataset, we should save it, so that we can start with that cleaned dataset in the next stage.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write\_csv}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ cleaned\_elections\_data,}
  \AttributeTok{file =} \StringTok{"cleaned\_elections\_data.csv"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{explore}{%
\subsection{Explore}\label{explore}}

At this point we would like to explore the dataset that we created. One way to better understand a dataset is to make a graph. In particular, here we would like to build the graph that we planned in Figure \ref{fig:canadaexampletable}.

First, we read in the dataset that we just created.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Read in the data \#\#\#\#}
\NormalTok{cleaned\_elections\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}
    \AttributeTok{file =} \StringTok{"cleaned\_elections\_data.csv"}\NormalTok{,}
    \AttributeTok{show\_col\_types =} \ConstantTok{FALSE}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

We can get a quick count of how many seats each party won using \texttt{count()} from \texttt{dplyr} \citep{citedplyr}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_elections\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{count}\NormalTok{(party)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 2}
\CommentTok{\#\textgreater{}   party              n}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}          \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 Bloc Québécois    32}
\CommentTok{\#\textgreater{} 2 Conservative     121}
\CommentTok{\#\textgreater{} 3 Green              3}
\CommentTok{\#\textgreater{} 4 Liberal          157}
\CommentTok{\#\textgreater{} 5 New Democratic    24}
\CommentTok{\#\textgreater{} 6 Other              1}
\end{Highlighting}
\end{Shaded}

To build the graph that we are interested in, we will rely on the \texttt{ggplot2} package \citep{citeggplot}. The key aspect of this package is that we build graphs by adding layers using `+', which we call the `add operator'. In particular we will create a bar chart using \texttt{geom\_bar()} from \texttt{ggplot2} \citep{citeggplot}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_elections\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ party)) }\SpecialCharTok{+} \CommentTok{\# aes abbreviates aesthetics and enables us }
  \CommentTok{\# to specify the x axis variable}
  \FunctionTok{geom\_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{02-drinking_from_a_fire_hose_files/figure-latex/unnamed-chunk-24-1.pdf}

This accomplishes what we set out to do. But we can make it look a bit nicer by modifying the default options (Figure \ref{fig:canadanice}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_elections\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ party)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+} \CommentTok{\# Make the theme neater}
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+} \CommentTok{\# Swap the x and y axis to make parties easier to read}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Party"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of seats"}\NormalTok{) }\CommentTok{\# Make the labels more meaningful}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{02-drinking_from_a_fire_hose_files/figure-latex/canadanice-1.pdf}
\caption{\label{fig:canadanice}Number of seats won, by political party, at the 2019 Canadian Federal Election}
\end{figure}

\hypertarget{communicate}{%
\subsection{Communicate}\label{communicate}}

To this point we have downloaded some data, cleaned it, and made a graph. We would typically need to communicate what we have done at some length. In this case, we can write a few paragraphs about what we did, why we did it, and what we found. An example follows.

\begin{quote}
Canada is a parliamentary democracy with 338 seats in the House of Commons, which is the house that forms government. There are two major parties---`Liberal' and `Conservative'---three minor parties---`Bloc Québécois', `New Democratic', and `Green'---and many smaller parties. The 2019 Federal Election occurred on 21 October, and more than 17 million votes were cast. We were interested in the number of seats that were won by each party.

We downloaded the results, on a seat-specific basis, from the Elections Canada website. We cleaned and tidied the dataset using the statistical programming language R \citep{citeR} as well as the packages \texttt{tidyverse} \citep{citetidyverse} and \texttt{janitor} \citep{janitor}. We then created a graph of the number of seats that each political party won (Figure \ref{fig:canadanice}).

We found that the Liberal Party won 157 seats, followed by the Conservative Party with 121 seats. The minor parties won the following number of seats: Bloc Québécois, 32 seats, New Democratic Party, 24 seats, and the Green Party, 3 seats. Finally, one independent candidate won a seat.

The distribution of seats is skewed toward the two major parties which could reflect relatively stable preferences on the part of Canadian voters, or possibly inertia due to the benefits of already being a major party such a national network and funding, or some other reason. A better understanding the reasons for this distribution are of interest in future work. While the dataset consists of everyone who voted, it worth noting that in Canada some are systematically excluded from voting; and it is much difficult for some to vote than others.
\end{quote}

\hypertarget{toronto-homelessness}{%
\section{Toronto homelessness}\label{toronto-homelessness}}

Toronto has a large homeless population \citep{torontohomeless}. Freezing winters mean it is important there are enough places in shelters. In this example we will make a table of shelter usage in the second half of 2021 that compares average use in each month. Our expectation is that there is greater usage in the colder months, for instance, December, compared with warmer months, for instance, July.

\hypertarget{plan-1}{%
\subsection{Plan}\label{plan-1}}

The dataset that we are interested in would need to have date, the shelter, and the number of beds that were occupied that night. A quick sketch of a dataset that would work is Figure \ref{fig:torontohomelessdataplan}.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{/Users/rohanalexander/Documents/book/figures/IMG_1817} 

}

\caption{Quick sketch of a dataset that could be useful for understanding shelter usage in Toronto}\label{fig:torontohomelessdataplan}
\end{figure}

We are interested in creating a table that has the monthly average number of beds occupied each night. The table would probably look something like Figure \ref{fig:houselessexampletable}.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{/Users/rohanalexander/Documents/book/figures/IMG_1818} 

}

\caption{Quick sketch of a table of the average number of beds occupied each month}\label{fig:houselessexampletable}
\end{figure}

\hypertarget{simulate-1}{%
\subsection{Simulate}\label{simulate-1}}

The next step is to simulate some data that could resemble our dataset.

Within R Studio Cloud make a new R Markdown file, save it, and make a new R code chunk and add preamble documentation. Then install and/or load the libraries that are needed. We will again use \texttt{tidyverse} \citep{Wickham2017}, \texttt{janitor} \citep{janitor}, and \texttt{tidyr} \citep{citetidyr}. As those were installed earlier, they do not need to be installed again. In this example we will also use \texttt{lubridate} \citep{GrolemundWickham2011}, which is part of the \texttt{tidyverse} and so it does not need to be installed independently. We will also use \texttt{opendatatoronto} \citep{citeSharla}, and \texttt{knitr} \citep{citeknitr} and these will need to be installed.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Preamble \#\#\#\#}
\CommentTok{\# Purpose: Get data about 2021 houseless shelter usage and make a table}
\CommentTok{\# Author: Rohan Alexander}
\CommentTok{\# Email: rohan.alexander@utoronto.ca}
\CommentTok{\# Date: 1 January 2022}
\CommentTok{\# Prerequisites: {-} }

\DocumentationTok{\#\#\#\# Workspace set{-}up \#\#\#\#}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"opendatatoronto"}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"lubridate"}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"knitr"}\NormalTok{)}

\FunctionTok{library}\NormalTok{(knitr)}
\FunctionTok{library}\NormalTok{(janitor)}
\FunctionTok{library}\NormalTok{(lubridate)}
\FunctionTok{library}\NormalTok{(opendatatoronto)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(tidyr)}
\end{Highlighting}
\end{Shaded}

To add a bit more detail to the earlier example, libraries contain code that other people have written. There are a few common ones that you will see regularly, especially the \texttt{tidyverse}. To use a package, we must first install it and then we need to load it. A package only needs to be installed once per computer but must be loaded every time. So, the packages that we installed earlier do not need to be reinstalled here.

Given that folks freely gave up their time to make \texttt{R} and the packages that we use, it is important to cite them. To get the information that is needed, we can use \texttt{citation()}. When run without any arguments, that provides the citation information for \texttt{R} itself, and when run with an argument that is the name of a package, it provides the citation information for that package.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{citation}\NormalTok{() }\CommentTok{\# Get the citation information for R}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} To cite R in publications use:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   R Core Team (2021). R: A language and environment}
\CommentTok{\#\textgreater{}   for statistical computing. R Foundation for}
\CommentTok{\#\textgreater{}   Statistical Computing, Vienna, Austria. URL}
\CommentTok{\#\textgreater{}   https://www.R{-}project.org/.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} A BibTeX entry for LaTeX users is}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   @Manual\{,}
\CommentTok{\#\textgreater{}     title = \{R: A Language and Environment for Statistical Computing\},}
\CommentTok{\#\textgreater{}     author = \{\{R Core Team\}\},}
\CommentTok{\#\textgreater{}     organization = \{R Foundation for Statistical Computing\},}
\CommentTok{\#\textgreater{}     address = \{Vienna, Austria\},}
\CommentTok{\#\textgreater{}     year = \{2021\},}
\CommentTok{\#\textgreater{}     url = \{https://www.R{-}project.org/\},}
\CommentTok{\#\textgreater{}   \}}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} We have invested a lot of time and effort in creating}
\CommentTok{\#\textgreater{} R, please cite it when using it for data analysis.}
\CommentTok{\#\textgreater{} See also \textquotesingle{}citation("pkgname")\textquotesingle{} for citing R packages.}
\FunctionTok{citation}\NormalTok{(}\StringTok{\textquotesingle{}tidyverse\textquotesingle{}}\NormalTok{) }\CommentTok{\# Get the citation information for a particular package}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   Wickham et al., (2019). Welcome to the tidyverse.}
\CommentTok{\#\textgreater{}   Journal of Open Source Software, 4(43), 1686,}
\CommentTok{\#\textgreater{}   https://doi.org/10.21105/joss.01686}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} A BibTeX entry for LaTeX users is}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   @Article\{,}
\CommentTok{\#\textgreater{}     title = \{Welcome to the \{tidyverse\}\},}
\CommentTok{\#\textgreater{}     author = \{Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D\textquotesingle{}Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani\},}
\CommentTok{\#\textgreater{}     year = \{2019\},}
\CommentTok{\#\textgreater{}     journal = \{Journal of Open Source Software\},}
\CommentTok{\#\textgreater{}     volume = \{4\},}
\CommentTok{\#\textgreater{}     number = \{43\},}
\CommentTok{\#\textgreater{}     pages = \{1686\},}
\CommentTok{\#\textgreater{}     doi = \{10.21105/joss.01686\},}
\CommentTok{\#\textgreater{}   \}}
\end{Highlighting}
\end{Shaded}

Turning to the simulation, we need three columns: `date', `shelter', and `occupancy'. This example will build on the earlier one by adding a seed using \texttt{set.seed()}. A seed enables us to always generate the same random data. Any integer can be used as the seed. In this case the seed will be 853. If you use that as your seed, then you should get the same random numbers as in this example. If you use a different seed, then you should expect different random numbers. Finally, we use \texttt{rep()} to repeat something a certain number of times. For instance, we repeat `Shelter 1', 184 times which accounts for half a year.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Simulate \#\#\#\#}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)   }

\NormalTok{simulated\_occupancy\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{date =} \FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \FunctionTok{as.Date}\NormalTok{(}\StringTok{"2021{-}07{-}01"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{183}\NormalTok{), }\AttributeTok{times =} \DecValTok{3}\NormalTok{), }
    \CommentTok{\# Based on Dirk Eddelbuettel: https://stackoverflow.com/a/21502386}
    \AttributeTok{shelter =} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{"Shelter 1"}\NormalTok{, }\AttributeTok{times =} \DecValTok{184}\NormalTok{), }
                \FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{"Shelter 2"}\NormalTok{, }\AttributeTok{times =} \DecValTok{184}\NormalTok{),}
                \FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{"Shelter 3"}\NormalTok{, }\AttributeTok{times =} \DecValTok{184}\NormalTok{)),}
    \AttributeTok{number\_occupied =} 
      \FunctionTok{rpois}\NormalTok{(}\AttributeTok{n =} \DecValTok{184}\SpecialCharTok{*}\DecValTok{3}\NormalTok{,}
            \AttributeTok{lambda =} \DecValTok{30}\NormalTok{) }\CommentTok{\# Draw 552 times from the Poisson distribution}
\NormalTok{    )}

\FunctionTok{head}\NormalTok{(simulated\_occupancy\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{}   date       shelter   number\_occupied}
\CommentTok{\#\textgreater{}   \textless{}date\textgreater{}     \textless{}chr\textgreater{}               \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 2021{-}07{-}01 Shelter 1              28}
\CommentTok{\#\textgreater{} 2 2021{-}07{-}02 Shelter 1              29}
\CommentTok{\#\textgreater{} 3 2021{-}07{-}03 Shelter 1              35}
\CommentTok{\#\textgreater{} 4 2021{-}07{-}04 Shelter 1              25}
\CommentTok{\#\textgreater{} 5 2021{-}07{-}05 Shelter 1              21}
\CommentTok{\#\textgreater{} 6 2021{-}07{-}06 Shelter 1              30}
\end{Highlighting}
\end{Shaded}

In this simulation we first create a list of all the dates in 2021. We repeat that list three times. We assume data for three shelters for every day of the year. To simulate the number of beds that are occupied each night, we draw from a Poisson distribution, assuming a mean number of 30 beds occupied per shelter.

\hypertarget{acquire-1}{%
\subsection{Acquire}\label{acquire-1}}

We use data made available about Toronto homeless shelters by the City of Toronto. The premise of the data is that each night at 4am a count is made of the occupied beds. To access the data, we use \texttt{opendatatoronto} \citep{citeSharla} and then save our own copy.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Acquire \#\#\#\#}
\CommentTok{\# Based on code from: }
\CommentTok{\# https://open.toronto.ca/dataset/daily{-}shelter{-}overnight{-}service{-}occupancy{-}capacity/}
\CommentTok{\# Thank you to Heath Priston for assistance}
\NormalTok{toronto\_shelters }\OtherTok{\textless{}{-}} 
  \CommentTok{\# Each package is associated with a unique id which can be found in }
  \CommentTok{\# \textquotesingle{}For Developers\textquotesingle{}:}
  \CommentTok{\# https://open.toronto.ca/dataset/daily{-}shelter{-}overnight{-}service{-}occupancy{-}capacity/}
  \FunctionTok{list\_package\_resources}\NormalTok{(}\StringTok{"21c83b32{-}d5a8{-}4106{-}a54f{-}010dbe49f6f2"}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \CommentTok{\# Within that package, we are interested in the 2021 dataset}
  \FunctionTok{filter}\NormalTok{(name }\SpecialCharTok{==} \StringTok{"daily{-}shelter{-}overnight{-}service{-}occupancy{-}capacity{-}2021"}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \CommentTok{\# Having reduce the dataset down to one row we can get the resource}
  \FunctionTok{get\_resource}\NormalTok{()}

\FunctionTok{write\_csv}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ toronto\_shelters, }
  \AttributeTok{file =} \StringTok{"toronto\_shelters.csv"}
\NormalTok{  )}

\FunctionTok{head}\NormalTok{(toronto\_shelters)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 6 x 32
#>     `_id` OCCUPANCY_DATE ORGANIZATION_ID ORGANIZATION_NAME  
#>     <dbl> <date>                   <dbl> <chr>              
#> 1 7272806 2021-01-01                  24 COSTI Immigrant Se~
#> 2 7272807 2021-01-01                  24 COSTI Immigrant Se~
#> 3 7272808 2021-01-01                  24 COSTI Immigrant Se~
#> 4 7272809 2021-01-01                  24 COSTI Immigrant Se~
#> 5 7272810 2021-01-01                  24 COSTI Immigrant Se~
#> 6 7272811 2021-01-01                  24 COSTI Immigrant Se~
#> # ... with 28 more variables: SHELTER_ID <dbl>,
#> #   SHELTER_GROUP <chr>, LOCATION_ID <dbl>,
#> #   LOCATION_NAME <chr>, LOCATION_ADDRESS <chr>,
#> #   LOCATION_POSTAL_CODE <chr>, LOCATION_CITY <chr>,
#> #   LOCATION_PROVINCE <chr>, PROGRAM_ID <dbl>,
#> #   PROGRAM_NAME <chr>, SECTOR <chr>, PROGRAM_MODEL <chr>,
#> #   OVERNIGHT_SERVICE_TYPE <chr>, PROGRAM_AREA <chr>, ...
\end{verbatim}

Not much needs to be done to this to make it similar to the dataset that we were interested in (Figure \ref{fig:torontohomelessdataplan}). We need to change the names to make them easier to type using \texttt{clean\_names()}, reduce the columns to only those that are relevant using \texttt{select()}, and only keep the second half of the year using \texttt{filter()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{toronto\_shelters\_clean }\OtherTok{\textless{}{-}} 
  \FunctionTok{clean\_names}\NormalTok{(toronto\_shelters) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{select}\NormalTok{(occupancy\_date, id, occupied\_beds) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(occupancy\_date }\SpecialCharTok{\textgreater{}=} \FunctionTok{as\_date}\NormalTok{(}\StringTok{"2021{-}07{-}01"}\NormalTok{))}

\FunctionTok{head}\NormalTok{(toronto\_shelters\_clean)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{}   occupancy\_date      id occupied\_beds}
\CommentTok{\#\textgreater{}   \textless{}date\textgreater{}           \textless{}dbl\textgreater{}         \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 2021{-}12{-}27     7323151            50}
\CommentTok{\#\textgreater{} 2 2021{-}12{-}27     7323152            18}
\CommentTok{\#\textgreater{} 3 2021{-}12{-}27     7323153            28}
\CommentTok{\#\textgreater{} 4 2021{-}12{-}27     7323154            50}
\CommentTok{\#\textgreater{} 5 2021{-}12{-}27     7323155            NA}
\CommentTok{\#\textgreater{} 6 2021{-}12{-}27     7323156            NA}
\end{Highlighting}
\end{Shaded}

All that remains is to save the cleaned dataset.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write\_csv}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ toronto\_shelters\_clean, }
  \AttributeTok{file =} \StringTok{"cleaned\_toronto\_shelters.csv"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{explore-1}{%
\subsection{Explore}\label{explore-1}}

First, we load the dataset that we just created.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Explore \#\#\#\#}
\NormalTok{toronto\_shelters\_clean }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}
    \StringTok{"cleaned\_toronto\_shelters.csv"}\NormalTok{,}
    \AttributeTok{show\_col\_types =} \ConstantTok{FALSE}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

The dataset is on a daily basis for each shelter. We are interested in understanding average usage for each month. To do this, we need to and add a month column, which we do using \texttt{month()} from \texttt{lubridate} \citep{GrolemundWickham2011}. By default, \texttt{month()} provides the number of the month, and so we include two arguments `label' and `abbr' to get the full name of the month. We remove rows that do not have any data for the number of beds using \texttt{drop\_na()} from \texttt{tidyr}. And we then create a summary statistic on the basis of monthly groups, using \texttt{summarize()} from \texttt{dplyr} \citep{citedplyr}. We use \texttt{kable()} from \texttt{knitr} \citep{citeknitr} to create a table.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Based on code from Florence Vallée{-}Dubois and Lisa Lendway}
\NormalTok{toronto\_shelters\_clean }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{occupancy\_month =} \FunctionTok{month}\NormalTok{(occupancy\_date, }
                                 \AttributeTok{label =} \ConstantTok{TRUE}\NormalTok{, }
                                 \AttributeTok{abbr =} \ConstantTok{FALSE}\NormalTok{)) }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{drop\_na}\NormalTok{(occupied\_beds) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} \CommentTok{\# We only want rows that have data}
  \FunctionTok{group\_by}\NormalTok{(occupancy\_month) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} \CommentTok{\# We want to know the occupancy by month}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{number\_occupied =} \FunctionTok{mean}\NormalTok{(occupied\_beds)) }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r}
\hline
occupancy\_month & number\_occupied\\
\hline
July & 29.67137\\
\hline
August & 30.83975\\
\hline
September & 31.65405\\
\hline
October & 32.32991\\
\hline
November & 33.26980\\
\hline
December & 33.57806\\
\hline
\end{tabular}

As with before, this looks fine, and achieves what we set out to do. But we can make some tweaks to the defaults to make it look even better (Table \ref{tab:homelessoccupancy}). We can add a caption, make the column names easier to read, only show an appropriate level of decimal places, and improve the formatting.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{toronto\_shelters\_clean }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{occupancy\_month =} \FunctionTok{month}\NormalTok{(occupancy\_date, }
                                 \AttributeTok{label =} \ConstantTok{TRUE}\NormalTok{, }
                                 \AttributeTok{abbr =} \ConstantTok{FALSE}\NormalTok{)) }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{drop\_na}\NormalTok{(occupied\_beds) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} \CommentTok{\# We only want rows that have data}
  \FunctionTok{group\_by}\NormalTok{(occupancy\_month) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} \CommentTok{\# We want to know the occupancy by month}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{number\_occupied =} \FunctionTok{mean}\NormalTok{(occupied\_beds)) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{caption =} \StringTok{"Homeless shelter usage in Toronto in 2021"}\NormalTok{, }
        \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Month"}\NormalTok{, }\StringTok{"Average daily number of occupied beds"}\NormalTok{),}
        \AttributeTok{digits =} \DecValTok{1}\NormalTok{,}
        \AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{linesep =} \StringTok{""}
\NormalTok{        )}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:homelessoccupancy}Homeless shelter usage in Toronto in 2021}
\centering
\begin{tabular}[t]{lr}
\toprule
Month & Average daily number of occupied beds\\
\midrule
July & 29.7\\
August & 30.8\\
September & 31.7\\
October & 32.3\\
November & 33.3\\
December & 33.6\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{communicate-1}{%
\subsection{Communicate}\label{communicate-1}}

We need to write a few brief paragraphs about what we did, why we did it, and what we found. An example follows.

\begin{quote}
Toronto has a large homeless population. Freezing winters mean it is critical there are enough places in shelters. We are interested to understand how usage of shelters changes in colder months, compared with warmer months.

We use data provided by the City of Toronto about Toronto homeless shelter bed occupancy. Specifically, at 4am each night a count is made of the occupied beds. We are interested in averaging this over the month. We cleaned, tidied, and analyzed the dataset using the statistical programming language R \citep{citeR} as well as the packages \texttt{tidyverse} \citep{Wickham2017}, \texttt{janitor} \citep{janitor}, \texttt{tidyr} \citep{citetidyr}, \texttt{opendatatoronto} \citep{citeSharla}, \texttt{lubridate} \citep{GrolemundWickham2011}, and \texttt{knitr} \citep{citeknitr}. We then made a table of the average number of occupied beds each night for each month (Table \ref{tab:homelessoccupancy}).

We found that the daily average number of occupied beds was higher in December 2021 than July 2021, with 34 occupied beds in December, compared with 30 in July (Table \ref{tab:homelessoccupancy}). More generally, there was a steady increase in the daily average number of occupied beds between July and December, with a slight increase each month.

The dataset is on the basis of shelters, and so our results may be skewed by changes that are specific to especially large or especially small shelters. It may be that particular shelters are especially attractive in colder months. Additionally, we were concerned with counts of the number of occupied beds, but if the supply of beds changes over the season, then an additional statistic of interest would be proportion occupied.
\end{quote}

Although this example is only a few paragraphs, it could be reduced to form an abstract, or increased to form a full report. The first paragraph is a general overview, the second focuses on the data, the third on the results, and the fourth is a discussion. Each of these could be expanded to form sections of a short report.

\hypertarget{neonatal-mortality}{%
\section{Neonatal mortality}\label{neonatal-mortality}}

Neonatal mortality refers to a death that occurs within the first month of life, and in particular, the neonatal mortality rate (NMR) is the number of neonatal deaths per 1,000 live births \citep{unigme}. Reducing it is part of the third Sustainable Development Goal \citep{hug2019national}. In this example we will create a graph of the estimated NMR for the past fifty years for: Argentina, Australia, Canada, and Kenya.

\hypertarget{plan-2}{%
\subsection{Plan}\label{plan-2}}

For this example, we need to think about what our dataset should look like, and what the graph should look like.

The dataset needs to have columns that specify the country, and the year. It also needs to have a column with the NMR estimate for that year for that country. Roughly, it should look like Figure \ref{fig:nmrexampledata}.

\begin{figure}

{\centering \includegraphics[width=0.4\linewidth]{/Users/rohanalexander/Documents/book/figures/IMG_1812} 

}

\caption{Quick sketch of a potentially useful NMR dataset}\label{fig:nmrexampledata}
\end{figure}

We are interested to make a graph with year on the x-axis and estimated NMR on the y-axis. Each country should have its own series Roughly similar to Figure \ref{fig:nmrexamplegraph}.

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{/Users/rohanalexander/Documents/book/figures/IMG_1813} 

}

\caption{Quick sketch of a graph of NMR by country over time}\label{fig:nmrexamplegraph}
\end{figure}

\hypertarget{simulate-2}{%
\subsection{Simulate}\label{simulate-2}}

We would like to simulate some data that aligns with our plan. In this case we will need three columns: country, year, and NMR.

Within R Studio Cloud make a new R Markdown file and save it. Add preamble documentation and set-up the workspace. We will use \texttt{tidyverse} \citep{Wickham2017}, \texttt{janitor} \citep{janitor}, and \texttt{lubridate} \citep{GrolemundWickham2011}.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Preamble \#\#\#\#}
\CommentTok{\# Purpose: Obtain and prepare data about neonatal mortality for four}
\CommentTok{\# countries for the past fifty years and create a graph.}
\CommentTok{\# Author: Rohan Alexander}
\CommentTok{\# Email: rohan.alexander@utoronto.ca}
\CommentTok{\# Date: 1 January 2022}
\CommentTok{\# Prerequisites: {-} }

\DocumentationTok{\#\#\#\# Workspace set{-}up \#\#\#\#}
\FunctionTok{library}\NormalTok{(janitor)}
\FunctionTok{library}\NormalTok{(lubridate)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

The code contained in libraries can change from time to time as the authors update it and release new versions. We can see which version of a package we are using with \texttt{packageVersion()}. For instance, we are using version 1.3.1 of the \texttt{tidyverse} and version 2.1.0 of \texttt{janitor}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{packageVersion}\NormalTok{(}\StringTok{\textquotesingle{}tidyverse\textquotesingle{}}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] \textquotesingle{}1.3.1\textquotesingle{}}
\FunctionTok{packageVersion}\NormalTok{(}\StringTok{\textquotesingle{}janitor\textquotesingle{}}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] \textquotesingle{}2.1.0\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

To update the version of the package, we use \texttt{update.packages()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{update.packages}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

This does not need to be run, say, every day, but from time-to-time it is worth updating packages. While many packages take care to ensure backward compatibility, at a certain point this does not become reasonable, and so it is important to be aware the updating packages can result in old code needing to be updated.

Returning to the simulation, we repeat the name of each country 50 times with \texttt{rep()}, and enable the passing of 50 years. Finally, we draw from the uniform distribution with \texttt{runif()} to simulate an estimated NMR value for that year for that country.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Simulate data \#\#\#\#}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{simulated\_nmr\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{country =} 
      \FunctionTok{c}\NormalTok{(}
        \FunctionTok{rep}\NormalTok{(}\StringTok{\textquotesingle{}Argentina\textquotesingle{}}\NormalTok{, }\DecValTok{50}\NormalTok{),}
        \FunctionTok{rep}\NormalTok{(}\StringTok{\textquotesingle{}Australia\textquotesingle{}}\NormalTok{, }\DecValTok{50}\NormalTok{),}
        \FunctionTok{rep}\NormalTok{(}\StringTok{\textquotesingle{}Canada\textquotesingle{}}\NormalTok{, }\DecValTok{50}\NormalTok{),}
        \FunctionTok{rep}\NormalTok{(}\StringTok{\textquotesingle{}Kenya\textquotesingle{}}\NormalTok{, }\DecValTok{50}\NormalTok{)}
\NormalTok{        ),}
    \AttributeTok{year =} 
      \FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1971}\SpecialCharTok{:}\DecValTok{2020}\NormalTok{), }\DecValTok{4}\NormalTok{),}
    \AttributeTok{nmr =} 
      \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =} \DecValTok{200}\NormalTok{,}
            \AttributeTok{min =} \DecValTok{0}\NormalTok{, }
            \AttributeTok{max =} \DecValTok{100}\NormalTok{)}
\NormalTok{  )}

\FunctionTok{head}\NormalTok{(simulated\_nmr\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{}   country    year   nmr}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}     \textless{}int\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Argentina  1971 35.9 }
\CommentTok{\#\textgreater{} 2 Argentina  1972 12.0 }
\CommentTok{\#\textgreater{} 3 Argentina  1973 48.4 }
\CommentTok{\#\textgreater{} 4 Argentina  1974 31.6 }
\CommentTok{\#\textgreater{} 5 Argentina  1975  3.74}
\CommentTok{\#\textgreater{} 6 Argentina  1976 40.4}
\end{Highlighting}
\end{Shaded}

While this simulation works, it would be time-consuming and error-prone if we decided that instead of fifty years, we were interested in simulating, say, sixty years. One way to make this easier is to replace all instances of 50 with a variable. An example follows.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Simulate data \#\#\#\#}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{number\_of\_years }\OtherTok{\textless{}{-}} \DecValTok{50}

\NormalTok{simulated\_nmr\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{country =} 
      \FunctionTok{c}\NormalTok{(}
        \FunctionTok{rep}\NormalTok{(}\StringTok{\textquotesingle{}Argentina\textquotesingle{}}\NormalTok{, number\_of\_years),}
        \FunctionTok{rep}\NormalTok{(}\StringTok{\textquotesingle{}Australia\textquotesingle{}}\NormalTok{, number\_of\_years),}
        \FunctionTok{rep}\NormalTok{(}\StringTok{\textquotesingle{}Canada\textquotesingle{}}\NormalTok{, number\_of\_years),}
        \FunctionTok{rep}\NormalTok{(}\StringTok{\textquotesingle{}Kenya\textquotesingle{}}\NormalTok{, number\_of\_years)}
\NormalTok{        ),}
    \AttributeTok{year =} 
      \FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{number\_of\_years }\SpecialCharTok{+} \DecValTok{1970}\NormalTok{), }\DecValTok{4}\NormalTok{),}
    \AttributeTok{nmr =} 
      \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_years }\SpecialCharTok{*} \DecValTok{4}\NormalTok{,}
            \AttributeTok{min =} \DecValTok{0}\NormalTok{, }
            \AttributeTok{max =} \DecValTok{100}\NormalTok{)}
\NormalTok{  )}

\FunctionTok{head}\NormalTok{(simulated\_nmr\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{}   country    year   nmr}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}     \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Argentina  1971 35.9 }
\CommentTok{\#\textgreater{} 2 Argentina  1972 12.0 }
\CommentTok{\#\textgreater{} 3 Argentina  1973 48.4 }
\CommentTok{\#\textgreater{} 4 Argentina  1974 31.6 }
\CommentTok{\#\textgreater{} 5 Argentina  1975  3.74}
\CommentTok{\#\textgreater{} 6 Argentina  1976 40.4}
\end{Highlighting}
\end{Shaded}

The result will be the same, but now if we want to change from fifty to sixty years is to make the change in one place.

We can have confidence in this simulated dataset because it is relatively straight-forward, and we wrote the code for it. But when we turn to the real dataset, it is more difficult to be sure that it is what it claims to be. Even if we trust the data, it is important that we can share that confidence with others. One way forward is to establish some checks that prove our data are as they should be. For instance, we expect:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  That `country' is, and only is, one of these four: `Argentina', `Australia', `Canada', or `Kenya'.
\item
  Conversely, that `country' contains all those four countries.
\item
  That `year' is no smaller than 1971 and no larger than 2020, and is a number, not a letter.
\item
  That `nmr' is a value somewhere between 0 and 1,000, and is a number.
\end{enumerate}

We can write a series of tests based on these features, that we expect that dataset to pass.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Tests for simulated data}
\NormalTok{simulated\_nmr\_data}\SpecialCharTok{$}\NormalTok{country }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{unique}\NormalTok{() }\SpecialCharTok{==} \FunctionTok{c}\NormalTok{(}\StringTok{"Argentina"}\NormalTok{, }
                \StringTok{"Australia"}\NormalTok{, }
                \StringTok{"Canada"}\NormalTok{, }
                \StringTok{"Kenya"}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] TRUE TRUE TRUE TRUE}

\NormalTok{simulated\_nmr\_data}\SpecialCharTok{$}\NormalTok{country }\SpecialCharTok{|}\ErrorTok{\textgreater{}} \FunctionTok{unique}\NormalTok{() }\SpecialCharTok{|}\ErrorTok{\textgreater{}} \FunctionTok{length}\NormalTok{() }\SpecialCharTok{==} \DecValTok{4}
\CommentTok{\#\textgreater{} [1] TRUE}

\NormalTok{simulated\_nmr\_data}\SpecialCharTok{$}\NormalTok{year }\SpecialCharTok{|}\ErrorTok{\textgreater{}} \FunctionTok{min}\NormalTok{() }\SpecialCharTok{==} \DecValTok{1971}
\CommentTok{\#\textgreater{} [1] TRUE}

\NormalTok{simulated\_nmr\_data}\SpecialCharTok{$}\NormalTok{year }\SpecialCharTok{|}\ErrorTok{\textgreater{}} \FunctionTok{max}\NormalTok{() }\SpecialCharTok{==} \DecValTok{2020}
\CommentTok{\#\textgreater{} [1] TRUE}

\NormalTok{simulated\_nmr\_data}\SpecialCharTok{$}\NormalTok{nmr }\SpecialCharTok{|}\ErrorTok{\textgreater{}} \FunctionTok{min}\NormalTok{() }\SpecialCharTok{\textgreater{}=} \DecValTok{0}
\CommentTok{\#\textgreater{} [1] TRUE}

\NormalTok{simulated\_nmr\_data}\SpecialCharTok{$}\NormalTok{nmr }\SpecialCharTok{|}\ErrorTok{\textgreater{}} \FunctionTok{max}\NormalTok{() }\SpecialCharTok{\textless{}=} \DecValTok{1000}
\CommentTok{\#\textgreater{} [1] TRUE}

\NormalTok{simulated\_nmr\_data}\SpecialCharTok{$}\NormalTok{nmr }\SpecialCharTok{|}\ErrorTok{\textgreater{}} \FunctionTok{class}\NormalTok{() }\SpecialCharTok{==} \StringTok{"numeric"}
\CommentTok{\#\textgreater{} [1] TRUE}
\end{Highlighting}
\end{Shaded}

Having passed these tests, we can have confidence in the simulated dataset. More importantly, we can apply these tests to the real dataset. This enables us to have greater confidence in that dataset and to share that confidence with others.

\hypertarget{acquire-2}{%
\subsection{Acquire}\label{acquire-2}}

The UN Inter-agency Group for Child Mortality Estimation (IGME) provides estimates of the NMR -- \url{https://childmortality.org/} -- that we can download and save.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Acquire data \#\#\#\#}
\NormalTok{raw\_igme\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}
    \AttributeTok{file =}
      \StringTok{"https://childmortality.org/wp{-}content/uploads/2021/09/UNIGME{-}2021.csv"}\NormalTok{,}
    \AttributeTok{show\_col\_types =} \ConstantTok{FALSE}\NormalTok{) }

\FunctionTok{write\_csv}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ raw\_igme\_data, }
  \AttributeTok{file =} \StringTok{"igme.csv"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

We can take a quick look to get a better sense of it. We might be interested in what the dataset seems to look like (using \texttt{head()} and \texttt{tail()}), and what the names of the columns are (using \texttt{names()}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(raw\_igme\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 29}
\CommentTok{\#\textgreater{}   \textasciigrave{}Geographic area\textasciigrave{} Indicator         Sex   \textasciigrave{}Wealth Quintil\textasciitilde{}}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}             \textless{}chr\textgreater{}             \textless{}chr\textgreater{} \textless{}chr\textgreater{}           }
\CommentTok{\#\textgreater{} 1 Afghanistan       Neonatal mortali\textasciitilde{} Total Total           }
\CommentTok{\#\textgreater{} 2 Afghanistan       Neonatal mortali\textasciitilde{} Total Total           }
\CommentTok{\#\textgreater{} 3 Afghanistan       Neonatal mortali\textasciitilde{} Total Total           }
\CommentTok{\#\textgreater{} 4 Afghanistan       Neonatal mortali\textasciitilde{} Total Total           }
\CommentTok{\#\textgreater{} 5 Afghanistan       Neonatal mortali\textasciitilde{} Total Total           }
\CommentTok{\#\textgreater{} 6 Afghanistan       Neonatal mortali\textasciitilde{} Total Total           }
\CommentTok{\#\textgreater{} \# ... with 25 more variables: Series Name \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   Series Year \textless{}chr\textgreater{}, Regional group \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   TIME\_PERIOD \textless{}chr\textgreater{}, OBS\_VALUE \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   COUNTRY\_NOTES \textless{}chr\textgreater{}, CONNECTION \textless{}lgl\textgreater{},}
\CommentTok{\#\textgreater{} \#   DEATH\_CATEGORY \textless{}lgl\textgreater{}, CATEGORY \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   Observation Status \textless{}chr\textgreater{}, Unit of measure \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   Series Category \textless{}chr\textgreater{}, Series Type \textless{}chr\textgreater{}, ...}
\FunctionTok{names}\NormalTok{(raw\_igme\_data)}
\CommentTok{\#\textgreater{}  [1] "Geographic area"        "Indicator"             }
\CommentTok{\#\textgreater{}  [3] "Sex"                    "Wealth Quintile"       }
\CommentTok{\#\textgreater{}  [5] "Series Name"            "Series Year"           }
\CommentTok{\#\textgreater{}  [7] "Regional group"         "TIME\_PERIOD"           }
\CommentTok{\#\textgreater{}  [9] "OBS\_VALUE"              "COUNTRY\_NOTES"         }
\CommentTok{\#\textgreater{} [11] "CONNECTION"             "DEATH\_CATEGORY"        }
\CommentTok{\#\textgreater{} [13] "CATEGORY"               "Observation Status"    }
\CommentTok{\#\textgreater{} [15] "Unit of measure"        "Series Category"       }
\CommentTok{\#\textgreater{} [17] "Series Type"            "STD\_ERR"               }
\CommentTok{\#\textgreater{} [19] "REF\_DATE"               "Age Group of Women"    }
\CommentTok{\#\textgreater{} [21] "Time Since First Birth" "DEFINITION"            }
\CommentTok{\#\textgreater{} [23] "INTERVAL"               "Series Method"         }
\CommentTok{\#\textgreater{} [25] "LOWER\_BOUND"            "UPPER\_BOUND"           }
\CommentTok{\#\textgreater{} [27] "STATUS"                 "YEAR\_TO\_ACHIEVE"       }
\CommentTok{\#\textgreater{} [29] "Model Used"}
\end{Highlighting}
\end{Shaded}

We would like to clean up the names and only keep the rows and columns that we are interested in. Based on our plan, we are interested in rows where `Sex' is `Total', `Series Name' is `UN IGME estimate', `Geographic area' is one of `Argentina', `Australia', `Canada', and `Kenya', and the `Indicator' is `Neonatal mortality rate'. After this we are interested in just a few columns: `geographic\_area', `time\_period', and `obs\_value'.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_igme\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{clean\_names}\NormalTok{(raw\_igme\_data) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(sex }\SpecialCharTok{==} \StringTok{\textquotesingle{}Total\textquotesingle{}}\NormalTok{,}
\NormalTok{         series\_name }\SpecialCharTok{==} \StringTok{\textquotesingle{}UN IGME estimate\textquotesingle{}}\NormalTok{,}
\NormalTok{         geographic\_area }\SpecialCharTok{\%in\%} 
           \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Argentina\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Australia\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Canada\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Kenya\textquotesingle{}}\NormalTok{),}
\NormalTok{         indicator }\SpecialCharTok{==} \StringTok{\textquotesingle{}Neonatal mortality rate\textquotesingle{}}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{select}\NormalTok{(geographic\_area,}
\NormalTok{         time\_period,}
\NormalTok{         obs\_value)}

\FunctionTok{head}\NormalTok{(cleaned\_igme\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{}   geographic\_area time\_period obs\_value}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}           \textless{}chr\textgreater{}           \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Argentina       1970{-}06          24.9}
\CommentTok{\#\textgreater{} 2 Argentina       1971{-}06          24.7}
\CommentTok{\#\textgreater{} 3 Argentina       1972{-}06          24.6}
\CommentTok{\#\textgreater{} 4 Argentina       1973{-}06          24.6}
\CommentTok{\#\textgreater{} 5 Argentina       1974{-}06          24.5}
\CommentTok{\#\textgreater{} 6 Argentina       1975{-}06          24.1}
\end{Highlighting}
\end{Shaded}

Finally, we need to fix two final aspects: the class of `time\_period' is character when we need it to be a year, and the name of `obs\_value' should be `nmr' to be more informative.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_igme\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  cleaned\_igme\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{time\_period =} \FunctionTok{str\_remove}\NormalTok{(time\_period, }\StringTok{"{-}06"}\NormalTok{),}
         \AttributeTok{time\_period =} \FunctionTok{as.integer}\NormalTok{(time\_period)) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(time\_period }\SpecialCharTok{\textgreater{}=} \DecValTok{1971}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{nmr =}\NormalTok{ obs\_value,}
         \AttributeTok{year =}\NormalTok{ time\_period,}
         \AttributeTok{country =}\NormalTok{ geographic\_area)}

\FunctionTok{head}\NormalTok{(cleaned\_igme\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{}   country    year   nmr}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}     \textless{}int\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Argentina  1971  24.7}
\CommentTok{\#\textgreater{} 2 Argentina  1972  24.6}
\CommentTok{\#\textgreater{} 3 Argentina  1973  24.6}
\CommentTok{\#\textgreater{} 4 Argentina  1974  24.5}
\CommentTok{\#\textgreater{} 5 Argentina  1975  24.1}
\CommentTok{\#\textgreater{} 6 Argentina  1976  23.3}
\end{Highlighting}
\end{Shaded}

Finally, we can check that our dataset passes the tests that we developed based on the simulated dataset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Test the cleaned dataset}
\NormalTok{cleaned\_igme\_data}\SpecialCharTok{$}\NormalTok{country }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{unique}\NormalTok{() }\SpecialCharTok{==} \FunctionTok{c}\NormalTok{(}\StringTok{"Argentina"}\NormalTok{, }
                \StringTok{"Australia"}\NormalTok{, }
                \StringTok{"Canada"}\NormalTok{, }
                \StringTok{"Kenya"}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] TRUE TRUE TRUE TRUE}

\NormalTok{cleaned\_igme\_data}\SpecialCharTok{$}\NormalTok{country }\SpecialCharTok{|}\ErrorTok{\textgreater{}} \FunctionTok{unique}\NormalTok{() }\SpecialCharTok{|}\ErrorTok{\textgreater{}} \FunctionTok{length}\NormalTok{() }\SpecialCharTok{==} \DecValTok{4}
\CommentTok{\#\textgreater{} [1] TRUE}

\NormalTok{cleaned\_igme\_data}\SpecialCharTok{$}\NormalTok{year }\SpecialCharTok{|}\ErrorTok{\textgreater{}} \FunctionTok{min}\NormalTok{() }\SpecialCharTok{==} \DecValTok{1971}
\CommentTok{\#\textgreater{} [1] TRUE}

\NormalTok{cleaned\_igme\_data}\SpecialCharTok{$}\NormalTok{year }\SpecialCharTok{|}\ErrorTok{\textgreater{}} \FunctionTok{max}\NormalTok{() }\SpecialCharTok{==} \DecValTok{2020}
\CommentTok{\#\textgreater{} [1] TRUE}

\NormalTok{cleaned\_igme\_data}\SpecialCharTok{$}\NormalTok{nmr }\SpecialCharTok{|}\ErrorTok{\textgreater{}} \FunctionTok{min}\NormalTok{() }\SpecialCharTok{\textgreater{}=} \DecValTok{0}
\CommentTok{\#\textgreater{} [1] TRUE}

\NormalTok{cleaned\_igme\_data}\SpecialCharTok{$}\NormalTok{nmr }\SpecialCharTok{|}\ErrorTok{\textgreater{}} \FunctionTok{max}\NormalTok{() }\SpecialCharTok{\textless{}=} \DecValTok{1000}
\CommentTok{\#\textgreater{} [1] TRUE}

\NormalTok{cleaned\_igme\_data}\SpecialCharTok{$}\NormalTok{nmr }\SpecialCharTok{|}\ErrorTok{\textgreater{}} \FunctionTok{class}\NormalTok{() }\SpecialCharTok{==} \StringTok{"numeric"}
\CommentTok{\#\textgreater{} [1] TRUE}
\end{Highlighting}
\end{Shaded}

All that remains is to save the nicely cleaned dataset.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write\_csv}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ cleaned\_igme\_data, }
  \AttributeTok{file =} \StringTok{"cleaned\_igme\_data.csv"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{explore-2}{%
\subsection{Explore}\label{explore-2}}

We would like to make a graph of estimated NMR using the cleaned dataset. First, we read in the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Explore \#\#\#\#}
\NormalTok{cleaned\_igme\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}
    \AttributeTok{file =} \StringTok{"cleaned\_igme\_data.csv"}\NormalTok{,}
    \AttributeTok{show\_col\_types =} \ConstantTok{FALSE}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

We can now make the graph that we are interested in (Figure \ref{fig:nmrgraph}). We are interested in showing how NMR has changed over time and the difference between countries.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_igme\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ year, }\AttributeTok{y =}\NormalTok{ nmr, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Year"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Neonatal Mortality Rate (NMR)"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Country"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{02-drinking_from_a_fire_hose_files/figure-latex/nmrgraph-1.pdf}
\caption{\label{fig:nmrgraph}Neonatal Mortality Rate (NMR), for Argentina, Australia, Canada, and Kenya, (1971-2020)}
\end{figure}

\hypertarget{communicate-2}{%
\subsection{Communicate}\label{communicate-2}}

To this point we downloaded some data, cleaned it, wrote some tests, and made a graph. We would typically need to communicate what we have done at some length. In this case, we will write a few paragraphs about what we did, why we did it, and what we found.

\begin{quote}
Neonatal mortality refers to a death that occurs within the first month of life. In particular, the neonatal mortality rate (NMR) is the number of neonatal deaths per 1,000 live births \citep{alexander2018global}. We obtain estimates for NMR for four countries---Argentina, Australia, Canada, China, and Kenya---over the past fifty years.

The UN Inter-agency Group for Child Mortality Estimation (IGME) provides estimates of the NMR at the website: \url{https://childmortality.org/}. We downloaded their estimates then cleaned and tidied the dataset using the statistical programming language R \citep{citeR}.

We found considerable change in the estimated NMR over time and between the four countries of interest (Figure \ref{fig:nmrgraph}). We found that the 1970s tended to be associated with reductions in the estimated NMR. Australia and Canada were estimated to have a low NMR at that point and remained there through 2020, with slight improvements. The estimates for Argentina and Kenya continued to have substantial reductions through 2020. Data were only available from 1990 for China and the estimates show a substantial reduction in the NMR, especially in the 1990s and 2000s.

Our results suggest considerable improvements in estimated NMR over time. But it is worth emphasizing that estimates of the NMR are based on a statistical model and underlying data. The paradox of data availability is that often high-quality data are less easily available for countries with worse outcomes. For instance, \citet{alexander2018global} say `{[}t{]}here is large variability in the availability of data on neonatal mortality'. Our conclusions are subject to the model that underpins the estimates, and the quality of the underlying data and we did not independently verify either of these.
\end{quote}

\hypertarget{exercises-and-tutorial-1}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-1}}

\hypertarget{exercises-1}{%
\subsection{Exercises}\label{exercises-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Following \citet{citeBarrett}, please write a stack of four or five atomic habits that you could implement this week.
\item
  What is not one of the four challenges for mitigating bias mentioned in \citet{hao2019} (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Unknown unknowns.
  \item
    Imperfect processes.
  \item
    The definitions of fairness.
  \item
    Lack of social context.
  \item
    Disinterest given profit considerations.
  \end{enumerate}
\item
  When was the dataset that underpins \citet{chambliss1989mundanity} collected (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    August 1983 to August 1984
  \item
    January 1983 to August 1984
  \item
    January 1983 to January 1984
  \item
    August 1983 to January 1984
  \end{enumerate}
\item
  When \citet{chambliss1989mundanity} talks of stratification, what is he talking about?
\item
  How does \citet{chambliss1989mundanity} define `excellence' (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Prolonged performance at world-class level.
  \item
    All Olympic medal winners.
  \item
    Consistent superiority of performance.
  \item
    All national-level athletes.
  \end{enumerate}
\item
  Think about the following quote from \citet[p.81]{chambliss1989mundanity} and list three small skills or activities that could help you achieve excellence in data science.
\end{enumerate}

\begin{quote}
Excellence is mundane. Superlative performance is really a confluence of dozens of small skills or activities, each one learned or stumbled upon, which have been carefully drilled into habit and then are fitted together in a synthesized whole. There is nothing extraordinary or super-human in any one of those actions; only the fact that they are done consistently and correctly, and all together, produce excellence.
\end{quote}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  Which of the following are arguments for \texttt{read\_csv()} from \texttt{readr} \citep{citereadr} (select all that apply)? (Hint: You can access the help for the function with \texttt{?readr::read\_csv()}.)

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    `all\_cols'
  \item
    `file'
  \item
    `show\_col\_types'
  \item
    `number'
  \end{enumerate}
\item
  We used \texttt{rpois()} and \texttt{runif()} to draw from the Poisson and Uniform distributions, respectively. Which of the following can be used to draw from the Normal and Binomial distributions (select all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{rnormal()} and \texttt{rbinom()}
  \item
    \texttt{rnorm()} and \texttt{rbinomial()}
  \item
    \texttt{rnormal()} and \texttt{rbinomial()}
  \item
    \texttt{rnorm()} and \texttt{rbinom()}
  \end{enumerate}
\item
  What is the result of \texttt{sample(x\ =\ letters,\ size\ =\ 2)} when the seed is set to `853'? What about when the seed is set to `1234' (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    `\,``i'' ``q''\,' and `\,``p'' ``v''\,'
  \item
    `\,``e'' ``l''\,' and `\,``e'' ``r''\,'
  \item
    `\,``i'' ``q''\,' and `\,``e'' ``r''\,'
  \item
    `\,``e'' ``l''\,' and `\,``p'' ``v''\,'
  \end{enumerate}
\item
  Which function provides the recommended citation to cite R (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{cite(\textquotesingle{}R\textquotesingle{})}.
  \item
    \texttt{cite()}.
  \item
    \texttt{citation(\textquotesingle{}R\textquotesingle{})}.
  \item
    \texttt{citation()}.
  \end{enumerate}
\item
  How do we get the citation information for \texttt{opendatatoronto} (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    cite()
  \item
    citation()
  \item
    cite(`opendatatoronto')
  \item
    citation(`opendatatoronto')
  \end{enumerate}
\item
  Which argument needs to be changed to change the headings in \texttt{kable()} from \texttt{knitr} \citep{citeknitr} (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    `booktabs'
  \item
    `col.names'
  \item
    `digits'
  \item
    `linesep'
  \item
    `caption'
  \end{enumerate}
\item
  Which function is used to update packages (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{update.packages()}
  \item
    \texttt{upgrade.packages()}
  \item
    \texttt{revise.packages()}
  \item
    \texttt{renovate.packages()}
  \end{enumerate}
\item
  What are some features that we might typically expect of a column that claimed to be a year (select all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    The class is `character'.
  \item
    There are no negative numbers.
  \item
    There are letters in the column.
  \item
    Each entry has four digits.
  \end{enumerate}
\end{enumerate}

\hypertarget{tutorial-1}{%
\subsection{Tutorial}\label{tutorial-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Please pick either the seed-growing, or hair-length, example from Chapter \ref{telling-stories-with-data}. You should have already started to record some data, but you probably do not have much of it. First sketch an example of what the dataset could look like. Please use \texttt{sample()} to create a tibble that has twelve weeks' worth of simulated data.
\item
  Pretend that the dataset that you just generated is the actual data that you end up with. Please write a page or so to communicate what you did, why, and what you found.
\item
  Reflecting on \citet{chambliss1989mundanity}, please write a page or so about stratification and excellence as it relates to using programming languages, such as R or Python, for data science.
\end{enumerate}

\hypertarget{r-essentials}{%
\chapter{R essentials}\label{r-essentials}}

\textbf{Required material}

\begin{itemize}
\tightlist
\item
  Read \emph{The Kitchen Counter Observatory}, \citep{kieranskitchen}
\item
  Read \emph{R for Data Science}, Chapter 5 `Data transformation', \citep{r4ds}
\item
  Read \emph{Data Feminism}, Chapter 6 `The Numbers Don't Speak for Themselves', \citep{datafeminism2020}
\end{itemize}

\textbf{Key concepts and skills}

\begin{itemize}
\tightlist
\item
  Understanding foundational aspects of R and R Studio.
\item
  Being able to use key \texttt{dplyr} verbs.
\item
  Know fundamentals of class and how to manipulate it.
\item
  Ability to simulate data.
\item
  Ability to make graphs in \texttt{ggplot2}.
\item
  Comfort with other aspects of the \texttt{tidyverse} including importing data, dataset manipulation, string manipulation, and factors.
\item
  Develop strategies for when things do not work.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{forcats} \citep{citeforcats}
\item
  \texttt{ggplot2} \citep{citeggplot}
\item
  \texttt{haven} \citep{citehaven}
\item
  \texttt{stringr} \citep{citestringr}
\item
  \texttt{tidyr} \citep{citetidyr}
\item
  \texttt{tidyverse} \citep{citetidyverse}
\end{itemize}

\textbf{Key functions}

\begin{itemize}
\tightlist
\item
  \texttt{\textbar{}} `or'
\item
  \texttt{\&} `and'
\item
  \texttt{\textbar{}\textgreater{}} `pipe'
\item
  \texttt{\$} `extract'
\item
  \texttt{as.character()}
\item
  \texttt{as.integer()}
\item
  \texttt{c()}
\item
  \texttt{citation()}
\item
  \texttt{class()}
\item
  \texttt{dplyr::arrange()}
\item
  \texttt{dplyr::case\_when()}
\item
  \texttt{dplyr::count()}
\item
  \texttt{dplyr::filter()}
\item
  \texttt{dplyr::group\_by()}
\item
  \texttt{dplyr::if\_else()}
\item
  \texttt{dplyr::left\_join()}
\item
  \texttt{dplyr::mutate()}
\item
  \texttt{dplyr::pull()}
\item
  \texttt{dplyr::rename()}
\item
  \texttt{dplyr::select()}
\item
  \texttt{dplyr::slice()}
\item
  \texttt{dplyr::summarise()}
\item
  \texttt{forcats::as\_factor()}
\item
  \texttt{forcats::fct\_relevel()}
\item
  \texttt{function()}
\item
  \texttt{ggplot2::facet\_wrap()}
\item
  \texttt{ggplot2::geom\_density()}
\item
  \texttt{ggplot2::geom\_histogram()}
\item
  \texttt{ggplot2::geom\_point()}
\item
  \texttt{ggplot2::ggplot()}
\item
  \texttt{head()}
\item
  \texttt{janitor::clean\_names()}
\item
  \texttt{library()}
\item
  \texttt{lubridate::ymd()}
\item
  \texttt{max()}
\item
  \texttt{mean()}
\item
  \texttt{print()}
\item
  \texttt{readr::read\_csv()}
\item
  \texttt{rnorm()}
\item
  \texttt{round()}
\item
  \texttt{runif()}
\item
  \texttt{sample()}
\item
  \texttt{set.seed()}
\item
  \texttt{stringr::str\_detect()}
\item
  \texttt{stringr::str\_replace()}
\item
  \texttt{stringr::str\_squish()}
\item
  \texttt{sum()}
\item
  \texttt{tibble::tibble()}
\item
  \texttt{tidyr::pivot\_longer()}
\item
  \texttt{tidyr::pivot\_wider()}
\end{itemize}

\hypertarget{background}{%
\section{Background}\label{background}}

In this chapter we focus on foundational skills needed to use the statistical programming language R \citep{citeR} to tell stories with data. Some of it may not make sense at first, but these are skills and approaches that we will often use. You should initially just go through this chapter quickly, noting aspects that you do not understand. And then come back to this chapter from time to time as you continue through the rest of the book. That way you will see how the various bits fit into context.

R is an open-source language for statistical programming. You can download R for free from the Comprehensive R Archive Network (CRAN): \url{https://cran.r-project.org}. R Studio is an Integrated Development Environment (IDE) for R which makes the language easier to use and can be downloaded for free: \url{https://www.rstudio.com/products/rstudio/}.

The past ten years or so, have been characterized by the increased use of the \texttt{tidyverse}. This is `\ldots an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures' \citep{tidyversewebsite}. There are three distinctions to be clear about: the original R language, typically referred to as `base'; the `tidyverse' which is a coherent collection of packages that build on top of base, and other packages.

Essentially everything that we can do in the tidyverse, we can also do in base. But, as the \texttt{tidyverse} was built especially for data science it is often easier to use the tidyverse, especially when learning. Additionally, often everything that we can do in the tidyverse, we can also do with other packages. But, as the \texttt{tidyverse} is a coherent collection of packages, it is often easier to use the tidyverse, again, especially when learning. Eventually there are cases where it makes sense to trade-off the convenience and coherence of the \texttt{tidyverse} for some features of base or other packages. Indeed, we will see that at various points later in this book. For instance, the \texttt{tidyverse} can be slow, and so if one needs to import thousands of CSVs then it can make sense to switch away from \texttt{read\_csv()}. The appropriate use of base and non-tidyverse packages, or even other languages, rather than dogmatic insistence on a particular solution, is a sign of intellectual maturity.

Central to our use of the statistical programming language R is data, and most of the data that we use will have humans at the heart of it. Sometimes, dealing with human-centered data in this way can have a numbing effect, results in over-generalization, and potentially problematic work. Another sign of intellectual maturity is when it has the opposite effect.

\begin{quote}
In practice, I find that far from distancing you from questions of meaning, quantitative data forces you to confront them. The numbers draw you in. Working with data like this is an unending exercise in humility, a constant compulsion to think through what you can and cannot see, and a standing invitation to understand what the measures really capture---what they mean, and for whom.

\citet{kieranskitchen}
\end{quote}

\hypertarget{broader-impacts}{%
\section{Broader impacts}\label{broader-impacts}}

\begin{quote}
``We shouldn't have to think about the societal impact of our work because it's hard and other people can do it for us'' is a really bad argument. I stopped doing CV {[}computer vision{]} research because I saw the impact my work was having. I loved the work but the military applications and privacy concerns eventually became impossible to ignore. But basically all facial recognition work would not get published if we took Broader Impacts sections seriously. There is almost no upside and enormous downside risk. To be fair though i should have a lot of humility here. For most of grad school I bought in to the myth that science is apolitical and research is objectively moral and good no matter what the subject is.

Joe Redmon, 20 February 2020
\end{quote}

Although the term `data science' is ubiquitous in academia, industry, and even more generally, it is difficult to define. One deliberately antagonistic definition of data science is `{[}t{]}he inhumane reduction of humanity down to what can be counted' \citep{keyes2019}. While purposefully controversial, this definition highlights one reason for the increased demand for data science and quantitative methods over the past decade---individuals and their behavior are now at the heart of it. Many of the techniques have been around for many decades, but what makes them popular now is this human focus.

Unfortunately, even though much of the work may be focused on individuals, issues of privacy and consent, and ethical concerns more broadly, rarely seem front of mind. While there are some exceptions, in general, even at the same time as claiming that AI, machine learning, and data science are going to revolutionize society, consideration of these types of issues appears to have been largely treated as something that would be nice to have, rather than something that we may like to think of before we embrace the revolution.

For the most part, these types of issues are not new. In the sciences, there has been considerable recent ethical consideration around CRISPR technology and gene editing \citep{brokowski2019crispr}, but in an earlier time similar conversations were had, for instance, about Wernher von Braun being allowed to building rockets for the US \citep{neufeld2002wernher}. In medicine, of course, these concerns have been front-of-mind for some time \citep{american1848code}. Data science seems determined to have its own Tuskegee-moment rather than think about, and proactively deal appropriately with, these issues, based on the experiences of other fields.

That said, there is some evidence that data scientists are beginning to be more concerned about the ethics surrounding the practice. For instance, NeurIPS, a prestigious machine learning conference, has required a statement on ethics to accompany all submissions since 2020.

\begin{quote}
In order to provide a balanced perspective, authors are required to include a statement of the potential broader impact of their work, including its ethical aspects and future societal consequences. Authors should take care to discuss both positive and negative outcomes.

NeurIPS 2020 Conference Call For Papers
\end{quote}

The purpose of ethical consideration and concern for the broader impact of data science is not to prescriptively rule things in or out, but to provide an opportunity to raise some issues that should be paramount. The variety of data science applications, the relative youth of the field, and the speed of change, mean that such considerations are sometimes knowingly set aside, and this is acceptable to the rest of the field. This contrasts with fields such as science, medicine, engineering, and accounting. Possibly those fields are more self-aware (Figure \ref{fig:personalprobability}).

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{figures/probability} 

}

\caption{Probability, from XKCD}\label{fig:personalprobability}
\end{figure}

\hypertarget{r-r-studio-and-r-studio-cloud}{%
\section{R, R Studio, and R Studio Cloud}\label{r-r-studio-and-r-studio-cloud}}

R and R Studio are complementary, but they are not the same thing. Liza Bolton, Assistant Professor, Teaching Stream, University of Toronto explains their relationship by analogy where R is like the engine and R Studio is like the car. Although some of us use a car engine directly, most of us use a car to interact with the engine.

\hypertarget{r}{%
\subsection{R}\label{r}}

R -- \url{https://www.r-project.org/} -- is an open-source and free programming language that is focused on general statistics. Free in this context does not refer to a price of zero, but instead to the freedom that the creators give users to largely do what they want with it, although it also does have a price of zero. This is in contrast with an open-source programming language that is designed for general purpose, such as Python, or an open-source programming language that is focused on probability, such as Stan. It was created by Ross Ihaka and Robert Gentleman at the University of Auckland in the 1990s, and traces its provenance to S, which was developed at Bell Labs in the 1970s. It is maintained by the R Core Team and changes to this `base' of code occur methodically and with concern given to a variety of different priorities.

Many people build on this stable base, to extend the capabilities of R to better and more quickly suit their needs. They do this by creating packages. Typically, although not always, a package is a collection of R code, mostly functions, and this allows us to more easily do things that we want to do. These packages are managed by repositories such as CRAN and Bioconductor.

If you want to use a package then you first need to install it on your computer, and then you need to load it when you want to use it. Di Cook, Professor of Business Analytics at Monash University, describes this as analogous to a lightbulb. If you want light in your house, first you need to fit a lightbulb, and then you need to turn the switch on. Installing a package, say, \texttt{install.packages("tidyverse")}, is akin to fitting a lightbulb into a socket---you only need to do this once for each lightbulb. But then each time you want light you need to turn on the switch to the lightbulb, which in the R packages case, means calling the library, say, \texttt{library(tidyverse)}.

\begin{quote}
\textbf{Shoulders of giants} Dr Di Cook is Professor of Business Analytics at Monash University. After taking a PhD in statistics from Rutgers University in 1993 where she focused on statistical graphics, she was appointed as an assistant professor at Iowa State University, being promoted to full professor in 2005, and in 2015 she moved to Monash. One area of her research is data visualisation, especially interactive and dynamic graphics. One particularly important paper is \citet{buja1996interactive} which proposes a taxonomy of interactive data visualization and associated software XGobi.
\end{quote}

To install a package on your computer (again, we will need to do this only once per computer) we use \texttt{install.packages()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

And then when we want to use the package, we use \texttt{library()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

Having downloaded it, we can open R and use it directly. It is primarily designed to be interacted with through the command line. While this is functional, it can be useful to have a richer environment than the command line provides. In particular, it can be useful to install an Integrated Development Environment (IDE), which is an application that brings together various bits and pieces that will be used often. One common IDE for R is R Studio, although others such as Visual Studio are also used.

\hypertarget{r-studio}{%
\subsection{R Studio}\label{r-studio}}

R Studio is distinct to R, and they are different entities. R Studio builds on top of R to make it easier to use R. This is in the same way that one could use the internet from the command line, but most folks use a browser such as Chrome, Firefox, or Safari.

R Studio is free in the sense that we do not pay for it. It is also free in the sense of being able to take the code, modify it, and distribute that code. But it is important to recognize that R Studio is a company and so it is possible that the current situation could change. It can be downloaded: \url{https://www.rstudio.com/products/rstudio/}.

When we open R Studio it will look like Figure \ref{fig:first}.

\begin{figure}
\includegraphics[width=1\linewidth]{/Users/rohanalexander/Documents/book/figures/01-03_r_essentials/01} \caption{Opening R Studio for the first time}\label{fig:first}
\end{figure}

The left pane is a console in which you can type and execute R code line by line. Try it with 2+2 by clicking next to the prompt `\textgreater{}', typing `2+2', and then pressing `return/enter'.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2} \SpecialCharTok{+} \DecValTok{2}
\CommentTok{\#\textgreater{} [1] 4}
\end{Highlighting}
\end{Shaded}

The pane on the top right has information about your environment. For instance, when we create variables a list of their names and some properties will appear there. Try to type the following code, replacing my name with your name, next to the prompt, and again press enter:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_name }\OtherTok{\textless{}{-}} \StringTok{"Rohan"}
\end{Highlighting}
\end{Shaded}

You should notice a new value in the environment pane with the variable name and its value.

The pane in the bottom right is a file manager. At the moment it should just have two files: an R History file and a R Project file. We will get to what these are later, but for now we will create and save a file.

Run the following code, without worrying too much about the details for now. And you should see a new `.rds' file in your list of files.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{saveRDS}\NormalTok{(}\AttributeTok{object =}\NormalTok{ my\_name, }\AttributeTok{file =} \StringTok{"my\_first\_file.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{r-studio-cloud}{%
\subsection{R Studio Cloud}\label{r-studio-cloud}}

While you can and should download R Studio to your own computer, initially we will use R Studio Cloud: \url{https://rstudio.cloud/}. This is an online version that is provided by R Studio. We will use this so that you can focus on getting comfortable with R and R Studio in an environment that is consistent. This way you do not have to worry about what computer you have or installation permissions, amongst other things.

The free version of R Studio Cloud is free as is `no financial cost'. The trade-off is that it is not very powerful, and it is sometimes slow, but for the purposes of getting started it is enough.

\hypertarget{getting-started}{%
\section{Getting started}\label{getting-started}}

We will now start going through some code. It is important to actively write this all out yourself.

While working line-by-line in the console is fine, it is easier to write out a whole script that can then be run. We will do this by making an R Script (`File' -\textgreater{} `New File' -\textgreater{} `R Script'). The console pane will fall to the bottom left and an R Script will open in the top left. We will write some code that will get all of the Australian federal politicians and then construct a small table about the genders of the prime ministers. Some of this code will not make sense at this stage, but just type it all out to get in the habit and then run it. To run the whole script, we can click `Run' or we can highlight certain lines and then click `Run' to just run those lines.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Install the packages that we need}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"AustralianPoliticians"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the packages that we need to use this time}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(AustralianPoliticians)}

\CommentTok{\# Make a table of the counts of genders of the prime ministers}
\NormalTok{AustralianPoliticians}\SpecialCharTok{::}\FunctionTok{get\_auspol}\NormalTok{(}\StringTok{\textquotesingle{}all\textquotesingle{}}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(wasPrimeMinister }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{count}\NormalTok{(gender)}
\CommentTok{\#\textgreater{} \# A tibble: 2 x 2}
\CommentTok{\#\textgreater{}   gender     n}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}  \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 female     1}
\CommentTok{\#\textgreater{} 2 male      29}
\end{Highlighting}
\end{Shaded}

We can see that, as at the end of 2021, one female has been prime minister (Julia Gillard), while the other 29 prime ministers were male

One critical operator when programming is the `pipe': \texttt{\textbar{}\textgreater{}}. We read this as `and then'. This takes the output of a line of code and uses it as the first input to the next line of code. It makes code easier to read.

The idea of the pipe is that we take a dataset, and then do something to it. We used this in the earlier example. Another example follows where we will look at the first six lines of a dataset by piping it to \texttt{head()}. Notice that \texttt{head()} does not explicitly take any arguments in this example. It knows which data to display because the pipe does it implicitly.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AustralianPoliticians}\SpecialCharTok{::}\FunctionTok{get\_auspol}\NormalTok{(}\StringTok{\textquotesingle{}all\textquotesingle{}}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 20}
\CommentTok{\#\textgreater{}   uniqueID   surname allOtherNames      firstName commonName}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}      \textless{}chr\textgreater{}   \textless{}chr\textgreater{}              \textless{}chr\textgreater{}     \textless{}chr\textgreater{}     }
\CommentTok{\#\textgreater{} 1 Abbott1859 Abbott  Richard Hartley S\textasciitilde{} Richard   \textless{}NA\textgreater{}      }
\CommentTok{\#\textgreater{} 2 Abbott1869 Abbott  Percy Phipps       Percy     \textless{}NA\textgreater{}      }
\CommentTok{\#\textgreater{} 3 Abbott1877 Abbott  Macartney          Macartney Mac       }
\CommentTok{\#\textgreater{} 4 Abbott1886 Abbott  Charles Lydiard A\textasciitilde{} Charles   Aubrey    }
\CommentTok{\#\textgreater{} 5 Abbott1891 Abbott  Joseph Palmer      Joseph    \textless{}NA\textgreater{}      }
\CommentTok{\#\textgreater{} 6 Abbott1957 Abbott  Anthony John       Anthony   Tony      }
\CommentTok{\#\textgreater{} \# ... with 15 more variables: displayName \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   earlierOrLaterNames \textless{}chr\textgreater{}, title \textless{}chr\textgreater{}, gender \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   birthDate \textless{}date\textgreater{}, birthYear \textless{}dbl\textgreater{}, birthPlace \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   deathDate \textless{}date\textgreater{}, member \textless{}dbl\textgreater{}, senator \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   wasPrimeMinister \textless{}dbl\textgreater{}, wikidataID \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   wikipedia \textless{}chr\textgreater{}, adb \textless{}chr\textgreater{}, comments \textless{}chr\textgreater{}}
\end{Highlighting}
\end{Shaded}

We can save this R Script as `my\_first\_r\_script.R' (`File' -\textgreater{} `Save As'). At this point, our workspace should look something like Figure \ref{fig:third}.

\begin{figure}
\includegraphics[width=1\linewidth]{/Users/rohanalexander/Documents/book/figures/01-03_r_essentials/03} \caption{After running an R Script}\label{fig:third}
\end{figure}

One thing to be aware of is that each R Studio Cloud workspace is essentially a new computer. Because of this, we need to install any package that we want to use for each workspace. For instance, before we can use the \texttt{tidyverse}, we need to install it with \texttt{install.packages("tidyverse")}. This contrasts with using one's own computer.

A few final notes on R Studio Cloud:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In the Australian politician's example, we got our data from the website GitHub using an R package, but we can get data into a workspace from a local computer in a variety of ways. One way is to use the `upload' button in the `Files' panel.
\item
  R Studio Cloud allows some degree of collaboration. For instance, you can give someone else access to a workspace that you create. This could be useful for collaborating on an assignment, although it is not quite full featured yet and you cannot both be in the workspace at the same time, in contrast to, say, Google Docs.
\item
  There are a variety of weaknesses of R Studio Cloud, in particular the RAM limits. Additionally, like any web application, things break from time to time or go down.
\end{enumerate}

\hypertarget{the-dplyr-verbs}{%
\section{\texorpdfstring{The \texttt{dplyr} verbs}{The dplyr verbs}}\label{the-dplyr-verbs}}

One of the key packages that we will use is the \texttt{tidyverse} \citep{tidyverse}. The \texttt{tidyverse} is actually a package of packages, which means when we install the \texttt{tidyverse}, we actually install a whole bunch of different packages. The key package in the \texttt{tidyverse} in terms of manipulating data is \texttt{dplyr} \citep{citedplyr}.

There are five \texttt{dplyr} functions that are regularly used, and we will now go through each of these. These are commonly referred to as the \texttt{dplyr} verbs.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{select()}
\item
  \texttt{filter()}
\item
  \texttt{arrange()}
\item
  \texttt{mutate()}
\item
  \texttt{summarise()} or equally \texttt{summarize()}
\end{enumerate}

We will also cover \texttt{group\_by()}, and \texttt{count()} here as they are closely related.

As we have already installed the \texttt{tidyverse}, we just need to load it.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

And we will begin by again using some data about Australian politicians from the \texttt{AustralianPoliticians} package \citep{citeaustralianpoliticians}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(AustralianPoliticians)}

\NormalTok{australian\_politicians }\OtherTok{\textless{}{-}} 
  \FunctionTok{get\_auspol}\NormalTok{(}\StringTok{\textquotesingle{}all\textquotesingle{}}\NormalTok{)}

\FunctionTok{head}\NormalTok{(australian\_politicians)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 20}
\CommentTok{\#\textgreater{}   uniqueID   surname allOtherNames      firstName commonName}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}      \textless{}chr\textgreater{}   \textless{}chr\textgreater{}              \textless{}chr\textgreater{}     \textless{}chr\textgreater{}     }
\CommentTok{\#\textgreater{} 1 Abbott1859 Abbott  Richard Hartley S\textasciitilde{} Richard   \textless{}NA\textgreater{}      }
\CommentTok{\#\textgreater{} 2 Abbott1869 Abbott  Percy Phipps       Percy     \textless{}NA\textgreater{}      }
\CommentTok{\#\textgreater{} 3 Abbott1877 Abbott  Macartney          Macartney Mac       }
\CommentTok{\#\textgreater{} 4 Abbott1886 Abbott  Charles Lydiard A\textasciitilde{} Charles   Aubrey    }
\CommentTok{\#\textgreater{} 5 Abbott1891 Abbott  Joseph Palmer      Joseph    \textless{}NA\textgreater{}      }
\CommentTok{\#\textgreater{} 6 Abbott1957 Abbott  Anthony John       Anthony   Tony      }
\CommentTok{\#\textgreater{} \# ... with 15 more variables: displayName \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   earlierOrLaterNames \textless{}chr\textgreater{}, title \textless{}chr\textgreater{}, gender \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   birthDate \textless{}date\textgreater{}, birthYear \textless{}dbl\textgreater{}, birthPlace \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   deathDate \textless{}date\textgreater{}, member \textless{}dbl\textgreater{}, senator \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   wasPrimeMinister \textless{}dbl\textgreater{}, wikidataID \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   wikipedia \textless{}chr\textgreater{}, adb \textless{}chr\textgreater{}, comments \textless{}chr\textgreater{}}
\end{Highlighting}
\end{Shaded}

\hypertarget{select}{%
\subsection{\texorpdfstring{\texttt{select()}}{select()}}\label{select}}

We use \texttt{select()} to pick particular columns of a dataset. For instance, we might like to select the `firstName' column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{select}\NormalTok{(firstName) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 1}
\CommentTok{\#\textgreater{}   firstName}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}    }
\CommentTok{\#\textgreater{} 1 Richard  }
\CommentTok{\#\textgreater{} 2 Percy    }
\CommentTok{\#\textgreater{} 3 Macartney}
\CommentTok{\#\textgreater{} 4 Charles  }
\CommentTok{\#\textgreater{} 5 Joseph   }
\CommentTok{\#\textgreater{} 6 Anthony}
\end{Highlighting}
\end{Shaded}

In R, there are many ways to do things. Sometimes these are different ways to do the same thing, and other times they are different ways to do almost the same thing. For instance, another way to pick a particular column of a dataset is to use the `extract' operator `\$'. This is from base, as opposed to \texttt{select()} which is from the \texttt{tidyverse}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians}\SpecialCharTok{$}\NormalTok{firstName }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] "Richard"   "Percy"     "Macartney" "Charles"  }
\CommentTok{\#\textgreater{} [5] "Joseph"    "Anthony"}
\end{Highlighting}
\end{Shaded}

The two appear similar---both pick the `firstName' column---but they differ in the class of what they return. For the sake of completeness, if we combine \texttt{select()} with \texttt{pull()} then we get the same class of output as if we had used the extract operator.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{select}\NormalTok{(firstName) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{pull}\NormalTok{() }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] "Richard"   "Percy"     "Macartney" "Charles"  }
\CommentTok{\#\textgreater{} [5] "Joseph"    "Anthony"}
\end{Highlighting}
\end{Shaded}

We can also use \texttt{select()} to remove columns, by negating the column name.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{firstName) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 19}
\CommentTok{\#\textgreater{}   uniqueID   surname allOtherNames    commonName displayName}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}      \textless{}chr\textgreater{}   \textless{}chr\textgreater{}            \textless{}chr\textgreater{}      \textless{}chr\textgreater{}      }
\CommentTok{\#\textgreater{} 1 Abbott1859 Abbott  Richard Hartley\textasciitilde{} \textless{}NA\textgreater{}       Abbott, Ri\textasciitilde{}}
\CommentTok{\#\textgreater{} 2 Abbott1869 Abbott  Percy Phipps     \textless{}NA\textgreater{}       Abbott, Pe\textasciitilde{}}
\CommentTok{\#\textgreater{} 3 Abbott1877 Abbott  Macartney        Mac        Abbott, Mac}
\CommentTok{\#\textgreater{} 4 Abbott1886 Abbott  Charles Lydiard\textasciitilde{} Aubrey     Abbott, Au\textasciitilde{}}
\CommentTok{\#\textgreater{} 5 Abbott1891 Abbott  Joseph Palmer    \textless{}NA\textgreater{}       Abbott, Jo\textasciitilde{}}
\CommentTok{\#\textgreater{} 6 Abbott1957 Abbott  Anthony John     Tony       Abbott, To\textasciitilde{}}
\CommentTok{\#\textgreater{} \# ... with 14 more variables: earlierOrLaterNames \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   title \textless{}chr\textgreater{}, gender \textless{}chr\textgreater{}, birthDate \textless{}date\textgreater{},}
\CommentTok{\#\textgreater{} \#   birthYear \textless{}dbl\textgreater{}, birthPlace \textless{}chr\textgreater{}, deathDate \textless{}date\textgreater{},}
\CommentTok{\#\textgreater{} \#   member \textless{}dbl\textgreater{}, senator \textless{}dbl\textgreater{}, wasPrimeMinister \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   wikidataID \textless{}chr\textgreater{}, wikipedia \textless{}chr\textgreater{}, adb \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   comments \textless{}chr\textgreater{}}
\end{Highlighting}
\end{Shaded}

Finally, we can \texttt{select()} based on conditions. For instance, we can \texttt{select()} all of the columns that start with, say, `birth'.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"birth"}\NormalTok{)) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{}   birthDate  birthYear birthPlace  }
\CommentTok{\#\textgreater{}   \textless{}date\textgreater{}         \textless{}dbl\textgreater{} \textless{}chr\textgreater{}       }
\CommentTok{\#\textgreater{} 1 NA              1859 Bendigo     }
\CommentTok{\#\textgreater{} 2 1869{-}05{-}14        NA Hobart      }
\CommentTok{\#\textgreater{} 3 1877{-}07{-}03        NA Murrurundi  }
\CommentTok{\#\textgreater{} 4 1886{-}01{-}04        NA St Leonards }
\CommentTok{\#\textgreater{} 5 1891{-}10{-}18        NA North Sydney}
\CommentTok{\#\textgreater{} 6 1957{-}11{-}04        NA London}
\end{Highlighting}
\end{Shaded}

There are a variety of similar `selection helpers' including \texttt{starts\_with()}, \texttt{ends\_with()}, and \texttt{contains()}. More information about these is available in the help page for \texttt{select()} which can be accessed by running \texttt{?select()}.

At this point, we will use \texttt{select()} to reduce the width of our dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\OtherTok{\textless{}{-}}
\NormalTok{  australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{select}\NormalTok{(uniqueID,}
\NormalTok{         surname,}
\NormalTok{         firstName,}
\NormalTok{         gender,}
\NormalTok{         birthDate,}
\NormalTok{         birthYear,}
\NormalTok{         deathDate,}
\NormalTok{         member,}
\NormalTok{         senator,}
\NormalTok{         wasPrimeMinister)}

\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 10}
\CommentTok{\#\textgreater{}   uniqueID   surname firstName gender birthDate  birthYear}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}      \textless{}chr\textgreater{}   \textless{}chr\textgreater{}     \textless{}chr\textgreater{}  \textless{}date\textgreater{}         \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Abbott1859 Abbott  Richard   male   NA              1859}
\CommentTok{\#\textgreater{} 2 Abbott1869 Abbott  Percy     male   1869{-}05{-}14        NA}
\CommentTok{\#\textgreater{} 3 Abbott1877 Abbott  Macartney male   1877{-}07{-}03        NA}
\CommentTok{\#\textgreater{} 4 Abbott1886 Abbott  Charles   male   1886{-}01{-}04        NA}
\CommentTok{\#\textgreater{} 5 Abbott1891 Abbott  Joseph    male   1891{-}10{-}18        NA}
\CommentTok{\#\textgreater{} 6 Abbott1957 Abbott  Anthony   male   1957{-}11{-}04        NA}
\CommentTok{\#\textgreater{} \# ... with 4 more variables: deathDate \textless{}date\textgreater{},}
\CommentTok{\#\textgreater{} \#   member \textless{}dbl\textgreater{}, senator \textless{}dbl\textgreater{}, wasPrimeMinister \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

\hypertarget{filter}{%
\subsection{\texorpdfstring{\texttt{filter()}}{filter()}}\label{filter}}

We use \texttt{filter()} to pick particular rows of a dataset. For instance, we might be only interested in politicians that became prime minister.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(wasPrimeMinister }\SpecialCharTok{==} \DecValTok{1}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 30 x 10}
\CommentTok{\#\textgreater{}    uniqueID    surname firstName gender birthDate  birthYear}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}       \textless{}chr\textgreater{}   \textless{}chr\textgreater{}     \textless{}chr\textgreater{}  \textless{}date\textgreater{}         \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1 Abbott1957  Abbott  Anthony   male   1957{-}11{-}04        NA}
\CommentTok{\#\textgreater{}  2 Barton1849  Barton  Edmund    male   1849{-}01{-}18        NA}
\CommentTok{\#\textgreater{}  3 Bruce1883   Bruce   Stanley   male   1883{-}04{-}15        NA}
\CommentTok{\#\textgreater{}  4 Chifley1885 Chifley Joseph    male   1885{-}09{-}22        NA}
\CommentTok{\#\textgreater{}  5 Cook1860    Cook    Joseph    male   1860{-}12{-}07        NA}
\CommentTok{\#\textgreater{}  6 Curtin1885  Curtin  John      male   1885{-}01{-}08        NA}
\CommentTok{\#\textgreater{}  7 Deakin1856  Deakin  Alfred    male   1856{-}08{-}03        NA}
\CommentTok{\#\textgreater{}  8 Fadden1894  Fadden  Arthur    male   1894{-}04{-}13        NA}
\CommentTok{\#\textgreater{}  9 Fisher1862  Fisher  Andrew    male   1862{-}08{-}29        NA}
\CommentTok{\#\textgreater{} 10 Forde1890   Forde   Francis   male   1890{-}07{-}18        NA}
\CommentTok{\#\textgreater{} \# ... with 20 more rows, and 4 more variables:}
\CommentTok{\#\textgreater{} \#   deathDate \textless{}date\textgreater{}, member \textless{}dbl\textgreater{}, senator \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   wasPrimeMinister \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

We could also give \texttt{filter()} two conditions. For instance, we could look at politicians that become prime minister and were named Joseph, using the `and' operator `\&'.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(wasPrimeMinister }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ firstName }\SpecialCharTok{==} \StringTok{"Joseph"}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 10}
\CommentTok{\#\textgreater{}   uniqueID    surname firstName gender birthDate  birthYear}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}       \textless{}chr\textgreater{}   \textless{}chr\textgreater{}     \textless{}chr\textgreater{}  \textless{}date\textgreater{}         \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Chifley1885 Chifley Joseph    male   1885{-}09{-}22        NA}
\CommentTok{\#\textgreater{} 2 Cook1860    Cook    Joseph    male   1860{-}12{-}07        NA}
\CommentTok{\#\textgreater{} 3 Lyons1879   Lyons   Joseph    male   1879{-}09{-}15        NA}
\CommentTok{\#\textgreater{} \# ... with 4 more variables: deathDate \textless{}date\textgreater{},}
\CommentTok{\#\textgreater{} \#   member \textless{}dbl\textgreater{}, senator \textless{}dbl\textgreater{}, wasPrimeMinister \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

We get the same result if we use a comma instead of an ampersand.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(wasPrimeMinister }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, firstName }\SpecialCharTok{==} \StringTok{"Joseph"}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 10}
\CommentTok{\#\textgreater{}   uniqueID    surname firstName gender birthDate  birthYear}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}       \textless{}chr\textgreater{}   \textless{}chr\textgreater{}     \textless{}chr\textgreater{}  \textless{}date\textgreater{}         \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Chifley1885 Chifley Joseph    male   1885{-}09{-}22        NA}
\CommentTok{\#\textgreater{} 2 Cook1860    Cook    Joseph    male   1860{-}12{-}07        NA}
\CommentTok{\#\textgreater{} 3 Lyons1879   Lyons   Joseph    male   1879{-}09{-}15        NA}
\CommentTok{\#\textgreater{} \# ... with 4 more variables: deathDate \textless{}date\textgreater{},}
\CommentTok{\#\textgreater{} \#   member \textless{}dbl\textgreater{}, senator \textless{}dbl\textgreater{}, wasPrimeMinister \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

Similarly, we could look at politicians who were named, say, Myles or Ruth using the `or' operator `\textbar{}'

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(firstName }\SpecialCharTok{==} \StringTok{"Myles"} \SpecialCharTok{|}\NormalTok{ firstName }\SpecialCharTok{==} \StringTok{"Ruth"}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 10}
\CommentTok{\#\textgreater{}   uniqueID     surname firstName gender birthDate  birthYear}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}        \textless{}chr\textgreater{}   \textless{}chr\textgreater{}     \textless{}chr\textgreater{}  \textless{}date\textgreater{}         \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Coleman1931  Coleman Ruth      female 1931{-}09{-}27        NA}
\CommentTok{\#\textgreater{} 2 Ferricks1875 Ferric\textasciitilde{} Myles     male   1875{-}11{-}12        NA}
\CommentTok{\#\textgreater{} 3 Webber1965   Webber  Ruth      female 1965{-}03{-}24        NA}
\CommentTok{\#\textgreater{} \# ... with 4 more variables: deathDate \textless{}date\textgreater{},}
\CommentTok{\#\textgreater{} \#   member \textless{}dbl\textgreater{}, senator \textless{}dbl\textgreater{}, wasPrimeMinister \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

We could also pipe the result. For instance we could pipe from \texttt{filter()} to \texttt{select()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(firstName }\SpecialCharTok{==} \StringTok{"Ruth"} \SpecialCharTok{|}\NormalTok{ firstName }\SpecialCharTok{==} \StringTok{"Myles"}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{select}\NormalTok{(firstName, surname)}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 2}
\CommentTok{\#\textgreater{}   firstName surname }
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}     \textless{}chr\textgreater{}   }
\CommentTok{\#\textgreater{} 1 Ruth      Coleman }
\CommentTok{\#\textgreater{} 2 Myles     Ferricks}
\CommentTok{\#\textgreater{} 3 Ruth      Webber}
\end{Highlighting}
\end{Shaded}

If we happen to know the particular row number that is of interest then we could \texttt{filter()} to only that particular row. For instance, say the row 853 was of interest.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{row\_number}\NormalTok{() }\SpecialCharTok{==} \DecValTok{853}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 10}
\CommentTok{\#\textgreater{}   uniqueID     surname firstName gender birthDate  birthYear}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}        \textless{}chr\textgreater{}   \textless{}chr\textgreater{}     \textless{}chr\textgreater{}  \textless{}date\textgreater{}         \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Jakobsen1947 Jakobs\textasciitilde{} Carolyn   female 1947{-}09{-}11        NA}
\CommentTok{\#\textgreater{} \# ... with 4 more variables: deathDate \textless{}date\textgreater{},}
\CommentTok{\#\textgreater{} \#   member \textless{}dbl\textgreater{}, senator \textless{}dbl\textgreater{}, wasPrimeMinister \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

There is also a dedicated function to do this, which is \texttt{slice()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{853}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 10}
\CommentTok{\#\textgreater{}   uniqueID     surname firstName gender birthDate  birthYear}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}        \textless{}chr\textgreater{}   \textless{}chr\textgreater{}     \textless{}chr\textgreater{}  \textless{}date\textgreater{}         \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Jakobsen1947 Jakobs\textasciitilde{} Carolyn   female 1947{-}09{-}11        NA}
\CommentTok{\#\textgreater{} \# ... with 4 more variables: deathDate \textless{}date\textgreater{},}
\CommentTok{\#\textgreater{} \#   member \textless{}dbl\textgreater{}, senator \textless{}dbl\textgreater{}, wasPrimeMinister \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

While this may seem somewhat esoteric, it is especially useful if we would like to remove a particular row using negation, or duplicate specific rows. For instance, we could remove the first row.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 1,782 x 10}
\CommentTok{\#\textgreater{}    uniqueID    surname firstName gender birthDate  birthYear}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}       \textless{}chr\textgreater{}   \textless{}chr\textgreater{}     \textless{}chr\textgreater{}  \textless{}date\textgreater{}         \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1 Abbott1869  Abbott  Percy     male   1869{-}05{-}14        NA}
\CommentTok{\#\textgreater{}  2 Abbott1877  Abbott  Macartney male   1877{-}07{-}03        NA}
\CommentTok{\#\textgreater{}  3 Abbott1886  Abbott  Charles   male   1886{-}01{-}04        NA}
\CommentTok{\#\textgreater{}  4 Abbott1891  Abbott  Joseph    male   1891{-}10{-}18        NA}
\CommentTok{\#\textgreater{}  5 Abbott1957  Abbott  Anthony   male   1957{-}11{-}04        NA}
\CommentTok{\#\textgreater{}  6 Abel1939    Abel    John      male   1939{-}06{-}25        NA}
\CommentTok{\#\textgreater{}  7 Abetz1958   Abetz   Eric      male   1958{-}01{-}25        NA}
\CommentTok{\#\textgreater{}  8 Adams1943   Adams   Judith    female 1943{-}04{-}11        NA}
\CommentTok{\#\textgreater{}  9 Adams1951   Adams   Dick      male   1951{-}04{-}29        NA}
\CommentTok{\#\textgreater{} 10 Adamson1857 Adamson John      male   1857{-}02{-}18        NA}
\CommentTok{\#\textgreater{} \# ... with 1,772 more rows, and 4 more variables:}
\CommentTok{\#\textgreater{} \#   deathDate \textless{}date\textgreater{}, member \textless{}dbl\textgreater{}, senator \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   wasPrimeMinister \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

We could also only, say, only keep the first three rows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 10}
\CommentTok{\#\textgreater{}   uniqueID   surname firstName gender birthDate  birthYear}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}      \textless{}chr\textgreater{}   \textless{}chr\textgreater{}     \textless{}chr\textgreater{}  \textless{}date\textgreater{}         \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Abbott1859 Abbott  Richard   male   NA              1859}
\CommentTok{\#\textgreater{} 2 Abbott1869 Abbott  Percy     male   1869{-}05{-}14        NA}
\CommentTok{\#\textgreater{} 3 Abbott1877 Abbott  Macartney male   1877{-}07{-}03        NA}
\CommentTok{\#\textgreater{} \# ... with 4 more variables: deathDate \textless{}date\textgreater{},}
\CommentTok{\#\textgreater{} \#   member \textless{}dbl\textgreater{}, senator \textless{}dbl\textgreater{}, wasPrimeMinister \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

Finally, we could duplicate the first two rows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\FunctionTok{n}\NormalTok{())}
\CommentTok{\#\textgreater{} \# A tibble: 1,785 x 10}
\CommentTok{\#\textgreater{}    uniqueID   surname firstName gender birthDate  birthYear}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}      \textless{}chr\textgreater{}   \textless{}chr\textgreater{}     \textless{}chr\textgreater{}  \textless{}date\textgreater{}         \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1 Abbott1859 Abbott  Richard   male   NA              1859}
\CommentTok{\#\textgreater{}  2 Abbott1869 Abbott  Percy     male   1869{-}05{-}14        NA}
\CommentTok{\#\textgreater{}  3 Abbott1859 Abbott  Richard   male   NA              1859}
\CommentTok{\#\textgreater{}  4 Abbott1869 Abbott  Percy     male   1869{-}05{-}14        NA}
\CommentTok{\#\textgreater{}  5 Abbott1877 Abbott  Macartney male   1877{-}07{-}03        NA}
\CommentTok{\#\textgreater{}  6 Abbott1886 Abbott  Charles   male   1886{-}01{-}04        NA}
\CommentTok{\#\textgreater{}  7 Abbott1891 Abbott  Joseph    male   1891{-}10{-}18        NA}
\CommentTok{\#\textgreater{}  8 Abbott1957 Abbott  Anthony   male   1957{-}11{-}04        NA}
\CommentTok{\#\textgreater{}  9 Abel1939   Abel    John      male   1939{-}06{-}25        NA}
\CommentTok{\#\textgreater{} 10 Abetz1958  Abetz   Eric      male   1958{-}01{-}25        NA}
\CommentTok{\#\textgreater{} \# ... with 1,775 more rows, and 4 more variables:}
\CommentTok{\#\textgreater{} \#   deathDate \textless{}date\textgreater{}, member \textless{}dbl\textgreater{}, senator \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   wasPrimeMinister \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

\hypertarget{arrange}{%
\subsection{\texorpdfstring{\texttt{arrange()}}{arrange()}}\label{arrange}}

We use \texttt{arrange()} to change the order of the dataset based on the values of particular columns. For instance, we could arrange the politicians by their birthday.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(birthDate)}
\CommentTok{\#\textgreater{} \# A tibble: 1,783 x 10}
\CommentTok{\#\textgreater{}    uniqueID    surname firstName gender birthDate  birthYear}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}       \textless{}chr\textgreater{}   \textless{}chr\textgreater{}     \textless{}chr\textgreater{}  \textless{}date\textgreater{}         \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1 Braddon1829 Braddon Edward    male   1829{-}06{-}11        NA}
\CommentTok{\#\textgreater{}  2 Ferguson18\textasciitilde{} Fergus\textasciitilde{} John      male   1830{-}03{-}15        NA}
\CommentTok{\#\textgreater{}  3 Zeal1830    Zeal    William   male   1830{-}12{-}05        NA}
\CommentTok{\#\textgreater{}  4 Fraser1832  Fraser  Simon     male   1832{-}08{-}21        NA}
\CommentTok{\#\textgreater{}  5 Groom1833   Groom   William   male   1833{-}03{-}09        NA}
\CommentTok{\#\textgreater{}  6 Sargood1834 Sargood Frederick male   1834{-}05{-}30        NA}
\CommentTok{\#\textgreater{}  7 Fysh1835    Fysh    Philip    male   1835{-}03{-}01        NA}
\CommentTok{\#\textgreater{}  8 Playford18\textasciitilde{} Playfo\textasciitilde{} Thomas    male   1837{-}11{-}26        NA}
\CommentTok{\#\textgreater{}  9 Solomon1839 Solomon Elias     male   1839{-}09{-}02        NA}
\CommentTok{\#\textgreater{} 10 McLean1840  McLean  Allan     male   1840{-}02{-}03        NA}
\CommentTok{\#\textgreater{} \# ... with 1,773 more rows, and 4 more variables:}
\CommentTok{\#\textgreater{} \#   deathDate \textless{}date\textgreater{}, member \textless{}dbl\textgreater{}, senator \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   wasPrimeMinister \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

We could modify \texttt{arrange()} with \texttt{desc()} to change from ascending to descending order.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(birthDate))}
\CommentTok{\#\textgreater{} \# A tibble: 1,783 x 10}
\CommentTok{\#\textgreater{}    uniqueID   surname  firstName gender birthDate  birthYear}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}      \textless{}chr\textgreater{}    \textless{}chr\textgreater{}     \textless{}chr\textgreater{}  \textless{}date\textgreater{}         \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1 SteeleJoh\textasciitilde{} Steele{-}\textasciitilde{} Jordon    male   1994{-}10{-}14        NA}
\CommentTok{\#\textgreater{}  2 Chandler1\textasciitilde{} Chandler Claire    female 1990{-}06{-}01        NA}
\CommentTok{\#\textgreater{}  3 Roy1990    Roy      Wyatt     male   1990{-}05{-}22        NA}
\CommentTok{\#\textgreater{}  4 Thompson1\textasciitilde{} Thompson Phillip   male   1988{-}05{-}07        NA}
\CommentTok{\#\textgreater{}  5 Paterson1\textasciitilde{} Paterson James     male   1987{-}11{-}21        NA}
\CommentTok{\#\textgreater{}  6 Burns1987  Burns    Joshua    male   1987{-}02{-}06        NA}
\CommentTok{\#\textgreater{}  7 Smith1986  Smith    Marielle  female 1986{-}12{-}30        NA}
\CommentTok{\#\textgreater{}  8 Kakoschke\textasciitilde{} Kakosch\textasciitilde{} Skye      female 1985{-}12{-}19        NA}
\CommentTok{\#\textgreater{}  9 Simmonds1\textasciitilde{} Simmonds Julian    male   1985{-}08{-}29        NA}
\CommentTok{\#\textgreater{} 10 Gorman1984 Gorman   Patrick   male   1984{-}12{-}12        NA}
\CommentTok{\#\textgreater{} \# ... with 1,773 more rows, and 4 more variables:}
\CommentTok{\#\textgreater{} \#   deathDate \textless{}date\textgreater{}, member \textless{}dbl\textgreater{}, senator \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   wasPrimeMinister \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

And we could arrange based on more than one column. For instance, if two politicians have the same first name, then we could arrange based on their birthday.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(firstName, birthDate)}
\CommentTok{\#\textgreater{} \# A tibble: 1,783 x 10}
\CommentTok{\#\textgreater{}    uniqueID    surname firstName gender birthDate  birthYear}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}       \textless{}chr\textgreater{}   \textless{}chr\textgreater{}     \textless{}chr\textgreater{}  \textless{}date\textgreater{}         \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1 Blain1894   Blain   Adair     male   1894{-}11{-}21        NA}
\CommentTok{\#\textgreater{}  2 Dein1889    Dein    Adam      male   1889{-}03{-}04        NA}
\CommentTok{\#\textgreater{}  3 Armstrong1\textasciitilde{} Armstr\textasciitilde{} Adam      male   1909{-}07{-}01        NA}
\CommentTok{\#\textgreater{}  4 Bandt1972   Bandt   Adam      male   1972{-}03{-}11        NA}
\CommentTok{\#\textgreater{}  5 Ridgeway19\textasciitilde{} Ridgew\textasciitilde{} Aden      male   1962{-}09{-}18        NA}
\CommentTok{\#\textgreater{}  6 Bennett1933 Bennett Adrian    male   1933{-}01{-}21        NA}
\CommentTok{\#\textgreater{}  7 Gibson1935  Gibson  Adrian    male   1935{-}11{-}03        NA}
\CommentTok{\#\textgreater{}  8 Wynne1850   Wynne   Agar      male   1850{-}01{-}15        NA}
\CommentTok{\#\textgreater{}  9 Robertson1\textasciitilde{} Robert\textasciitilde{} Agnes     female 1882{-}07{-}31        NA}
\CommentTok{\#\textgreater{} 10 Pittard1902 Pittard Alan      male   1902{-}11{-}15        NA}
\CommentTok{\#\textgreater{} \# ... with 1,773 more rows, and 4 more variables:}
\CommentTok{\#\textgreater{} \#   deathDate \textless{}date\textgreater{}, member \textless{}dbl\textgreater{}, senator \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   wasPrimeMinister \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

We could achieve the same result by piping between two instances of \texttt{arrange()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(birthDate) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(firstName)}
\CommentTok{\#\textgreater{} \# A tibble: 1,783 x 10}
\CommentTok{\#\textgreater{}    uniqueID    surname firstName gender birthDate  birthYear}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}       \textless{}chr\textgreater{}   \textless{}chr\textgreater{}     \textless{}chr\textgreater{}  \textless{}date\textgreater{}         \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1 Blain1894   Blain   Adair     male   1894{-}11{-}21        NA}
\CommentTok{\#\textgreater{}  2 Dein1889    Dein    Adam      male   1889{-}03{-}04        NA}
\CommentTok{\#\textgreater{}  3 Armstrong1\textasciitilde{} Armstr\textasciitilde{} Adam      male   1909{-}07{-}01        NA}
\CommentTok{\#\textgreater{}  4 Bandt1972   Bandt   Adam      male   1972{-}03{-}11        NA}
\CommentTok{\#\textgreater{}  5 Ridgeway19\textasciitilde{} Ridgew\textasciitilde{} Aden      male   1962{-}09{-}18        NA}
\CommentTok{\#\textgreater{}  6 Bennett1933 Bennett Adrian    male   1933{-}01{-}21        NA}
\CommentTok{\#\textgreater{}  7 Gibson1935  Gibson  Adrian    male   1935{-}11{-}03        NA}
\CommentTok{\#\textgreater{}  8 Wynne1850   Wynne   Agar      male   1850{-}01{-}15        NA}
\CommentTok{\#\textgreater{}  9 Robertson1\textasciitilde{} Robert\textasciitilde{} Agnes     female 1882{-}07{-}31        NA}
\CommentTok{\#\textgreater{} 10 Pittard1902 Pittard Alan      male   1902{-}11{-}15        NA}
\CommentTok{\#\textgreater{} \# ... with 1,773 more rows, and 4 more variables:}
\CommentTok{\#\textgreater{} \#   deathDate \textless{}date\textgreater{}, member \textless{}dbl\textgreater{}, senator \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   wasPrimeMinister \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

When we use \texttt{arrange()} it is important to be clear about precedence. For instance, changing to birthday and then first name would give a different arrangement.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(birthYear, firstName)}
\CommentTok{\#\textgreater{} \# A tibble: 1,783 x 10}
\CommentTok{\#\textgreater{}    uniqueID    surname firstName gender birthDate birthYear}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}       \textless{}chr\textgreater{}   \textless{}chr\textgreater{}     \textless{}chr\textgreater{}  \textless{}date\textgreater{}        \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1 Edwards1842 Edwards Richard   male   NA             1842}
\CommentTok{\#\textgreater{}  2 Sawers1844  Sawers  William   male   NA             1844}
\CommentTok{\#\textgreater{}  3 Barker1846  Barker  Stephen   male   NA             1846}
\CommentTok{\#\textgreater{}  4 Corser1852  Corser  Edward    male   NA             1852}
\CommentTok{\#\textgreater{}  5 Lee1856     Lee     Henry     male   NA             1856}
\CommentTok{\#\textgreater{}  6 Grant1857   Grant   John      male   NA             1857}
\CommentTok{\#\textgreater{}  7 Palmer1859  Palmer  Albert    male   NA             1859}
\CommentTok{\#\textgreater{}  8 Riley1859   Riley   Edward    male   NA             1859}
\CommentTok{\#\textgreater{}  9 Abbott1859  Abbott  Richard   male   NA             1859}
\CommentTok{\#\textgreater{} 10 Kennedy1860 Kennedy Thomas    male   NA             1860}
\CommentTok{\#\textgreater{} \# ... with 1,773 more rows, and 4 more variables:}
\CommentTok{\#\textgreater{} \#   deathDate \textless{}date\textgreater{}, member \textless{}dbl\textgreater{}, senator \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   wasPrimeMinister \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

A nice way to arrange by a variety of columns is to use \texttt{across()}. It enables us to use the `selection helpers' such as \texttt{starts\_with()} that were mentioned in association with \texttt{select()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(firstName, birthYear))) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 10}
\CommentTok{\#\textgreater{}   uniqueID    surname  firstName gender birthDate  birthYear}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}       \textless{}chr\textgreater{}    \textless{}chr\textgreater{}     \textless{}chr\textgreater{}  \textless{}date\textgreater{}         \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Blain1894   Blain    Adair     male   1894{-}11{-}21        NA}
\CommentTok{\#\textgreater{} 2 Armstrong1\textasciitilde{} Armstro\textasciitilde{} Adam      male   1909{-}07{-}01        NA}
\CommentTok{\#\textgreater{} 3 Bandt1972   Bandt    Adam      male   1972{-}03{-}11        NA}
\CommentTok{\#\textgreater{} 4 Dein1889    Dein     Adam      male   1889{-}03{-}04        NA}
\CommentTok{\#\textgreater{} 5 Ridgeway19\textasciitilde{} Ridgeway Aden      male   1962{-}09{-}18        NA}
\CommentTok{\#\textgreater{} 6 Bennett1933 Bennett  Adrian    male   1933{-}01{-}21        NA}
\CommentTok{\#\textgreater{} \# ... with 4 more variables: deathDate \textless{}date\textgreater{},}
\CommentTok{\#\textgreater{} \#   member \textless{}dbl\textgreater{}, senator \textless{}dbl\textgreater{}, wasPrimeMinister \textless{}dbl\textgreater{}}

\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{\textquotesingle{}birth\textquotesingle{}}\NormalTok{))) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 10}
\CommentTok{\#\textgreater{}   uniqueID     surname firstName gender birthDate  birthYear}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}        \textless{}chr\textgreater{}   \textless{}chr\textgreater{}     \textless{}chr\textgreater{}  \textless{}date\textgreater{}         \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Braddon1829  Braddon Edward    male   1829{-}06{-}11        NA}
\CommentTok{\#\textgreater{} 2 Ferguson1830 Fergus\textasciitilde{} John      male   1830{-}03{-}15        NA}
\CommentTok{\#\textgreater{} 3 Zeal1830     Zeal    William   male   1830{-}12{-}05        NA}
\CommentTok{\#\textgreater{} 4 Fraser1832   Fraser  Simon     male   1832{-}08{-}21        NA}
\CommentTok{\#\textgreater{} 5 Groom1833    Groom   William   male   1833{-}03{-}09        NA}
\CommentTok{\#\textgreater{} 6 Sargood1834  Sargood Frederick male   1834{-}05{-}30        NA}
\CommentTok{\#\textgreater{} \# ... with 4 more variables: deathDate \textless{}date\textgreater{},}
\CommentTok{\#\textgreater{} \#   member \textless{}dbl\textgreater{}, senator \textless{}dbl\textgreater{}, wasPrimeMinister \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

\hypertarget{mutate}{%
\subsection{\texorpdfstring{\texttt{mutate()}}{mutate()}}\label{mutate}}

We use \texttt{mutate()} when we want to make a new column. For instance, perhaps we want to make a new column that is 1 if a person was both a member and a senator and 0 otherwise. That is to say that our new column would denote politicians that served in both the upper and the lower house.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\OtherTok{\textless{}{-}} 
\NormalTok{  australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{was\_both =} \FunctionTok{if\_else}\NormalTok{(member }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ senator }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))}

\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{select}\NormalTok{(member, senator, was\_both)}
\CommentTok{\#\textgreater{} \# A tibble: 1,783 x 3}
\CommentTok{\#\textgreater{}    member senator was\_both}
\CommentTok{\#\textgreater{}     \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1      0       1        0}
\CommentTok{\#\textgreater{}  2      1       1        1}
\CommentTok{\#\textgreater{}  3      0       1        0}
\CommentTok{\#\textgreater{}  4      1       0        0}
\CommentTok{\#\textgreater{}  5      1       0        0}
\CommentTok{\#\textgreater{}  6      1       0        0}
\CommentTok{\#\textgreater{}  7      1       0        0}
\CommentTok{\#\textgreater{}  8      0       1        0}
\CommentTok{\#\textgreater{}  9      0       1        0}
\CommentTok{\#\textgreater{} 10      1       0        0}
\CommentTok{\#\textgreater{} \# ... with 1,773 more rows}
\end{Highlighting}
\end{Shaded}

We could use \texttt{mutate()} with math, such as addition and subtraction. For instance, we could calculate the age that the politicians are (or would have been) in 2022.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\OtherTok{\textless{}{-}} 
\NormalTok{  australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{age =} \DecValTok{2022} \SpecialCharTok{{-}}\NormalTok{ lubridate}\SpecialCharTok{::}\FunctionTok{year}\NormalTok{(birthDate))}

\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{select}\NormalTok{(uniqueID, age)}
\CommentTok{\#\textgreater{} \# A tibble: 1,783 x 2}
\CommentTok{\#\textgreater{}    uniqueID     age}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}      \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1 Abbott1859    NA}
\CommentTok{\#\textgreater{}  2 Abbott1869   153}
\CommentTok{\#\textgreater{}  3 Abbott1877   145}
\CommentTok{\#\textgreater{}  4 Abbott1886   136}
\CommentTok{\#\textgreater{}  5 Abbott1891   131}
\CommentTok{\#\textgreater{}  6 Abbott1957    65}
\CommentTok{\#\textgreater{}  7 Abel1939      83}
\CommentTok{\#\textgreater{}  8 Abetz1958     64}
\CommentTok{\#\textgreater{}  9 Adams1943     79}
\CommentTok{\#\textgreater{} 10 Adams1951     71}
\CommentTok{\#\textgreater{} \# ... with 1,773 more rows}
\end{Highlighting}
\end{Shaded}

There are a variety of functions that are especially useful when constructing new columns. These include \texttt{log()} which will compute the natural logarithm, \texttt{lead()} which will bring values up by one row, \texttt{lag()} which will push values down by one row, and \texttt{cumsum()} which creates a cumulative sum of the column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{select}\NormalTok{(uniqueID, age) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{log\_age =} \FunctionTok{log}\NormalTok{(age)) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{}   uniqueID     age log\_age}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}      \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Abbott1859    NA   NA   }
\CommentTok{\#\textgreater{} 2 Abbott1869   153    5.03}
\CommentTok{\#\textgreater{} 3 Abbott1877   145    4.98}
\CommentTok{\#\textgreater{} 4 Abbott1886   136    4.91}
\CommentTok{\#\textgreater{} 5 Abbott1891   131    4.88}
\CommentTok{\#\textgreater{} 6 Abbott1957    65    4.17}

\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{select}\NormalTok{(uniqueID, age) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lead\_age =} \FunctionTok{lead}\NormalTok{(age)) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{}   uniqueID     age lead\_age}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}      \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Abbott1859    NA      153}
\CommentTok{\#\textgreater{} 2 Abbott1869   153      145}
\CommentTok{\#\textgreater{} 3 Abbott1877   145      136}
\CommentTok{\#\textgreater{} 4 Abbott1886   136      131}
\CommentTok{\#\textgreater{} 5 Abbott1891   131       65}
\CommentTok{\#\textgreater{} 6 Abbott1957    65       83}

\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{select}\NormalTok{(uniqueID, age) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lag\_age =} \FunctionTok{lag}\NormalTok{(age)) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{}   uniqueID     age lag\_age}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}      \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Abbott1859    NA      NA}
\CommentTok{\#\textgreater{} 2 Abbott1869   153      NA}
\CommentTok{\#\textgreater{} 3 Abbott1877   145     153}
\CommentTok{\#\textgreater{} 4 Abbott1886   136     145}
\CommentTok{\#\textgreater{} 5 Abbott1891   131     136}
\CommentTok{\#\textgreater{} 6 Abbott1957    65     131}

\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{select}\NormalTok{(uniqueID, age) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(age)) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{cumulative\_age =} \FunctionTok{cumsum}\NormalTok{(age)) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{}   uniqueID     age cumulative\_age}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}      \textless{}dbl\textgreater{}          \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Abbott1869   153            153}
\CommentTok{\#\textgreater{} 2 Abbott1877   145            298}
\CommentTok{\#\textgreater{} 3 Abbott1886   136            434}
\CommentTok{\#\textgreater{} 4 Abbott1891   131            565}
\CommentTok{\#\textgreater{} 5 Abbott1957    65            630}
\CommentTok{\#\textgreater{} 6 Abel1939      83            713}
\end{Highlighting}
\end{Shaded}

As we have in earlier examples, we can also use \texttt{mutate()} in combination with \texttt{across()}. This includes the potential use of the selection helpers. For instance, we could count the number of characters in both the first and last names at the same time.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(firstName, surname), str\_count)) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{select}\NormalTok{(uniqueID, firstName, surname)}
\CommentTok{\#\textgreater{} \# A tibble: 1,783 x 3}
\CommentTok{\#\textgreater{}    uniqueID   firstName surname}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}          \textless{}int\textgreater{}   \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{}  1 Abbott1859         7       6}
\CommentTok{\#\textgreater{}  2 Abbott1869         5       6}
\CommentTok{\#\textgreater{}  3 Abbott1877         9       6}
\CommentTok{\#\textgreater{}  4 Abbott1886         7       6}
\CommentTok{\#\textgreater{}  5 Abbott1891         6       6}
\CommentTok{\#\textgreater{}  6 Abbott1957         7       6}
\CommentTok{\#\textgreater{}  7 Abel1939           4       4}
\CommentTok{\#\textgreater{}  8 Abetz1958          4       5}
\CommentTok{\#\textgreater{}  9 Adams1943          6       5}
\CommentTok{\#\textgreater{} 10 Adams1951          4       5}
\CommentTok{\#\textgreater{} \# ... with 1,773 more rows}
\end{Highlighting}
\end{Shaded}

Finally, we use \texttt{case\_when()} when we need to make a new column on the basis of more than two conditional statements. For instance, we may have some years and want to group them into decades.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{year\_of\_birth =}\NormalTok{ lubridate}\SpecialCharTok{::}\FunctionTok{year}\NormalTok{(birthDate),}
         \AttributeTok{decade\_of\_birth =} 
           \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{             year\_of\_birth }\SpecialCharTok{\textless{}=} \DecValTok{1929} \SpecialCharTok{\textasciitilde{}} \StringTok{"pre{-}1930"}\NormalTok{,}
\NormalTok{             year\_of\_birth }\SpecialCharTok{\textless{}=} \DecValTok{1939} \SpecialCharTok{\textasciitilde{}} \StringTok{"1930s"}\NormalTok{,}
\NormalTok{             year\_of\_birth }\SpecialCharTok{\textless{}=} \DecValTok{1949} \SpecialCharTok{\textasciitilde{}} \StringTok{"1940s"}\NormalTok{,}
\NormalTok{             year\_of\_birth }\SpecialCharTok{\textless{}=} \DecValTok{1959} \SpecialCharTok{\textasciitilde{}} \StringTok{"1950s"}\NormalTok{,}
\NormalTok{             year\_of\_birth }\SpecialCharTok{\textless{}=} \DecValTok{1969} \SpecialCharTok{\textasciitilde{}} \StringTok{"1960s"}\NormalTok{,}
\NormalTok{             year\_of\_birth }\SpecialCharTok{\textless{}=} \DecValTok{1979} \SpecialCharTok{\textasciitilde{}} \StringTok{"1970s"}\NormalTok{,}
\NormalTok{             year\_of\_birth }\SpecialCharTok{\textless{}=} \DecValTok{1989} \SpecialCharTok{\textasciitilde{}} \StringTok{"1980s"}\NormalTok{,}
             \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{"Unknown or error"}
\NormalTok{             )}
\NormalTok{  ) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{select}\NormalTok{(uniqueID, year\_of\_birth, decade\_of\_birth)}
\CommentTok{\#\textgreater{} \# A tibble: 1,783 x 3}
\CommentTok{\#\textgreater{}    uniqueID   year\_of\_birth decade\_of\_birth }
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}              \textless{}dbl\textgreater{} \textless{}chr\textgreater{}           }
\CommentTok{\#\textgreater{}  1 Abbott1859            NA Unknown or error}
\CommentTok{\#\textgreater{}  2 Abbott1869          1869 pre{-}1930        }
\CommentTok{\#\textgreater{}  3 Abbott1877          1877 pre{-}1930        }
\CommentTok{\#\textgreater{}  4 Abbott1886          1886 pre{-}1930        }
\CommentTok{\#\textgreater{}  5 Abbott1891          1891 pre{-}1930        }
\CommentTok{\#\textgreater{}  6 Abbott1957          1957 1950s           }
\CommentTok{\#\textgreater{}  7 Abel1939            1939 1930s           }
\CommentTok{\#\textgreater{}  8 Abetz1958           1958 1950s           }
\CommentTok{\#\textgreater{}  9 Adams1943           1943 1940s           }
\CommentTok{\#\textgreater{} 10 Adams1951           1951 1950s           }
\CommentTok{\#\textgreater{} \# ... with 1,773 more rows}
\end{Highlighting}
\end{Shaded}

We could accomplish this with a series of \texttt{if\_else()} statements, but \texttt{case\_when()} is more clear. The cases are evaluated in order and as soon as there is a match \texttt{case\_when()} does not continue to the remainder of the cases. So it can be useful to have a catch-all at the end that will signal if there is a potential issue that we might like to know about.

\hypertarget{summarise}{%
\subsection{\texorpdfstring{\texttt{summarise()}}{summarise()}}\label{summarise}}

We use \texttt{summarise()} when we would like to make new, condensed, summary variables. For instance, perhaps we would like to know the minimum, average, and maximum of some column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{youngest =} \FunctionTok{min}\NormalTok{(age, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
            \AttributeTok{oldest =} \FunctionTok{max}\NormalTok{(age, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
            \AttributeTok{average =} \FunctionTok{mean}\NormalTok{(age, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 3}
\CommentTok{\#\textgreater{}   youngest oldest average}
\CommentTok{\#\textgreater{}      \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1       28    193    101.}
\end{Highlighting}
\end{Shaded}

As an aside, \texttt{summarise()} and \texttt{summarize()} are equivalent and we can use either.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{youngest =} \FunctionTok{min}\NormalTok{(age, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
            \AttributeTok{oldest =} \FunctionTok{max}\NormalTok{(age, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
            \AttributeTok{average =} \FunctionTok{mean}\NormalTok{(age, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 3}
\CommentTok{\#\textgreater{}   youngest oldest average}
\CommentTok{\#\textgreater{}      \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1       28    193    101.}
\end{Highlighting}
\end{Shaded}

By default, \texttt{summarise()} will provide one row of output for a whole dataset. For instance, in the earlier example we found the youngest, oldest, and average across all politicians. However, we can create more groups in our dataset using \texttt{group\_by()}. And we can then apply another function within the context of those groups. We could use many functions on the basis of groups, but the \texttt{summarise()} function is particularly powerful in conjunction with \texttt{group\_by()}. For instance, we could group by gender, and then get age-based summary statistics.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(gender) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{youngest =} \FunctionTok{min}\NormalTok{(age, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
            \AttributeTok{oldest =} \FunctionTok{max}\NormalTok{(age, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
            \AttributeTok{average =} \FunctionTok{mean}\NormalTok{(age, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}
\CommentTok{\#\textgreater{} \# A tibble: 2 x 4}
\CommentTok{\#\textgreater{}   gender youngest oldest average}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}     \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 female       32    140    66.0}
\CommentTok{\#\textgreater{} 2 male         28    193   106.}
\end{Highlighting}
\end{Shaded}

Similarly, we could look at youngest, oldest, and mean age at death by gender.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{days\_lived =}\NormalTok{ deathDate }\SpecialCharTok{{-}}\NormalTok{ birthDate) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(days\_lived)) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(gender) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{min\_days =} \FunctionTok{min}\NormalTok{(days\_lived),}
    \AttributeTok{mean\_days =} \FunctionTok{mean}\NormalTok{(days\_lived) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} \FunctionTok{round}\NormalTok{(),}
    \AttributeTok{max\_days =} \FunctionTok{max}\NormalTok{(days\_lived)}
\NormalTok{    )}
\CommentTok{\#\textgreater{} \# A tibble: 2 x 4}
\CommentTok{\#\textgreater{}   gender min\_days   mean\_days  max\_days  }
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}  \textless{}drtn\textgreater{}     \textless{}drtn\textgreater{}     \textless{}drtn\textgreater{}    }
\CommentTok{\#\textgreater{} 1 female 14856 days 28857 days 35560 days}
\CommentTok{\#\textgreater{} 2 male   12380 days 27376 days 36416 days}
\end{Highlighting}
\end{Shaded}

And so we learn that female members of parliament on average lived slightly longer than male members of parliament.

We can use \texttt{group\_by()} on the basis of more than one group. For instance, we could look at the average number of days lived by gender and by house.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{days\_lived =}\NormalTok{ deathDate }\SpecialCharTok{{-}}\NormalTok{ birthDate) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(days\_lived)) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(gender, member) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{min\_days =} \FunctionTok{min}\NormalTok{(days\_lived),}
    \AttributeTok{mean\_days =} \FunctionTok{mean}\NormalTok{(days\_lived) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} \FunctionTok{round}\NormalTok{(),}
    \AttributeTok{max\_days =} \FunctionTok{max}\NormalTok{(days\_lived)}
\NormalTok{    )}
\CommentTok{\#\textgreater{} \# A tibble: 4 x 5}
\CommentTok{\#\textgreater{} \# Groups:   gender [2]}
\CommentTok{\#\textgreater{}   gender member min\_days   mean\_days  max\_days  }
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}   \textless{}dbl\textgreater{} \textless{}drtn\textgreater{}     \textless{}drtn\textgreater{}     \textless{}drtn\textgreater{}    }
\CommentTok{\#\textgreater{} 1 female      0 21746 days 29517 days 35560 days}
\CommentTok{\#\textgreater{} 2 female      1 14856 days 27538 days 33442 days}
\CommentTok{\#\textgreater{} 3 male        0 13619 days 27133 days 36416 days}
\CommentTok{\#\textgreater{} 4 male        1 12380 days 27496 days 36328 days}
\end{Highlighting}
\end{Shaded}

We can use \texttt{count()} to create counts by groups. For instance, the number of politicians by gender.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(gender) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{count}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 2 x 2}
\CommentTok{\#\textgreater{} \# Groups:   gender [2]}
\CommentTok{\#\textgreater{}   gender     n}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}  \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 female   240}
\CommentTok{\#\textgreater{} 2 male    1543}
\end{Highlighting}
\end{Shaded}

In addition to the \texttt{count()}, we could make a proportion.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(gender) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{count}\NormalTok{() }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{proportion =}\NormalTok{ n}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{sum}\NormalTok{(n)))}
\CommentTok{\#\textgreater{} \# A tibble: 2 x 3}
\CommentTok{\#\textgreater{}   gender     n proportion}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}  \textless{}int\textgreater{}      \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 female   240      0.135}
\CommentTok{\#\textgreater{} 2 male    1543      0.865}
\end{Highlighting}
\end{Shaded}

Using \texttt{count()} is essentially the same as using \texttt{group\_by()} and then \texttt{summarise()}, and we get the same result in that way.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(gender) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{n =} \FunctionTok{n}\NormalTok{())}
\CommentTok{\#\textgreater{} \# A tibble: 2 x 2}
\CommentTok{\#\textgreater{}   gender     n}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}  \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 female   240}
\CommentTok{\#\textgreater{} 2 male    1543}
\end{Highlighting}
\end{Shaded}

And there is a similarly helpful function for \texttt{mutate()}, which is \texttt{add\_count()}. The difference is that the number will be added in a column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(gender) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{add\_count}\NormalTok{() }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{select}\NormalTok{(uniqueID, gender, n)}
\CommentTok{\#\textgreater{} \# A tibble: 1,783 x 3}
\CommentTok{\#\textgreater{} \# Groups:   gender [2]}
\CommentTok{\#\textgreater{}    uniqueID   gender     n}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}      \textless{}chr\textgreater{}  \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{}  1 Abbott1859 male    1543}
\CommentTok{\#\textgreater{}  2 Abbott1869 male    1543}
\CommentTok{\#\textgreater{}  3 Abbott1877 male    1543}
\CommentTok{\#\textgreater{}  4 Abbott1886 male    1543}
\CommentTok{\#\textgreater{}  5 Abbott1891 male    1543}
\CommentTok{\#\textgreater{}  6 Abbott1957 male    1543}
\CommentTok{\#\textgreater{}  7 Abel1939   male    1543}
\CommentTok{\#\textgreater{}  8 Abetz1958  male    1543}
\CommentTok{\#\textgreater{}  9 Adams1943  female   240}
\CommentTok{\#\textgreater{} 10 Adams1951  male    1543}
\CommentTok{\#\textgreater{} \# ... with 1,773 more rows}
\end{Highlighting}
\end{Shaded}

\hypertarget{base}{%
\section{Base}\label{base}}

While the \texttt{tidyverse} was established relatively recently to help with data science, R existed long before this. There is a host of functionality that is built into R especially around the core needs of programming and statisticians.

In particular, we will cover:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{class()}
\item
  data simulation
\item
  \texttt{function()}, \texttt{for()}, and \texttt{apply()}
\end{enumerate}

There is no need to install any additional packages, as this functionality comes with R.

\hypertarget{class}{%
\subsection{\texorpdfstring{\texttt{class()}}{class()}}\label{class}}

In everyday usage `a, b, c, \ldots{}' are letters and `1, 2, 3,\ldots{}' are numbers. And we use letters and numbers differently, for instance we do not add letters. Similarly, R needs to have some way of distinguishing different classes of content. And to define the properties that each class has, `how it behaves, and how it relates to other types of objects' \citep{advancedr}.

Classes have a hierarchy. For instance, we are `human', which is itself `animal'. All `humans' are `animals', but not all `animals' are `humans'. Similarly, all integers are numbers, but not all numbers are integers. We can find out the class of an object in R with \texttt{class()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a\_number }\OtherTok{\textless{}{-}} \DecValTok{8}
\FunctionTok{class}\NormalTok{(a\_number)}
\CommentTok{\#\textgreater{} [1] "numeric"}

\NormalTok{a\_letter }\OtherTok{\textless{}{-}} \StringTok{"a"}
\FunctionTok{class}\NormalTok{(a\_letter)}
\CommentTok{\#\textgreater{} [1] "character"}
\end{Highlighting}
\end{Shaded}

The classes that we cover here are `numeric', `character', `factor', `date', and `data.frame'.

The first thing to know is that, in the same way that a frog can become a prince, we can sometimes change the class of an object in R. For instance, we could start with a `numeric', change it to a `character' with \texttt{as.character()}, and then a `factor' with \texttt{as.factor()}. But if we tried to make it into a date with \texttt{as.Date()} we would get an error because no all numbers have the properties that are needed to be a date.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a\_number }\OtherTok{\textless{}{-}} \DecValTok{8}
\NormalTok{a\_number}
\CommentTok{\#\textgreater{} [1] 8}
\FunctionTok{class}\NormalTok{(a\_number)}
\CommentTok{\#\textgreater{} [1] "numeric"}

\NormalTok{a\_number }\OtherTok{\textless{}{-}} \FunctionTok{as.character}\NormalTok{(a\_number)}
\NormalTok{a\_number}
\CommentTok{\#\textgreater{} [1] "8"}
\FunctionTok{class}\NormalTok{(a\_number)}
\CommentTok{\#\textgreater{} [1] "character"}

\NormalTok{a\_number }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(a\_number)}
\NormalTok{a\_number}
\CommentTok{\#\textgreater{} [1] 8}
\CommentTok{\#\textgreater{} Levels: 8}
\FunctionTok{class}\NormalTok{(a\_number)}
\CommentTok{\#\textgreater{} [1] "factor"}
\end{Highlighting}
\end{Shaded}

Compared with `numeric' and `character' classes, the `factor' class might be less familiar. A `factor' is used for categorical data that can only take certain values \citep{advancedr}. For instance, typical usage of a factor variable would be a binary, such as `day' or `night'. It is also often used for age-groups, such as `18-29', `30-44', `45-60', `60+' (as opposed to age, which would often be a `numeric'); and sometimes for level of education: `less than high school', `high school', `college', `undergraduate degree', `postgraduate degree'. We can find the allowed levels for a `factor' using \texttt{levels()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{age\_groups }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(}
  \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}18{-}29\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}30{-}44\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}45{-}60\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}60+\textquotesingle{}}\NormalTok{)}
\NormalTok{)}
\NormalTok{age\_groups}
\CommentTok{\#\textgreater{} [1] 18{-}29 30{-}44 45{-}60 60+  }
\CommentTok{\#\textgreater{} Levels: 18{-}29 30{-}44 45{-}60 60+}
\FunctionTok{class}\NormalTok{(age\_groups)}
\CommentTok{\#\textgreater{} [1] "factor"}
\FunctionTok{levels}\NormalTok{(age\_groups)}
\CommentTok{\#\textgreater{} [1] "18{-}29" "30{-}44" "45{-}60" "60+"}
\end{Highlighting}
\end{Shaded}

Dates are an especially tricky class and quickly become complicated. Nonetheless, at a foundational level, we can use \texttt{as.Date()} to convert a character that looks like a date into an actual date. This enables us to, say, perform addition and subtraction, when we would not be able to do that with a character.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{looks\_like\_a\_date\_but\_is\_not }\OtherTok{\textless{}{-}} \StringTok{"2022{-}01{-}01"}
\NormalTok{looks\_like\_a\_date\_but\_is\_not}
\CommentTok{\#\textgreater{} [1] "2022{-}01{-}01"}
\FunctionTok{class}\NormalTok{(looks\_like\_a\_date\_but\_is\_not)}
\CommentTok{\#\textgreater{} [1] "character"}
\NormalTok{is\_a\_date }\OtherTok{\textless{}{-}} \FunctionTok{as.Date}\NormalTok{(looks\_like\_a\_date\_but\_is\_not)}
\NormalTok{is\_a\_date}
\CommentTok{\#\textgreater{} [1] "2022{-}01{-}01"}
\FunctionTok{class}\NormalTok{(is\_a\_date)}
\CommentTok{\#\textgreater{} [1] "Date"}
\NormalTok{is\_a\_date }\SpecialCharTok{+} \DecValTok{3}
\CommentTok{\#\textgreater{} [1] "2022{-}01{-}04"}
\end{Highlighting}
\end{Shaded}

The final class that we discuss here is `data.frame'. This looks like a spreadsheet and is commonly used to store the data that we will analyze. Formally, `a data frame is a list of equal-length vectors' \citep{advancedr}. It will have column and row names which we can see using \texttt{colnames()} and \texttt{rownames()}, although often the names of the rows are just numbers.

To illustrate this, we use the `ResumeNames' dataset from \texttt{AER} \citep{citeaer}. This package can be installed in the same way as any other package from CRAN. This dataset comprises cross-sectional data about resume content, especially the name used on the resume, and associated information about whether the candidate received a call-back for 4,870 fictitious resumes. The dataset was created by \citet{bertrand2004emily} who sent fictitious resumes in response to job advertisements in Boston and Chicago that differed in whether the resume was assigned a `very African American sounding name or a very White sounding name'. They found considerable discrimination whereby `White names receive 50 percent more callbacks for interviews'.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"AER"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(AER)}
\FunctionTok{data}\NormalTok{(}\StringTok{"ResumeNames"}\NormalTok{, }\AttributeTok{package =} \StringTok{"AER"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ResumeNames }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{}      name gender ethnicity quality call    city jobs}
\CommentTok{\#\textgreater{} 1 Allison female      cauc     low   no chicago    2}
\CommentTok{\#\textgreater{} 2 Kristen female      cauc    high   no chicago    3}
\CommentTok{\#\textgreater{} 3 Lakisha female      afam     low   no chicago    1}
\CommentTok{\#\textgreater{} 4 Latonya female      afam    high   no chicago    4}
\CommentTok{\#\textgreater{} 5  Carrie female      cauc    high   no chicago    3}
\CommentTok{\#\textgreater{} 6     Jay   male      cauc     low   no chicago    2}
\CommentTok{\#\textgreater{}   experience honors volunteer military holes school email}
\CommentTok{\#\textgreater{} 1          6     no        no       no   yes     no    no}
\CommentTok{\#\textgreater{} 2          6     no       yes      yes    no    yes   yes}
\CommentTok{\#\textgreater{} 3          6     no        no       no    no    yes    no}
\CommentTok{\#\textgreater{} 4          6     no       yes       no   yes     no   yes}
\CommentTok{\#\textgreater{} 5         22     no        no       no    no    yes   yes}
\CommentTok{\#\textgreater{} 6          6    yes        no       no    no     no    no}
\CommentTok{\#\textgreater{}   computer special college minimum equal     wanted}
\CommentTok{\#\textgreater{} 1      yes      no     yes       5   yes supervisor}
\CommentTok{\#\textgreater{} 2      yes      no      no       5   yes supervisor}
\CommentTok{\#\textgreater{} 3      yes      no     yes       5   yes supervisor}
\CommentTok{\#\textgreater{} 4      yes     yes      no       5   yes supervisor}
\CommentTok{\#\textgreater{} 5      yes      no      no    some   yes  secretary}
\CommentTok{\#\textgreater{} 6       no     yes     yes    none   yes      other}
\CommentTok{\#\textgreater{}   requirements reqexp reqcomm reqeduc reqcomp reqorg}
\CommentTok{\#\textgreater{} 1          yes    yes      no      no     yes     no}
\CommentTok{\#\textgreater{} 2          yes    yes      no      no     yes     no}
\CommentTok{\#\textgreater{} 3          yes    yes      no      no     yes     no}
\CommentTok{\#\textgreater{} 4          yes    yes      no      no     yes     no}
\CommentTok{\#\textgreater{} 5          yes    yes      no      no     yes    yes}
\CommentTok{\#\textgreater{} 6           no     no      no      no      no     no}
\CommentTok{\#\textgreater{}                           industry}
\CommentTok{\#\textgreater{} 1                    manufacturing}
\CommentTok{\#\textgreater{} 2                    manufacturing}
\CommentTok{\#\textgreater{} 3                    manufacturing}
\CommentTok{\#\textgreater{} 4                    manufacturing}
\CommentTok{\#\textgreater{} 5 health/education/social services}
\CommentTok{\#\textgreater{} 6                            trade}
\FunctionTok{class}\NormalTok{(ResumeNames)}
\CommentTok{\#\textgreater{} [1] "data.frame"}
\FunctionTok{colnames}\NormalTok{(ResumeNames)}
\CommentTok{\#\textgreater{}  [1] "name"         "gender"       "ethnicity"   }
\CommentTok{\#\textgreater{}  [4] "quality"      "call"         "city"        }
\CommentTok{\#\textgreater{}  [7] "jobs"         "experience"   "honors"      }
\CommentTok{\#\textgreater{} [10] "volunteer"    "military"     "holes"       }
\CommentTok{\#\textgreater{} [13] "school"       "email"        "computer"    }
\CommentTok{\#\textgreater{} [16] "special"      "college"      "minimum"     }
\CommentTok{\#\textgreater{} [19] "equal"        "wanted"       "requirements"}
\CommentTok{\#\textgreater{} [22] "reqexp"       "reqcomm"      "reqeduc"     }
\CommentTok{\#\textgreater{} [25] "reqcomp"      "reqorg"       "industry"}
\end{Highlighting}
\end{Shaded}

We can examine the class of the vectors, i.e.~the columns, that make-up a data frame by specifying the column name.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(ResumeNames}\SpecialCharTok{$}\NormalTok{name)}
\CommentTok{\#\textgreater{} [1] "factor"}
\FunctionTok{class}\NormalTok{(ResumeNames}\SpecialCharTok{$}\NormalTok{jobs)}
\CommentTok{\#\textgreater{} [1] "integer"}
\end{Highlighting}
\end{Shaded}

Sometimes it is helpful to be able to change the classes of many columns at once. We can do this by using \texttt{mutate()} and \texttt{across()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(ResumeNames}\SpecialCharTok{$}\NormalTok{name)}
\CommentTok{\#\textgreater{} [1] "factor"}
\FunctionTok{class}\NormalTok{(ResumeNames}\SpecialCharTok{$}\NormalTok{gender)}
\CommentTok{\#\textgreater{} [1] "factor"}
\FunctionTok{class}\NormalTok{(ResumeNames}\SpecialCharTok{$}\NormalTok{ethnicity)}
\CommentTok{\#\textgreater{} [1] "factor"}

\NormalTok{ResumeNames }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(name, gender, ethnicity), as.character)) }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{}      name gender ethnicity quality call    city jobs}
\CommentTok{\#\textgreater{} 1 Allison female      cauc     low   no chicago    2}
\CommentTok{\#\textgreater{} 2 Kristen female      cauc    high   no chicago    3}
\CommentTok{\#\textgreater{} 3 Lakisha female      afam     low   no chicago    1}
\CommentTok{\#\textgreater{} 4 Latonya female      afam    high   no chicago    4}
\CommentTok{\#\textgreater{} 5  Carrie female      cauc    high   no chicago    3}
\CommentTok{\#\textgreater{} 6     Jay   male      cauc     low   no chicago    2}
\CommentTok{\#\textgreater{}   experience honors volunteer military holes school email}
\CommentTok{\#\textgreater{} 1          6     no        no       no   yes     no    no}
\CommentTok{\#\textgreater{} 2          6     no       yes      yes    no    yes   yes}
\CommentTok{\#\textgreater{} 3          6     no        no       no    no    yes    no}
\CommentTok{\#\textgreater{} 4          6     no       yes       no   yes     no   yes}
\CommentTok{\#\textgreater{} 5         22     no        no       no    no    yes   yes}
\CommentTok{\#\textgreater{} 6          6    yes        no       no    no     no    no}
\CommentTok{\#\textgreater{}   computer special college minimum equal     wanted}
\CommentTok{\#\textgreater{} 1      yes      no     yes       5   yes supervisor}
\CommentTok{\#\textgreater{} 2      yes      no      no       5   yes supervisor}
\CommentTok{\#\textgreater{} 3      yes      no     yes       5   yes supervisor}
\CommentTok{\#\textgreater{} 4      yes     yes      no       5   yes supervisor}
\CommentTok{\#\textgreater{} 5      yes      no      no    some   yes  secretary}
\CommentTok{\#\textgreater{} 6       no     yes     yes    none   yes      other}
\CommentTok{\#\textgreater{}   requirements reqexp reqcomm reqeduc reqcomp reqorg}
\CommentTok{\#\textgreater{} 1          yes    yes      no      no     yes     no}
\CommentTok{\#\textgreater{} 2          yes    yes      no      no     yes     no}
\CommentTok{\#\textgreater{} 3          yes    yes      no      no     yes     no}
\CommentTok{\#\textgreater{} 4          yes    yes      no      no     yes     no}
\CommentTok{\#\textgreater{} 5          yes    yes      no      no     yes    yes}
\CommentTok{\#\textgreater{} 6           no     no      no      no      no     no}
\CommentTok{\#\textgreater{}                           industry}
\CommentTok{\#\textgreater{} 1                    manufacturing}
\CommentTok{\#\textgreater{} 2                    manufacturing}
\CommentTok{\#\textgreater{} 3                    manufacturing}
\CommentTok{\#\textgreater{} 4                    manufacturing}
\CommentTok{\#\textgreater{} 5 health/education/social services}
\CommentTok{\#\textgreater{} 6                            trade}

\FunctionTok{class}\NormalTok{(ResumeNames}\SpecialCharTok{$}\NormalTok{name)}
\CommentTok{\#\textgreater{} [1] "factor"}
\FunctionTok{class}\NormalTok{(ResumeNames}\SpecialCharTok{$}\NormalTok{gender)}
\CommentTok{\#\textgreater{} [1] "factor"}
\FunctionTok{class}\NormalTok{(ResumeNames}\SpecialCharTok{$}\NormalTok{ethnicity)}
\CommentTok{\#\textgreater{} [1] "factor"}
\end{Highlighting}
\end{Shaded}

There are many ways for code to not run but having an issue with the class is always among the first things to check. Common issues are variables that we think should be `character' or `numeric', actually being `factor'. And variables that we think should be `numeric' actually being `character'.

\hypertarget{simulating-data}{%
\subsection{Simulating data}\label{simulating-data}}

Simulating data is a key skill for telling believable stories with data. In order to simulate data, we need to be able to randomly draw from statistical distributions and other collections. R has a variety of functions to make this easier, including: the normal distribution, \texttt{rnorm()}; the uniform distribution, \texttt{runif()}; the Poisson distribution, \texttt{rpois}; the binomial distribution, \texttt{rbinom}; and many others. To randomly sample from a collection of items, we can use \texttt{sample()}.

When dealing with randomness, the need for reproducibility makes it important, paradoxically, that the randomness is repeatable. That is to say, another person needs to be able to draw the random numbers that we draw. We do this by setting a seed for our random draws using \texttt{set.seed()}.

We could get observations from the standard normal distribution and put the those into a data frame.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{number\_of\_observations }\OtherTok{\textless{}{-}} \DecValTok{5}

\NormalTok{simulated\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{person =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{number\_of\_observations),}
    \AttributeTok{std\_normal\_observations =} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_observations,}
                                    \AttributeTok{mean =} \DecValTok{0}\NormalTok{,}
                                    \AttributeTok{sd =} \DecValTok{1}\NormalTok{)}
\NormalTok{    )}

\NormalTok{simulated\_data}
\CommentTok{\#\textgreater{}   person std\_normal\_observations}
\CommentTok{\#\textgreater{} 1      1             {-}0.35980342}
\CommentTok{\#\textgreater{} 2      2             {-}0.04064753}
\CommentTok{\#\textgreater{} 3      3             {-}1.78216227}
\CommentTok{\#\textgreater{} 4      4             {-}1.12242282}
\CommentTok{\#\textgreater{} 5      5             {-}1.00278400}
\end{Highlighting}
\end{Shaded}

We could then add draws from the uniform, Poisson, and binomial distributions, using \texttt{cbind()} to bring the columns of the original dataset and the new one together.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_data }\OtherTok{\textless{}{-}}
  \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{uniform\_observations =} 
      \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_observations, }\AttributeTok{min =} \DecValTok{0}\NormalTok{, }\AttributeTok{max =} \DecValTok{10}\NormalTok{),}
    \AttributeTok{poisson\_observations =} 
      \FunctionTok{rpois}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_observations, }\AttributeTok{lambda =} \DecValTok{100}\NormalTok{),}
    \AttributeTok{binomial\_observations =} 
      \FunctionTok{rbinom}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_observations, }\AttributeTok{size =} \DecValTok{2}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{cbind}\NormalTok{(simulated\_data)}

\NormalTok{simulated\_data}
\CommentTok{\#\textgreater{}   uniform\_observations poisson\_observations}
\CommentTok{\#\textgreater{} 1            9.6219155                   81}
\CommentTok{\#\textgreater{} 2            7.2269016                   91}
\CommentTok{\#\textgreater{} 3            0.8252921                   84}
\CommentTok{\#\textgreater{} 4            1.0379810                  100}
\CommentTok{\#\textgreater{} 5            3.0942004                   97}
\CommentTok{\#\textgreater{}   binomial\_observations person std\_normal\_observations}
\CommentTok{\#\textgreater{} 1                     2      1             {-}0.35980342}
\CommentTok{\#\textgreater{} 2                     1      2             {-}0.04064753}
\CommentTok{\#\textgreater{} 3                     1      3             {-}1.78216227}
\CommentTok{\#\textgreater{} 4                     1      4             {-}1.12242282}
\CommentTok{\#\textgreater{} 5                     1      5             {-}1.00278400}
\end{Highlighting}
\end{Shaded}

Finally, we will add a favorite color to each observation with \texttt{sample()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{favorite\_color =} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{" white "}\NormalTok{), }
                             \AttributeTok{size =}\NormalTok{ number\_of\_observations,}
                             \AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{    ) }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{cbind}\NormalTok{(simulated\_data)}

\NormalTok{simulated\_data}
\CommentTok{\#\textgreater{}   favorite\_color uniform\_observations poisson\_observations}
\CommentTok{\#\textgreater{} 1           blue            9.6219155                   81}
\CommentTok{\#\textgreater{} 2           blue            7.2269016                   91}
\CommentTok{\#\textgreater{} 3           blue            0.8252921                   84}
\CommentTok{\#\textgreater{} 4         white             1.0379810                  100}
\CommentTok{\#\textgreater{} 5           blue            3.0942004                   97}
\CommentTok{\#\textgreater{}   binomial\_observations person std\_normal\_observations}
\CommentTok{\#\textgreater{} 1                     2      1             {-}0.35980342}
\CommentTok{\#\textgreater{} 2                     1      2             {-}0.04064753}
\CommentTok{\#\textgreater{} 3                     1      3             {-}1.78216227}
\CommentTok{\#\textgreater{} 4                     1      4             {-}1.12242282}
\CommentTok{\#\textgreater{} 5                     1      5             {-}1.00278400}
\end{Highlighting}
\end{Shaded}

We set the option `replace' to `TRUE' because we are only choosing between two items, but each time we choose we want the possibility that either are chosen. Depending on the simulation we may need to think about whether `replace' should be `TRUE' or `FALSE'. Another useful optional argument in \texttt{sample()} is to adjust the probability with which each item is drawn. The default is that all options are equally likely, but we could specify particular probabilities if we wanted to with `prob'. As always with functions, we can find more in the help file, for instance \texttt{?sample}.

\hypertarget{function-for-and-apply}{%
\subsection{\texorpdfstring{\texttt{function()}, \texttt{for()}, and \texttt{apply()}}{function(), for(), and apply()}}\label{function-for-and-apply}}

R `is a functional programming language' \citep{advancedr}. This means that we foundationally write, use, and compose functions, which are collections of code that accomplish something specific.

There are a lot of functions in R that other people have written, and we can use. Almost any common statistical or data science task that we might need to accomplish likely already has a function that has been written by someone else and made available to us, either as part of the base R installation or a package. But we will need to write our own functions from time to time, especially for more-specific tasks.
We define a function using \texttt{function()}, and then assign a name. We will likely need to include some inputs and outputs for the function. Inputs are specified between round brackets. The specific task that the function is to accomplish goes between braces.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{print\_names }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(some\_names) \{}
  \FunctionTok{print}\NormalTok{(some\_names)}
\NormalTok{\}}

\FunctionTok{print\_names}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"rohan"}\NormalTok{, }\StringTok{"monica"}\NormalTok{))}
\CommentTok{\#\textgreater{} [1] "rohan"  "monica"}
\end{Highlighting}
\end{Shaded}

We can specify defaults for the inputs in case the person using the function does not supply them.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{print\_names }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{some\_names =} \FunctionTok{c}\NormalTok{(}\StringTok{"edward"}\NormalTok{, }\StringTok{"hugo"}\NormalTok{)) \{}
  \FunctionTok{print}\NormalTok{(some\_names)}
\NormalTok{\}}

\FunctionTok{print\_names}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] "edward" "hugo"}
\end{Highlighting}
\end{Shaded}

One common scenario is that we want to apply a function multiple times. Like many programming languages, we can use a \texttt{for()} loop for this. The look of a \texttt{for()} loop in R is similar to \texttt{function()}, in that we define what we are iterating over in the round brackets, and the function to apply in braces.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{) \{}
  \FunctionTok{print}\NormalTok{(i)}
\NormalTok{\}}
\CommentTok{\#\textgreater{} [1] 1}
\CommentTok{\#\textgreater{} [1] 2}
\CommentTok{\#\textgreater{} [1] 3}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}\AttributeTok{x1 =} \DecValTok{66}\NormalTok{, }\AttributeTok{x2 =} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\SpecialCharTok{:}\DecValTok{1}\NormalTok{, }\DecValTok{2}\SpecialCharTok{:}\DecValTok{5}\NormalTok{))}
\FunctionTok{dimnames}\NormalTok{(x)[[}\DecValTok{1}\NormalTok{]] }\OtherTok{\textless{}{-}}\NormalTok{ letters[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{]}
\FunctionTok{class}\NormalTok{(x)}
\CommentTok{\#\textgreater{} [1] "matrix" "array"}
\FunctionTok{apply}\NormalTok{(x, }\DecValTok{2}\NormalTok{, mean, }\AttributeTok{trim =}\NormalTok{ .}\DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} x1 x2 }
\CommentTok{\#\textgreater{} 66  3}
\end{Highlighting}
\end{Shaded}

Because R is a programming language that is focused on statistics, we are often interested in arrays or matrices. We us \texttt{apply()} to apply a function to rows (`MARGIN = 1') or columns (`MARGIN = 2').

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_data}
\CommentTok{\#\textgreater{}   favorite\_color uniform\_observations poisson\_observations}
\CommentTok{\#\textgreater{} 1           blue            9.6219155                   81}
\CommentTok{\#\textgreater{} 2           blue            7.2269016                   91}
\CommentTok{\#\textgreater{} 3           blue            0.8252921                   84}
\CommentTok{\#\textgreater{} 4         white             1.0379810                  100}
\CommentTok{\#\textgreater{} 5           blue            3.0942004                   97}
\CommentTok{\#\textgreater{}   binomial\_observations person std\_normal\_observations}
\CommentTok{\#\textgreater{} 1                     2      1             {-}0.35980342}
\CommentTok{\#\textgreater{} 2                     1      2             {-}0.04064753}
\CommentTok{\#\textgreater{} 3                     1      3             {-}1.78216227}
\CommentTok{\#\textgreater{} 4                     1      4             {-}1.12242282}
\CommentTok{\#\textgreater{} 5                     1      5             {-}1.00278400}
\FunctionTok{apply}\NormalTok{(}\AttributeTok{X =}\NormalTok{ simulated\_data, }\AttributeTok{MARGIN =} \DecValTok{2}\NormalTok{, }\AttributeTok{FUN =}\NormalTok{ unique)}
\CommentTok{\#\textgreater{} $favorite\_color}
\CommentTok{\#\textgreater{} [1] "blue"    " white "}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $uniform\_observations}
\CommentTok{\#\textgreater{} [1] "9.6219155" "7.2269016" "0.8252921" "1.0379810"}
\CommentTok{\#\textgreater{} [5] "3.0942004"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $poisson\_observations}
\CommentTok{\#\textgreater{} [1] " 81" " 91" " 84" "100" " 97"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $binomial\_observations}
\CommentTok{\#\textgreater{} [1] "2" "1"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $person}
\CommentTok{\#\textgreater{} [1] "1" "2" "3" "4" "5"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $std\_normal\_observations}
\CommentTok{\#\textgreater{} [1] "{-}0.35980342" "{-}0.04064753" "{-}1.78216227" "{-}1.12242282"}
\CommentTok{\#\textgreater{} [5] "{-}1.00278400"}
\end{Highlighting}
\end{Shaded}

\hypertarget{making-graphs-with-ggplot2}{%
\section{\texorpdfstring{Making graphs with \texttt{ggplot2}}{Making graphs with ggplot2}}\label{making-graphs-with-ggplot2}}

If the key package in the \texttt{tidyverse} in terms of manipulating data is \texttt{dplyr} \citep{citedplyr}, then the key package in the \texttt{tidyverse} in terms of creating graphs is \texttt{ggplot2} \citep{citeggplot}. It is part of the \texttt{tidyverse} collection of packages and so does not need to be explicitly installed or loaded if the \texttt{tidyverse} has been loaded.

More formally, \texttt{ggplot2} works by defining layers which build to form a graph, based around the `grammar of graphics' (hence, the `gg'). Instead of the pipe operator (\texttt{\textbar{}\textgreater{}}) ggplot uses the add operator \texttt{+}.

There are three key aspects that need to be specified to build a graph with \texttt{ggplot2}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  data;
\item
  aesthetics / mapping; and
\item
  type.
\end{enumerate}

To get started we will obtain some GDP data for OECD countries \citep{citeoecdgdp}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{oecd\_gdp }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"https://stats.oecd.org/sdmx{-}json/data/DP\_LIVE/.QGDP.../OECD?contentType=csv\&detail=code\&separator=comma\&csv{-}lang=en"}\NormalTok{)}

\FunctionTok{write\_csv}\NormalTok{(oecd\_gdp, }\StringTok{\textquotesingle{}inputs/data/oecd\_gdp.csv\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 6 x 8
#>   LOCATION INDICATOR SUBJECT MEASURE  FREQUENCY TIME  Value
#>   <chr>    <chr>     <chr>   <chr>    <chr>     <chr> <dbl>
#> 1 OECD     QGDP      TOT     PC_CHGPP A         1962   5.70
#> 2 OECD     QGDP      TOT     PC_CHGPP A         1963   5.20
#> 3 OECD     QGDP      TOT     PC_CHGPP A         1964   6.38
#> 4 OECD     QGDP      TOT     PC_CHGPP A         1965   5.35
#> 5 OECD     QGDP      TOT     PC_CHGPP A         1966   5.75
#> 6 OECD     QGDP      TOT     PC_CHGPP A         1967   3.96
#> # ... with 1 more variable: Flag Codes <chr>
\end{verbatim}

We are interested, firstly, in making a bar chart of GDP change in the third quarter of 2021 for ten countries: Australia, Canada, Chile, Indonesia, Germany, Great Britain, New Zealand, South Africa, Spain, and the US.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{oecd\_gdp\_most\_recent }\OtherTok{\textless{}{-}} 
\NormalTok{  oecd\_gdp }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(TIME }\SpecialCharTok{==} \StringTok{"2021{-}Q3"}\NormalTok{,}
\NormalTok{         SUBJECT }\SpecialCharTok{==} \StringTok{"TOT"}\NormalTok{,}
\NormalTok{         LOCATION }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"AUS"}\NormalTok{, }\StringTok{"CAN"}\NormalTok{, }\StringTok{"CHL"}\NormalTok{, }\StringTok{"DEU"}\NormalTok{, }\StringTok{"GBR"}\NormalTok{,}
                         \StringTok{"IDN"}\NormalTok{, }\StringTok{"ESP"}\NormalTok{, }\StringTok{"NZL"}\NormalTok{, }\StringTok{"USA"}\NormalTok{, }\StringTok{"ZAF"}\NormalTok{),}
\NormalTok{         MEASURE }\SpecialCharTok{==} \StringTok{"PC\_CHGPY"}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{european =} \FunctionTok{if\_else}\NormalTok{(LOCATION }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"DEU"}\NormalTok{, }\StringTok{"GBR"}\NormalTok{, }\StringTok{"ESP"}\NormalTok{),}
                             \StringTok{"European"}\NormalTok{,}
                             \StringTok{"Not european"}\NormalTok{),}
         \AttributeTok{hemisphere =} \FunctionTok{if\_else}\NormalTok{(LOCATION }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"CAN"}\NormalTok{, }\StringTok{"DEU"}\NormalTok{, }\StringTok{"GBR"}\NormalTok{, }\StringTok{"ESP"}\NormalTok{, }\StringTok{"USA"}\NormalTok{),}
                             \StringTok{"Northern Hemisphere"}\NormalTok{,}
                             \StringTok{"Southern Hemisphere"}\NormalTok{),}
\NormalTok{         )}
\end{Highlighting}
\end{Shaded}

We start with \texttt{ggplot} and specify a mapping/aesthetic, which in this case means specifying the x-axis and the y-axis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{oecd\_gdp\_most\_recent }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ LOCATION, }\AttributeTok{y =}\NormalTok{ Value))}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-r_essentials_files/figure-latex/unnamed-chunk-68-1.pdf}

Now we need to specify the type of graph that we are interested in. In this case we want a bar chart and we do this by adding \texttt{geom\_bar()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{oecd\_gdp\_most\_recent }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ LOCATION, }\AttributeTok{y =}\NormalTok{ Value)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat=}\StringTok{"identity"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-r_essentials_files/figure-latex/unnamed-chunk-69-1.pdf}

We can color the bars by whether the country is European by adding another aesthetic, `fill'.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{oecd\_gdp\_most\_recent }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ LOCATION, }\AttributeTok{y =}\NormalTok{ Value, }\AttributeTok{fill =}\NormalTok{ european)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat=}\StringTok{"identity"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-r_essentials_files/figure-latex/unnamed-chunk-70-1.pdf}

Finally, we could make it look nicer by: adding labels, \texttt{labs()}; changing the color, \texttt{scale\_fill\_brewer()}; and the background, \texttt{theme\_classic()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{oecd\_gdp\_most\_recent }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ LOCATION, }\AttributeTok{y =}\NormalTok{ Value, }\AttributeTok{fill =}\NormalTok{ european)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat=}\StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Quarterly change in GDP for ten OECD countries in 2021Q3"}\NormalTok{, }
       \AttributeTok{x =} \StringTok{"Countries"}\NormalTok{, }
       \AttributeTok{y =} \StringTok{"Change (\%)"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Is European?"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-r_essentials_files/figure-latex/unnamed-chunk-71-1.pdf}

Facets enable us to that we create subplots that focus on specific aspects of our data. They are invaluable because they allow us to add another variable to a graph without having to make a 3D graph. We use \texttt{facet\_wrap()} to add a facet and specify the variable that we would like to facet by. In this case, we facet by hemisphere.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{oecd\_gdp\_most\_recent }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ LOCATION, }\AttributeTok{y =}\NormalTok{ Value, }\AttributeTok{fill =}\NormalTok{ european)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat=}\StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Quarterly change in GDP for ten OECD countries in 2021Q3"}\NormalTok{, }
       \AttributeTok{x =} \StringTok{"Countries"}\NormalTok{, }
       \AttributeTok{y =} \StringTok{"Change (\%)"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Is European?"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{hemisphere, }
              \AttributeTok{scales =} \StringTok{"free\_x"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-r_essentials_files/figure-latex/unnamed-chunk-72-1.pdf}

\hypertarget{exploring-the-tidyverse}{%
\section{\texorpdfstring{Exploring the \texttt{tidyverse}}{Exploring the tidyverse}}\label{exploring-the-tidyverse}}

We have focused on two aspects of the \texttt{tidyverse}: \texttt{dplyr}, and \texttt{ggplot2}. However, the \texttt{tidyverse} comprises a variety of different packages and functions. We will now go through four common aspects:

\begin{itemize}
\tightlist
\item
  Importing data and \texttt{tibble()}
\item
  Joining and pivoting datasets
\item
  String manipulation and \texttt{stringr}
\item
  Factor variables and \texttt{forcats}
\end{itemize}

\hypertarget{importing-data-and-tibble}{%
\subsection{\texorpdfstring{Importing data and \texttt{tibble()}}{Importing data and tibble()}}\label{importing-data-and-tibble}}

There are a variety of ways to get data into R so that we can use it. For CSV files, there is \texttt{read\_csv()} from \texttt{readr} \citep{citereadr}, and for dta files, there is \texttt{read\_dta()} from \texttt{haven} \citep{citehaven}.

CSVs are a common format and have many advantages including the fact that they typically do not modify the data. Each column is separated by a comma, and each row is a record. We can provide \texttt{read\_csv()} with a URL or a local file to read. There are a variety of different options that can be passed to \texttt{read\_csv()} including the ability to specify whether the dataset has column names, the types of the columns, and how many lines to skip. If we do not specify the types of the columns then \texttt{read\_csv()} will make a guess by looking at the dataset.

We use \texttt{read\_dta()} to read .dta files, which are commonly produced by the statistical program Stata. This means that they are common in fields such as sociology, political science, and economics. This format separates the data from its labels and so we typically reunite these using \texttt{to\_factor()} from \texttt{labelled} \citep{citelabelled}. \texttt{haven} is part of the \texttt{tidyverse}, but is not automatically loaded by default, in contrast to a package such as \texttt{ggplot2}, and so we would need to run \texttt{library(haven)}.

Typically a dataset enters R as a `data.frame'. While this can be useful, another helpful class for a dataset is `tibble'. These can be created using \texttt{tibble()} from the \texttt{tibble} package which is part of the \texttt{tidyverse}. A tibble is a data frame, with some particular changes that make it easier to work with, including not converting strings to factors by default, showing the class of columns, and printing nicely.

We can make a tibble manually, if need be, for instance, when we simulate data. But we typically import data directly as a tibble, for instance, when we use \texttt{read\_csv()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{people\_as\_dataframe }\OtherTok{\textless{}{-}} 
  \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{names =} \FunctionTok{c}\NormalTok{(}\StringTok{"rohan"}\NormalTok{, }\StringTok{"monica"}\NormalTok{),}
             \AttributeTok{website =} \FunctionTok{c}\NormalTok{(}\StringTok{"rohanalexander.com"}\NormalTok{, }\StringTok{"monicaalexander.com"}\NormalTok{),}
             \AttributeTok{fav\_color =} \FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{" white "}\NormalTok{)}
\NormalTok{             )}
\FunctionTok{class}\NormalTok{(people\_as\_dataframe)}
\CommentTok{\#\textgreater{} [1] "data.frame"}
\NormalTok{people\_as\_dataframe}
\CommentTok{\#\textgreater{}    names             website fav\_color}
\CommentTok{\#\textgreater{} 1  rohan  rohanalexander.com      blue}
\CommentTok{\#\textgreater{} 2 monica monicaalexander.com    white}

\NormalTok{people\_as\_tibble }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{names =} \FunctionTok{c}\NormalTok{(}\StringTok{"rohan"}\NormalTok{, }\StringTok{"monica"}\NormalTok{),}
         \AttributeTok{website =} \FunctionTok{c}\NormalTok{(}\StringTok{"rohanalexander.com"}\NormalTok{, }\StringTok{"monicaalexander.com"}\NormalTok{),}
         \AttributeTok{fav\_color =} \FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{" white "}\NormalTok{)}
\NormalTok{         )}
\NormalTok{people\_as\_tibble}
\CommentTok{\#\textgreater{} \# A tibble: 2 x 3}
\CommentTok{\#\textgreater{}   names  website             fav\_color}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}  \textless{}chr\textgreater{}               \textless{}chr\textgreater{}    }
\CommentTok{\#\textgreater{} 1 rohan  rohanalexander.com  "blue"   }
\CommentTok{\#\textgreater{} 2 monica monicaalexander.com " white "}
\FunctionTok{class}\NormalTok{(people\_as\_tibble)}
\CommentTok{\#\textgreater{} [1] "tbl\_df"     "tbl"        "data.frame"}
\end{Highlighting}
\end{Shaded}

\hypertarget{dataset-manipulation-with-joins-and-pivots}{%
\subsection{Dataset manipulation with joins and pivots}\label{dataset-manipulation-with-joins-and-pivots}}

There are two dataset manipulations that are often needed: joins and pivots.

We often have a situation where we have two, or more, datasets and we are interested in combining them. We can join datasets together in a variety of ways. A common way is to use \texttt{left\_join()} from \texttt{dplyr} \citep{citedplyr}. This is most useful where there is one main dataset that we are using and there is another dataset with some useful variables that we want to add to that. The critical aspect is that we have column/s that we can use to link the two datasets. Here we will create two tibbles and then join them on the basis of names.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{main\_dataset }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{names =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}rohan\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}monica\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}edward\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}hugo\textquotesingle{}}\NormalTok{),}
    \AttributeTok{status =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}adult\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}adult\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}child\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}infant\textquotesingle{}}\NormalTok{)}
\NormalTok{  )}
\NormalTok{main\_dataset}
\CommentTok{\#\textgreater{} \# A tibble: 4 x 2}
\CommentTok{\#\textgreater{}   names  status}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}  \textless{}chr\textgreater{} }
\CommentTok{\#\textgreater{} 1 rohan  adult }
\CommentTok{\#\textgreater{} 2 monica adult }
\CommentTok{\#\textgreater{} 3 edward child }
\CommentTok{\#\textgreater{} 4 hugo   infant}

\NormalTok{supplementary\_dataset }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{names =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}rohan\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}monica\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}edward\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}hugo\textquotesingle{}}\NormalTok{),}
    \AttributeTok{favorite\_food =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}pasta\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}salmon\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}pizza\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}milk\textquotesingle{}}\NormalTok{)}
\NormalTok{  )}
\NormalTok{supplementary\_dataset}
\CommentTok{\#\textgreater{} \# A tibble: 4 x 2}
\CommentTok{\#\textgreater{}   names  favorite\_food}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}  \textless{}chr\textgreater{}        }
\CommentTok{\#\textgreater{} 1 rohan  pasta        }
\CommentTok{\#\textgreater{} 2 monica salmon       }
\CommentTok{\#\textgreater{} 3 edward pizza        }
\CommentTok{\#\textgreater{} 4 hugo   milk}

\NormalTok{main\_dataset }\OtherTok{\textless{}{-}} 
\NormalTok{  main\_dataset }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{left\_join}\NormalTok{(supplementary\_dataset, }\AttributeTok{by =} \StringTok{"names"}\NormalTok{)}

\NormalTok{main\_dataset}
\CommentTok{\#\textgreater{} \# A tibble: 4 x 3}
\CommentTok{\#\textgreater{}   names  status favorite\_food}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}  \textless{}chr\textgreater{}  \textless{}chr\textgreater{}        }
\CommentTok{\#\textgreater{} 1 rohan  adult  pasta        }
\CommentTok{\#\textgreater{} 2 monica adult  salmon       }
\CommentTok{\#\textgreater{} 3 edward child  pizza        }
\CommentTok{\#\textgreater{} 4 hugo   infant milk}
\end{Highlighting}
\end{Shaded}

There are a variety of other options to join datasets, including \texttt{inner\_join()}, \texttt{right\_join()}, and \texttt{full\_join()}.

Another common dataset manipulation task is pivoting them. Datasets tend to be either long or wide. Generally, in the \texttt{tidyverse}, and certainly for \texttt{ggplot2}, we need long data. To go from one to the other we use \texttt{pivot\_longer()} and \texttt{pivot\_wider()} from \texttt{tidyr} \citep{citetidyr}.

We will create some wide data on whether `mark' or `lauren' won a running race in each of three years.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pivot\_example\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{year =} \FunctionTok{c}\NormalTok{(}\DecValTok{2019}\NormalTok{, }\DecValTok{2020}\NormalTok{, }\DecValTok{2021}\NormalTok{),}
         \AttributeTok{mark =} \FunctionTok{c}\NormalTok{(}\StringTok{"first"}\NormalTok{, }\StringTok{"second"}\NormalTok{, }\StringTok{"first"}\NormalTok{),}
         \AttributeTok{lauren =} \FunctionTok{c}\NormalTok{(}\StringTok{"second"}\NormalTok{, }\StringTok{"first"}\NormalTok{, }\StringTok{"second"}\NormalTok{))}

\NormalTok{pivot\_example\_data}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 3}
\CommentTok{\#\textgreater{}    year mark   lauren}
\CommentTok{\#\textgreater{}   \textless{}dbl\textgreater{} \textless{}chr\textgreater{}  \textless{}chr\textgreater{} }
\CommentTok{\#\textgreater{} 1  2019 first  second}
\CommentTok{\#\textgreater{} 2  2020 second first }
\CommentTok{\#\textgreater{} 3  2021 first  second}
\end{Highlighting}
\end{Shaded}

This dataset is in wide format at the moment. To get it into long format, we need a column that specifies the person, and another that specifies the result. We use \texttt{pivot\_longer()} to achieve this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_pivoted\_longer }\OtherTok{\textless{}{-}} 
\NormalTok{  pivot\_example\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
\NormalTok{  tidyr}\SpecialCharTok{::}\FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(}\StringTok{"mark"}\NormalTok{, }\StringTok{"lauren"}\NormalTok{),}
              \AttributeTok{names\_to =} \StringTok{"person"}\NormalTok{,}
               \AttributeTok{values\_to =} \StringTok{"position"}\NormalTok{)}

\FunctionTok{head}\NormalTok{(data\_pivoted\_longer)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{}    year person position}
\CommentTok{\#\textgreater{}   \textless{}dbl\textgreater{} \textless{}chr\textgreater{}  \textless{}chr\textgreater{}   }
\CommentTok{\#\textgreater{} 1  2019 mark   first   }
\CommentTok{\#\textgreater{} 2  2019 lauren second  }
\CommentTok{\#\textgreater{} 3  2020 mark   second  }
\CommentTok{\#\textgreater{} 4  2020 lauren first   }
\CommentTok{\#\textgreater{} 5  2021 mark   first   }
\CommentTok{\#\textgreater{} 6  2021 lauren second}
\end{Highlighting}
\end{Shaded}

Occasionally, we need to go from long data to wide data. We use \texttt{pivot\_wider()} to do this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_pivoted\_wider }\OtherTok{\textless{}{-}} 
\NormalTok{  data\_pivoted\_longer }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
\NormalTok{  tidyr}\SpecialCharTok{::}\FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{id\_cols =} \FunctionTok{c}\NormalTok{(}\StringTok{"year"}\NormalTok{, }\StringTok{"person"}\NormalTok{),}
                     \AttributeTok{names\_from =} \StringTok{"person"}\NormalTok{,}
                     \AttributeTok{values\_from =} \StringTok{"position"}\NormalTok{)}

\FunctionTok{head}\NormalTok{(data\_pivoted\_wider)}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 3}
\CommentTok{\#\textgreater{}    year mark   lauren}
\CommentTok{\#\textgreater{}   \textless{}dbl\textgreater{} \textless{}chr\textgreater{}  \textless{}chr\textgreater{} }
\CommentTok{\#\textgreater{} 1  2019 first  second}
\CommentTok{\#\textgreater{} 2  2020 second first }
\CommentTok{\#\textgreater{} 3  2021 first  second}
\end{Highlighting}
\end{Shaded}

\hypertarget{string-manipulation-and-stringr}{%
\subsection{\texorpdfstring{String manipulation and \texttt{stringr}}{String manipulation and stringr}}\label{string-manipulation-and-stringr}}

In R we often create a string with double quotes, although using single quotes works too. For instance \texttt{c("a",\ "b")} consists of two strings `a' and `b', that are contained in a character vector. There are a variety of ways to manipulate strings in R and we focus on \texttt{stringr} \citep{citestringr}. This is automatically loaded when we load the \texttt{tidyverse}.

If we want to look for whether a string contains certain content, then we can use \texttt{str\_detect()}. And if we want to remove or change some particular content then we can use \texttt{str\_remove()} or \texttt{str\_replace()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset\_of\_strings }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{names =} \FunctionTok{c}\NormalTok{(}\StringTok{"rohan alexander"}\NormalTok{, }
              \StringTok{"monica alexander"}\NormalTok{, }
              \StringTok{"edward alexander"}\NormalTok{, }
              \StringTok{"hugo alexander"}\NormalTok{)}
\NormalTok{  )}

\NormalTok{dataset\_of\_strings }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{is\_rohan =} \FunctionTok{str\_detect}\NormalTok{(names, }\StringTok{"rohan"}\NormalTok{),}
         \AttributeTok{make\_howlett =} \FunctionTok{str\_replace}\NormalTok{(names, }\StringTok{"alexander"}\NormalTok{, }\StringTok{"howlett"}\NormalTok{),}
         \AttributeTok{remove\_rohan =} \FunctionTok{str\_remove}\NormalTok{(names, }\StringTok{"rohan"}\NormalTok{)}
\NormalTok{         )}
\CommentTok{\#\textgreater{} \# A tibble: 4 x 4}
\CommentTok{\#\textgreater{}   names            is\_rohan make\_howlett   remove\_rohan     }
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}            \textless{}lgl\textgreater{}    \textless{}chr\textgreater{}          \textless{}chr\textgreater{}            }
\CommentTok{\#\textgreater{} 1 rohan alexander  TRUE     rohan howlett  " alexander"     }
\CommentTok{\#\textgreater{} 2 monica alexander FALSE    monica howlett "monica alexande\textasciitilde{}}
\CommentTok{\#\textgreater{} 3 edward alexander FALSE    edward howlett "edward alexande\textasciitilde{}}
\CommentTok{\#\textgreater{} 4 hugo alexander   FALSE    hugo howlett   "hugo alexander"}
\end{Highlighting}
\end{Shaded}

There are a variety of other functions that are often especially useful in data cleaning. For instance, we can use \texttt{str\_length()} to find out how long a string is, and \texttt{str\_c()} to bring strings together.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset\_of\_strings }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{length\_is =} \FunctionTok{str\_length}\NormalTok{(}\AttributeTok{string =}\NormalTok{ names),}
         \AttributeTok{name\_and\_length =} \FunctionTok{str\_c}\NormalTok{(names, length\_is, }\AttributeTok{sep =} \StringTok{" {-} "}\NormalTok{)}
\NormalTok{         )}
\CommentTok{\#\textgreater{} \# A tibble: 4 x 3}
\CommentTok{\#\textgreater{}   names            length\_is name\_and\_length      }
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}                \textless{}int\textgreater{} \textless{}chr\textgreater{}                }
\CommentTok{\#\textgreater{} 1 rohan alexander         15 rohan alexander {-} 15 }
\CommentTok{\#\textgreater{} 2 monica alexander        16 monica alexander {-} 16}
\CommentTok{\#\textgreater{} 3 edward alexander        16 edward alexander {-} 16}
\CommentTok{\#\textgreater{} 4 hugo alexander          14 hugo alexander {-} 14}
\end{Highlighting}
\end{Shaded}

Finally, \texttt{separate()} from \texttt{tidyr}, although not part of \texttt{stringr}, is indispensable for string manipulation. It turns one character column into many.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset\_of\_strings }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{separate}\NormalTok{(}\AttributeTok{col =}\NormalTok{ names,}
           \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"first"}\NormalTok{, }\StringTok{"last"}\NormalTok{),}
           \AttributeTok{sep =} \StringTok{" "}\NormalTok{,}
           \AttributeTok{remove =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 4 x 3}
\CommentTok{\#\textgreater{}   names            first  last     }
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}            \textless{}chr\textgreater{}  \textless{}chr\textgreater{}    }
\CommentTok{\#\textgreater{} 1 rohan alexander  rohan  alexander}
\CommentTok{\#\textgreater{} 2 monica alexander monica alexander}
\CommentTok{\#\textgreater{} 3 edward alexander edward alexander}
\CommentTok{\#\textgreater{} 4 hugo alexander   hugo   alexander}
\end{Highlighting}
\end{Shaded}

\hypertarget{factor-variables-and-forcats}{%
\subsection{\texorpdfstring{Factor variables and \texttt{forcats}}{Factor variables and forcats}}\label{factor-variables-and-forcats}}

A factor is a collection of strings that are categories. Sometimes there will be an inherent ordering. For instance, the days of the week have an order -- Monday, Tuesday, Wednesday, \ldots{} -- which is not alphabetical. But there is no requirement for that to be the case, for instance gender: female, male, and other; or pregnancy status: pregnant or not pregnant. Factors feature prominently in base R. They can be useful because they ensure that only appropriate strings are allowed. For instance, if `days\_of\_the\_week' was a factor variable then `January' would not be allowed. But they can add a great deal of complication, and so they have a less prominent role in \texttt{tidyverse}. Nonetheless taking advantage of factors is useful in certain circumstances. For instance, when plotting the days of the week we probably want them in the usual ordering than in the alphabetical ordering that would result if we had them as a character variable. While factors are built into base R, one \texttt{tidyverse} package that is especially useful when using factors is \texttt{forcats} \citep{citeforcats}.

Sometimes we have a character vector, and we will want it ordered in a particular way. The default is that a character vector is ordered alphabetically, but we may not want that. For instance, the days of the week would look strange on a graph if they were alphabetically ordered: Friday, Monday, Saturday, Sunday, Thursday, Tuesday, and Wednesday!

The way to change the ordering is to change the variable from a character to a factor. We can use \texttt{fct\_relevel()} from \texttt{forcats} \citep{citeforcats} to specify an ordering.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{days\_data }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{days =}
      \FunctionTok{c}\NormalTok{(}
        \StringTok{"Monday"}\NormalTok{,}
        \StringTok{"Tuesday"}\NormalTok{,}
        \StringTok{"Wednesday"}\NormalTok{,}
        \StringTok{"Thursday"}\NormalTok{,}
        \StringTok{"Friday"}\NormalTok{,}
        \StringTok{"Saturday"}\NormalTok{,}
        \StringTok{"Sunday"}
\NormalTok{      ),}
    \AttributeTok{some\_value =} \FunctionTok{c}\NormalTok{(}\FunctionTok{sample.int}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{7}\NormalTok{))}
\NormalTok{  )}

\NormalTok{days\_data }\OtherTok{\textless{}{-}}
\NormalTok{  days\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{days\_as\_factor =} \FunctionTok{factor}\NormalTok{(days),}
    \AttributeTok{days\_as\_factor =} \FunctionTok{fct\_relevel}\NormalTok{(}
\NormalTok{      days,}
      \StringTok{"Monday"}\NormalTok{,}
      \StringTok{"Tuesday"}\NormalTok{,}
      \StringTok{"Wednesday"}\NormalTok{,}
      \StringTok{"Thursday"}\NormalTok{,}
      \StringTok{"Friday"}\NormalTok{,}
      \StringTok{"Saturday"}\NormalTok{,}
      \StringTok{"Sunday"}
\NormalTok{    )}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

And we can compare the results by graphing first with the original character vector on the x-axis, and then another graph with the factor vector on the x-axis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{days\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ days, }\AttributeTok{y =}\NormalTok{ some\_value)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}

\NormalTok{days\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ days\_as\_factor, }\AttributeTok{y =}\NormalTok{ some\_value)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.5\linewidth]{03-r_essentials_files/figure-latex/unnamed-chunk-82-1} \includegraphics[width=0.5\linewidth]{03-r_essentials_files/figure-latex/unnamed-chunk-82-2}

\hypertarget{exercises-and-tutorial-2}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-2}}

\hypertarget{exercises-2}{%
\subsection{Exercises}\label{exercises-2}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is R?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A open-source statistical programming language
  \item
    A programming language created by Guido van Rossum
  \item
    A closed source statistical programming language
  \item
    An integrated development environment (IDE)
  \end{enumerate}
\item
  What are three advantages of R? What are three disadvantages?
\item
  What is R Studio?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    An integrated development environment (IDE).
  \item
    A closed source paid program.
  \item
    A programming language created by Guido van Rossum
  \item
    A statistical programming language.
  \end{enumerate}
\item
  What is the class of the output of `2 + 2' (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    character
  \item
    factor
  \item
    numeric
  \item
    date
  \end{enumerate}
\item
  Say we had run: \texttt{my\_name\ \textless{}-\ \textquotesingle{}Rohan\textquotesingle{}}. What would be the result of running \texttt{print(my\_name)} (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    `Edward'
  \item
    `Monica'
  \item
    `Hugo'
  \item
    `Rohan'
  \end{enumerate}
\item
  Say we had a dataset with two columns: `name', and `age'. Which verb should we use to pick just `name' (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{tidyverse::select()}
  \item
    \texttt{tidyverse::mutate()}
  \item
    \texttt{tidyverse::filter()}
  \item
    \texttt{tidyverse::rename()}
  \end{enumerate}
\item
  Say we had loaded \texttt{AustralianPoliticians} and \texttt{tidyverse} and then run the following code: \texttt{australian\_politicians\ \textless{}-\ AustralianPoliticians::get\_auspol(\textquotesingle{}all\textquotesingle{})}. How could we select all of the columns that end with `Name' (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{australian\_politicians\ \textbar{}\textgreater{}\ select(contains("Name"))}
  \item
    \texttt{australian\_politicians\ \textbar{}\textgreater{}\ select(starts\_with("Name"))}
  \item
    \texttt{australian\_politicians\ \textbar{}\textgreater{}\ select(matches("Name"))}
  \item
    \texttt{australian\_politicians\ \textbar{}\textgreater{}\ select(ends\_with("Name"))}
  \end{enumerate}
\item
  Under what circumstances, in terms of the names of the columns, would the use of `contains()' potentially give different answers to using `ends\_with()' in the above question?
\item
  Which of the following are not \texttt{tidyverse} verbs (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{select()}
  \item
    \texttt{filter()}
  \item
    \texttt{arrange()}
  \item
    \texttt{mutate()}
  \item
    \texttt{visualize()}
  \end{enumerate}
\item
  Which function would make a new column (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{select()}
  \item
    \texttt{filter()}
  \item
    \texttt{arrange()}
  \item
    \texttt{mutate()}
  \item
    \texttt{visualize()}
  \end{enumerate}
\item
  Which function would focus on particular rows (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{select()}
  \item
    \texttt{filter()}
  \item
    \texttt{arrange()}
  \item
    \texttt{mutate()}
  \item
    \texttt{summarise()}
  \end{enumerate}
\item
  Which combination of two functions could provide a mean of a dataset, by sex (pick two)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{summarise()}
  \item
    \texttt{filter()}
  \item
    \texttt{arrange()}
  \item
    \texttt{mutate()}
  \item
    \texttt{group\_by()}
  \end{enumerate}
\item
  Assume a variable called `age' is an integer. Which line of code would create a column that is its exponential (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{mutate(exp\_age\ =\ exponential(age))}
  \item
    \texttt{mutate(exp\_age\ =\ exponent(age))}
  \item
    \texttt{mutate(exp\_age\ =\ exp(age))}
  \item
    \texttt{mutate(exp\_age\ =\ expon(age))}
  \end{enumerate}
\item
  Assume a column called `age'. Which line of code could create a column that contains the value from five rows above?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{mutate(five\_before\ =\ lag(age))}
  \item
    \texttt{mutate(five\_before\ =\ lead(age))}
  \item
    \texttt{mutate(five\_before\ =\ lag(age,\ n\ =\ 5))}
  \item
    \texttt{mutate(five\_before\ =\ lead(age,\ n\ =\ 5))}
  \end{enumerate}
\item
  What would be the output of \texttt{class(\textquotesingle{}edward\textquotesingle{})} (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    `numeric'
  \item
    `character'
  \item
    `data.frame'
  \item
    `vector'
  \end{enumerate}
\item
  Which function would enable us to draw once from three options `blue, white, red', with 10 per cent probability on `blue' and `white', and the remainder on `red'?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{sample(c(\textquotesingle{}blue\textquotesingle{},\ \textquotesingle{}white\textquotesingle{},\ \textquotesingle{}red\textquotesingle{}),\ prob\ =\ c(0.1,\ 0.1,\ 0.8))}
  \item
    \texttt{sample(c(\textquotesingle{}blue\textquotesingle{},\ \textquotesingle{}white\textquotesingle{},\ \textquotesingle{}red\textquotesingle{}),\ size\ =\ 1)}
  \item
    \texttt{sample(c(\textquotesingle{}blue\textquotesingle{},\ \textquotesingle{}white\textquotesingle{},\ \textquotesingle{}red\textquotesingle{}),\ size\ =\ 1,\ prob\ =\ c(0.8,\ 0.1,\ 0.1))}
  \item
    \texttt{sample(c(\textquotesingle{}blue\textquotesingle{},\ \textquotesingle{}white\textquotesingle{},\ \textquotesingle{}red\textquotesingle{}),\ size\ =\ 1,\ prob\ =\ c(0.1,\ 0.1,\ 0.8))}
  \end{enumerate}
\item
  Which code simulates 10,000 draws from a normal distribution with a mean of 27 and a standard deviation of 3 (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{rnorm(10000,\ mean\ =\ 27,\ sd\ =\ 3)}
  \item
    \texttt{rnorm(27,\ mean\ =\ 10000,\ sd\ =\ 3)}
  \item
    \texttt{rnorm(3,\ mean\ =\ 10000,\ sd\ =\ 27)}
  \item
    \texttt{rnorm(27,\ mean\ =\ 3,\ sd\ =\ 1000)}
  \end{enumerate}
\item
  What are the three key aspects of the grammar of graphics (select all)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    data
  \item
    aesthetics
  \item
    type
  \item
    \texttt{geom\_histogram()}
  \end{enumerate}
\end{enumerate}

\hypertarget{tutorial-2}{%
\subsection{Tutorial}\label{tutorial-2}}

\begin{quote}
I think we should be suspicious when we find ourselves attracted to data---very, very thin and weak data---that seem to justify beliefs that have held great currency in lots of societies throughout history, in a way that is conducive to the oppression of large segments of the population

Amia Srinivasan, 22 September 2021
\end{quote}

Reflect on the quote from Amia Srinivasan, Chichele Professor of Social and Political Theory, All Souls College, Oxford, and \citet{datafeminism2020}, especially Chapter 6, and in two pages discuss a dataset that you are familiar with.

\hypertarget{reproducible-workflows}{%
\chapter{Reproducible workflows}\label{reproducible-workflows}}

\textbf{Required material}

\begin{itemize}
\tightlist
\item
  Read \emph{What has happened down here is the winds have changed}, \citep{Gelman2016}
\item
  Read \emph{Good enough practices in scientific computing}, \citep{wilsongoodenough}
\item
  Watch \emph{Overcoming barriers to sharing code}, \citep{monicatalks}
\item
  Watch \emph{Make a reprex\ldots{} Please}, \citep{sharlatalks}
\item
  Read \emph{The tidyverse style guide}, `Part: Analyses', \citep{tidyversestyleguide}
\end{itemize}

\textbf{Key concepts and skills}

\begin{itemize}
\tightlist
\item
  Reproducibility is a requirement, and this implies sharing data, code, and environment.
\item
  Reproducibility is enhanced by using R Markdown, R Projects, and Git and GitHub.
\item
  R Markdown involves marking text as certain types and then building the document.
\item
  R Projects enable a file structure that is not dependent on a specific directory set-up.
\item
  Git and GitHub make it easier to share code and data.

  \begin{itemize}
  \tightlist
  \item
    Get the latest changes: \texttt{git\ pull}.
  \item
    Add your updates: \texttt{git\ add\ -A}.
  \item
    Check on everything: \texttt{git\ status}.
  \item
    Commit your changes: \texttt{git\ commit\ -m\ "Short\ description\ of\ changes"}.
  \item
    Push your changes to GitHub: \texttt{git\ push}.
  \end{itemize}
\item
  Restart R often (`Session' -\textgreater{} `Restart R and Clear Output').
\item
  Debugging is a skill, that improves with practice.
\item
  One key debugging skill is being able to make a reproducible example that reproduces the issue for others.
\item
  Appropriate code structure and comments are a critical aspect of reproducibility because they help others understand.
\end{itemize}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\begin{quote}
Suppose you have cancer and you have to choose between a black box AI surgeon that cannot explain how it works but has a 90\% cure rate and a human surgeon with an 80\% cure rate. Do you want the AI surgeon to be illegal?

Geoffrey Hinton, 20 February 2020.
\end{quote}

\begin{quote}
The number one thing to keep in mind about machine learning is that performance is evaluated on samples from one dataset, but the model is used in production on samples that may not necessarily follow the same characteristics\ldots{} The finance industry has a saying for this: ``past performance is no guarantee of future results''. Your model scoring X on your test dataset doesn't mean it will perform at level X on the next N situations it encounters in the real world. The future may not be like the past.

So when asking the question, ``would you rather use a model that was evaluated as 90\% accurate, or a human that was evaluated as 80\% accurate'', the answer depends on whether your data is typical per the evaluation process. Humans are adaptable, models are not. If significant uncertainty is involved, go with the human. They may have inferior pattern recognition capabilities (versus models trained on enormous amounts of data), but they understand what they do, they can reason about it, and they can improvise when faced with novelty

If every possible situation is known and you want to prioritize scalability and cost-reduction, go with the model. Models exist to encode and operationalize human cognition in well-understood situations. (``well understood'' meaning either that it can be explicitly described by a programmer, or that you can amass a dataset that densely samples the distribution of possible situations -- which must be static)

François Chollet, 20 February 2020.
\end{quote}

If science is about systematically building and organizing knowledge in terms of testable explanations and predictions, then data science takes this and focuses on data. This means that building, organizing, and sharing knowledge is the critical aspect. Creating knowledge, once, in a way that only you can do it, does not meet this standard. Hence, the need for reproducible workflows for data science.

\citet{Alexander2019} says `\protect\hyperlink{r}{r}esearch is reproducible if it can be reproduced exactly, given all the materials used in the study\ldots{} {[}hence{]} materials need to be provided!\ldots{} {[}M{]}aterials usually means data, code and software.' The minimum requirement is that another person is able to `\protect\hyperlink{r}{r}eproduce the data, methods and results (including figures, tables)'. Similarly, \citet{Gelman2016} identifies how large an issue this is in various social sciences. The problem with work that is not reproducible, is that it does not contribute to our stock of knowledge about the world. Since \citet{Gelman2016}, a great deal of work has been done in many social sciences and the situation has improved a little, but much work remains. And the situation is similar in the life sciences \citep{heil2021reproducibility} and computer science \citep{pineau2021improving}.

Some of the examples that \citet{Gelman2016} talks about, which turned out to not reproduce are not that important in the scheme of things. But at the same time, we saw, and continue to see, similar approaches being used in areas with big impacts. For instance, many governments have created `nudge' units that implement public policy \citep{sunstein2017economics} and they are increasingly using algorithms that they do not make open \citep{chouldechova18a}. Similarly, businesses are increasingly implementing machine learning methods.

At a minimum, and with few exceptions, we must release our code, datasets, and environment. Without the data, we do not know what a finding speaks to \citep{miyakawa2020no}. More banally, we also do not know if there are mistakes or aspects that were inadvertently overlooked \citep{merali2010computational} \citep{hillelwayne} \citep{natefixesmistake}.

To be more specific, consider \citet{wang2018deep} who use deep neural networks to train a model to distinguish between gay and heterosexual men. (\citet{murphy2017} provides a summary of the paper and the associated issues, along with comments from the authors.) To do this, \citet[p.~248]{wang2018deep} needed a dataset of photos of folks that were `adult, Caucasian, fully visible, and of a gender that matched the one reported on the user's profile'. They verified this using Amazon Mechanical Turk, an online platform that pays workers a small amount of money to complete specific tasks. Figure \ref{fig:instructionsexample}, from \citet{wang2018deep} supplemental materials, shows the instructions provided to the Mechanical Turk workers for this task. Some of the issues with these instructions include that Obama had a white mother and a black father but has been classified as `Black'; and Latino is an ethnicity, rather than a race \citep{Mattson2017}. The classification task may seem objective, but, perhaps unthinkingly, echoes the views of Americans with a certain class and background.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/mechanical_turk_instructions} 

}

\caption{Instructions given to Mechanical Turk workers for removing incomplete, non-Caucasian, nonadult, and nonhuman male faces}\label{fig:instructionsexample}
\end{figure}

This is just one specific concern about one part of the \citet{wang2018deep} workflow. Broader concerns are raised by others including \citet{Gelman_2018}. The main issue is that statistical models are specific to the data on which they were trained. And the only reason that we can identify likely issues in the model of \citet{wang2018deep} is because, despite not releasing the specific dataset that they used, they were nonetheless open about their procedure. For our work to be credible, it needs to be reproducible by others.

Some of the steps that we can take to make our work more reproducible include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ensure the entire workflow is documented and this may involve addressing questions such as:

  \begin{itemize}
  \tightlist
  \item
    How was the raw dataset obtained and is access likely to be persistent and available to others?
  \item
    What specific steps are being taken to transform the raw data in the data that were analyzed, and how can this be made available to others?
  \item
    What analysis has been done, and how clearly can this be shared?
  \item
    How has the final paper or report been built and to what extent can others follow that process themselves?
  \end{itemize}
\item
  Not worrying about perfect reproducibility initially, but instead focusing on trying to improve with each successive project. For instance, each of the following requirements are increasingly more onerous and there is no need to be concerned about not being able to the last, until we can do the first:

  \begin{itemize}
  \tightlist
  \item
    Can you run your entire workflow again?
  \item
    Can `another person' run your entire workflow again?
  \item
    Can `future' you run your entire workflow again?
  \item
    Can `future' `another person' run your entire workflow again?
  \end{itemize}
\item
  Including a detailed discussion about the limitations of the dataset and the approach in the final paper or report.
\end{enumerate}

The workflow that we follow is summarized in Figure \ref{fig:workflow}.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{figures/IMG_1847} 

}

\caption{Workflow for telling stories with data}\label{fig:workflow}
\end{figure}

There are various tools that we can use at the different stages that will improve the reproducibility of this workflow. This includes the use of R Markdown, R Projects, and Git and GitHub.

\hypertarget{r-markdown}{%
\section{R Markdown}\label{r-markdown}}

\hypertarget{getting-started-1}{%
\subsection{Getting started}\label{getting-started-1}}

R Markdown is a mark-up language similar to HyperText Markup Language (HTML) or LaTeX, in comparison to a `What You See Is What You Get' (WYSIWYG) language, such as Microsoft Word. This means that all the aspects are consistent, for instance, all top-level heading will look the same. But, it means that we must use symbols to designate how we would like certain aspects to appear. And it is only when we build the mark-up that we get to see what it looks like.

R Markdown is a variant of Markdown that is specifically designed to allow R code chunks to be included. One advantage is that we can get a `live' document in which code executes and then forms part of the document. Another advantage of R Markdown is that very similar code can compile into a variety of documents, including html pages and PDFs. R Markdown also has default options set up for including a title, author, and date sections. One disadvantage is that it can take a while for a document to compile because all the code needs to run. \citet{rmarkdownforscientists} is especially useful for instructions on how to achieve specific outcomes with R Markdown.

We can create a new R Markdown document within R Studio (`File' -\textgreater{} `New File' -\textgreater{} `R Markdown Document').

\hypertarget{essential-commands}{%
\subsection{Essential commands}\label{essential-commands}}

Essential markdown commands include those for emphasis, headers, lists, links, and images. A reminder of these is included in R Studio (`Help' -\textgreater{} `Markdown Quick Reference').

\begin{itemize}
\tightlist
\item
  Emphasis: \texttt{*italic*}, \texttt{**bold**}
\item
  Headers (these go on their own line with a blank line before and after): \texttt{\#\ First\ level\ header}, \texttt{\#\#\ Second\ level\ header}, \texttt{\#\#\#\ Third\ level\ header}
\item
  Unordered list, with sub-lists:
\end{itemize}

\begin{verbatim}
* Item 1
* Item 2
    + Item 2a
    + Item 2b
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Ordered list, with sub-lists:
\end{itemize}

\begin{verbatim}
1. Item 1
2. Item 2
3. Item 3
    + Item 3a
    + Item 3b
\end{verbatim}

\begin{itemize}
\tightlist
\item
  URLs can be added by including an address because it will auto-link: \url{https://www.tellingstorieswithdata.com}, or by linking some text \texttt{{[}the\ address\ of\ this\ book{]}(https://www.tellingstorieswithdata.com)} results in \href{https://www.tellingstorieswithdata.com}{the address of this book}.
\end{itemize}

Once we have added some aspects, then we may want to see the actual document. To build the document click `Knit'.

\hypertarget{r-chunks}{%
\subsection{R chunks}\label{r-chunks}}

We can include code for R and many other languages in code chunks within an R Markdown document. Then when we knit the document, the code will run and be included in the document.

To create an R chunk, we start with three backticks and then within curly braces we tell R Markdown that this is an R chunk. Anything inside this chunk will be considered R code and run as such. For instance, we could load the \texttt{tidyverse} and \texttt{AER} and make a graph of the number of times a survey respondent visited the doctor in the past two weeks.

\begin{verbatim}
```{r}
library(tidyverse)
library(AER)

data("DoctorVisits", package = "AER")

DoctorVisits %>%
  ggplot(aes(x = visits)) +
  geom_histogram(stat = "count")
```
\end{verbatim}

The output of that code is Figure \ref{fig:doctervisits}.

\begin{figure}
\centering
\includegraphics{04-workflow_files/figure-latex/doctervisits-1.pdf}
\caption{\label{fig:doctervisits}Number of doctor visits in the past two weeks, based on the 1977--1978 Australian Health Survey}
\end{figure}

There are various evaluation options that are available in chunks. We include these by putting a comma after \texttt{r} and then specifying any options before the closing curly brace. Helpful options include:

\begin{itemize}
\tightlist
\item
  \texttt{echo\ =\ FALSE}: run the code and include the output, but do not print the code in the document.
\item
  \texttt{include\ =\ FALSE}: run the code but do not output anything and do not print the code in the document.
\item
  \texttt{eval\ =\ FALSE}: do not run the code, and hence do not include the outputs, but do print the code in the document.
\item
  \texttt{warning\ =\ FALSE}: do not display warnings.
\item
  \texttt{message\ =\ FALSE}: do not display messages.
\end{itemize}

For instance, we could include the output, but not the code, and suppress any warnings.

\begin{verbatim}
```{r, echo = FALSE, warning = FALSE}
library(tidyverse)
library(AER)

data("DoctorVisits", package = "AER")

DoctorVisits %>%
  ggplot(aes(x = visits)) +
  geom_histogram(stat = "count")
```
\end{verbatim}

\hypertarget{abstracts-and-pdf-outputs}{%
\subsection{Abstracts and PDF outputs}\label{abstracts-and-pdf-outputs}}

An abstract is a short summary of the paper. In the default preamble, we can add a section for an abstract. Similarly, we can change the output from \texttt{html\_document} to \texttt{pdf\_document} to produce a PDF. This uses LaTeX in the background and may require the installation of supporting packages.

\begin{verbatim}
---
title: My document
author: Rohan Alexander
date: 1 January 2022
output: pdf_document
abstract: "This is my abstract."
---
\end{verbatim}

\hypertarget{references}{%
\subsection{References}\label{references}}

We can include references by specifying a bib file in the preamble and then calling it within the text, as needed.

\begin{verbatim}
---
title: My document
author: Rohan Alexander
date: 1 January 2022
output: pdf_document
abstract: "This is my abstract."
bibliography: bibliography.bib
---
\end{verbatim}

We would need to make a separate file called `bibliography.bib' and save it next to the R Markdown file. In the bib file we need an entry for the item that is to be referenced. For instance, the citation for R can be obtained with \texttt{citation()} and this can be added to the `bibliography.bib' file. Similarly, the citation for a package can be found by including the package name, for instance \texttt{citation(\textquotesingle{}tidyverse\textquotesingle{})}. It can be helpful to use Google Scholar, or doi2bib, to get citations for books or articles.

\begin{verbatim}
@Manual{,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2021},
    url = {https://www.R-project.org/},
  }
@Article{,
    title = {Welcome to the {tidyverse}},
    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},
    year = {2019},
    journal = {Journal of Open Source Software},
    volume = {4},
    number = {43},
    pages = {1686},
    doi = {10.21105/joss.01686},
  }
\end{verbatim}

We need to create a unique key that we use to refer to this item in the text. This can be anything, provided it is unique, but meaningful ones can be easier to remember, for instance `citeR'.

\begin{verbatim}
@Manual{citeR,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2021},
    url = {https://www.R-project.org/},
  }
@Article{citetidyverse,
    title = {Welcome to the {tidyverse}},
    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},
    year = {2019},
    journal = {Journal of Open Source Software},
    volume = {4},
    number = {43},
    pages = {1686},
    doi = {10.21105/joss.01686},
  }
\end{verbatim}

To cite R in the R Markdown document we include \texttt{@citeR}, which would put the brackets around the year, like this: \citet{citeR} or \texttt{{[}@citeR{]}}, which would put the brackets around the whole thing, like this: \citep{citetidyverse}.

\hypertarget{cross-references}{%
\subsection{Cross-references}\label{cross-references}}

It can be useful to cross-reference figures, tables, and equations. This makes it easier to refer to them in the text. To do this for a figure we refer to the name of the R chunk that creates or contains the figure. For instance, \texttt{(Figure\ \textbackslash{}@ref(fig:uniquename))} will produce: (Figure \ref{fig:uniquename}) as the name of the R chunk is \texttt{uniquename}. We also need to add `fig' in front of the chunk name so that R Markdown knows that this is a figure. We then include a `fig.cap' in the R chunk that specifies a caption.

\begin{Shaded}
\begin{Highlighting}[]
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r uniquename, fig.cap = "Number of illnesses in the past two weeks, based on the 1977{-}{-}1978 Australian Health Survey", echo = TRUE\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(AER)}

\FunctionTok{data}\NormalTok{(}\StringTok{"DoctorVisits"}\NormalTok{, }\AttributeTok{package =} \StringTok{"AER"}\NormalTok{)}

\NormalTok{DoctorVisits }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ illness)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{stat =} \StringTok{"count"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{04-workflow_files/figure-latex/uniquename-1.pdf}
\caption{\label{fig:uniquename}Number of illnesses in the past two weeks, based on the 1977--1978 Australian Health Survey}
\end{figure}

We can take a similar, but slightly different, approach to cross-reference tables. For instance, \texttt{(Table\ \textbackslash{}@ref(tab:docvisittable))} will produce: (Table \ref{tab:docvisittable}). In this case we specify `tab' before the unique reference to the table, so that R Markdown knows that it is a table. For tables we need to include the caption in the main content, as a `caption', rather than in a `fig.cap' chunk option as is the case for figures.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DoctorVisits }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{count}\NormalTok{(visits) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\AttributeTok{caption =} \StringTok{"Number of visits to the doctor in the past two weeks, based on the 1977{-}{-}1978 Australian Health Survey"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:docvisittable}Number of visits to the doctor in the past two weeks, based on the 1977--1978 Australian Health Survey}
\centering
\begin{tabular}[t]{r|r}
\hline
visits & n\\
\hline
0 & 4141\\
\hline
1 & 782\\
\hline
2 & 174\\
\hline
3 & 30\\
\hline
4 & 24\\
\hline
5 & 9\\
\hline
6 & 12\\
\hline
7 & 12\\
\hline
8 & 5\\
\hline
9 & 1\\
\hline
\end{tabular}
\end{table}

Finally, we can also cross-reference equations. To that we need to add a tag \texttt{(\textbackslash{}\#eq:macroidentity)} which we then reference. For instance, use \texttt{Equation\ \textbackslash{}@ref(eq:macroidentity).} to produce Equation \eqref{eq:macroidentity}.

\begin{verbatim}
\begin{equation}
Y = C + I + G + (X - M) \label{eq:macroidentity}
\end{equation}
\end{verbatim}

\begin{equation}
Y = C + I + G + (X - M) \label{eq:macroidentity}
\end{equation}

When using cross-references, it is important that the R chunks have simple labels. In general, try to keep the names simple but unique, and if possible, avoid punctuation and stick to letters. Do not use underbars in the labels because that will cause an error.

\hypertarget{r-projects-and-file-structure}{%
\section{R projects and file structure}\label{r-projects-and-file-structure}}

We can use R Studio to create an R project. This means that we can keep all the files (data, analysis, report, etc) associated with a particular project together. A project can be created in R Studio `File' -\textgreater{} `New Project', then select `empty project', name the project and decide where to save it. For instance, a project focused on maternal mortality, may be called `maternalmortality', and might be saved within a folder of other projects. The use of R Projects is `the only practical convention that creates reliable, polite behavior across different computers or users and over time.' \citep{whattheyforgot}. Further, projects are `neither new, nor unique to R', but are a well-established part of software development.

Once a project has been created, a new file with the extension `.RProj' will appear in that folder. As an example, of a folder with an R Project, an example R Markdown document, and an appropriate file structure is available: \url{https://github.com/RohanAlexander/starter_folder}. That can be downloaded: `Code' -\textgreater{} `Download ZIP'.

The main advantage of using an R Project is that we are more easily able to reference other files in a self-contained way. That means when others want to reproduce our work, they know that all the file references and structure should not need to be changed. It means that files are referenced in relation to where the `.Rproj' file is. For instance, instead of reading a csv from, say, \texttt{"\textasciitilde{}/Documents/projects/book/data/"} you can read it in from \texttt{book/data/}. It may be that someone else does not have a `projects' folder, and so the former would not work for them, while the latter would.

The use of R projects is required to meet the minimal level of reproducibility. The use of functions such as \texttt{setwd()}, and computer-specific file paths, bind the work to your computer in a way that is not appropriate.

There are a variety of ways to set-up a folder. A variant of \citet{wilsongoodenough} that is often useful is shown in the example: \url{https://github.com/RohanAlexander/starter_folder}. Here we have an `inputs' folder that contains raw data (which should never be modified \citep{wilsongoodenough}) and literature related to the project (which cannot be modified). An `outputs' folder contains data that we create using R, as well as the paper that we are writing. And a `scripts' folder is what modifies the raw data and saves it into `outputs'. Useful other aspects include a `README.md' which will specify overview details about the project, and a LICENSE.

\hypertarget{git-and-github}{%
\section{Git and GitHub}\label{git-and-github}}

We use the combination of Git and GitHub to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  enhance the reproducibility of work by making it easier to share code and data;
\item
  make it easier to share work;
\item
  improve workflow by encouraging systematic approaches; and
\item
  make it easier to work in teams.
\end{enumerate}

Git is a version control system. One way one often starts doing version control is to have various versions of the one file: `first\_go.R', `first\_go-fixed.R', `first\_go-fixed-with-mons-edits.R'. But this soon becomes cumbersome. One often soon turns to dates, for instance: `2022-01-01-analysis.R', `2022-01-02-analysis.R', `2022-01-03-analysis.R', etc. While this keeps a record it can be difficult to search when we need to go back, because it can be difficult to remember the date some change was made. In any case, it quickly gets unwieldy for a project that is being regularly worked on.

Instead of this, we use Git so that we can have one version of the file, say, `analysis.R' and then use Git to keep a record of the changes to that file, and a snapshot of that file at a given point in time.

We determine when Git takes that snapshot, and when we take that snapshot, we additionally include a message saying what changed between this snapshot and the last. In that way, there is only ever one version of the file, but the history can be more easily searched.

The issue is that Git was designed for software developers. As such, while it works, it can be a little ungainly for non-developers (Figure \ref{fig:hackernews}).

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{/Users/rohanalexander/Documents/book/figures/hacker_news} 

}

\caption{An infamous response to the launch of Dropbox in 2007, trivializing the use-case for Dropbox, and while this user's approach would probably work for them, it probably would not for most folks.}\label{fig:hackernews}
\end{figure}

Hence, GitHub, GitLab, and various other companies offer easier-to-use services that build on Git. We will introduce GitHub here because it `is by far the most dominant code-hosting platform' \citep[p.~21]{eghbal2020working} and it is built into R Studio, but other options have advantages.

One of the challenging aspects of Git is the terminology. Folders are called `repos'. Saving is called a `commit'. One gets used to it eventually, but initially feeling confused is entirely normal.

\citet{happygit} is especially useful for setting up and using Git and GitHub.

\hypertarget{git}{%
\subsection{Git}\label{git}}

We need to git check whether Git is installed. Open R Studio, go to the Terminal, type the following, and then enter/return.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git} \AttributeTok{{-}{-}version}
\end{Highlighting}
\end{Shaded}

If you get a version number, then you are done (Figure \ref{fig:gitone}).

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{/Users/rohanalexander/Documents/book/figures/git_one} 

}

\caption{How to access the Terminal within R Studio}\label{fig:gitone}
\end{figure}

If you have a Mac then Git should come pre-installed, if you have Windows then there is a chance, and if you have Linux then you probably do not need this guide. If you do not get a version number, then you need to install it. To do that you should follow the instructions specific to your operating system in Chapter 5 of \citet{happygit}.

After we have Git, then we need to tell it our username and email. We need to do this because Git adds this information whenever we take a `snapshot', or to use Git's language, whenever we make a commit.

Again, within the Terminal, type the following, replacing the details with yours, and then enter/return after each line.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ config }\AttributeTok{{-}{-}global}\NormalTok{ user.name }\StringTok{\textquotesingle{}Rohan Alexander\textquotesingle{}}
\FunctionTok{git}\NormalTok{ config }\AttributeTok{{-}{-}global}\NormalTok{ user.email }\StringTok{\textquotesingle{}rohan.alexander@utoronto.ca\textquotesingle{}}
\FunctionTok{git}\NormalTok{ config }\AttributeTok{{-}{-}global} \AttributeTok{{-}{-}list}
\end{Highlighting}
\end{Shaded}

The details that you enter here will be public. There are various ways to hide your email address if you need to do this and GitHub provides instructions about this. Again, if you have issues, or need more detailed instructions about this step, then please see Chapter 7 of \citet{happygit}.

\hypertarget{github}{%
\subsection{GitHub}\label{github}}

Now that Git is set-up we need to set-up GitHub. The first step is to create an account on GitHub: \url{https://github.com} (Figure \ref{fig:githubone}).

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{/Users/rohanalexander/Documents/book/figures/github_1} 

}

\caption{GitHub sign-up screen}\label{fig:githubone}
\end{figure}

We now need to make a new folder (which is called a `repo' in Git). Look for a `+' in the top right, and then select `New Repository' (Figure \ref{fig:githubtwo}).

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{/Users/rohanalexander/Documents/book/figures/github_2} 

}

\caption{Start process of creating a new repository}\label{fig:githubtwo}
\end{figure}

At this point we can add a sensible name for the repo. Leave it as public for now, because it can always be deleted later. And check the box to `Initialize this repository with a README'. Leave `Add .gitignore' set to `None'. After that, click `Create repository' (Figure \ref{fig:githubthree}).

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{/Users/rohanalexander/Documents/book/figures/github_3} 

}

\caption{Finish creating a new repository}\label{fig:githubthree}
\end{figure}

This will take us to a screen that is fairly empty, but the details that we need are in the green `Clone or Download' button, which we can copy by clicking the clipboard (Figure \ref{fig:githubfour}).

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{/Users/rohanalexander/Documents/book/figures/github_4} 

}

\caption{Get the details of your new repository}\label{fig:githubfour}
\end{figure}

Now returning to R Studio, we open Terminal, and use \texttt{cd} to navigate to the directory where we want to create this folder. Then type the following, replacing repo details with your own, and enter/return.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ clone https://github.com/RohanAlexander/test.git}
\end{Highlighting}
\end{Shaded}

At this point, a new folder has been created and we can now use it.

To use GitHub for a project that we are actively working on we follow a procedure:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The first thing to do is almost always to pull the latest changes with \texttt{git\ pull}. To do this, open Terminal, and navigate to the folder using \texttt{cd}. Then type \texttt{git\ pull} and enter/return.
\item
  We can then make a change to the folder, for instance, update the README, and then save it as normal.
\item
  Once this is done, we need to `add', `commit', and `push'.

  \begin{itemize}
  \tightlist
  \item
    As before, use \texttt{cd} to navigate to the folder, then type \texttt{git\ status} and enter/return to see if there are any changes (you should see some reference to the change you made).
  \item
    Then type \texttt{git\ add\ -A} and enter/return. This adds the changes to the staging area.
  \item
    Then type \texttt{git\ status} and enter/return to verify that you are happy with what was added.
  \item
    Then type \texttt{git\ commit\ -m\ "Minor\ update\ to\ README"} and enter/return. The message should be informative about the change that was made.
  \item
    Again, type \texttt{git\ status} and enter/return to check on everything.
  \item
    Finally, type \texttt{git\ push} and enter/return to push everything to GitHub.
  \end{itemize}
\end{enumerate}

To summarize the workflow (assuming we are already in the relevant folder):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ pull}
\FunctionTok{git}\NormalTok{ status}
\FunctionTok{git}\NormalTok{ add }\AttributeTok{{-}A}
\FunctionTok{git}\NormalTok{ status}
\FunctionTok{git}\NormalTok{ commit }\AttributeTok{{-}m} \StringTok{"Short commit message"}
\FunctionTok{git}\NormalTok{ status}
\FunctionTok{git}\NormalTok{ push}
\end{Highlighting}
\end{Shaded}

\hypertarget{using-github-within-r-studio-with-the-git-pane}{%
\subsection{Using GitHub within R Studio with the Git pane}\label{using-github-within-r-studio-with-the-git-pane}}

The procedure that we went through is useful to better understand what is happening when we use Git and GitHub but can be a bit cumbersome. Usefully, GitHub is built into R Studio and so we can use the Git pane to move away from the Terminal.

Get started by creating a new repo in GitHub, as before, and copy the repo information, as before. At this point, open R Studio, and create a new project using version control (`Files' -\textgreater{} `New Project' -\textgreater{} `Version Control' -\textgreater{} `Git', and then paste the information for the repo). Follow through the rest of the set-up by naming the project something sensible, saving it somewhere sensible, and clicking `Open in new session', before creating the project. This will then create a new folder with an R Project which will be Git-initialized and linked to the GitHub repo.

If we then open this R Project, we will have a `Git' tab (Figure \ref{fig:rstudiogit}).

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{/Users/rohanalexander/Documents/book/figures/RStudio_git} 

}

\caption{The Git pane in R Studio}\label{fig:rstudiogit}
\end{figure}

We can then use Git through that tab. As before, we first want to `pull', but we can do this by clicking the blue down arrow. As before, we want to commit the files that have changes, and we do this by selecting the `staged' checkbox against the files that we would like to commit. Then `Commit'. Again, we want to include a message with our commit, and we do this by typing a message in the `Commit message' box and then `Commit'. Finally, we can `Push'. Further details on this workflow are available in Chapter 12 of \citet{happygit}.

\hypertarget{using-github-with-usethis}{%
\subsection{\texorpdfstring{Using GitHub with \texttt{usethis}}{Using GitHub with usethis}}\label{using-github-with-usethis}}

We used the Git pane in R Studio to reduce the need to use the Terminal, but it did not remove the need to go to GitHub and set-up a new project. Having set-up Git and GitHub, we can further improve our workflow by using \texttt{usethis} to do much of the work \citep{citeusethis}.

After installing and loading \texttt{usethis} we need to check that we have set-up Git with \texttt{usethis::git\_sitrep()}. This should print information about the user. We can use \texttt{usethis::use\_git\_config()} to update the username or email address. For instance,

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(usethis)}

\FunctionTok{use\_git\_config}\NormalTok{(}\AttributeTok{user.name =} \StringTok{"Rohan Alexander"}\NormalTok{, }\AttributeTok{user.email =} \StringTok{"rohan.alexander@utoronto.ca"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can then create a new R Project (`File' -\textgreater{} `New Project' -\textgreater{} `New Directory' -\textgreater{} `New Project' -\textgreater{} Add a name and save it in a sensible location and click `Open in new session'.)

We then use \texttt{usethis::use\_git()} to initiate things. It will ask if we are happy to commit files.

Having committed, we can use \texttt{usethis::use\_github()} to push to GitHub.

\hypertarget{using-r-in-practice}{%
\section{Using R in practice}\label{using-r-in-practice}}

\hypertarget{dealing-with-errors}{%
\subsection{Dealing with errors}\label{dealing-with-errors}}

\begin{quote}
When you are programming, eventually your code will break, when I say eventually, I mean like probably 10 or 20 times a day.

\citet{sharlatalks}
\end{quote}

Everyone who uses R, or any programming language for that matter, has trouble find them at some point. This is normal. Programming is hard and everyone struggles sometimes. At some point code will not run or will throw an error. This is normal, and it happens to everyone. Everyone gets frustrated.

To move forward we develop strategies to work through the issues:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  If you are get an error message, then sometimes it will be useful. Try to read it carefully to see if there is anything of use in it.
\item
  Try to search, say on Google, for the error message. It can be useful to include `tidyverse' or `in R' in the search to help make the results more appropriate. Sometimes Stack Overflow results can be useful.
\item
  Look at the help file for the function, by putting `?' before the function, for instance, \texttt{?pivot\_wider()}. A common issue is to use a slightly incorrect argument name or format, such as accidentally including a string instead of an object name.
\item
  Look at where the error is happening and remove code until the error is resolved, and then slowly add code back again.
\item
  Check the class of the object, with \texttt{class()}, for instance, \texttt{class(data\_set\$data\_column)}. Ensuring that it is what it expected.
\item
  Restart R (`Session' -\textgreater{} `Restart R and Clear Output') and load everything again.
\item
  Restart the computer.
\item
  Search for what you are trying to do, rather than the error, being sure to include `tidyverse' or `in R' in the search to help make the results more appropriate. For instance, `save PDF of graph in R made using ggplot'. Sometimes there are relevant blog posts or Stack Overflow answers that will help.
\item
  Making a small, self-contained, reproducible example `reprex' to see if the issue can be isolated and to enable others to help.
\end{enumerate}

More generally, while this is rarely possible to do, it is almost always helpful to take a break and come back the next day.

\hypertarget{reproducible-examples}{%
\subsection{Reproducible examples}\label{reproducible-examples}}

\begin{quote}
No one can advise or help you---no one. There is only one thing you should do. Go into yourself.

\citet{rilke}
\end{quote}

Asking for help is a skill like any other. We get better at it with practice. It is important to try not to say `this doesn't work', `I tried everything', `your code does not work', or `here is the error message, what do I do?'. In general, it is not possible to help based on these comments, because there are too many possible issues. You need to make it easy for others to help you. This involves a few steps.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Provide a small, self-contained, example of your data, and code, and detail what is going wrong.
\item
  Document what you have tried so far, including which Stack Overflow and R Studio Community pages have you looked at, and why are they not quite what you are after?
\item
  Be clear about the outcome that you would like.
\end{enumerate}

Begin by creating a minimal REPRoducible EXample, a `reprex'. This is code that contains what is needed to reproduce the error, but only what is needed. This means that the code it likely a smaller, simpler, version that nonetheless reproduces the error.

Sometimes this process enables one to solve the problem. If it does not, then it gives someone else a fighting chance of being able to help. It is important to recognize that there is almost no chance that you have got a problem that someone has not addressed before. It is more likely that the main difficulty is in trying to communicate what you are trying to do and what is happening, in a way that allows others to recognize both. Developing tenacity is important.

To develop reproducible examples, the \texttt{reprex} package \citep{citereprex} is especially useful. To use it we:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Load the \texttt{reprex} package: \texttt{library(reprex)}.
\item
  Highlight and copy the code that is giving issues.
\item
  Run \texttt{reprex()} in the Console.
\end{enumerate}

If the code is self-contained, then it will preview in the Viewer. If it is not, then it will error, and the code needs to be re-written so that it is self-contained.

If you need data to reproduce the error, then you should use data that is built into R. There are a large number of datasets that are built into R and can be seen using \texttt{library(help\ =\ "datasets")}. But if possible, you should use a common option such as `mtcars' or `faithful'.

\hypertarget{mentality}{%
\subsection{Mentality}\label{mentality}}

\begin{quote}
(Y)ou are a real, valid, \emph{competent} user and programmer no matter what IDE you develop in or what tools you use to make your work work for you

(L)et's break down the gates, there's enough room for everyone

Sharla Gelfand, \href{https://twitter.com/sharlagelfand/status/1237365576701542400}{10 March 2020}.
\end{quote}

If you write code, then you are a programmer regardless of how you do it, what you are using it for, or who you are. But there are a few traits that one tends to notice great programmers have in common.

\begin{itemize}
\tightlist
\item
  Focused: Often having an aim to `learn R' or something similar tends to be problematic, because there is no real end point to that. It tends to be more efficient to have smaller, more specific goals, such as `make a histogram about the 2019 Canadian Election with ggplot'. This is something that can be focused on and achieved in a few hours. The issue with goals that are more nebulous, such as `I want to learn R', is that it becomes easy to get lost on tangents, much more difficult to get help. This can be demoralizing and lead to folks quitting too early.
\item
  Curious: It is almost always useful to have a go. In general, the worst that happens is that you waste your time. You can rarely break something irreparably with code. If you want to know what happens if you pass a `vector' instead of a `dataframe' to \texttt{ggplot()} then try it.
\item
  Pragmatic: At the same time, it can be useful to stick within reasonable bounds, and make one small change each time. For instance, say you want to run some regressions, and are curious about the possibility of using the \texttt{tidymodels} package \citep{citeTidymodels} instead of \texttt{lm()}. A pragmatic way to proceed is to use one aspect from the \texttt{tidymodels} package initially and then make another change next time.
\item
  Tenacious: Again, this is a balancing act. There are always unexpected problems and issues with every project. On the one hand, persevering despite these is a good tendency. But on the other hand, sometimes one does need to be prepared to give up on something if it does not seem like a break-through is possible. Here mentors can be useful as they tend to be a better judge of what is reasonable. It is also where appropriate planning is useful.
\item
  Planned: It is almost always useful to excessively plan what you are going to do. For instance, you may want to make a histogram of the 2019 Canadian Election. You should plan the steps that are needed and even to sketch out how each step might be implemented. For instance, the first step is to get the data. What packages might be useful? Where might the data be? What is the back-up plan if the data do not exist there?
\item
  Done is better than perfect: We all have various perfectionist tendencies to a certain extent, but it can be useful to initially try to turn them off to a certain extent. In the first instance, try to write code that works, especially in the early days. You can always come back and improve aspects of it. But it is important to actually ship. Ugly code that gets the job done, is better than beautiful code that is never finished.
\end{itemize}

\hypertarget{code-comments-and-style}{%
\subsection{Code comments and style}\label{code-comments-and-style}}

Code must be commented \citep{lee2018ten}. Comments should focus on why certain code was written, (and to a lesser extent, why a common option is not selected).

There is no one way to write code, especially in R. However, there are some general guidelines that will make it easier for you even if you are just working on your own. It is important to recognize that most projects will evolve over time, and one purpose served by code comments are as `{[}m{]}essages left for your future self (or near-future others) {[}that{]} help retrace and justify your decisions' \citep{bowers2011six}.

Comments in R can be added by including the \# symbol. We do not have to put a comment at the start of the line, it can be midway through. In general, we do not need to comment what every aspect of your code is doing but we should comment parts that are not obvious. For instance, if we read in some value then we may like to comment where it is coming from.

You should comment why you are doing something \citep{tidyversestyleguide}. What are you trying to achieve?

You must comment to explain weird things. Like if you are removing some specific row, say row 27, then why are you removing that row? It may seem obvious in the moment, but future-you in six months will not remember.

You should break your code into sections. For instance, setting up the workspace, reading in datasets, manipulating and cleaning the dataset, analyzing the datasets, and finally producing tables and figures. Each of these should be separated with comments explaining what is going on, and sometimes into separate files, depending on the length.

Additionally, at the top of each file it is important to basic information, such as the purpose of the file, and pre-requisites or dependencies, the date, the author and contact information, and finally and red-flags or todos.

At the very least every R script needs a preamble and a clear demarcation of sections.

\begin{verbatim}
#### Preamble ####
# Purpose: Brief sentence about what this script does
# Author: Your name
# Data: The date it was written
# Contact: Add your email
# License: Think about how your code may be used
# Pre-requisites: 
# - Maybe you need some data or some other script to have been run?


#### Workspace setup ####
# do not keep the install.packages line - comment out if need be
# Load libraries
library(tidyverse)

# Read in the raw data. 
raw_data <- readr::read_csv("inputs/data/raw_data.csv")


#### Next section ####
...
\end{verbatim}

\hypertarget{exercises-and-tutorial-3}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-3}}

\hypertarget{exercises-3}{%
\subsection{Exercises}\label{exercises-3}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  According to \citet{Alexander2019} research is reproducible if (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    It is published in peer-reviewed journals.
  \item
    All of the materials used in the study are provided.
  \item
    It can be reproduced exactly without the authors providing materials.
  \item
    It can be reproduced exactly, given all the materials used in the study.
  \end{enumerate}
\item
  According to the timeline of \citet{Gelman2016}, a) when did Paul Meehl identify various issues; and b) when did null hypothesis significance testing (NHST) become controversial (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    1970s-1980s; 1990s-2000.
  \item
    1960s-1970s; 1980s-1990.
  \item
    1970s-1980s; 1980s-1990.
  \item
    1960s-1970s; 1990s-2000.
  \end{enumerate}
\item
  Which of the following are components of the project layout recommended by \citet{wilsongoodenough} (select all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    requirements.txt
  \item
    doc
  \item
    data
  \item
    LICENSE
  \item
    CITATION
  \item
    README
  \item
    src
  \item
    results
  \end{enumerate}
\item
  Based on \citet{monicatalks} please write a paragraph about some of the barriers you overcame, or still face, with regard to sharing code that you wrote.
\item
  According to \citet{sharlatalks}, what is the key part of `If you need help getting unstuck, the first step is to create a reprex, or reproducible example. The goal of a reprex is to package your problematic code in such a way that other people can run it and feel your pain. Then, hopefully, they can provide a solution and put you out of your misery.' (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    package your problematic code
  \item
    other people can run it and feel your pain
  \item
    the first step is to create a reprex
  \item
    they can provide a solution and put you out of your misery
  \end{enumerate}
\item
  According to \citet{sharlatalks}, what are the three key aspects of a reprex (select all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    data
  \item
    only the libraries that are necessary and all the libraries that are necessary
  \item
    relevant code and only relevant code
  \end{enumerate}
\item
  According to \citet{tidyversestyleguide} for naming files, how would these files `00\_get\_data.R', `get data.R' be classified (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    bad; bad.
  \item
    good; bad.
  \item
    bad; good.
  \item
    good; good.
  \end{enumerate}
\item
  Which of the following would result in bold text in R Markdown (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{**bold**}
  \item
    \texttt{\#\#bold\#\#}
  \item
    \texttt{*bold*}
  \item
    \texttt{\#bold\#}
  \end{enumerate}
\item
  Which option would hide the warnings in a R Markdown R chunk (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{echo\ =\ FALSE}
  \item
    \texttt{include\ =\ FALSE}
  \item
    \texttt{eval\ =\ FALSE}
  \item
    \texttt{warning\ =\ FALSE}
  \item
    \texttt{message\ =\ FALSE}
  \end{enumerate}
\item
  Which options would run the R code chunk, but not display the code in a R Markdown R chunk (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{echo\ =\ FALSE}
  \item
    \texttt{include\ =\ FALSE}
  \item
    \texttt{eval\ =\ FALSE}
  \item
    \texttt{warning\ =\ FALSE}
  \item
    \texttt{message\ =\ FALSE}
  \end{enumerate}
\item
  Why are R Projects important (select all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    They help with reproducibility.
  \item
    They make it easier to share code.
  \item
    They make your workspace more organized.
  \item
    They ensure reproducibility.
  \end{enumerate}
\item
  Please discuss a circumstance in which an R Project would be useful.
\item
  Consider this sequence: `\texttt{git\ pull}, \texttt{git\ status}, \_\_\_\_\_\_\_\_, \texttt{git\ status}, \texttt{git\ commit\ -m\ "My\ message"}, \texttt{git\ push}'. What is the missing step (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{git\ add\ -A}.
  \item
    \texttt{git\ status}.
  \item
    \texttt{git\ pull}.
  \item
    \texttt{git\ push}.
  \end{enumerate}
\item
  Assuming the libraries and datasets have been loaded, what is the mistake in this code: \texttt{DoctorVisits\ \textbar{}\textgreater{}\ select("visits")} (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{"visits"}
  \item
    \texttt{DoctorVisits}
  \item
    \texttt{select}
  \item
    \texttt{\textbar{}\textgreater{}}
  \end{enumerate}
\item
  What is a reprex and why is it important to be able to make one (select all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A reproducible example that enables your error to be reproduced.
  \item
    A reproducible example that helps others help you.
  \item
    A reproducible example during the construction of which you may solve your own problem.
  \item
    A reproducible example that demonstrates you have actually tried to help yourself.
  \end{enumerate}
\item
  The following code produces an error. Please use \texttt{reprex} \citep{citereprex} to build a reproducible example that you could use to get help with it, and submit the reprex.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{oecd\_gdp }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"https://stats.oecd.org/sdmx{-}json/data/DP\_LIVE/.QGDP.../OECD?contentType=csv\&detail=code\&separator=comma\&csv{-}lang=en"}\NormalTok{)}

\FunctionTok{head}\NormalTok{(oecd\_gdp)}

\FunctionTok{library}\NormalTok{(forcats)}
\FunctionTok{library}\NormalTok{(dplyr)}

\NormalTok{oecd\_gdp\_most\_recent }\OtherTok{\textless{}{-}} 
\NormalTok{  oecd\_gdp }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(TIME }\SpecialCharTok{==} \StringTok{"2021{-}Q3"}\NormalTok{,}
\NormalTok{         SUBJECT }\SpecialCharTok{==} \StringTok{"TOT"}\NormalTok{,}
\NormalTok{         LOCATION }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"AUS"}\NormalTok{, }\StringTok{"CAN"}\NormalTok{, }\StringTok{"CHL"}\NormalTok{, }\StringTok{"DEU"}\NormalTok{, }\StringTok{"GBR"}\NormalTok{,}
                         \StringTok{"IDN"}\NormalTok{, }\StringTok{"ESP"}\NormalTok{, }\StringTok{"NZL"}\NormalTok{, }\StringTok{"USA"}\NormalTok{, }\StringTok{"ZAF"}\NormalTok{),}
\NormalTok{         MEASURE }\SpecialCharTok{==} \StringTok{"PC\_CHGPY"}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{european =} \FunctionTok{if\_else}\NormalTok{(LOCATION }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"DEU"}\NormalTok{, }\StringTok{"GBR"}\NormalTok{, }\StringTok{"ESP"}\NormalTok{),}
                             \StringTok{"European"}\NormalTok{,}
                             \StringTok{"Not european"}\NormalTok{),}
         \AttributeTok{hemisphere =} \FunctionTok{if\_else}\NormalTok{(LOCATION }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"CAN"}\NormalTok{, }\StringTok{"DEU"}\NormalTok{, }\StringTok{"GBR"}\NormalTok{, }\StringTok{"ESP"}\NormalTok{, }\StringTok{"USA"}\NormalTok{),}
                             \StringTok{"Northern Hemisphere"}\NormalTok{,}
                             \StringTok{"Southern Hemisphere"}\NormalTok{),}
\NormalTok{         )}

\FunctionTok{library}\NormalTok{(ggplot)}
\FunctionTok{library}\NormalTok{(patchwork)}

\NormalTok{oecd\_gdp\_most\_recent }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ LOCATION, }\AttributeTok{y =}\NormalTok{ Value)) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat=}\StringTok{"identity"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tutorial-3}{%
\subsection{Tutorial}\label{tutorial-3}}

Please put together a small R Markdown file that downloads a dataset using \texttt{opendatatoronto}, cleans it, and makes a graph. Then exchange it with someone else. Ask them to both read the code and to run it, and to then provide you with feedback about both aspects. Write a page or two of single-spaced content, about the comments that you received and changes that you could make going forward.

\hypertarget{paper}{%
\subsection{Paper}\label{paper}}

At about this point, Paper 1 (Appendix \ref{paper-1}) would be appropriate.

\hypertarget{part-communication}{%
\part{Communication}\label{part-communication}}

\hypertarget{on-writing}{%
\chapter{On writing}\label{on-writing}}

\textbf{Required material}

\begin{quote}
If you want to be a writer, you must do two things above all others: read a lot and write a lot. There's no way around these two things that I'm aware of, no shortcut.

\citet[p.~145]{stephenking}
\end{quote}

\begin{itemize}
\tightlist
\item
  Read \emph{On Writing Well}, (any edition is fine) \citep{zinsser}.
\item
  Read \emph{Publication, publication}, \citep{king2006publication}
\item
  Read two of the following well-written quantitative papers:

  \begin{itemize}
  \tightlist
  \item
    \emph{Asset prices in an exchange economy}, \citep{lucas1978asset}.
  \item
    \emph{Individuals, institutions, and innovation in the debates of the French Revolution}, \citep{barron2018individuals}.
  \item
    \emph{Modeling: optimal marathon performance on the basis of physiological factors}, \citep{joyner1991modeling}.
  \item
    \emph{On reproducible econometric research}, \citep{koenker2009reproducible}.
  \item
    \emph{Prevented mortality and greenhouse gas emissions from historical and projected nuclear power}, \citep{kharecha2013prevented}.
  \item
    \emph{Sample selection bias as a specification error}, \citep{heckman1979sample}.
  \item
    \emph{Seeing like a market}, \citep{fourcade2017seeing}.
  \item
    \emph{Simpson's paradox and the hot hand in basketball}, \citep{wardrop1995simpson}.
  \item
    \emph{Smoking and carcinoma of the lung}, \citep{doll1950smoking}.
  \item
    \emph{Some studies in machine learning using the game of checkers}, \citep{samuel1959some}.
  \item
    \emph{Statistical methods for assessing agreement between two methods of clinical measurement}, \citep{bland1986statistical}.
  \item
    \emph{The mundanity of excellence: An ethnographic report on stratification and Olympic swimmers}, \citep{chambliss1989mundanity}.
  \item
    \emph{The probable error of a mean}, \citep{student1908probable}.
  \end{itemize}
\item
  Read two of the following articles from \emph{The New Yorker}:

  \begin{itemize}
  \tightlist
  \item
    \emph{Funny Like a Guy}, Tad Friend
  \item
    \emph{Going the Distance}, David Remnick
  \item
    \emph{Happy Feet}, Alexandra Jacobs
  \item
    \emph{Levels of the Game}, John McPhee
  \item
    \emph{Reporting from Hiroshima}, John Hersey
  \item
    \emph{The Catastrophist}, Elizabeth Kolbert
  \item
    \emph{The Quiet German}, George Packer
  \end{itemize}
\item
  Read two of the following articles from miscellaneous publications:

  \begin{itemize}
  \tightlist
  \item
    \emph{Blades of Glory}, Holly Anderson
  \item
    \emph{Born to Run}, Walt Harrington
  \item
    \emph{Dropped}, Jason Fagone
  \item
    \emph{Federer as Religious Experience}, David Foster Wallace
  \item
    \emph{Generation Why?}, Zadie Smith
  \item
    \emph{One hundred years of arm bars}, David Samuels
  \item
    \emph{Out in the Great Alone}, Brian Phillips
  \item
    \emph{Pearls Before Breakfast}, Gene Weingarten
  \item
    \emph{The Cult of `Jurassic Park'}, Bryan Curtis
  \item
    \emph{The House that Hova Built}, Zadie Smith
  \item
    \emph{The Re-Education of Chris Copeland}, Flinder Boyd
  \item
    \emph{The Sea of Crisis}, Brian Phillips
  \end{itemize}
\end{itemize}

\textbf{Key concepts and skills}

\begin{itemize}
\tightlist
\item
  To get better at writing, write, ideally every day.
\item
  Write for the reader.
\item
  Have one message that you want to communicate.
\item
  Get to a first draft as quickly as possible.
\item
  Rewrite brutally.
\item
  Remove as many words as possible.
\end{itemize}

\hypertarget{introduction-1}{%
\section{Introduction}\label{introduction-1}}

\begin{quote}
{[}T{]}he duty of a scientist is not only to find new things, but to communicate them successfully in at least three forms: 1) Writing papers and books. 2) Prepared public talks. 3) Impromptu talks.

\citet[p.~65]{hamming1996}
\end{quote}

\begin{quote}
People who need to write: founders, VCs, lawyers, software engineers, designers, painters, data scientists, musicians, filmmakers, creative directors, physical trainers, teachers, writers.
Learn to write.

Sahil Lavingia, 3 February 2020.
\end{quote}

\begin{quote}
Writing well has done just as much for me as knowing how to code. I'd add that if you're intimidated by writing, start a blog and write often about something you're interested in. You'll get better. At least that's what I've done for the past 10 years. :)

Vicki Boykis, 3 February 2020.
\end{quote}

We need to write in order to tell our stories. Writing allows us to communicate efficiently. It is also a way to work out what we believe and allows us to get feedback on our ideas. Effective papers are tightly written and well-organized, which makes the story flow and easy to follow. Proper sentence structure, spelling, vocabulary, and grammar are important because they remove distractions and enable each point to be clearly articulated. Effective papers demonstrate understanding of the topic by confidently using relevant terms and techniques and considering issues without being overly verbose. Graphs, tables, and references are used to enhance both the story and its credibility.

This chapter is about writing. By the end of it, you will have a better idea of how to write short, detailed, quantitative papers that communicate what you want them to, and do not waste the reader's time. We write for the reader, not for ourselves. Specifically, we write to be useful to the reader, where `{[}u{]}seful writing tells people something true and important that they didn't already know, and tells them as unequivocally as possible' \citep{grahamhowtowriteusefully}. That said, the greatest benefit of writing nonetheless often accrues to the writer, even when we write for our audience. This is because the process of writing is a way to work out what we think and how we came to believe it.

\begin{quote}
The way to do a piece of writing is three or four times over, never once. For me, the hardest part comes first, getting something---anything---out in front of me. Sometimes in a nervous frenzy I just fling words as if I were flinging mud at a wall. Blurt out, heave out, babble out something---anything---as a first draft. With that, you have a achieved a sort of nucleus. Then, as you work it over and alter it, you begin to shape sentences that score higher with the ear and the eye. Edit it again---top to bottom. The chances are that about now you'll be seeing something that you are sort of eager for others to see. And all that takes times. What I have left out is the interstitial time. You finish that first awful blurting, and then you put the thing aside. You get in your car and drive home. On the way, your mind is still knitting at the words. You think of a better way to say something, a good phrase to correct a certain problem. Without that drafted version---if it did not exist---you obviously would not be thinking of things that would improve it. In short, you may be actually writing only two or three hours a day, but your mind, in one way or another, is working on it twenty-four hours a day---yes, while you sleep---but only if some sort of draft of earlier version already exists. Until it exists, writing has not really begun.

\citet[p.~159]{draftnumberfour}
\end{quote}

The process of writing is a process of re-writing. And the critical task is to get to a first draft as quickly as possible. A complete first draft of a five-to-ten-page quantitative paper can be done in a day. Until that complete first draft exists, it is useful to try to not to delete or even revise anything that was written, regardless of how bad it may seem. Just write.

One of the most intimidating things in the world is a blank page, and we deal with this by immediately adding headings such as: `Introduction', `Data', `Model', `Results', and `Discussion'. And then add fields in the top matter for the various bits and pieces that are needed, such as `title', `date', `author' and `abstract'. This creates a generic outline, and its role is akin to placing on the counter, the ingredients that we will use to prepare dinner \citep{draftnumberfour}.

Having established this generic outline, we need to develop an understanding of what we are exploring through developing a research question. In theory, we develop a research question, answer it, and then we do all the writing; but that rarely actually happens \citep{franklin2005exploratory}. Instead, we typically have some idea of the question, and our answer, and these become less vague as we write. This is because it is through the process of writing that we refine our thinking \citep[p.~131]{stephenking}. Having put down some thoughts about the research question, we can start to add dot points in each of the sections, adding sub-sections, with informative sub-headings as needed. We then go back and expand those dot points into paragraphs.

While writing the first draft it is important to ignore the feeling that one is not good enough, or that it is impossible. Just write. We need words on paper, even if they are bad, and the first draft is when we accomplish this. Remove all distractions and just write. Perfectionism is the enemy, and should be set aside. Sometimes this can be accomplished by getting up very early to write, or by creating a deadline, or with a glass or two of wine. One friend puts her baby to sleep to the sound of her typing, with the result being that she must keep typing otherwise the baby will wake up. Creating a sense of urgency can be useful and rather than adding proper citations as we go, which could slow us down, just add something like `{[}TODO: CITE R HERE{]}'. Do similar with graphs and tables. That is, include textual descriptions such as `{[}TODO: ADD GRAPH THAT SHOWS EACH COUNTRY OVER TIME HERE{]}' instead of actual graphs and tables. Focus on adding content, even if it is poorly written, or not ideal. When this is all done, a first draft exists!

This first draft will be bad. But it is by writing a bad first draft that we can get to a good second draft, a great third draft, and eventually excellence \citep[p.~20]{birdbybird}. That first draft will be too long, it will not make sense, it will contain claims that cannot be supported, and some claims that should not be. Having focused on adding content while writing the first draft, to turn that into a second draft, we use the `delete' key extensively, as well as `cut' and `paste'. Printing out the paper and using a red pen to move or remove is especially helpful. The process of going from a first draft to a second draft is best done in one sitting, to help with flow and consistency of the story. One aspect of this first re-write is enhancing the story that we want to tell. And another aspect is taking out everything that is not the story \citep[p.~57]{stephenking}.

As we go through what was written in each of the sections, we try to bring some sense to it, with special consideration to how it supports our story. This revision process is the essence of writing \citep[p.~160]{draftnumberfour}. We should also fix the references, and add the real graphs and tables. As part of this re-writing process, the paper's central message tends to develop, and our answers to the research questions tend to become clearer. At this point, aspects such as the introduction can be returned to and, finally, the abstract. Typos and other issues affect the credibility of the work, and so it is important that these are fixed as part of the second draft.

We now have a paper that is sensible. The job is to now make it brilliant. Print it out again, and again go through it on paper. It is especially important to brutally remove everything that does not contribute to the story. At about this stage, we may be starting to get too close to the paper. We write for our reader, and so this is a great opportunity to give it to someone else for their comments. We ask them for feedback that enables us to better understand the weak parts of the story. After addressing these, it can be helpful to go through the paper once more, this time reading it aloud. A paper tends to never be `done' and it is more that at a certain point we either run out of time or become sick of the sight of it.

\hypertarget{developing-research-questions}{%
\section{Developing research questions}\label{developing-research-questions}}

Both qualitative and quantitative approaches have their place, but here we focus on quantitative approaches. Qualitative research is important as well, and often the most interesting work has a little of both. When conducting quantitative analysis, we are subject to issues such as data quality, scales, measurement, and sources. We are often especially interested in trying to tease out causality. Regardless, we are trying to learn something about the world. Our research questions need to take this all into account.

Broadly, there are two ways to go about research:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  data-first; or
\item
  question-first.
\end{enumerate}

\hypertarget{data-first}{%
\subsection{Data-first}\label{data-first}}

When being data-first, the main issue is working out the questions that can be reasonably answered with the available data. When deciding what these are, it is useful to consider:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Theory: Is there a reasonable expectation that there is something causal that could be determined? For instance, if the question involves charting the stock market, then it might be better to consider haruspex because at least that way we would have something to eat. Questions usually need to have some plausible theoretical underpinning to help avoid spurious relationships.
\item
  Importance: There are plenty of trivial questions that can be answered, but it important to not waste our time or that of the reader. Having an important question can also help with motivation when we find ourselves in, say, the fourth straight week of cleaning data and de-bugging code. It can also make it easier to attract talented employees and funding. That said, there is a balance that is needed. But it is important that the question has a decent chance of being answered. And so attacking a generational-defining question might be best broken up into smaller chunks.
\item
  Availability: Is there a reasonable expectation of additional data being available in the future? This could allow us to answer related questions and turn this one paper into a research agenda.
\item
  Iteration: Is this something that could be run multiple times, or is it a once-off analysis? If it is the former, then it becomes possible to start answering specific research questions and then iterate. But if we can only get access to the data once then we need to think about broader questions.
\end{enumerate}

There's a saying, sometimes attributed to Xiao-Li Meng that all of statistics is a missing data problem. And so paradoxically, another way to ask data-first questions to think about which data we do not have. For instance, returning to the neonatal and maternal mortality examples discussed in, respectively, Chapters \ref{telling-stories-with-data} and \ref{drinking-from-a-fire-hose}, the fundamental problem is that we do not have perfect and complete data about cause of death. If we did, then we could count the number of relevant deaths. Having established the missing data problem, we can take a data-driven approach by looking at the data we do have, and then ask research questions that speak to the extent that we can use that to approximate our hypothetical perfect and complete dataset.

\hypertarget{question-first}{%
\subsection{Question-first}\label{question-first}}

When trying to be question-first, there is the inverse different issue of being concerned about data availability. The `FINER framework' is used in medicine to help guide the development of research questions. It recommends asking questions that are: Feasible, Interesting, Novel, Ethical, and Relevant \citep{hulley2007designing}. \citet{farrugia2010research} builds on FINER with PICOT, which recommends additional considerations: Population, Intervention, Comparison group, Outcome of interest, and Time. It can feel overwhelming trying to write out a question. One way to go about it is to ask a very specific question. Another is to decide whether we are interested in descriptive, predictive, inferential, or causal analysis.

These then lead to different types of questions, for instance, descriptive analysis: `What does \(x\) look like?'; predictive analysis: `What will happen to \(x\)?'; inferential: `How can we explain \(x\)?'; and causal: `What impact does \(x\) have on \(y\)?'. Each of these have a role to play.

Often time will be constrained, possibly in interesting ways and these can guide the specifics of the research question. If we are interested in the effect of Trump's tweets on the stock market, then that can be done just by looking at the minutes (milliseconds?) after he tweets. But what if we are interested in the effect of a cancer drug on long term outcomes? If the effect takes 20 years, then we must either wait a while, or we need to look at people who were treated in 2000, but then we have selection effects and different circumstances to if we give the drug today. Often the only reasonable thing to do is to build a statistical model, but then we need adequate sample sizes, etc.

When answering questions usually, the creation of a counterfactual is crucial. Briefly, a counterfactual is an if-then statement in which the `if' is false. Consider the example of Humpty Dumpty from Lewis Carroll's \emph{Through the Looking-Glass} \citep{throughthelookingglass}.

\begin{quote}
`What tremendously easy riddles you ask!' Humpty Dumpty growled out. `Of course I don't think so! Why, if ever I did fall off---which there's no chance of---but if I did---' Here he pursed his lips and looked so solemn and grand that Alice could hardly help laughing. `If I did fall,' he went on, `The King has promised me---with his very own mouth-to-to-'
\end{quote}

Humpty is satisfied with what would happen if he were to fall off, even though he is similarly satisfied that this would never happen. It is this comparison group that often determines the answer to a question. For instance, consider the effect of VO2 max on the outcome of bike race. If we compare over the general population then it is an important variable, but if we only compare over elite athletes, then it is less important, because of selection.

\hypertarget{writing}{%
\section{Writing}\label{writing}}

\begin{quote}
I had not indeed published anything before I commenced ``The Professor'', but in many a crude effort, destroyed almost as soon as composed, I had got over any such taste as I might once have had for ornamented and redundant composition, and come to prefer what was plain and homely.

\emph{The Professor} \citep{theprofessor}.
\end{quote}

We discuss the following components: title, abstract, introduction, data, results, discussion, figures, tables, equations, and technical terms. Throughout all sections of a paper it is important that we are as brief and specific as possible.

\hypertarget{title}{%
\subsection{Title}\label{title}}

A title is the first opportunity that we have to engage our reader in our story. Ideally, we are able to tell our reader exactly what we found. Effective titles are critical because otherwise papers will be ignored by readers. While a title does not have to be `cute', it does need to be effective. This means it needs to make the story clear.

One example of a title that is good enough is `On the 2016 Brexit referendum'. This title is useful because the reader at least knows what the paper will be about. But it is not particular informative or enticing. A slightly better variant could be `On the 'Vote Leave' outcome in the 2016 Brexit referendum'. This variant adds specifically which is particularly informative. Finally, another variant would be `Vote Leave outperforms in rural areas in the 2016 Brexit referendum: Evidence from a Bayesian hierarchical model'. Here the reader knows the approach of the paper and also the main take-away.

We will consider a few examples of particularly effective titles. \citet{hug2019national} uses `National, regional, and global levels and trends in neonatal mortality between 1990 and 2017, with scenario-based projections to 2030: a systematic analysis'. Here it is clear what the paper is about and the methods that are used. \citet{Alexander2020} uses `The Increased Effect of Elections and Changing Prime Ministers on Topics Discussed in the Australian Federal Parliament between 1901 and 2018'. While the method used in that paper is not clear from the title, the main finding it, along with a good deal of information about what the content will be. And finally, \citet{alexander2018trends} uses `Trends in Black and White Opioid Mortality in the United States, 1979--2015'.

A title is often among the last aspects of a paper to be finalized. While getting through the first draft, we would typically just use a working title that is good enough to get the job done. We then refine it over the course of redrafting. The title needs to reflect the final story of the paper, and this is not usually something that we know at the start. We are interested in striking a balance between getting our reader interested enough to read the paper, and conveying enough of the content so as to be useful \citep{hayotacademicstyle}. We can think here of classic books, such as Macaulay's \emph{History of England from the Accession of James the Second}, or Churchill's \emph{A History of the English-Speaking Peoples}. Both are clear about what the content is, and, for their target audience, spark interest.

One specific approach is the form: `Exciting content: Specific content', for instance, `Returning to their roots: Examining the performance of 'Vote Leave' in the 2016 Brexit referendum'. \citet{kennedy2020know} provides a particular nice example of this approach with `Know your population and know your model: Using model-based regression and poststratification to generalize findings beyond the observed sample', as does \citet{craiu2019hiring} with `The Hiring Gambit: In Search of the Twofer Data Scientist'. A close variant of this is `A question? And an answer'. For instance, \citet{cahill2020increase} with `What increase in modern contraceptive use is needed in FP2020 countries to reach 75\% demand satisfied by 2030? An assessment using the Accelerated Transition Method and Family Planning Estimation Model'. As one gains experience with this variant, it becomes possible to know when it is appropriate to drop the answer part yet remain effective, such as \citet{briggs2021does} with `Why Does Aid Not Target the Poorest?'. Another specific approach is `Specific content then broad content' or inversely. For instance `Rurality, elites, and support for 'Vote Leave' in the 2016 Brexit referendum' or `Support for 'Vote Leave' in the 2016 Brexit referendum, rurality and elites. This approach is used by \citet{tolley2021gender} with `Gender, municipal party politics, and Montreal's first woman mayor'.

\hypertarget{abstract}{%
\subsection{Abstract}\label{abstract}}

For a five-to-ten-page paper, a good abstract is a three to five sentence paragraph. For a longer paper the abstract can be slightly longer. The abstract needs to specify the story of the paper, and the objective of an abstract is to convey what was done and why it matters. To do this an abstract typically touches on the context of the work, its objectives, approach, and findings.

More specifically, a good recipe for an abstract is: first sentence: specify the general area of the paper and encourage the reader; second sentence: specify the dataset and methods at a general level; third sentence: specify the headline result; and a fourth sentence about implications.

We see this pattern in a variety of abstracts. For instance, \citet{tolley2021gender} draw in the reader with their first sentence by mentioning the election of the first woman mayor in 400 years. The second sentence is clear about what is done in the paper. The third paper tells the reader how it is done i.e.~a survey. And the fourth sentence adds some detail. The fifth and final sentence makes the main take-away from the paper clear.

\begin{quote}
In 2017, Montreal elected Valérie Plante, the first woman mayor in the city's 400-year history. Using this election as a case study, we show how gender did and did not influence the outcome. A survey of Montreal electors suggests that gender was not a salient factor in vote choice. Although gender did not matter much for voters, it did shape the organization of the campaign and party. We argue that Plante's victory can be explained in part by a strategy that showcased a less leader-centric party and a degendered campaign that helped counteract stereotypes about women's unsuitability for positions of political leadership.
\end{quote}

Similarly, \citet{beauregard2021antiwomen} make broader environment clear within the first two sentences, and the specific contribution of this paper to that environment. The third and fourth sentences makes the data source clear and also the main findings. The fifth and sixth sentences add specificity here that would be of interest to likely readers of this abstract i.e.~academic political science experts. And then the final sentence makes it clear the position of the authors.

\begin{quote}
Previous research on support for gender quotas focuses on attitudes toward gender equality and government intervention as explanations. We argue the role of attitudes toward women in understanding support for policies aiming to increase the presence of women in politics is ambivalent---both hostile and benevolent forms of sexism contribute in understanding support, albeit in different ways. Using original data from a survey conducted on a probability-based sample of Australian respondents, our findings demonstrate that hostile sexists are more likely to oppose increasing of women's presence in politics through the adoption of gender quotas. Benevolent sexists, on the other hand, are more likely to support these policies than respondents exhibiting low levels of benevolent sexism. We argue this is because benevolent sexism holds that women are pure and need protection; they do not have what it takes to succeed in politics without the assistance of quotas. Finally, we show that while women are more likely to support quotas, ambivalent sexism has the same relationship with support among both women and men. These findings suggest that aggregate levels of public support for gender quotas do not necessarily represent greater acceptance of gender equality generally.
\end{quote}

And finally, \citet{briggs2021does} begins with a claim that seems unquestionably true. In the second sentence he then claims to have found that it is false. The third sentence specifies the extent of this claim, and the fourth sentence details how he comes to this position, before providing more detail. The final two sentences speak broad implications and importance.

\begin{quote}
Foreign-aid projects typically have local effects, so they need to be placed close to the poor if they are to reduce poverty. I show that, conditional on local population levels, World Bank (WB) project aid targets richer parts of countries. This relationship holds over time and across world regions. I test five donor-side explanations for pro-rich targeting using a pre-registered conjoint experiment on WB Task Team Leaders (TTLs). TTLs perceive aid-receiving governments as most interested in targeting aid politically and controlling implementation. They also believe that aid works better in poorer or more remote areas, but that implementation in these areas is uniquely difficult. These results speak to debates in distributive politics, international bargaining over aid, and principal-agent issues in international organizations. The results also suggest that tweaks to WB incentive structures to make ease of project implementation less important may encourage aid to flow to poorer parts of countries.
\end{quote}

The journal \emph{Nature} provides a guide for constructing an abstract. They recommend a structure that results in an abstract of six parts, that add up to around 200 words.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  A basic introductory sentence that is comprehensible to a wide audience.
\item
  A more detailed sentence about background that is relevant to likely readers.
\item
  A sentence that states the general problem.
\item
  Sentences that summarize and then explain the main results.
\item
  A sentence about general context.
\item
  And finally, a sentence about the broader perspective.
\end{enumerate}

\hypertarget{introduction-2}{%
\subsection{Introduction}\label{introduction-2}}

An introduction needs to be self-contained and convey everything that a reader needs to know. It is important to recognize that we are not writing a mystery story. Instead, we want to give-away the most important points in the introduction. For a six-page paper, an introduction may be two or three paragraphs of main content. \citet[p.~90]{hayotacademicstyle} describes the goal of an introduction is to engage the reader, locate them in some discipline and background, and then tell them what happens in the rest of the paper. It is completely reader-focused.

The introduction should set the scene and give the reader some background. For instance, we typically start a little broader. This provides some context to the paper. We then describe how the paper fits into that context, and give some high-level results, especially focused on the one key result that is the main part of the story. We provide more detail here than we provided in the abstract, but not the full extent. And the final bit of main content is to broadly discuss next steps. Finally, we finish the introduction with an additional short final paragraph that highlights the structure of the paper.

As an example (with made-up details):

\begin{quote}
The UK Conservative Party has always done well in rural electorates. And the 2016 Brexit vote was no different with a significant different in support between rural and urban areas. But even by the standard of rural support for conservative issues, support for `Vote Leave' was unusually strong with `Vote Leave' being most heavily supported in the East Midlands and the East of England, while the strongest support for `Remain' was in Greater London.

In this paper we look at why the performance of `Vote Leave' in the 2016 Brexit referendum was so correlated with rurality. We construct a model in which support for `Vote Leave' at a voting area level, is explained by the number of farms in the area, the average internet connectivity, and the median age. We find that as the median age of an area increases, the likelihood that an area supported `Vote Leave' decreases by 14 percentage points. Future work could look at the effect of having a Conservative MP which would allow a more nuanced understanding of these effects.

The remainder of this paper is structured as follows: Section 2 discusses the data, Section 3 discusses the model, Section 4 presents the results, and finally Section 5 discusses our findings and some weaknesses.
\end{quote}

The introduction needs to be self-contained and tell your reader everything that they need to know. A reader should be able to only read the introduction and have an accurate picture of all the major aspects that they would if they were to read the whole paper. It would be rare to include graphs or tables in the introduction. An introduction always closes with the structure of the paper. For instance (and this is just a rough guide) an introduction for a 10-page paper, should probably be about 3 or 4 paragraphs, or 10 per cent, but it depends on specifics.

\hypertarget{data}{%
\subsection{Data}\label{data}}

Robert Caro, Lyndon B. Johnson's biographer, describes the importance of conveying `a sense of place' when writing biography \citep[p.~141]{caroonworking}. This he defines as `the physical setting in which a book's action is occurring: to see it clearly enough, in sufficient detail, so that he feels as if he himself were present while the action is occurring.' He provides the following example:

\begin{quote}
When Rebekah walked out the front door of that little house, there was nothing---a roadrunner streaking behind some rocks with something long and wet dangling from his beak, perhaps, or a rabbit disappearing around a bush so fast that all she really saw was the flash of a white tail---but otherwise nothing. There was no movement except for the ripple of the leaves in the scattered trees, no sound except for the constant whisper of the wind\ldots{} If Rebekah climbed, almost in desperation, the hill in the back of the house, what she saw from its crest was more hills, an endless vista of hills, hills on which there was visible not a single house\ldots{} hills on which nothing moved, empty hills with, above them, empty sky; a hawk circling silently overhead was an event. Bus most of all, there was nothing human, no one to talk to.

\citet[p.~146]{caroonworking}
\end{quote}

How thoroughly we can imagine the circumstances of Rebekah Baines Johnson (Lyndon B. Johnson's mother). We need to provide our reader with the same sense of place for our dataset.
When writing our papers, we need to achieve that same sense of place, for our data, as Caro is able to provide for the Hill county. We do this by being as explicit as possible about showing our dataset. We typically have a whole section about it and this is designed to show the reader, as closely as possible, the actual data that underpin our story.

When writing the data section, we are beginning our answer to the critical question about our claims, which is, how is it possible to know this? \citep[p.~78]{draftnumberfour}. The preeminent example of a data section is provided by \citet{doll1950smoking}, who are interested in the effect smoking between control and treatment groups. They begin by clearly describing their dataset. They then use tables to display relevant cross-tabs. And use graphs to contrast their groups.

In the data section we need to thoroughly discuss the variables in the dataset that we are using. If there are other datasets that could have been used, but were not, then these should be mentioned and our choices justified. If variables were constructed or combined, then this process and motivation should be explained.

To get a sense of the data, it is important that the reader is able to understand what the data that underpin the results look like. This means that we should graph the actual data that are used in our analysis, or as close to them as possible. And we should also include tables of summary statistics. If the dataset was created from some other source, then it can also help to include an example of that original source. For instance, if the dataset was created from survey responses then the survey form should be included, potentially in an appendix.

The data section will also have figures and tables. Here some judgment is required. While it is important that the reader has the opportunity to understand the details, it may be that some are better placed in an appendix. Figure and tables are a critical aspect of convincing people of a story. In a graph we can show the data and then let the reader decide for themselves. And using a table, we can more easily summarize our dataset. At the very least, every variable needs to be shown in a graph and summarized in a table. Figures and tables should be numbered and then cross-referenced in the text, for instance, ``Figure 1 shows\ldots{}'', ``Table 1 describes\ldots{}''. For every graph and table there should be extensive accompanying text that describes their main aspects, and adds additional detail.

We discuss the components of graphs and tables, including titles and labels, in Chapter \ref{static-communication}. But here we will discuss captions, as they are between text and the graph or table. Captions need to be informative and self-contained. As \citet[p.~57]{elementsofgraphingdata} says, the `interplay between graph, caption, and text is a delicate one', however the reader should be able to read only the caption and understand what the graph or table shows. A caption that is two of three lines long would is not necessarily inappropriate. And all aspects of the graph or table should be explained. For instance, consider Figures \ref{fig:bowleygraphisnice} and \ref{fig:bowleytableisnice} from \citet[p.~151]{bowley}, which are both exceptionally clear, and self-contained.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{/Users/rohanalexander/Documents/book/figures/HANDIRIS3} 

}

\caption{Example of a well-captioned figure}\label{fig:bowleygraphisnice}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{/Users/rohanalexander/Documents/book/figures/HANDIRIS1} 

}

\caption{Example of a well-captioned table}\label{fig:bowleytableisnice}
\end{figure}

The choice between a table and a graph comes down to how much information is to be conveyed. In general, if there is specific information that should be considered, such as a summary statistic, then a table is a good option, while if we are interested in the reader making comparisons and understanding trends then a graph is a good option \citep{gelman2002let}.

Finally, if there is relevant literature then we would discuss it throughout the paper as appropriate. For instance, when there is literature relevant to the data then it should be discussed in this section, literature relevant to the model, results, or discussion should be mentioned as appropriate in those sections. It is rarely necessary to have a separate literature review section.

\hypertarget{model}{%
\subsection{Model}\label{model}}

We will often build a statistical model that we will use to explore the data, and we often have a specific section about this. At a minimum it is important to clearly specify equation/s that describe the model being used, and explain their components with plain language and cross-references.

The model section typically begins with the model being written out, explained, and justified. Depending on the expected reader, some background may be needed. After specifying the model with appropriate mathematical notation and cross-referencing it, the components of the model are then typically defined and explained. It is especially important to define each aspect of the notation. This helps convince the reader that the model was well-chosen and enhances the credibility of the paper. The model's variables should correspond to those that were discussed in the data section, making a clear link between the two sections.

There should be some discussion of how features enter the model and why. For instance, some examples could include, why use ages rather than age-groups, why does state/province have a levels effect, and why is gender a categorical variable. In general, we are trying to convey a sense that this is the model for the situation. We want the reader to understand how the aspects that were discussed in the data section assert themselves in the modelling decisions that were made.

The model section should close with some discussion of the assumptions that underpin the model, and a brief discussion of alternative models, or variants, and strengths and weaknesses made clear. It should be clear in the reader's mind why it was this model that was chosen.

At some point in this section, it is usually appropriate to specify the software that was used to run the model, and to provide some evidence of thought about the circumstances in which the model may not be appropriate. The later point would typically be expanded on in the discussion. And there should be evidence of model validation and checking, model convergence, and/or diagnostic issues. Again, there is a balance needed here, and some of this content may be more appropriate placed in appendices.

When technical terms are used, they should be briefly explained in plain language for readers who might not be familiar with it. For instance, \citet{monicababynames} integrates an explanation of the Gini coefficient that brings the reader along.

\begin{quote}
To look at the concentration of baby names, let's calculate the Gini coefficient for each country, sex and year. The Gini coefficient measures dispersion or inequality among values of a frequency distribution. It can take any value between 0 and 1. In the case of income distributions, a Gini coefficient of 1 would mean one person has all the income. In this case, a Gini coefficient of 1 would mean that all babies have the same name. In contrast, a Gini coefficient of 0 would mean names are evenly distributed across all babies.
\end{quote}

\hypertarget{results}{%
\subsection{Results}\label{results}}

Two excellent examples of results sections provided by \citet{kharecha2013prevented} and \citet{kiang2021racial}. In the results section, we want to communicate the outcomes of the model in a clear way and without too much in the way of discussion of implications. The results section likely requires summary statistics, tables, and graphs. Each of those aspects should be cross-referenced and have text associated with them that details what is seen in them. This section should strictly relay results; that is, we are interested in what the results are, rather than what they mean.

This section would also typically include table/s of coefficient estimates based on the modelling that we used to further explore the data. Various features of the estimates should be discussed, and differences between the models explained. It may be that different subsets of the data are considered separately. Again, all graphs and tables need to have plain language text accompany them. A rough guide is that the amount of text should be at least equal to the amount of space taken up by the tables and graphs. For instance, if a full page is used to display a table of coefficient estimates, then that should be cross-referenced and accompanied by at least a full page of text about that table.

\hypertarget{discussion}{%
\subsection{Discussion}\label{discussion}}

A discussion section may be the final section of a paper and would typically have four or five sub-sections.

The discussion section would typically begin with a sub-section that comprises a one- or two-paragraph summary of what was done in the paper. This would be followed by two or three sub-sections that are devoted to the key things that we learn about the world from this paper. For instance, there are typically a few implications that come from the modelling results. These few sub-sections are the main opportunity to justify or detail the implications of the story being told in the paper. Typically, these sub-sections do not see newly introduced graphs or tables, but are instead focused on what we learn from those that were introduced in earlier sections. It may be that some of the results are discussed in relation to what others have found, and differences could be attempted to be reconciled here.

Following these sub-sections of what we learn about the world, we would typically have a sub-section focused on some of the weaknesses of what was done. This could concern aspects such as the data that were used, the approach, and the model. And the final sub-section is typically a few paragraphs that specify what is left to learn, and how future work could proceed.

In general, we would expect this section to take at least twenty-five per cent of the total paper. For instance, in an eight page paper, we would expect at least two pages of discussion.

\hypertarget{brevity-typos-and-grammar}{%
\subsection{Brevity, typos, and grammar}\label{brevity-typos-and-grammar}}

Brevity is important. Partly this is because we write for the reader, and the reader has other priorities. But it is also because as the writer it focuses us to consider what our most important points are, how we can best support them, and where our arguments are weakest. Jean Chrétien, the former Canadian Prime Minister, describes how `{[}t{]}o allow me to get to the heart of an issue quickly, I asked the officials to summarize their documents in two or three pages and attach the rest of the materials as background information. I soon discovered that this was a problem only for those who didn't really know what they were talking about.' \citep[p.~105]{jeanchretien}.

This experience is not unique to Canada. For instance, Oliver Letwin, the former British Conservative Cabinet member, describes there as being `a huge amount of terrible guff, at huge, colossal, humongous length coming from some departments' and how he asked `for them to be one quarter of the length' \citep{institueforgovernment}. He found that the departments were able to accommodate this request without losing anything important.

This experience is also not new. For instance, Churchill asked for brevity during the Second World War, saying `the discipline of setting out the real points concisely will prove an aid to clearer thinking'. And the letter from Szilard and Einstein to FDR that was the catalyst for the Manhattan Project was only two pages.

This experience is also not unique to academia. For instance, one of the foundations of Amazon, which is one of the world's largest companies, is clear writing. Specifically, instead of PowerPoint presentations, Jeff Bezos asked for `{[}w{]}ell structured, narrative text\ldots{} {[}which{]} forces better thought and better understanding of what's more important than what, and how things are related.'

\citet{zinsser} goes further and describes `the secret of good writing' being `to strip every sentence to its cleanest components.' Every sentence should be simplified to its essence. And every word that does not contribute should be removed.

Typos and other grammatical mistakes affect the credibility of claims. If the reader cannot trust us to use a spell-checker, then why should they trust us to use logistic regression? Microsoft Word and Google Docs are useful here for their spell-checkers: copy/paste from R Markdown, look for the red and green lines, and fix them in R Markdown.

We are not worried about the n-th degree of grammatical content. Instead, we are interested in grammar and sentence structure that occurs in conversational language use \citep[p.~118]{stephenking}. The way to develop that comfort is by reading a lot, and asking others to read your work also.

Unnecessary words, typos, and grammatical issues should be removed from papers with a fanatical zeal.

\hypertarget{rules}{%
\subsection{Rules}\label{rules}}

A variety of authors have established rules for writing, including famously, \citet{politicsandtheenglishlanguage}, which were reimagined by \citet{johnsontheeconomist}. And \citet{fiske2021words} have a list of rules for scientific papers. A further reimagining, focused on telling stories with data, could be:

\begin{itemize}
\tightlist
\item
  Focus on the reader and their needs. Everything else is comment.
\item
  Establish a logical structure and rely on that structure to tell the story.
\item
  Write a first draft as quickly as possible.
\item
  Re-write that extensively and without favor.
\item
  Aim to be concise and direct. Remove as many words as possible.
\item
  Using words precisely. Stock-markets rise or fall, not improve or worsen.
\item
  Use short sentence where possible.
\item
  Avoid jargon.
\item
  Write as though your work will be on the front page of a newspaper. Because it could be.
\end{itemize}

\hypertarget{exercises-and-tutorial-4}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-4}}

\hypertarget{exercises-4}{%
\subsection{Exercises}\label{exercises-4}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  According to \citet{king2006publication}, what is the key task of subheadings (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Enable a reader who randomly falls asleep but keeps turning pages to know where they are.
  \item
    Be broad and sweeping so that a reader is impressed by the importance of the paper.
  \item
    Use acronyms to integrate the paper into the literature.
  \end{enumerate}
\item
  According to \citet{king2006publication}, what is the maximum length of an abstract (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Two hundred words.
  \item
    Two hundred and fifty words.
  \item
    One hundred words.
  \item
    One hundred and fifty words.
  \end{enumerate}
\item
  According to \citet{king2006publication}, in a paper, raw computer output should be (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Commented out.
  \item
    Not included.
  \item
    Included.
  \end{enumerate}
\item
  According to \citet{king2006publication}, if our standard error was 0.05 then which of the following specificity for a coefficient would be silly (select all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    2.7182818
  \item
    2.718282
  \item
    2.72
  \item
    2.7
  \item
    2.7183
  \item
    2.718
  \item
    3
  \item
    2.71828
  \end{enumerate}
\item
  When should we try not to use the `delete' key (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    While writing the first draft.
  \item
    While writing the second draft.
  \item
    While writing the third draft.
  \item
    The `delete' key should always be used.
  \end{enumerate}
\item
  How long should a first draft take to write of a five-to-ten-page paper (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    One hour
  \item
    One day
  \item
    One week
  \item
    One month
  \end{enumerate}
\item
  What is a key aspect of the re-drafting process (select all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Going through it with a red pen to remove unneeded words.
  \item
    Printing the paper and reading a physical copy.
  \item
    Cutting and pasting to enhance flow.
  \item
    Reading it aloud.
  \item
    Exchanging it with others.
  \end{enumerate}
\item
  What are three features of a good research question (write a paragraph or two)?
\item
  What are some of the challenges of being `data-first' (write a paragraph or two)?
\item
  What are some of the challenges of being `question-first' (write a paragraph or two)?
\item
  What is a counterfactual (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    If-then statements in which the if does not happen.
  \item
    If-then statements in which the if happens.
  \item
    Statements that are either true or false.
  \item
    Statements that are neither true or false.
  \end{enumerate}
\item
  Which of the following is the best title (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    ``Problem Set 1''
  \item
    ``Unemployment''
  \item
    ``Examining England's Unemployment (2010-2020)''
  \item
    ``England's Unemployment Increased between 2010 and 2020''
  \end{enumerate}
\item
  Which of the following is the best title (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    ``Problem Set 2''
  \item
    ``Standard errors''
  \item
    ``On standard errors with small samples''
  \end{enumerate}
\item
  Which word/s can be removed from the following sentence without affecting its meaning (select all that apply)? `Like many parents, when our children were born, one of the first things that my wife and I did regularly was read stories to them.'

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    first
  \item
    regularly
  \item
    stories
  \end{enumerate}
\item
  Please write a new title for either \citet{barron2018individuals} or \citet{fourcade2017seeing}.
\item
  Please write a new title for the first article from the list of articles from \emph{The New Yorker} that you read.
\item
  Please write a new title for the other article from the list of articles from \emph{The New Yorker} that you read.
\item
  Please write a new four-sentence abstract for \citet{chambliss1989mundanity}
\item
  Please write a new four-sentence abstract for \citet{doll1950smoking} or \citet{student1908probable} or \citet{kharecha2013prevented}.
\item
  Please write an abstract for the first article from the list of `miscellaneous' articles that you read.
\item
  Please write an abstract for the other article from the list of `miscellaneous' articles that you read.
\item
  Using only the 1000-most popular words in the English language -- \url{https://xkcd.com/simplewriter/} -- re-write the following so that it retains its original meaning:
\end{enumerate}

\begin{quote}
When using data, we try to tell a convincing story. It may be as exciting as predicting elections, as banal as increasing internet advertising click rates, as serious as finding the cause of a disease, or as fun as forecasting basketball games. In any case the key elements are the same.
\end{quote}

\hypertarget{tutorial-4}{%
\subsection{Tutorial}\label{tutorial-4}}

\citet[p.~xii]{caroonworking} writes at least one thousand words almost every day. In this tutorial we will write every day for a week. Begin by picking seven of the well-written papers specified above. Each day complete the following tasks:

\begin{itemize}
\tightlist
\item
  Transcribe, by writing each word yourself, the entire introduction.
\item
  (This idea comes from \citet[p.~186]{draftnumberfour}.) Re-write the introduction so that it is five lines (or 10 per cent, whichever is less) shorter.
\item
  Transcribe, by writing each word yourself, the abstract.
\item
  Re-write a new, four-sentence, abstract for the paper.
\item
  (This idea comes from comes from Chelsea Parlett-Pelleriti.) Write a second version of your new abstract using only the one-thousand most popular words in the English language: \url{https://xkcd.com/simplewriter/}.
\item
  Detail three points about the way the paper is written that you like
\item
  Detail one point about the way the paper is written that you do not like.
\end{itemize}

Submit all seven papers.

\hypertarget{static-communication}{%
\chapter{Static communication}\label{static-communication}}

\textbf{Required material}

\begin{itemize}
\tightlist
\item
  Read \emph{R for Data Science}, Chapter 28 `Graphics for communication', \citep{r4ds}.
\item
  Read \emph{Data Visualization: A Practical Introduction}, Chapters 3 `Make a plot', 4 `Show the right numbers', and 5 `Graph tables, add labels, make notes', \citep{healyviz}.
\item
  Read \emph{Testing Statistical Charts: What Makes a Good Graph?}, \citep{vanderplas2020testing}.
\item
  Read \emph{Data Feminism}, Chapter 3 `On Rational, Scientific, Objective Viewpoints from Mythical, Imaginary, Impossible Standpoints', \citep{datafeminism2020}.
\end{itemize}

\textbf{Key concepts and skills}

\begin{itemize}
\tightlist
\item
  Knowing the importance of showing the reader the actual dataset, or as close as is possible.
\item
  Using a variety of different graph options, including bar charts, scatterplots, line plots, and histograms.
\item
  Knowing how to use tables to show part of a dataset, communicate summary statistics, and display regression results.
\item
  Approaching maps as a type of a graph.
\item
  Comfort with geocoding places.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{datasauRus} \citep{citedatasauRus}
\item
  \texttt{ggmap} \citep{KahleWickham2013}
\item
  \texttt{kableExtra} \citep{citekableextra}
\item
  \texttt{knitr} \citep{citeknitr}
\item
  \texttt{maps}
\item
  \texttt{modelsummary} \citep{citemodelsummary}
\item
  \texttt{opendatatoronto} \citep{citeSharla}
\item
  \texttt{patchwork} \citep{citepatchwork}
\item
  \texttt{tidyverse} \citep{citetidyverse}
\item
  \texttt{viridis} \citep{viridis}
\item
  \texttt{WDI} \citep{WDI}
\end{itemize}

\textbf{Key functions}

\begin{itemize}
\tightlist
\item
  \texttt{ggmap::get\_googlemap()}
\item
  \texttt{ggmap::get\_stamenmap()}
\item
  \texttt{ggmap::ggmap()}
\item
  \texttt{ggplot2::coord\_map()}
\item
  \texttt{ggplot2::facet\_wrap()}
\item
  \texttt{ggplot2::geom\_abline()}
\item
  \texttt{ggplot2::geom\_bar()}
\item
  \texttt{ggplot2::geom\_boxplot()}
\item
  \texttt{ggplot2::geom\_dotplot()}
\item
  \texttt{ggplot2::geom\_freqpoly()}
\item
  \texttt{ggplot2::geom\_histogram()}
\item
  \texttt{ggplot2::geom\_jitter()}
\item
  \texttt{ggplot2::geom\_line()}
\item
  \texttt{ggplot2::geom\_path()}
\item
  \texttt{ggplot2::geom\_point()}
\item
  \texttt{ggplot2::geom\_polygon()}
\item
  \texttt{ggplot2::geom\_smooth()}
\item
  \texttt{ggplot2::geom\_step()}
\item
  \texttt{ggplot2::ggplot()}
\item
  \texttt{ggplot2::ggsave()}
\item
  \texttt{ggplot2::labeller()}
\item
  \texttt{ggplot2::labs()}
\item
  \texttt{ggplot2::map\_data()}
\item
  \texttt{ggplot2::scale\_color\_brewer()}
\item
  \texttt{ggplot2::scale\_colour\_viridis\_d()}
\item
  \texttt{ggplot2::scale\_fill\_brewer()}
\item
  \texttt{ggplot2::scale\_fill\_viridis()}
\item
  \texttt{ggplot2::stat\_qq()}
\item
  \texttt{ggplot2::stat\_qq\_line()}
\item
  \texttt{ggplot2::theme()}
\item
  \texttt{ggplot2::theme\_bw()}
\item
  \texttt{ggplot2::theme\_classic()}
\item
  \texttt{ggplot2::theme\_linedraw()}
\item
  \texttt{ggplot2::theme\_minimal()}
\item
  \texttt{kableExtra::add\_header\_above()}
\item
  \texttt{knitr::kable()}
\item
  \texttt{lm()}
\item
  \texttt{maps::map()}
\item
  \texttt{modelsummary::datasummary()}
\item
  \texttt{modelsummary::datasummary\_balance()}
\item
  \texttt{modelsummary::datasummary\_correlation()}
\item
  \texttt{modelsummary::datasummary\_skim()}
\item
  \texttt{modelsummary::modelsummary()}
\item
  \texttt{WDI::WDI()}
\item
  \texttt{WDI::WDIsearch()}
\end{itemize}

\hypertarget{introduction-3}{%
\section{Introduction}\label{introduction-3}}

When telling stories with data, we would like the data to do much of the work of convincing our reader. The paper is the medium, and the data are the message. To that end, we want to try to show our reader the data that allowed us to come to our understanding of the story. We use graphs, tables, and maps to help achieve this.

The critical task is to show the actual data that underpin our analysis, or as close to it as we can. For instance, if our dataset consists of 2,500 responses to a survey, then at some point in our paper we would expect a graph that contains 2,500 points. To do this we build graphs using \texttt{ggplot2} \citep{citeggplot}. We will go through a variety of different options here including bar charts, scatterplots, line plots, and histograms.

In contrast to the role of graphs, which is to show the actual data, or as close to it as possible, the role of tables is typically to show an extract of the dataset or convey various summary statistics. We will build tables using \texttt{knitr} \citep{citeknitr} and \texttt{kableExtra} \citep{citekableextra} initially, and then \texttt{modelsummary} \citep{citemodelsummary}.

Finally, we cover maps as a variant of graphs that are used to show a particular type of data. We will build static maps using \texttt{ggmap} \citep{KahleWickham2013}, having obtained the geocoded data that we need using \texttt{tidygeocoder} \citep{citetidygeocoder}.

\hypertarget{graphs}{%
\section{Graphs}\label{graphs}}

Graphs are a critical aspect of compelling stories told with data.

\begin{quote}
Graphs allow us to explore data to see overall patterns and to see detailed behavior; no other approach can compete in revealing the structure of data so thoroughly. Graphs allow us to view complex mathematical models fitted to data, and they allow us to assess the validity of such models.

\citet[p.~5]{elementsofgraphingdata}
\end{quote}

In a way, the graphing of data is an information coding process where we create a glyph, or purposeful mark, that we mean to convey information to our audience. The audience must decode our glyph. The success of our graph turns on how much information is lost in this process. It is the decoding that is the critical aspect \citep[p.~221]{elementsofgraphingdata}, which means that we are creating graphs for the audience. If nothing else is possible, the most important feature is to convey as much of the actual data as possible.

To see why this is important we begin by using the dataset `datasaurus\_dozen' from \texttt{datasauRus} \citep{citedatasauRus}. After installing and loading the necessary packages, we can take a quick look at the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}datasauRus\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(datasauRus)}

\FunctionTok{head}\NormalTok{(datasaurus\_dozen)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{}   dataset     x     y}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}   \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 dino     55.4  97.2}
\CommentTok{\#\textgreater{} 2 dino     51.5  96.0}
\CommentTok{\#\textgreater{} 3 dino     46.2  94.5}
\CommentTok{\#\textgreater{} 4 dino     42.8  91.4}
\CommentTok{\#\textgreater{} 5 dino     40.8  88.3}
\CommentTok{\#\textgreater{} 6 dino     38.7  84.9}
\NormalTok{datasaurus\_dozen }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{count}\NormalTok{(dataset)}
\CommentTok{\#\textgreater{} \# A tibble: 13 x 2}
\CommentTok{\#\textgreater{}    dataset        n}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}      \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{}  1 away         142}
\CommentTok{\#\textgreater{}  2 bullseye     142}
\CommentTok{\#\textgreater{}  3 circle       142}
\CommentTok{\#\textgreater{}  4 dino         142}
\CommentTok{\#\textgreater{}  5 dots         142}
\CommentTok{\#\textgreater{}  6 h\_lines      142}
\CommentTok{\#\textgreater{}  7 high\_lines   142}
\CommentTok{\#\textgreater{}  8 slant\_down   142}
\CommentTok{\#\textgreater{}  9 slant\_up     142}
\CommentTok{\#\textgreater{} 10 star         142}
\CommentTok{\#\textgreater{} 11 v\_lines      142}
\CommentTok{\#\textgreater{} 12 wide\_lines   142}
\CommentTok{\#\textgreater{} 13 x\_shape      142}
\end{Highlighting}
\end{Shaded}

We can see that the dataset consists of values for `x' and `y', which should be plotted on the x-axis and y-axis, respectively. We can further see that there are thirteen different values in the variable `dataset' including: ``dino'', ``star'', ``away'', and ``bullseye''. We will focus on those four and generate summary statistics for each (Table \ref{tab:datasaurussummarystats}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# From Julia Silge: }
\CommentTok{\# https://juliasilge.com/blog/datasaurus{-}multiclass/}
\NormalTok{datasaurus\_dozen }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{filter}\NormalTok{(dataset }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"dino"}\NormalTok{, }\StringTok{"star"}\NormalTok{, }\StringTok{"away"}\NormalTok{, }\StringTok{"bullseye"}\NormalTok{)) }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(dataset) }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(x, y),}
                   \FunctionTok{list}\NormalTok{(}\AttributeTok{mean =}\NormalTok{ mean,}
                        \AttributeTok{sd =}\NormalTok{ sd)),}
            \AttributeTok{x\_y\_cor =} \FunctionTok{cor}\NormalTok{(x, y)) }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}
    \AttributeTok{caption =} 
      \StringTok{"Mean and standard deviation for four \textquotesingle{}datasaurus\textquotesingle{} datasets"}\NormalTok{,}
    \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Dataset"}\NormalTok{, }
                  \StringTok{"x mean"}\NormalTok{, }
                  \StringTok{"x sd"}\NormalTok{, }
                  \StringTok{"y mean"}\NormalTok{, }
                  \StringTok{"y sd"}\NormalTok{, }
                  \StringTok{"correlation"}\NormalTok{),}
    \AttributeTok{digits =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{linesep =} \StringTok{""}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:datasaurussummarystats}Mean and standard deviation for four 'datasaurus' datasets}
\centering
\begin{tabular}[t]{lrrrrr}
\toprule
Dataset & x mean & x sd & y mean & y sd & correlation\\
\midrule
away & 54.3 & 16.8 & 47.8 & 26.9 & -0.1\\
bullseye & 54.3 & 16.8 & 47.8 & 26.9 & -0.1\\
dino & 54.3 & 16.8 & 47.8 & 26.9 & -0.1\\
star & 54.3 & 16.8 & 47.8 & 26.9 & -0.1\\
\bottomrule
\end{tabular}
\end{table}

Despite the similarities of the summary statistics, it turns out the different `datasets' are actually very different beasts when we graph the actual data (Figure \ref{fig:datasaurusgraph}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{datasaurus\_dozen }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(dataset }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"dino"}\NormalTok{, }\StringTok{"star"}\NormalTok{, }\StringTok{"away"}\NormalTok{, }\StringTok{"bullseye"}\NormalTok{)) }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x, }\AttributeTok{y=}\NormalTok{y, }\AttributeTok{colour=}\NormalTok{dataset)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(dataset), }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{colour =} \StringTok{"Dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/datasaurusgraph-1.pdf}
\caption{\label{fig:datasaurusgraph}Graph of four `datasaurus' datasets}
\end{figure}

This is a variant of the famous `Anscombe's Quartet'. The key takeaway is that it is important to plot the actual data and not rely on summary statistics. The `anscombe' dataset is built into R.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(anscombe)}
\CommentTok{\#\textgreater{}   x1 x2 x3 x4   y1   y2    y3   y4}
\CommentTok{\#\textgreater{} 1 10 10 10  8 8.04 9.14  7.46 6.58}
\CommentTok{\#\textgreater{} 2  8  8  8  8 6.95 8.14  6.77 5.76}
\CommentTok{\#\textgreater{} 3 13 13 13  8 7.58 8.74 12.74 7.71}
\CommentTok{\#\textgreater{} 4  9  9  9  8 8.81 8.77  7.11 8.84}
\CommentTok{\#\textgreater{} 5 11 11 11  8 8.33 9.26  7.81 8.47}
\CommentTok{\#\textgreater{} 6 14 14 14  8 9.96 8.10  8.84 7.04}
\end{Highlighting}
\end{Shaded}

It consists of six observations for four different datasets, again with x and y values for each observation. We need to manipulate this dataset with \texttt{pivot\_longer()} to get it into a `tidy format'.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# From Nick Tierney: }
\CommentTok{\# https://www.njtierney.com/post/2020/06/01/tidy{-}anscombe/}
\CommentTok{\# Code from pivot\_longer() vignette.}
\NormalTok{tidy\_anscombe }\OtherTok{\textless{}{-}} 
\NormalTok{  anscombe }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{pivot\_longer}\NormalTok{(}\FunctionTok{everything}\NormalTok{(),}
               \AttributeTok{names\_to =} \FunctionTok{c}\NormalTok{(}\StringTok{".value"}\NormalTok{, }\StringTok{"set"}\NormalTok{),}
               \AttributeTok{names\_pattern =} \StringTok{"(.)(.)"}
\NormalTok{               )}
\end{Highlighting}
\end{Shaded}

We can again first create some summary statistics (Table \ref{tab:anscombesummarystats}) and then graph the data (Figure \ref{fig:anscombegraph}). And we again see the importance of graphing the actual data, rather than relying on summary statistics.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_anscombe }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(set) }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(x, y),}
                   \FunctionTok{list}\NormalTok{(}\AttributeTok{mean =}\NormalTok{ mean, }\AttributeTok{sd =}\NormalTok{ sd)),}
            \AttributeTok{x\_y\_cor =} \FunctionTok{cor}\NormalTok{(x, y)) }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}
    \AttributeTok{caption =} \StringTok{"Mean and standard deviation for Anscombe"}\NormalTok{,}
    \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Dataset"}\NormalTok{, }
                  \StringTok{"x mean"}\NormalTok{, }
                  \StringTok{"x sd"}\NormalTok{, }
                  \StringTok{"y mean"}\NormalTok{, }
                  \StringTok{"y sd"}\NormalTok{, }
                  \StringTok{"correlation"}\NormalTok{),}
    \AttributeTok{digits =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{linesep =} \StringTok{""}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:anscombesummarystats}Mean and standard deviation for Anscombe}
\centering
\begin{tabular}[t]{lrrrrr}
\toprule
Dataset & x mean & x sd & y mean & y sd & correlation\\
\midrule
1 & 9 & 3.3 & 7.5 & 2 & 0.8\\
2 & 9 & 3.3 & 7.5 & 2 & 0.8\\
3 & 9 & 3.3 & 7.5 & 2 & 0.8\\
4 & 9 & 3.3 & 7.5 & 2 & 0.8\\
\bottomrule
\end{tabular}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_anscombe }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y, }\AttributeTok{colour =}\NormalTok{ set)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(set), }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{colour =} \StringTok{"Dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/anscombegraph-1.pdf}
\caption{\label{fig:anscombegraph}Recreation of Anscombe's Quartet}
\end{figure}

\hypertarget{bar-charts}{%
\subsection{Bar charts}\label{bar-charts}}

We typically use a bar chart when we have a categorical variable that we want to focus on. We saw an example of this in Chapter \ref{drinking-from-a-fire-hose} where we constructed a graph of the number of occupied beds. The geom that we primarily use is \texttt{geom\_bar()}, but there are many variants to cater for specific situations.

We will use a dataset from the 1997-2001 British Election Panel Study that was put together by \citet{fox2006effect}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Vincent Arel Bundock provides access to this dataset.}
\NormalTok{beps }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}
    \AttributeTok{file =} 
      \StringTok{"https://vincentarelbundock.github.io/Rdatasets/csv/carData/BEPS.csv"}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(beps)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 11}
\CommentTok{\#\textgreater{}    ...1 vote     age economic.cond.n\textasciitilde{} economic.cond.h\textasciitilde{} Blair}
\CommentTok{\#\textgreater{}   \textless{}dbl\textgreater{} \textless{}chr\textgreater{}  \textless{}dbl\textgreater{}            \textless{}dbl\textgreater{}            \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1     1 Liber\textasciitilde{}    43                3                3     4}
\CommentTok{\#\textgreater{} 2     2 Labour    36                4                4     4}
\CommentTok{\#\textgreater{} 3     3 Labour    35                4                4     5}
\CommentTok{\#\textgreater{} 4     4 Labour    24                4                2     2}
\CommentTok{\#\textgreater{} 5     5 Labour    41                2                2     1}
\CommentTok{\#\textgreater{} 6     6 Labour    47                3                4     4}
\CommentTok{\#\textgreater{} \# ... with 5 more variables: Hague \textless{}dbl\textgreater{}, Kennedy \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   Europe \textless{}dbl\textgreater{}, political.knowledge \textless{}dbl\textgreater{}, gender \textless{}chr\textgreater{}}
\end{Highlighting}
\end{Shaded}

The dataset consists of which party the person supports, along with various demographic, economic, and political variables. In particular, we have the age of the respondents. We could begin by making a graph of this age distribution using \texttt{geom\_bar()} (Figure \ref{fig:bepfitst}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beps }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/bepfitst-1.pdf}
\caption{\label{fig:bepfitst}Distribution of ages in the 1997-2001 British Election Panel Study}
\end{figure}

By default, \texttt{geom\_bar()} has created a count of the number of times each age appears in the dataset. It does this because the default `stat' for \texttt{geom\_bar()} is `count'. This saves us from having to create that statistic ourselves. But if we had already constructed a count (for instance, with \texttt{beps\ \textbar{}\textgreater{}\ count(age)}), then we could also specify a column of values for the y-axis and then use \texttt{stat\ =\ "identity"}.

We may also like to consider different groupings of the data, for instance, looking at age-groups by which party the respondent supports (Figure \ref{fig:bepsecond}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beps }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/bepsecond-1.pdf}
\caption{\label{fig:bepsecond}Distribution of ages, and vote preference, in the 1997-2001 British Election Panel Study}
\end{figure}

The default is that these different groups are stacked, but they can be placed side-by-side with \texttt{position\ =\ "dodge"} (Figure \ref{fig:bepthird}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beps }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/bepthird-1.pdf}
\caption{\label{fig:bepthird}Distribution of ages, and vote preference, in the 1997-2001 British Election Panel Study}
\end{figure}

At this point, we may like to address the general look of the graph. There are various themes that are built into \texttt{ggplot2}. Some of these include \texttt{theme\_bw()}, \texttt{theme\_classic()}, \texttt{theme\_dark()}, and \texttt{theme\_minimal()}. A full list is available at the \texttt{ggplot2} \href{https://github.com/rstudio/cheatsheets/blob/main/data-visualization.pdf}{cheatsheet}. We can use these themes by adding them as a layer (Figure \ref{fig:bepthemes}). Here we can use \texttt{patchwork} \citep{citepatchwork} to bring together multiple graphs. To do this we assign the graph to a name, and then use `+' to signal which should be next to each other, `/' to signal which would be on top, and brackets for precedence.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(patchwork)}

\NormalTok{theme\_bw }\OtherTok{\textless{}{-}} 
\NormalTok{  beps }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}

\NormalTok{theme\_classic }\OtherTok{\textless{}{-}} 
\NormalTok{  beps }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}

\NormalTok{theme\_dark }\OtherTok{\textless{}{-}} 
\NormalTok{  beps }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_dark}\NormalTok{()}

\NormalTok{theme\_minimal }\OtherTok{\textless{}{-}} 
\NormalTok{  beps }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}

\NormalTok{(theme\_bw }\SpecialCharTok{+}\NormalTok{ theme\_classic) }\SpecialCharTok{/}\NormalTok{ (theme\_dark }\SpecialCharTok{+}\NormalTok{ theme\_minimal)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/bepthemes-1.pdf}
\caption{\label{fig:bepthemes}Distribution of ages, and vote preference, in the 1997-2001 British Election Panel Study, illustrating different themes}
\end{figure}

We can install themes from other packages, including \texttt{ggthemes} \citep{ggthemes}, and \texttt{hrbrthemes} \citep{hrbrthemes}. And we can also build our own.

The default labels use dby \texttt{ggplot2} are from the name of the relevant variable, and it is often useful to add more detail. We could add a title and caption at this point. A caption can be useful to add information about the source of the dataset. A title can be useful when the graph is going to be considered outside of the context of our paper. But in the case of a graph that will be included in a paper, the need to cross-reference all graphs that are in a paper means that included a title within \texttt{labs()} is unnecessary (Figure \ref{fig:withnicelabels}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beps }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age of respondent"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of respondents"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Voted for"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Distribution of ages, and vote preference, in the 1997{-}2001 British Election Panel Study"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Source: 1997{-}2001 British Election Panel Study."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/withnicelabels-1.pdf}
\caption{\label{fig:withnicelabels}Distribution of ages, and vote preference, in the 1997-2001 British Election Panel Study}
\end{figure}

We use facets to create `many little graphics that are variations of a single graphic' \citep[p.~219]{grammarofgraphics}. They are especially useful when we want to specifically compare across some variable, but have already used color. For instance, we may be interested to explain vote, by age and gender (Figure \ref{fig:facets}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beps }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age of respondent"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of respondents"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Voted for"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(gender))}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/facets-1.pdf}
\caption{\label{fig:facets}Distribution of age by gender, and vote preference, in the 1997-2001 British Election Panel Study}
\end{figure}

We could change \texttt{facet\_wrap()} to wrap vertically instead of horizontally with \texttt{dir\ =\ "v"}. Alternatively, we could specify a number of rows, say \texttt{nrow\ =\ 2}, or a number of columns, say \texttt{ncol\ =\ 2}. Additionally, by default, both facets will have the same scales. We could enable both facets to have different scales with \texttt{scales\ =\ "free"}, or just the x-axis \texttt{scales\ =\ "free\_x"}, or just the y-axis \texttt{scales\ =\ "free\_y"} (Figure \ref{fig:facetsfancy}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beps }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age of respondent"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of respondents"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Voted for"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(gender),}
             \AttributeTok{dir =} \StringTok{"v"}\NormalTok{,}
             \AttributeTok{scales =} \StringTok{"free"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/facetsfancy-1.pdf}
\caption{\label{fig:facetsfancy}Distribution of age by gender, and vote preference, in the 1997-2001 British Election Panel Study}
\end{figure}

Finally, we can change the labels of the facets using \texttt{labeller()} (Figure \ref{fig:facetsfancylabels}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new\_labels }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\AttributeTok{female =} \StringTok{"Female"}\NormalTok{, }\AttributeTok{male =} \StringTok{"Male"}\NormalTok{)}

\NormalTok{beps }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age of respondent"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of respondents"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Voted for"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(gender),}
             \AttributeTok{dir =} \StringTok{"v"}\NormalTok{,}
             \AttributeTok{scales =} \StringTok{"free"}\NormalTok{,}
             \AttributeTok{labeller =} \FunctionTok{labeller}\NormalTok{(}\AttributeTok{gender =}\NormalTok{ new\_labels))}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/facetsfancylabels-1.pdf}
\caption{\label{fig:facetsfancylabels}Distribution of age by gender, and vote preference, in the 1997-2001 British Election Panel Study}
\end{figure}

There are a variety of different ways to change the colors, and many palettes are available including from \texttt{RColorBrewer} \citep{RColorBrewer}, which we specify with \texttt{scale\_fill\_brewer()}, and \texttt{viridis} \citep{viridis}, which we specify with \texttt{scale\_fill\_viridis()} and is particularly focused on color-blind palettes (Figure \ref{fig:usecolor}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(viridis)}
\FunctionTok{library}\NormalTok{(patchwork)}

\NormalTok{RColorBrewerBrBG }\OtherTok{\textless{}{-}} 
\NormalTok{  beps }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age of respondent"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of respondents"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Voted for"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Blues"}\NormalTok{)}

\NormalTok{RColorBrewerSet2 }\OtherTok{\textless{}{-}} 
\NormalTok{  beps }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age of respondent"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of respondents"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Voted for"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}

\NormalTok{viridis }\OtherTok{\textless{}{-}} 
\NormalTok{  beps }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age of respondent"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of respondents"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Voted for"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{scale\_fill\_viridis}\NormalTok{(}\AttributeTok{discrete =} \ConstantTok{TRUE}\NormalTok{)}

\NormalTok{viridismagma }\OtherTok{\textless{}{-}} 
\NormalTok{  beps }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age of respondent"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Voted for"}\NormalTok{) }\SpecialCharTok{+}
   \FunctionTok{scale\_fill\_viridis}\NormalTok{(}\AttributeTok{discrete =} \ConstantTok{TRUE}\NormalTok{, }
                      \AttributeTok{option =} \StringTok{"magma"}\NormalTok{)}

\NormalTok{RColorBrewerBrBG }\SpecialCharTok{/} 
\NormalTok{  RColorBrewerSet2 }\SpecialCharTok{/}
\NormalTok{  viridis }\SpecialCharTok{/}
\NormalTok{  viridismagma}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/usecolor-1.pdf}
\caption{\label{fig:usecolor}Distribution of age and vote preference, in the 1997-2001 British Election Panel Study}
\end{figure}

Details of the variety of palettes available in \texttt{RColorBrewer} and \texttt{viridis} are available in their help files. Many different palettes are available, and we can also build our own. That said, color is something to be considered with a great deal of care and it should only be added to increase the amount of information that is communicated \citep{elementsofgraphingdata}. Colors should not be added to graphs unnecessarily---that is to say, they must play some role. Typically, that role is to distinguish different groups, and that implies making the colors dissimilar. Colors may also be appropriate if there is some relationship between the color and the variable, for instance if making a graph of sales of, say, mangoes and raspberries, it could help the reader if the colors were yellow and red, respectively \citep[p.~121]{franconeri2021science}.

\hypertarget{scatterplots}{%
\subsection{Scatterplots}\label{scatterplots}}

We are often interested in the relationship between two variables. We can use scatterplots to show this information. Unless there is a good reason to move to a different option, a scatterplot is almost always the best choice \citep{weissgerber2015beyond}. Indeed, `among all forms of statistical graphics, the scatterplot may be considered the most versatile and generally useful invention in the entire history of statistical graphics.' \citep[p.~121]{historyofdataviz} To illustrate scatterplots, we use \texttt{WDI} \citep{WDI} to download some economic indicators from the World Bank.

\begin{quote}
\textbf{Oh, you think we have good data on that!} Gross Domestic Product (GDP) `combines in a single figure, and with no double counting, all the output (or production) carried out by all the firms, non-profit institutions, government bodies and households in a given country during a given period, regardless of the type of goods and services produced, provided that the production takes place within the country's economic territory' (\citet{EssentialMacroAggregates}, p.~15). The modern concept was developed by Simon Kuznets and is widely used and reported. There is a certain comfort in having a definitive and concrete single number to describe something as complicated as the entire economic activity of a country. And it is crucial that we have such summary statistics. But as with any summary statistic, its strength is also its weakness. A single number necessarily loses information about constituent components, and these distributional differences are critical. It highlights short term economic progress over longer term improvements. And `the quantitative definiteness of the estimates makes it easy to forget their dependence upon imperfect data and the consequently wide margins of possible error to which both totals and components are liable' \citep[p.~xxvi]{NationalIncomeAndItsComposition}. Reliance on any one summary measure of economic performance presents a misguided picture not only of a country's economy, but also of its peoples.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}WDI\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(WDI)}
\FunctionTok{WDIsearch}\NormalTok{(}\StringTok{"gdp growth"}\NormalTok{)}
\CommentTok{\#\textgreater{}      indicator             }
\CommentTok{\#\textgreater{} [1,] "5.51.01.10.gdp"      }
\CommentTok{\#\textgreater{} [2,] "6.0.GDP\_growth"      }
\CommentTok{\#\textgreater{} [3,] "NV.AGR.TOTL.ZG"      }
\CommentTok{\#\textgreater{} [4,] "NY.GDP.MKTP.KD.ZG"   }
\CommentTok{\#\textgreater{} [5,] "NY.GDP.MKTP.KN.87.ZG"}
\CommentTok{\#\textgreater{}      name                                    }
\CommentTok{\#\textgreater{} [1,] "Per capita GDP growth"                 }
\CommentTok{\#\textgreater{} [2,] "GDP growth (annual \%)"                 }
\CommentTok{\#\textgreater{} [3,] "Real agricultural GDP growth rates (\%)"}
\CommentTok{\#\textgreater{} [4,] "GDP growth (annual \%)"                 }
\CommentTok{\#\textgreater{} [5,] "GDP growth (annual \%)"}
\FunctionTok{WDIsearch}\NormalTok{(}\StringTok{"inflation"}\NormalTok{)}
\CommentTok{\#\textgreater{}      indicator             }
\CommentTok{\#\textgreater{} [1,] "FP.CPI.TOTL.ZG"      }
\CommentTok{\#\textgreater{} [2,] "FP.FPI.TOTL.ZG"      }
\CommentTok{\#\textgreater{} [3,] "FP.WPI.TOTL.ZG"      }
\CommentTok{\#\textgreater{} [4,] "NY.GDP.DEFL.87.ZG"   }
\CommentTok{\#\textgreater{} [5,] "NY.GDP.DEFL.KD.ZG"   }
\CommentTok{\#\textgreater{} [6,] "NY.GDP.DEFL.KD.ZG.AD"}
\CommentTok{\#\textgreater{}      name                                               }
\CommentTok{\#\textgreater{} [1,] "Inflation, consumer prices (annual \%)"            }
\CommentTok{\#\textgreater{} [2,] "Inflation, food prices (annual \%)"                }
\CommentTok{\#\textgreater{} [3,] "Inflation, wholesale prices (annual \%)"           }
\CommentTok{\#\textgreater{} [4,] "Inflation, GDP deflator (annual \%)"               }
\CommentTok{\#\textgreater{} [5,] "Inflation, GDP deflator (annual \%)"               }
\CommentTok{\#\textgreater{} [6,] "Inflation, GDP deflator: linked series (annual \%)"}
\FunctionTok{WDIsearch}\NormalTok{(}\StringTok{"population, total"}\NormalTok{)}
\CommentTok{\#\textgreater{}           indicator                name }
\CommentTok{\#\textgreater{}       "SP.POP.TOTL" "Population, total"}
\FunctionTok{WDIsearch}\NormalTok{(}\StringTok{"Unemployment, total"}\NormalTok{)}
\CommentTok{\#\textgreater{}      indicator          }
\CommentTok{\#\textgreater{} [1,] "SL.UEM.TOTL.NE.ZS"}
\CommentTok{\#\textgreater{} [2,] "SL.UEM.TOTL.ZS"   }
\CommentTok{\#\textgreater{}      name                                                                 }
\CommentTok{\#\textgreater{} [1,] "Unemployment, total (\% of total labor force) (national estimate)"   }
\CommentTok{\#\textgreater{} [2,] "Unemployment, total (\% of total labor force) (modeled ILO estimate)"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{WDI}\NormalTok{(}\AttributeTok{indicator =} \FunctionTok{c}\NormalTok{(}\StringTok{"FP.CPI.TOTL.ZG"}\NormalTok{,}
                    \StringTok{"NY.GDP.MKTP.KD.ZG"}\NormalTok{,}
                    \StringTok{"SP.POP.TOTL"}\NormalTok{,}
                    \StringTok{"SL.UEM.TOTL.NE.ZS"}
\NormalTok{                    ),}
      \AttributeTok{country =} \FunctionTok{c}\NormalTok{(}\StringTok{"AU"}\NormalTok{, }\StringTok{"ET"}\NormalTok{, }\StringTok{"IN"}\NormalTok{, }\StringTok{"US"}\NormalTok{)}
\NormalTok{      )}
\end{Highlighting}
\end{Shaded}

At this point we may like to change the names to be more meaningful and only keep the columns that we need.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{inflation =}\NormalTok{ FP.CPI.TOTL.ZG,}
         \AttributeTok{gdp\_growth =}\NormalTok{ NY.GDP.MKTP.KD.ZG,}
         \AttributeTok{population =}\NormalTok{ SP.POP.TOTL,}
         \AttributeTok{unemployment\_rate =}\NormalTok{ SL.UEM.TOTL.NE.ZS}
\NormalTok{         ) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{iso2c)}

\FunctionTok{head}\NormalTok{(world\_bank\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 6}
\CommentTok{\#\textgreater{}   country    year inflation gdp\_growth population}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}     \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}      \textless{}dbl\textgreater{}      \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Australia  1960     3.73       NA      10276477}
\CommentTok{\#\textgreater{} 2 Australia  1961     2.29        2.48   10483000}
\CommentTok{\#\textgreater{} 3 Australia  1962    {-}0.319       1.29   10742000}
\CommentTok{\#\textgreater{} 4 Australia  1963     0.641       6.21   10950000}
\CommentTok{\#\textgreater{} 5 Australia  1964     2.87        6.98   11167000}
\CommentTok{\#\textgreater{} 6 Australia  1965     3.41        5.98   11388000}
\CommentTok{\#\textgreater{} \# ... with 1 more variable: unemployment\_rate \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

Now let us look at income as a function of years of education (Figure \ref{fig:scattorplot}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth, }\AttributeTok{y =}\NormalTok{ inflation, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\CommentTok{\#\textgreater{} Warning: Removed 26 rows containing missing values}
\CommentTok{\#\textgreater{} (geom\_point).}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/scattorplot-1.pdf}
\caption{\label{fig:scattorplot}Relationship between inflation and GDP for Australia, Ethiopia, India, and the US}
\end{figure}

As with the bar plots, we change the theme, and update the labels (Figure \ref{fig:scatterplotnicer}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth, }\AttributeTok{y =}\NormalTok{ inflation, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Inflation"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"GDP growth"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Relationship between inflation and GDP growth"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}
\CommentTok{\#\textgreater{} Warning: Removed 26 rows containing missing values}
\CommentTok{\#\textgreater{} (geom\_point).}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/scatterplotnicer-1.pdf}
\caption{\label{fig:scatterplotnicer}Relationship between inflation and GDP for Australia, Ethiopia, India, and the US}
\end{figure}

We use `color' instead of `fill' because we are using dots rather than bars. This also then slightly affects how we change the palette (Figure \ref{fig:scatterplotnicercolor}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(patchwork)}

\NormalTok{RColorBrewerBrBG }\OtherTok{\textless{}{-}}
\NormalTok{  world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth, }\AttributeTok{y =}\NormalTok{ inflation, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Inflation"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"GDP growth"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Relationship between inflation and GDP growth"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Blues"}\NormalTok{)}

\NormalTok{RColorBrewerSet2 }\OtherTok{\textless{}{-}} 
\NormalTok{  world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth, }\AttributeTok{y =}\NormalTok{ inflation, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Inflation"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"GDP growth"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Relationship between inflation and GDP growth"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}

\NormalTok{viridis }\OtherTok{\textless{}{-}} 
\NormalTok{  world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth, }\AttributeTok{y =}\NormalTok{ inflation, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Inflation"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"GDP growth"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Relationship between inflation and GDP growth"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_colour\_viridis\_d}\NormalTok{()}

\NormalTok{viridismagma }\OtherTok{\textless{}{-}} 
\NormalTok{  world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth, }\AttributeTok{y =}\NormalTok{ inflation, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Inflation"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"GDP growth"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Relationship between inflation and GDP growth"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_colour\_viridis\_d}\NormalTok{(}\AttributeTok{option =} \StringTok{"magma"}\NormalTok{)}

\NormalTok{(RColorBrewerBrBG }\SpecialCharTok{+}\NormalTok{ RColorBrewerSet2) }\SpecialCharTok{/}
\NormalTok{ (viridis }\SpecialCharTok{+}\NormalTok{ viridismagma)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/scatterplotnicercolor-1.pdf}
\caption{\label{fig:scatterplotnicercolor}Relationship between inflation and GDP for Australia, Ethiopia, India, and the US}
\end{figure}

The dots of a dot plot often overlap. We can address this situation in one of two ways: adding a degree of transparency to our dots with `alpha' (Figure \ref{fig:alphaplot}). The value for `alpha' can vary between 0, which is fully transparent, and 1, which is completely opaque. We can also specify a small amount by which we are comfortable if the points move with \texttt{geom\_jitter()} (Figure \ref{fig:jitterplot}). We can specify which direction movement occurs with `width' or `height'. The decision between these two options turns on the degree to which exact accuracy matters, and the number of points.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth, }\AttributeTok{y =}\NormalTok{ inflation, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Inflation"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"GDP growth"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Relationship between inflation and GDP growth"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}
\CommentTok{\#\textgreater{} Warning: Removed 26 rows containing missing values}
\CommentTok{\#\textgreater{} (geom\_point).}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/alphaplot-1.pdf}
\caption{\label{fig:alphaplot}Relationship between inflation and GDP for Australia, Ethiopia, India, and the US}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth, }\AttributeTok{y =}\NormalTok{ inflation, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Inflation"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"GDP growth"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Relationship between inflation and GDP growth"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}
\CommentTok{\#\textgreater{} Warning: Removed 26 rows containing missing values}
\CommentTok{\#\textgreater{} (geom\_point).}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/jitterplot-1.pdf}
\caption{\label{fig:jitterplot}Relationship between inflation and GDP for Australia, Ethiopia, India, and the US}
\end{figure}

A common use case for a scatterplot is to illustrate a relationship between two variables. It can be useful to add a line of best fit using \texttt{geom\_smooth()} (Figure \ref{fig:scattorplottwo}). By default \texttt{geom\_smooth()} will impose a X relationship. By default, loess smoothing is used for datasets with less than 1,000 observations, but we can specify the relationship using `method', change the color with `color' and remove standard errors with `se'. We use \texttt{geom\_smooth()} to add a layer to the graph, and so it inherits all the settings that it can from \texttt{ggplot()}. For instance, that is why here we have one line for each country. We could overwrite that by specifying a particular color, in which case we would only have one line.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{defaults }\OtherTok{\textless{}{-}} 
\NormalTok{  world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth, }\AttributeTok{y =}\NormalTok{ inflation, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Inflation"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"GDP growth"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Relationship between inflation and GDP growth"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}

\NormalTok{straightline }\OtherTok{\textless{}{-}} 
\NormalTok{  world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth, }\AttributeTok{y =}\NormalTok{ inflation, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =}\NormalTok{ lm, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Inflation"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"GDP growth"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Relationship between inflation and GDP growth"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}

\NormalTok{onestraightline }\OtherTok{\textless{}{-}} 
\NormalTok{  world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth, }\AttributeTok{y =}\NormalTok{ inflation, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =}\NormalTok{ lm, }\AttributeTok{color =} \StringTok{"black"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Inflation"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"GDP growth"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Relationship between inflation and GDP growth"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}

\NormalTok{(defaults }\SpecialCharTok{+}\NormalTok{ straightline }\SpecialCharTok{+}\NormalTok{ onestraightline)}
\CommentTok{\#\textgreater{} \textasciigrave{}geom\_smooth()\textasciigrave{} using method = \textquotesingle{}loess\textquotesingle{} and formula \textquotesingle{}y \textasciitilde{} x\textquotesingle{}}
\CommentTok{\#\textgreater{} Warning: Removed 26 rows containing non{-}finite values}
\CommentTok{\#\textgreater{} (stat\_smooth).}
\CommentTok{\#\textgreater{} Warning: Removed 26 rows containing missing values}
\CommentTok{\#\textgreater{} (geom\_point).}
\CommentTok{\#\textgreater{} \textasciigrave{}geom\_smooth()\textasciigrave{} using formula \textquotesingle{}y \textasciitilde{} x\textquotesingle{}}
\CommentTok{\#\textgreater{} Warning: Removed 26 rows containing non{-}finite values}
\CommentTok{\#\textgreater{} (stat\_smooth).}

\CommentTok{\#\textgreater{} Warning: Removed 26 rows containing missing values (geom\_point).}
\CommentTok{\#\textgreater{} \textasciigrave{}geom\_smooth()\textasciigrave{} using formula \textquotesingle{}y \textasciitilde{} x\textquotesingle{}}
\CommentTok{\#\textgreater{} Warning: Removed 26 rows containing non{-}finite values}
\CommentTok{\#\textgreater{} (stat\_smooth).}

\CommentTok{\#\textgreater{} Warning: Removed 26 rows containing missing values (geom\_point).}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/scattorplottwo-1.pdf}
\caption{\label{fig:scattorplottwo}Relationship between inflation and GDP for Australia, Ethiopia, India, and the US}
\end{figure}

\hypertarget{line-plots}{%
\subsection{Line plots}\label{line-plots}}

We can use a line plot when we have variables that should be joined together, for instance, economic time series. We will continue with the dataset from the World Bank and focus on initially on GDP (Figure \ref{fig:lineplot}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ year, }\AttributeTok{y =}\NormalTok{ gdp\_growth, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{()}
\CommentTok{\#\textgreater{} Warning: Removed 25 row(s) containing missing values}
\CommentTok{\#\textgreater{} (geom\_path).}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/lineplot-1.pdf}
\caption{\label{fig:lineplot}GDP over time for Australia, Ethiopia, India, and the US}
\end{figure}

As before, we can adjust the theme and labels (Figure \ref{fig:lineplottwo}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ year, }\AttributeTok{y =}\NormalTok{ gdp\_growth, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Year"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"GDP"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"GDP over time"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}
\CommentTok{\#\textgreater{} Warning: Removed 25 row(s) containing missing values}
\CommentTok{\#\textgreater{} (geom\_path).}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/lineplottwo-1.pdf}
\caption{\label{fig:lineplottwo}GDP over time for Australia, Ethiopia, India, and the US}
\end{figure}

We can use a slight variant, \texttt{geom\_step()} to focus attention on the change from year to year (Figure \ref{fig:stepplot}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ year, }\AttributeTok{y =}\NormalTok{ gdp\_growth, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_step}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Year"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"GDP"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"GDP over time"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}
\CommentTok{\#\textgreater{} Warning: Removed 25 row(s) containing missing values}
\CommentTok{\#\textgreater{} (geom\_path).}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/stepplot-1.pdf}
\caption{\label{fig:stepplot}GDP over time for Australia, Ethiopia, India, and the US}
\end{figure}

The Phillips curve is the name given to plot of the relationship between unemployment and inflation over time. An inverse relationship is sometimes found in the data, for instance in the UK between 1861 and 1957 \citep{phillips1958relation}. We have a variety of ways to investigate this including \texttt{geom\_path()} which links values in the order they appear in the datset. In Figure \ref{fig:phillipsmyboy} we show a Phillips curve for the US between 1960 and 2020, and it is clear that any relationship that once existed between these variables does not appear to still exist.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{filter}\NormalTok{(country }\SpecialCharTok{==} \StringTok{"United States"}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ unemployment\_rate, }\AttributeTok{y =}\NormalTok{ inflation)) }\SpecialCharTok{+}
  \FunctionTok{geom\_path}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Unemployment rate"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Inflation"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/phillipsmyboy-1.pdf}
\caption{\label{fig:phillipsmyboy}Phillips curve for the US (1960-2020)}
\end{figure}

\hypertarget{histograms}{%
\subsection{Histograms}\label{histograms}}

A histogram is useful to show the shape of a continuous variable and works by constructing counts of the number of observations in different subsets of the support, called `bins'. In Figure \ref{fig:hisogramone} we examine the distribution of GDP in the US.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(country }\SpecialCharTok{==} \StringTok{"United States"}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/hisogramone-1.pdf}
\caption{\label{fig:hisogramone}Distribution of income}
\end{figure}

And again we can add a theme and labels (Figure \ref{fig:hisogramtwo}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(country }\SpecialCharTok{==} \StringTok{"United States"}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of occurrences"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}
\CommentTok{\#\textgreater{} \textasciigrave{}stat\_bin()\textasciigrave{} using \textasciigrave{}bins = 30\textasciigrave{}. Pick better value with}
\CommentTok{\#\textgreater{} \textasciigrave{}binwidth\textasciigrave{}.}
\CommentTok{\#\textgreater{} Warning: Removed 1 rows containing non{-}finite values}
\CommentTok{\#\textgreater{} (stat\_bin).}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/hisogramtwo-1.pdf}
\caption{\label{fig:hisogramtwo}Distribution of GDP in the United States}
\end{figure}

The key component determining the shape of a histogram is the number of bins. This can be specified in one of two ways: 1) specifying the number of `bins' to include, or 2) specifying how wide they should be with `binwidth' (Figure \ref{fig:hisogrambins}).

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{twobins }\OtherTok{\textless{}{-}} 
\NormalTok{  world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(country }\SpecialCharTok{==} \StringTok{"United States"}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of occurrences"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}

\NormalTok{fivebins }\OtherTok{\textless{}{-}} 
\NormalTok{  world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(country }\SpecialCharTok{==} \StringTok{"United States"}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of occurrences"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}

\NormalTok{twentybins }\OtherTok{\textless{}{-}} 
\NormalTok{  world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(country }\SpecialCharTok{==} \StringTok{"United States"}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins =} \DecValTok{20}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of occurrences"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}

\NormalTok{halfbinwidth }\OtherTok{\textless{}{-}} 
\NormalTok{  world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(country }\SpecialCharTok{==} \StringTok{"United States"}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of occurrences"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}

\NormalTok{twobinwidth }\OtherTok{\textless{}{-}} 
\NormalTok{  world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(country }\SpecialCharTok{==} \StringTok{"United States"}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of occurrences"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}

\NormalTok{fivebinwidth }\OtherTok{\textless{}{-}} 
\NormalTok{  world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(country }\SpecialCharTok{==} \StringTok{"United States"}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of occurrences"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}

\NormalTok{(twobins }\SpecialCharTok{+}\NormalTok{ fivebins }\SpecialCharTok{+}\NormalTok{ twentybins) }\SpecialCharTok{/}\NormalTok{ (halfbinwidth }\SpecialCharTok{+}\NormalTok{ twobinwidth }\SpecialCharTok{+}\NormalTok{ fivebinwidth)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/hisogrambins-1.pdf}
\caption{\label{fig:hisogrambins}Distribution of GDP in the United States}
\end{figure}

The histogram is smoothing the data, and the number of bins affects how much smoothing occurs. When there are only two bins then the data are very smooth, but we have lost a great deal of accuracy. More specifically, `the histogram estimator is a piecewise constant function where the height of the function is proportional to the number of observations in each bin' \citep[p.~303]{wasserman}. Too few bins result in a biased estimator, while too many bins results in an estimator with high variance. Our decision as to the number of bins, or their width, is concerned with trying to balance bias and variance. This will depend on a variety of concerns including the subject matter and the goal \citep[p.~135]{elementsofgraphingdata}.

Finally, while we can use `fill' to distinguish between different types of observations, it can get quite messy. It is usually better to give away showing the distribution with columns and instead trace the outline of the distribution, using \texttt{geom\_freqpoly()} (Figure \ref{fig:freq}) or to build it up using dots with \texttt{geom\_dotplot()} (Figure \ref{fig:dotplot}) .

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_freqpoly}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of occurrences"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/freq-1.pdf}
\caption{\label{fig:freq}Distribution of GDP in the United States}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth, }\AttributeTok{group =}\NormalTok{ country, }\AttributeTok{fill =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_dotplot}\NormalTok{(}\AttributeTok{method =} \StringTok{\textquotesingle{}histodot\textquotesingle{}}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.4}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of occurrences"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/dotplot-1.pdf}
\caption{\label{fig:dotplot}Distribution of GDP in the United States}
\end{figure}

\hypertarget{boxplots}{%
\subsection{Boxplots}\label{boxplots}}

Boxplots are almost never an appropriate choice because they hide the distribution of data, rather than show it. Unless we need to compare the summary statistics of many variables at once, then they should almost never be used. This is because the same boxplot can apply to very different distributions. To see this, consider some simulated data from the beta distribution of two types. One type of data contains draws from two beta distributions: one that is right skewed and another that is left skewed. The other type of data contains draws from a beta distribution with no skew.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}
\NormalTok{both\_left\_and\_right\_skew }\OtherTok{\textless{}{-}} 
  \FunctionTok{c}\NormalTok{(}
    \FunctionTok{rbeta}\NormalTok{(}\DecValTok{500}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{),}
    \FunctionTok{rbeta}\NormalTok{(}\DecValTok{500}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{    )}

\NormalTok{no\_skew }\OtherTok{\textless{}{-}} 
  \FunctionTok{rbeta}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}

\NormalTok{beta\_distributions }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{observation =} \FunctionTok{c}\NormalTok{(both\_left\_and\_right\_skew, no\_skew),}
    \AttributeTok{source =} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"Left and right skew"}\NormalTok{, }\DecValTok{1000}\NormalTok{),}
               \FunctionTok{rep}\NormalTok{(}\StringTok{"No skew"}\NormalTok{, }\DecValTok{1000}\NormalTok{)}
\NormalTok{               )}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

We can first compare the boxplots of the two series (Figure \ref{fig:boxplotfirst}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta\_distributions }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ source, }\AttributeTok{y =}\NormalTok{ observation)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/boxplotfirst-1.pdf}
\caption{\label{fig:boxplotfirst}Data drawn from beta distributions with different parameters}
\end{figure}

But if we plot the actual data then we can see how different they are (Figure \ref{fig:freqpolyofdistributions}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta\_distributions }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ observation, }\AttributeTok{color =}\NormalTok{ source)) }\SpecialCharTok{+}
  \FunctionTok{geom\_freqpoly}\NormalTok{(}\AttributeTok{binwidth =} \FloatTok{0.05}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/freqpolyofdistributions-1.pdf}
\caption{\label{fig:freqpolyofdistributions}Data drawn from beta distributions with different parameters}
\end{figure}

One way forward, if a boxplot must be included, is to include the actual data as a layer on top of the boxplot (Figure \ref{fig:bloxplotandoverlay}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta\_distributions }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ source, }\AttributeTok{y =}\NormalTok{ observation)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/bloxplotandoverlay-1.pdf}
\caption{\label{fig:bloxplotandoverlay}Data drawn from beta distributions with different parameters}
\end{figure}

An even better solution is to graph the quantiles of each distribution against (Figure \ref{fig:qqplotftw}. The Q-Q plot was developed by \citet{wilk1968probability} and requires us to plot the distributions against each other.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta\_distributions }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{sample =}\NormalTok{ observation, }\AttributeTok{color =}\NormalTok{ source)) }\SpecialCharTok{+}
  \FunctionTok{stat\_qq}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{stat\_qq\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{11-static_communication_files/figure-latex/qqplotftw-1.pdf}
\caption{\label{fig:qqplotftw}Data drawn from beta distributions with different parameters}
\end{figure}

\hypertarget{tables}{%
\section{Tables}\label{tables}}

Tables are critical for telling a compelling story. Tables can communicate less information than a graph, but they can do so at a high fidelity. We primarily use tables in three ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  To show some of our actual dataset, for which we use \texttt{kable()} from \texttt{knitr} \citep{citeknitr}, alongside \texttt{kableExtra} \citep{citekableextra}.
\item
  To communicate summary statistics, for which we use \texttt{gt} \citep{citegt} and \texttt{modelsummary} \citep{citemodelsummary}.
\item
  To display regression results, for which we use \texttt{modelsummary} \citep{citemodelsummary}.
\end{enumerate}

\hypertarget{showing-part-of-a-dataset}{%
\subsection{Showing part of a dataset}\label{showing-part-of-a-dataset}}

We will illustrate showing part of a dataset using \texttt{kable()} from \texttt{knitr} and drawing on \texttt{kableExtra} for enhancement. We will use the World Bank dataset on inflation and GDP from earlier.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(knitr)}
\FunctionTok{head}\NormalTok{(world\_bank\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 6}
\CommentTok{\#\textgreater{}   country    year inflation gdp\_growth population}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}     \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}      \textless{}dbl\textgreater{}      \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Australia  1960     3.73       NA      10276477}
\CommentTok{\#\textgreater{} 2 Australia  1961     2.29        2.48   10483000}
\CommentTok{\#\textgreater{} 3 Australia  1962    {-}0.319       1.29   10742000}
\CommentTok{\#\textgreater{} 4 Australia  1963     0.641       6.21   10950000}
\CommentTok{\#\textgreater{} 5 Australia  1964     2.87        6.98   11167000}
\CommentTok{\#\textgreater{} 6 Australia  1965     3.41        5.98   11388000}
\CommentTok{\#\textgreater{} \# ... with 1 more variable: unemployment\_rate \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

To begin, we can display the first ten rows with the default \texttt{kable()} settings.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{kable}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r|r|r|r}
\hline
country & year & inflation & gdp\_growth & population & unemployment\_rate\\
\hline
Australia & 1960 & 3.7288136 & NA & 10276477 & NA\\
\hline
Australia & 1961 & 2.2875817 & 2.483271 & 10483000 & NA\\
\hline
Australia & 1962 & -0.3194888 & 1.294468 & 10742000 & NA\\
\hline
Australia & 1963 & 0.6410256 & 6.214949 & 10950000 & NA\\
\hline
Australia & 1964 & 2.8662420 & 6.978540 & 11167000 & NA\\
\hline
Australia & 1965 & 3.4055728 & 5.980893 & 11388000 & NA\\
\hline
Australia & 1966 & 3.2934132 & 2.381966 & 11651000 & NA\\
\hline
Australia & 1967 & 3.4782609 & 6.303650 & 11799000 & NA\\
\hline
Australia & 1968 & 2.5210084 & 5.095103 & 12009000 & NA\\
\hline
Australia & 1969 & 3.2786885 & 7.043526 & 12263000 & NA\\
\hline
\end{tabular}

In order to be able to cross-reference it in text, we need to add a caption with `caption'. We can also make the column names more information with `col.names' and specify the number of digits to be displayed (Table \ref{tab:gdpfirst}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{kable}\NormalTok{(}
    \AttributeTok{caption =} \StringTok{"First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the US"}\NormalTok{,}
    \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Country"}\NormalTok{, }\StringTok{"Year"}\NormalTok{, }\StringTok{"Inflation"}\NormalTok{, }\StringTok{"GDP growth"}\NormalTok{, }\StringTok{"Population"}\NormalTok{, }\StringTok{"Unemployment rate"}\NormalTok{),}
    \AttributeTok{digits =} \DecValTok{1}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:gdpfirst}First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the US}
\centering
\begin{tabular}[t]{l|r|r|r|r|r}
\hline
Country & Year & Inflation & GDP growth & Population & Unemployment rate\\
\hline
Australia & 1960 & 3.7 & NA & 10276477 & NA\\
\hline
Australia & 1961 & 2.3 & 2.5 & 10483000 & NA\\
\hline
Australia & 1962 & -0.3 & 1.3 & 10742000 & NA\\
\hline
Australia & 1963 & 0.6 & 6.2 & 10950000 & NA\\
\hline
Australia & 1964 & 2.9 & 7.0 & 11167000 & NA\\
\hline
Australia & 1965 & 3.4 & 6.0 & 11388000 & NA\\
\hline
Australia & 1966 & 3.3 & 2.4 & 11651000 & NA\\
\hline
Australia & 1967 & 3.5 & 6.3 & 11799000 & NA\\
\hline
Australia & 1968 & 2.5 & 5.1 & 12009000 & NA\\
\hline
Australia & 1969 & 3.3 & 7.0 & 12263000 & NA\\
\hline
\end{tabular}
\end{table}

When producing PDFs, the `booktabs' option makes a host of small changes to the default display and results in tables that look better (Table \ref{tab:gdpbookdtabs}). When using `booktabs' we additionally should specify `linesep' otherwise \texttt{kable()} adds a small space every five lines. (None of this will show up for html output.)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{kable}\NormalTok{(}
    \AttributeTok{caption =} \StringTok{"First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the US"}\NormalTok{,}
    \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Country"}\NormalTok{, }\StringTok{"Year"}\NormalTok{, }\StringTok{"Inflation"}\NormalTok{, }\StringTok{"GDP growth"}\NormalTok{, }\StringTok{"Population"}\NormalTok{, }\StringTok{"Unemployment rate"}\NormalTok{),}
    \AttributeTok{digits =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{, }
    \AttributeTok{linesep =} \StringTok{""}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:gdpbookdtabs}First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the US}
\centering
\begin{tabular}[t]{lrrrrr}
\toprule
Country & Year & Inflation & GDP growth & Population & Unemployment rate\\
\midrule
Australia & 1960 & 3.7 & NA & 10276477 & NA\\
Australia & 1961 & 2.3 & 2.5 & 10483000 & NA\\
Australia & 1962 & -0.3 & 1.3 & 10742000 & NA\\
Australia & 1963 & 0.6 & 6.2 & 10950000 & NA\\
Australia & 1964 & 2.9 & 7.0 & 11167000 & NA\\
Australia & 1965 & 3.4 & 6.0 & 11388000 & NA\\
Australia & 1966 & 3.3 & 2.4 & 11651000 & NA\\
Australia & 1967 & 3.5 & 6.3 & 11799000 & NA\\
Australia & 1968 & 2.5 & 5.1 & 12009000 & NA\\
Australia & 1969 & 3.3 & 7.0 & 12263000 & NA\\
\bottomrule
\end{tabular}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{kable}\NormalTok{(}
    \AttributeTok{caption =} \StringTok{"First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the US"}\NormalTok{,}
    \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Country"}\NormalTok{, }\StringTok{"Year"}\NormalTok{, }\StringTok{"Inflation"}\NormalTok{, }\StringTok{"GDP growth"}\NormalTok{, }\StringTok{"Population"}\NormalTok{, }\StringTok{"Unemployment rate"}\NormalTok{),}
    \AttributeTok{digits =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{booktabs =} \ConstantTok{TRUE}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:gdpbookdtabsnolinesep}First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the US}
\centering
\begin{tabular}[t]{lrrrrr}
\toprule
Country & Year & Inflation & GDP growth & Population & Unemployment rate\\
\midrule
Australia & 1960 & 3.7 & NA & 10276477 & NA\\
Australia & 1961 & 2.3 & 2.5 & 10483000 & NA\\
Australia & 1962 & -0.3 & 1.3 & 10742000 & NA\\
Australia & 1963 & 0.6 & 6.2 & 10950000 & NA\\
Australia & 1964 & 2.9 & 7.0 & 11167000 & NA\\
\addlinespace
Australia & 1965 & 3.4 & 6.0 & 11388000 & NA\\
Australia & 1966 & 3.3 & 2.4 & 11651000 & NA\\
Australia & 1967 & 3.5 & 6.3 & 11799000 & NA\\
Australia & 1968 & 2.5 & 5.1 & 12009000 & NA\\
Australia & 1969 & 3.3 & 7.0 & 12263000 & NA\\
\bottomrule
\end{tabular}
\end{table}

We can specify the alignment of the columns using a character vector of `l' (left), `c' (centre), and `r' (right) (Table \ref{tab:gdpalign}). Additionally, (and this is not relevant for this table), we could specify groupings for numbers that are at least one thousand using `format.args = list(big.mark = ``,'')'.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{kable}\NormalTok{(}
    \AttributeTok{caption =} \StringTok{"First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the US"}\NormalTok{,}
    \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Country"}\NormalTok{, }\StringTok{"Year"}\NormalTok{, }\StringTok{"Inflation"}\NormalTok{, }\StringTok{"GDP growth"}\NormalTok{, }\StringTok{"Population"}\NormalTok{, }\StringTok{"Unemployment rate"}\NormalTok{),}
    \AttributeTok{digits =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{, }
    \AttributeTok{linesep =} \StringTok{""}\NormalTok{,}
    \AttributeTok{align =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}c\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}c\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}r\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}r\textquotesingle{}}\NormalTok{),}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:gdpalign}First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the US}
\centering
\begin{tabular}[t]{llccrr}
\toprule
Country & Year & Inflation & GDP growth & Population & Unemployment rate\\
\midrule
Australia & 1960 & 3.7 & NA & 10276477 & NA\\
Australia & 1961 & 2.3 & 2.5 & 10483000 & NA\\
Australia & 1962 & -0.3 & 1.3 & 10742000 & NA\\
Australia & 1963 & 0.6 & 6.2 & 10950000 & NA\\
Australia & 1964 & 2.9 & 7.0 & 11167000 & NA\\
Australia & 1965 & 3.4 & 6.0 & 11388000 & NA\\
Australia & 1966 & 3.3 & 2.4 & 11651000 & NA\\
Australia & 1967 & 3.5 & 6.3 & 11799000 & NA\\
Australia & 1968 & 2.5 & 5.1 & 12009000 & NA\\
Australia & 1969 & 3.3 & 7.0 & 12263000 & NA\\
\bottomrule
\end{tabular}
\end{table}

We can use \texttt{kableExtra} \citep{citekableextra} to add extra functionality to \texttt{kable}. For instance, we could add a row that groups some of the columns (Table \ref{tab:gdpalign}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(kableExtra)}

\NormalTok{world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{kable}\NormalTok{(}
    \AttributeTok{caption =} \StringTok{"First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the US"}\NormalTok{,}
    \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Country"}\NormalTok{, }\StringTok{"Year"}\NormalTok{, }\StringTok{"Inflation"}\NormalTok{, }\StringTok{"GDP growth"}\NormalTok{, }\StringTok{"Population"}\NormalTok{, }\StringTok{"Unemployment rate"}\NormalTok{),}
    \AttributeTok{digits =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{, }
    \AttributeTok{linesep =} \StringTok{""}\NormalTok{,}
    \AttributeTok{align =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}c\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}c\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}r\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}r\textquotesingle{}}\NormalTok{),}
\NormalTok{  ) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{add\_header\_above}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{" "} \OtherTok{=} \DecValTok{2}\NormalTok{, }\StringTok{"Economic indicators"} \OtherTok{=} \DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:gdpkableextra}First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the US}
\centering
\begin{tabular}[t]{llccrr}
\toprule
\multicolumn{2}{c}{ } & \multicolumn{4}{c}{Economic indicators} \\
\cmidrule(l{3pt}r{3pt}){3-6}
Country & Year & Inflation & GDP growth & Population & Unemployment rate\\
\midrule
Australia & 1960 & 3.7 & NA & 10276477 & NA\\
Australia & 1961 & 2.3 & 2.5 & 10483000 & NA\\
Australia & 1962 & -0.3 & 1.3 & 10742000 & NA\\
Australia & 1963 & 0.6 & 6.2 & 10950000 & NA\\
Australia & 1964 & 2.9 & 7.0 & 11167000 & NA\\
Australia & 1965 & 3.4 & 6.0 & 11388000 & NA\\
Australia & 1966 & 3.3 & 2.4 & 11651000 & NA\\
Australia & 1967 & 3.5 & 6.3 & 11799000 & NA\\
Australia & 1968 & 2.5 & 5.1 & 12009000 & NA\\
Australia & 1969 & 3.3 & 7.0 & 12263000 & NA\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{communicating-summary-statistics}{%
\subsection{Communicating summary statistics}\label{communicating-summary-statistics}}

We can use \texttt{datasummary()} from \texttt{modelsummary} to create tables of summary statistics from our dataset.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(modelsummary)}

\NormalTok{world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{datasummary\_skim}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{table}
\centering
\begin{tabular}[t]{lrrrrrrr>{}r}
\toprule
  & Unique (\#) & Missing (\%) & Mean & SD & Min & Median & Max &   \\
\midrule
year & 61 & 0 & \num{1990.0} & \num{17.6} & \num{1960.0} & \num{1990.0} & \num{2020.0} & \includegraphics[width=0.67in, height=0.17in]{11-static_communication_files/figure-latex//hist_130942aeb3db.pdf}\\
inflation & 238 & 3 & \num{6.1} & \num{6.3} & \num{-9.8} & \num{4.3} & \num{44.4} & \includegraphics[width=0.67in, height=0.17in]{11-static_communication_files/figure-latex//hist_13096d0e606f.pdf}\\
gdp\_growth & 220 & 10 & \num{4.2} & \num{3.7} & \num{-11.1} & \num{3.9} & \num{13.9} & \includegraphics[width=0.67in, height=0.17in]{11-static_communication_files/figure-latex//hist_130978440ce.pdf}\\
population & 244 & 0 & \num{304177482.9} & \num{380093166.9} & \num{10276477.0} & \num{147817291.5} & \num{1380004385.0} & \includegraphics[width=0.67in, height=0.17in]{11-static_communication_files/figure-latex//hist_13091760ca9a.pdf}\\
unemployment\_rate & 104 & 52 & \num{6.0} & \num{1.9} & \num{1.2} & \num{5.7} & \num{10.9} & \includegraphics[width=0.67in, height=0.17in]{11-static_communication_files/figure-latex//hist_13096fd4d8e3.pdf}\\
\bottomrule
\end{tabular}
\end{table}

By default it summarizes the `numeric' variables, but we can ask for the `categorical' variables (Table \ref{tab:testdatasummary}). Additionally we can add cross-references in the same way as \texttt{kable()}, that is, include a title and then cross-reference the name of the R chunk.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{datasummary\_skim}\NormalTok{(}\AttributeTok{type =} \StringTok{"categorical"}\NormalTok{,}
                   \AttributeTok{title =} \StringTok{"Summary of categorical variables for the inflation and GDP dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:testdatasummary}Summary of categorical variables for the inflation and GDP dataset}
\centering
\begin{tabular}[t]{lrr}
\toprule
country & N & \%\\
\midrule
Australia & 61 & \num{25.0}\\
Ethiopia & 61 & \num{25.0}\\
India & 61 & \num{25.0}\\
United States & 61 & \num{25.0}\\
\bottomrule
\end{tabular}
\end{table}

We can create a table that shows the correlation between variables using \texttt{datasummary\_correlation()} (Table \ref{tab:correlationtable}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{datasummary\_correlation}\NormalTok{(}
    \AttributeTok{title =} \StringTok{"Correlation between the variables for the inflation and GDP dataset"}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:correlationtable}Correlation between the variables for the inflation and GDP dataset}
\centering
\begin{tabular}[t]{lrrrrr}
\toprule
  & year & inflation & gdp\_growth & population & unemployment\_rate\\
\midrule
year & 1 & . & . & . & .\\
inflation & \num{.00} & 1 & . & . & .\\
gdp\_growth & \num{.10} & \num{.00} & 1 & . & .\\
population & \num{.24} & \num{.07} & \num{.15} & 1 & .\\
unemployment\_rate & \num{-.13} & \num{-.14} & \num{-.31} & \num{-.35} & 1\\
\bottomrule
\end{tabular}
\end{table}

We typically need a table of descriptive statistics that we could add to our paper (Table \ref{tab:descriptivestats}). This is in contrast to Table \ref{tab:testdatasummary} which would likely not be included in a paper. We can add a note about the source of the data using `notes'.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{datasummary\_balance}\NormalTok{(}\AttributeTok{formula =} \SpecialCharTok{\textasciitilde{}}\NormalTok{country,}
                    \AttributeTok{data =}\NormalTok{ world\_bank\_data,}
                    \AttributeTok{title =} \StringTok{"Descriptive statistics for the inflation and GDP dataset"}\NormalTok{,}
                    \AttributeTok{notes =} \StringTok{"Data source: World Bank."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:descriptivestats}Descriptive statistics for the inflation and GDP dataset}
\centering
\begin{tabular}[t]{lrrrrrrrr}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{2}{c}{Australia (N=61)} & \multicolumn{2}{c}{Ethiopia (N=61)} & \multicolumn{2}{c}{India (N=61)} & \multicolumn{2}{c}{United States (N=61)} \\
\cmidrule(l{3pt}r{3pt}){2-3} \cmidrule(l{3pt}r{3pt}){4-5} \cmidrule(l{3pt}r{3pt}){6-7} \cmidrule(l{3pt}r{3pt}){8-9}
  & Mean & Std. Dev. & Mean  & Std. Dev.  & Mean   & Std. Dev.   & Mean    & Std. Dev.   \\
\midrule
year & 1990.0 & 17.8 & 1990.0 & 17.8 & 1990.0 & 17.8 & 1990.0 & 17.8\\
inflation & 4.7 & 3.8 & 8.7 & 10.4 & 7.4 & 5.0 & 3.7 & 2.8\\
gdp\_growth & 3.4 & 1.8 & 5.9 & 6.4 & 5.0 & 3.3 & 2.9 & 2.2\\
population & 17244215.9 & 4328625.6 & 55662437.9 & 27626912.1 & 888774544.9 & 292997809.4 & 255028733.1 & 45603604.8\\
unemployment\_rate & 6.8 & 1.7 & 2.6 & 0.9 & 3.5 & 1.4 & 6.0 & 1.6\\
\bottomrule
\multicolumn{9}{l}{\rule{0pt}{1em}Data source: World Bank.}\\
\end{tabular}
\end{table}

\hypertarget{display-regression-results}{%
\subsection{Display regression results}\label{display-regression-results}}

Finally, one common reason for needing a table is to report regression results. We will do this using \texttt{modelsummary()} from \texttt{modelsummary} \citep{citemodelsummary}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{first\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ gdp\_growth }\SpecialCharTok{\textasciitilde{}}\NormalTok{ inflation, }
                  \AttributeTok{data =}\NormalTok{ world\_bank\_data)}

\FunctionTok{modelsummary}\NormalTok{(first\_model)}
\end{Highlighting}
\end{Shaded}

\begin{table}
\centering
\begin{tabular}[t]{lc}
\toprule
  & Model 1\\
\midrule
(Intercept) & \num{4.157}\\
 & (\num{0.352})\\
inflation & \num{-0.002}\\
 & (\num{0.041})\\
\midrule
Num.Obs. & \num{218}\\
R2 & \num{0.000}\\
R2 Adj. & \num{-0.005}\\
AIC & \num{1195.1}\\
BIC & \num{1205.3}\\
Log.Lik. & \num{-594.554}\\
F & \num{0.002}\\
\bottomrule
\end{tabular}
\end{table}

We can put a variety of different of different models together (Table \ref{tab:twomodels}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{second\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ gdp\_growth }\SpecialCharTok{\textasciitilde{}}\NormalTok{ inflation }\SpecialCharTok{+}\NormalTok{ country, }
                  \AttributeTok{data =}\NormalTok{ world\_bank\_data)}

\NormalTok{third\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ gdp\_growth }\SpecialCharTok{\textasciitilde{}}\NormalTok{ inflation }\SpecialCharTok{+}\NormalTok{ country }\SpecialCharTok{+}\NormalTok{ population, }
                  \AttributeTok{data =}\NormalTok{ world\_bank\_data)}

\FunctionTok{modelsummary}\NormalTok{(}\FunctionTok{list}\NormalTok{(first\_model, second\_model, third\_model),}
             \AttributeTok{title =} \StringTok{"Explaining GDP as a function of inflation"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:twomodels}Explaining GDP as a function of inflation}
\centering
\begin{tabular}[t]{lccc}
\toprule
  & Model 1 & Model 2 & Model 3\\
\midrule
(Intercept) & \num{4.157} & \num{3.728} & \num{3.668}\\
 & (\num{0.352}) & (\num{0.495}) & (\num{0.494})\\
inflation & \num{-0.002} & \num{-0.075} & \num{-0.072}\\
 & (\num{0.041}) & (\num{0.041}) & (\num{0.041})\\
countryEthiopia &  & \num{2.872} & \num{2.716}\\
 &  & (\num{0.757}) & (\num{0.758})\\
countryIndia &  & \num{1.854} & \num{-0.561}\\
 &  & (\num{0.655}) & (\num{1.520})\\
countryUnited States &  & \num{-0.524} & \num{-1.176}\\
 &  & (\num{0.646}) & (\num{0.742})\\
population &  &  & \num{0.000}\\
 &  &  & (\num{0.000})\\
\midrule
Num.Obs. & \num{218} & \num{218} & \num{218}\\
R2 & \num{0.000} & \num{0.110} & \num{0.123}\\
R2 Adj. & \num{-0.005} & \num{0.093} & \num{0.102}\\
AIC & \num{1195.1} & \num{1175.7} & \num{1174.5}\\
BIC & \num{1205.3} & \num{1196.0} & \num{1198.2}\\
Log.Lik. & \num{-594.554} & \num{-581.844} & \num{-580.266}\\
F & \num{0.002} & \num{6.587} & \num{5.939}\\
\bottomrule
\end{tabular}
\end{table}

We can adjust the number of significant digits (Table \ref{tab:twomodelstwo}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{modelsummary}\NormalTok{(}\FunctionTok{list}\NormalTok{(first\_model, second\_model, third\_model),}
             \AttributeTok{fmt =} \DecValTok{1}\NormalTok{,}
             \AttributeTok{title =} \StringTok{"Two models of GDP as a function of inflation"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:twomodelstwo}Two models of GDP as a function of inflation}
\centering
\begin{tabular}[t]{lccc}
\toprule
  & Model 1 & Model 2 & Model 3\\
\midrule
(Intercept) & \num{4.2} & \num{3.7} & \num{3.7}\\
 & (\num{0.4}) & (\num{0.5}) & (\num{0.5})\\
inflation & \num{0.0} & \num{-0.1} & \num{-0.1}\\
 & (\num{0.0}) & (\num{0.0}) & (\num{0.0})\\
countryEthiopia &  & \num{2.9} & \num{2.7}\\
 &  & (\num{0.8}) & (\num{0.8})\\
countryIndia &  & \num{1.9} & \num{-0.6}\\
 &  & (\num{0.7}) & (\num{1.5})\\
countryUnited States &  & \num{-0.5} & \num{-1.2}\\
 &  & (\num{0.6}) & (\num{0.7})\\
population &  &  & \num{0.0}\\
 &  &  & (\num{0.0})\\
\midrule
Num.Obs. & \num{218} & \num{218} & \num{218}\\
R2 & \num{0.000} & \num{0.110} & \num{0.123}\\
R2 Adj. & \num{-0.005} & \num{0.093} & \num{0.102}\\
AIC & \num{1195.1} & \num{1175.7} & \num{1174.5}\\
BIC & \num{1205.3} & \num{1196.0} & \num{1198.2}\\
Log.Lik. & \num{-594.554} & \num{-581.844} & \num{-580.266}\\
F & \num{0.002} & \num{6.587} & \num{5.939}\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{maps}{%
\section{Maps}\label{maps}}

In many ways maps can be thought of as another type of graph, where the x-axis is latitude, the y-axis is longitude, and there is some outline or a background image. We have seen this type of set-up are used to this type of set-up, for instance, in the \texttt{ggplot2} setting, this is quite familiar.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_polygon}\NormalTok{( }\CommentTok{\# First draw an outline}
    \AttributeTok{data =}\NormalTok{ some\_data, }
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ latitude, }
        \AttributeTok{y =}\NormalTok{ longitude,}
        \AttributeTok{group =}\NormalTok{ group}
\NormalTok{        )) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{( }\CommentTok{\# Then add points of interest}
    \AttributeTok{data =}\NormalTok{ some\_other\_data, }
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ latitude, }
        \AttributeTok{y =}\NormalTok{ longitude)}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

And while there are some small complications, for the most part it is as straight-forward as that. The first step is to get some data. There is some geographic data built into \texttt{ggplot2}, and there is additional information in \texttt{maps}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(maps)}

\NormalTok{canada }\OtherTok{\textless{}{-}} \FunctionTok{map\_data}\NormalTok{(}\AttributeTok{database =} \StringTok{"world"}\NormalTok{, }\AttributeTok{regions =} \StringTok{"canada"}\NormalTok{)}
\NormalTok{canadian\_cities }\OtherTok{\textless{}{-}}\NormalTok{ maps}\SpecialCharTok{::}\NormalTok{canada.cities}

\FunctionTok{head}\NormalTok{(canada)}
\CommentTok{\#\textgreater{}        long      lat group order region    subregion}
\CommentTok{\#\textgreater{} 1 {-}59.78760 43.93960     1     1 Canada Sable Island}
\CommentTok{\#\textgreater{} 2 {-}59.92227 43.90391     1     2 Canada Sable Island}
\CommentTok{\#\textgreater{} 3 {-}60.03775 43.90664     1     3 Canada Sable Island}
\CommentTok{\#\textgreater{} 4 {-}60.11426 43.93911     1     4 Canada Sable Island}
\CommentTok{\#\textgreater{} 5 {-}60.11748 43.95337     1     5 Canada Sable Island}
\CommentTok{\#\textgreater{} 6 {-}59.93604 43.93960     1     6 Canada Sable Island}

\FunctionTok{head}\NormalTok{(canadian\_cities)}
\CommentTok{\#\textgreater{}            name country.etc    pop   lat    long capital}
\CommentTok{\#\textgreater{} 1 Abbotsford BC          BC 157795 49.06 {-}122.30       0}
\CommentTok{\#\textgreater{} 2      Acton ON          ON   8308 43.63  {-}80.03       0}
\CommentTok{\#\textgreater{} 3 Acton Vale QC          QC   5153 45.63  {-}72.57       0}
\CommentTok{\#\textgreater{} 4    Airdrie AB          AB  25863 51.30 {-}114.02       0}
\CommentTok{\#\textgreater{} 5    Aklavik NT          NT    643 68.22 {-}135.00       0}
\CommentTok{\#\textgreater{} 6    Albanel QC          QC   1090 48.87  {-}72.42       0}
\end{Highlighting}
\end{Shaded}

With that information in hand we can then create a map of Canada that shows the cities with a population over 1,000. (The \texttt{geom\_polygon()} function within \texttt{ggplot2} draws shapes, by connecting points within groups. And the \texttt{coord\_map()} function adjusts for the fact that we are making something that is 2D map to represent something that is 3D.)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_polygon}\NormalTok{(}\AttributeTok{data =}\NormalTok{ canada,}
               \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ long,}
                   \AttributeTok{y =}\NormalTok{ lat,}
                   \AttributeTok{group =}\NormalTok{ group),}
               \AttributeTok{fill =} \StringTok{"white"}\NormalTok{, }
               \AttributeTok{colour =} \StringTok{"grey"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_map}\NormalTok{(}\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{40}\NormalTok{, }\DecValTok{70}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ canadian\_cities}\SpecialCharTok{$}\NormalTok{long, }
                 \AttributeTok{y =}\NormalTok{ canadian\_cities}\SpecialCharTok{$}\NormalTok{lat),}
             \AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{,}
             \AttributeTok{color =} \StringTok{"black"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Longitude"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Latitude"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{11-static_communication_files/figure-latex/unnamed-chunk-23-1.pdf}

As is often the case with R, there are many different ways to get started creating static maps. We have already seen how they can be built using only \texttt{ggplot2}, but \texttt{ggmap} brings additional functionality \citep{KahleWickham2013}.

There are two essential components to a map:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  some border or background image (also known as a tile); and
\item
  something of interest within that border or on top of that tile.
\end{enumerate}

In \texttt{ggmap}, we use an open-source option for our tile, Stamen Maps: maps.stamen.com. And we use plot points based on latitude and longitude.

\hypertarget{australian-polling-places}{%
\subsection{Australian polling places}\label{australian-polling-places}}

In Australia people go to specific locations, called booths, to vote. These booths have latitudes and longitudes and so we can plot these. One reason we may like to do this is to notice patterns over geographies.

To get started we need to get a tile. We are going to use \texttt{ggmap} to get a tile from Stamen Maps, which builds on OpenStreetMap (openstreetmap.org). The main argument to this function is to specify a bounding box. This requires two latitudes - one for the top of the box and one for the bottom of the box - and two longitudes - one for the left of the box and one for the right of the box. It can be useful to use Google Maps, or an alternative, to find the values of these that you need. The bounding box provides the coordinates of the edges that you are interested in. In this case we have provided it with coordinates such that it will be centered around Canberra, Australia, which is a small city that was created for the purposes of being the capital.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggmap)}

\NormalTok{bbox }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\AttributeTok{left =} \FloatTok{148.95}\NormalTok{, }\AttributeTok{bottom =} \SpecialCharTok{{-}}\FloatTok{35.5}\NormalTok{, }\AttributeTok{right =} \FloatTok{149.3}\NormalTok{, }\AttributeTok{top =} \SpecialCharTok{{-}}\FloatTok{35.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Once you have defined the bounding box, then the function \texttt{get\_stamenmap()} will get the tiles in that area. The number of tiles that it needs to get depends on the zoom, and the type of tiles that it gets depends on the maptype. We have used a black-and-white type of map but the helpfile specifies others. At this point we can the map to maps to \texttt{ggmap()} and it will plot the tile! It will be actively downloading these tiles, and so it needs an internet connection.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{canberra\_stamen\_map }\OtherTok{\textless{}{-}} \FunctionTok{get\_stamenmap}\NormalTok{(bbox, }\AttributeTok{zoom =} \DecValTok{11}\NormalTok{, }\AttributeTok{maptype =} \StringTok{"toner{-}lite"}\NormalTok{)}

\FunctionTok{ggmap}\NormalTok{(canberra\_stamen\_map)}
\end{Highlighting}
\end{Shaded}

\includegraphics{11-static_communication_files/figure-latex/unnamed-chunk-25-1.pdf}

Once we have a map then we can use \texttt{ggmap()} to plot it. (That circle in the middle of the map is where the Australian Parliament House is. Yes, the Australian parliament is surrounded by circular roads, which Australians call `roundabouts', actually Australians thought this was such a great idea, that we surrounded it by two of them.)

Now we want to get some data that we plot on top of our tiles. We will just plot the location of the polling places, based on which `division' (the Australian equivalent to `ridings' in Canada) it is. This is available here: \url{https://results.aec.gov.au/20499/Website/Downloads/HouseTppByPollingPlaceDownload-20499.csv}. The Australian Electoral Commission (AEC) is the official government agency that is responsible for elections in Australia.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read in the booths data for each year}
\NormalTok{booths }\OtherTok{\textless{}{-}}
\NormalTok{  readr}\SpecialCharTok{::}\FunctionTok{read\_csv}\NormalTok{(}
    \StringTok{"https://results.aec.gov.au/24310/Website/Downloads/GeneralPollingPlacesDownload{-}24310.csv"}\NormalTok{,}
    \AttributeTok{skip =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{guess\_max =} \DecValTok{10000}
\NormalTok{  )}

\FunctionTok{head}\NormalTok{(booths)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 15}
\CommentTok{\#\textgreater{}   State DivisionID DivisionNm PollingPlaceID}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}      \textless{}dbl\textgreater{} \textless{}chr\textgreater{}               \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 ACT          318 Bean                93925}
\CommentTok{\#\textgreater{} 2 ACT          318 Bean                93927}
\CommentTok{\#\textgreater{} 3 ACT          318 Bean                11877}
\CommentTok{\#\textgreater{} 4 ACT          318 Bean                11452}
\CommentTok{\#\textgreater{} 5 ACT          318 Bean                 8761}
\CommentTok{\#\textgreater{} 6 ACT          318 Bean                 8763}
\CommentTok{\#\textgreater{} \# ... with 11 more variables: PollingPlaceTypeID \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   PollingPlaceNm \textless{}chr\textgreater{}, PremisesNm \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   PremisesAddress1 \textless{}chr\textgreater{}, PremisesAddress2 \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   PremisesAddress3 \textless{}chr\textgreater{}, PremisesSuburb \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   PremisesStateAb \textless{}chr\textgreater{}, PremisesPostCode \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   Latitude \textless{}dbl\textgreater{}, Longitude \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

This dataset is for the whole of Australia, but as we are just going to plot the area around Canberra we filter to that and only to booths that are geographic (the AEC has various options for people who are in hospital, or not able to get to a booth, etc, and these are still `booths' in this dataset).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Reduce the booths data to only rows with that have latitude and longitude}
\NormalTok{booths\_reduced }\OtherTok{\textless{}{-}}
\NormalTok{  booths }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{filter}\NormalTok{(State }\SpecialCharTok{==} \StringTok{"ACT"}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{select}\NormalTok{(PollingPlaceID, DivisionNm, Latitude, Longitude) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(Longitude)) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} \CommentTok{\# Remove rows that do not have a geography}
  \FunctionTok{filter}\NormalTok{(Longitude }\SpecialCharTok{\textless{}} \DecValTok{165}\NormalTok{) }\CommentTok{\# Remove Norfolk Island}
\end{Highlighting}
\end{Shaded}

Now we can use \texttt{ggmap} in the same way as before to plot our underlying tiles, and then build on that using \texttt{geom\_point()} to add our points of interest.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggmap}\NormalTok{(canberra\_stamen\_map,}
      \AttributeTok{extent =} \StringTok{"normal"}\NormalTok{,}
      \AttributeTok{maprange =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ booths\_reduced,}
             \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Longitude,}
                 \AttributeTok{y =}\NormalTok{ Latitude,}
                 \AttributeTok{colour =}\NormalTok{ DivisionNm),) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{name =} \StringTok{"2019 Division"}\NormalTok{, }\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_map}\NormalTok{(}
    \AttributeTok{projection =} \StringTok{"mercator"}\NormalTok{,}
    \AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\FunctionTok{attr}\NormalTok{(map, }\StringTok{"bb"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{ll.lon, }\FunctionTok{attr}\NormalTok{(map, }\StringTok{"bb"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{ur.lon),}
    \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\FunctionTok{attr}\NormalTok{(map, }\StringTok{"bb"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{ll.lat, }\FunctionTok{attr}\NormalTok{(map, }\StringTok{"bb"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{ur.lat)}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Longitude"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Latitude"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{panel.grid.major =} \FunctionTok{element\_blank}\NormalTok{(),}
        \AttributeTok{panel.grid.minor =} \FunctionTok{element\_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\includegraphics{11-static_communication_files/figure-latex/unnamed-chunk-28-1.pdf}

We may like to save the map so that we do not have to draw it every time, and we can do that in the same way as any other graph, using \texttt{ggsave()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggsave}\NormalTok{(}\StringTok{"map.pdf"}\NormalTok{, }\AttributeTok{width =} \DecValTok{20}\NormalTok{, }\AttributeTok{height =} \DecValTok{10}\NormalTok{, }\AttributeTok{units =} \StringTok{"cm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Finally, the reason that we used Stamen Maps and OpenStreetMap is because it is open source, but you can also use Google Maps. This requires you to first register a credit card with Google, and specify a key, but with low usage should be free. Using Google Maps, \texttt{get\_googlemap()}, brings some advantages over \texttt{get\_stamenmap()}, for instance it will attempt to find a placename, rather than needing to specify a bounding box.

\hypertarget{toronto-bike-parking}{%
\subsection{Toronto bike parking}\label{toronto-bike-parking}}

Let us see another example of a static map, this time using Toronto data accessed using \texttt{opendatatoronto} \citep{citeSharla}. The dataset that we are going to plot is available here: \url{https://open.toronto.ca/dataset/street-furniture-bicycle-parking/}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Based on: https://open.toronto.ca/dataset/street{-}furniture{-}bicycle{-}parking/.}
\FunctionTok{library}\NormalTok{(opendatatoronto)}
\CommentTok{\# (The string identifies the package.)}
\NormalTok{resources }\OtherTok{\textless{}{-}} \FunctionTok{list\_package\_resources}\NormalTok{(}\StringTok{"71e6c206{-}96e1{-}48f1{-}8f6f{-}0e804687e3be"}\NormalTok{)}
\CommentTok{\# In this case there is only one dataset within this resource so just need the first one    }
\NormalTok{bike\_parking\_locations }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(resources, }\FunctionTok{row\_number}\NormalTok{()}\SpecialCharTok{==}\DecValTok{1}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} \FunctionTok{get\_resource}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(bike\_parking\_locations)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 20}
\CommentTok{\#\textgreater{}     \textasciigrave{}\_id\textasciigrave{} OBJECTID ID       ADDRESSNUMBERTEXT ADDRESSSTREET }
\CommentTok{\#\textgreater{}     \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{} \textless{}chr\textgreater{}    \textless{}chr\textgreater{}             \textless{}chr\textgreater{}         }
\CommentTok{\#\textgreater{} 1 3613143        4 BP{-}11699 70                The Pond Rd   }
\CommentTok{\#\textgreater{} 2 3613144        9 BP{-}11900 8                 Assiniboine Rd}
\CommentTok{\#\textgreater{} 3 3613145       56 BP{-}11338 1495              Queen St W    }
\CommentTok{\#\textgreater{} 4 3613146       65 BP{-}03501 8                 Kensington Ave}
\CommentTok{\#\textgreater{} 5 3613147      100 BP{-}03280 87                Avenue Rd     }
\CommentTok{\#\textgreater{} 6 3613148      109 BP{-}12883 21                Canniff St    }
\CommentTok{\#\textgreater{} \# ... with 15 more variables: FRONTINGSTREET \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   SIDE \textless{}chr\textgreater{}, FROMSTREET \textless{}chr\textgreater{}, DIRECTION \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   SITEID \textless{}lgl\textgreater{}, WARD \textless{}chr\textgreater{}, BIA \textless{}chr\textgreater{}, ASSETTYPE \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   STATUS \textless{}chr\textgreater{}, SDE\_STATE\_ID \textless{}dbl\textgreater{}, X \textless{}dbl\textgreater{}, Y \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   LONGITUDE \textless{}dbl\textgreater{}, LATITUDE \textless{}dbl\textgreater{}, geometry \textless{}chr\textgreater{}}
\end{Highlighting}
\end{Shaded}

First, we need to clean the data are little. There are some clear errors in the ADDRESSNUMBERTEXT field, but not too many, so we just ignore it and focus on the data that we are interested in.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bike\_data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{ward =}\NormalTok{ bike\_parking\_locations}\SpecialCharTok{$}\NormalTok{WARD,}
  \AttributeTok{id =}\NormalTok{ bike\_parking\_locations}\SpecialCharTok{$}\NormalTok{ID,}
  \AttributeTok{status =}\NormalTok{ bike\_parking\_locations}\SpecialCharTok{$}\NormalTok{STATUS,}
  \AttributeTok{street\_address =} \FunctionTok{paste}\NormalTok{(}
\NormalTok{    bike\_parking\_locations}\SpecialCharTok{$}\NormalTok{ADDRESSNUMBERTEXT,}
\NormalTok{    bike\_parking\_locations}\SpecialCharTok{$}\NormalTok{ADDRESSSTREET}
\NormalTok{  ),}
  \AttributeTok{latitude =}\NormalTok{ bike\_parking\_locations}\SpecialCharTok{$}\NormalTok{LATITUDE,}
  \AttributeTok{longitude =}\NormalTok{ bike\_parking\_locations}\SpecialCharTok{$}\NormalTok{LONGITUDE}
\NormalTok{)}
\FunctionTok{rm}\NormalTok{(bike\_parking\_locations)}
\end{Highlighting}
\end{Shaded}

Some of the bike racks were temporary so remove them and also let us just look at the area around the University of Toronto, which is Ward 11

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Only keep ones that still exist}
\NormalTok{bike\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  bike\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{filter}\NormalTok{(status }\SpecialCharTok{==} \StringTok{"Existing"}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{status)}

\NormalTok{bike\_data }\OtherTok{\textless{}{-}}\NormalTok{ bike\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(ward }\SpecialCharTok{==} \DecValTok{11}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{ward)}
\end{Highlighting}
\end{Shaded}

If you look at the dataset at this point, then you will notice that there is a row for every bike parking spot. But we do not really need to know that, because sometimes there are lots right next to each other. Instead, we'd just like the one point. So, we want to create a count by address, and then just get one instance per address.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bike\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  bike\_data }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(street\_address) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{number\_of\_spots =} \FunctionTok{n}\NormalTok{(),}
         \AttributeTok{running\_total =} \FunctionTok{row\_number}\NormalTok{()}
\NormalTok{         ) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(running\_total }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{id, }\SpecialCharTok{{-}}\NormalTok{running\_total)}

\FunctionTok{head}\NormalTok{(bike\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 4}
\CommentTok{\#\textgreater{}   street\_address   latitude longitude number\_of\_spots}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}               \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}           \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 8 Kensington Ave     43.7     {-}79.4               1}
\CommentTok{\#\textgreater{} 2 87 Avenue Rd         43.7     {-}79.4               4}
\CommentTok{\#\textgreater{} 3 162 Mc Caul St       43.7     {-}79.4               1}
\CommentTok{\#\textgreater{} 4 147 Baldwin St       43.7     {-}79.4               2}
\CommentTok{\#\textgreater{} 5 888 Yonge St         43.7     {-}79.4               1}
\CommentTok{\#\textgreater{} 6 180 Elizabeth St     43.7     {-}79.4              10}
\end{Highlighting}
\end{Shaded}

Now we can grab our tile and add our bike rack data onto it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bbox }\OtherTok{\textless{}{-}}
  \FunctionTok{c}\NormalTok{(}
    \AttributeTok{left =} \SpecialCharTok{{-}}\FloatTok{79.420390}\NormalTok{,}
    \AttributeTok{bottom =} \FloatTok{43.642658}\NormalTok{,}
    \AttributeTok{right =} \SpecialCharTok{{-}}\FloatTok{79.383354}\NormalTok{,}
    \AttributeTok{top =} \FloatTok{43.672557}
\NormalTok{  )}

\NormalTok{toronto\_stamen\_map }\OtherTok{\textless{}{-}}
  \FunctionTok{get\_stamenmap}\NormalTok{(bbox, }\AttributeTok{zoom =} \DecValTok{14}\NormalTok{, }\AttributeTok{maptype =} \StringTok{"toner{-}lite"}\NormalTok{)}

\FunctionTok{ggmap}\NormalTok{(toronto\_stamen\_map,  }\AttributeTok{maprange =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ bike\_data,}
             \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ longitude,}
                 \AttributeTok{y =}\NormalTok{ latitude),}
             \AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Longitude"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Latitude"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\includegraphics{11-static_communication_files/figure-latex/unnamed-chunk-37-1.pdf}

\hypertarget{geocoding}{%
\subsection{Geocoding}\label{geocoding}}

To this point we assumed that we already had geocoded data. But the places `Canberra, Australia', or `Ottawa, Canada', are just names, they do not actually inherently have a location. In order to plot them we need to get a latitude and longitude for them. The process of going from names to coordinates is called geocoding.

There are a range of options to geocode data in R, but \texttt{tidygeocoder} is especially useful \citep{citetidygeocoder}. To get started using the package we need a dataframe of locations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{some\_locations }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{city =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Canberra\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Ottawa\textquotesingle{}}\NormalTok{),}
         \AttributeTok{country =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Australia\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Canada\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidygeocoder}\SpecialCharTok{::}\FunctionTok{geo}\NormalTok{(}\AttributeTok{city =}\NormalTok{ some\_locations}\SpecialCharTok{$}\NormalTok{city, }
                  \AttributeTok{country =}\NormalTok{ some\_locations}\SpecialCharTok{$}\NormalTok{country, }
                  \AttributeTok{method =} \StringTok{\textquotesingle{}osm\textquotesingle{}}\NormalTok{)}
\CommentTok{\#\textgreater{} Passing 2 addresses to the Nominatim single address geocoder}
\CommentTok{\#\textgreater{} Query completed in: 2 seconds}
\CommentTok{\#\textgreater{} \# A tibble: 2 x 4}
\CommentTok{\#\textgreater{}   city     country     lat  long}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}    \textless{}chr\textgreater{}     \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Canberra Australia {-}35.3 149. }
\CommentTok{\#\textgreater{} 2 Ottawa   Canada     45.4 {-}75.7}
\end{Highlighting}
\end{Shaded}

\hypertarget{exercises-and-tutorial-5}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-5}}

\hypertarget{exercises-5}{%
\subsection{Exercises}\label{exercises-5}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  I have a dataset that contains measurements of height (in cm) for a sample of 300 penguins, who are either the Adeline or Emperor species. I am interested in visualizing the distribution of heights by species in a graphical way. Please discuss whether a pie chart is an appropriate type of graph to use. What about a box and whisker plot? Finally, what are some considerations if you made a histogram? {[}Please write a paragraph or two for each aspect.{]}
\item
  Assume the dataset and columns exist. Would this code work? \texttt{data\ \textbar{}\textgreater{}\ ggplot(aes(x\ =\ col\_one))\ \textbar{}\textgreater{}\ geom\_point()} (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Yes
  \item
    No
  \end{enumerate}
\item
  If I have categorical data, which geom should I use to plot it (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{geom\_bar()}
  \item
    \texttt{geom\_point()}
  \item
    \texttt{geom\_abline()}
  \item
    \texttt{geom\_boxplot()}
  \end{enumerate}
\item
  Why are boxplots often inappropriate (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    They hide the full distribution of the data.
  \item
    They are hard to make.
  \item
    They are ugly.
  \item
    The mode is clearly displayed.
  \end{enumerate}
\item
  Which of the following, if any, are elements of the layered grammar of graphics \citep{wickham2010layered} (select all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A default dataset and set of mappings from variables to aesthetics.
  \item
    One or more layers, with each layer having one geometric object, one statistical transformation, one position adjustment, and optionally, one dataset and set of aesthetic mappings.
  \item
    Colors that enable the reader to understand the main point.
  \item
    A coordinate system.
  \item
    The facet specification.
  \item
    One scale for each aesthetic mapping used.
  \end{enumerate}
\end{enumerate}

\hypertarget{tutorial-5}{%
\subsection{Tutorial}\label{tutorial-5}}

Using R Markdown, please create a graph using \texttt{ggplot2} and a map using \texttt{ggmap} and add explanatory text to accompany both. This should take one or two pages for each of them.

For the graph, please reflect on \citet{vanderplas2020testing} and add a few paragraphs about the different options that you considered that the graph more effective.

For the map, please reflect on the following quote from Heather Krause: `maps only show people who aren't invisible to the makers' as well as Chapter 3 from \citet{datafeminism2020} and add a few paragraphs related to this.

\hypertarget{interactive-communication}{%
\chapter{Interactive communication}\label{interactive-communication}}

\textbf{Required material}

\begin{itemize}
\tightlist
\item
  Read \emph{M-F-E-O: postcards + distill}, \citep{hillpostcards}.
\item
  Read \emph{Building a blog with distill}, \citep{citemockblog}.
\item
  Read \emph{Geocomputation with R}, Chapter 2 `Geographic data in R', \citep{lovelace2019geocomputation}.
\item
  Read \emph{Mastering Shiny}, Chaper 1 `Your first Shiny app', \citep{wickham2021mastering}.
\end{itemize}

\textbf{Key concepts and skills}

\begin{itemize}
\tightlist
\item
  Building a website using within the R environment using: \texttt{postcards} \citep{citepostcards}, \texttt{distill} \citep{citedistill}, and \texttt{blogdown} \citep{citeblogdown}.
\item
  Adding interaction to maps, using \texttt{leaflet} \citep{ChengKarambelkarXie2017} and \texttt{mapdeck} \citep{citemapdeck}.
\item
  Adding interaction to graphs, using \texttt{Shiny} \citep{citeshiny}.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{blogdown} \citep{citeblogdown}
\item
  \texttt{distill} \citep{citedistill}
\item
  \texttt{leaflet} \citep{ChengKarambelkarXie2017}
\item
  \texttt{mapdeck} \citep{citemapdeck}
\item
  \texttt{postcards} \citep{citepostcards}
\item
  \texttt{tidyverse} \citep{citetidyverse}
\item
  \texttt{usethis} \citep{citeusethis}
\end{itemize}

\textbf{Key functions}

\begin{itemize}
\tightlist
\item
  \texttt{blogdown::serve\_site()}
\item
  \texttt{distill::create\_article()}
\item
  \texttt{leaflet::addCircleMarkers()}
\item
  \texttt{leaflet::addLegend()}
\item
  \texttt{leaflet::addMarkers()}
\item
  \texttt{leaflet::addTiles()}
\item
  \texttt{leaflet::leaflet()}
\item
  \texttt{mapdeck:::add\_scatterplot()}
\item
  \texttt{mapdeck::mapdeck()}
\item
  \texttt{mapdeck::mapdeck\_style()}
\item
  \texttt{shiny::fluidPage()}
\item
  \texttt{shiny::mainPanel()}
\item
  \texttt{shiny::plotOutput()}
\item
  \texttt{shiny::renderPlot()}
\item
  \texttt{shiny::shinyApp()}
\item
  \texttt{usethis::edit\_r\_environ()}
\item
  \texttt{usethis::use\_git()}
\item
  \texttt{usethis::use\_github()}
\end{itemize}

\hypertarget{introduction-4}{%
\section{Introduction}\label{introduction-4}}

Books and papers have been the primary mediums for communication for thousands of years. But with the rise of computers, and especially the internet, in recent decades, these static approaches have been complemented with interactive approaches. Fundamentally, the internet is about making files available others. If we additionally allow them to do something with what we make available, then we need to take a variety of additional aspects into consideration.

In this chapter we begin by covering how to create and publish a website. This serves as a place to host a portfolio of work. After that we cover adding interaction to maps and graphs, which are two that nicely lend themselves to this.

\hypertarget{making-a-website}{%
\section{Making a website}\label{making-a-website}}

A website is a critical part of communication. For instance, it is a place to make a portfolio of work publicly available. One way to make a website is to use \texttt{blogdown} \citep{citeblogdown}. \texttt{blogdown} is a package that allows you to make websites, not just blogs, notwithstanding its name, largely within R Studio. It builds on `Hugo', which is a popular general framework for making websites. \texttt{blogdown} enables us to freely and quickly get a website up-and-running. It is easy to add content from time-to-time. And it integrates with R Markdown, which makes it easy to share work. But \texttt{blogdown} is brittle. Because it is so dependent on Hugo, features that work today may not work tomorrow. Also, owners of Hugo templates can update them at any time, without thought to existing users. \texttt{blogdown} is a good option if we know what we are doing, or have a specific use-case, or style, in mind. But two other alternatives are better starting points.

The first is \texttt{distill} \citep{citedistill}. Again, this is an R package that wraps around another framework, in this case `Distill'. But in contrast to Hugo, Distill is more focused on common needs in data science, and is also only maintained by one group, so it may be considered a more stable choice. That said, the default \texttt{distill} site is a little plain looking. As such, following \citet{hillpostcards}, we will pair it with a third option, \texttt{postcards} \citep{citepostcards}.

The third option, and the one that we will start with, is \texttt{postcards} \citep{citepostcards}. This is a tailored solution that creates simple biographical websites that look great. Having set-up GitHub in R Studio, it is literally possible to have a \texttt{postcards} website online in five minutes.

\hypertarget{postcards}{%
\subsection{Postcards}\label{postcards}}

Begin by installing \texttt{postcards}, with \texttt{install.packages(\textquotesingle{}postcards\textquotesingle{})} and then creating a new project for the website (`File' -\textgreater{} `New Project' -\textgreater{} `New Directory' -\textgreater{} `Postcards Website'). We can then pick a name and location for the project, and select a postcards theme. In this case, we can start with `trestles' but this can be changed later. Click the option to `Open in new session' and then create the project.

That will open a new file and we can now build the site by clicking `Knit'. This will result in a one-page website (Figure \ref{fig:trestles})/

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/trestles} \caption{Example of a website made with `postcards` using the 'trestles' theme}\label{fig:trestles}
\end{figure}

We can now update the basic content to match our own (Figure \ref{fig:trestlesredux}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/trestlesredux} \caption{Example of Trestles website with updated details}\label{fig:trestlesredux}
\end{figure}

Once the details are personalized, then we push it to GitHub. GitHub would try to build the site, which we do not want, so we first add a hidden file to turn that off, by running this in the console:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{file.create}\NormalTok{(}\StringTok{\textquotesingle{}.nojekyll\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then, assuming GitHub was set-up in Chapter \ref{reproducible-workflows}, we can use \texttt{usethis} \citep{citeusethis} to get it onto GitHub. We use \texttt{use\_git()} to initialize a Git repository, and then \texttt{use\_github()} pushes it to GitHub.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(usethis)}
\FunctionTok{use\_git}\NormalTok{()}
\FunctionTok{use\_github}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

The project will then be on GitHub. We can use GitHub pages to host it: `Settings -\textgreater{} Pages' and then change the source to `main' or `master', depending on your settings. GitHub will let you know the address that you can share to visit your site.

\hypertarget{distill}{%
\subsection{Distill}\label{distill}}

We will now use \texttt{distill} \citep{citedistill} to build additional infrastructure around our \texttt{postcards} site, following \citet{hillpostcards}. After that we will explore some of the aspects of \texttt{distill} that make it a nice choice, and mention some of the trade-offs. First, we install \texttt{distill} with \texttt{install.packages(\textquotesingle{}distill\textquotesingle{})}, and again, create a new project for the website (`File' -\textgreater{} `New Project' -\textgreater{} `New Directory' -\textgreater{} `Distill Blog').

We can then pick a name and location for the project, and set a title. Select `Configure for GitHub Pages' and also `Open in a new session'. These options can be changed \emph{ex post}. It should look something like Figure \ref{fig:distillone}.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/distill_one} \caption{Example settings for setting up `distill`}\label{fig:distillone}
\end{figure}

At this point we can click `Build Website' in the Build tab, and we should see the default website (Figure \ref{fig:distilltwo}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/distill_two} \caption{Example of default `distill` website}\label{fig:distilltwo}
\end{figure}

Again, now we need to update it to reflect our own details. The default for a `Distill Blog' is that the blog is the homepage. We can change that to use the \texttt{postcards} page as the homepage. First we change the name of `index.Rmd' to `blog.Rmd' and then create a new `trestles' page:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{postcards}\SpecialCharTok{::}\FunctionTok{create\_postcard}\NormalTok{(}\AttributeTok{file =} \StringTok{"index.Rmd"}\NormalTok{, }\AttributeTok{template =} \StringTok{"trestles"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The trestles page will open, and we need to add the following line in the yaml file: \texttt{site:\ distill::distill\_website}. In Figure \ref{fig:distillthree} it was added at line 16, and then we can rebuild the website.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/distill_three} \caption{Updating the yaml to change the homepage}\label{fig:distillthree}
\end{figure}

We can make the same changes to the default content as earlier, updating the links, image, and bio. The advantage of using \texttt{distill} is that we now have additional pages, not just a one-page website, and we also have a blog. By default, we have an `about' page, but some other pages that may be useful, depending on the particular use-case, include: `research', `teaching', `talks', `projects', `software', `datasets'. As an example, we will add and edit a page called `software' using \texttt{distill::create\_article(file\ =\ \textquotesingle{}software\textquotesingle{})}.

That will create and open an R Markdown document. To add it to the website, open '\_site.yml' and then add a line to the `navbar' (Figure \ref{fig:distillfour}. After this is done then we can rebuild the website, and the `software' page will have been added.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/distill_four} \caption{Adding another page to the website}\label{fig:distillfour}
\end{figure}

We can continue with this process until we are happy with the website. For instance, we may want to add a blog. To do this we follow the same pattern as before, but with `blog' instead of `software'.

When we are happy with our website, we can push it to GitHub and then use GitHub Pages to host it, in the same way that we did with the \texttt{postcards} site.

Using the \texttt{distill} is a good option when we need a multi-page website, but still want a fairly controlled environment. There are many options that can be changed, and \citet{hillpostcards} is a good starting point, in addition to the \texttt{distill} homepage: \url{https://rstudio.github.io/distill/}.

That said, \texttt{distill} is opinionated. While it is a great option, if we want something a little more flexible then \texttt{blogdown} might be a better option.

\hypertarget{blogdown}{%
\subsection{Blogdown}\label{blogdown}}

Using \texttt{blogdown} \citep{citeblogdown} is more work than Google sites or Squarespace. It requires a little more knowledge than using a basic Wordpress site. And if you want to customize absolutely every aspect of your website, or need everything to be `just so' then \texttt{blogdown} may not be a good option. Further, \texttt{blogdown} is still under active development and various aspects may break in future releases. However, \texttt{blogdown} allows a variety and level of expression that is not possible with \texttt{distill}. \citet{citealisonhillblogdown} and \citet{blogdownbook} are excellent options for learning more about \texttt{blogdown}.

First we will need to install \texttt{blogdown} \texttt{install.packages("blogdown")}. And then create a new project for the website (`File' -\textgreater{} `New Project' -\textgreater{} `New Directory' -\textgreater{} `Website using blogdown'). At this point you can set a name and location, and also select `Open in a new session' (Figure \ref{fig:blogdownone}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/blogdown_one} \caption{Example settings for setting up blogdown}\label{fig:blogdownone}
\end{figure}

We can click `Build Website' from the `Build' pane, but then an extra step is needed; we need to serve the site \texttt{blogdown:::serve\_site()}. After this, the site will show in the `Viewer' pane (Figure \ref{fig:blogdowntwo}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/blogdown_two} \caption{Serving default blogdown site}\label{fig:blogdowntwo}
\end{figure}

At this point, the default website is being `served' locally. This means that changes we make will be reflected in the website that we see in the Viewer pane. To see the website in a web browser, click `Show in new window' on the top left of the Viewer. That will open the website using the address that the R Studio also tells you.

We now want to update the content, starting with the `About' section. To do that go to `content -\textgreater{} about.md' and add your own content. One nice aspect of \texttt{blogdown} is that it will automatically re-load the content when you save, so you should see your changes immediately show up. You may also like to change the logo. You could do this by adding a square image to `public/images/' and then changing the call to `logo.png' in `config.yaml'. When we are happy with it, we can make our website public in the same way as we did for \texttt{postcards}.

One advantage of using \texttt{blogdown} is that it allows us to use Hugo templates. This provides a large number of beautifully crafted websites. To pick a theme we go to the Hugo themes page: \url{https://themes.gohugo.io}. There are hundreds of different themes. In general, most of them can be made to work with \texttt{blogdown}, but sometimes it can be a bit of a hassle to get them working.

One nice option is Apéro: \url{https://hugo-apero-docs.netlify.app}. We can specify the use of this them as part of creating a new site (`File -\textgreater{} New Project -\textgreater{} New Directory -\textgreater{} Website using blogdown'). At this point, in addition to setting the name and location, we can specify a theme. Specifically, in `Hugo theme' field, we specify a GitHub username and repository, which in this case is `hugo-apero/apero' (Figure \ref{fig:blogdownthree}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/blogdown_three} \caption{Using the Apéro theme}\label{fig:blogdownthree}
\end{figure}

\hypertarget{interactive-maps}{%
\section{Interactive maps}\label{interactive-maps}}

The nice thing about interactive maps is that we can let our user decide what they are interested in. For instance, in the case of Canadian politics, some people will be interested in Toronto ridings, while others will be interested in Manitoba, etc. But it would be difficult to present a map that focuses on both of those, so an interactive map is a great option for allowing users to focus on what they want.

That said, it is important to be cognizant of what we are doing when we build maps, and more broadly, what is being done at scale to enable us to be able to build our own maps. For instance, with regard to Google, \citet{mcquire2019one} says:

\begin{quote}
Google began life in 1998 as a company famously dedicated to organising the vast amounts of data on the Internet. But over the last two decades its ambitions have changed in a crucial way. Extracting data such as words and numbers from the physical world is now merely a stepping-stone towards apprehending and organizing the physical world as data. Perhaps this shift is not surprising at a moment when it has become possible to comprehend human identity as a form of (genetic) `code'. However, apprehending and organizing the world as data under current settings is likely to take us well beyond Heidegger's `standing reserve' in which modern technology enframed `nature' as productive resource. In the 21st century, it is the stuff of human life itself---from genetics to bodily appearances, mobility, gestures, speech, and behaviour---that is being progressively rendered as productive resource that can not only be harvested continuously but subject to modulation over time.
\end{quote}

Does this mean that we should not use or build interactive maps? Of course not. But it is important to be aware of the fact that this is a frontier, and the boundaries of appropriate use are still being determined. Indeed, the literal boundaries of the maps themselves are being consistently determined and updated. The move to digital maps, compared with physical printed maps, means that it is actually possible for different users to be presented with different realities. For instance, `\ldots Google routinely takes sides in border disputes. Take, for instance, the representation of the border between Ukraine and Russia. In Russia, the Crimean Peninsula is represented with a hard-line border as Russian-controlled, whereas Ukrainians and others see a dotted-line border. The strategically important peninsula is claimed by both nations and was violently seized by Russia in 2014, one of many skirmishes over control' \citet{washingtonpostmaps}.

\hypertarget{leaflet}{%
\subsection{Leaflet}\label{leaflet}}

We can use \texttt{leaflet} \citep{ChengKarambelkarXie2017} to make interactive maps. The essentials are similar to \texttt{ggmap} \citep{KahleWickham2013}, but there are many additional aspects beyond that. We can redo the bike map from Chapter \ref{static-communication}.

In the same way as a graph in \texttt{ggplot2} begins with \texttt{ggplot()}, a map in \texttt{leaflet} begins with \texttt{leaflet()}. Here we can specify data, and other options such as width and height. After this, we add `layers' in the same way that we added them in \texttt{ggplot2}. The first layer that we add is a tile, using \texttt{addTiles()}. In this case, the default is from OpenStreeMap. After that we add markers with \texttt{addMarkers()} to show the location of each bike parking spot.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(leaflet)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{bike\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"outputs/data/bikes.csv"}\NormalTok{)}

\FunctionTok{leaflet}\NormalTok{(}\AttributeTok{data =}\NormalTok{ bike\_data) }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{addTiles}\NormalTok{() }\SpecialCharTok{|}\ErrorTok{\textgreater{}}  \CommentTok{\# Add default OpenStreetMap map tiles}
  \FunctionTok{addMarkers}\NormalTok{(}\AttributeTok{lng =}\NormalTok{ bike\_data}\SpecialCharTok{$}\NormalTok{longitude, }
             \AttributeTok{lat =}\NormalTok{ bike\_data}\SpecialCharTok{$}\NormalTok{latitude, }
             \AttributeTok{popup =}\NormalTok{ bike\_data}\SpecialCharTok{$}\NormalTok{street\_address,}
             \AttributeTok{label =} \SpecialCharTok{\textasciitilde{}}\FunctionTok{as.character}\NormalTok{(bike\_data}\SpecialCharTok{$}\NormalTok{number\_of\_spots))}
\end{Highlighting}
\end{Shaded}

\includegraphics{12-interactive_communication_files/figure-latex/unnamed-chunk-4-1.png}

There are two new arguments. The first is `popup', which is what happens when the user clicks on the marker. Here the address will be provided. The second is `label', which is what happens when the user hovers on the marker. Here the number of parking spots is specified.

We can try another example, this time of the fire stations in Toronto. We can use data from Open Data Toronto using \texttt{opendatatoronto} \citep{citeSharla}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(opendatatoronto)}
\CommentTok{\# Get starter code from: https://open.toronto.ca/dataset/fire{-}station{-}locations/}
\NormalTok{fire\_stations\_locations }\OtherTok{\textless{}{-}} \FunctionTok{get\_resource}\NormalTok{(}\StringTok{\textquotesingle{}9d1b7352{-}32ce{-}4af2{-}8681{-}595ce9e47b6e\textquotesingle{}}\NormalTok{)}
\CommentTok{\# Grab the lat and long {-} thanks https://stackoverflow.com/questions/47661354/converting{-}geometry{-}to{-}longitude{-}latitude{-}coordinates{-}in{-}r}
\NormalTok{fire\_stations\_locations }\OtherTok{\textless{}{-}} 
\NormalTok{  fire\_stations\_locations }\SpecialCharTok{|}\ErrorTok{\textgreater{}} 
\NormalTok{  tidyr}\SpecialCharTok{::}\FunctionTok{extract}\NormalTok{(geometry, }\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}lon\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}lat\textquotesingle{}}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{((.*), (.*)}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{)\textquotesingle{}}\NormalTok{, }\AttributeTok{convert =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(fire\_stations\_locations)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 26}
\CommentTok{\#\textgreater{}   \textasciigrave{}\_id\textasciigrave{}    ID NAME     ADDRESS   ADDRESS\_POINT\_ID ADDRESS\_ID}
\CommentTok{\#\textgreater{}   \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}chr\textgreater{}    \textless{}chr\textgreater{}                \textless{}dbl\textgreater{}      \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1     1    21 FIRE ST\textasciitilde{} 900 TAPS\textasciitilde{}          4236992     363382}
\CommentTok{\#\textgreater{} 2     2    60 FIRE ST\textasciitilde{} 106 ASCO\textasciitilde{}           764237      70190}
\CommentTok{\#\textgreater{} 3     3    61 FIRE ST\textasciitilde{} 65 HENDR\textasciitilde{}           819425     127148}
\CommentTok{\#\textgreater{} 4     4    55 FIRE ST\textasciitilde{} 260 ADEL\textasciitilde{}         12763904     484214}
\CommentTok{\#\textgreater{} 5     5    24 FIRE ST\textasciitilde{} 745 MEAD\textasciitilde{}          6349868     357277}
\CommentTok{\#\textgreater{} 6     6    74 FIRE ST\textasciitilde{} 140 LANS\textasciitilde{}         10757599     157562}
\CommentTok{\#\textgreater{} \# ... with 20 more variables: CENTRELINE\_ID \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   MAINT\_STAGE \textless{}chr\textgreater{}, ADDRESS\_NUMBER \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   LINEAR\_NAME\_FULL \textless{}chr\textgreater{}, POSTAL\_CODE \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   GENERAL\_USE \textless{}chr\textgreater{}, CLASS\_FAMILY\_DESC \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   ADDRESS\_ID\_LINK \textless{}dbl\textgreater{}, PLACE\_NAME \textless{}chr\textgreater{}, X \textless{}lgl\textgreater{},}
\CommentTok{\#\textgreater{} \#   Y \textless{}lgl\textgreater{}, LATITUDE \textless{}lgl\textgreater{}, LONGITUDE \textless{}lgl\textgreater{},}
\CommentTok{\#\textgreater{} \#   WARD\_NAME \textless{}chr\textgreater{}, MUNICIPALITY\_NAME \textless{}chr\textgreater{}, ...}
\end{Highlighting}
\end{Shaded}

There is a lot of information here, but we will just plot the location of each fire station along with their name and address.

We will introduce a different type of marker here, which is circles. This will allow us to use different colors for the outcomes of each type. There are four possible outcomes: ``Fire/Ambulance Stations'', ``Fire Station'', ``Restaurant'', ``Unknown''.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(leaflet)}

\NormalTok{pal }\OtherTok{\textless{}{-}}
  \FunctionTok{colorFactor}\NormalTok{(}\StringTok{"Dark2"}\NormalTok{, }\AttributeTok{domain =}\NormalTok{ fire\_stations\_locations}\SpecialCharTok{$}\NormalTok{GENERAL\_USE }\SpecialCharTok{|}\ErrorTok{\textgreater{}} \FunctionTok{unique}\NormalTok{())}

\FunctionTok{leaflet}\NormalTok{() }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{addTiles}\NormalTok{() }\SpecialCharTok{|}\ErrorTok{\textgreater{}}  \CommentTok{\# Add default OpenStreetMap map tiles}
  \FunctionTok{addCircleMarkers}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ fire\_stations\_locations,}
    \AttributeTok{lng =}\NormalTok{ fire\_stations\_locations}\SpecialCharTok{$}\NormalTok{lon,}
    \AttributeTok{lat =}\NormalTok{ fire\_stations\_locations}\SpecialCharTok{$}\NormalTok{lat,}
    \AttributeTok{color =} \FunctionTok{pal}\NormalTok{(fire\_stations\_locations}\SpecialCharTok{$}\NormalTok{GENERAL\_USE),}
    \AttributeTok{popup =} \FunctionTok{paste}\NormalTok{(}
      \StringTok{"\textless{}b\textgreater{}Name:\textless{}/b\textgreater{}"}\NormalTok{,}
      \FunctionTok{as.character}\NormalTok{(fire\_stations\_locations}\SpecialCharTok{$}\NormalTok{NAME),}
      \StringTok{"\textless{}br\textgreater{}"}\NormalTok{,}
      \StringTok{"\textless{}b\textgreater{}Address:\textless{}/b\textgreater{}"}\NormalTok{,}
      \FunctionTok{as.character}\NormalTok{(fire\_stations\_locations}\SpecialCharTok{$}\NormalTok{ADDRESS),}
      \StringTok{"\textless{}br\textgreater{}"}
\NormalTok{    )}
\NormalTok{  ) }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{addLegend}\NormalTok{(}
    \StringTok{"bottomright"}\NormalTok{,}
    \AttributeTok{pal =}\NormalTok{ pal,}
    \AttributeTok{values =}\NormalTok{ fire\_stations\_locations}\SpecialCharTok{$}\NormalTok{GENERAL\_USE }\SpecialCharTok{|}\ErrorTok{\textgreater{}} \FunctionTok{unique}\NormalTok{(),}
    \AttributeTok{title =} \StringTok{"Type"}\NormalTok{,}
    \AttributeTok{opacity =} \DecValTok{1}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\includegraphics{12-interactive_communication_files/figure-latex/unnamed-chunk-8-1.png}

\hypertarget{mapdeck}{%
\subsection{Mapdeck}\label{mapdeck}}

\texttt{mapdeck} \citep{citemapdeck} is based on WebGL. This means the web browser will do a lot of work for us. This enables us to accomplish things with \texttt{mapdeck} that \texttt{leaflet} struggles with, such as larger datasets.

To this point we have used `stamen maps' as our underlying tile, but \texttt{mapdeck} uses `Mapbox': \url{https://www.mapbox.com/}. This requires registering an account and obtaining a token. This is free and only needs to be done once. Once we have that token we add it to our R environment (the details of this process are covered in Chapter \ref{gather-data}) by running \texttt{usethis::edit\_r\_environ()}, which will open a text file. There we can add our Mapbox secret token.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MAPBOX\_TOKEN }\OtherTok{=} \StringTok{\textquotesingle{}PUT\_YOUR\_MAPBOX\_SECRET\_HERE\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

We then save this `.Renviron' file, and restart R (`Session' -\textgreater{} `Restart R').

Having obtained a token, we can create a plot of our firefighters data from earlier.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mapdeck)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Attaching package: \textquotesingle{}mapdeck\textquotesingle{}}
\CommentTok{\#\textgreater{} The following object is masked from \textquotesingle{}package:tibble\textquotesingle{}:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     add\_column}

\FunctionTok{mapdeck}\NormalTok{(}\AttributeTok{style =} \FunctionTok{mapdeck\_style}\NormalTok{(}\StringTok{\textquotesingle{}dark\textquotesingle{}}\NormalTok{)}
\NormalTok{        ) }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{add\_scatterplot}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ fire\_stations\_locations, }
    \AttributeTok{lat =} \StringTok{"lat"}\NormalTok{, }
    \AttributeTok{lon =} \StringTok{"lon"}\NormalTok{, }
    \AttributeTok{layer\_id =} \StringTok{\textquotesingle{}scatter\_layer\textquotesingle{}}\NormalTok{,}
    \AttributeTok{radius =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{radius\_min\_pixels =} \DecValTok{5}\NormalTok{,}
    \AttributeTok{radius\_max\_pixels =} \DecValTok{100}\NormalTok{,}
    \AttributeTok{tooltip =} \StringTok{"ADDRESS"}
\NormalTok{  )}
\CommentTok{\#\textgreater{} Registered S3 method overwritten by \textquotesingle{}jsonify\textquotesingle{}:}
\CommentTok{\#\textgreater{}   method     from    }
\CommentTok{\#\textgreater{}   print.json jsonlite}
\end{Highlighting}
\end{Shaded}

\includegraphics{12-interactive_communication_files/figure-latex/unnamed-chunk-10-1.png}

\hypertarget{shiny}{%
\section{Shiny}\label{shiny}}

\texttt{shiny} \citep{citeshiny} is a way of making interactive web applications using R. It is fun, but fiddly. Here we are going to step through one way to take advantage of \texttt{shiny}. Which is to quickly add some interactivity to our graphs. We will return to \texttt{shiny} in more detail in Chapter \ref{deploying-models}.

We are going to make an interactive graph based on the `babynames' dataset from \texttt{babynames} \citep{citebabynames}. First, we will build a static version (Figure \ref{fig:babynames}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(babynames)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{top\_five\_names\_by\_year }\OtherTok{\textless{}{-}}
\NormalTok{  babynames }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(year, sex) }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n)) }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{)}

\NormalTok{top\_five\_names\_by\_year }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ n, }\AttributeTok{fill =}\NormalTok{ sex)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Babies with that name"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Occurances"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Sex"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{12-interactive_communication_files/figure-latex/babynames-1.pdf}
\caption{\label{fig:babynames}Popular baby names}
\end{figure}

We can see the most popular boys names tend to be more clustered, compared with the most-popular girls names, which may be more spread out. However, one thing that we might be interested in is how the effect of the `bins' parameter shapes what we see. We might like to use interactivity to explore different values.

To get started, create a new \texttt{shiny} app (`File -\textgreater{} New File -\textgreater{} Shiny Web App'). Give it a name, such as `not\_my\_first\_shiny' and then leave all the other options as the default. A new file `app.R' will open and we click `Run app' to see what it looks like.

Now replace the content in that file, `app.R', with the content below, and then again click `Run app'

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(shiny)}

\CommentTok{\# Define UI for application that draws a histogram}
\NormalTok{ui }\OtherTok{\textless{}{-}} \FunctionTok{fluidPage}\NormalTok{(}
  \CommentTok{\# Application title}
  \FunctionTok{titlePanel}\NormalTok{(}\StringTok{"Count of names for five most popular names each year."}\NormalTok{),}
  
  \CommentTok{\# Sidebar with a slider input for number of bins}
  \FunctionTok{sidebarLayout}\NormalTok{(}\FunctionTok{sidebarPanel}\NormalTok{(}
    \FunctionTok{sliderInput}\NormalTok{(}
      \AttributeTok{inputId =} \StringTok{"number\_of\_bins"}\NormalTok{,}
      \AttributeTok{label =} \StringTok{"Number of bins:"}\NormalTok{,}
      \AttributeTok{min =} \DecValTok{1}\NormalTok{,}
      \AttributeTok{max =} \DecValTok{50}\NormalTok{,}
      \AttributeTok{value =} \DecValTok{30}
\NormalTok{    )}
\NormalTok{  ),}
  
  \CommentTok{\# Show a plot of the generated distribution}
  \FunctionTok{mainPanel}\NormalTok{(}\FunctionTok{plotOutput}\NormalTok{(}\StringTok{"distPlot"}\NormalTok{)))}
\NormalTok{)}

\CommentTok{\# Define server logic required to draw a histogram}
\NormalTok{server }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(input, output) \{}
\NormalTok{  output}\SpecialCharTok{$}\NormalTok{distPlot }\OtherTok{\textless{}{-}} \FunctionTok{renderPlot}\NormalTok{(\{}
    \CommentTok{\# Draw the histogram with the specified number of bins}
\NormalTok{    top\_five\_names\_by\_year }\SpecialCharTok{|}\ErrorTok{\textgreater{}}
      \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ n, }\AttributeTok{fill =}\NormalTok{ sex)) }\SpecialCharTok{+}
      \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{, }\AttributeTok{bins =}\NormalTok{ input}\SpecialCharTok{$}\NormalTok{number\_of\_bins) }\SpecialCharTok{+}
      \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
      \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\SpecialCharTok{+}
      \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Babies with that name"}\NormalTok{,}
           \AttributeTok{y =} \StringTok{"Occurances"}\NormalTok{,}
           \AttributeTok{fill =} \StringTok{"Sex"}\NormalTok{)}
\NormalTok{  \})}
\NormalTok{\}}

\CommentTok{\# Run the application}
\FunctionTok{shinyApp}\NormalTok{(}\AttributeTok{ui =}\NormalTok{ ui, }\AttributeTok{server =}\NormalTok{ server)}
\end{Highlighting}
\end{Shaded}

You should find that you are served an interactive graph where you can change the number of bins and it should look like Figure \ref{fig:shinyone}.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/shiny_one} \caption{Example of Shiny app where the user controls the number of bins}\label{fig:shinyone}
\end{figure}

\hypertarget{exercises-and-tutorial-6}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-6}}

\hypertarget{exercises-6}{%
\subsection{Exercises}\label{exercises-6}}

\hypertarget{tutorial-6}{%
\subsection{Tutorial}\label{tutorial-6}}

\hypertarget{paper-1}{%
\subsection{Paper}\label{paper-1}}

At about this point, Paper 2 (Appendix \ref{paper-2}) would be appropriate.

\hypertarget{part-measure-and-acquire}{%
\part{Measure and acquire}\label{part-measure-and-acquire}}

\hypertarget{gather-data}{%
\chapter{Gather data}\label{gather-data}}

\textbf{STATUS: Under construction.}

\textbf{Recommended reading}

\begin{itemize}
\item
  Algorithmic thinking in the public interest: navigating technical, legal, and ethical hurdles to web scraping in the social sciences
\item
  Benoit, Kenneth, 2019, `Text as data: An overview', 17 July, \url{https://kenbenoit.net/pdfs/28\%20Benoit\%20Text\%20as\%20Data\%20draft\%202.pdf}.
\item
  Bolton, Liza, 2019, `A quick look at museums per capita', 26 March, \url{http://blog.dataembassy.co.nz/museums-per-capita/}.
\item
  Bryan, Jennifer, and Jim Hester, 2020, \emph{What They Forgot to Teach You About R}, Chapter 7, \url{https://rstats.wtf/index.html}.
\item
  Cardoso, Tom, 2019, `Introduction to scraping', \url{https://github.com/tomcardoso/intro-to-scraping}.
\item
  Clavelle, Tyler, 2017, `Using R to extract data from web APIs', 5 June, \url{https://www.tylerclavelle.com/code/2017/randapis/}.
\item
  Cooksey, Brian, 2014, `An Introduction to APIs', Zapier, 22 April, \url{https://zapier.com/learn/apis/}.
\item
  Dogucu, Mine, and Mine Çetinkaya-Runde, 2020 ,`Web Scraping in the Statistics and Data Science Curriculum: Challenges and Opportunities', 6 May.
\item
  Gelfand, Sharla, 2019, `Crying @ Sephora', 8 November, \url{https://sharla.party/post/crying-sephora/}.
\item
  Goldman, Shayna, 2019, `How Much Do NHL Players Really Make? Part 2: Taxes', \url{https://hockey-graphs.com/2019/01/08/how-much-do-nhl-players-really-make-part-2-taxes/}.
\item
  Graham, Shawn, 2019, `Scraping with rvest', 7 November, \url{https://electricarchaeology.ca/2019/11/07/scraping-with-rvest/}.
\item
  Henze, Martin, 2020, `Web Scraping with rvest + Astro Throwback', 23 January, \url{https://heads0rtai1s.github.io/2020/01/23/rvest-intro-astro/}.
\item
  Hudon, Caitlin, 2017, `'Blue Christmas: A data-driven search for the most depressing Christmas song', 22 December, \url{https://caitlinhudon.com/2017/12/22/blue-christmas/}.
\item
  Luscombe, Alex, 2020, `A Gentle Introduction to Tesseract OCR', 3 June, \url{https://alexluscombe.ca/post/ocr-tutorial/}.
\item
  Luscombe, Alex, 2020, `Getting your .pdfs into R', 5 August, \url{https://alexluscombe.ca/post/r-pdftools/}.
\item
  Luscombe, Alex, 2020, `Parsing your .pdfs in R', 10 August, \url{https://alexluscombe.ca/post/parsing-pdfs/}.
\item
  Marshall, James, `HTML Made Really Easy', \url{https://www.jmarshall.com/easy/html/}.
\item
  Marshall, James, `HTTP Made Really Easy', \url{https://www.jmarshall.com/easy/http/}.
\item
  Nakagawara, Ryo, 2020, `Intro to \{polite\} Web Scraping of Soccer Data with R!', 14 May, \url{https://ryo-n7.github.io/2020-05-14-webscrape-soccer-data-with-R/}.
\item
  Pavlik, Kaylin, 2020, `How do fiber types appear together in yarn blends?', 17 February, \url{https://www.kaylinpavlik.com/ravelry-yarn-fibers/}.
\item
  Silge, Julia and David Robinson, 2020, \emph{Text Mining with R}, Chapters 1, 3, and 6, \url{https://www.tidytextmining.com/}.
\item
  Silge, Julia, 2017, `Scraping CRAN with rvest', 5 March, \url{https://juliasilge.com/blog/scraping-cran/}.
\item
  Smale, David, 2020, `Daniel Johnston', \url{https://davidsmale.netlify.com/portfolio/daniel-johnston/}.
\item
  Taddy, Matt, 2019, \emph{Business Data Science}, Chapter 8, pp.~231-259.
\item
  Wickham, Hadley, `Managing Secrets', \url{https://cran.r-project.org/web/packages/httr/vignettes/secrets.html}.
\item
  Wickham, Hadley, 2014, `rvest: easy web scraping with R', 24 November, \url{https://blog.rstudio.com/2014/11/24/rvest-easy-web-scraping-with-r/}.
\item
  Wickham, Hadley, nd, `Getting started with httr', \url{https://cran.r-project.org/web/packages/httr/vignettes/quickstart.html}.
\end{itemize}

\textbf{Recommended viewing}

\begin{itemize}
\tightlist
\item
  D'Agostino McGowan, Lucy, 2020 `Harnessing the Power of the Web via R Clients for Web APIs', talk at ASA Joint Statistical Meeting 2018, \url{https://www.lucymcgowan.com/talk/asa_joint_statistical_meeting_2018/}.
\item
  Tatman, Rachel, 2018, `Character Encoding and You', 21 February, \url{https://youtu.be/2U9EHYqc59Y}.
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\tightlist
\item
  Use APIs where possible because the data provider has specified the data they would like to make available to you, and the conditions under which they are making it available.
\item
  Often R packages have been written to make it easier to use APIs.
\item
  Use R environments to manage your keys.
\item
  Using the verb GET (`a GET request') means providing a URL and the server will return something, using the verb POST (a POST request') means providing some data and the server will deal with that data.
\item
  Cleaning data
\item
  Graphing data to tell a story
\item
  Respectfully scraping data
\item
  Approaching extracting text from PDFs as a workflow.
\item
  Planning what is needed at the start.
\item
  Starting small and then iterating.
\item
  Putting in place checks.
\item
  Gathering text data.
\item
  Preparing text datasets.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{babynames}
\item
  \texttt{broom}
\item
  \texttt{dplyr}
\item
  \texttt{ggplot2}
\item
  \texttt{gutenbergr}
\item
  \texttt{janitor}
\item
  \texttt{jsonlite}
\item
  \texttt{pdftools}
\item
  \texttt{purrr}
\item
  \texttt{rtweet}
\item
  \texttt{rvest}
\item
  \texttt{spotifyr}
\item
  \texttt{stringi}
\item
  \texttt{tidymodels}
\item
  \texttt{tidytext}
\item
  \texttt{tidyverse}
\item
  \texttt{usethis}
\end{itemize}

\textbf{Key functions/etc}

\begin{itemize}
\tightlist
\item
  \texttt{as\_factor()}
\item
  \texttt{as\_tibble()}
\item
  \texttt{bind\_tf\_idf()}
\item
  \texttt{c()}
\item
  \texttt{case\_when()}
\item
  \texttt{cat()}
\item
  \texttt{edit\_r\_environ()}
\item
  \texttt{file()}
\item
  \texttt{fromJSON()}
\item
  \texttt{function()}
\item
  \texttt{GET()}
\item
  \texttt{get\_artist\_audio\_features()}
\item
  \texttt{get\_favorites()}
\item
  \texttt{get\_my\_top\_artists\_or\_tracks()}
\item
  \texttt{html\_node()}
\item
  \texttt{html\_nodes()}
\item
  \texttt{html\_text()}
\item
  \texttt{pdf\_data()}
\item
  \texttt{pdf\_text()}
\item
  \texttt{pmap\_dfr()}
\item
  \texttt{read\_html()}
\item
  \texttt{readRDS()}
\item
  \texttt{safely()}
\item
  \texttt{search\_tweets()}
\item
  \texttt{sleep()}
\item
  \texttt{tesseract()}
\item
  \texttt{unnest\_tokens()}
\item
  \texttt{walk2()}
\item
  \texttt{write\_html()}
\item
  \texttt{write\_lines()}
\end{itemize}

\hypertarget{apis}{%
\section{APIs}\label{apis}}

{[}Get some interesting ones from here: \url{https://bookdown.org/paul/apis_for_social_scientists/}{]}

In everyday language, and for our purposes, an Application Programming Interface (API) is simply a situation in which someone has set up specific files on their computer such that you can follow their instructions to get them. For instance, when you use a gif on Slack, Slack asks Giphy's server for the appropriate gif, Giphy's server gives that gif to Slack and then Slack inserts it into your chat. The way in which Slack and Giphy interact is determined by Giphy's API. More strictly, an API is just an application that runs on a server that we access using the HTTP protocol.

In our case, we are going to focus on using APIs for gathering data. I'll tailor the language that I use toward that:

\begin{quote}
{[}a{]}n API is the tool that makes a website's data digestible for a computer. Through it, a computer can view and edit data, just like a person can by loading pages and submitting forms.

\citet{zapierapis}, Chapter 1.
\end{quote}

For instance, you could go to \href{https://www.google.ca/maps}{Google Maps} and then scroll and click and drag to center the map on Canberra, Australia, or you could just paste this into your browser: \url{https://www.google.ca/maps/@-35.2812958,149.1248113,16z}. You just used the Google Maps API.\footnote{There are at least six great coffee shops shown just in this section of map including: Mocan \& Green Grout; The Cupping Room; Barrio Collective Coffee; Lonsdale Street Cafe; Two Before Ten; and Red Brick. There are also two coffee shops that I love but that most wouldn't classify as `great' including: The Street Theatre Cafe; and the CBE Cafe.} The result should be a map that looks something like Figure \ref{fig:focuson2020} .

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/googlemaps} \caption{Example of Google Maps, as at 25 January 2021.}\label{fig:focuson2020}
\end{figure}

The advantage of using an API is that the data provider specifies exactly the data that they are willing to provide, and the terms under which they will provide it. These terms may include things like rate limits (i.e.~how often you can ask for data), and what you can do with the data (e.g.~maybe you're not allowed to use it for commercial purposes, or to republish it, or whatever). Additionally, because the API is being provided specifically for you to use it, it is less likely to be subject to unexpected changes. Because of this it is ethically and legally clear that when an API is available you should try to use it.

We're going to run through some case studies interacting with APIs in R. In the first we will deal directly with an API. That works and is a handy skill to have, but there are a lot of R packages that wrap around APIs making it easier for you to use an API within `familiar surroundings'. I'll also run through two fun APIs that have R packages built around them.

\hypertarget{case-study---arxiv}{%
\section{Case study - arXiv}\label{case-study---arxiv}}

In this section we introduce GET requests in which we use an API directly. We will use the \texttt{httr} package \citep{citehttr}. A GET request tries to obtain some specific data and the main argument is \texttt{url}. Exactly as before with the Google Maps example! In that case, the specific information was a map and some information about it.

For this example we'll look at \href{https://arxiv.org}{arXiv}, which is a repository for academic articles before they go through peer-review. I'll ask arXiv to return some information about a paper that I recently uploaded with a former student. The content that is returned will be a series of information about that paper.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages(\textquotesingle{}httr\textquotesingle{})}
\FunctionTok{library}\NormalTok{(httr)}
\NormalTok{arxiv }\OtherTok{\textless{}{-}}\NormalTok{ httr}\SpecialCharTok{::}\FunctionTok{GET}\NormalTok{(}\StringTok{\textquotesingle{}http://export.arxiv.org/api/query?id\_list=2101.05225\textquotesingle{}}\NormalTok{)}
\FunctionTok{class}\NormalTok{(arxiv)}
\CommentTok{\#\textgreater{} [1] "response"}
\FunctionTok{content}\NormalTok{(arxiv, }\StringTok{"text"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{cat}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} \textless{}?xml version="1.0" encoding="UTF{-}8"?\textgreater{}}
\CommentTok{\#\textgreater{} \textless{}feed xmlns="http://www.w3.org/2005/Atom"\textgreater{}}
\CommentTok{\#\textgreater{}   \textless{}link href="http://arxiv.org/api/query?search\_query\%3D\%26id\_list\%3D2101.05225\%26start\%3D0\%26max\_results\%3D10" rel="self" type="application/atom+xml"/\textgreater{}}
\CommentTok{\#\textgreater{}   \textless{}title type="html"\textgreater{}ArXiv Query: search\_query=\&amp;id\_list=2101.05225\&amp;start=0\&amp;max\_results=10\textless{}/title\textgreater{}}
\CommentTok{\#\textgreater{}   \textless{}id\textgreater{}http://arxiv.org/api/p9UZyl2Vt0cHwPSKinDSThE23qI\textless{}/id\textgreater{}}
\CommentTok{\#\textgreater{}   \textless{}updated\textgreater{}2022{-}01{-}26T00:00:00{-}05:00\textless{}/updated\textgreater{}}
\CommentTok{\#\textgreater{}   \textless{}opensearch:totalResults xmlns:opensearch="http://a9.com/{-}/spec/opensearch/1.1/"\textgreater{}1\textless{}/opensearch:totalResults\textgreater{}}
\CommentTok{\#\textgreater{}   \textless{}opensearch:startIndex xmlns:opensearch="http://a9.com/{-}/spec/opensearch/1.1/"\textgreater{}0\textless{}/opensearch:startIndex\textgreater{}}
\CommentTok{\#\textgreater{}   \textless{}opensearch:itemsPerPage xmlns:opensearch="http://a9.com/{-}/spec/opensearch/1.1/"\textgreater{}10\textless{}/opensearch:itemsPerPage\textgreater{}}
\CommentTok{\#\textgreater{}   \textless{}entry\textgreater{}}
\CommentTok{\#\textgreater{}     \textless{}id\textgreater{}http://arxiv.org/abs/2101.05225v1\textless{}/id\textgreater{}}
\CommentTok{\#\textgreater{}     \textless{}updated\textgreater{}2021{-}01{-}13T17:37:07Z\textless{}/updated\textgreater{}}
\CommentTok{\#\textgreater{}     \textless{}published\textgreater{}2021{-}01{-}13T17:37:07Z\textless{}/published\textgreater{}}
\CommentTok{\#\textgreater{}     \textless{}title\textgreater{}On consistency scores in text data with an implementation in R\textless{}/title\textgreater{}}
\CommentTok{\#\textgreater{}     \textless{}summary\textgreater{}  In this paper, we introduce a reproducible cleaning process for the text}
\CommentTok{\#\textgreater{} extracted from PDFs using n{-}gram models. Our approach compares the originally}
\CommentTok{\#\textgreater{} extracted text with the text generated from, or expected by, these models using}
\CommentTok{\#\textgreater{} earlier text as stimulus. To guide this process, we introduce the notion of a}
\CommentTok{\#\textgreater{} consistency score, which refers to the proportion of text that is expected by}
\CommentTok{\#\textgreater{} the model. This is used to monitor changes during the cleaning process, and}
\CommentTok{\#\textgreater{} across different corpuses. We illustrate our process on text from the book Jane}
\CommentTok{\#\textgreater{} Eyre and introduce both a Shiny application and an R package to make our}
\CommentTok{\#\textgreater{} process easier for others to adopt.}
\CommentTok{\#\textgreater{} \textless{}/summary\textgreater{}}
\CommentTok{\#\textgreater{}     \textless{}author\textgreater{}}
\CommentTok{\#\textgreater{}       \textless{}name\textgreater{}Ke{-}Li Chiu\textless{}/name\textgreater{}}
\CommentTok{\#\textgreater{}     \textless{}/author\textgreater{}}
\CommentTok{\#\textgreater{}     \textless{}author\textgreater{}}
\CommentTok{\#\textgreater{}       \textless{}name\textgreater{}Rohan Alexander\textless{}/name\textgreater{}}
\CommentTok{\#\textgreater{}     \textless{}/author\textgreater{}}
\CommentTok{\#\textgreater{}     \textless{}arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom"\textgreater{}13 pages, 0 figures\textless{}/arxiv:comment\textgreater{}}
\CommentTok{\#\textgreater{}     \textless{}link href="http://arxiv.org/abs/2101.05225v1" rel="alternate" type="text/html"/\textgreater{}}
\CommentTok{\#\textgreater{}     \textless{}link title="pdf" href="http://arxiv.org/pdf/2101.05225v1" rel="related" type="application/pdf"/\textgreater{}}
\CommentTok{\#\textgreater{}     \textless{}arxiv:primary\_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/\textgreater{}}
\CommentTok{\#\textgreater{}     \textless{}category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/\textgreater{}}
\CommentTok{\#\textgreater{}   \textless{}/entry\textgreater{}}
\CommentTok{\#\textgreater{} \textless{}/feed\textgreater{}}
\CommentTok{\#\textgreater{} }
\end{Highlighting}
\end{Shaded}

We get a variety of information about this paper including the title, abstract, and authors.

\hypertarget{case-study---rtweet}{%
\section{Case study - rtweet}\label{case-study---rtweet}}

Twitter is a rich source of text and other data. The Twitter API is the way in which Twitter ask that you interact with Twitter in order to gather these data. The \texttt{rtweet} package \citep{rtweet-package} is built around this API and allows us to interact with it in ways that are similar to using any other R package. Initially all you need a regular Twitter account.

Get started by install the library if you need and then calling it.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages(\textquotesingle{}rtweet\textquotesingle{})}
\FunctionTok{library}\NormalTok{(rtweet)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

To get started we need to authorise rtweet. We start that process by calling a function from the package.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{get\_favorites}\NormalTok{(}\AttributeTok{user =} \StringTok{"RohanAlexander"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This will open a browser on your computer, and you will then have to log into your regular Twitter account as shown in Figure \ref{fig:rtweetlogin}.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/rtweet} \caption{rtweet authorisation page}\label{fig:rtweetlogin}
\end{figure}

Once that is done we can actually get my favourites and then save them.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rohans\_favs }\OtherTok{\textless{}{-}} \FunctionTok{get\_favorites}\NormalTok{(}\StringTok{"RohanAlexander"}\NormalTok{)}

\FunctionTok{saveRDS}\NormalTok{(rohans\_favs, }\StringTok{"dont\_push/rohans\_favs.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

And then looking at the most recent favourite, we can see it was when Professor Bolton tweeted about one of the stellar students in ISSC.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rohans\_favs }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(created\_at)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(screen\_name, text)}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 2}
\CommentTok{\#\textgreater{}   screen\_name text                                                              }
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}       \textless{}chr\textgreater{}                                                             }
\CommentTok{\#\textgreater{} 1 les\_ja      I\textquotesingle{}ve signed an offer letter, so I think I can formally announce: \textasciitilde{}}
\end{Highlighting}
\end{Shaded}

Let's look at who is tweeting about R, using one of the common R hashtags: \#rstats. I've removed retweets so that we hopefully get some actual interesting projects.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rstats\_tweets }\OtherTok{\textless{}{-}} \FunctionTok{search\_tweets}\NormalTok{(}
  \AttributeTok{q =} \StringTok{"\#rstats"}\NormalTok{,}
  \AttributeTok{include\_rts =} \ConstantTok{FALSE}
\NormalTok{)}

\FunctionTok{saveRDS}\NormalTok{(rstats\_tweets, }\StringTok{"dont\_push/rstats\_tweets.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

And then have a look at them.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(rstats\_tweets)}
\CommentTok{\#\textgreater{}  [1] "user\_id"                 "status\_id"              }
\CommentTok{\#\textgreater{}  [3] "created\_at"              "screen\_name"            }
\CommentTok{\#\textgreater{}  [5] "text"                    "source"                 }
\CommentTok{\#\textgreater{}  [7] "display\_text\_width"      "reply\_to\_status\_id"     }
\CommentTok{\#\textgreater{}  [9] "reply\_to\_user\_id"        "reply\_to\_screen\_name"   }
\CommentTok{\#\textgreater{} [11] "is\_quote"                "is\_retweet"             }
\CommentTok{\#\textgreater{} [13] "favorite\_count"          "retweet\_count"          }
\CommentTok{\#\textgreater{} [15] "quote\_count"             "reply\_count"            }
\CommentTok{\#\textgreater{} [17] "hashtags"                "symbols"                }
\CommentTok{\#\textgreater{} [19] "urls\_url"                "urls\_t.co"              }
\CommentTok{\#\textgreater{} [21] "urls\_expanded\_url"       "media\_url"              }
\CommentTok{\#\textgreater{} [23] "media\_t.co"              "media\_expanded\_url"     }
\CommentTok{\#\textgreater{} [25] "media\_type"              "ext\_media\_url"          }
\CommentTok{\#\textgreater{} [27] "ext\_media\_t.co"          "ext\_media\_expanded\_url" }
\CommentTok{\#\textgreater{} [29] "ext\_media\_type"          "mentions\_user\_id"       }
\CommentTok{\#\textgreater{} [31] "mentions\_screen\_name"    "lang"                   }
\CommentTok{\#\textgreater{} [33] "quoted\_status\_id"        "quoted\_text"            }
\CommentTok{\#\textgreater{} [35] "quoted\_created\_at"       "quoted\_source"          }
\CommentTok{\#\textgreater{} [37] "quoted\_favorite\_count"   "quoted\_retweet\_count"   }
\CommentTok{\#\textgreater{} [39] "quoted\_user\_id"          "quoted\_screen\_name"     }
\CommentTok{\#\textgreater{} [41] "quoted\_name"             "quoted\_followers\_count" }
\CommentTok{\#\textgreater{} [43] "quoted\_friends\_count"    "quoted\_statuses\_count"  }
\CommentTok{\#\textgreater{} [45] "quoted\_location"         "quoted\_description"     }
\CommentTok{\#\textgreater{} [47] "quoted\_verified"         "retweet\_status\_id"      }
\CommentTok{\#\textgreater{} [49] "retweet\_text"            "retweet\_created\_at"     }
\CommentTok{\#\textgreater{} [51] "retweet\_source"          "retweet\_favorite\_count" }
\CommentTok{\#\textgreater{} [53] "retweet\_retweet\_count"   "retweet\_user\_id"        }
\CommentTok{\#\textgreater{} [55] "retweet\_screen\_name"     "retweet\_name"           }
\CommentTok{\#\textgreater{} [57] "retweet\_followers\_count" "retweet\_friends\_count"  }
\CommentTok{\#\textgreater{} [59] "retweet\_statuses\_count"  "retweet\_location"       }
\CommentTok{\#\textgreater{} [61] "retweet\_description"     "retweet\_verified"       }
\CommentTok{\#\textgreater{} [63] "place\_url"               "place\_name"             }
\CommentTok{\#\textgreater{} [65] "place\_full\_name"         "place\_type"             }
\CommentTok{\#\textgreater{} [67] "country"                 "country\_code"           }
\CommentTok{\#\textgreater{} [69] "geo\_coords"              "coords\_coords"          }
\CommentTok{\#\textgreater{} [71] "bbox\_coords"             "status\_url"             }
\CommentTok{\#\textgreater{} [73] "name"                    "location"               }
\CommentTok{\#\textgreater{} [75] "description"             "url"                    }
\CommentTok{\#\textgreater{} [77] "protected"               "followers\_count"        }
\CommentTok{\#\textgreater{} [79] "friends\_count"           "listed\_count"           }
\CommentTok{\#\textgreater{} [81] "statuses\_count"          "favourites\_count"       }
\CommentTok{\#\textgreater{} [83] "account\_created\_at"      "verified"               }
\CommentTok{\#\textgreater{} [85] "profile\_url"             "profile\_expanded\_url"   }
\CommentTok{\#\textgreater{} [87] "account\_lang"            "profile\_banner\_url"     }
\CommentTok{\#\textgreater{} [89] "profile\_background\_url"  "profile\_image\_url"}

\NormalTok{rstats\_tweets }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(screen\_name, text) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 2}
\CommentTok{\#\textgreater{}   screen\_name    text                                                           }
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}          \textless{}chr\textgreater{}                                                          }
\CommentTok{\#\textgreater{} 1 RahaPhD        "\#WFH multitasking woes:  I was just sitting here, working on \textasciitilde{}}
\CommentTok{\#\textgreater{} 2 AmandaKMontoya "Teaching with the \#PublishingPaidMe data this week in my intr\textasciitilde{}}
\CommentTok{\#\textgreater{} 3 digitalke1     "130 \#MachineLearning ProjectsSolved and Explained\textbackslash{}n@ruben\_arc\textasciitilde{}}
\CommentTok{\#\textgreater{} 4 digitalke1     "\#Infographic: 6 simple steps to effectively analyse data.\textbackslash{}nVi\textasciitilde{}}
\CommentTok{\#\textgreater{} 5 dataclaudius   "When Did the US Senate Best Reflect the US Population? via \#r\textasciitilde{}}
\CommentTok{\#\textgreater{} 6 alexpghayes    "has anyone written an \#rstats package to interface with SNAP \textasciitilde{}}
\end{Highlighting}
\end{Shaded}

There is a bunch of other things that you can do just using a regular user account, and if you're interested then you should try the examples in the \texttt{rtweet} package documentation: \url{https://rtweet.info/index.html}. But more is available once you register as a developer (\url{https://developer.twitter.com/en/apply-for-access}). The Twitter API document is surprisingly readable, and you may enjoy some of it: \url{https://developer.twitter.com/en/docs}.

When I introduced APIs I said that the `data provider specifies exactly the data that they are willing to provide\ldots{}' and we have certainly been able to take advantage of what they provide But I continued `\ldots and the terms under which they will provide it' and here we haven't done our part. In particular, I took some tweets and saved them. If I had pushed these to GitHub, then it's possible I may have accidently stored sensitive information if there happened to be some in the tweets. Or if I had taken enough tweets to start to do some reasonable statistical analysis then even if there wasn't sensitive information, I may have violated the terms if I had pushed those saved tweets to GitHub. Finally, I linked a Twitter username, in this case \texttt{@Liza\_Bolton} with Professor Bolton. I happened to ask her if this was okay, but if I hadn't done that then I would have been violating the Twitter terms of service.

If you use Twitter data, please take a moment to look at the terms: \url{https://developer.twitter.com/en/developer-terms/more-on-restricted-use-cases}.

\hypertarget{case-study---spotifyr}{%
\section{Case study - spotifyr}\label{case-study---spotifyr}}

For the next example I will introduce the \texttt{spotifyr} package \citep{spotifyr}. Again, this is a wrapper that has been developed around an API, in this case the Spotify API. You should install the package from the developer's GitHub repo using \texttt{devtools} \citep{citeDevtools}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# devtools::install\_github(\textquotesingle{}charlie86/spotifyr\textquotesingle{})}
\FunctionTok{library}\NormalTok{(spotifyr)}
\end{Highlighting}
\end{Shaded}

In order to use this account, you need a Spotify Developer Account, which you can set-up here: \url{https://developer.spotify.com/dashboard/}. That'll have you log in with your Spotify details and then accept their terms (it's worth looking at some of these and I'll follow up on a few below) as in Figure \ref{fig:spotifyaccept}.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/spotify} \caption{rtweet authorisation page}\label{fig:spotifyaccept}
\end{figure}

What we need from here is a `Client ID' and you can just fill out some basic details. In our case we probably `don't know' what we're building, which means that Spotify requires us to use a non-commercial agreement, which is fine. In order to use the Spotify API we need a Client ID and a Client Secret.

These are things that you want to keep to yourself. There are a variety of ways of keeping this secret, (and my understanding is that a helpful package is on its way) but we'll keep them in our System Environment. In this way, when we push to GitHub they won't be included. To do this we need to be careful about the naming, because \texttt{spotifyr} will look in our environment for specifically named keys.

To do this we are going to use the \texttt{usethis} package \citep{citeusethis}. If you don't have that then please install it. There is a file called `.Renviron' which we will open and add our secrets to. This file also controls things like your default library location and more information is available at \citet{renvironrstudio} and \citet{whattheyforgot}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{usethis}\SpecialCharTok{::}\FunctionTok{edit\_r\_environ}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

When you run that function it will open a file. There you can add your Spotify secrets.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SPOTIFY\_CLIENT\_ID }\OtherTok{=} \StringTok{\textquotesingle{}PUT\_YOUR\_CLIENT\_ID\_HERE\textquotesingle{}}
\NormalTok{SPOTIFY\_CLIENT\_SECRET }\OtherTok{=} \StringTok{\textquotesingle{}PUT\_YOUR\_SECRET\_HERE\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

Save your `.Renviron' file, and then restart R (Session -\textgreater{} Restart R). You can now draw on that variable when you need.

Some functions that require your secrets as arguments will now just work. For instance, we will get information about Radiohead using \texttt{get\_artist\_audio\_features()}. One of the arguments is \texttt{authorization}, but as that is set to default to look at the R Environment, we don't need to do anything further.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{radiohead }\OtherTok{\textless{}{-}} \FunctionTok{get\_artist\_audio\_features}\NormalTok{(}\StringTok{\textquotesingle{}radiohead\textquotesingle{}}\NormalTok{)}
\FunctionTok{saveRDS}\NormalTok{(radiohead, }\StringTok{"inputs/radiohead.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{radiohead }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"inputs/radiohead.rds"}\NormalTok{)}

\FunctionTok{names}\NormalTok{(radiohead)}
\CommentTok{\#\textgreater{}  [1] "artist\_name"                  "artist\_id"                   }
\CommentTok{\#\textgreater{}  [3] "album\_id"                     "album\_type"                  }
\CommentTok{\#\textgreater{}  [5] "album\_images"                 "album\_release\_date"          }
\CommentTok{\#\textgreater{}  [7] "album\_release\_year"           "album\_release\_date\_precision"}
\CommentTok{\#\textgreater{}  [9] "danceability"                 "energy"                      }
\CommentTok{\#\textgreater{} [11] "key"                          "loudness"                    }
\CommentTok{\#\textgreater{} [13] "mode"                         "speechiness"                 }
\CommentTok{\#\textgreater{} [15] "acousticness"                 "instrumentalness"            }
\CommentTok{\#\textgreater{} [17] "liveness"                     "valence"                     }
\CommentTok{\#\textgreater{} [19] "tempo"                        "track\_id"                    }
\CommentTok{\#\textgreater{} [21] "analysis\_url"                 "time\_signature"              }
\CommentTok{\#\textgreater{} [23] "artists"                      "available\_markets"           }
\CommentTok{\#\textgreater{} [25] "disc\_number"                  "duration\_ms"                 }
\CommentTok{\#\textgreater{} [27] "explicit"                     "track\_href"                  }
\CommentTok{\#\textgreater{} [29] "is\_local"                     "track\_name"                  }
\CommentTok{\#\textgreater{} [31] "track\_preview\_url"            "track\_number"                }
\CommentTok{\#\textgreater{} [33] "type"                         "track\_uri"                   }
\CommentTok{\#\textgreater{} [35] "external\_urls.spotify"        "album\_name"                  }
\CommentTok{\#\textgreater{} [37] "key\_name"                     "mode\_name"                   }
\CommentTok{\#\textgreater{} [39] "key\_mode"}

\NormalTok{radiohead }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(artist\_name, track\_name, album\_name) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{}   artist\_name                               track\_name}
\CommentTok{\#\textgreater{} 1   Radiohead                      Airbag {-} Remastered}
\CommentTok{\#\textgreater{} 2   Radiohead            Paranoid Android {-} Remastered}
\CommentTok{\#\textgreater{} 3   Radiohead Subterranean Homesick Alien {-} Remastered}
\CommentTok{\#\textgreater{} 4   Radiohead     Exit Music (For a Film) {-} Remastered}
\CommentTok{\#\textgreater{} 5   Radiohead                    Let Down {-} Remastered}
\CommentTok{\#\textgreater{} 6   Radiohead                Karma Police {-} Remastered}
\CommentTok{\#\textgreater{}                      album\_name}
\CommentTok{\#\textgreater{} 1 OK Computer OKNOTOK 1997 2017}
\CommentTok{\#\textgreater{} 2 OK Computer OKNOTOK 1997 2017}
\CommentTok{\#\textgreater{} 3 OK Computer OKNOTOK 1997 2017}
\CommentTok{\#\textgreater{} 4 OK Computer OKNOTOK 1997 2017}
\CommentTok{\#\textgreater{} 5 OK Computer OKNOTOK 1997 2017}
\CommentTok{\#\textgreater{} 6 OK Computer OKNOTOK 1997 2017}
\end{Highlighting}
\end{Shaded}

Let's just make a quick graph looking at track length over time.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{radiohead }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ album\_release\_year, }\AttributeTok{y =}\NormalTok{ duration\_ms)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{20-gather_files/figure-latex/unnamed-chunk-10-1.pdf}

Just because we can, let's settle an argument. I've always said that Radiohead of quite depressing, but they're my wife's favourite band. Let's see how depressing they are. Spotify provides various information about each track, including `valence', which Spotify \href{https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/}{define} as `(a) measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g.~happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g.~sad, depressed, angry).' Higher values are happier. Let's compare someone who we know it likely to be happy - Taylor Swift - with Radiohead.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{swifty }\OtherTok{\textless{}{-}} \FunctionTok{get\_artist\_audio\_features}\NormalTok{(}\StringTok{\textquotesingle{}taylor swift\textquotesingle{}}\NormalTok{)}
\FunctionTok{saveRDS}\NormalTok{(swifty, }\StringTok{"inputs/swifty.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{swifty }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"inputs/swifty.rds"}\NormalTok{)}

\FunctionTok{tibble}\NormalTok{(}\AttributeTok{name =} \FunctionTok{c}\NormalTok{(swifty}\SpecialCharTok{$}\NormalTok{artist\_name, radiohead}\SpecialCharTok{$}\NormalTok{artist\_name),}
       \AttributeTok{year =} \FunctionTok{c}\NormalTok{(swifty}\SpecialCharTok{$}\NormalTok{album\_release\_year, radiohead}\SpecialCharTok{$}\NormalTok{album\_release\_year),}
       \AttributeTok{valence =} \FunctionTok{c}\NormalTok{(swifty}\SpecialCharTok{$}\NormalTok{valence, radiohead}\SpecialCharTok{$}\NormalTok{valence)}
\NormalTok{               ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ year, }\AttributeTok{y =}\NormalTok{ valence, }\AttributeTok{color =}\NormalTok{ name)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Year"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Valence"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Name"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{20-gather_files/figure-latex/unnamed-chunk-12-1.pdf}

Finally, for the sake of embarrassment, let's look at our most played artists.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{top\_artists }\OtherTok{\textless{}{-}} \FunctionTok{get\_my\_top\_artists\_or\_tracks}\NormalTok{(}\AttributeTok{type =} \StringTok{\textquotesingle{}artists\textquotesingle{}}\NormalTok{, }\AttributeTok{time\_range =} \StringTok{\textquotesingle{}long\_term\textquotesingle{}}\NormalTok{, }\AttributeTok{limit =} \DecValTok{20}\NormalTok{)}

\FunctionTok{saveRDS}\NormalTok{(top\_artists, }\StringTok{"inputs/top\_artists.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{top\_artists }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"inputs/top\_artists.rds"}\NormalTok{)}

\NormalTok{top\_artists }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(name, popularity)}
\CommentTok{\#\textgreater{}                   name popularity}
\CommentTok{\#\textgreater{} 1            Radiohead         81}
\CommentTok{\#\textgreater{} 2  Bombay Bicycle Club         66}
\CommentTok{\#\textgreater{} 3                Drake        100}
\CommentTok{\#\textgreater{} 4        Glass Animals         74}
\CommentTok{\#\textgreater{} 5                JAY{-}Z         85}
\CommentTok{\#\textgreater{} 6        Laura Marling         65}
\CommentTok{\#\textgreater{} 7       Sufjan Stevens         75}
\CommentTok{\#\textgreater{} 8      Vampire Weekend         73}
\CommentTok{\#\textgreater{} 9     Sturgill Simpson         65}
\CommentTok{\#\textgreater{} 10          Nick Drake         66}
\CommentTok{\#\textgreater{} 11        Dire Straits         78}
\CommentTok{\#\textgreater{} 12               Lorde         80}
\CommentTok{\#\textgreater{} 13         Marian Hill         65}
\CommentTok{\#\textgreater{} 14       José González         68}
\CommentTok{\#\textgreater{} 15       Stevie Wonder         79}
\CommentTok{\#\textgreater{} 16          Disclosure         82}
\CommentTok{\#\textgreater{} 17      Ben Folds Five         52}
\CommentTok{\#\textgreater{} 18       Ainslie Wills         40}
\CommentTok{\#\textgreater{} 19            Coldplay         89}
\CommentTok{\#\textgreater{} 20               alt{-}J         75}
\end{Highlighting}
\end{Shaded}

So pretty much my wife and I like what everyone else likes, with the exception of Ainslie Wills, who is an Australian and I suspect we used to listen to her when we were homesick.

How amazing that we live in a world that all that information is available with very little effort or cost.

Again, there is a lot more at the package's website: \url{https://www.rcharlie.com/spotifyr/}. A very nice little application of the Spotify API using some statistical analysis is \citet{kaylinpavlik}.

\hypertarget{scraping}{%
\section{Scraping}\label{scraping}}

\hypertarget{introduction-5}{%
\subsection{Introduction}\label{introduction-5}}

Web-scraping is a way to get data from websites into R. Rather than going to a website ourselves through a browser, we write code that does it for us. This opens up a lot of data to us, but on the other hand, it is not typically data that is being made available for these purposes and so it is important to be respectful of it. While generally not illegal, the specifics with regard to the legality of web-scraping depends on jurisdictions and the specifics of what you're doing, and so it is also important to be mindful of this. And finally, web-scraping imposes a cost on the website host, and so it is important to reduce this to the extent that it's possible.

That all said, web-scraping is an invaluable source of data. But they are typically datasets that can be created as a by-product of someone trying to achieve another aim. For instance, a retailer may have a website with their products and their prices. That has not been created deliberately as a source of data, but we can scrape it to create a dataset. As such, the following principles guide my web-scraping.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Avoid it. Try to use an API wherever possible.
\item
  Abide by their desires. Some websites have a file `robots.txt' that contains information about what they are comfortable with scrapers doing, for instance `\url{https://www.google.com/robots.txt}'. If they have one of these then you should read it and abide by it.
\item
  Reduce the impact.

  \begin{itemize}
  \tightlist
  \item
    Firstly, slow down your scraper, for instance, rather than having it visit the website every second, slow it down (using \texttt{sys.sleep()}). If you're only after a few hundred files then why not just have it visit once a minute, running in the background overnight?
  \item
    Secondly, consider the timing of when you run the scraper. For instance, if it's a retailer then why not set your script to run from 10pm through to the morning, when fewer customers are likely to need the site? If it's a government website and they have a big monthly release then why not avoid that day?
  \end{itemize}
\item
  Take only what you need. For instance, don't scrape the entire of Wikipedia if all you need is to know the names of the 10 largest cities in Canada. This reduces the impact on their website and allows you to more easily justify what you are doing.
\item
  Only scrape once. Save everything as you go so that you don't have to re-collect data. Similarly, once you have the data, you should keep that separate and not modify it. Of course, if you need data over time then you will need to go back, but this is different to needlessly re-scraping a page.
\item
  Don't republish the pages that you scraped. (This is in contrast to datasets that you create from it.)
\item
  Take ownership and ask permission if possible. At a minimum level your scripts should have your contact details in them. Depending on the circumstances, it may be worthwhile asking for permission before you scrape.
\end{enumerate}

\hypertarget{getting-started-2}{%
\subsection{Getting started}\label{getting-started-2}}

Web-scraping is possible by taking advantage of the underlying structure of a webpage. We use patterns in the HTML/CSS to get the data that we want. To look at the underlying HTML/CSS you can either: 1) open a browser, right-click, and choose something like `Inspect'; or 2) save the website and then open it with a text editor rather than a browser.

HTML/CSS is a markup language comprised of matching tags. If you want text to be bold then you would use something like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}b}\OperatorTok{\textgreater{}}\NormalTok{My bold text\textless{}/b}\OperatorTok{\textgreater{}}
\end{Highlighting}
\end{Shaded}

Similarly, if you want a list then you start and end the list as well as each item.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}ul}\OperatorTok{\textgreater{}}
\NormalTok{  \textless{}li}\OperatorTok{\textgreater{}}\NormalTok{Learn webscraping\textless{}/li}\OperatorTok{\textgreater{}}
\NormalTok{  \textless{}li}\OperatorTok{\textgreater{}}\NormalTok{Do data science\textless{}/li}\OperatorTok{\textgreater{}}
\NormalTok{  \textless{}li}\OperatorTok{\textgreater{}}\NormalTok{Proft\textless{}/li}\OperatorTok{\textgreater{}}
\NormalTok{\textless{}/ul}\OperatorTok{\textgreater{}}
\end{Highlighting}
\end{Shaded}

When scraping we will search for these tags.

To get started, this is some HTML/CSS from my website. Let's say that we want to grab my name from it. We can see that the name is in bold, so we want to probably focus on that feature and extract it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{website\_extract }\OtherTok{\textless{}{-}} \StringTok{"\textless{}p\textgreater{}Hi, I’m \textless{}b\textgreater{}Rohan\textless{}/b\textgreater{} Alexander.\textless{}/p\textgreater{}"}
\end{Highlighting}
\end{Shaded}

We will use the \texttt{rvest} package \citet{citervest}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("rvest")}
\FunctionTok{library}\NormalTok{(rvest)}

\NormalTok{rohans\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(website\_extract)}

\NormalTok{rohans\_data}
\CommentTok{\#\textgreater{} \{html\_document\}}
\CommentTok{\#\textgreater{} \textless{}html\textgreater{}}
\CommentTok{\#\textgreater{} [1] \textless{}body\textgreater{}\textless{}p\textgreater{}Hi, I’m \textless{}b\textgreater{}Rohan\textless{}/b\textgreater{} Alexander.\textless{}/p\textgreater{}\textless{}/body\textgreater{}}
\end{Highlighting}
\end{Shaded}

The language used by \texttt{rvest} to look for tags is `node', so we will focus on bold nodes. By default \texttt{html\_nodes()} returns the tags as well. We can focus on the text that they contain, using \texttt{html\_text()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rohans\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_nodes}\NormalTok{(}\StringTok{"b"}\NormalTok{)}
\CommentTok{\#\textgreater{} \{xml\_nodeset (1)\}}
\CommentTok{\#\textgreater{} [1] \textless{}b\textgreater{}Rohan\textless{}/b\textgreater{}}

\NormalTok{first\_name }\OtherTok{\textless{}{-}} 
\NormalTok{  rohans\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_nodes}\NormalTok{(}\StringTok{"b"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{html\_text}\NormalTok{()}

\NormalTok{first\_name}
\CommentTok{\#\textgreater{} [1] "Rohan"}
\end{Highlighting}
\end{Shaded}

The result is that we learn my first name.

\hypertarget{case-study---rohans-books}{%
\section{Case study - Rohan's books}\label{case-study---rohans-books}}

\hypertarget{introduction-6}{%
\subsection{Introduction}\label{introduction-6}}

In this case study we are going to scrape a list of books that I own, clean it, and look at the distribution of the first letters of author surnames. It is slightly more complicated than the example above, but the underlying approach is the same - download the website, look for the nodes of interest, extract the information, clean it.

\hypertarget{gather}{%
\subsection{Gather}\label{gather}}

Again, the key library that we are using is the \texttt{rvest} library. This makes it easier to download a website, and to then navigate the html to find the aspects that we are interested in. You should create a new project in a new folder (File -\textgreater{} New Project). Within that new folder you should make three new folders: \texttt{inputs}, \texttt{outputs}, and \texttt{scripts.}

In the scripts folder you should write and save a script along these lines. This script loads the libraries that we need, then visits my website, and saves a local copy.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Contact details \#\#\#\#}
\CommentTok{\# Title: Get data from rohanalexander.com}
\CommentTok{\# Purpose: This script gets data from Rohan\textquotesingle{}s website about the books that he }
\CommentTok{\# owns. It calls his website and then saves the dataset to inputs.}
\CommentTok{\# Author: Rohan Alexander}
\CommentTok{\# Contact: rohan.alexander@utoronto.ca}
\CommentTok{\# Last updated: 20 May 2020}


\DocumentationTok{\#\#\#\# Set up workspace \#\#\#\#}
\FunctionTok{library}\NormalTok{(rvest)}
\FunctionTok{library}\NormalTok{(tidyverse)}


\DocumentationTok{\#\#\#\# Get html \#\#\#\#}
\NormalTok{rohans\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(}\StringTok{"https://rohanalexander.com/bookshelf.html"}\NormalTok{)}
\CommentTok{\# This takes a website as an input and will read it into R, in the same way that we }
\CommentTok{\# can read a, say, CSV into R.}

\FunctionTok{write\_html}\NormalTok{(rohans\_data, }\StringTok{"inputs/my\_website/raw\_data.html"}\NormalTok{) }
\CommentTok{\# Always save your raw dataset as soon as you get it so that you have a record }
\CommentTok{\# of it. This is the equivalent of, say, write\_csv() that we have used earlier.}
\end{Highlighting}
\end{Shaded}

\hypertarget{clean}{%
\subsection{Clean}\label{clean}}

Now we need to navigate the HTML to get the aspects that we want, and to then put them into some sensible structure. I always try to get the data into a tibble as early as possible. While it's possible to work with the nested data, I move to a tibble so that the usual verbs that I'm used to can be used.

In the scripts folder you should write and save a new R script along these lines. First, we need to add the top matter, read in the libraries and the data that we scraped.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Contact details \#\#\#\#}
\CommentTok{\# Title: Clean data from rohanaledander.com}
\CommentTok{\# Purpose: This script cleans data that was downloaded in 01{-}get\_data.R.}
\CommentTok{\# Author: Rohan Alexander}
\CommentTok{\# Contact: rohan.alexander@utoronto.ca}
\CommentTok{\# Pre{-}requisites: Need to have run 01\_get\_data.R and have saved the data.}
\CommentTok{\# Last updated: 20 May 2020}


\DocumentationTok{\#\#\#\# Set up workspace \#\#\#\#}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(rvest)}

\NormalTok{rohans\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(}\StringTok{"inputs/my\_website/raw\_data.html"}\NormalTok{)}

\NormalTok{rohans\_data}
\CommentTok{\#\textgreater{} \{html\_document\}}
\CommentTok{\#\textgreater{} \textless{}html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""\textgreater{}}
\CommentTok{\#\textgreater{} [1] \textless{}head\textgreater{}\textbackslash{}n\textless{}meta http{-}equiv="Content{-}Type" content="text/html; charset=UTF{-}8 ...}
\CommentTok{\#\textgreater{} [2] \textless{}body\textgreater{}\textbackslash{}n\textbackslash{}n\textless{}!{-}{-}radix\_placeholder\_front\_matter{-}{-}\textgreater{}\textbackslash{}n\textbackslash{}n\textless{}script id="distill{-}fr ...}
\end{Highlighting}
\end{Shaded}

Now we need to identify the data that we are interested in using html tags and convert it to a tibble. If you look at the website, then you should notice that we are likely trying to focus on list items (Figure \ref{fig:rohansbooks}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/rohansbooks} \caption{Some of Rohan's books}\label{fig:rohansbooks}
\end{figure}

Let's look at the source (Figure \ref{fig:rohanssourceone}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/sourcetop} \caption{Source code for top of the page}\label{fig:rohanssourceone}
\end{figure}

There's a lot of debris, but scrolling down we eventually get to a list (Figure \ref{fig:rohanssourcetwo}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/sourcelist} \caption{Source code for list}\label{fig:rohanssourcetwo}
\end{figure}

The tag for a list item is `li', so we modify the earlier code to focus on that and to get the text.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Clean data \#\#\#\#}
\CommentTok{\# Identify the lines that have books on them based on the list html tag}
\NormalTok{text\_data }\OtherTok{\textless{}{-}}\NormalTok{ rohans\_data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{html\_nodes}\NormalTok{(}\StringTok{"li"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{html\_text}\NormalTok{()}

\NormalTok{all\_books }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{books =}\NormalTok{ text\_data)}

\FunctionTok{head}\NormalTok{(all\_books)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 1}
\CommentTok{\#\textgreater{}   books                                                                         }
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}                                                                         }
\CommentTok{\#\textgreater{} 1 "{-}“A Little Life”, Hanya Yanighara. Recommended by Lauren."                   }
\CommentTok{\#\textgreater{} 2 "“The Andromeda Strain”, Michael Crichton."                                   }
\CommentTok{\#\textgreater{} 3 "“Is There Life After Housework”, Don Aslett.\textbackslash{}nGot given this at the Museum o\textasciitilde{}}
\CommentTok{\#\textgreater{} 4 "“The Chosen”, Chaim Potok."                                                  }
\CommentTok{\#\textgreater{} 5 "“The Forsyth Saga”, John Galsworthy."                                        }
\CommentTok{\#\textgreater{} 6 "“Freakonomics”, Steven Levitt and Stephen Dubner."}
\end{Highlighting}
\end{Shaded}

We now need to clean the data. First we want to separate the title and the author

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# All content is just one string, so need to separate title and author}
\NormalTok{all\_books }\OtherTok{\textless{}{-}}
\NormalTok{  all\_books }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{separate}\NormalTok{(books, }\AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"title"}\NormalTok{, }\StringTok{"author"}\NormalTok{), }\AttributeTok{sep =} \StringTok{"”"}\NormalTok{)}

\CommentTok{\# Remove leading comma and clean up the titles a little}
\NormalTok{all\_books }\OtherTok{\textless{}{-}}
\NormalTok{  all\_books }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{author =} \FunctionTok{str\_remove\_all}\NormalTok{(author, }\StringTok{"\^{}, "}\NormalTok{),}
         \AttributeTok{author =} \FunctionTok{str\_squish}\NormalTok{(author),}
         \AttributeTok{title =} \FunctionTok{str\_remove}\NormalTok{(title, }\StringTok{"“"}\NormalTok{),}
         \AttributeTok{title =} \FunctionTok{str\_remove}\NormalTok{(title, }\StringTok{"\^{}{-}"}\NormalTok{)}
\NormalTok{         )}

\FunctionTok{head}\NormalTok{(all\_books)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 2}
\CommentTok{\#\textgreater{}   title                         author                                          }
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}                         \textless{}chr\textgreater{}                                           }
\CommentTok{\#\textgreater{} 1 A Little Life                 Hanya Yanighara. Recommended by Lauren.         }
\CommentTok{\#\textgreater{} 2 The Andromeda Strain          Michael Crichton.                               }
\CommentTok{\#\textgreater{} 3 Is There Life After Housework Don Aslett. Got given this at the Museum of Cle\textasciitilde{}}
\CommentTok{\#\textgreater{} 4 The Chosen                    Chaim Potok.                                    }
\CommentTok{\#\textgreater{} 5 The Forsyth Saga              John Galsworthy.                                }
\CommentTok{\#\textgreater{} 6 Freakonomics                  Steven Levitt and Stephen Dubner.}
\end{Highlighting}
\end{Shaded}

Finally, some specific cleaning is needed.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Some authors have comments after their name, so need to get rid of them, although there are some exceptions that will not work}
\CommentTok{\# J. K. Rowling.}
\CommentTok{\# M. Mitchell Waldrop.}
\CommentTok{\# David A. Price}
\NormalTok{all\_books }\OtherTok{\textless{}{-}}
\NormalTok{  all\_books }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{author =} \FunctionTok{str\_replace\_all}\NormalTok{(author,}
                              \FunctionTok{c}\NormalTok{(}\StringTok{"J. K. Rowling."} \OtherTok{=} \StringTok{"J K Rowling."}\NormalTok{,}
                                \StringTok{"M. Mitchell Waldrop."} \OtherTok{=} \StringTok{"M Mitchell Waldrop."}\NormalTok{,}
                                \StringTok{"David A. Price"} \OtherTok{=} \StringTok{"David A Price"}\NormalTok{)}
\NormalTok{                              )}
\NormalTok{         ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{separate}\NormalTok{(author, }\AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"author\_correct"}\NormalTok{, }\StringTok{"throw\_away"}\NormalTok{), }\AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{."}\NormalTok{, }\AttributeTok{extra =} \StringTok{"drop"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{throw\_away) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{author =}\NormalTok{ author\_correct)}

\CommentTok{\# Some books have multiple authors, so need to separate them}
\CommentTok{\# One has multiple authors:}
\CommentTok{\# "Daniela Witten, Gareth James, Robert Tibshirani, and Trevor Hastie"}
\NormalTok{all\_books }\OtherTok{\textless{}{-}}
\NormalTok{  all\_books }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{author =} \FunctionTok{str\_replace}\NormalTok{(author,}
                              \StringTok{"Daniela Witten, Gareth James, Robert Tibshirani, and Trevor Hastie"}\NormalTok{,}
                              \StringTok{"Daniela Witten and Gareth James and Robert Tibshirani and Trevor Hastie"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{separate}\NormalTok{(author, }\AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"author\_first"}\NormalTok{, }\StringTok{"author\_second"}\NormalTok{, }\StringTok{"author\_third"}\NormalTok{, }\StringTok{"author\_fourth"}\NormalTok{), }\AttributeTok{sep =} \StringTok{" and "}\NormalTok{, }\AttributeTok{fill =} \StringTok{"right"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{starts\_with}\NormalTok{(}\StringTok{"author\_"}\NormalTok{),}
               \AttributeTok{names\_to =} \StringTok{"author\_position"}\NormalTok{,}
               \AttributeTok{values\_to =} \StringTok{"author"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{author\_position) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(author))}

\FunctionTok{head}\NormalTok{(all\_books)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 2}
\CommentTok{\#\textgreater{}   title                         author          }
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}                         \textless{}chr\textgreater{}           }
\CommentTok{\#\textgreater{} 1 A Little Life                 Hanya Yanighara }
\CommentTok{\#\textgreater{} 2 The Andromeda Strain          Michael Crichton}
\CommentTok{\#\textgreater{} 3 Is There Life After Housework Don Aslett      }
\CommentTok{\#\textgreater{} 4 The Chosen                    Chaim Potok     }
\CommentTok{\#\textgreater{} 5 The Forsyth Saga              John Galsworthy }
\CommentTok{\#\textgreater{} 6 Freakonomics                  Steven Levitt}
\end{Highlighting}
\end{Shaded}

It looks there is some at the end because I have a best of. I'll just get rid of those manually because it's not the focus.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_books }\OtherTok{\textless{}{-}} 
\NormalTok{  all\_books }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{118}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{explore-3}{%
\subsection{Explore}\label{explore-3}}

Finally, just because we have the data now, so we may as well try to do something with it, let's look at the distribution of the first letter of the author names.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_books }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{first\_letter =} \FunctionTok{str\_sub}\NormalTok{(author, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{    ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(first\_letter) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 21 x 2}
\CommentTok{\#\textgreater{} \# Groups:   first\_letter [21]}
\CommentTok{\#\textgreater{}    first\_letter     n}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}        \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{}  1 ""               1}
\CommentTok{\#\textgreater{}  2 "A"              8}
\CommentTok{\#\textgreater{}  3 "B"              5}
\CommentTok{\#\textgreater{}  4 "C"              4}
\CommentTok{\#\textgreater{}  5 "D"             10}
\CommentTok{\#\textgreater{}  6 "E"              3}
\CommentTok{\#\textgreater{}  7 "F"              1}
\CommentTok{\#\textgreater{}  8 "G"             10}
\CommentTok{\#\textgreater{}  9 "H"              6}
\CommentTok{\#\textgreater{} 10 "I"              1}
\CommentTok{\#\textgreater{} \# ... with 11 more rows}
\end{Highlighting}
\end{Shaded}

\hypertarget{case-study---canadian-prime-ministers}{%
\section{Case study - Canadian Prime Ministers}\label{case-study---canadian-prime-ministers}}

\hypertarget{introduction-7}{%
\subsection{Introduction}\label{introduction-7}}

In this case study we are interested in how long Canadian prime ministers lived, based on the year that they were born. We will scrape data from Wikipedia, clean it, and then make a graph.

The key library that we will use for scraping is \texttt{rvest}. This adds a lot of functions that will make life easier. That said, every time you scrape a website things will change. Each scrape will largely be bespoke, even if you can borrow some code from earlier projects that you have completed. It is completely normal to feel frustrated at times. It helps to begin with an end in mind.

To that end, let's generate some simulated data. Ideally, we want a table that has a row for each prime minister, a column for their name, and a column each for the birth and death years. If they are still alive, then that death year can be empty. We know that birth and death years should be somewhere between 1700 and 1990, and that death year should be larger than birth year. Finally, we also know that the years should be integers, and the names should be characters. So, we want something that looks roughly like this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(babynames)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{simulated\_dataset }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{prime\_minister =} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =}\NormalTok{ babynames }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(prop }\SpecialCharTok{\textgreater{}} \FloatTok{0.01}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
                                   \FunctionTok{select}\NormalTok{(name) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{unique}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{unlist}\NormalTok{(), }
                                 \AttributeTok{size =} \DecValTok{10}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{),}
         \AttributeTok{birth\_year =} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{1700}\SpecialCharTok{:}\DecValTok{1990}\NormalTok{), }\AttributeTok{size =} \DecValTok{10}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{),}
         \AttributeTok{years\_lived =} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{50}\SpecialCharTok{:}\DecValTok{100}\NormalTok{), }\AttributeTok{size =} \DecValTok{10}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{),}
         \AttributeTok{death\_year =}\NormalTok{ birth\_year }\SpecialCharTok{+}\NormalTok{ years\_lived) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(prime\_minister, birth\_year, death\_year, years\_lived) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(birth\_year)}

\FunctionTok{head}\NormalTok{(simulated\_dataset)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 4}
\CommentTok{\#\textgreater{}   prime\_minister birth\_year death\_year years\_lived}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}               \textless{}int\textgreater{}      \textless{}int\textgreater{}       \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 Tracy                1706       1800          94}
\CommentTok{\#\textgreater{} 2 Ruth                 1748       1828          80}
\CommentTok{\#\textgreater{} 3 Michelle             1793       1857          64}
\CommentTok{\#\textgreater{} 4 Andrew               1817       1879          62}
\CommentTok{\#\textgreater{} 5 Donna                1846       1905          59}
\CommentTok{\#\textgreater{} 6 Susan                1856       1948          92}
\end{Highlighting}
\end{Shaded}

One of the advantages of generating a simulated dataset is that if you are working in groups then one person can start making the graph, using the simulated dataset, while the other person gathers the data. In terms of a graph, we want something like Figure \ref{fig:pmsgraphexample}.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/IMG_4185} \caption{Sketch of planned graph.}\label{fig:pmsgraphexample}
\end{figure}

\hypertarget{gather-1}{%
\subsection{Gather}\label{gather-1}}

We are starting with a question that is of interest, which how long each Canadian prime minister lived. As such, we need to identify a source of data While there are likely to be plenty of data sources that have the births and deaths of each prime minister, we want one that we can trust, and as we are going to be scraping, we want one that has some structure to it. The Wikipedia page (\url{https://en.wikipedia.org/wiki/List_of_prime_ministers_of_Canada}) fits both these criteria. As it is a popular page the information is more likely to be correct, and the data are available in a table.

We load the library and then we read in the data from the relevant page. The key function here is \texttt{read\_html()}, which you can use in the same way as, say, \texttt{read\_csv()}, except that it takes a html page as an input. Once you call \texttt{read\_html()} then the page is downloaded to your own computer, and it is usually a good idea to save this, using \texttt{write\_html()} as it is your raw data. Saving it also means that we don't have to keep visiting the website when we want to start again with our cleaning, and so it is part of being polite. However, it is likely not our property (in the case of Wikipedia, we might be okay), and so you should probably not share it.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rvest)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{raw\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(}\StringTok{"https://en.wikipedia.org/wiki/List\_of\_prime\_ministers\_of\_Canada"}\NormalTok{)}
\FunctionTok{write\_html}\NormalTok{(raw\_data, }\StringTok{"inputs/wiki/pms.html"}\NormalTok{) }\CommentTok{\# Note that we save the file as a html file.}
\end{Highlighting}
\end{Shaded}

\hypertarget{clean-1}{%
\subsection{Clean}\label{clean-1}}

Websites are made up of html, which is a markup language. We are looking for patterns in the mark-up that we can use to help us get closer to the data that we want. This is an iterative process and requires a lot of trial and error. Even simple examples will take time. You can look at the html by using a browser, right clicking, and then selecting \texttt{view\ page\ source}. Similarly, you could open the html file using a text editor.

\hypertarget{by-inspection}{%
\subsubsection{By inspection}\label{by-inspection}}

We are looking for patterns that we can use to select the information that is of interest - names, birth year, and death year. When we look at the html it looks like there is something going on with \texttt{\textless{}tr\textgreater{}}, and then \texttt{\textless{}td\textgreater{}} (thanks to Thomas Rosenthal for identifying this). We select those nodes using \texttt{html\_nodes()}, which takes the tags as an input. If you only want the first one then there is a singular version, \texttt{html\_node()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read in our saved data}
\NormalTok{raw\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(}\StringTok{"inputs/wiki/pms.html"}\NormalTok{)}

\CommentTok{\# We can parse tags in order}
\NormalTok{parse\_data\_inspection }\OtherTok{\textless{}{-}} 
\NormalTok{  raw\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_nodes}\NormalTok{(}\StringTok{"tr"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_nodes}\NormalTok{(}\StringTok{"td"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_text}\NormalTok{() }\CommentTok{\# html\_text removes any remaining html tags}

\CommentTok{\# But this code does exactly the same thing {-} the nodes are just pushed into }
\CommentTok{\# the one function call}
\NormalTok{parse\_data\_inspection }\OtherTok{\textless{}{-}} 
\NormalTok{  raw\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_nodes}\NormalTok{(}\StringTok{"tr td"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_text}\NormalTok{()}

\FunctionTok{head}\NormalTok{(parse\_data\_inspection)}
\CommentTok{\#\textgreater{} [1] "Abbreviation key:"                                                                                                                                                                                                                              }
\CommentTok{\#\textgreater{} [2] "No.: Incumbent number, Min.: Ministry, Refs: References\textbackslash{}n"                                                                                                                                                                                      }
\CommentTok{\#\textgreater{} [3] "Colour key:"                                                                                                                                                                                                                                    }
\CommentTok{\#\textgreater{} [4] "\textbackslash{}n\textbackslash{}n  Liberal Party of Canada\textbackslash{}n \textbackslash{}n  Historical Conservative parties (including Liberal{-}Conservative, Conservative (Historical),     Unionist, National Liberal and Conservative, Progressive Conservative) \textbackslash{}n  Conservative Party of Canada\textbackslash{}n\textbackslash{}n"}
\CommentTok{\#\textgreater{} [5] "Provinces key:"                                                                                                                                                                                                                                 }
\CommentTok{\#\textgreater{} [6] "AB: Alberta, BC: British Columbia, MB: Manitoba, NS: Nova Scotia,ON: Ontario, QC: Quebec, SK: Saskatchewan\textbackslash{}n"}
\end{Highlighting}
\end{Shaded}

At this point our data is in a character vector, we want to convert it to a table, and reduce the data down to just the information that we want. The key that is going to allow us to do this is the fact that there seems to be a blank line (which in html is denoted by \texttt{\textbackslash{}n}) before the key information that we need. So, once we identify that line then we can filter to just the line below it!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{parsed\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{raw\_text =}\NormalTok{ parse\_data\_inspection) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# Convert the character vector to a table}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{is\_PM =} \FunctionTok{if\_else}\NormalTok{(raw\_text }\SpecialCharTok{==} \StringTok{"}\SpecialCharTok{\textbackslash{}n\textbackslash{}n}\StringTok{"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{), }\CommentTok{\# Look for the blank line that is }
         \CommentTok{\# above the row that we want}
         \AttributeTok{is\_PM =} \FunctionTok{lag}\NormalTok{(is\_PM, }\AttributeTok{n =} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# Identify the actual row that we want}
  \FunctionTok{filter}\NormalTok{(is\_PM }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) }\CommentTok{\# Just get the rows that we want}

\FunctionTok{head}\NormalTok{(parsed\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 2}
\CommentTok{\#\textgreater{}   raw\_text                                                                 is\_PM}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}                                                                    \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 "\textbackslash{}nSir John A. MacDonald(1815–1891)MP for Kingston, ON\textbackslash{}n"                    1}
\CommentTok{\#\textgreater{} 2 "\textbackslash{}nAlexander Mackenzie(1822–1892)MP for Lambton, ON\textbackslash{}n"                       1}
\CommentTok{\#\textgreater{} 3 "\textbackslash{}nSir John A. MacDonald(1815–1891)MP for Victoria, BC until 1882MP for\textasciitilde{}     1}
\CommentTok{\#\textgreater{} 4 "\textbackslash{}nSir John Abbott(1821–1893)Senator for Quebec\textbackslash{}n"                           1}
\CommentTok{\#\textgreater{} 5 "\textbackslash{}nSir John Thompson(1845–1894)MP for Antigonish, NS\textbackslash{}n"                      1}
\CommentTok{\#\textgreater{} 6 "\textbackslash{}nSir Mackenzie Bowell(1823–1917)Senator for Ontario\textbackslash{}n"                     1}
\end{Highlighting}
\end{Shaded}

\hypertarget{using-the-selector-gadget}{%
\subsubsection{Using the selector gadget}\label{using-the-selector-gadget}}

If you are comfortable with html then you might be able to see patterns, but one tool that may help is the SelectorGadget: \url{https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html}. This allows you to pick and choose the elements that you want, and then gives you the input to give to \texttt{html\_nodes()} (Figure \ref{fig:selectorgadget})

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/selectorgadget} \caption{Using the Selector Gadget to identify the tag, as at 13 March 2020.}\label{fig:selectorgadget}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read in our saved data}
\NormalTok{raw\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(}\StringTok{"inputs/wiki/pms.html"}\NormalTok{)}

\CommentTok{\# We can parse tags in order}
\NormalTok{parse\_data\_selector\_gadget }\OtherTok{\textless{}{-}} 
\NormalTok{  raw\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_nodes}\NormalTok{(}\StringTok{"td:nth{-}child(3)"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_text}\NormalTok{() }\CommentTok{\# html\_text removes any remaining html tags}

\FunctionTok{head}\NormalTok{(parse\_data\_selector\_gadget)}
\CommentTok{\#\textgreater{} [1] "\textbackslash{}nSir John A. MacDonald(1815–1891)MP for Kingston, ON\textbackslash{}n"                                                            }
\CommentTok{\#\textgreater{} [2] "\textbackslash{}nAlexander Mackenzie(1822–1892)MP for Lambton, ON\textbackslash{}n"                                                               }
\CommentTok{\#\textgreater{} [3] "\textbackslash{}nSir John A. MacDonald(1815–1891)MP for Victoria, BC until 1882MP for Carleton, ON until 1887MP for Kingston, ON\textbackslash{}n"}
\CommentTok{\#\textgreater{} [4] "\textbackslash{}nSir John Abbott(1821–1893)Senator for Quebec\textbackslash{}n"                                                                   }
\CommentTok{\#\textgreater{} [5] "\textbackslash{}nSir John Thompson(1845–1894)MP for Antigonish, NS\textbackslash{}n"                                                              }
\CommentTok{\#\textgreater{} [6] "\textbackslash{}nSir Mackenzie Bowell(1823–1917)Senator for Ontario\textbackslash{}n"}
\end{Highlighting}
\end{Shaded}

In this case there is one prime minister - Robert Borden - who changed party and we would need to filter away that row: \texttt{\textbackslash{}nUnionist\ Party\textbackslash{}n"}.

\hypertarget{clean-data}{%
\subsubsection{Clean data}\label{clean-data}}

Now that we have the parsed data, we need to clean it to match what we wanted. In particular we want a names column, as well as columns for birth year and death year. We will use \texttt{separate()} to take advantage of the fact that it looks like the dates are distinguished by brackets.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{initial\_clean }\OtherTok{\textless{}{-}} 
\NormalTok{  parsed\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{separate}\NormalTok{(raw\_text, }
            \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"Name"}\NormalTok{, }\StringTok{"not\_name"}\NormalTok{), }
            \AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{("}\NormalTok{,}
            \AttributeTok{remove =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# The remove = FALSE option here means that we }
  \CommentTok{\# keep the original column that we are separating.}
  \FunctionTok{separate}\NormalTok{(not\_name, }
            \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"Date"}\NormalTok{, }\StringTok{"all\_the\_rest"}\NormalTok{), }
            \AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{)"}\NormalTok{,}
            \AttributeTok{remove =} \ConstantTok{FALSE}\NormalTok{)}

\FunctionTok{head}\NormalTok{(initial\_clean)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 6}
\CommentTok{\#\textgreater{}   raw\_text           Name     not\_name          Date  all\_the\_rest         is\_PM}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}              \textless{}chr\textgreater{}    \textless{}chr\textgreater{}             \textless{}chr\textgreater{} \textless{}chr\textgreater{}                \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 "\textbackslash{}nSir John A. Ma\textasciitilde{} "\textbackslash{}nSir \textasciitilde{} "1815–1891)MP fo\textasciitilde{} 1815\textasciitilde{} "MP for Kingston, O\textasciitilde{}     1}
\CommentTok{\#\textgreater{} 2 "\textbackslash{}nAlexander Mack\textasciitilde{} "\textbackslash{}nAlex\textasciitilde{} "1822–1892)MP fo\textasciitilde{} 1822\textasciitilde{} "MP for Lambton, ON\textasciitilde{}     1}
\CommentTok{\#\textgreater{} 3 "\textbackslash{}nSir John A. Ma\textasciitilde{} "\textbackslash{}nSir \textasciitilde{} "1815–1891)MP fo\textasciitilde{} 1815\textasciitilde{} "MP for Victoria, B\textasciitilde{}     1}
\CommentTok{\#\textgreater{} 4 "\textbackslash{}nSir John Abbot\textasciitilde{} "\textbackslash{}nSir \textasciitilde{} "1821–1893)Senat\textasciitilde{} 1821\textasciitilde{} "Senator for Quebec\textasciitilde{}     1}
\CommentTok{\#\textgreater{} 5 "\textbackslash{}nSir John Thomp\textasciitilde{} "\textbackslash{}nSir \textasciitilde{} "1845–1894)MP fo\textasciitilde{} 1845\textasciitilde{} "MP for Antigonish,\textasciitilde{}     1}
\CommentTok{\#\textgreater{} 6 "\textbackslash{}nSir Mackenzie \textasciitilde{} "\textbackslash{}nSir \textasciitilde{} "1823–1917)Senat\textasciitilde{} 1823\textasciitilde{} "Senator for Ontari\textasciitilde{}     1}
\end{Highlighting}
\end{Shaded}

Finally, we need to clean up the columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  initial\_clean }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(Name, Date) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{separate}\NormalTok{(Date, }\AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"Birth"}\NormalTok{, }\StringTok{"Died"}\NormalTok{), }\AttributeTok{sep =} \StringTok{"–"}\NormalTok{, }\AttributeTok{remove =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# The }
  \CommentTok{\# PMs who have died have their birth and death years separated by a hyphen, but }
  \CommentTok{\# you need to be careful with the hyphen as it seems to be a slightly odd type of }
  \CommentTok{\# hyphen and you need to copy/paste it.}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Birth =} \FunctionTok{str\_remove}\NormalTok{(Birth, }\StringTok{"b. "}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# Alive PMs have slightly different format}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Date) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Name =} \FunctionTok{str\_remove}\NormalTok{(Name, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# Remove some html tags that remain}
  \FunctionTok{mutate\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(Birth, Died), }\SpecialCharTok{\textasciitilde{}}\FunctionTok{as.integer}\NormalTok{(.)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# Change birth and death to integers}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Age\_at\_Death =}\NormalTok{ Died }\SpecialCharTok{{-}}\NormalTok{ Birth) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# Add column of the number of years they lived}
  \FunctionTok{distinct}\NormalTok{() }\CommentTok{\# Some of the PMs had two goes at it.}

\FunctionTok{head}\NormalTok{(cleaned\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 4}
\CommentTok{\#\textgreater{}   Name                  Birth  Died Age\_at\_Death}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}                 \textless{}int\textgreater{} \textless{}int\textgreater{}        \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 Sir John A. MacDonald  1815  1891           76}
\CommentTok{\#\textgreater{} 2 Alexander Mackenzie    1822  1892           70}
\CommentTok{\#\textgreater{} 3 Sir John Abbott        1821  1893           72}
\CommentTok{\#\textgreater{} 4 Sir John Thompson      1845  1894           49}
\CommentTok{\#\textgreater{} 5 Sir Mackenzie Bowell   1823  1917           94}
\CommentTok{\#\textgreater{} 6 Sir Charles Tupper     1821  1915           94}
\end{Highlighting}
\end{Shaded}

\hypertarget{explore-4}{%
\subsection{Explore}\label{explore-4}}

At this point we'd like to make a graph that illustrates how long each prime minister lived. If they are still alive then we would like them to run to the end, but we would like to colour them differently.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{still\_alive =} \FunctionTok{if\_else}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(Died), }\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{),}
         \AttributeTok{Died =} \FunctionTok{if\_else}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(Died), }\FunctionTok{as.integer}\NormalTok{(}\DecValTok{2020}\NormalTok{), Died)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Name =} \FunctionTok{as\_factor}\NormalTok{(Name)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Birth, }
             \AttributeTok{xend =}\NormalTok{ Died,}
             \AttributeTok{y =}\NormalTok{ Name,}
             \AttributeTok{yend =}\NormalTok{ Name, }
             \AttributeTok{color =}\NormalTok{ still\_alive)) }\SpecialCharTok{+}
  \FunctionTok{geom\_segment}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Year of birth"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Prime minister"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"PM is alive"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Canadian Prime Minister, by year of birth"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{20-gather_files/figure-latex/unnamed-chunk-35-1.pdf}

\hypertarget{pdfs}{%
\section{PDFs}\label{pdfs}}

\hypertarget{introduction-8}{%
\subsection{Introduction}\label{introduction-8}}

In contrast to an API, a PDF is usually only produced for human (not computer) consumption. The nice thing about PDFs is that they are static and constant. And it is nice that they make data available at all. But the trade-off is that:

\begin{itemize}
\tightlist
\item
  It is not overly useful to do larger-scale statistical analysis.
\item
  We don't know how the PDF was put together so we don't know whether we can trust it.
\item
  We can't manipulate the data to get results that we are interested in.
\end{itemize}

Indeed, sometimes governments publish data as PDFs because they don't actually want you to be able to analyse it! Being able to get data from PDFs opens up a large number of datasets for you, some of which we'll see in this chapter.

There are two important aspects to keep in mind when approaching a PDF with a mind to extracting data from it:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Begin with an end in mind. Planning and then literally sketching out what you want from a final dataset/graph/paper stops you wasting time and keeps you focused.
\item
  Start simple, then iterate. The quickest way to make a complicated model is often to first build a simple model and then complicate it. Start with just trying to get one page of the PDF working or even just one line. Then iterate from there.
\end{enumerate}

In this chapter we start by walking through several examples and then go through three case studies of varying difficulty.

\hypertarget{getting-started-3}{%
\subsection{Getting started}\label{getting-started-3}}

Figure \ref{fig:firstpdfexample} is a PDF that consists of just the first sentence from Jane Eyre taken from Project Gutenberg \citet{janeeyre}.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/inputs/pdfs/first_example} \caption{First sentence of Jane Eyre}\label{fig:firstpdfexample}
\end{figure}

We will use the package \texttt{pdftools} \citet{citepdftools} to get the text in this one page PDF into R.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("pdftools")}
\FunctionTok{library}\NormalTok{(pdftools)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{first\_example }\OtherTok{\textless{}{-}}\NormalTok{ pdftools}\SpecialCharTok{::}\FunctionTok{pdf\_text}\NormalTok{(}\StringTok{"inputs/pdfs/first\_example.pdf"}\NormalTok{)}

\NormalTok{first\_example}
\CommentTok{\#\textgreater{} [1] "There was no possibility of taking a walk that day.\textbackslash{}n"}

\FunctionTok{class}\NormalTok{(first\_example)}
\CommentTok{\#\textgreater{} [1] "character"}
\end{Highlighting}
\end{Shaded}

We can see that the PDF has been correctly read in, as a character vector.

We will now try a slightly more complicated example that consists of the first few paragraphs of Jane Eyre (Figure \ref{fig:secondpdfexample}). Also notice that now we have the chapter heading as well.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/inputs/pdfs/second_example} \caption{First few paragraphs of Jane Eyre}\label{fig:secondpdfexample}
\end{figure}

We use the same function as before.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{second\_example }\OtherTok{\textless{}{-}}\NormalTok{ pdftools}\SpecialCharTok{::}\FunctionTok{pdf\_text}\NormalTok{(}\StringTok{"inputs/pdfs/second\_example.pdf"}\NormalTok{)}

\NormalTok{second\_example}
\CommentTok{\#\textgreater{} [1] "CHAPTER I\textbackslash{}nThere was no possibility of taking a walk that day. We had been wandering, indeed, in the\textbackslash{}nleafless shrubbery an hour in the morning; but since dinner (Mrs. Reed, when there was no\textbackslash{}ncompany, dined early) the cold winter wind had brought with it clouds so sombre, and a rain so\textbackslash{}npenetrating, that further out{-}door exercise was now out of the question.\textbackslash{}n\textbackslash{}nI was glad of it: I never liked long walks, especially on chilly afternoons: dreadful to me was the\textbackslash{}ncoming home in the raw twilight, with nipped fingers and toes, and a heart saddened by the\textbackslash{}nchidings of Bessie, the nurse, and humbled by the consciousness of my physical inferiority to\textbackslash{}nEliza, John, and Georgiana Reed.\textbackslash{}n\textbackslash{}nThe said Eliza, John, and Georgiana were now clustered round their mama in the drawing{-}room:\textbackslash{}nshe lay reclined on a sofa by the fireside, and with her darlings about her (for the time neither\textbackslash{}nquarrelling nor crying) looked perfectly happy. Me, she had dispensed from joining the group;\textbackslash{}nsaying, “She regretted to be under the necessity of keeping me at a distance; but that until she\textbackslash{}nheard from Bessie, and could discover by her own observation, that I was endeavouring in good\textbackslash{}nearnest to acquire a more sociable and childlike disposition, a more attractive and sprightly\textbackslash{}nmanner—something lighter, franker, more natural, as it were—she really must exclude me from\textbackslash{}nprivileges intended only for contented, happy, little children.”\textbackslash{}n\textbackslash{}n“What does Bessie say I have done?” I asked.\textbackslash{}n\textbackslash{}n“Jane, I don’t like cavillers or questioners; besides, there is something truly forbidding in a child\textbackslash{}ntaking up her elders in that manner. Be seated somewhere; and until you can speak pleasantly,\textbackslash{}nremain silent.”\textbackslash{}n\textbackslash{}nA breakfast{-}room adjoined the drawing{-}room, I slipped in there. It contained a bookcase: I soon\textbackslash{}npossessed myself of a volume, taking care that it should be one stored with pictures. I mounted\textbackslash{}ninto the window{-}seat: gathering up my feet, I sat cross{-}legged, like a Turk; and, having drawn the\textbackslash{}nred moreen curtain nearly close, I was shrined in double retirement.\textbackslash{}n\textbackslash{}nFolds of scarlet drapery shut in my view to the right hand; to the left were the clear panes of\textbackslash{}nglass, protecting, but not separating me from the drear November day. At intervals, while\textbackslash{}nturning over the leaves of my book, I studied the aspect of that winter afternoon. Afar, it offered\textbackslash{}na pale blank of mist and cloud; near a scene of wet lawn and storm{-}beat shrub, with ceaseless\textbackslash{}nrain sweeping away wildly before a long and lamentable blast.\textbackslash{}n"}

\FunctionTok{class}\NormalTok{(second\_example)}
\CommentTok{\#\textgreater{} [1] "character"}
\end{Highlighting}
\end{Shaded}

Again, we have a character vector. The end of each line is signalled by `\textbackslash n', but other than that it looks pretty good.

Finally, we consider the first two pages.

We use the same function as before.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{third\_example }\OtherTok{\textless{}{-}}\NormalTok{ pdftools}\SpecialCharTok{::}\FunctionTok{pdf\_text}\NormalTok{(}\StringTok{"inputs/pdfs/third\_example.pdf"}\NormalTok{)}

\NormalTok{third\_example}
\CommentTok{\#\textgreater{} [1] "CHAPTER I\textbackslash{}nThere was no possibility of taking a walk that day. We had been wandering, indeed, in the\textbackslash{}nleafless shrubbery an hour in the morning; but since dinner (Mrs. Reed, when there was no\textbackslash{}ncompany, dined early) the cold winter wind had brought with it clouds so sombre, and a rain so\textbackslash{}npenetrating, that further out{-}door exercise was now out of the question.\textbackslash{}n\textbackslash{}nI was glad of it: I never liked long walks, especially on chilly afternoons: dreadful to me was the\textbackslash{}ncoming home in the raw twilight, with nipped fingers and toes, and a heart saddened by the\textbackslash{}nchidings of Bessie, the nurse, and humbled by the consciousness of my physical inferiority to\textbackslash{}nEliza, John, and Georgiana Reed.\textbackslash{}n\textbackslash{}nThe said Eliza, John, and Georgiana were now clustered round their mama in the drawing{-}room:\textbackslash{}nshe lay reclined on a sofa by the fireside, and with her darlings about her (for the time neither\textbackslash{}nquarrelling nor crying) looked perfectly happy. Me, she had dispensed from joining the group;\textbackslash{}nsaying, “She regretted to be under the necessity of keeping me at a distance; but that until she\textbackslash{}nheard from Bessie, and could discover by her own observation, that I was endeavouring in good\textbackslash{}nearnest to acquire a more sociable and childlike disposition, a more attractive and sprightly\textbackslash{}nmanner—something lighter, franker, more natural, as it were—she really must exclude me from\textbackslash{}nprivileges intended only for contented, happy, little children.”\textbackslash{}n\textbackslash{}n“What does Bessie say I have done?” I asked.\textbackslash{}n\textbackslash{}n“Jane, I don’t like cavillers or questioners; besides, there is something truly forbidding in a child\textbackslash{}ntaking up her elders in that manner. Be seated somewhere; and until you can speak pleasantly,\textbackslash{}nremain silent.”\textbackslash{}n\textbackslash{}nA breakfast{-}room adjoined the drawing{-}room, I slipped in there. It contained a bookcase: I soon\textbackslash{}npossessed myself of a volume, taking care that it should be one stored with pictures. I mounted\textbackslash{}ninto the window{-}seat: gathering up my feet, I sat cross{-}legged, like a Turk; and, having drawn the\textbackslash{}nred moreen curtain nearly close, I was shrined in double retirement.\textbackslash{}n\textbackslash{}nFolds of scarlet drapery shut in my view to the right hand; to the left were the clear panes of\textbackslash{}nglass, protecting, but not separating me from the drear November day. At intervals, while\textbackslash{}nturning over the leaves of my book, I studied the aspect of that winter afternoon. Afar, it offered\textbackslash{}na pale blank of mist and cloud; near a scene of wet lawn and storm{-}beat shrub, with ceaseless\textbackslash{}nrain sweeping away wildly before a long and lamentable blast.\textbackslash{}n\textbackslash{}nI returned to my book—Bewick’s History of British Birds: the letterpress thereof I cared little\textbackslash{}nfor, generally speaking; and yet there were certain introductory pages that, child as I was, I could\textbackslash{}nnot pass quite as a blank. They were those which treat of the haunts of sea{-}fowl; of “the solitary\textbackslash{}nrocks and promontories” by them only inhabited; of the coast of Norway, studded with isles from\textbackslash{}nits southern extremity, the Lindeness, or Naze, to the North Cape—\textbackslash{}n\textbackslash{}n“Where the Northern Ocean, in vast whirls,\textbackslash{}nBoils round the naked, melancholy isles\textbackslash{}n"}
\CommentTok{\#\textgreater{} [2] "Of farthest Thule; and the Atlantic surge\textbackslash{}nPours in among the stormy Hebrides.”\textbackslash{}n\textbackslash{}nNor could I pass unnoticed the suggestion of the bleak shores of Lapland, Siberia, Spitzbergen,\textbackslash{}nNova Zembla, Iceland, Greenland, with “the vast sweep of the Arctic Zone, and those forlorn\textbackslash{}nregions of dreary space,—that reservoir of frost and snow, where firm fields of ice, the\textbackslash{}naccumulation of centuries of winters, glazed in Alpine heights above heights, surround the pole,\textbackslash{}nand concentre the multiplied rigours of extreme cold.” Of these death{-}white realms I formed an\textbackslash{}nidea of my own: shadowy, like all the half{-}comprehended notions that float dim through\textbackslash{}nchildren’s brains, but strangely impressive. The words in these introductory pages connected\textbackslash{}nthemselves with the succeeding vignettes, and gave significance to the rock standing up alone in\textbackslash{}na sea of billow and spray; to the broken boat stranded on a desolate coast; to the cold and ghastly\textbackslash{}nmoon glancing through bars of cloud at a wreck just sinking.\textbackslash{}n\textbackslash{}nI cannot tell what sentiment haunted the quite solitary churchyard, with its inscribed headstone;\textbackslash{}nits gate, its two trees, its low horizon, girdled by a broken wall, and its newly{-}risen crescent,\textbackslash{}nattesting the hour of eventide.\textbackslash{}n\textbackslash{}nThe two ships becalmed on a torpid sea, I believed to be marine phantoms.\textbackslash{}n\textbackslash{}nThe fiend pinning down the thief’s pack behind him, I passed over quickly: it was an object of\textbackslash{}nterror.\textbackslash{}n\textbackslash{}nSo was the black horned thing seated aloof on a rock, surveying a distant crowd surrounding a\textbackslash{}ngallows.\textbackslash{}n\textbackslash{}nEach picture told a story; mysterious often to my undeveloped understanding and imperfect\textbackslash{}nfeelings, yet ever profoundly interesting: as interesting as the tales Bessie sometimes narrated on\textbackslash{}nwinter evenings, when she chanced to be in good humour; and when, having brought her ironing{-}\textbackslash{}ntable to the nursery hearth, she allowed us to sit about it, and while she got up Mrs. Reed’s lace\textbackslash{}nfrills, and crimped her nightcap borders, fed our eager attention with passages of love and\textbackslash{}nadventure taken from old fairy tales and other ballads; or (as at a later period I discovered) from\textbackslash{}nthe pages of Pamela, and Henry, Earl of Moreland.\textbackslash{}n\textbackslash{}nWith Bewick on my knee, I was then happy: happy at least in my way. I feared nothing but\textbackslash{}ninterruption, and that came too soon. The breakfast{-}room door opened.\textbackslash{}n\textbackslash{}n“Boh! Madam Mope!” cried the voice of John Reed; then he paused: he found the room\textbackslash{}napparently empty.\textbackslash{}n\textbackslash{}n“Where the dickens is she!” he continued. “Lizzy! Georgy! (calling to his sisters) Joan is not\textbackslash{}nhere: tell mama she is run out into the rain—bad animal!”\textbackslash{}n\textbackslash{}n“It is well I drew the curtain,” thought I; and I wished fervently he might not discover my hiding{-}\textbackslash{}nplace: nor would John Reed have found it out himself; he was not quick either of vision or\textbackslash{}nconception; but Eliza just put her head in at the door, and said at once—\textbackslash{}n"}

\FunctionTok{class}\NormalTok{(third\_example)}
\CommentTok{\#\textgreater{} [1] "character"}
\end{Highlighting}
\end{Shaded}

Now, notice that the first page is the first element of the character vector and the second page is the second element.

As we're most familiar with rectangular data we'll try to get it into that format as quickly as possible. And then we can use our regular tools to deal with it.

First we want to convert the character vector into a tibble. At this point we may like to add page numbers as well.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{jane\_eyre }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{raw\_text =}\NormalTok{ third\_example,}
                    \AttributeTok{page\_number =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

We probably now want to separate the lines so that each line is an observation. We can do that by looking for the `\textbackslash n' remembering that we need to escape the backslash as it's a special character.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{jane\_eyre }\OtherTok{\textless{}{-}} \FunctionTok{separate\_rows}\NormalTok{(jane\_eyre, raw\_text, }\AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{n"}\NormalTok{, }\AttributeTok{convert =} \ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{head}\NormalTok{(jane\_eyre)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 2}
\CommentTok{\#\textgreater{}   raw\_text                                                           page\_number}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}                                                                    \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 "CHAPTER I"                                                                  1}
\CommentTok{\#\textgreater{} 2 "There was no possibility of taking a walk that day. We had been \textasciitilde{}           1}
\CommentTok{\#\textgreater{} 3 "leafless shrubbery an hour in the morning; but since dinner (Mrs\textasciitilde{}           1}
\CommentTok{\#\textgreater{} 4 "company, dined early) the cold winter wind had brought with it c\textasciitilde{}           1}
\CommentTok{\#\textgreater{} 5 "penetrating, that further out{-}door exercise was now out of the q\textasciitilde{}           1}
\CommentTok{\#\textgreater{} 6 ""                                                                           1}
\end{Highlighting}
\end{Shaded}

\hypertarget{case-study-us-total-fertility-rate-by-state-and-year-2000-2018}{%
\section{Case-study: US Total Fertility Rate, by state and year (2000-2018)}\label{case-study-us-total-fertility-rate-by-state-and-year-2000-2018}}

\hypertarget{introduction-9}{%
\subsection{Introduction}\label{introduction-9}}

If you're married to a demographer it is not too long until you are asked to look at a US Department of Health and Human Services Vital Statistics Report. In this case we are interested in trying to get the total fertility rate (the average number of births per woman assuming that woman experience the current age-specific fertility rates throughout their reproductive years)\footnote{And if you'd like to know more about this then I'd recommend starting a PhD with \href{https://www.monicaalexander.com/}{Monica Alexander}.} for each state for nineteen years. Annoyingly, the US persists in only making this data available in PDFs, but it makes a nice case study.

In the case of the year 2000 the table that we are interested in is on page 40 of a PDF that is available \url{https://www.cdc.gov/nchs/data/nvsr/nvsr50/nvsr50_05.pdf} and it is the column labelled: ``Total fertility rate'' (Figure \ref{fig:dhsexample}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/dhs_example} \caption{Example Vital Statistics Report, from 2000}\label{fig:dhsexample}
\end{figure}

\hypertarget{begin-with-an-end-in-mind}{%
\subsection{Begin with an end in mind}\label{begin-with-an-end-in-mind}}

The first step when getting data out of a PDF is to sketch out what you eventually want. A PDF typically contains a lot of information, and so it is handy to be very clear about what you need. This helps keep you focused, and prevents scope creep, but it is also helpful when thinking about data checks. Literally write down on paper what you have in mind.

In this case, what is needed is a table with a column for state, year and TFR (Figure \ref{fig:tfrdesired}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/tfr_desired} \caption{Desired output from the PDF}\label{fig:tfrdesired}
\end{figure}

\hypertarget{start-simple-then-iterate.}{%
\subsection{Start simple, then iterate.}\label{start-simple-then-iterate.}}

There are 19 different PDFs, and we are interested in a particular column in a particular table in each of them. Unfortunately, there is nothing magical about what is coming. This first step requires working out the link for each, and the page and column name that is of interest. In the end, this looks like this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{monicas\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"inputs/tfr\_tables\_info.csv"}\NormalTok{)}

\NormalTok{monicas\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(year, page, table, column\_name, url) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{gt}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\captionsetup[table]{labelformat=empty,skip=1pt}
\begin{longtable}{rrrll}
\toprule
year & page & table & column\_name & url \\ 
\midrule
2000 & 40 & 10 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr50/nvsr50\_05.pdf \\ 
2001 & 41 & 10 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr51/nvsr51\_02.pdf \\ 
2002 & 46 & 10 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr52/nvsr52\_10.pdf \\ 
2003 & 45 & 10 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr54/nvsr54\_02.pdf \\ 
2004 & 52 & 11 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr55/nvsr55\_01.pdf \\ 
2005 & 52 & 11 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr56/nvsr56\_06.pdf \\ 
2006 & 49 & 11 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr57/nvsr57\_07.pdf \\ 
2007 & 41 & 11 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr58/nvsr58\_24.pdf \\ 
2008 & 43 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr59/nvsr59\_01.pdf \\ 
2009 & 43 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr60/nvsr60\_01.pdf \\ 
2010 & 42 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr61/nvsr61\_01.pdf \\ 
2011 & 40 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr62/nvsr62\_01.pdf \\ 
2012 & 38 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr62/nvsr62\_09.pdf \\ 
2013 & 37 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr64/nvsr64\_01.pdf \\ 
2014 & 38 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr64/nvsr64\_12.pdf \\ 
2015 & 42 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr66/nvsr66\_01.pdf \\ 
2016 & 29 & 8 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr67/nvsr67\_01.pdf \\ 
2016 & 30 & 8 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr67/nvsr67\_01.pdf \\ 
2017 & 23 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr67/nvsr67\_08-508.pdf \\ 
2017 & 24 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr67/nvsr67\_08-508.pdf \\ 
2018 & 23 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr68/nvsr68\_13-508.pdf \\ 
 \bottomrule
\end{longtable}

The first step is to get some code that works for one of them. I'll step through the code in a lot more detail than normal because we're going to use these pieces a lot.

We will choose the year 2000. We first download the data and save it.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{download.file}\NormalTok{(}\AttributeTok{url =}\NormalTok{ monicas\_data}\SpecialCharTok{$}\NormalTok{url[}\DecValTok{1}\NormalTok{], }
              \AttributeTok{destfile =} \StringTok{"inputs/pdfs/dhs/year\_2000.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We now want to read the PDF in as a character vector.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dhs\_2000 }\OtherTok{\textless{}{-}}\NormalTok{ pdftools}\SpecialCharTok{::}\FunctionTok{pdf\_text}\NormalTok{(}\StringTok{"inputs/pdfs/dhs/year\_2000.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Convert it to a tibble, so that we can use familiar verbs on it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dhs\_2000 }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{raw\_data =}\NormalTok{ dhs\_2000)}

\FunctionTok{head}\NormalTok{(dhs\_2000)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 1}
\CommentTok{\#\textgreater{}   raw\_data                                                                      }
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}                                                                         }
\CommentTok{\#\textgreater{} 1 "Volume 50, Number 5                                                         \textasciitilde{}}
\CommentTok{\#\textgreater{} 2 "2   National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\textbackslash{}n\textbackslash{}n\textbackslash{}\textasciitilde{}}
\CommentTok{\#\textgreater{} 3 "                                                                            \textasciitilde{}}
\CommentTok{\#\textgreater{} 4 "4   National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\textbackslash{}n\textbackslash{}n\textbackslash{}\textasciitilde{}}
\CommentTok{\#\textgreater{} 5 "                                                                            \textasciitilde{}}
\CommentTok{\#\textgreater{} 6 "6   National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\textbackslash{}n\textbackslash{}n \textasciitilde{}}
\end{Highlighting}
\end{Shaded}

Grab the page that is of interest (remembering that each page is a element of the character vector, hence a row in the tibble).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dhs\_2000 }\OtherTok{\textless{}{-}} 
\NormalTok{  dhs\_2000 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{slice}\NormalTok{(monicas\_data}\SpecialCharTok{$}\NormalTok{page[}\DecValTok{1}\NormalTok{])}

\FunctionTok{head}\NormalTok{(dhs\_2000)}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 1}
\CommentTok{\#\textgreater{}   raw\_data                                                                      }
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}                                                                         }
\CommentTok{\#\textgreater{} 1 "40 National Vital Statistics Report, Vol. 50, No. 5, Revised May 15, 20022\textbackslash{}n\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

Now we want to separate the rows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dhs\_2000 }\OtherTok{\textless{}{-}} 
\NormalTok{  dhs\_2000 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{separate\_rows}\NormalTok{(raw\_data, }\AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{n"}\NormalTok{, }\AttributeTok{convert =} \ConstantTok{FALSE}\NormalTok{)}

\FunctionTok{head}\NormalTok{(dhs\_2000)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 1}
\CommentTok{\#\textgreater{}   raw\_data                                                                      }
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}                                                                         }
\CommentTok{\#\textgreater{} 1 "40 National Vital Statistics Report, Vol. 50, No. 5, Revised May 15, 20022"  }
\CommentTok{\#\textgreater{} 2 ""                                                                            }
\CommentTok{\#\textgreater{} 3 "Table 10. Number of births, birth rates, fertility rates, total fertility ra\textasciitilde{}}
\CommentTok{\#\textgreater{} 4 "United States, each State and territory, 2000"                               }
\CommentTok{\#\textgreater{} 5 "[By place of residence. Birth rates are live births per 1,000 estimated popu\textasciitilde{}}
\CommentTok{\#\textgreater{} 6 "estimated in each area; total fertility rates are sums of birth rates for 5{-}\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

Now we are searching for patterns that we can use. (If you have a lot of tables that you are interested in grabbing from PDFs then it may also be worthwhile considering the \texttt{tabulizer} package which is specifically designed for that \citep{citetabulizergross}. The issue is that it depends on Java and I always seem to run into trouble when I need to use Java so I avoid it when I can.)

Let's look at the first ten lines of content.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dhs\_2000[}\DecValTok{13}\SpecialCharTok{:}\DecValTok{22}\NormalTok{,]}
\CommentTok{\#\textgreater{} \# A tibble: 10 x 1}
\CommentTok{\#\textgreater{}    raw\_data                                                                     }
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}                                                                        }
\CommentTok{\#\textgreater{}  1 "                                  State                                    \textasciitilde{}}
\CommentTok{\#\textgreater{}  2 "                                                                           \textasciitilde{}}
\CommentTok{\#\textgreater{}  3 "                                                                           \textasciitilde{}}
\CommentTok{\#\textgreater{}  4 ""                                                                           }
\CommentTok{\#\textgreater{}  5 ""                                                                           }
\CommentTok{\#\textgreater{}  6 "United States 1 ......................................................     \textasciitilde{}}
\CommentTok{\#\textgreater{}  7 ""                                                                           }
\CommentTok{\#\textgreater{}  8 "Alabama ...............................................................    \textasciitilde{}}
\CommentTok{\#\textgreater{}  9 "Alaska ................................................................... \textasciitilde{}}
\CommentTok{\#\textgreater{} 10 "Arizona .................................................................  \textasciitilde{}}
\end{Highlighting}
\end{Shaded}

It doesn't get much better than this:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We have dots separating the states from the data.
\item
  We have a space between each of the columns.
\end{enumerate}

So we can now separate this in to separate columns. First we want to match on when there is at least two dots (remembering that the dot is a special character and so needs to be escaped).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dhs\_2000 }\OtherTok{\textless{}{-}} 
\NormalTok{  dhs\_2000 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{separate}\NormalTok{(}\AttributeTok{col =}\NormalTok{ raw\_data, }
           \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{, }\StringTok{"data"}\NormalTok{), }
           \AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.\{2,\}"}\NormalTok{, }
           \AttributeTok{remove =} \ConstantTok{FALSE}\NormalTok{,}
           \AttributeTok{fill =} \StringTok{"right"}
\NormalTok{           )}

\FunctionTok{head}\NormalTok{(dhs\_2000)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{}   raw\_data                              state                              data }
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}                                 \textless{}chr\textgreater{}                              \textless{}chr\textgreater{}}
\CommentTok{\#\textgreater{} 1 "40 National Vital Statistics Report\textasciitilde{} "40 National Vital Statistics Rep\textasciitilde{} \textless{}NA\textgreater{} }
\CommentTok{\#\textgreater{} 2 ""                                    ""                                 \textless{}NA\textgreater{} }
\CommentTok{\#\textgreater{} 3 "Table 10. Number of births, birth r\textasciitilde{} "Table 10. Number of births, birt\textasciitilde{} \textless{}NA\textgreater{} }
\CommentTok{\#\textgreater{} 4 "United States, each State and terri\textasciitilde{} "United States, each State and te\textasciitilde{} \textless{}NA\textgreater{} }
\CommentTok{\#\textgreater{} 5 "[By place of residence. Birth rates\textasciitilde{} "[By place of residence. Birth ra\textasciitilde{} \textless{}NA\textgreater{} }
\CommentTok{\#\textgreater{} 6 "estimated in each area; total ferti\textasciitilde{} "estimated in each area; total fe\textasciitilde{} \textless{}NA\textgreater{}}
\end{Highlighting}
\end{Shaded}

We get the expected warnings about the top and the bottom as they don't have multiple dots.

(Another option here is to use the \texttt{pdf\_data()} function which would allow us to use location rather than delimiters.)

We can now separate the data based on spaces. There is an inconsistent number of spaces, so we first squish any example of more than one space into just one.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dhs\_2000 }\OtherTok{\textless{}{-}} 
\NormalTok{  dhs\_2000 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{data =} \FunctionTok{str\_squish}\NormalTok{(data)) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  tidyr}\SpecialCharTok{::}\FunctionTok{separate}\NormalTok{(}\AttributeTok{col =}\NormalTok{ data, }
           \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"number\_of\_births"}\NormalTok{, }
                    \StringTok{"birth\_rate"}\NormalTok{, }
                    \StringTok{"fertility\_rate"}\NormalTok{, }
                    \StringTok{"TFR"}\NormalTok{, }
                    \StringTok{"teen\_births\_all"}\NormalTok{, }
                    \StringTok{"teen\_births\_15\_17"}\NormalTok{, }
                    \StringTok{"teen\_births\_18\_19"}\NormalTok{), }
           \AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s"}\NormalTok{, }
           \AttributeTok{remove =} \ConstantTok{FALSE}
\NormalTok{           )}

\FunctionTok{head}\NormalTok{(dhs\_2000)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 10}
\CommentTok{\#\textgreater{}   raw\_data      state     data  number\_of\_births birth\_rate fertility\_rate TFR  }
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}         \textless{}chr\textgreater{}     \textless{}chr\textgreater{} \textless{}chr\textgreater{}            \textless{}chr\textgreater{}      \textless{}chr\textgreater{}          \textless{}chr\textgreater{}}
\CommentTok{\#\textgreater{} 1 "40 National\textasciitilde{} "40 Nati\textasciitilde{} \textless{}NA\textgreater{}  \textless{}NA\textgreater{}             \textless{}NA\textgreater{}       \textless{}NA\textgreater{}           \textless{}NA\textgreater{} }
\CommentTok{\#\textgreater{} 2 ""            ""        \textless{}NA\textgreater{}  \textless{}NA\textgreater{}             \textless{}NA\textgreater{}       \textless{}NA\textgreater{}           \textless{}NA\textgreater{} }
\CommentTok{\#\textgreater{} 3 "Table 10. N\textasciitilde{} "Table 1\textasciitilde{} \textless{}NA\textgreater{}  \textless{}NA\textgreater{}             \textless{}NA\textgreater{}       \textless{}NA\textgreater{}           \textless{}NA\textgreater{} }
\CommentTok{\#\textgreater{} 4 "United Stat\textasciitilde{} "United \textasciitilde{} \textless{}NA\textgreater{}  \textless{}NA\textgreater{}             \textless{}NA\textgreater{}       \textless{}NA\textgreater{}           \textless{}NA\textgreater{} }
\CommentTok{\#\textgreater{} 5 "[By place o\textasciitilde{} "[By pla\textasciitilde{} \textless{}NA\textgreater{}  \textless{}NA\textgreater{}             \textless{}NA\textgreater{}       \textless{}NA\textgreater{}           \textless{}NA\textgreater{} }
\CommentTok{\#\textgreater{} 6 "estimated i\textasciitilde{} "estimat\textasciitilde{} \textless{}NA\textgreater{}  \textless{}NA\textgreater{}             \textless{}NA\textgreater{}       \textless{}NA\textgreater{}           \textless{}NA\textgreater{} }
\CommentTok{\#\textgreater{} \# ... with 3 more variables: teen\_births\_all \textless{}chr\textgreater{}, teen\_births\_15\_17 \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   teen\_births\_18\_19 \textless{}chr\textgreater{}}
\end{Highlighting}
\end{Shaded}

This is all looking fairly great. The only thing left is to clean up.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dhs\_2000 }\OtherTok{\textless{}{-}} 
\NormalTok{  dhs\_2000 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(state, TFR) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{13}\SpecialCharTok{:}\DecValTok{69}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{year =} \DecValTok{2000}\NormalTok{)}

\NormalTok{dhs\_2000}
\CommentTok{\#\textgreater{} \# A tibble: 57 x 3}
\CommentTok{\#\textgreater{}    state                                                            TFR     year}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}                                                            \textless{}chr\textgreater{}  \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1 "                                  State                       \textasciitilde{} \textless{}NA\textgreater{}    2000}
\CommentTok{\#\textgreater{}  2 "                                                              \textasciitilde{} \textless{}NA\textgreater{}    2000}
\CommentTok{\#\textgreater{}  3 "                                                              \textasciitilde{} \textless{}NA\textgreater{}    2000}
\CommentTok{\#\textgreater{}  4 ""                                                               \textless{}NA\textgreater{}    2000}
\CommentTok{\#\textgreater{}  5 ""                                                               \textless{}NA\textgreater{}    2000}
\CommentTok{\#\textgreater{}  6 "United States 1 "                                               2,130\textasciitilde{}  2000}
\CommentTok{\#\textgreater{}  7 ""                                                               \textless{}NA\textgreater{}    2000}
\CommentTok{\#\textgreater{}  8 "Alabama "                                                       2,021\textasciitilde{}  2000}
\CommentTok{\#\textgreater{}  9 "Alaska "                                                        2,437\textasciitilde{}  2000}
\CommentTok{\#\textgreater{} 10 "Arizona "                                                       2,652\textasciitilde{}  2000}
\CommentTok{\#\textgreater{} \# ... with 47 more rows}
\end{Highlighting}
\end{Shaded}

And we're done for that year. Now we want to take these pieces, put them into a function and then run that function over all 19 years.

\hypertarget{iterating}{%
\subsection{Iterating}\label{iterating}}

\hypertarget{get-the-pdfs}{%
\subsubsection{Get the PDFs}\label{get-the-pdfs}}

The first part is downloading each of the 19 PDFs that we need. We're going to build on the code that we used before. That code was:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{download.file}\NormalTok{(}\AttributeTok{url =}\NormalTok{ monicas\_data}\SpecialCharTok{$}\NormalTok{url[}\DecValTok{1}\NormalTok{], }\AttributeTok{destfile =} \StringTok{"inputs/pdfs/dhs/year\_2000.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To modify this we need:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  To have it iterate through each of the lines in the dataset that contains our CSVs (i.e.~where it says 1, we want 1, then 2, then 3, etc.).
\item
  Where it has a filename, we need it to iterate through our desired filenames (i.e.~year\_2000, then year\_2001, then year\_2002, etc).
\item
  We'd like for it to do all of this in a way that is a little robust to errors. For instance, if one of the URLs is wrong or the internet drops out then we'd like it to just move onto the next PDF, and then warn us at the end that it missed one, not to stop. (This doesn't really matter because it's only 19 files, but it's pretty easy to find yourself doing this for thousands of files).
\end{enumerate}

We will draw on the \texttt{purrr} package for this \citet{citepurrr}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(purrr)}
\NormalTok{monicas\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  monicas\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{pdf\_name =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"inputs/pdfs/dhs/year\_"}\NormalTok{, year, }\StringTok{".pdf"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{purrr}\SpecialCharTok{::}\FunctionTok{walk2}\NormalTok{(monicas\_data}\SpecialCharTok{$}\NormalTok{url, monicas\_data}\SpecialCharTok{$}\NormalTok{pdf\_name, purrr}\SpecialCharTok{::}\FunctionTok{safely}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\FunctionTok{download.file}\NormalTok{(.x , .y)))}
\end{Highlighting}
\end{Shaded}

What this code does it take the function \texttt{download.file()} and give it two arguments: \texttt{.x} and \texttt{.y}. The function \texttt{walk2()} then applies that function to the inputs that we give it, in this case the URLs columns is the \texttt{.x} and the pdf\_names column is the \texttt{.y}. Finally, the \texttt{safely()} function means that if there are any failures then it just moves onto the next file instead of throwing an error.

We now have each of the PDFs saved and we can move onto getting the data from them.

\hypertarget{get-data-from-the-pdfs}{%
\subsubsection{Get data from the PDFs}\label{get-data-from-the-pdfs}}

Now we need to get the data from the PDFs. As before, we're going to build on the code that we used before. That code (overly condensed) was:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dhs\_2000 }\OtherTok{\textless{}{-}}\NormalTok{ pdftools}\SpecialCharTok{::}\FunctionTok{pdf\_text}\NormalTok{(}\StringTok{"inputs/pdfs/dhs/year\_2000.pdf"}\NormalTok{)}

\NormalTok{dhs\_2000 }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{raw\_data =}\NormalTok{ dhs\_2000) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{slice}\NormalTok{(monicas\_data}\SpecialCharTok{$}\NormalTok{page[}\DecValTok{1}\NormalTok{]) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{separate\_rows}\NormalTok{(raw\_data, }\AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{n"}\NormalTok{, }\AttributeTok{convert =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{separate}\NormalTok{(}\AttributeTok{col =}\NormalTok{ raw\_data, }\AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{, }\StringTok{"data"}\NormalTok{), }\AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.\{2,\}"}\NormalTok{, }\AttributeTok{remove =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{data =} \FunctionTok{str\_squish}\NormalTok{(data)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{separate}\NormalTok{(}\AttributeTok{col =}\NormalTok{ data, }
           \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"number\_of\_births"}\NormalTok{, }\StringTok{"birth\_rate"}\NormalTok{, }\StringTok{"fertility\_rate"}\NormalTok{, }\StringTok{"TFR"}\NormalTok{, }\StringTok{"teen\_births\_all"}\NormalTok{, }\StringTok{"teen\_births\_15\_17"}\NormalTok{, }\StringTok{"teen\_births\_18\_19"}\NormalTok{), }
           \AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s"}\NormalTok{, }
           \AttributeTok{remove =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(state, TFR) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{13}\SpecialCharTok{:}\DecValTok{69}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{year =} \DecValTok{2000}\NormalTok{)}

\NormalTok{dhs\_2000}
\end{Highlighting}
\end{Shaded}

There are a bunch of aspects here that have been hardcoded, but the first thing that we want to iterate is the argument to \texttt{pdf\_text()}, then the number in in \texttt{slice()} will also need to change (that is doing the work to get only the page that we are interested in).

Two aspects are hardcoded, and these may need to be updated. In particular: 1) The separate only works if each of the tables has the same columns in the same order; and 2) the slice (which restricts the data to just the states) only works in this particular case. Finally, we add the year only at the end, whereas we'd need to bring that up earlier in the process.

We'll start by writing a function that will go through all the files, grab the data, get the page of interest, and then expand the rows. We'll then use a function from \texttt{purrr} to apply that function to all of the PDFs and to output a tibble.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_pdf\_convert\_to\_tibble }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(pdf\_name, page, year)\{}
  
\NormalTok{  dhs\_table\_of\_interest }\OtherTok{\textless{}{-}} 
    \FunctionTok{tibble}\NormalTok{(}\AttributeTok{raw\_data =}\NormalTok{ pdftools}\SpecialCharTok{::}\FunctionTok{pdf\_text}\NormalTok{(pdf\_name)) }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{slice}\NormalTok{(page) }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{separate\_rows}\NormalTok{(raw\_data, }\AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{n"}\NormalTok{, }\AttributeTok{convert =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{separate}\NormalTok{(}\AttributeTok{col =}\NormalTok{ raw\_data, }
             \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{, }\StringTok{"data"}\NormalTok{), }
             \AttributeTok{sep =} \StringTok{"[�|}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.]}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s+(?=[[:digit:]])"}\NormalTok{, }
             \AttributeTok{remove =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{mutate}\NormalTok{(}
      \AttributeTok{data =} \FunctionTok{str\_squish}\NormalTok{(data),}
      \AttributeTok{year\_of\_data =}\NormalTok{ year)}

  \FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"Done with"}\NormalTok{, year))}
  
  \FunctionTok{return}\NormalTok{(dhs\_table\_of\_interest)}
\NormalTok{\}}

\NormalTok{raw\_dhs\_data }\OtherTok{\textless{}{-}}\NormalTok{ purrr}\SpecialCharTok{::}\FunctionTok{pmap\_dfr}\NormalTok{(monicas\_data }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(pdf\_name, page, year),}
\NormalTok{                                get\_pdf\_convert\_to\_tibble)}
\CommentTok{\#\textgreater{} [1] "Done with 2000"}
\CommentTok{\#\textgreater{} [1] "Done with 2001"}
\CommentTok{\#\textgreater{} [1] "Done with 2002"}
\CommentTok{\#\textgreater{} [1] "Done with 2003"}
\CommentTok{\#\textgreater{} [1] "Done with 2004"}
\CommentTok{\#\textgreater{} [1] "Done with 2005"}
\CommentTok{\#\textgreater{} [1] "Done with 2006"}
\CommentTok{\#\textgreater{} [1] "Done with 2007"}
\CommentTok{\#\textgreater{} [1] "Done with 2008"}
\CommentTok{\#\textgreater{} [1] "Done with 2009"}
\CommentTok{\#\textgreater{} [1] "Done with 2010"}
\CommentTok{\#\textgreater{} [1] "Done with 2011"}
\CommentTok{\#\textgreater{} [1] "Done with 2012"}
\CommentTok{\#\textgreater{} [1] "Done with 2013"}
\CommentTok{\#\textgreater{} [1] "Done with 2014"}
\CommentTok{\#\textgreater{} [1] "Done with 2015"}
\CommentTok{\#\textgreater{} [1] "Done with 2016"}
\CommentTok{\#\textgreater{} [1] "Done with 2016"}
\CommentTok{\#\textgreater{} [1] "Done with 2017"}
\CommentTok{\#\textgreater{} [1] "Done with 2017"}
\CommentTok{\#\textgreater{} [1] "Done with 2018"}

\FunctionTok{head}\NormalTok{(raw\_dhs\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 4}
\CommentTok{\#\textgreater{}   raw\_data                       state                        data  year\_of\_data}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}                          \textless{}chr\textgreater{}                        \textless{}chr\textgreater{}        \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 "40 National Vital Statistics\textasciitilde{} "40 National Vital Statisti\textasciitilde{} 50, \textasciitilde{}         2000}
\CommentTok{\#\textgreater{} 2 ""                             ""                           \textless{}NA\textgreater{}          2000}
\CommentTok{\#\textgreater{} 3 "Table 10. Number of births, \textasciitilde{} "Table 10. Number of births\textasciitilde{} \textless{}NA\textgreater{}          2000}
\CommentTok{\#\textgreater{} 4 "United States, each State an\textasciitilde{} "United States, each State \textasciitilde{} \textless{}NA\textgreater{}          2000}
\CommentTok{\#\textgreater{} 5 "[By place of residence. Birt\textasciitilde{} "[By place of residence. Bi\textasciitilde{} \textless{}NA\textgreater{}          2000}
\CommentTok{\#\textgreater{} 6 "estimated in each area; tota\textasciitilde{} "estimated in each area; to\textasciitilde{} \textless{}NA\textgreater{}          2000}
\end{Highlighting}
\end{Shaded}

Now we need to clean up the state names and then filter on them.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{states }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Alabama"}\NormalTok{, }\StringTok{"Alaska"}\NormalTok{, }\StringTok{"Arizona"}\NormalTok{, }\StringTok{"Arkansas"}\NormalTok{, }\StringTok{"California"}\NormalTok{, }\StringTok{"Colorado"}\NormalTok{, }
            \StringTok{"Connecticut"}\NormalTok{, }\StringTok{"Delaware"}\NormalTok{, }\StringTok{"Florida"}\NormalTok{, }\StringTok{"Georgia"}\NormalTok{, }\StringTok{"Hawaii"}\NormalTok{, }\StringTok{"Idaho"}\NormalTok{, }
            \StringTok{"Illinois"}\NormalTok{, }\StringTok{"Indiana"}\NormalTok{, }\StringTok{"Iowa"}\NormalTok{, }\StringTok{"Kansas"}\NormalTok{, }\StringTok{"Kentucky"}\NormalTok{, }\StringTok{"Louisiana"}\NormalTok{, }
            \StringTok{"Maine"}\NormalTok{, }\StringTok{"Maryland"}\NormalTok{, }\StringTok{"Massachusetts"}\NormalTok{, }\StringTok{"Michigan"}\NormalTok{, }\StringTok{"Minnesota"}\NormalTok{, }
            \StringTok{"Mississippi"}\NormalTok{, }\StringTok{"Missouri"}\NormalTok{, }\StringTok{"Montana"}\NormalTok{, }\StringTok{"Nebraska"}\NormalTok{, }\StringTok{"Nevada"}\NormalTok{, }
            \StringTok{"New Hampshire"}\NormalTok{, }\StringTok{"New Jersey"}\NormalTok{, }\StringTok{"New Mexico"}\NormalTok{, }\StringTok{"New York"}\NormalTok{, }\StringTok{"North Carolina"}\NormalTok{, }
            \StringTok{"North Dakota"}\NormalTok{, }\StringTok{"Ohio"}\NormalTok{, }\StringTok{"Oklahoma"}\NormalTok{, }\StringTok{"Oregon"}\NormalTok{, }\StringTok{"Pennsylvania"}\NormalTok{, }
            \StringTok{"Rhode Island"}\NormalTok{, }\StringTok{"South Carolina"}\NormalTok{, }\StringTok{"South Dakota"}\NormalTok{, }\StringTok{"Tennessee"}\NormalTok{, }\StringTok{"Texas"}\NormalTok{, }
            \StringTok{"Utah"}\NormalTok{, }\StringTok{"Vermont"}\NormalTok{, }\StringTok{"Virginia"}\NormalTok{, }\StringTok{"Washington"}\NormalTok{, }\StringTok{"West Virginia"}\NormalTok{, }\StringTok{"Wisconsin"}\NormalTok{, }
            \StringTok{"Wyoming"}\NormalTok{, }\StringTok{"District of Columbia"}\NormalTok{)}

\NormalTok{raw\_dhs\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  raw\_dhs\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{state =} \FunctionTok{str\_remove\_all}\NormalTok{(state, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{."}\NormalTok{),}
         \AttributeTok{state =} \FunctionTok{str\_remove\_all}\NormalTok{(state, }\StringTok{"�"}\NormalTok{),}
         \AttributeTok{state =} \FunctionTok{str\_remove\_all}\NormalTok{(state, }\StringTok{"\textbackslash{}u0008"}\NormalTok{),}
         \AttributeTok{state =} \FunctionTok{str\_replace\_all}\NormalTok{(state, }\StringTok{"United States 1"}\NormalTok{, }\StringTok{"United States"}\NormalTok{),}
         \AttributeTok{state =} \FunctionTok{str\_replace\_all}\NormalTok{(state, }\StringTok{"United States1"}\NormalTok{, }\StringTok{"United States"}\NormalTok{),}
         \AttributeTok{state =} \FunctionTok{str\_replace\_all}\NormalTok{(state, }\StringTok{"United States 2"}\NormalTok{, }\StringTok{"United States"}\NormalTok{),}
         \AttributeTok{state =} \FunctionTok{str\_replace\_all}\NormalTok{(state, }\StringTok{"United States2"}\NormalTok{, }\StringTok{"United States"}\NormalTok{),}
         \AttributeTok{state =} \FunctionTok{str\_replace\_all}\NormalTok{(state, }\StringTok{"United States²"}\NormalTok{, }\StringTok{"United States"}\NormalTok{),}
\NormalTok{         ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{state =} \FunctionTok{str\_squish}\NormalTok{(state)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(state }\SpecialCharTok{\%in\%}\NormalTok{ states)}

\FunctionTok{head}\NormalTok{(raw\_dhs\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 4}
\CommentTok{\#\textgreater{}   raw\_data                              state   data                year\_of\_data}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}                                 \textless{}chr\textgreater{}   \textless{}chr\textgreater{}                      \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Alabama ............................\textasciitilde{} Alabama 63,299 14.4 65.0 2\textasciitilde{}         2000}
\CommentTok{\#\textgreater{} 2 Alaska .............................\textasciitilde{} Alaska  9,974 16.0 74.6 2,\textasciitilde{}         2000}
\CommentTok{\#\textgreater{} 3 Arizona ............................\textasciitilde{} Arizona 85,273 17.5 84.4 2\textasciitilde{}         2000}
\CommentTok{\#\textgreater{} 4 Arkansas ...........................\textasciitilde{} Arkans\textasciitilde{} 37,783 14.7 69.1 2\textasciitilde{}         2000}
\CommentTok{\#\textgreater{} 5 California .........................\textasciitilde{} Califo\textasciitilde{} 531,959 15.8 70.7 \textasciitilde{}         2000}
\CommentTok{\#\textgreater{} 6 Colorado ...........................\textasciitilde{} Colora\textasciitilde{} 65,438 15.8 73.1 2\textasciitilde{}         2000}
\end{Highlighting}
\end{Shaded}

The next step is to separate the data and get the correct column from it. We're going to separate based on spaces once it is cleaned up.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{raw\_dhs\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  raw\_dhs\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{data =} \FunctionTok{str\_remove\_all}\NormalTok{(data, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{*"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{separate}\NormalTok{(data, }\AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"col\_1"}\NormalTok{, }\StringTok{"col\_2"}\NormalTok{, }\StringTok{"col\_3"}\NormalTok{, }\StringTok{"col\_4"}\NormalTok{, }\StringTok{"col\_5"}\NormalTok{, }
                          \StringTok{"col\_6"}\NormalTok{, }\StringTok{"col\_7"}\NormalTok{, }\StringTok{"col\_8"}\NormalTok{, }\StringTok{"col\_9"}\NormalTok{, }\StringTok{"col\_10"}\NormalTok{), }
           \AttributeTok{sep =} \StringTok{" "}\NormalTok{,}
           \AttributeTok{remove =} \ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{head}\NormalTok{(raw\_dhs\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 14}
\CommentTok{\#\textgreater{}   raw\_data    state data   col\_1 col\_2 col\_3 col\_4 col\_5 col\_6 col\_7 col\_8 col\_9}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}       \textless{}chr\textgreater{} \textless{}chr\textgreater{}  \textless{}chr\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{}}
\CommentTok{\#\textgreater{} 1 Alabama ..\textasciitilde{} Alab\textasciitilde{} 63,29\textasciitilde{} 63,2\textasciitilde{} 14.4  65.0  2,02\textasciitilde{} 62.9  37.9  97.3  \textless{}NA\textgreater{}  \textless{}NA\textgreater{} }
\CommentTok{\#\textgreater{} 2 Alaska ...\textasciitilde{} Alas\textasciitilde{} 9,974\textasciitilde{} 9,974 16.0  74.6  2,43\textasciitilde{} 42.4  23.6  69.4  \textless{}NA\textgreater{}  \textless{}NA\textgreater{} }
\CommentTok{\#\textgreater{} 3 Arizona ..\textasciitilde{} Ariz\textasciitilde{} 85,27\textasciitilde{} 85,2\textasciitilde{} 17.5  84.4  2,65\textasciitilde{} 69.1  41.1  111.3 \textless{}NA\textgreater{}  \textless{}NA\textgreater{} }
\CommentTok{\#\textgreater{} 4 Arkansas .\textasciitilde{} Arka\textasciitilde{} 37,78\textasciitilde{} 37,7\textasciitilde{} 14.7  69.1  2,14\textasciitilde{} 68.5  36.7  114.1 \textless{}NA\textgreater{}  \textless{}NA\textgreater{} }
\CommentTok{\#\textgreater{} 5 California\textasciitilde{} Cali\textasciitilde{} 531,9\textasciitilde{} 531,\textasciitilde{} 15.8  70.7  2,18\textasciitilde{} 48.5  28.6  75.6  \textless{}NA\textgreater{}  \textless{}NA\textgreater{} }
\CommentTok{\#\textgreater{} 6 Colorado .\textasciitilde{} Colo\textasciitilde{} 65,43\textasciitilde{} 65,4\textasciitilde{} 15.8  73.1  2,35\textasciitilde{} 49.2  28.6  79.8  \textless{}NA\textgreater{}  \textless{}NA\textgreater{} }
\CommentTok{\#\textgreater{} \# ... with 2 more variables: col\_10 \textless{}chr\textgreater{}, year\_of\_data \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

We can now grab the correct column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tfr\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  raw\_dhs\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{TFR =} \FunctionTok{if\_else}\NormalTok{(year\_of\_data }\SpecialCharTok{\textless{}} \DecValTok{2008}\NormalTok{, col\_4, col\_3)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(state, year\_of\_data, TFR) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{year =}\NormalTok{ year\_of\_data)}
\FunctionTok{head}\NormalTok{(tfr\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{}   state       year TFR    }
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}      \textless{}dbl\textgreater{} \textless{}chr\textgreater{}  }
\CommentTok{\#\textgreater{} 1 Alabama     2000 2,021.0}
\CommentTok{\#\textgreater{} 2 Alaska      2000 2,437.0}
\CommentTok{\#\textgreater{} 3 Arizona     2000 2,652.5}
\CommentTok{\#\textgreater{} 4 Arkansas    2000 2,140.0}
\CommentTok{\#\textgreater{} 5 California  2000 2,186.0}
\CommentTok{\#\textgreater{} 6 Colorado    2000 2,356.5}
\end{Highlighting}
\end{Shaded}

Finally, we need to convert the case.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(tfr\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{}   state       year TFR    }
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}      \textless{}dbl\textgreater{} \textless{}chr\textgreater{}  }
\CommentTok{\#\textgreater{} 1 Alabama     2000 2,021.0}
\CommentTok{\#\textgreater{} 2 Alaska      2000 2,437.0}
\CommentTok{\#\textgreater{} 3 Arizona     2000 2,652.5}
\CommentTok{\#\textgreater{} 4 Arkansas    2000 2,140.0}
\CommentTok{\#\textgreater{} 5 California  2000 2,186.0}
\CommentTok{\#\textgreater{} 6 Colorado    2000 2,356.5}

\NormalTok{tfr\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  tfr\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{TFR =} \FunctionTok{str\_remove\_all}\NormalTok{(TFR, }\StringTok{","}\NormalTok{),}
         \AttributeTok{TFR =} \FunctionTok{as.numeric}\NormalTok{(TFR))}

\FunctionTok{head}\NormalTok{(tfr\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{}   state       year   TFR}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}      \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Alabama     2000 2021 }
\CommentTok{\#\textgreater{} 2 Alaska      2000 2437 }
\CommentTok{\#\textgreater{} 3 Arizona     2000 2652.}
\CommentTok{\#\textgreater{} 4 Arkansas    2000 2140 }
\CommentTok{\#\textgreater{} 5 California  2000 2186 }
\CommentTok{\#\textgreater{} 6 Colorado    2000 2356.}
\end{Highlighting}
\end{Shaded}

And run some checks.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# tfr\_data \%\textgreater{}\% }
\CommentTok{\#   skimr::skim()}
\end{Highlighting}
\end{Shaded}

In particular we want for there to be 51 states and for there to be 19 years.

And we're done.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(tfr\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{}   state       year   TFR}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}      \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Alabama     2000 2021 }
\CommentTok{\#\textgreater{} 2 Alaska      2000 2437 }
\CommentTok{\#\textgreater{} 3 Arizona     2000 2652.}
\CommentTok{\#\textgreater{} 4 Arkansas    2000 2140 }
\CommentTok{\#\textgreater{} 5 California  2000 2186 }
\CommentTok{\#\textgreater{} 6 Colorado    2000 2356.}

\FunctionTok{write\_csv}\NormalTok{(tfr\_data, }\StringTok{"outputs/monicas\_tfr.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{semi-structured}{%
\section{Semi-structured}\label{semi-structured}}

\hypertarget{json-and-xml}{%
\subsection{JSON and XML}\label{json-and-xml}}

\hypertarget{optical-character-recognition}{%
\section{Optical Character Recognition}\label{optical-character-recognition}}

All of the above is predicated on having a PDF that is already `digitized'. But what if it is images? In that case you need to first use Optical Character Recognition (OCR). The go-to package is Tesseract \citep{citetesseract}. This is a R wrapper around the Tesseract open-source OCR engine.

Let's see an example with a scan from the first page of Jane Eyre (Figure \ref{fig:janescan}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/jane_scan} \caption{Scan of first page of Jane Eyre.}\label{fig:janescan}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages(\textquotesingle{}tesseract\textquotesingle{})}
\FunctionTok{library}\NormalTok{(tesseract)}
\NormalTok{text }\OtherTok{\textless{}{-}}\NormalTok{ tesseract}\SpecialCharTok{::}\FunctionTok{ocr}\NormalTok{(here}\SpecialCharTok{::}\FunctionTok{here}\NormalTok{(}\StringTok{"figures/jane\_scan.png"}\NormalTok{), }\AttributeTok{engine =} \FunctionTok{tesseract}\NormalTok{(}\StringTok{"eng"}\NormalTok{))}
\FunctionTok{cat}\NormalTok{(text)}
\CommentTok{\#\textgreater{} 1 THERE was no possibility of taking a walk that day. We had}
\CommentTok{\#\textgreater{} been wandering, indeed, in the leafless shrubbery an hour in}
\CommentTok{\#\textgreater{} the morning; but since dinner (Mrs Reed, when there was no com{-}}
\CommentTok{\#\textgreater{} pany, dined early) the cold winter wind had brought with it clouds}
\CommentTok{\#\textgreater{} so sombre, and a rain so penetrating, that further out{-}door exercise}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} was now out of the question.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} I was glad of it: I never liked long walks, especially on chilly}
\CommentTok{\#\textgreater{} afternoons: dreadful to me was the coming home in the raw twi{-}}
\CommentTok{\#\textgreater{} light, with nipped fingers and toes, and a heart saddened by the}
\CommentTok{\#\textgreater{} chidings of Bessie, the nurse, and humbled by the consciousness of}
\CommentTok{\#\textgreater{} my physical inferiority to Eliza, John, and Georgiana Reed.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} The said Eliza, John, and Georgiana were now clustered round}
\CommentTok{\#\textgreater{} their mama in the drawing{-}room: she lay reclined on a sofa by the}
\CommentTok{\#\textgreater{} fireside, and with her darlings about her (for the time neither quar{-}}
\CommentTok{\#\textgreater{} relling nor crying) looked perfectly happy. Me, she had dispensed}
\CommentTok{\#\textgreater{} from joining the group; saying, ‘She regretted to be under the}
\CommentTok{\#\textgreater{} necessity of keeping me at a distance; but that until she heard from}
\CommentTok{\#\textgreater{} Bessie, and could discover by her own observation that I was}
\CommentTok{\#\textgreater{} endeavouring in good earnest to acquire a more sociable and}
\CommentTok{\#\textgreater{} child{-}like disposition, a more attractive and sprightly manner—}
\CommentTok{\#\textgreater{} something lighter, franker, more natural as it were—she really}
\CommentTok{\#\textgreater{} must exclude me from privileges intended only for contented,}
\CommentTok{\#\textgreater{} happy, littie children.’}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} ‘What does Bessie say I have done?’ I asked.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} ‘Jane, I don’t like cavillers or questioners: besides, there is}
\CommentTok{\#\textgreater{} something truly forbidding in a child taking up her elders in that}
\CommentTok{\#\textgreater{} manner. Be seated somewhere; and until you can speak pleasantly,}
\CommentTok{\#\textgreater{} remain silent.’}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} . Bs aT sae] eae}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} i; AN TCM TAN | Beal | Sees}
\CommentTok{\#\textgreater{} a) \} ; | i)}
\CommentTok{\#\textgreater{} i i 4 | | A ae | i | eee eek?}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} a an eames yi | bee}
\CommentTok{\#\textgreater{} 1 nea elem | | oe pee}
\CommentTok{\#\textgreater{} i i ae BC i i Hale}
\CommentTok{\#\textgreater{} oul | ec hi}
\CommentTok{\#\textgreater{} pan || i re a al! |}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} ase \} Oty 2 RIES ORT Sata ariel}
\CommentTok{\#\textgreater{} SEEN BE — =——\_}
\CommentTok{\#\textgreater{} 15}
\end{Highlighting}
\end{Shaded}

\hypertarget{text}{%
\section{Text}\label{text}}

\emph{Aspects of this section have been previously published.}

\hypertarget{introduction-10}{%
\subsection{Introduction}\label{introduction-10}}

Text data is all around us, and in many cases is some of the earliest types of data that we are exposed to. Recent increases in computational power, the development of new methods, and the enormous availability of text, means that there has been a great deal of interest in using text as data. Initial methods tend to focus, essentially, on converting text into numbers and then analysing them using traditional methods. More recent methods have begun to take advantage of the structure that is inherent in text, to draw additional meaning. The difference is perhaps akin to a child who can group similar colors, compared with a child who knows what objects are; although both crocodiles and trees are green, and you can do something with that knowledge, you can do more by knowing that a crocodile could eat you, and a tree probably won't.

In this section we cover a variety of techniques designed to equip you with the basics of using text as data. One of the great things about text data is that it is typically not generated for the purposes of our analysis. That's great because it removes one of the unobservable variables that we typically have to worry about. The trade-off is that we typically have to do a bunch more work to get it into a form that we can work with.

\hypertarget{getting-text-data}{%
\subsection{Getting text data}\label{getting-text-data}}

Text as data is an exciting tool to apply. But many guides assume that you already have a nice dataset. Because we've focused on workflow in these notes, we know that's not likely to be true! In this section we will scrape some text from a website. We've already seen examples of scraping, but in general those were focused on exploiting tables in the website. Here we're going to instead focus on paragraphs of text, hence we'll focus on different html/css tags.

We're going to us the \texttt{rvest} package to make it easier to scrape data. We're also going to use the \texttt{purrr} package to apply a function to a bunch of different URLs. For those of you with a little bit of programming, this is an alternative to using a for loop. For those of you with a bit of CS, this is a package that adds functional programming to R.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rvest)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\CommentTok{\# Some websites}
\NormalTok{address\_to\_visit }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"https://www.rba.gov.au/monetary{-}policy/rba{-}board{-}minutes/2020/2020{-}03{-}03.html"}\NormalTok{,}
                    \StringTok{"https://www.rba.gov.au/monetary{-}policy/rba{-}board{-}minutes/2020/2020{-}02{-}04.html"}\NormalTok{,}
                    \StringTok{"https://www.rba.gov.au/monetary{-}policy/rba{-}board{-}minutes/2019/2019{-}12{-}03.html"}\NormalTok{,}
                    \StringTok{"https://www.rba.gov.au/monetary{-}policy/rba{-}board{-}minutes/2019/2019{-}11{-}05.html"}\NormalTok{,}
                    \StringTok{"https://www.rba.gov.au/monetary{-}policy/rba{-}board{-}minutes/2019/2019{-}10{-}01.html"}\NormalTok{,}
                    \StringTok{"https://www.rba.gov.au/monetary{-}policy/rba{-}board{-}minutes/2019/2019{-}09{-}03.html"}
\NormalTok{                    )}

\CommentTok{\# Save names}
\NormalTok{save\_name }\OtherTok{\textless{}{-}}\NormalTok{ address\_to\_visit }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{str\_remove}\NormalTok{(}\StringTok{"https://www.rba.gov.au/monetary{-}policy/rba{-}board{-}minutes/"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{str\_remove}\NormalTok{(}\StringTok{".html"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{str\_remove}\NormalTok{(}\StringTok{"20[:digit:]\{2\}/"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{str\_c}\NormalTok{(}\StringTok{"inputs/rba/"}\NormalTok{, ., }\StringTok{".csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Create the function that will visit address\_to\_visit and save to save\_name files.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{visit\_address\_and\_save\_content }\OtherTok{\textless{}{-}}
  \ControlFlowTok{function}\NormalTok{(name\_of\_address\_to\_visit,}
\NormalTok{           name\_of\_file\_to\_save\_as) \{}
    \CommentTok{\# The function takes two inputs}
\NormalTok{    name\_of\_address\_to\_visit }\OtherTok{\textless{}{-}}\NormalTok{ address\_to\_visit[}\DecValTok{1}\NormalTok{]}
\NormalTok{    name\_of\_file\_to\_save\_as }\OtherTok{\textless{}{-}}\NormalTok{ save\_name[}\DecValTok{1}\NormalTok{]}
    
    \FunctionTok{read\_html}\NormalTok{(name\_of\_address\_to\_visit) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# Go to the website and read the html}
      \FunctionTok{html\_node}\NormalTok{(}\StringTok{"\#content"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# Find the content part}
      \FunctionTok{html\_text}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# Extract the text of the content part}
      \FunctionTok{write\_lines}\NormalTok{(name\_of\_file\_to\_save\_as) }\CommentTok{\# Save as a text file}
    \FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"Done with"}\NormalTok{, name\_of\_address\_to\_visit, }\StringTok{"at"}\NormalTok{, }\FunctionTok{Sys.time}\NormalTok{()))  }
    \CommentTok{\# Helpful so that you know progress when running it on all the records}
    \FunctionTok{Sys.sleep}\NormalTok{(}\FunctionTok{sample}\NormalTok{(}\DecValTok{30}\SpecialCharTok{:}\DecValTok{60}\NormalTok{, }\DecValTok{1}\NormalTok{)) }\CommentTok{\# Space out each request by somewhere between }
    \CommentTok{\# 30 and 60 seconds each so that we don\textquotesingle{}t overwhelm their server}
\NormalTok{  \}}

\CommentTok{\# If there is an error then ignore it and move to the next one}
\NormalTok{visit\_address\_and\_save\_content }\OtherTok{\textless{}{-}}
  \FunctionTok{safely}\NormalTok{(visit\_address\_and\_save\_content)}
\end{Highlighting}
\end{Shaded}

We now apply that function to our list of URLs.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Walk through the addresses and apply the function to each}
\FunctionTok{walk2}\NormalTok{(address\_to\_visit,}
\NormalTok{      save\_name,}
      \SpecialCharTok{\textasciitilde{}} \FunctionTok{visit\_address\_and\_save\_content}\NormalTok{(.x, .y))}
\end{Highlighting}
\end{Shaded}

The result is a bunch of files with saved text data.

In this case we used scraping, but there are, of course, many ways. We may be able to use APIs, for instance, In the case of the Airbnb dataset that we examined earlier in the notes. If you are lucky then it may simply be that there is a column that contains text data in your dataset.

\hypertarget{preparing-text-datasets}{%
\subsection{Preparing text datasets}\label{preparing-text-datasets}}

\emph{This section draws on Sharla Gelfand's blog post, linked in the required readings.}

As much as I would like to stick with Australian economics and politics examples, I realise that this is probably only of limited interest to most of you. As such, in this section we will consider a dataset of Sephora reviews. Please read Sharla's blog post (\url{https://sharla.party/post/crying-sephora/}) for another take on this dataset.

In this section we assume that there is some text data that you have gathered. At this point we need to change it into a form that we can work with. For some applications this will be counts of words. For others it may be some variant of this. The dataset that we are going to use is from Sephora, was scraped by \href{https://twitter.com/crabbage_/}{Connie} and I originally became aware of it because of \href{https://sharla.party/post/crying-sephora/}{Sharla}.

First let's read in the data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This code is taken from https://sharla.party/post/crying{-}sephora/}
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{library}\NormalTok{(jsonlite)}
\FunctionTok{library}\NormalTok{(tidytext)}

\NormalTok{crying }\OtherTok{\textless{}{-}} 
\NormalTok{  jsonlite}\SpecialCharTok{::}\FunctionTok{fromJSON}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/everestpipkin/datagardens/master/students/khanniie/5\_newDataSet/crying\_dataset.json"}\NormalTok{,}
  \AttributeTok{simplifyDataFrame =} \ConstantTok{TRUE}
\NormalTok{)}

\NormalTok{crying }\OtherTok{\textless{}{-}} \FunctionTok{as\_tibble}\NormalTok{(crying[[}\StringTok{"reviews"}\NormalTok{]])}

\FunctionTok{head}\NormalTok{(crying)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 6}
\CommentTok{\#\textgreater{}   date        product\_info$bra\textasciitilde{} $name $type $url  review\_body review\_title stars}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}       \textless{}chr\textgreater{}             \textless{}chr\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{}       \textless{}chr\textgreater{}        \textless{}chr\textgreater{}}
\CommentTok{\#\textgreater{} 1 29 Mar 2016 Too Faced         Bett\textasciitilde{} Masc\textasciitilde{} http\textasciitilde{} "Now I can\textasciitilde{} AWESOME      5 st\textasciitilde{}}
\CommentTok{\#\textgreater{} 2 29 Sep 2016 Too Faced         Bett\textasciitilde{} Masc\textasciitilde{} http\textasciitilde{} "This hold\textasciitilde{} if you\textquotesingle{}re s\textasciitilde{} 5 st\textasciitilde{}}
\CommentTok{\#\textgreater{} 3 23 May 2017 Too Faced         Bett\textasciitilde{} Masc\textasciitilde{} http\textasciitilde{} "I just bo\textasciitilde{} Hate it      1 st\textasciitilde{}}
\CommentTok{\#\textgreater{} 4 15 Aug 2017 Too Faced         Bett\textasciitilde{} Masc\textasciitilde{} http\textasciitilde{} "To start \textasciitilde{} Nearly perf\textasciitilde{} 5 st\textasciitilde{}}
\CommentTok{\#\textgreater{} 5 21 Sep 2016 Too Faced         Bett\textasciitilde{} Masc\textasciitilde{} http\textasciitilde{} "This masc\textasciitilde{} Amazing!!    5 st\textasciitilde{}}
\CommentTok{\#\textgreater{} 6 30 May 2016 Too Faced         Bett\textasciitilde{} Masc\textasciitilde{} http\textasciitilde{} "Let\textquotesingle{}s tal\textasciitilde{} Tricky but \textasciitilde{} 5 st\textasciitilde{}}
\CommentTok{\#\textgreater{} \# ... with 1 more variable: userid \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(crying)}
\CommentTok{\#\textgreater{} [1] "date"         "product\_info" "review\_body"  "review\_title" "stars"       }
\CommentTok{\#\textgreater{} [6] "userid"}
\end{Highlighting}
\end{Shaded}

We'll focus on the \texttt{review\_body} variable and the number of stars \texttt{stars} that the reviewer gave. Most of them are 5 stars, so we'll just focus on whether or not the review is five stars.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{crying }\OtherTok{\textless{}{-}} 
\NormalTok{  crying }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(review\_body, stars) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{stars =} \FunctionTok{str\_remove}\NormalTok{(stars, }\StringTok{" stars?"}\NormalTok{),  }\CommentTok{\# The question mark at the end means it\textquotesingle{}l get rid of \textquotesingle{}star\textquotesingle{} and \textquotesingle{}stars\textquotesingle{}.}
         \AttributeTok{stars =} \FunctionTok{as.integer}\NormalTok{(stars)}
\NormalTok{         ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{five\_stars =} \FunctionTok{if\_else}\NormalTok{(stars }\SpecialCharTok{==} \DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))}

\FunctionTok{table}\NormalTok{(crying}\SpecialCharTok{$}\NormalTok{stars)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  1  2  3  4  5 }
\CommentTok{\#\textgreater{}  6  2  4 14 79}
\end{Highlighting}
\end{Shaded}

In this example we are going to split everything into separate words. When we do this it is just searching for a space, and so what other types of elements are going to be considered `words'?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{crying\_by\_words }\OtherTok{\textless{}{-}} 
\NormalTok{  crying }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  tidytext}\SpecialCharTok{::}\FunctionTok{unnest\_tokens}\NormalTok{(word, review\_body, }\AttributeTok{token =} \StringTok{"words"}\NormalTok{)}

\FunctionTok{head}\NormalTok{(crying\_by\_words)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{}   stars five\_stars word }
\CommentTok{\#\textgreater{}   \textless{}int\textgreater{}      \textless{}dbl\textgreater{} \textless{}chr\textgreater{}}
\CommentTok{\#\textgreater{} 1     5          1 now  }
\CommentTok{\#\textgreater{} 2     5          1 i    }
\CommentTok{\#\textgreater{} 3     5          1 can  }
\CommentTok{\#\textgreater{} 4     5          1 cry  }
\CommentTok{\#\textgreater{} 5     5          1 all  }
\CommentTok{\#\textgreater{} 6     5          1 i}
\end{Highlighting}
\end{Shaded}

We now want to count the number of times each word is used by each of the star classifications.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{crying\_by\_words }\OtherTok{\textless{}{-}} 
\NormalTok{  crying\_by\_words }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(stars, word, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}

\FunctionTok{head}\NormalTok{(crying\_by\_words)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{}   stars word      n}
\CommentTok{\#\textgreater{}   \textless{}int\textgreater{} \textless{}chr\textgreater{} \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1     5 i       348}
\CommentTok{\#\textgreater{} 2     5 and     249}
\CommentTok{\#\textgreater{} 3     5 the     239}
\CommentTok{\#\textgreater{} 4     5 it      211}
\CommentTok{\#\textgreater{} 5     5 a       193}
\CommentTok{\#\textgreater{} 6     5 this    178}

\NormalTok{crying\_by\_words }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(stars }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{}   stars word      n}
\CommentTok{\#\textgreater{}   \textless{}int\textgreater{} \textless{}chr\textgreater{} \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1     1 the      39}
\CommentTok{\#\textgreater{} 2     1 i        24}
\CommentTok{\#\textgreater{} 3     1 and      21}
\CommentTok{\#\textgreater{} 4     1 it       21}
\CommentTok{\#\textgreater{} 5     1 to       19}
\CommentTok{\#\textgreater{} 6     1 my       16}
\end{Highlighting}
\end{Shaded}

So you can see that the most popular word for five-star reviews is `i', and that the most popular word for one star reviews is `the'.

At this point, we can use the data to do a whole bunch of different things, but one nice measure to look at is term frequency e.g.~in this case how many times is a word used in reviews with a particular star rating. The issue is that there are a lot of words that are commonly used regardless of context. As such, we may also like to look at the inverse document frequency in which we `penalise' words that occur in many particular star ratings. For instance, `the' probably occurs in both one star and five star reviews and so its idf is lower than `hate' which probably only occurs in one star reviews. The term frequency--inverse document frequency (tf-idf) is then the product of these.

We can create this value using the \texttt{bind\_tf\_idf()} function from the \texttt{tidytext} package, and this will create a bunch of new columns, one for each word and star combination.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This code, and the one in the next block, is from Julia Silge: https://juliasilge.com/blog/sherlock{-}holmes{-}stm/}
\NormalTok{crying\_by\_words\_tf\_idf }\OtherTok{\textless{}{-}} 
\NormalTok{  crying\_by\_words }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_tf\_idf}\NormalTok{(word, stars, n) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{tf\_idf)}

\FunctionTok{head}\NormalTok{(crying\_by\_words\_tf\_idf)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 6}
\CommentTok{\#\textgreater{}   stars word              n      tf   idf tf\_idf}
\CommentTok{\#\textgreater{}   \textless{}int\textgreater{} \textless{}chr\textgreater{}         \textless{}int\textgreater{}   \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1     2 below             1 0.00826  1.61 0.0133}
\CommentTok{\#\textgreater{} 2     2 boy               1 0.00826  1.61 0.0133}
\CommentTok{\#\textgreater{} 3     2 choice            1 0.00826  1.61 0.0133}
\CommentTok{\#\textgreater{} 4     2 contrary          1 0.00826  1.61 0.0133}
\CommentTok{\#\textgreater{} 5     2 exceptionally     1 0.00826  1.61 0.0133}
\CommentTok{\#\textgreater{} 6     2 migrates          1 0.00826  1.61 0.0133}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{crying\_by\_words\_tf\_idf }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(stars) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{top\_n}\NormalTok{(}\DecValTok{10}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  ungroup }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{word =} \FunctionTok{reorder\_within}\NormalTok{(word, tf\_idf, stars)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{stars =} \FunctionTok{as\_factor}\NormalTok{(stars)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(stars }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(word, tf\_idf, }\AttributeTok{fill =}\NormalTok{ stars)) }\SpecialCharTok{+}
    \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(stars), }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{scale\_x\_reordered}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Word"}\NormalTok{, }
         \AttributeTok{y =} \StringTok{"tf{-}idf"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{20-gather_files/figure-latex/unnamed-chunk-71-1.pdf}

\hypertarget{exercises-and-tutorial-7}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-7}}

\hypertarget{exercises-7}{%
\subsection{Exercises}\label{exercises-7}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In your own words, what is an API (write a paragraph or two)?
\item
  Find two APIs and discuss how you could use them to tell interesting stories (write a paragraph or two for each).
\item
  Find two APIs that have an R packages written around them. How could you use these to tell interesting stories? (Write a paragraph or two for each.)
\item
  What is the main argument to \texttt{httr::GET()} (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    `url'
  \item
    `website'
  \item
    `domain'
  \item
    `location'
  \end{enumerate}
\item
  Name three reasons why we should be respectful when getting scraping data from websites (write a paragraph or two).
\item
  What features of a website do we typically take advantage of when we parse the code (select all)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    HTML/CSS mark-up.
  \item
    Cookies.
  \item
    Facebook beacons.
  \item
    Code comments.
  \end{enumerate}
\item
  What are three advantages and three disadvantages of scraping compared with using an API (write a paragraph or two)?
\item
  What are three delimiters that could be useful when trying to bring order to the PDF that you read in as a character vector (write a paragraph or two)?
\item
  What do I need to put inside ``SOMETHING\_HERE'' if I want to match regular expressions for a full stop i.e.~``.'' (hint: see the `strings' cheat sheet) (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{.}
  \item
    \texttt{\textbackslash{}.}
  \item
    \texttt{\textbackslash{}\textbackslash{}.}
  \item
    \texttt{\textbackslash{}\textbackslash{}\textbackslash{}.}
  \end{enumerate}
\item
  Name three reasons for sketching out what you want before starting to try to extract data from a PDF (write a paragraph or two for each).
\item
  If you are interested in demographic data then what are three checks that you might like to do? What are three if you are interested in economic data such as GDP, interest rates, and exchange rates? (Write an explanation for each.)
\item
  What does the \texttt{purrr} package do (select all)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Enhances R's functional programming toolkit.
  \item
    Makes loops easier to code and read.
  \item
    Checks the consistency of datasets.
  \item
    Identifies issues in data structures and proposes replacements.
  \end{enumerate}
\item
  Which of these are functions from the \texttt{purrr} package (select all)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{map()}
  \item
    \texttt{walk()}
  \item
    \texttt{run()}
  \item
    \texttt{safely()}
  \end{enumerate}
\item
  Why should we use \texttt{safely()} when scraping data (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    To protect us from hackers.
  \item
    To avoid side effects of pages with issues.
  \item
    To slow down our scraping to an appropriate speed.
  \end{enumerate}
\item
  What are some principles to follow when scraping (select all)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Avoid it if possible
  \item
    Follow the site's guidance
  \item
    Slow down
  \item
    Use a scalpel not an axe.\\
  \end{enumerate}
\item
  What is a robots.txt file (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    The instructions that Frankenstein followed.
  \item
    Notes that web scrapers should follow when scraping.
  \end{enumerate}
\item
  What is the html tag for an item in list (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{li}
  \item
    \texttt{body}
  \item
    \texttt{b}
  \item
    \texttt{em}
  \end{enumerate}
\item
  If I have the following text data `rohan\_alexander' in a column called `names' and want to split it into first name and surname based on the underbar what function should I use (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{separate()}
  \item
    \texttt{slice()}
  \item
    \texttt{spacing()}
  \item
    \texttt{text\_to\_columns()}
  \end{enumerate}
\end{enumerate}

\hypertarget{tutorial-7}{%
\subsection{Tutorial}\label{tutorial-7}}

Gather some data yourself using a method that is introduced here - APIs directly or via a wrapper package, web scraping, PDF parsing, OCR, or text. Write a few paragraphs about the data source, what you gathered, and how you went about it. What took longer than you expected? When did it become fun? What would you do differently next time you do this? Please include a link to your GitHub repo so I can see the code, but it won't be strictly marked - this is more about encouraging you to have a go. (Start with something tiny and very specific, get that working, and then increase the scope - almost everything will be more difficult and time-consuming than you think - and don't forget to plan it out before you start.)

\hypertarget{hunt-data}{%
\chapter{Hunt data}\label{hunt-data}}

\textbf{STATUS: Under construction.}

\textbf{Required reading}

\begin{itemize}
\tightlist
\item
  Banerjee, Abhijit Vinayak, 2020, `Field Experiments and the Practice of Economics', \emph{American Economic Review}, Vol. 110, No.~7, pp.~1937-1951.
\item
  Berry, Donald, 1989, `Comment: Ethics and ECMO', \emph{Statistical Science}, Vol 4, No 4, pp.~306-310.
\item
  Duflo, Esther, 2020, `Field Experiments and the Practice of Policy', \emph{American Economic Review}, Vol. 110, No.~7, pp.~1952-1973 (or watch the speech detailed below).
\item
  Fisher, Ronald, 1935, \emph{The Design of Experiments}, pp.~20-29, \url{https://archive.org/details/in.ernet.dli.2015.502684/page/n33/mode/2up}.
\item
  Fry, Hanna, 2020, `Experiments on Trial', \emph{The New Yorker}, 2 March, pp.~61-65, \url{https://www.newyorker.com/magazine/2020/03/02/big-tech-is-testing-you}.
\item
  Gertler, Paul, Sebastian Martinez, Patrick Premand, Laura Rawlings, and Christel Vermeersch, \emph{Impact Evaluation in Practice}, Chapters 3 and 4, \url{https://www.worldbank.org/en/programs/sief-trust-fund/publication/impact-evaluation-in-practice}.
\item
  Hill, Austin Bradford, 1965, `The Environment and Disease: Association or Causation?', \emph{Proceedings of the Royal Society of Medicine}, 58, 5, 295-300.
\item
  Kohavi, Ron and Stefan Thomke, 2017, `The Surprising Power of Online Experiments', \emph{Harvard Business Review}, September-October, \url{https://hbr.org/2017/09/the-surprising-power-of-online-experiments}.
\item
  Kohavi, Ron, Diane Tang, and Ya Xu, 2020, \emph{Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing}, Cambridge University Press. (This sounds like a lot, but it's a light book - it's more about providing examples of issues to think about.) (Freely available through the U of T library.)
\item
  Taback, Nathan, 2020, \emph{Design of Experiments and Observational Studies}, Chapter 8 - Completely Randomized Designs: Comparing More Than Two Treatments, \url{https://scidesign.github.io/designbook/completely-randomized-designs-comparing-more-than-two-treatments.html}.
\item
  Taylor, Sean, Dean Eckles, 2017, `Randomized experiments to detect and estimate social influence in networks', \emph{arXiv}, \url{https://arxiv.org/abs/1709.09636v1}.
\item
  Ware, James H., 1989, `Investigating Therapies of Potentially Great Benefit: ECMO', \emph{Statistical Science}, Vol 4, No 4, pp.~298-306.
\item
  Wu, Changbao and Mary E. Thompson, 2020, \emph{Sampling Theory and Practice}, Springer, Chapters 1-3, and 5 (freely available through the U of T library).
\end{itemize}

\textbf{Required viewing}

\begin{itemize}
\tightlist
\item
  Ge, Kathy, 2021, `Experimentation and product design at Uber', \emph{Toronto Data Workshop}, 4 February, \url{https://youtu.be/UYzXElJTovg}.
\item
  Register, Yim, 2020, `Introduction to Sampling and Randomization', \emph{Online Causal Inference Seminar}, 14 November, \url{https://youtu.be/U272FFxG8LE}.
\item
  Xu, Ya, 2020, `Causal inference challenges in industry, a perspective from experiences at LinkedIn', \emph{Online Causal Inference Seminar}, 16 July, \url{https://youtu.be/OoKsLAvyIYA}.
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
  Angrist, Joshua D., and Jörn-Steffen Pischke, 2008, \emph{Mostly harmless econometrics: An empiricist's companion}, Princeton University Press, Chapter 2.
\item
  Banerjee, Abhijit Vinayak, Esther Duflo, Rachel Glennerster, and Dhruva Kothari, 2010, `Improving immunisation coverage in rural India: clustered randomised controlled evaluation of immunisation campaigns with and without incentives', \emph{BMJ}, 340, c2220.
\item
  Beaumont, Jean-François, 2020, `Are probability surveys bound to disappear for the production of official statistics?', \emph{Survey Methodology}, 46 (1), Statistics Canada, Catalogue No.~12-001-X.
\item
  Christian, Brian, 2012, `The A/B Test: Inside the Technology That's Changing the Rules of Business', \emph{Wired}, 25 April, \url{https://www.wired.com/2012/04/ff-abtesting/}.
\item
  Dablander, Fabian, 2020, ``An Introduction to Causal Inference'', \emph{PsyArXiv}, 13 February, \url{doi:10.31234/osf.io/b3fkw}, \url{https://psyarxiv.com/b3fkw}.
\item
  Deaton, Angus, 2010, `Instruments, Randomization, and Learning about Development', \emph{Journal of Economic Literature}, vol.~48, no. 2, pp.~424-455.
\item
  Duflo, Esther, Rachel Glennerster, and Michael Kremer, 2007, `Using Randomization In Development Economics Research: A Toolkit', \url{https://economics.mit.edu/files/806}.
\item
  Gordon, Brett R., Florian Zettelmeyer, Neha Bhargava, and Dan Chapsky, 2019, `A Comparison of Approaches to Advertising Measurement: Evidence from Big Field Experiments at Facebook', \emph{Marketing Science}, Vol. 38, No.~2, March--April, pp.~193--225.
\item
  Groves, Robert M., 2011, `Three Eras of Survey Research', \emph{Public Opinion Quarterly}, 75 (5), pp.~861--871, \url{https://doi.org/10.1093/poq/nfr057}.
\item
  Hillygus, D. Sunshine, 2011, `The evolution of election polling in the United States', \emph{Public Opinion Quarterly}, 75 (5), pp.~962-981.
\item
  Imai, Kosuke, 2017, \emph{Quantitative Social Science: An Introduction}, Princeton University Press, Ch 2.3, 2.4, 4.3.
\item
  Jeffries, Adrianne, Leon Yin, and Surya Mattu, 2020, `Swinging the Vote?', \emph{The Markup}, 26 February, \url{https://themarkup.org/google-the-giant/2020/02/26/wheres-my-email}.
\item
  Kohavi, Ron, Alex Deng, Brian Frasca, Roger Longbotham, Toby Walker, and Ya Xu. 2012. Trustworthy online controlled experiments: five puzzling outcomes explained. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD '12). Association for Computing Machinery, New York, NY, USA, 786--794. \url{DOI:https://doi.org/10.1145/2339530.2339653}
\item
  Landesberg, Eddie, Molly Davies, and Stephanie Yee, 2019, `Want to make good business decisions? Learn causality', \emph{MultiThreaded, Stitchfix blog}, 19 December, \url{https://multithreaded.stitchfix.com/blog/2019/12/19/good-marketing-decisions/}.
\item
  Levay, Kevin E., Jeremy Freese, and James N. Druckman, 2016, `The demographic and political composition of Mechanical Turk samples', \emph{Sage Open}, 6 (1), 2158244016636433.
\item
  Lewis, Randall A., and David H. Reiley, 2014 `Online ads and offline sales: Measuring the effects of retail advertising via a controlled experiment on Yahoo!', \emph{Quantitative Marketing and Economics}, Vol 12, pp.~235--266.
\item
  Mullinix, Kevin J., Leeper, Thomas J., Druckman, James N. and Freese, Jeremy, 2015, `The generalizability of survey experiments', \emph{Journal of Experimental Political Science}, 2 (2), pp.~109-138.
\item
  Novak, Greg, Sven Schmit, and Dave Spiegel, 2020, Experimentation with resource constraints, 18 November, StitchFix Blog, \url{https://multithreaded.stitchfix.com/blog/2020/11/18/virtual-warehouse/}.
\item
  Prepared for the AAPOR Executive Council by a Task Force operating under the auspices of the AAPOR Standards Committee, with members including:, Reg Baker, Stephen J. Blumberg, J. Michael Brick, Mick P. Couper, Melanie Courtright, J. Michael Dennis, Don Dillman, Martin R. Frankel, Philip Garland, Robert M. Groves, Courtney Kennedy, Jon Krosnick, Paul J. Lavrakas, Sunghee Lee, Michael Link, Linda Piekarski, Kumar Rao, Randall K. Thomas, Dan Zahs, 2010, `Research Synthesis: AAPOR Report on Online Panels', \emph{Public Opinion Quarterly}, 74 (4), pp.~711--781, \url{https://doi.org/10.1093/poq/nfq048}.
\item
  Ryan, A. C., A. R. MacKenzie, S. Watkins, and R. Timmis, 2012, `World War II contrails: a case study of aviation‐induced cloudiness', \emph{International journal of climatology}, 32, no. 11, pp.~1745-1753.
\item
  Said, Chris, 2020, `Optimizing sample sizes in A/B testing, Part I: General summary', 10 January, \url{https://chris-said.io/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-I/}. (See also parts 2 and 3).
\item
  Stolberg, Michael, 2006, `Inventing the randomized double-blind trial: the Nuremberg salt test of 1835', \emph{Journal of the Royal Society of Medicine}, 99, no. 12, pp.~642-643.
\item
  Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel, 2019, popular science background, \url{https://www.nobelprize.org/uploads/2019/10/popular-economicsciencesprize2019-2.pdf}.
\item
  Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel, 2019, scientific background, \url{https://www.nobelprize.org/uploads/2019/10/advanced-economicsciencesprize2019.pdf}.
\item
  Taddy, Matt, 2019, \emph{Business Data Science}, Chapter 5.
\item
  Urban, Steve, Rangarajan Sreenivasan, and Vineet Kannan, 2016, `It's All A/Bout Testing: The Netflix Experimentation Platform', \emph{Netflix Technology Blog}, 29 April, \url{https://netflixtechblog.com/its-all-a-bout-testing-the-netflix-experimentation-platform-4e1ca458c15}.
\item
  VWO, `A/B Testing Guide', \url{https://vwo.com/ab-testing/}.
\item
  Yeager, David S., Jon A. Krosnick, LinChiat Chang, Harold S. Javitz, Matthew S. Levendusky, Alberto Simpser, Rui Wang, 2011, `Comparing the Accuracy of RDD Telephone Surveys and Internet Surveys Conducted with Probability and Non-Probability Samples', \emph{Public Opinion Quarterly}, 75 (4), pp.~709--747, \url{https://doi.org/10.1093/poq/nfr020}.
\item
  Yin, Xuan and Ercan Yildiz, 2020, `The Causal Analysis of Cannibalization in Online Products', \emph{Code as Craft, Etsy blog}, 24 February, \url{https://codeascraft.com/2020/02/24/the-causal-analysis-of-cannibalization-in-online-products/}.
\end{itemize}

\textbf{Recommended listening}

\begin{itemize}
\tightlist
\item
  Galef, Julia, 2020, `Episode 246: Deaths of despair / Effective altruism (Angus Deaton)', \emph{Rationally Speaking}, from 35:30 through to the end, available at: \url{http://rationallyspeakingpodcast.org/show/episode-246-deaths-of-despair-effective-altruism-angus-deato.html}.
\end{itemize}

\textbf{Recommended viewing}

\begin{itemize}
\tightlist
\item
  Duflo, Esther, 2020, `Inteview with Esther Duflo', 12 October, \emph{Online Causal Inference Seminar}, \url{https://youtu.be/WWW9q3oMYxU}.
\item
  Duflo, Esther, 2019, `Nobel Prize Lecture', 8 December 2019, Stockholm: \url{https://www.nobelprize.org/prizes/economic-sciences/2019/duflo/lecture/}.
\item
  Tipton, Elizabeth, 2020, `Will this Intervention Work in this Population? Designing Randomized Trials for Generalization', \emph{Online Causal Inference Seminar}, 14 April, \url{https://youtu.be/HYP32wzEZMA}.
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\tightlist
\item
  Treatment and control groups.
\item
  Internal and external validity.
\item
  Average treatment effect.
\item
  Generating simulated datasets.
\item
  Defining populations, frames and samples.
\item
  Distinguishing probability and non-probability sampling
\item
  Distinguishing strata and clusters.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{broom}
\item
  \texttt{ggplot2}
\item
  \texttt{tidyverse}
\end{itemize}

\textbf{Key functions/etc}

\begin{itemize}
\tightlist
\item
  \texttt{aov()}
\item
  \texttt{rnorm()}
\item
  \texttt{sample()}
\item
  \texttt{t.test()}
\end{itemize}

\hypertarget{experiments-and-randomised-controlled-trials}{%
\section{Experiments and randomised controlled trials}\label{experiments-and-randomised-controlled-trials}}

\hypertarget{introduction-11}{%
\subsection{Introduction}\label{introduction-11}}

\emph{First a note on Ronald Fisher and Francis Galton. Fisher and Galton are the intellectual grandfathers of much of the work that we cover. In some cases it is directly their work, in other cases it is work that built on their contributions. Both of these men believed in eugenics, amongst other things that are generally reprehensible.}

This chapter is about experiments. This is a situation in which we can explicitly control and vary some aspects. The advantage of this is that identification should be clear. There is a treatment group that is treated and a control group that is not. These are randomly split. And so if they end up different then it must be because of the treatment. Unfortunately, life is rarely so smooth. Arguing about how similar the treatment and control groups were tends to carry on indefinitely, because our ability to speak to internal validity affects our ability to speak to external validity.

It's also important to note that the statistics of this were designed in agricultural settings `does fertilizer work?', etc. In those settings you can more easily divide a field into `treated' and `non-treated', and the magnitude of the effect is large. In general, these same statistical approaches are still used today (especially in the social sciences) but often inappropriately. If you hear someone talking about `having enough power' and similar phrases, then it's \emph{not} necessarily that they're \emph{not} right, but it usually pays to take a step back and really think about what is being done and whether they really know what they're doing.

\hypertarget{motivation-and-notation}{%
\subsection{Motivation and notation}\label{motivation-and-notation}}

\begin{quote}
Never forget: if your sampling is in any way non-representative, your observe{[}d{]} data is not sufficient for population estimates. You \emph{must} deal with design, sampling issues, data quality, and misclassification. Otherwise you'll just be wrong.

Dan Simpson, \href{https://twitter.com/dan_p_simpson/status/1222924987667046400}{30 January 2020}.
\end{quote}

When Monica and I moved to San Francisco, the Giants immediately won the baseball, and the Warriors began a historic streak. We moved to Chicago and the Cubs won the baseball for the first time in a hundred years. We then moved to Massachusetts, and the Patriots won the Super Bowl again and again and again. Finally, we moved to Toronto, and the Raptors won the basketball. Should a city pay us to live there or could their funds be better spent elsewhere?

One way to get at the answer would be to run an experiment. Make a list of the North American cities with major sports teams, and then roll a dice and send us to live there for a year. If we had enough lifetimes, then we could work it out. The fundamental issue is that we cannot both live in a city and not live in a city. Experiments and randomised controlled trials are circumstances in which we try to randomly allocate some treatment, so as to have a belief that everything else was constant (or at least ignorable).

In the words of \citet[p.~3]{hernanrobins2020} an action, \(A\), is also known `as an intervention, an exposure, or a treatment.' I'll typically use `treated/control' language, reflecting whether an action was imposed or not. That treatment random variable will typically be binary, that is 0 or 1, `treated' or `not treated/control/comparison'. We'll then typically have some outcome random variable, \(Y\), which will typically be binary, able to be made binary, or continuous, although we'll touch on other options. An example of a binary outcome could be vote choice - `Conservative' vs `Not Conservative' - noticing there that I grouped all the other parties into simply `Not Conservative' to force the binary outcome.

Further following \citet[p.~4]{hernanrobins2020}, but in the notation of \citet[p.~48]{gertler2016impact} we describe a treatment as `causal' when \((Y|a=0)\neq (Y|a=1)\). As discussed above, the fundamental problem of causal inference is that we cannot both treat and control the one individual. So when we want to know the effect of the treatment, we need to compare it with the counterfactual, which is what would have happened if the individual were not treated. So causal inference turns out to be fundamentally a missing data problem.\footnote{There's a joke in statistics, okay, well, TBH, I have a joke about statistics, and it's that at some point every professor is like `\ldots{} and so X really just boils down to a missing data problem' and it's funny because, that's kind of the fundamental issue of statistics, we'd not really need the science if we had all the data. In hindsight, this is not really a joke, but I'm a father now so I'll just lean into it.}

To quote from \citet[p.48]{gertler2016impact}, in the context of evaluating income in response to an intervention program:

\begin{quote}
To put it another way, we would like to measure income at the same point in time for the same unit of observation (a person, in this case), but in two different states of the world. If it were possible to do this, we would be observing how much income the same individual would have had at the same point in time both with and without the program, so that the only possible explanation for any difference in that person's income would be the program. By comparing the same individual with herself at the same moment, we would have managed to eliminate any outside factors that might also have explained the difference in outcomes. We could then be confident that the relationship between the vocational training program and the change in income is causal\ldots{} {[}A{]} unit either participated in the program or did not participate. The unit cannot be observed simultaneously in two different states (in other words, with and without the program).
\end{quote}

As we cannot compared treatment and control in one particular individual, we instead compare the average of two groups - all those treated and all those not. We are looking to estimate the counterfactual. We usually consider a default that there's no effect and we require evidence for us to change our mind. As we're interested in what is happening in groups, we turn to expectations, and notions of probability to express ourselves. Hence, we'll make claims that talk, on average. Maybe wearing fun socks really does make you have a lucky day, but on average across the population, it's probably not the case.\footnote{As someone who oddly is somewhat superstitious, believes fully in the irony gods, and does have a pair of lucky, fun, socks, this example was not randomly chosen.}

It's worth pointing out that we don't just have to be interested in the average effect. We may consider the median, or variance, or whatever. Nonetheless, if we were interested in the average effect, then one way to proceed would be to divide the dataset into two - treated and not treated - have an effect column of 0s and 1s, sum the column and divide it by the length of the column, and then look at the ratio. This would be an estimator, which is a way of putting together a guess of something of interest. The estimand is the thing of interest, in this case the average effect, and the estimate is whatever our guess turns out to be. To give another example, following \citet{gelmanhillvehtari2020}:

\begin{quote}
An estimand, or quantity of interest, is some summary of parameters or data that somebody is interested in estimating. For example, in the regression model, \(y = a + bx + \epsilon\), the parameters \(a\) and \(b\) might be of interest\ldots. We use the data to construct estimates of parameters and other quantities of interest.
\end{quote}

More broadly, \citet{Cunningham2021} defines causal inference as `\ldots the leveraging of theory and deep knowledge of institutional details to estimate the impact of events and choices on a given outcome of interest.' In the previous chapter we discussed gathering data which we observed about the world. In this chapter we are going to be more active. \citet{Cunningham2021} says that experimental data `is collected in something akin to a laboratory environment. In a traditional experiment, the researcher participates actively in the process being recorded.' That is, if we want to use this data then as researchers we have to go out and hunt it, if you like.

\hypertarget{randomised-sampling}{%
\subsection{Randomised sampling}\label{randomised-sampling}}

Correlation can be enough in some settings, but in order to be able to make forecasts when things change and the circumstances are slightly different we need to understand causation. The key is the counterfactual - what would have happened in the absence of the treatment. Ideally we could keep everything else constant, randomly divide the world into two groups, and then treat one and not the other. Then we can be pretty confident that any difference between the two groups is due to that treatment. The reason for this is that if we have some population and we randomly select two groups from it, then our two groups (so long as they are both big enough) should have the same characteristics as the population. Randomised controlled trials (RCTs) and A/B testing attempts to get us as close to this `gold standard' as we can hope. RCTs are often described as the `gold standard', for instance by \citet{athey2017state}. In doing so, we're not saying that RCTs are perfect, just that they're generally better than most of the other options. There is plenty that is wrong with RCTs.

Remember that our challenge is \citep[p.51-52]{gertler2016impact}:

\begin{quote}
\ldots to identify a treatment group and a comparison group that are statistically identical, on average, in the absence of the program. If the two groups are identical, with the sole exception that one group participates in the program and the other does not, then we can be sure that any difference in outcomes must be due to the program. Finding such comparison groups is the crux of any impact evaluation, regardless of what type of program is being evaluated. Simply put, without a comparison group that yields an accurate estimate of the counterfactual, the true impact of a program cannot be established.
\end{quote}

We might be worried about underlying trends (the issues with before/after comparison), or selection bias (the issue with self-selection), either of which would result in biased estimators. Our solution is randomisation.

To get started, let's generate a simulated dataset and then sample from it. In general, this is a good way to approach problems:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  generate a simulated dataset;
\item
  do your analysis on the simulated dataset; and
\item
  take your analysis to the real dataset.
\end{enumerate}

The reason this is a good approach is that you know roughly what the outcomes should be in step 2, whereas if you go directly to the real dataset then you don't know if unexpected outcomes are likely due to your own analysis errors, or actual results. The first time you generate a simulated dataset it will take a while, but after a bit of practice you'll get good at it. There are also packages that can help, including \texttt{DeclareDesign} \citep{citedeclaredesign} and \texttt{survey} \citep{citesurvey}. Another good reason it's useful to take this approach of simulation is that when you're working in teams the analysis can get started before the data collection and cleaning is completed. That simulation will also help the collection and cleaning team think about tests they should run on their data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}
\CommentTok{\# Construct a population so that 25 per cent of people like blue and 75 per cent }
\CommentTok{\# like white.}
\NormalTok{population }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{person =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10000}\NormalTok{),}
         \AttributeTok{favourite\_color =} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\StringTok{"Blue"}\NormalTok{, }\StringTok{"White"}\NormalTok{), }
                                  \AttributeTok{size  =} \DecValTok{10000}\NormalTok{, }
                                  \AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{,}
                                  \AttributeTok{prob =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.75}\NormalTok{)),}
         \AttributeTok{supports\_the\_leafs =} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{), }
                                  \AttributeTok{size  =} \DecValTok{10000}\NormalTok{, }
                                  \AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{,}
                                  \AttributeTok{prob =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.80}\NormalTok{, }\FloatTok{0.20}\NormalTok{)),}
\NormalTok{         ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{in\_frame =} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{1}\NormalTok{),}
                        \AttributeTok{size  =} \DecValTok{10000}\NormalTok{, }
                        \AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{group =} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{),}
                        \AttributeTok{size  =} \DecValTok{10000}\NormalTok{, }
                        \AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{group =} \FunctionTok{ifelse}\NormalTok{(in\_frame }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, group, }\ConstantTok{NA}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

We'll get more into this terminology later, but the sampling frame is subset of the population that can actually be sampled, for instance they are listed somewhere. For instance, \href{https://jazzystats.com/}{Lauren Kennedy} likes to use the analogy of a city's population, and the phonebook - almost everyone is in there (or at least they used to be), so the population and the sampling frame are almost the same, but they are not.

Now look at the mean for two groups drawn out of the sampling frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{population }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(in\_frame }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(group }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(group, favourite\_color) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 4 x 3}
\CommentTok{\#\textgreater{} \# Groups:   group, favourite\_color [4]}
\CommentTok{\#\textgreater{}   group favourite\_color     n}
\CommentTok{\#\textgreater{}   \textless{}int\textgreater{} \textless{}chr\textgreater{}           \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1     1 Blue              114}
\CommentTok{\#\textgreater{} 2     1 White             420}
\CommentTok{\#\textgreater{} 3     2 Blue              105}
\CommentTok{\#\textgreater{} 4     2 White             369}
\end{Highlighting}
\end{Shaded}

We are probably convinced by looking at it, but to formally test if there is a difference in the two samples, we can use a t-test.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(broom)}

\NormalTok{population }\OtherTok{\textless{}{-}} 
\NormalTok{  population }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{color\_as\_integer =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{    favourite\_color }\SpecialCharTok{==} \StringTok{"White"} \SpecialCharTok{\textasciitilde{}} \DecValTok{0}\NormalTok{,}
\NormalTok{    favourite\_color }\SpecialCharTok{==} \StringTok{"Blue"} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{,}
    \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \DecValTok{999}
\NormalTok{  ))}

\NormalTok{group\_1 }\OtherTok{\textless{}{-}} 
\NormalTok{  population }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(group }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(color\_as\_integer) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as.vector}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{unlist}\NormalTok{()}

\NormalTok{group\_2 }\OtherTok{\textless{}{-}} 
\NormalTok{  population }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(group }\SpecialCharTok{==} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(color\_as\_integer) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{unlist}\NormalTok{()}

\FunctionTok{t.test}\NormalTok{(group\_1, group\_2)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Welch Two Sample t{-}test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  group\_1 and group\_2}
\CommentTok{\#\textgreater{} t = {-}0.30825, df = 988.57, p{-}value = 0.758}
\CommentTok{\#\textgreater{} alternative hypothesis: true difference in means is not equal to 0}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}  {-}0.05919338  0.04312170}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{} mean of x mean of y }
\CommentTok{\#\textgreater{} 0.2134831 0.2215190}

\CommentTok{\# We could also use the tidy function in the broom package.}
\FunctionTok{tidy}\NormalTok{(}\FunctionTok{t.test}\NormalTok{(group\_1, group\_2))}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 10}
\CommentTok{\#\textgreater{}   estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high}
\CommentTok{\#\textgreater{}      \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 {-}0.00804     0.213     0.222    {-}0.308   0.758      989.  {-}0.0592    0.0431}
\CommentTok{\#\textgreater{} \# ... with 2 more variables: method \textless{}chr\textgreater{}, alternative \textless{}chr\textgreater{}}
\end{Highlighting}
\end{Shaded}

If properly done then not only will we get a `representative' share of people with the favourite color blue, but we should also get a representative share of people who support the Maple Leafs. Why should that happen when we haven't randomised on these variables? Let's start by looking at our dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{population }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(in\_frame }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(group }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(group, supports\_the\_leafs) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 4 x 3}
\CommentTok{\#\textgreater{} \# Groups:   group, supports\_the\_leafs [4]}
\CommentTok{\#\textgreater{}   group supports\_the\_leafs     n}
\CommentTok{\#\textgreater{}   \textless{}int\textgreater{} \textless{}chr\textgreater{}              \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1     1 No                   102}
\CommentTok{\#\textgreater{} 2     1 Yes                  432}
\CommentTok{\#\textgreater{} 3     2 No                    81}
\CommentTok{\#\textgreater{} 4     2 Yes                  393}
\end{Highlighting}
\end{Shaded}

This is very exciting. We have a representative share on `unobservables' (in this case we do `observe' them - to illustrate the point - but we didn't select on them). We get this because they were correlated. But it will breakdown in a number of ways that we will discuss. It also assumes large enough groups - if we sampled in Toronto are we likely to get a `representative' share of people who support the Canadiens? What about \href{https://www.fc-hansa.de/}{F.C. Hansa Rostock}? If we want to check that the two groups are the same then what can we do? Exactly what we did above - just check if we can identify a difference between the two groups based on observables (we looked at the mean, but we could look at other aspects as well).

\hypertarget{anova}{%
\subsection{ANOVA}\label{anova}}

\begin{quote}
`I refuse to teach anova.'

Statistics professor who prefers to remain anonymous.
\end{quote}

Analysis of Variation (ANOVA) was introduced by Fisher while he was working on statistical problems in agriculture. To steal \href{https://darrendahly.github.io}{Darren L Dahly's} `favorite joke of all time' \citep{citeDahly}:

\begin{quote}
Q: ``What's the difference between agricultural and medical research?''

A: ``The former isn't conducted by farmers.''
\end{quote}

We need to cover ANOVA because of its importance historically, but in general you probably shouldn't actually use ANOVA day-to-day. There's nothing wrong with it, in the right circumstances, it's more just that it is a hundred years old and the number of modern use-case where it's still your best-bet is pretty small. In any case, typically, the null is that all of the groups are from the same distribution.

We can run ANOVA with the function built into R - \texttt{aov()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{just\_two\_groups }\OtherTok{\textless{}{-}}\NormalTok{ population }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(in\_frame }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(group }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}

\FunctionTok{aov}\NormalTok{(group }\SpecialCharTok{\textasciitilde{}}\NormalTok{ favourite\_color, }
    \AttributeTok{data =}\NormalTok{ just\_two\_groups) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{tidy}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 2 x 6}
\CommentTok{\#\textgreater{}   term               df    sumsq meansq statistic p.value}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}           \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 favourite\_color     1   0.0238 0.0238    0.0952   0.758}
\CommentTok{\#\textgreater{} 2 Residuals        1006 251.     0.250    NA       NA}
\end{Highlighting}
\end{Shaded}

In this case, we fail to reject the null that the samples are the same. This all said, it's just linear regression. So I'm not sure why it got a fancy name.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(group }\SpecialCharTok{\textasciitilde{}}\NormalTok{ favourite\_color, }
    \AttributeTok{data =}\NormalTok{ just\_two\_groups) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{tidy}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 2 x 5}
\CommentTok{\#\textgreater{}   term                 estimate std.error statistic   p.value}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}                   \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 (Intercept)            1.48      0.0338    43.8   1.67e{-}235}
\CommentTok{\#\textgreater{} 2 favourite\_colorWhite  {-}0.0118    0.0382    {-}0.308 7.58e{-}  1}
\end{Highlighting}
\end{Shaded}

My favourite discussion of ANOVA is \citet[Chapter 8]{taback2020}.

\hypertarget{treatment-and-control}{%
\subsection{Treatment and control}\label{treatment-and-control}}

If the treated and control groups are the same in all ways and remain that way, then we have internal validity, which is to say that our control will work as a counterfactual and our results can speak to a difference between these groups in that study.

In the words of \citet[p.~71]{gertler2016impact}:

\begin{quote}
Internal validity means that the estimated impact of the program is net of all other potential confounding factors---or, in other words, that the comparison group provides an accurate estimate of the counterfactual, so that we are estimating the true impact of the program.
\end{quote}

If the group to which we applied our randomisation were representative of the broader population, and the experimental set-up were fairly similar to outside conditions, then we further have external validity. That means that the difference that we find does not just apply in our own experiment, but also in the broader population.

Again, in the words of \citet[p.~73]{gertler2016impact}:

\begin{quote}
External validity means that the evaluation sample accurately represents the population of eligible units. The results of the evaluation can then be generalized to the population of eligible units. We use random sampling to ensure that the evaluation sample accurately reflects the population of eligible units so that impacts identified in the evaluation sample can be extrapolated to the population.
\end{quote}

But this means we need randomisation twice. How does this trade-off happen and to what extent does it matter?

As such, we are interested in the effect of being `treated'. This may be that we charge different prices (continuous treatment variable), or that we compare different colours on a website (discrete treatment variable, and a staple of A/B testing). If we consider just discrete treatments (so that we can use dummy variables) then need to make sure that all of the groups are otherwise the same. How can we do this? One way is to ignore the treatment variable and to examine all other variables - can you detect a difference between the groups based on any other variables? In the website example, are there a similar number of:

\begin{itemize}
\tightlist
\item
  PC/Mac users?
\item
  Safari/Chrome/Firefox/other users?
\item
  Mobile/desktop users?
\item
  Users from certain locations?
\end{itemize}

These are all threats to the validity of our claims.

But if done properly, that is if the treatment is truly independent, then we can estimate an `average treatment effect', which in a binary treatment variable setting is:
\[\mbox{ATE} = \mbox{E}[y|d=1] - \mbox{E}[y|d=0].\]

That is, the difference between the treated group, \(d = 1\), and the control group, \(d = 0\), when measured by the expected value of some outcome variable, \(y\). So the mean causal effect is simply the difference between the two expectations!

Let's again get stuck into some code. First we need to generate some data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}
\NormalTok{example\_data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{person =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{1000}\NormalTok{),}
                       \AttributeTok{treatment =} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{1}\NormalTok{, }\AttributeTok{size  =} \DecValTok{1000}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{                       )}
\CommentTok{\# We want to make the outcome slightly more likely if they were treated than if not.}
\NormalTok{example\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  example\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rowwise}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{outcome =} \FunctionTok{if\_else}\NormalTok{(treatment }\SpecialCharTok{==} \DecValTok{0}\NormalTok{, }
                           \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{mean =} \DecValTok{5}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
                           \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{mean =} \DecValTok{6}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}
\NormalTok{                           )}
\NormalTok{         )}

\NormalTok{example\_data}\SpecialCharTok{$}\NormalTok{treatment }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(example\_data}\SpecialCharTok{$}\NormalTok{treatment)}

\NormalTok{example\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ outcome, }
             \AttributeTok{fill =}\NormalTok{ treatment)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{,}
                 \AttributeTok{binwidth =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Outcome"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of people"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Person was treated"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{21-hunt_files/figure-latex/unnamed-chunk-8-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{example\_regression }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(outcome }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treatment, }\AttributeTok{data =}\NormalTok{ example\_data)}

\FunctionTok{tidy}\NormalTok{(example\_regression)}
\CommentTok{\#\textgreater{} \# A tibble: 2 x 5}
\CommentTok{\#\textgreater{}   term        estimate std.error statistic  p.value}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}          \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 (Intercept)     5.00    0.0430     116.  0       }
\CommentTok{\#\textgreater{} 2 treatment1      1.01    0.0625      16.1 5.14e{-}52}
\end{Highlighting}
\end{Shaded}

But then reality happens. Your experiment cannot run for too long otherwise people may be treated many times, or become inured to the treatment, but it cannot be too short otherwise you can't measure longer term outcomes. You cannot have a `representative' sample on every cross-tab, but if not then the treatment and control will be different. Practical difficulties may make it difficult to follow up with certain groups.

Questions to ask (if they haven't been answered already) include:

\begin{itemize}
\tightlist
\item
  How are the participants being selected into the frame for consideration?
\item
  How are they being selected for treatment? We would hope this is a lottery, but this term is applied to a variety of situations. Additionally, early `success' can lead to pressure to treat everyone.
\item
  How is treatment being assessed?
\item
  To what extent is random allocation ethical and fair? Some argue that shortages mean it is reasonable to randomly allocate, but that may depend on how linear the benefits are. It may also be difficult to establish boundaries. If we only want to include people in Ontario then that may be clear, but what about `students' in Ontario - who is a student, and who is making the decision?
\end{itemize}

Bias and other issues are not the end of the world. But you need to think about it carefully. In the famous example, Abraham Wald was given data on the planes that came back to Britain after being shot at in WW2. The question is where to place the armour. One option is to place it over the bullet holes. Wald recognised that there is a selection effect here - these are the planes that made it back - they didn't need the armour, but instead we should put the armour where there were no bullet holes.

To consider an example that may be closer to home - how would the results of a survey differ if I only asked students who completed this course what was difficult about it and not those who dropped out? While, as Dan suggests, we should work to try to make the dataset as good as possible, it may be possible to use the model to control for some of the bias. If there is a variable that is correlated with say, attrition, then we can add it to the model. Either by itself, or as an interaction.

What if there is a correlation between the individuals? For instance, what if there were some `hidden variable' that we didn't know about, such as province, and it turned out that people from the same province were similar? In that case we could use `wider' standard errors.

But a better way to deal with this may be to change the experiment. For instance, we discussed stratified sampling - perhaps we should stratify by province? How would we implement this? And of course, these days we'd not really use a 100-year-old method but would instead use Bayes-based approaches.

\hypertarget{case-study---fishers-tea-party}{%
\section{Case study - Fisher's tea party}\label{case-study---fishers-tea-party}}

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/drinking_tea} \caption{Afternoon Tea Party (1890–1891), by Mary Cassatt (American, 1844-1926), as downloaded from https://artvee.com/dl/afternoon-tea-party.}\label{fig:ladiesdrinkingtea}
\end{figure}

Fisher (see note above) introduced a, now, famous example of an experiment designed to see if a person can distinguish between a cup of tea when the milk was added first, or last.\footnote{I'm personally very attached to this example as this issue also matters a lot to my father}

From \citet[p.13]{fisherdesignofexperiments}:

\begin{quote}
A lady declares that by tasting a cup of tea made with milk she can discriminate whether the milk or the tea infusion was first added to the cup. We will consider the problem of designing an experiment by means of which this assertion can be tested.
\end{quote}

Fisher continues:

\begin{quote}
Our experiment consists in mixing eight cups of tea, four in one way and four in the other, and presenting them to the subject for judgment in a random order. The subject has been told in advance of what the test will consist, namely that she will be asked to taste eight cups, that these shall be four of each kind, and that they shall be presented to her in a random order, that is in an order not determined arbitrarily by human choice, but by the actual manipulation of the physical apparatus used in games of chance, cards, dice, roulettes, etc., or, more expeditiously, from a published collection of random sampling-numbers purporting to give the actual results of such manipulation. Her task is to divide the 8 cups into two sets of 4, agreeing, if possible, with the treatments received.
\end{quote}

To summarize, the set-up is:

\begin{itemize}
\tightlist
\item
  Eight randomly ordered cups of tea.
\item
  Four had tea put in first.
\item
  Four had milk put in first.
\item
  The person has to choose the four that are the same.
\item
  The person knows it's an experiment.
\end{itemize}

We'll now try this experiment. So brew some tea, grab eight cups, and pour eight cups of tea for a friend that you're isolating with\footnote{For posteriority, 2020 was quite a year.} - four where you put the milk in first and four where you put the milk in last. Make sure you use the same amount of tea and milk in each! Don't forget to randomise the order, possibly even using the following code:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sample}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{), }\AttributeTok{size =} \DecValTok{8}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 3 7 6 4 1 8 2 5}
\end{Highlighting}
\end{Shaded}

Then have your friend guess which four you put milk in first and which four you put milk in last!

To decide if the person's choices were likely to have occurred at random or not, we need to think about the probability of this happening by chance. First count the number of successes out of the four that were chosen. \citet[p.14]{fisherdesignofexperiments} claims there are: \({8 \choose 4} = \frac{8!}{4!(8-4)!}=70\) possible outcomes.

By chance, there are two ways for the person to be perfectly correct (because we are only asking them to be grouped): correctly identify all the ones that were milk-first (one outcome out of 70) or correctly identify all the ones that were tea-first (one outcome out of 70), so the chance of that is \(2/70 \approx 0.028\). Now, as \citet[p.15]{fisherdesignofexperiments} says,

\begin{quote}
`{[}i{]}t is open to the experimenter to be more or less exacting in respect of the smallness of the probability he would require before he would be willing to admit that his observations have demonstrated a positive result'.
\end{quote}

You need to decide what evidence it takes for you to be convinced. If there's no possible evidence that will dissuade you from your view (that there is no difference between milk-first and tea-first) then what is the point of doing an experiment? In any case, if the null is that they can't distinguish, but they correctly separate them all, then at the five-per-cent level, we reject the null.

What if they miss one? Similarly, by chance there are 16 ways for a person to be `off-by-one'. Either they think there was one that was milk-first when it was tea-first - there are, \({4 \choose 1}\), four ways this could happen - or they think there was one that was tea-first when it was milk-first - again, there are, \({4 \choose 1}\), four ways this could happen. But these outcomes are independent, so the probability is \(\frac{4\times 4}{70} \approx 0.228\). And so on. So, we fail to reject the null.

Finally, an aside on this magical `5 per cent'. Fisher himself describes this as merely `usual and convenient' \citep[ p.15]{fisherdesignofexperiments}. \citet[p.16]{fisherdesignofexperiments} continues:

\begin{quote}
In order to assert that a natural phenomenon is experimentally demonstrable we need, not an isolated record, but a reliable method of procedure. In relation to the test of significance, we may say that a phenomenon is experimentally demonstrable when we know how to conduct an experiment which will rarely fail to give us a statistically significant result.
\end{quote}

At the start of these notes, I said that Fisher held views that we would consider reprehensible today. My guess is, were he around today, he would think our use of p-values as discrediting. Do not just go searching for meaning in constellations of stars. Thoroughly interrogate your data and think precisely about the statistical methods you are applying. For conclusions that you want to hold up in the long-run, aim to use as simple, and as understandable, statistical methods as you can. Ensure that you can explain and justify your statistical decisions without recourse to astrology.

\includegraphics[width=0.6\linewidth]{/Users/rohanalexander/Documents/book/figures/significant}
Source: \url{https://xkcd.com/882/}

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/wisdom_over_fortune} \caption{'The triumph of wisdom over fortune' by Otto van Veen (Flemish, 1556 - 1629), as downloaded from https://artvee.com/dl/the-triumph-of-wisdom-over-fortune.}\label{fig:unnamed-chunk-11}
\end{figure}

\hypertarget{case-study---tuskegee-syphilis-study}{%
\section{Case study - Tuskegee Syphilis Study}\label{case-study---tuskegee-syphilis-study}}

The Tuskegee Syphilis Study is an infamous medical trial in which Black Americans with syphilis (and a `control group' without) were not given appropriate treatment, nor even told they had syphilis, well after standard syphilis treatments were established in the mid-1940s \citep{marcella}. The study began in 1932 when poor Black Americans in the South were identified and offered compensation including `hot meals, the guise of treatment, and burial payments' \citep{marcella}. The men were not treated for syphilis. Further, and this is almost unbelievable, some of the men were drafted, told they had syphilis, and ordered to get treatment. This treatment was blocked. By the time the study was stopped, `the majority of the study's victims were deceased, many from syphilis-related causes.' \citep{marcella}.

The study continued through to 1972, only stopping when it was leaked and published in newspapers. In response the US established requirements for Institutional Review Boards and President Clinton made a formal apology in 1997. \citet{brandt1978racism} as quoted by \citet{marcella} says `\,``In retrospect the Tuskegee Study revealed more about the pathology of racism than the pathology of syphilis; more about the nature of scientific inquiry than the nature of the disease process\ldots. The degree of deception and the damages have been severely underestimated.''\,'

On the Tuskegee Syphilis Study \href{https://www.monicaalexander.com}{Professor Monica Alexander} says:

\begin{quote}
While it may be illegal to do this exact research these days, it doesn't mean that unethical research doesn't still happen, and we see it all the time in ML and health. Just because you can't explicitly discriminate when you design experiments, doesn't mean you can't implicitly discriminate.
\end{quote}

For an example of this, start with \citet{obermeyer2019dissecting}:

\begin{quote}
Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5\%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.
\end{quote}

\hypertarget{sampling-and-survey-essentials}{%
\section{Sampling and survey essentials}\label{sampling-and-survey-essentials}}

\hypertarget{introduction-12}{%
\subsection{Introduction}\label{introduction-12}}

Statistics is the cold, hard, core of using data. Statisticians have spent considerable time and effort thinking about the properties that various samples of data will have and how they enable us to speak to implications for the broader population.

Let's say that we have some data. For instance, a particular toddler goes to sleep at 6:00pm every night. We might be interested to know whether that bed-time is common more generally among all toddlers, or if we have an unusual toddler. We only have one toddler so our ability to use his bed time to speak about all toddlers is limited. But what about if we talk to our friends who also have toddlers? How many friends, and friends of friends, do we have to ask because we can begin to feel comfortable speaking about some underlying truth of toddler bedtime?

In the wonderful phrase of \citet[p.~3]{wuandthompson} `{[}s{]}tatistics is the science of how to collect and analyze data, and draw statements and conclusions about unknown populations. The term population usually refers to a real or hypothetical set of units with characteristics and attributes which can be modelled by random variables and their respective probability distributions.'. In my own much less wonderful phrasing, `statistics involves having some data and trying to say something sensible about it'. I mean, it's really up to you which one you want to go with.

In the case of surveys, our population is a finite set of \(N\) labels: `person 1', `person 2', `person 3', \ldots, `person \(N\)'. It is important here to recognise that there is a difference between the population of interest to a survey and a population in the sense that it is used when we talk of limits and similar infinity concepts in statistics. For instance, from time to time, you hear people who work with census data say that they don't need to worry about confidence intervals because they have the whole population of the country. Nothing could be further from the truth.

\citet[p.~4]{wuandthompson} have a lovely example of the ambiguity that surrounds the definition of a population. Let's consider the population of voters. In Canada that means anyone who is 18 or older. Fine. But what if we are interested in consumers - what is the definition of hipsters? I regularly eat avocado toast, (+1), but I've never had bullet coffee (-1). Am I in the population or not?

More things are formally defined than you may realise. For instance, the idea of a rural area is precisely defined. A property is either in a rural area or not. But then we come to the lovely example of \citet[p.~4]{wuandthompson} when it comes to whether someone is a smoker. If a 15 year old has had 100 cigarettes then it's pretty clear that we need to treat them differently than if they have had none. But if a 100 year old has had 100 cigarettes then we consider them to have none. That's fine, but what is the age at which this changes? Further, think about how this changes over time. At one point, parents used to be worried if children had more than two hours of screen time, now those same children (and possibly even the parents) regularly likely spend more than eight hours in front of a screen if they work in an office job.

So we come to some critical terminology:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Population: `The set of all units covered by the main objective of the study.' \citet[p.~5]{wuandthompson}.
\item
  Frame: `Lists of sampling units' \citet[p.~9]{wuandthompson} where sampling units are either the observational units themselves or the clusters.
\item
  Sample: Those who complete and return the survey.
\end{enumerate}

To be a little more concrete about this, consider that we are trying to conduct a survey about the attitudes Australians who live in Toronto. So the target population is all Australians who live in Toronto, the frame might be all those Australians who live in Toronto who use Facebook, because we are going to use Facebook to choose who to sample. And then finally, if we take that Facebook list of all Australians living in Canada and we gave each one a chance at being surveyed then that would be our sampled population, but if we just picked the ones that I know then it would just be Dan, Monica, and Liza (from New Zealand but we'll claim her because that's a thing that Australians do).

In that example the target population and the frame will be different because not all Australians who live in Toronto are on Facebook. Similarly, if not everyone that we gave the survey to actually completed the survey then the sample and the frame would be different.

Having identified a population of interest and a frame (i.e.~a list that gets the closest to that population) At this point we distinguish between probability and non-probability sampling.

With probability sampling, every member of the frame has some chance of being sampled. Consider the example of the Australian Election Study - they get a list of all the addresses in Australia, and then randomly choose some to send letters to. The `randomista' and RCT revolution that we discuss later, is needed because of a lack of probability sampling, but when it exists it plays a role here. Importantly it ensure that we are clear about the role of uncertainty \citep[p.~11]{wuandthompson}. The trade-off is that it is expensive and difficult. Note that each unit in the frame doesn't have to have the same probability necessarily, it just needs to be determined by a probability measure.

In contrast, with non-probability sampling we focus on populations that are `readily available' or convenient, satisfy certain quotas, based on judgement, or those that volunteer. The difference between probability and non-probability sampling is that of degree - we typically cannot force someone to take our survey, and hence, there is almost also as aspect of volunteering.

While acknowledging that it is a spectrum, most of statistics was developed based on probability sampling. But much of modern sampling is done using non-probability sampling. In particular, a common approach is to have a bunch of Facebook ads trying to recruit a panel of people in exchange for compensation. This panel is then the group that is sent various surveys as necessary. But think for a moment about the implications of this - what type of people are likely to respond to such an ad? I don't know who Canada's richest person is, but are they likely to be in this panel? Is your grandmother likely to respond to that ad? What about you - do you even use Facebook?

In some cases it is possible to do a census. Nation-states typically do one every five to ten years. But there is a reason that it is only nation states that do them - they are expensive, time-consuming, and surprisingly, they are sometimes not as accurate as we may hope because of how general they need to be. Hence, the role of surveys. Note, however that censuses will typically have many of the same concerns.

When we consider our population, it will typically have some ordering. This may be as simple as a country having states/provinces. We consider a stratified structure to be one in which we can divide the population into mutually exclusive and collectively exhaustive sub-populations, or strata. Examples of strata in \citet[p.~8]{wuandthompson} include provinces, federal electoral districts, or health regions. But strata need not be geographic, and it may be possible to use different majors. We use stratification to help with the efficiency of sampling or with the balance of the survey. For instance, if we surveyed provinces in proportion to their population, then even a survey of 10,000 responses would only expect to have 10 responses from the Yukon.

The other word that is used that takes advantage of the ordering of some population is clusters. Again, these are collectively exhaustive and mutually exclusive. Again, they may be geographically based, but need not be. The difference between stratified sampling and cluster sampling, is that `under stratified sampling, sample data are collected from every stratum, (whereas) under cluster sampling, only a portion of the clusters has members in the final sample' \citet[p.~8]{wuandthompson}. That all said, this difference can become less clear in practice, especially \emph{ex post} - what if you stratify then randomly sample within that strata, but no one is selected - but in terms of intention the difference is clear.

We now turn to the first of our claims, which is that if we have a perfect frame and no non-response, then our sample results will match that of the population. We'd of course be very worried if that weren't the case, but it's nice to have it stated. We establish some type of population mean for the study variable, \(\mu_y\), and population means for the auxiliary variables \(\mu_x\), which could be things like age, gender, etc. Remembering that when we do this in the real world, we may have many study variables, and indeed, some overlap. If a variable is an indicator then in this set-up all we have to do is to work out the proportion in order to estimate it, which is \(P\). And finally, we get a rule of thumb for large samples whereby the variance in this binary and perfect setting becomes \(\sigma_y^2 = P/(1-P)\) \citep[p.~11]{wuandthompson}.

Finally, we conclude with the steps that you should consider. These are all critical. Strong reports would grapple with all of these.

\hypertarget{simple-random-sampling}{%
\subsection{Simple random sampling}\label{simple-random-sampling}}

TBD

\hypertarget{stratified-and-cluster-sampling}{%
\subsection{Stratified and cluster sampling}\label{stratified-and-cluster-sampling}}

TBD

\hypertarget{snowball-sampling-and-confidant-methods}{%
\subsection{Snowball sampling and confidant methods}\label{snowball-sampling-and-confidant-methods}}

TBD

\hypertarget{case-study---the-oregon-health-insurance-experiment}{%
\section{Case study - The Oregon Health Insurance Experiment}\label{case-study---the-oregon-health-insurance-experiment}}

The Oregon Health Insurance Experiment involved 74,922 adults in Oregon from 2008 to 2010. The opportunity to apply for health insurance was randomly allocated and then health and earnings evaluated. It was found that \citep{finkelstein2012oregon}:

\begin{quote}
In the year after random assignment, the treatment group selected by the lottery was about 25 percentage points more likely to have insurance than the control group that was not selected. We find that in this first year, the treatment group had substantively and statistically significantly higher health care utilization (including primary and preventive care as well as hospitalizations), lower out-of-pocket medical expenditures and medical debt (including fewer bills sent to collection), and better self-reported physical and mental health than the control group.
\end{quote}

A lottery was used to determine which of the 89,824 individuals who signed up would be allowed to apply for Medicaid. This random allocation of insurance allowed the researchers to understand the effect of health insurance. It's not usually possible to compare those with and without insurance because the type of people that sign up to get health insurance differ to those who don't - that decision is `confounded' with other variables. They use administrative data, such as hospital discharge data, credit reports that were matched to 68.5 per cent of lottery participants, and mortality records, which will be uncommon. Interestingly this collection of data is actually fairly restrained and so they included a survey conducted via mail.

Turning to external validity, the authors restrain themselves and say \citep{finkelstein2012oregon}:

\begin{quote}
Our estimates of the impact of public health insurance apply to able-bodied uninsured adults below 100 percent of poverty who express interest in insurance coverage. This is a population of considerable policy interest.
\end{quote}

A lottery was used to allocate 10,000 places in the state-run Medicaid. A lottery was judged fair because `the state (correctly) anticipated that the demand for the program among eligible individuals would far exceed the 10,000 available new enrollment slots' \citep{finkelstein2012oregon}. People had a month to sign up to enter the draw. The draws were conducted over a six-month period and those who were selected had the opportunity to sign up. 35,169 individuals were selected (the household of those who actually won the draw was given the opportunity) but only 30 per cent of them completed the paperwork and were eligible (typically they earned too much). The insurance lasted indefinitely.

The model they consider is \citep{finkelstein2012oregon}:

\begin{equation}
y_{ihj} = \beta_0 + \beta_1\mbox{Lottery} + X_{ih}\beta+2 + V_{ih}\beta_3 + \epsilon_{ihj} \label{eq:oregon}
\end{equation}

Equation \eqref{eq:oregon} explains various \(j\) outcomes (such as health) for an individual \(i\) in household \(h\) as a function of an indicator variable as to whether household \(h\) was selected by the lottery. Hence, `(t)he coefficient on Lottery, \(\beta_1\), is the main coefficient of interest, and gives the average difference in (adjusted) means between the treatment group (the lottery winners) and the control group (those not selected by the lottery).'

To complete the specification of Equation \eqref{eq:oregon}, \(X_{ih}\) is a set of variables that are correlated with the probability of being treated. These adjust for that impact to a certain extent. An example of that is the number of individuals in a household. And finally, \(V_{ih}\) is a set of variables that are not correlated with the lottery. These variables include demographics, hospital discharge and lottery draw.

There is a wide range of literature related to this intervention. More papers are available \href{https://www.povertyactionlab.org/evaluation/oregon-health-insurance-experiment-united-states}{here}.

\hypertarget{case-study---student-coaching-how-far-can-technology-go}{%
\section{Case study - Student Coaching: How Far Can Technology Go?}\label{case-study---student-coaching-how-far-can-technology-go}}

There is a general concern about students dropping out of university before they finish their degree. If you work one-on-one with a student then this addresses the issue. But that doesn't scale. The point of this experiment was to see if technology-based options could be more efficient. The focus was the University of Toronto, and in particular first-year economics courses in Fall 2015.

The intervention was administered to students as part of an economics class. Students received 2 per cent of their grade for completing the exercise. The specific exercise depended on the group of the student. The intervention involved three treatments as well as a control group that was just given a Big Five personality traits test. Additional information that was obtained included `the highest level of education obtained by students' parents, the amount of education they expect to obtain, whether they are first-year or international students, and their work and study time plans for the upcoming year.' \citep[p.~6]{Oreopoulos2018}.

The treatments were \citep[p.~4]{Oreopoulos2018}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  `{[}A{]} one-time, online exercise completed during the first two weeks of class in the fall'. This exercise was `designed to get them thinking about the future they envision and the steps they could take in the upcoming year at U of T to help make that future a reality. They were told that the exercise was designed for their benefit and to take their time while completing it. The online module lasted approximately 60 to 90 minutes and led students through a series of writing exercises in which they wrote about their ideal futures, both at work and at home, what they would like to accomplish in the current year at U of T, how they intend on following certain study strategies to meet their goals, and whether they want to get involved with extracurricular activities at the university' \citep[p.~6]{Oreopoulos2018}.
\item
  `{[}T{]}he online intervention plus text and email messaging throughout the full academic year'. This involved the students being given `the opportunity to provide their phone numbers and participate in a text and email messaging campaign lasting throughout both the fall semester in 2015 and the winter semester in 2016' \citep[p.~8]{Oreopoulos2018}. All students in this group got the emails, but only those that provided phone numbers got the messages. They were able to opt out, but `few chose to do so' \citep[p.~8]{Oreopoulos2018}. This was a two-way interaction in which students could ask questions. Some asked for the `locations of certain facilities on campus or how to stay on residence during the holiday break, while others said they need help with English skills or specific courses. Some students expressed relatively deep emotions, such as feeling anxious about family pressure to succeed in school or from doing poorly on an evaluation' \citep[p.~9]{Oreopoulos2018}. A response was usually given within an hour.
\item
  `{[}T{]}he online intervention plus one-on-one coaching in which students are assigned to upper-year undergraduate coaches'. `Coaches were available to meet with students to answer any questions via Skype, phone, or in person, and would send their students regular text and email messages of advice, encouragement, and motivation, much like the \href{mailto:You@UofT}{\nolinkurl{You@UofT}} program described above. In contrast to the messaging program, however, coaches were instructed to be proactive and regularly monitor their students' progress. Whereas the \href{mailto:You@UofT}{\nolinkurl{You@UofT}} program attempts to ``nudge'' students in the right direction with academic advice, coaches play a greater ``support'' role, sensitively guiding students through problems.' \citep[p.~11]{Oreopoulos2018}. This coaching program was only available at UTM. `Our coaching treatment group was established by randomly drawing twenty-four students from the group of students that were randomly assigned into the text message campaign treatment. At the conclusion of the online exercise, instead of being invited to provide a phone number for the purpose of receiving text messages, these twenty-four students were given the opportunity to participate in a pilot coaching program. A total of seventeen students agreed to participate in the coaching program, while seven students declined.'
\end{enumerate}

\begin{quote}
Our coaching treatment group was established by randomly drawing twenty-four students from the group of students that were randomly assigned into the text message campaign treatment. At the conclusion of the online exercise, instead of being invited to provide a phone number for the purpose of receiving text messages, these twenty-four students were given the opportunity to participate in a pilot coaching program. A total of seventeen students agreed to participate in the coaching program, while seven students declined.

\citep[p.~14]{Oreopoulos2018}
\end{quote}

The model they consider is \citep[p.~15]{Oreopoulos2018}:

\begin{equation}
y_{ij} = \alpha + \beta_1\mbox{Online}_i + \beta_2\mbox{Text}_i + + \beta_3\mbox{Coach}_i + \delta_j + \mu \mbox{First year}_i + \epsilon_{ij} \label{eq:toronto}
\end{equation}

Equation \eqref{eq:toronto} explains the outcome of student \(i\) at campus \(j\) based on `indicators for each of the three treatment exercises students were given, campus fixed effects, and a first-year student indicator.' The main parameters of interest are \(\beta_1\), \(\beta_2\) and \(\beta_3\). The main outcomes were course grades, GPA, credits earned and failed.

It was found, that the one-on-one coaching `increased grades by approximately 5 percentage points', while the other treatments had `had no detectable impact'. One set of results are summarised in Figure \ref{fig:torontointervention}.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/toronto_intervention} \caption{Example of the results of the intervention.}\label{fig:torontointervention}
\end{figure}

The results are important not only in a teaching context, but also for businesses hoping to retain customers. More papers are available \href{https://www.povertyactionlab.org/evaluation/student-coaching-how-far-can-technology-go}{here}.

\hypertarget{case-study---civic-honesty-around-the-globe}{%
\section{Case study - Civic Honesty Around The Globe}\label{case-study---civic-honesty-around-the-globe}}

Trust isn't something that we think regularly about, but it's actually fairly fundamental to most interactions, both economic and personal. For instance, many of us get paid after we do some work - we're trusting our employer will make good; and vice versa - if you get paid in advance then they are trusting you. In a strictly naive, one-shot, transaction-cost-less world, this doesn't make sense. If you get paid in advance, the incentive is for you to take the money and run in the last pay period before you quit, and through backward induction everything falls apart. Of course, we don't live in such a world. For one thing there are transaction costs, for another generally we have repeated interactions, and finally, in my experience, the world usually ends up being fairly small.

Understanding the extent of honestly in different countries may help us to explain economic development and other aspects of interest such as tax compliance, but it's fairly hard to measure. We can't really ask people how honest they are - wouldn't the liars lie, resulting in a lemons problem \citep{akerlof1978market}? To get around, this \citet{cohn2019civic} conduct an experiment in 355 cities across 40 countries where they `turn in' either: a wallet with the local equivalent of US\$13.45 in it, or no money. They are interested in whether the `recipient' attempts to return the wallet. They find that `{[}in virtually all countries, citizens were more likely to return wallets that contained more money' \citep[p.~1]{cohn2019civic}.

The set-up of the experiment is fascinating. They `turn in' 17,303 wallets to various institutions including: `(i) banks; (ii) theaters, museums, or other cultural establishments; (iii) post offices; (iv) hotels; and (v) police stations, courts of law, or other public offices' \citep[p.~1]{cohn2019civic}. These institutions were roughly equally sampled, although banks were slightly over sampled and post offices were slightly under-sampled. The importance of such institutions in the economy is generally well-accepted \citep{acemoglu2001colonial} and they are common across most countries. Importantly for the experiment, they `typically have a public reception area where we could perform the drop-offs' \citep[p.~1]{cohn2019civic}.

The way the experiment worked is that a research assistant turned the wallet in to an employee at a counter in the public reception area, saying `Hi, I found this {[}showing the wallet{]} on the street just around the corner. {[}Place wallet on counter.{]} Somebody must have lost it. I'm in a hurry and have to go. Can you please take care of it?' \citep[p.~2]{cohn2019civic}. The outcome of interest is whether an email is sent to the unique address on a business card in the wallet within 100 days. The research assistant had to note various features of the setting, including features such as the gender, age-group, and busyness of the `recipient'.

The wallets were transparent, and the business card had a name and email contact details. It also had a key and a grocery list (Figure \ref{fig:walletsexample}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/wallet_example} \caption{Example of the wallet.}\label{fig:walletsexample}
\end{figure}

The grocery list was an attempt to convince the `recipient' that the `owner' was a local. Language and currency were adapted to local conditions. The key is only useful to the `owner', not to the `recipient' of the wallet and was included to test for altruistic concerns.

The primary treatment in the experiment is whether the wallet contained money or not. The key outcome was whether the wallet was attempted to be returned or not. It was found that `{[}t{]}he median response time was roughly 26 minutes across all countries, and about 88\% of emails arrived within 24 hours' \citep[p.~10]{cohn2019civicaddendum}. If an email was received, then 3 hours later a response was sent, saying that the owner had left town, the contents were unimportant to them and that they could keep it or donate it to charity \citep[p.~9]{cohn2019civicaddendum}.

Considerable differences were found between countries (Figure \ref{fig:wallets}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/wallet_results} \caption{Key finding as to wallet return rates.}\label{fig:wallets}
\end{figure}

Figure \ref{fig:wallets} shows that in almost all countries wallets with money were more likely to be returned than wallets without. The authors further conducted the experiment with the equivalent of US\$94.15 in three countries - Poland, the UK, and the US - and found that reporting rates further increased. In those same three countries further tests were done comparing the situation when the wallet always contained money, but the presence of the key was varied. The wallet was slightly more likely to be reported when there was a key.

The full set of 40 countries were chosen based on having enough cities with populations of at least 100,000, as well as the ability for the research assistants to safely visit and withdraw cash. The cities were chosen starting with the largest ones and there were usually 400 observations in each country \citep[p.~5]{cohn2019civicaddendum}. Real-world concerns affected the specifics of the experiment. For instance, in `\ldots India, we made a last minute change by replacing Chennai with Coimbatore due to severe flooding that took place in February 2015. In Kenya we did not carry out data collection in the last city visited (Malindi) because the research assistant was arrested and interrogated by the military police for suspicious activity' \citep[p.~5]{cohn2019civicaddendum}.

In addition to the experiments, \citet{cohn2019civic} conducted surveys that allowed them to understand some reasons for their findings. It also allowed them to be specific about the respondents. The survey involved 2,525 respondents (829 in the UK, 809 in Poland, and 887 in the US) \citep[p.~36]{cohn2019civicaddendum}. `To qualify for participation, individuals had to pass a simple attention check and meet the demographic quotas (based on age, gender, and residence) set by Qualtrics to construct the representative samples. Participants received a flat payment of US \$4.00 for their participation' \citep[p.~36]{cohn2019civicaddendum}. The participants were given one of the scenarios and then asked to answer questions.

Annoyingly the authors don't explicitly specify the estimating equation. However they do say that important covariates about the `recipient' include: gender, age-group, busyness, whether they were local, spoke English, understood the situation, friendliness, presence of: a computer, co-workers, bystanders, security cameras, security guards. Important covariates at a country-level include: country GDP, soil fertility, latitude, distance to water, temperature and its volatility, precipitation and its volatility, elevation, terrain roughness, pathogens, language features such as: pronouns, politeness, future time; share of protestants; family ties; state history; years of democracy; executive constraints; judicial independence; constitutional review; electoral rule; and primary school enrollment in 1920.

The code or data for the paper are available: \citet{walletsdata}.

\hypertarget{ab-testing}{%
\section{A/B testing}\label{ab-testing}}

\hypertarget{introduction-13}{%
\subsection{Introduction}\label{introduction-13}}

\begin{quote}
Large companies, particularly tech companies, have developed incredibly sophisticated infrastructure for running complex experiments. In the tech industry, these experiments are often called A/B tests because they compare the effectiveness of two treatments: A and B. Such experiments are frequently run for things like increasing click-through rates on ads, but the same experimental infrastructure can also be used for research that advances scientific understanding.

\citet[p.~185]{Salganik2018}.
\end{quote}

The past decade has probably seen the most experiments ever run by several orders of magnitude with the extensive use of A/B testing on websites. Every time you are online you are probably subject to tens, hundreds, or potentially thousands, of different A/B tests. If you use apps like TikTok then this could run to the tens of thousands. While, at their heart, they are still just surveys that result in data that need to be analysed, they have several interesting features, which we will discuss.

The opening example of \citet[p.~3]{kohavi} is a particularly nice illustration.

\begin{quote}
In 2012, an employee working on Bing, Microsoft's search engine, suggested changing how ad headlines display (Kohavi and Thomke 2017). The idea was to lengthen the title line of ads by combining it with the text from the first line below the title, as shown in Figure 1.1.

Nobody thought this simple change, among the hundreds suggested, would be the best revenue-generating idea in Bing's history!

The feature was prioritized low and languished in the backlog for more than six months until a software developer decided to try the change, given how easy it was to code. He implemented the idea and began evaluating the idea on real users, randomly showing some of them the new title layout and others the old one. User interactions with the website were recorded, including ad clicks and the revenue generated from them. This is an example of an A/B test, the simplest type of controlled experiment that compares two variants: A and B, or a Control and a Treatment.

A few hours after starting the test, a revenue-too-high alert triggered, indicating that something was wrong with the experiment. The Treatment, that is, the new title layout, was generating too much money from ads. Such ``too good to be true'' alerts are very useful, as they usually indicate a serious bug, such as cases where revenue was logged twice (double billing) or where only ads displayed, and the rest of the web page was broken.

For this experiment, however, the revenue increase was valid. Bing's revenue increased by a whopping 12\%, which at the time translated to over \$100M annually in the US alone, without significantly hurting key user-experience metrics. The experiment was replicated multiple times over a long period.

The example typifies several key themes in online controlled experiments:

\begin{itemize}
\tightlist
\item
  It is hard to assess the value of an idea. In this case, a simple change worth over \$100M/year was delayed for months.
\item
  Small changes can have a big impact. A \$100M/year return-on-investment (ROI) on a few days' work for one engineer is about as extreme as it gets.
\item
  Experiments with big impact are rare. Bing runs over 10,000 experiments a year, but simple features resulting in such a big improvement happen only once every few years.
\item
  The overhead of running an experiment must be small. Bing's engineers had access to ExP, Microsoft's experimentation system, which made it easy to scientifically evaluate the idea.
\item
  The overall evaluation criterion (OEC, described more later in this chapter) must be clear. In this case, revenue was a key component of the OEC, but revenue alone is insufficient as an OEC. It could lead to plastering the web site with ads, which is known to hurt the user experience. Bing uses an OEC that weighs revenue against user-experience metrics, including Sessions per user (are users abandoning or increasing engagement) and several other components. The key point is that user-experience metrics did not significantly degrade even though revenue increased dramatically.
\end{itemize}
\end{quote}

In these notes, I'm going to use A/B testing to strictly refer to the situation in which we're dealing with a tech firm, and some type of change in code. If we are dealing with the physical world then we'll stick with RCTs.

I'm usually fairly dismissive of the CS folks who adopt different language for concepts that have been around for a long time. However, in the case of A/B testing I think that it's possibly justified. There is something different about doing tens of thousands of small experiments all the time, compared with our normal RCT set-up of one experiment conducted over months. And finally, if you don't work in a tech firm, then don't discount the difficulty of shifting to an experimental set-up. You may think that it's easy to go to a workplace and say `hey, let's test stuff before we spend thousands/millions of dollars'. You'd be wrong. In my opinion, the hardest part of A/B testing isn't the science, it's the politics.

\hypertarget{delivery}{%
\subsection{Delivery}\label{delivery}}

Drawing on \citet[p.~153-161]{kohavi}, we first consider how we will be delivering the A/B test. In the case of a RCT it's fairly obvious how we deliver it - for instance, make a person come to a doctor's clinic and inject them with a drug or a placebo. In the case of A/B testing, it's less obvious - do you run it `server-side' or `client-side'? What this means is, do you just change the website - `server side', or do you change an app - `client side'. This may seem like a silly issue, but it affects: 1) release; and 2) data transmission.

In the case of the effect on release, it's easy and normal to update a website all the time, so small changes can be easily implemented in the case of server-side. However, in the case of client-side, let's say an app, it's likely a much bigger deal.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  It needs to get through an app store (a bigger or lesser deal depending on which one).
\item
  It need to go through a release cycle (a bigger or lesser deal depending on the specifics of the company and how it ships).
\item
  Users have the opportunity to not upgrade. Are they likely different to those that do upgrade? (Yes.)
\end{enumerate}

Now, in the case of the effect on data transmission, again server-side is less of a big deal - you kind of get the data as part of the user interacting. But in the case of client-side - it's not necessarily the case that the user will have the internet at the time they're using your application, and if they do, they may have limitations on the data uploads. The phone may limit data transmission depending on its effect on battery, CPU, general performance, etc. Then maybe you decide to cache, but then the user may find it weird that some minor app takes up as much size as their photos.

The effect of all this is that you need to plan and build this into your expectations - don't promise results the day after a release if you're evaluating a client-side change. Adjust for the fact that your results are conditional and gather data on those conditions e.g.~battery level or whatever. Adjust in your analysis for different devices and platforms, etc. This is a lovely opportunity for multilevel regression.

\hypertarget{instrumentation}{%
\subsection{Instrumentation}\label{instrumentation}}

Drawing on \citet[p.~162 - 165]{kohavi}, I'll now discuss instrumentation. \citet{kohavi} use the name `instrumentation'. I'd prefer something like `measurement methods' so that we don't confuse this with the entirely different concept of instrumental variables later in the course, but instrumentation is what is used in industry, so we'll use that here too.

Regardless of what it's called, the point of this is that you need to consider how you are getting your data in the first place. For instance, if we put a cookie on your device then different types of users will remove that at different rates. Using things like beacons can be great (this is when you force the user to `download' some tiny thing they don't notice so that you know they've gone somewhere - see `email' etc). But again, there are practical issues - do we force the beacon before the main content loads - which makes for a worse customer experience; or do we allow the beacon to load after the main content, in which case we may get a biased sample?

There are likely different servers and databases for different faces of the product. For instance, Twitter in Australia, compared with Twitter in Canada, compared with Twitter on my phone's app, compared with Twitter accessed via the browser. Joining these different datasets can be difficult and requires either a unique id or some probabilistic approach.

\citet[p.~165]{kohavi} recommend changing the culture of your workplace to ensure instrumentation is normalised, which I mean, yeah, good luck.

\hypertarget{randomisation-unit}{%
\subsection{Randomisation unit}\label{randomisation-unit}}

Again, drawing on \citet[p.~162 - 165]{kohavi}, we need to be very aware of what are we actually randomising over? Again, this is something that's kind of obvious in normal RCTs, but gets like really interesting in the case of A/B testing. Let's consider the malaria netting experiments - either a person/village/state gets a net or it doesn't. Easy (relatively). But in the case of server-side A/B testing - are we randomising the page, the session, or the user?

To think about this, let's think about colour. Let's say that we change our logo from red to blue on the `home' page. If we're randomising at the page level, then when the user goes to the `about' page the logo could be back to red. If we're randomising at the session level, then it'll be blue while they're using the website that time, but if they close it and come back then it'll be red. Finally, if we're randomising at a user level then it'll always be red for me, but always blue for my friend. That last bit assumes perfect identity tracking, which might be generally okay if you're Google or Facebook, but for anyone else is going to be a challenge - what if you visit cbc.ca on your phone and then on your laptop? You're likely considered a different `user'.

Does this matter? It's a trade-off between consistency and importance.

We are always interested in whether the treatment and control groups have been created randomly. One way to test it is an A/A test. \citet[p.~129]{taddy2019} describes how `AB platforms typically run ``AA'' tests that show the same website in groups A and B. If you see a significant difference between groups in an AA trial, then something is likely wrong in your randomization.'.

\citet[p.201]{kohavi} says similarly, that `(w)e highly recommend running continuous A/A tests in parallel with other experiments to uncover problems, including distribution mismatches and plat- form anomalies.'

\hypertarget{partnerships}{%
\subsection{Partnerships}\label{partnerships}}

Unless we work at a Facebook/Twitter type firm, it may not be possible to run A/B tests ourselves at scale. While we can randomise our own personal website fairly easily, for most of us there won't be many visitors. Hence it can be important to partner with such firms. \citet[p.~187]{Salganik2018} draws our attention to the fact that there may be tension between `the researchers and the partners'.

As an example, \citet{Salganik2018} discusses a situation where one treatment (out of the three that were possible) accounted for 98 per cent of the sample because Facebook wanted to treat everyone. The researchers were only able to convince `them to hold back 1 per cent for a related treatment and 1 per cent for a control group.' He continues:

\begin{quote}
without the control group, it would have been basically impossible to measure the effect of the Info + Social treatment because it would have been a ``perturb and observe'' experiment, rather than a randomized controlled experiment. This example provides a valuable practical lesson for working with partners: sometimes you create an experiment by convincing someone to deliver a treatment and sometimes you create an experiment by convincing someone not to deliver a treatment (i.e.~to create a control group).

\citet[p.~188]{Salganik2018}.
\end{quote}

In order to identify such opportunities, \citet[p.~188]{Salganik2018} advises us `to notice a real problem that you can solve while you are doing interesting science'. \citet{Salganik2018} closes with four other pieces of advice:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  `(Y)ou should think as much as possible before any data have been collected' \citet[p.~189]{Salganik2018}.
\item
  `(Y)ou should consider designing a series of experiments that reinforce each other' \citet[p.~190]{Salganik2018}.
\item
  You should `(c)reate zero variable cost data', by: 1) trying to replace human work with computer work; and 2) creating fun experiments that participants want to participate in \citep[ p.~191]{Salganik2018}.
\item
  You should `(b)uild ethics into your design: replace, refine, and reduce', that is `(m)ake your experiment more humane by replacing experiments with non-experimental studies, refining the treatments {[}to be as harmless as possible{]}, and reducing the number of participants' \citep[ p.~196]{Salganik2018}.
\end{enumerate}

\hypertarget{speed-vs-quality}{%
\subsection{Speed vs quality}\label{speed-vs-quality}}

Don't peek at your results early and then call off the rest of the experiment if you've got significance. You essentially ruin everything that underpins statistics if you do that.

\hypertarget{conflicting-priorities}{%
\subsection{Conflicting priorities}\label{conflicting-priorities}}

One of the interesting aspects of A/B testing is that we're usually running them not because we desperately care about the specific outcome, but because that feeds into some other measure that we care about. For instance, do we care whether the website is quite-dark-blue or slightly-darker-blue or white? Probably not, but we probably care a lot about the company share price. But then what if picking the best blue comes at a cost to the share price?

Obviously, this is a bit contrived, so let's pretend that we work at a food delivery app and that we're the junior data scientist in charge of driver satisfaction. We do some A/B tests and we find that drivers are always happier when they are able to deliver food to the customer faster. Faster is better, always. But one way to achieve faster deliveries, is for them to not put the food into a hot box that will maintain the temperature. Something like that might save 30 seconds, which is significant on a 10-15 minute deliver. Unfortunately, although making a decision like that on the basis of A/B tests designed to optimize driver-satisfaction, would ultimately likely make the customer experience worse. If customers receive cold food, (when it's meant to be hot) then they may stop using the service and so this is likely bad for the app in the longer term.

This trade-off may be obvious if you're running the driver-experiment and you're looking at the customer complaints. Maybe on a small team or in a start-up you would be. But if you work for a larger team, you'd likely not and so ensuring that A/B tests aren't resulting in false optimization is something that is especially interesting, and not a typical trade-off in a normal RCT.

\hypertarget{case-study---upworthy}{%
\section{Case study - Upworthy}\label{case-study---upworthy}}

The trouble with much of A/B testing is that because it's done by firms we typically don't have datasets that we can use. However, J. Nathan Matias (Cornell), Kevin Munger (Penn State), and Marianne Aubin Le Quere (Cornell) obtained a dataset of A/B tests from Upworthy that they provide access to \citep{upworthy}. You are able to request access to the dataset here: \url{https://upworthy.natematias.com} (this request may take a couple of weeks to be processed). Upworthy was a click-bait news company that used A/B testing to optimize their content. More details are provided by \citet{aboutupworthy}.

Let's have a quick look at the data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{upworthy }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(here}\SpecialCharTok{::}\FunctionTok{here}\NormalTok{(}\StringTok{"dont\_push/upworthy{-}archive{-}exploratory{-}packages{-}03.12.2020.csv"}\NormalTok{))}

\NormalTok{upworthy }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 17}
\CommentTok{\#\textgreater{}    ...1 created\_at          updated\_at          clickability\_test\_id     excerpt}
\CommentTok{\#\textgreater{}   \textless{}dbl\textgreater{} \textless{}dttm\textgreater{}              \textless{}dttm\textgreater{}              \textless{}chr\textgreater{}                    \textless{}chr\textgreater{}  }
\CommentTok{\#\textgreater{} 1     0 2014{-}11{-}20 06:43:16 2016{-}04{-}02 16:33:38 546d88fb84ad38b2ce000024 Things\textasciitilde{}}
\CommentTok{\#\textgreater{} 2     1 2014{-}11{-}20 06:43:44 2016{-}04{-}02 16:25:54 546d88fb84ad38b2ce000024 Things\textasciitilde{}}
\CommentTok{\#\textgreater{} 3     2 2014{-}11{-}20 06:44:59 2016{-}04{-}02 16:25:54 546d88fb84ad38b2ce000024 Things\textasciitilde{}}
\CommentTok{\#\textgreater{} 4     3 2014{-}11{-}20 06:54:36 2016{-}04{-}02 16:25:54 546d902c26714c6c44000039 Things\textasciitilde{}}
\CommentTok{\#\textgreater{} 5     4 2014{-}11{-}20 06:54:57 2016{-}04{-}02 16:31:45 546d902c26714c6c44000039 Things\textasciitilde{}}
\CommentTok{\#\textgreater{} 6     5 2014{-}11{-}20 06:55:07 2016{-}04{-}02 16:25:54 546d902c26714c6c44000039 Things\textasciitilde{}}
\CommentTok{\#\textgreater{} \# ... with 12 more variables: headline \textless{}chr\textgreater{}, lede \textless{}chr\textgreater{}, slug \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   eyecatcher\_id \textless{}chr\textgreater{}, impressions \textless{}dbl\textgreater{}, clicks \textless{}dbl\textgreater{}, significance \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   first\_place \textless{}lgl\textgreater{}, winner \textless{}lgl\textgreater{}, share\_text \textless{}chr\textgreater{}, square \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   test\_week \textless{}dbl\textgreater{}}

\NormalTok{upworthy }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{names}\NormalTok{()}
\CommentTok{\#\textgreater{}  [1] "...1"                 "created\_at"           "updated\_at"          }
\CommentTok{\#\textgreater{}  [4] "clickability\_test\_id" "excerpt"              "headline"            }
\CommentTok{\#\textgreater{}  [7] "lede"                 "slug"                 "eyecatcher\_id"       }
\CommentTok{\#\textgreater{} [10] "impressions"          "clicks"               "significance"        }
\CommentTok{\#\textgreater{} [13] "first\_place"          "winner"               "share\_text"          }
\CommentTok{\#\textgreater{} [16] "square"               "test\_week"}
\end{Highlighting}
\end{Shaded}

From the documentation: `The Upworthy Research Archive contains packages within tests. On Upworthy, packages are bundles of headlines and images that were randomly assigned to people on the website as part of a test. Tests can include many packages.' So each row is a package and it should be part of a test `clickability\_test\_id'.

We have a variety of variables. We'll focus on `created\_at', `clickability\_test\_id' so that we can create comparison groups, `headline', `impressions' which is the number of people that saw the package, and `clicks' which is the number that clicked on that package. So within each batch of tests, we're interested in the effect of varied headlines on impressions and clicks.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{upworthy\_restricted }\OtherTok{\textless{}{-}} 
\NormalTok{  upworthy }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(created\_at, clickability\_test\_id, headline, impressions, clicks)}

\FunctionTok{head}\NormalTok{(upworthy\_restricted)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 5}
\CommentTok{\#\textgreater{}   created\_at          clickability\_test\_id     headline       impressions clicks}
\CommentTok{\#\textgreater{}   \textless{}dttm\textgreater{}              \textless{}chr\textgreater{}                    \textless{}chr\textgreater{}                \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 2014{-}11{-}20 06:43:16 546d88fb84ad38b2ce000024 They\textquotesingle{}re Being\textasciitilde{}        3052    150}
\CommentTok{\#\textgreater{} 2 2014{-}11{-}20 06:43:44 546d88fb84ad38b2ce000024 They\textquotesingle{}re Being\textasciitilde{}        3033    122}
\CommentTok{\#\textgreater{} 3 2014{-}11{-}20 06:44:59 546d88fb84ad38b2ce000024 They\textquotesingle{}re Being\textasciitilde{}        3092    110}
\CommentTok{\#\textgreater{} 4 2014{-}11{-}20 06:54:36 546d902c26714c6c44000039 This Is What \textasciitilde{}        3526     90}
\CommentTok{\#\textgreater{} 5 2014{-}11{-}20 06:54:57 546d902c26714c6c44000039 This Is What \textasciitilde{}        3506    120}
\CommentTok{\#\textgreater{} 6 2014{-}11{-}20 06:55:07 546d902c26714c6c44000039 This Is What \textasciitilde{}        3380     98}
\end{Highlighting}
\end{Shaded}

We are going to focus on the text contained in headlines. We also want to remove the effect of different pictures, by comparing on the same image. I'm interested in whether headlines that asked a question got more clicks than those that didn't.

To identify whether a headline asks a question, I'm going to just search for a question mark. Although there are more complicated constructions that we could use, this will be enough to get started.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{upworthy\_restricted }\OtherTok{\textless{}{-}} 
\NormalTok{  upworthy\_restricted }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{asks\_question =}\NormalTok{ stringr}\SpecialCharTok{::}\FunctionTok{str\_detect}\NormalTok{(}\AttributeTok{string =}\NormalTok{ headline, }\AttributeTok{pattern =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{?"}\NormalTok{))}

\NormalTok{upworthy\_restricted }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{count}\NormalTok{(asks\_question)}
\CommentTok{\#\textgreater{} \# A tibble: 2 x 2}
\CommentTok{\#\textgreater{}   asks\_question     n}
\CommentTok{\#\textgreater{}   \textless{}lgl\textgreater{}         \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 FALSE         19130}
\CommentTok{\#\textgreater{} 2 TRUE           3536}
\end{Highlighting}
\end{Shaded}

Now for every test, for every picture, we want to know whether asking a question affected the number of clicks.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{to\_question\_or\_not\_to\_question }\OtherTok{\textless{}{-}} 
\NormalTok{  upworthy\_restricted }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(clickability\_test\_id, asks\_question) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{ave\_clicks =} \FunctionTok{mean}\NormalTok{(clicks)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{()}

\NormalTok{look\_at\_differences }\OtherTok{\textless{}{-}} 
\NormalTok{  to\_question\_or\_not\_to\_question }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{id\_cols =}\NormalTok{ clickability\_test\_id,}
              \AttributeTok{names\_from =}\NormalTok{ asks\_question,}
              \AttributeTok{values\_from =}\NormalTok{ ave\_clicks) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{ave\_clicks\_not\_question =} \StringTok{\textasciigrave{}}\AttributeTok{FALSE}\StringTok{\textasciigrave{}}\NormalTok{,}
         \AttributeTok{ave\_clicks\_is\_question =} \StringTok{\textasciigrave{}}\AttributeTok{TRUE}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(ave\_clicks\_not\_question)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(ave\_clicks\_is\_question)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{difference\_in\_clicks =}\NormalTok{ ave\_clicks\_is\_question }\SpecialCharTok{{-}}\NormalTok{ ave\_clicks\_not\_question)}

\NormalTok{look\_at\_differences}\SpecialCharTok{$}\NormalTok{difference\_in\_clicks }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mean}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] {-}4.890435}
\end{Highlighting}
\end{Shaded}

So we find that in general, having a question in the headline may slightly decrease the number of clicks on a headline, although if there is an effect it does not appear to be very large (Figure \ref{fig:upworthy}).

\begin{figure}
\centering
\includegraphics{21-hunt_files/figure-latex/upworthy-1.pdf}
\caption{\label{fig:upworthy}Comparison of the average number of clicks when a headline contains a question mark or not.}
\end{figure}

\hypertarget{implementing-surveys}{%
\section{Implementing surveys}\label{implementing-surveys}}

\hypertarget{google}{%
\subsection{Google}\label{google}}

\hypertarget{facebook}{%
\subsection{Facebook}\label{facebook}}

\hypertarget{survey-monkey}{%
\subsection{Survey Monkey}\label{survey-monkey}}

\hypertarget{mechanical-turk}{%
\subsection{Mechanical Turk}\label{mechanical-turk}}

\hypertarget{prolific}{%
\subsection{Prolific}\label{prolific}}

\hypertarget{qualtrics}{%
\subsection{Qualtrics}\label{qualtrics}}

\hypertarget{other}{%
\subsection{Other}\label{other}}

\hypertarget{sensor-data}{%
\section{Sensor data}\label{sensor-data}}

\hypertarget{foi}{%
\section{FOI}\label{foi}}

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \setcounter{enumi}{2018}
  \tightlist
  \item
    Walby, Kevin and Alex Luscombe, eds.~Freedom of Information and Social Science Research Design. New York: Routledge.
  \end{enumerate}
\end{itemize}

\hypertarget{next-steps}{%
\section{Next steps}\label{next-steps}}

Large scale experiments are happening all around us. These days I feel we all know a lot more about healthcare experiments than perhaps we'd like to know and the AstraZeneca/Oxford situation is especially interesting, for instance, \citet{OxfordAstraZeneca}, but see \citet{Bastian2020} for how this is actually possibly more complicated.

There are also well-known experiments that tried to see if big government programs are effective, such as:

\begin{itemize}
\tightlist
\item
  The RAND Health Insurance Experiment randomly gave health insurance to people in the US between 1974 and 1982 \citep{randhealth}.
\item
  The Oregon Health Study randomly gave health insurance in Oregon in 2008 \citep{finkelstein2012oregon}.
\end{itemize}

\hypertarget{exercises-and-tutorial-8}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-8}}

\hypertarget{exercises-8}{%
\subsection{Exercises}\label{exercises-8}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In your own words, what is the role of randomisation in constructing a counterfactual (write two or three paragraphs)?
\item
  What is external validity (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Findings from an experiment hold in that setting.
  \item
    Findings from an experiment hold outside that setting.
  \item
    Findings from an experiment that has been repeated many times.
  \item
    Findings from an experiment for which code and data are available.
  \end{enumerate}
\item
  What is internal validity (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Findings from an experiment hold in that setting.
  \item
    Findings from an experiment hold outside that setting.
  \item
    Findings from an experiment that has been repeated many times.
  \item
    Findings from an experiment for which code and data are available.
  \end{enumerate}
\item
  If we have a dataset named `netflix\_data', with the columns `person' and `tv\_show' and `hours', (person is a character class uniqueID for every person, tv\_show is a character class name of a tv show, and hours is double expressing the number of hours that person watched that tv show). Could you please write some code that would randomly assign people into one of two groups? The data looks like this:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{netflix\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{person =} \FunctionTok{c}\NormalTok{(}\StringTok{"Rohan"}\NormalTok{, }\StringTok{"Rohan"}\NormalTok{, }\StringTok{"Monica"}\NormalTok{, }\StringTok{"Monica"}\NormalTok{, }\StringTok{"Monica"}\NormalTok{, }
                    \StringTok{"Patricia"}\NormalTok{, }\StringTok{"Patricia"}\NormalTok{, }\StringTok{"Helen"}\NormalTok{),}
         \AttributeTok{tv\_show =} \FunctionTok{c}\NormalTok{(}\StringTok{"Broadchurch"}\NormalTok{, }\StringTok{"Duty{-}Shame"}\NormalTok{, }\StringTok{"Broadchurch"}\NormalTok{, }\StringTok{"Duty{-}Shame"}\NormalTok{, }
                     \StringTok{"Shetland"}\NormalTok{, }\StringTok{"Broadchurch"}\NormalTok{, }\StringTok{"Shetland"}\NormalTok{, }\StringTok{"Duty{-}Shame"}\NormalTok{),}
         \AttributeTok{hours =} \FunctionTok{c}\NormalTok{(}\FloatTok{6.8}\NormalTok{, }\FloatTok{8.0}\NormalTok{, }\FloatTok{0.8}\NormalTok{, }\FloatTok{9.2}\NormalTok{, }\FloatTok{3.2}\NormalTok{, }\FloatTok{4.0}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{10.2}\NormalTok{)}
\NormalTok{         )}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  In the context of randomisation, what does stratification mean to you (write a paragraph or two)?
\item
  How could you check that your randomisation had been done appropriately (write two or three paragraphs)?
\item
  Identify three companies that conduct A/B testing commercially and write a short paper about how they work and the trade-offs of each. Are there any notable Toronto-based or Canadian companies? Why do you think this might be the case?
\item
  Pretend that you work as a junior analyst for a large consulting firm. Further, pretend that your consulting firm has taken a contract to put together a facial recognition model for the Canada Border Services Agency's Inland Enforcement branch. Taking a page or two, please discuss your thoughts on this matter. What would you do and why?
\item
  What are some types of probability sampling, and in what circumstances might you want to implement them (write two or three pages)?
\item
  There have been some substantial political polling `misses' in recent years (Trump and Brexit come to mind). To what extent do you think non-response bias was the cause of this (write a page or two, being sure to ground your writing with citations)?
\item
  What is an estimate (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A rule for calculating an estimate of a given quantity based on observed data.
  \item
    The quantity of interest.
  \item
    The result.
  \item
    Unknown numbers that determine a statistical model.
  \end{enumerate}
\item
  What is an estimator (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A rule for calculating an estimate of a given quantity based on observed data.
  \item
    The quantity of interest.
  \item
    The result.
  \item
    Unknown numbers that determine a statistical model.
  \end{enumerate}
\item
  What is an estimand (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A rule for calculating an estimate of a given quantity based on observed data.
  \item
    The quantity of interest.
  \item
    The result.
  \item
    Unknown numbers that determine a statistical model.
  \end{enumerate}
\item
  What is a parameter (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A rule for calculating an estimate of a given quantity based on observed data.
  \item
    The quantity of interest.
  \item
    The result.
  \item
    Unknown numbers that determine a statistical model.
  \end{enumerate}
\item
  It seems like a lot of businesses have closed in downtown Toronto since the pandemic. To investigate this, I decide to walk along some blocks downtown and count the number of businesses that are closed and open. To decide which blocks to walk, I open a map of Toronto, start at the lake, and then pick every 10th street. This type of sampling is (select all)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Cluster sampling.
  \item
    Systematic sampling.
  \item
    Stratified sampling.
  \item
    Simple random sampling.
  \item
    Convenience sampling.
  \end{enumerate}
\item
  Please name some reasons why you may wish to use cluster sampling (select all)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Balance in responses.
  \item
    Administrative convenience.
  \item
    Efficiency in terms of money.
  \item
    Underlying systematic concerns.
  \item
    Estimation of sub-populations.
  \end{enumerate}
\item
  Please consider Beaumont, 2020, `Are probability surveys bound to disappear for the production of official statistics?'. With reference to that paper, do you think that probability surveys will disappear, and why or why not (please write a paragraph or two)?
\item
  \citet[p.~298]{ware1989} mentions `a randomized play the winner design'. What is it?
\item
  \citet[p.~299]{ware1989} mentions `adaptive randomization'. What is it, in your own words?
\item
  \citet[p.~299]{ware1989} mentions `randomized-consent'. He continues that it was `attractive in this setting because a standard approach to informed consent would require that parents of infants near death be approached to give informed consent for an invasive surgical procedure that would then, in some instances, not be administered. Those familiar with the agonizing experience of having a child in a neonatal intensive care unit can appreciate that the process of obtaining informed consent would be both frightening and stressful to parents'. To what extent do you agree with this position, especially given, as Ware (1989), p.~305, mentions `the need to withhold information about the study from parents of infants receiving CMT'?
\item
  \citet[p.~300]{ware1989} mentions `equipoise'. In your own words, please define and discuss it, using an example from your own experience.
\item
  What is power (in a statistical context)?
\end{enumerate}

\hypertarget{tutorial-8}{%
\subsection{Tutorial}\label{tutorial-8}}

The purpose of this tutorial is to ensure that it is clear in your mind how thoroughly you should know your dataset. It builds on the `memory palace' technique used by professional memorisers, as described by \citet{moonwalkingwitheinstein}.

Please think about your childhood home, or another building that you know intimately. Imagine yourself standing at the front of it. Describe what it looks like. Then `walk' into the front and throughout the house, again describing each aspect in as much detail as you can imagine. What are each of the rooms used for and what are their distinguishing features? How does it smell? What does this all evoke in you? Please write a page or two.

Now think about a dataset that you're interested in. Please do this same exercise, but for the dataset.

\hypertarget{farm-data}{%
\chapter{Farm data}\label{farm-data}}

\textbf{STATUS: Under construction.}

\textbf{Required reading}

\begin{itemize}
\tightlist
\item
  \citet{crawford}, Chapter 3.
\item
  \citet{statcanhistory}, Chapter 10 - Data quality assessment.
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\tightlist
\item
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
\end{itemize}

\textbf{Key functions/etc}

\begin{itemize}
\tightlist
\item
\end{itemize}

\hypertarget{overview}{%
\section{Overview}\label{overview}}

There are a variety of sources of data that have been produced for the purposes of being used as datasets. One thinks here especially of censuses. \citet[p.~30-31]{whitby} provides an enthralling overview, describing how `(t)he earliest censuses suggested in writing come\ldots{} from China's Yellow River valley' and that they were used for more than just purposes of taxation and conscription. He continues, highlighting the links between the census and Christianity, for instance from the Book of Luke `In those days Caesar Augustus issued a decree that a census should be taken of the entire Roman world', which led to David and Mary travelling to Bethlehem.

Taxation was a substantial motivator for censuses. \citet{jones1953census} describes how census records survive that `were probably engraved in the late third or early fourth century A.D., when Diocletian and his colleagues and successors are known to have been active in carrying out censuses to serve as the basis of their new system of taxation'. And detailed records of this sort have been abused. For instance, \citet{luebke1994locating} say how `(t)he Nazi regime gathered its information with two relatively conventional tools of modern administration: the national census and police registration'.

Another source of data deliberately put together to be a dataset include economic conditions such as unemployment, inflation, and GDP. Interestingly, \citet{rockoff2019controversies} describes how these economic statistics were not actually developed by the federal government, even though it `eventually took over the role of producing these statistics on a regular basis.' The broader point is that these types of datasets and censuses are typically put together by governments. They have the powers of the state behind them.

Another, similarly large and established source of data are from long-running large surveys. These are conducted on a regular basis, and while not usually directly conducted by the government, they are usually funded, one way or another, by the government. For instance, here we often think of electoral surveys, such as the Canadian Election Study, which has run in association with every federal election since 1965, and similarly the British Election Study which has been associated with every general election since 1964.

Finally, there has been a large push toward open data in government. While the term has become contentious because of how it has occurred in practice, the underlying principle - that the government should make available the data that is has - is undeniable.

In this chapter we cover these datasets, which I term `farmed data'. They are typically fairly nicely put together and the work of collecting, preparing and cleaning these datasets has typically been done. They are also, usually, conducted on a set release cycle. For instance, most developed countries release unemployment and inflation dataset on a monthly basis, GDP on a quarterly basis, and a census every five to ten years.

While these datasets have always been useful, they were developed for a time when much analysis was conducted without the use of scripts and programming languages. A cottage industry of R package development has sprung up around making it easier to get these datasets into R. In this chapter we cover a few that are especially useful.

It is important to recognise that data are not neutral. Thinking clearly about who is included in the dataset, and who is systematically excluded, is critical. As \citet[p.~121]{crawford} says:

\begin{quote}
The way data is understood, captured, classified, and named is fundamentally an act of world-making and containment. It has enormous ramifications for the way artificial intelligence works in the world and which communities are most affected. The myth of data collection as a benevolent practice in computer science has obscured its operations of power, protecting those who profit most while avoiding responsibility for its consequences.
\end{quote}

\hypertarget{censuses}{%
\section{Censuses}\label{censuses}}

MEasuring homelessness?

\url{https://www.ncbi.nlm.nih.gov/books/NBK218229/}

``S-Night survey conducted by the US Census Bureau in 1990''

``Street count surveys are used in many cities to count the number of homeless people in the streets at a point in time and gain a better understanding of the needs of homeless populations. In surveys such as the S-Night survey conducted by the US Census Bureau in 1990, enumerators are sent to pre-identified sites to enumerate homeless people while other survey personnel (``plants'') are planted among homeless people and indistinguishable from actual homeless. The ratio of plants seen by the enumerators to the number of plants deployed is used to inform the detection probability of homeless and provide an adjustment to the homeless undercount. In practice, one cannot know for sure which plants were seen because enumerators cannot distinguish between plants and homeless people. We can only rely on the plants' judgement of whether (yes, no or maybe) they think they were seen by an enumerator. The presence of ``maybes'' in the data leads to more unknown parameters than data points, which makes estimation of detection probabilities difficult. We propose to solve this problem by developing a Bayesian hierarchical model that uses hierarchical priors on detection probabilities across survey years and/or across cities. Such hierarchical modeling of the data is challenging because the data is available at various aggregated levels of the population (e.g., \texttt{seen} = plant AND seen + homeless AND seen, \texttt{maybes} = plant AND seen AND not interviewed AND maybe + plant AND not seen AND maybe.) The new methodology will be applied to a simulated reconstruction of the original S-night survey data and our estimates will be compared to those of the original analysis of the S-night data."

\hypertarget{canada}{%
\subsection{Canada}\label{canada}}

The first census in Canada was conducted in 1666. There were 3,215 inhabitants that were counted and the census `recorded their age, sex, marital status and occupation' \citep{statcanhistory}. In 1867 a decennial census was required to `determine representation by population in the new Parliament.' Regular censuses have occurred since then, the most recent in 2021.

In general, data from the Canadian census is not as easily available as in other countries. The `Individuals File, 2016 Census Public Use Microdata Files (PUMF)' - \url{https://www150.statcan.gc.ca/n1/en/catalogue/98M0001X} - is probably the best first step, although it does not provide much detail. It is a 2.7 per cent sample from the 2016 census. It is free, but must be requested, after which Statistics Canada will email access details.

Another way to access data from the Canadian census is to use \texttt{cancensus}, which is an R package that provides access to the Canadian census \citep{citecancensus}. The use of this package requires an API key, which can be requested by creating an account here: \url{https://censusmapper.ca/users/sign_up} and then clicking `edit profile'. The package has a helper function \texttt{cancensus::set\_api\_key("ADD\_YOUR\_API\_KEY\_HERE",\ install\ =\ TRUE)} that makes it easier to add the key to your `.Renviron' file. Once that is done you can use the package to access the data.

The main function in the package is \texttt{cancensus::get\_census()}, and that requires an argument for the census of interest, and a variety of other factors. In this example we will use the 2016 census

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(cancensus)}

\NormalTok{ontario\_population }\OtherTok{\textless{}{-}} 
  \FunctionTok{get\_census}\NormalTok{(}\AttributeTok{dataset =} \StringTok{"CA16"}\NormalTok{,}
             \AttributeTok{level =} \StringTok{"Regions"}\NormalTok{,}
             \AttributeTok{vectors =} \StringTok{"v\_CA16\_1"}\NormalTok{, }
             \AttributeTok{regions =} \FunctionTok{list}\NormalTok{(}\AttributeTok{PR=}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}35\textquotesingle{}}\NormalTok{)}
\NormalTok{                            )}
\NormalTok{             )}
\CommentTok{\#\textgreater{} }
\NormalTok{Downloading}\SpecialCharTok{:} \DecValTok{170}\NormalTok{ B     }
\NormalTok{Downloading}\SpecialCharTok{:} \DecValTok{170}\NormalTok{ B     }
\NormalTok{Downloading}\SpecialCharTok{:} \DecValTok{170}\NormalTok{ B     }
\NormalTok{Downloading}\SpecialCharTok{:} \DecValTok{170}\NormalTok{ B     }
\NormalTok{Downloading}\SpecialCharTok{:} \DecValTok{170}\NormalTok{ B     }
\NormalTok{Downloading}\SpecialCharTok{:} \DecValTok{170}\NormalTok{ B}

\FunctionTok{head}\NormalTok{(ontario\_population)}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 9}
\CommentTok{\#\textgreater{}   GeoUID Type  \textasciigrave{}Region Name\textasciigrave{} \textasciigrave{}Area (sq km)\textasciigrave{} Population Dwellings Households}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}  \textless{}fct\textgreater{} \textless{}fct\textgreater{}                  \textless{}dbl\textgreater{}      \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}      \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 35     PR    Ontario              986722.   13448494   5598391    5169174}
\CommentTok{\#\textgreater{} \# ... with 2 more variables: C\_UID \textless{}chr\textgreater{}, v\_CA16\_1: Age Stats \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

The package is fiddly, however it is worthwhile to ensure a reproducible workflow if the data that you are interested in are included. The two helper functions--- \texttt{list\_census\_regions()} and \texttt{list\_census\_vectors()} --- may be useful identify the arguments of interest.

\hypertarget{usa}{%
\subsection{USA}\label{usa}}

The requirement for a US Census is included in their constitution, and there is fairly decent, although clunky, general access. However, the US is in an interesting situation where there may actually be better options. For instance, the American Community Survey (ACS) is a survey that is comparable to the questions asked on many censuses that is conducted monthly. By the end of the year, it ends up with millions of responses. Although the ACS is smaller than a census, the advantage is that it is available on a more timely basis.

The best way to access the ACS is to use the Integrated Public Use Microdata Series (IPUMS). You will need to create an account, but it is free and easy to do.

Lol, just use IPUMS.

\hypertarget{australia}{%
\subsection{Australia}\label{australia}}

\hypertarget{other-government-data}{%
\section{Other government data}\label{other-government-data}}

\hypertarget{unemployment}{%
\subsection{Unemployment}\label{unemployment}}

\hypertarget{weather}{%
\subsection{Weather}\label{weather}}

\hypertarget{that-canadian-survey-that-we-used-in-sta304-from-the-library}{%
\subsection{That Canadian survey that we used in STA304 from the library}\label{that-canadian-survey-that-we-used-in-sta304-from-the-library}}

\hypertarget{open-government-data}{%
\section{Open Government Data}\label{open-government-data}}

\hypertarget{canada-1}{%
\subsection{Canada}\label{canada-1}}

\begin{itemize}
\item
  Canadian Government Open Data: \url{https://open.canada.ca/en/open-data}.
\item
  City of Toronto Open Data Portal
\end{itemize}

\hypertarget{electoral-studies}{%
\section{Electoral Studies}\label{electoral-studies}}

\hypertarget{canadian-electoral-study}{%
\subsection{Canadian Electoral Study}\label{canadian-electoral-study}}

\hypertarget{australian-electoral-study}{%
\subsection{Australian Electoral Study}\label{australian-electoral-study}}

\hypertarget{cces}{%
\subsection{CCES}\label{cces}}

\hypertarget{other-1}{%
\section{Other}\label{other-1}}

\begin{itemize}
\tightlist
\item
  University of Toronto Dataverse: \url{https://dataverse.scholarsportal.info/dataverse/toronto}.
\item
  Data is Plural structured archive: \url{https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/edit\#gid=0}.
\item
  Kaggle Datasets: \url{https://www.kaggle.com/datasets}.
\item
  Figure Eight: \url{https://www.figure-eight.com/data-for-everyone/}.
\item
  Google dataset search: \url{https://datasetsearch.research.google.com/}.
\item
  Awesome Data: \url{https://github.com/awesomedata/awesome-public-datasets}.
\end{itemize}

\hypertarget{exercises-and-tutorial-9}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-9}}

\hypertarget{exercises-9}{%
\subsection{Exercises}\label{exercises-9}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Please identify three other sources of data that you are interested in and describe where are they available (please include a link or code)?
\item
  Please focus on one of those sources. What steps do you have to go through in order to get a dataset that can be analysed in R?
\item
  Let's say you take a job at RBC (a Canadian bank) and they already have some quantitative data for you to use. What are some questions that you should explore when deciding whether that data will be useful to you?
\item
  Write three points (you are welcome to use dot points) about why government data may be especially useful?
\item
  Please pick a government of interest and find their inflation statistics. To what extent do you know about how these data were gathered?
\item
  With reference to \citet{chen2019forensic} and \citet{martinez2019much} to what extent do you think we can trust government statistics? Please mention at least three governments in your answer.
\item
  The 2021 census in Canada asked, firstly, `What was this person's sex at birth? Sex refers to sex assigned at birth. Male/Female', and then `What is this person's gender? Refers to current gender which may be different from sex assigned at birth and may be different from what is indicated on legal documents. Male/Female/Or please specify this person's gender (space for a typed or handwritten answer)'. With reference to \citet{statcan2020}, please discuss the extent to which you think this is an appropriate way for census to have proceeded. You are welcome to discuss the case of a different country if you are more familiar with that.
\item
  Pretend that we have conducted a survey of everyone in Canada, where we asked for age, sex, and gender. Your friend claims that there is no need to worry about uncertainty `because we have the whole population'. Is your friend right or wrong, and why?
\end{enumerate}

\hypertarget{tutorial-9}{%
\subsection{Tutorial}\label{tutorial-9}}

\hypertarget{part-preparation}{%
\part{Preparation}\label{part-preparation}}

\hypertarget{cleaning-and-preparing-data}{%
\chapter{Cleaning and preparing data}\label{cleaning-and-preparing-data}}

\textbf{STATUS: Under construction.}

\textbf{Required reading}

\begin{itemize}
\tightlist
\item
  \emph{Data Feminism}, Chapter 5 \citep{datafeminism2020}.
\item
  \emph{R for Data Science}, Chapter 9 \citep{r4ds}.
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
  \emph{We Gave Four Good Pollsters the Same Raw Data. They Had Four Different Results}, \citep{cohn2016}.
\item
  \emph{Data Cleaning Procedures for the 1993 Robert Wood Johnson Foundation Employer Health Insurance Survey}, \citep{euller1997data}.
\item
  \emph{Data Cleaning}, \citep{ilyas2019data}.
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\tightlist
\item
  Planning an end-point, and simulating the data that you'd like, are key elements of cleaning and preparing data.
\item
  Begin on a small sample of the dataset, write code to fix that, and then iterate and generalize to additional tranches.
\item
  Develop a series of tests and checks that your final dataset should pass so that the features of the dataset are clear.
\item
  Be especially concerned about the class of variables, having clear names, and that the values of each variable are as expected given all this.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{janitor} \citep{janitor}
\item
  \texttt{stringr} \citep{citestringr}
\item
  \texttt{tidyr} \citep{citetidyr}
\end{itemize}

\textbf{Key functions/etc}

\begin{itemize}
\tightlist
\item
  \texttt{dplyr::count()}
\item
  \texttt{dplyr::mutate()}
\item
  \texttt{dplyr::select()}
\item
  \texttt{janitor::clean\_names()}
\item
  \texttt{stringr::str\_replace\_all()}
\item
  \texttt{stringr::str\_trim()}
\item
  \texttt{tidyr::pivot\_longer()}
\item
  \texttt{tidyr::separate()}
\item
  \texttt{tidyr::separate\_rows()}
\end{itemize}

\hypertarget{introduction-14}{%
\section{Introduction}\label{introduction-14}}

\begin{quote}
``Well, Lyndon, you may be right and they may be every bit as intelligent as you say,'' said Rayburn, ``but I'd feel a whole lot better about them if just one of them had run for sheriff once.''

Sam Rayburn's reaction to Lyndon Johnson's enthusiasm about Kennedy's incoming cabinet, as quoted by \citet[p.~41]{halberstam}.
\end{quote}

In earlier chapters we have done data cleaning and preparation, but in this chapter, we get into the weeds. I do not trust anyone who works with data who has not spent time, at some point in their career, cleaning data. And by the end of this chapter I think that you'll feel the same way. To clean and prepare data is to make a lot of different decisions, many of which are important.

For a long time, data cleaning and preparation was largely overlooked. We now realise that was a mistake. It is no longer possible to trust almost any result in disciplines that apply statistics. The reproducibility crisis, which started in psychology but has now extended to many other fields in the physical and social sciences, has brought to light issues such as p-value `hacking', researcher degrees of freedom, file-drawer issues, and even data and results fabrication \citep{gelman2013garden}. Steps are now being put in place to address these. However, there has been relatively little focus on the data gathering, cleaning, and preparation aspects of applied statistics, despite evidence that decisions made during these steps greatly affect statistical results \citep{huntington2020influence}. In this chapter we focus on these issues.

While the statistical practices that underpin data science are themselves correct and robust when applied to `fake' datasets created in a simulated environment, data science is typically not conducted with these types of datasets. For instance, data scientists are interested in:

\begin{quote}
\ldots the kind of messy, unfiltered, and possibly unclean data---tainted by heteroskedasticity, complex dependence and missingness patterns---that until recently were avoided in polite conversations between more traditional statisticians\ldots{}

\citep{craiu2019hiring}.
\end{quote}

Big data does not resolve this issue, and may even exacerbate it, for instance `without taking data quality into account, population inferences with Big Data are subject to a Big Data Paradox: the more the data, the surer we fool ourselves' \citep{meng2018statistical}. It is important to note that the issues that are found in much applied statistics research are not necessarily associated with researcher quality, or their biases \citep{silberzahn2018many}. Instead, they are a result of the environment within which data science is conducted. This chapter aims to give you the tools to explicitly think about this work.

\citet{gelman2020most} writing about the most important statistical ideas of the past 50 years say that:

\begin{quote}
\ldots each of them was not so much a method for solving an existing problem, as an opening to new ways of thinking about statistics and new ways of data analysis. To put it another way, each of these ideas was a codification, bringing inside the tent an approach that had been considered more a matter of taste or philosophy than statistics.
\end{quote}

I see the focus on data cleaning and preparation in this chapter as analogous, insofar, as it represents a codification, or bringing inside the tent, of aspects that are typically (incorrectly) considered those of taste rather than statistics.

The approach that I recommend that you follow is:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Duplicate.
\item
  Plan the end state.
\item
  Execute that plan on a tiny sample.
\item
  Write tests and documentation.
\item
  Iterate the plan.
\item
  Generalize the execution.
\item
  Update tests and documentation.
\end{enumerate}

You will need to use all your skills to this point to be effective, but this is the very stuff of statistical sciences! Be dogged, but sensible. The best is the enemy of the good here. It's better to have 90 per cent of the data cleaned and prepared, and to start exploring that, before deciding whether it's worth the effort to clean and prepare the remaining 10 per cent because that remainder will likely take an awful lot of time and effort.

As \citet{van2005data} say, all data regardless of whether they were obtained from hunting, gathering, or farming, will have issues and it is critical that you understand how to `deal with errors from various sources' and understand `their effects on study results'. To clean data is to analyze data. As \citet{thatrandyauperson} says `The act of cleaning data is the act of preferentially transforming data so that your chosen analysis algorithm produces interpretable results. That is also the act of data analysis.'. We are attemping to triangulate the situation.

\hypertarget{workflow}{%
\section{Workflow}\label{workflow}}

\hypertarget{save-a-copy-of-the-raw-data}{%
\subsection{Save a copy of the raw data}\label{save-a-copy-of-the-raw-data}}

The first step is to save the raw data into a separate folder location. It is critical that you save the raw data to the extent that is possible \citep{wilsongoodenough}. In an ideal situation, duplicate the folder that contains the raw data, and then rename the duplicated folder `cleaned' and then only ever modify the data in that cleaned folder.

\hypertarget{begin-with-an-end-in-mind-1}{%
\subsection{Begin with an end in mind}\label{begin-with-an-end-in-mind-1}}

Planning the end state, or forcing yourself to begin with an end in mind is important for a variety of reasons. As with scraping data, it helps us to be pre-active about scope-creep, but with data cleaning I see a bigger benefit being that it forces us to really think about what we want the final dataset to look like. As before, I recommend first sketching the dataset. The key features of your sketch will be aspects such as the names of the columns, their class, and the possible range of values. It might look something like Figure \ref{fig:sketchdataplan} .

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/sketchofdataset} \caption{Example of a dataset end plan}\label{fig:sketchdataplan}
\end{figure}

Notice that this process has made it clear that we want the full names of the states, rather than abbreviations. And that population should be in millions, rather than some other units. The process of sketching out an end-point forces us to make decisions and be clear about our desired end state.

We then implement that using code to simulate data. Again, this process forces us to think about what reasonable values look like in our dataset because we are literally forced to decide which functions to use. Thinking carefully about the membership of each column here, for instance if the column is meant to be `gender' then values such as `male', `female', `other', and `unknown' may be expected, but a number such as `1,000' would likely be unexpected. It also forces us to be explicit about variable names because we have to assign the outputs of those functions to a variable.

Building on the example above, perhaps it might look something like this.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{simulated\_states\_population }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{state =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Alabama\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Alaska\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Arizona\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Arkansas\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}California\textquotesingle{}}\NormalTok{),}
    \AttributeTok{population =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{40}\NormalTok{)}
\NormalTok{  )}

\NormalTok{simulated\_states\_population}
\CommentTok{\#\textgreater{} \# A tibble: 5 x 2}
\CommentTok{\#\textgreater{}   state      population}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}           \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Alabama           5  }
\CommentTok{\#\textgreater{} 2 Alaska            0.7}
\CommentTok{\#\textgreater{} 3 Arizona           7  }
\CommentTok{\#\textgreater{} 4 Arkansas          3  }
\CommentTok{\#\textgreater{} 5 California       40}
\end{Highlighting}
\end{Shaded}

Our purpose, during data cleaning and preparation is to then bring our dataset close to that plan.

The desired end-state will typically be `tidy data' where \citep[p.~4]{wickham2014tidy}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each variable is a column
\item
  Each observation is a row
\item
  Each observational unit is a table.
\end{enumerate}

The following is an example of data in a tidy format:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{name =} \FunctionTok{c}\NormalTok{(}\StringTok{"Alfred"}\NormalTok{, }\StringTok{"Ben"}\NormalTok{, }\StringTok{"Chris"}\NormalTok{, }\StringTok{"Daniela"}\NormalTok{, }\StringTok{"Emma"}\NormalTok{),}
  \AttributeTok{age\_group =} \FunctionTok{c}\NormalTok{(}\StringTok{"0{-}9"}\NormalTok{, }\StringTok{"10{-}19"}\NormalTok{, }\StringTok{"0{-}9"}\NormalTok{, }\StringTok{"0{-}9"}\NormalTok{, }\StringTok{"10{-}19"}\NormalTok{),}
  \AttributeTok{response\_time =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{8}\NormalTok{),}
\NormalTok{  )}
\CommentTok{\#\textgreater{} \# A tibble: 5 x 3}
\CommentTok{\#\textgreater{}   name    age\_group response\_time}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}   \textless{}chr\textgreater{}             \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Alfred  0{-}9                   0}
\CommentTok{\#\textgreater{} 2 Ben     10{-}19                 9}
\CommentTok{\#\textgreater{} 3 Chris   0{-}9                   3}
\CommentTok{\#\textgreater{} 4 Daniela 0{-}9                   3}
\CommentTok{\#\textgreater{} 5 Emma    10{-}19                 8}
\end{Highlighting}
\end{Shaded}

\hypertarget{start-small}{%
\subsection{Start small}\label{start-small}}

At this point, we then have our first look at the data that we are dealing with. Regardless of what the raw data look like we want to try to get it into a rectangular dataset as quickly as possible because then we can use our familiar verbs. Let's assume here that we're starting with some \texttt{.txt} file.

The first step is to look for regularities in the dataset. We are wanting to end up with tabular data, which means that we need some type of deliminator to distinguish different columns. Ideally this might be features such as a comma, a semicolon, a tab, a double space, or a line break.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Alabama, 5 million.}
\CommentTok{\# Alaska, 0.7 million.}
\CommentTok{\# Arizona, 7 million.}
\CommentTok{\# Arkansas, 3 million.}
\CommentTok{\# California, 40 million.}
\end{Highlighting}
\end{Shaded}

In worse cases there may be a feature of the dataset that we can take advantage of. For instance, possibly the data look like the following:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# State is Alabama and population is 5 million.}
\CommentTok{\# State is Alaska and population is 0.7 million.}
\CommentTok{\# State is Arizona and population is 7 million.}
\CommentTok{\# State is Arkansas and population is 3 million.}
\CommentTok{\# State is California and population is 40 million.}
\end{Highlighting}
\end{Shaded}

In this case, although we don't have a traditional deliminator we can use the regularity of `State is' and ' and population is ' to get what we need.

A more difficult case is this following:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Alabama 5 Alaska 0.7 Arizona 7 Arkansas 3 California 40}
\end{Highlighting}
\end{Shaded}

One way to approach this is to take advantage of the different classes and values that we're looking for. For instance, in this case, we know that we're after US states, so there are only 50 possible options and we could use the existence of these as a deliminator. We could also use the fact that population is a number here, and so split based on a space followed by a number.

To move forward, I'll assume that last example and move it into a rectangular dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{raw\_data }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Alabama 5 Alaska 0.7 Arizona 7 Arkansas 3 California 40\textquotesingle{}}\NormalTok{)}

\NormalTok{data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{raw =}\NormalTok{ raw\_data)}

\NormalTok{data }\OtherTok{\textless{}{-}} 
\NormalTok{  data }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  tidyr}\SpecialCharTok{::}\FunctionTok{separate}\NormalTok{(}\AttributeTok{col =}\NormalTok{ raw, }
                  \AttributeTok{into =}\NormalTok{ letters[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{],}
                  \AttributeTok{sep =} \StringTok{"(?\textless{}=[[:digit:]]) "}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =}\NormalTok{ letters[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{],}
               \AttributeTok{names\_to =} \StringTok{"drop\_me"}\NormalTok{,}
               \AttributeTok{values\_to =} \StringTok{"separate\_me"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  tidyr}\SpecialCharTok{::}\FunctionTok{separate}\NormalTok{(}\AttributeTok{col =}\NormalTok{ separate\_me, }
                  \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}state\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}population\textquotesingle{}}\NormalTok{),}
                  \AttributeTok{sep =} \StringTok{" (?=[[:digit:]])"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{drop\_me)}

\NormalTok{data}
\CommentTok{\#\textgreater{} \# A tibble: 5 x 2}
\CommentTok{\#\textgreater{}   state      population}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}      \textless{}chr\textgreater{}     }
\CommentTok{\#\textgreater{} 1 Alabama    5         }
\CommentTok{\#\textgreater{} 2 Alaska     0.7       }
\CommentTok{\#\textgreater{} 3 Arizona    7         }
\CommentTok{\#\textgreater{} 4 Arkansas   3         }
\CommentTok{\#\textgreater{} 5 California 40}
\end{Highlighting}
\end{Shaded}

\hypertarget{write-tests-and-documentation.}{%
\subsection{Write tests and documentation.}\label{write-tests-and-documentation.}}

Having established a rectangular dataset, albeit a messy one, we should begin to look at the classes that we have. We don't necessarily want to fix the classes at this point, because that can result in us losing data. But we look at the class to see what it is, and then compare it to our simulated dataset to see where it needs to get to. We note the columns where it is different.

Before changing the class and before going onto more bespoke issues, you should deal with some of the common issues in each class. Some common issues are:

\begin{itemize}
\tightlist
\item
  Commas and other punctuation, such as denomination signs in columns that should be numeric.
\item
  Inconsistent formatting of dates, such as `December' and also `Dec' and `12'.
\item
  Unexpected characters, especially in unicode, which may not display consistently.
\end{itemize}

Typically, we want to fix anything immediately obvious. For instance, remove commas that have been used to group digits in currencies. However, the situation will typically quickly become dire. What we need to do is to look at the membership of each group, and then triage what we will fix. We should probably make the decision of how to triage based on what is likely to have the largest impact. That usually means starting with the counts, sorting in descending order, and then dealing with each as they come.

When the tests of membership are passed, then finally change the class, and run all the tests again. We're adapting this idea from the software development approach of unit testing. Tests are crucial because the enable us to understand whether software (or in this case data) is fit for purpose \citep{buildingsoftwaretogether}, Chapter `Testing'.

Let's run through an example with a collection of strings, some of which are slightly wrong. This type of output is typical of OCR, which often gets most of the way there, but not quite.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{messy\_string }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Rohan, Rhan, ROhan, Roham, ROhan, Rohan, Rohan, R0han, Rohan , 5Ohan\textquotesingle{}}\NormalTok{)}
\NormalTok{messy\_string}
\CommentTok{\#\textgreater{} [1] "Rohan, Rhan, ROhan, Roham, ROhan, Rohan, Rohan, R0han, Rohan , 5Ohan"}
\end{Highlighting}
\end{Shaded}

As before, we first want to get this into a rectangular dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{names =}\NormalTok{ messy\_string) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  tidyr}\SpecialCharTok{::}\FunctionTok{separate\_rows}\NormalTok{(names, }\AttributeTok{sep =} \StringTok{", "}\NormalTok{) }
\NormalTok{my\_data}
\CommentTok{\#\textgreater{} \# A tibble: 10 x 1}
\CommentTok{\#\textgreater{}    names   }
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}   }
\CommentTok{\#\textgreater{}  1 "Rohan" }
\CommentTok{\#\textgreater{}  2 "Rhan"  }
\CommentTok{\#\textgreater{}  3 "ROhan" }
\CommentTok{\#\textgreater{}  4 "Roham" }
\CommentTok{\#\textgreater{}  5 "ROhan" }
\CommentTok{\#\textgreater{}  6 "Rohan" }
\CommentTok{\#\textgreater{}  7 "Rohan" }
\CommentTok{\#\textgreater{}  8 "R0han" }
\CommentTok{\#\textgreater{}  9 "Rohan "}
\CommentTok{\#\textgreater{} 10 "5Ohan"}
\end{Highlighting}
\end{Shaded}

We now need decide which of these errors we are going to fix. To help us decide which are most important, we'll create a count.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(names, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 7 x 2}
\CommentTok{\#\textgreater{}   names        n}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}    \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 "Rohan"      3}
\CommentTok{\#\textgreater{} 2 "ROhan"      2}
\CommentTok{\#\textgreater{} 3 "5Ohan"      1}
\CommentTok{\#\textgreater{} 4 "R0han"      1}
\CommentTok{\#\textgreater{} 5 "Rhan"       1}
\CommentTok{\#\textgreater{} 6 "Roham"      1}
\CommentTok{\#\textgreater{} 7 "Rohan "     1}
\end{Highlighting}
\end{Shaded}

The most common element is the correct one, which is great. The next one - `ROhan' - looks like the `o' has been incorrectly capitalized, and the one after that - `5Ohan' - is distinguished by the `5' instead of an `R'. Let's quickly fix these issues and then redo the count.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  my\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{names =} \FunctionTok{str\_replace\_all}\NormalTok{(names, }\StringTok{\textquotesingle{}ROhan\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Rohan\textquotesingle{}}\NormalTok{),}
         \AttributeTok{names =} \FunctionTok{str\_replace\_all}\NormalTok{(names, }\StringTok{\textquotesingle{}5Ohan\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Rohan\textquotesingle{}}\NormalTok{)}
\NormalTok{         )}

\NormalTok{my\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(names, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 5 x 2}
\CommentTok{\#\textgreater{}   names        n}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}    \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 "Rohan"      6}
\CommentTok{\#\textgreater{} 2 "R0han"      1}
\CommentTok{\#\textgreater{} 3 "Rhan"       1}
\CommentTok{\#\textgreater{} 4 "Roham"      1}
\CommentTok{\#\textgreater{} 5 "Rohan "     1}
\end{Highlighting}
\end{Shaded}

Already this is much better and 60 per cent of the values are correct, compared with earlier where it was 30 per cent. There are two more obvious errors - `Rhan' and `Roham' - with the first missing an `o' and the second having an `m' where the `n' should be. Again, we can quickly update and fix those.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  my\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{names =} \FunctionTok{str\_replace\_all}\NormalTok{(names, }\StringTok{\textquotesingle{}Rhan\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Rohan\textquotesingle{}}\NormalTok{),}
         \AttributeTok{names =} \FunctionTok{str\_replace\_all}\NormalTok{(names, }\StringTok{\textquotesingle{}Roham\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Rohan\textquotesingle{}}\NormalTok{)}
\NormalTok{         )}

\NormalTok{my\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(names, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 2}
\CommentTok{\#\textgreater{}   names        n}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}    \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 "Rohan"      8}
\CommentTok{\#\textgreater{} 2 "R0han"      1}
\CommentTok{\#\textgreater{} 3 "Rohan "     1}
\end{Highlighting}
\end{Shaded}

We have achieved an 80 per cent fix with not too much effort. The two remaining issues are more subtle. The first - `R0han' - has occurred because the `o' has been incorrectly coded as an `0'. In some fonts this will show up, but in others it will be more difficult to see. This is a common issue, especially with OCR, and something to be aware of. The second - `Rohan' - is similarly subtle and is occurring because there is a trailing space. Again, trailing and leading spaces are a common issue. After we fix these two remaining issues then we will have all entries corrected.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  my\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{names =} \FunctionTok{str\_replace\_all}\NormalTok{(names, }\StringTok{\textquotesingle{}R0han\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Rohan\textquotesingle{}}\NormalTok{),}
         \AttributeTok{names =}\NormalTok{ stringr}\SpecialCharTok{::}\FunctionTok{str\_trim}\NormalTok{(names, }\AttributeTok{side =} \FunctionTok{c}\NormalTok{(}\StringTok{"right"}\NormalTok{))}
\NormalTok{         )}

\NormalTok{my\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(names, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 2}
\CommentTok{\#\textgreater{}   names     n}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{} \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 Rohan    10}
\end{Highlighting}
\end{Shaded}

I've been doing the tests in my head in this example - I know that we're hoping for `Rohan'. But we can start to document this test as well. One way is to look to see if values other than `Rohan' exist in the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{check\_me }\OtherTok{\textless{}{-}} 
\NormalTok{  my\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(names }\SpecialCharTok{!=} \StringTok{"Rohan"}\NormalTok{)}

\ControlFlowTok{if}\NormalTok{ (}\FunctionTok{nrow}\NormalTok{(check\_me) }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{) \{}
  \FunctionTok{print}\NormalTok{(}\StringTok{"Still have values that are not Rohan!"}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{iterate-generalize-and-update}{%
\subsection{Iterate, generalize and update}\label{iterate-generalize-and-update}}

We could now iterate the plan. In this most recent case we started with 10 entries. There is no reason that we couldn't increase this to 100 or even 1,000. We may need to generalize the cleaning proceedures and tests. But eventually we would start to being the dataset into some sort of order.

\hypertarget{case-study---kenya-census}{%
\section{Case study - Kenya census}\label{case-study---kenya-census}}

To make this all more clear, let's return to the Kenyan census that we downloaded as PDFs in Chapter \ref{gather-data}.

The distribution of population by age, sex, and administrative unit from the 2019 Kenyan census can be downloaded here: \url{https://www.knbs.or.ke/?wpdmpro=2019-kenya-population-and-housing-census-volume-iii-distribution-of-population-by-age-sex-and-administrative-units}.

And while it is great that they make it easily available, and it is easy to look-up a particular result, it is not overly useful to do larger-scale data analysis, such as building a Bayesian hierarchical model.

In this section we will convert a PDF of Kenyan census results of counts, by age and sex, by county and sub-county, into a tidy dataset that can be analysed. I will draw on and introduce a bunch of handy packages including: \texttt{janitor} by \citet{janitor}, \texttt{pdftools} by \citet{pdftools}, \texttt{tidyverse} by \citet{tidyverse}, and \texttt{stringi} by \citet{stringi}.

\hypertarget{set-up}{%
\subsection{Set-up}\label{set-up}}

To get started I need to load the necessary packages.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(janitor)}
\FunctionTok{library}\NormalTok{(pdftools)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(stringi)}
\end{Highlighting}
\end{Shaded}

And then I need to read in the PDF that I want to convert.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read in the PDF}
\NormalTok{all\_content }\OtherTok{\textless{}{-}}\NormalTok{ pdftools}\SpecialCharTok{::}\FunctionTok{pdf\_text}\NormalTok{(}\StringTok{"inputs/pdfs/2019\_Kenya\_census.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The \texttt{pdf\_text} function from \texttt{pdftools} is useful when you have a PDF and you want to read the content into R. For many recently produced PDFs it'll work pretty well, but there are alternatives. If the PDF is an image, then it won't work, and you'll need to turn to OCR.

You can see a page of the PDF here:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\StringTok{"figures/2020{-}04{-}10{-}screenshot{-}of{-}census.png"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.85\linewidth]{figures/2020-04-10-screenshot-of-census}

\hypertarget{extract}{%
\subsection{Extract}\label{extract}}

The first challenge is to get the dataset into a format that we can more easily manipulate. The way that I am going to do this is to consider each page of the PDF and extract the relevant parts. To do this, I first write a function that I want to apply to each page.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# The function is going to take an input of a page}
\NormalTok{get\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(i)\{}
  \CommentTok{\# Just look at the page of interest}
  \CommentTok{\# Based on https://stackoverflow.com/questions/47793326/tabulize{-}function{-}in{-}r}
\NormalTok{  just\_page\_i }\OtherTok{\textless{}{-}}\NormalTok{ stringi}\SpecialCharTok{::}\FunctionTok{stri\_split\_lines}\NormalTok{(all\_content[[i]])[[}\DecValTok{1}\NormalTok{]] }
  
  \CommentTok{\# Grab the name of the location}
\NormalTok{  area }\OtherTok{\textless{}{-}}\NormalTok{ just\_page\_i[}\DecValTok{3}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{str\_squish}\NormalTok{()}
\NormalTok{  area }\OtherTok{\textless{}{-}} \FunctionTok{str\_to\_title}\NormalTok{(area)}
  
  \CommentTok{\# Grab the type of table}
\NormalTok{  type\_of\_table }\OtherTok{\textless{}{-}}\NormalTok{ just\_page\_i[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{str\_squish}\NormalTok{()}
  
  \CommentTok{\# Get rid of the top matter}
\NormalTok{  just\_page\_i\_no\_header }\OtherTok{\textless{}{-}}\NormalTok{ just\_page\_i[}\DecValTok{5}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(just\_page\_i)] }\CommentTok{\# Just manually for now, but could create some rules if needed}
  
  \CommentTok{\# Get rid of the bottom matter}
\NormalTok{  just\_page\_i\_no\_header\_no\_footer }\OtherTok{\textless{}{-}}\NormalTok{ just\_page\_i\_no\_header[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{62}\NormalTok{] }\CommentTok{\# Just manually for now, but could create some rules if needed}
  
  \CommentTok{\# Convert into a tibble}
\NormalTok{  demography\_data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{all =}\NormalTok{ just\_page\_i\_no\_header\_no\_footer)}
  
  \CommentTok{\# \# Split columns}
\NormalTok{  demography\_data }\OtherTok{\textless{}{-}}
\NormalTok{    demography\_data }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{all =} \FunctionTok{str\_squish}\NormalTok{(all)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# Any space more than two spaces is squished down to one}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{all =} \FunctionTok{str\_replace}\NormalTok{(all, }\StringTok{"10 {-}14"}\NormalTok{, }\StringTok{"10{-}14"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{all =} \FunctionTok{str\_replace}\NormalTok{(all, }\StringTok{"Not Stated"}\NormalTok{, }\StringTok{"NotStated"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# Any space more than two spaces is squished down to one}
    \FunctionTok{separate}\NormalTok{(}\AttributeTok{col =}\NormalTok{ all,}
             \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"male"}\NormalTok{, }\StringTok{"female"}\NormalTok{, }\StringTok{"total"}\NormalTok{, }\StringTok{"age\_2"}\NormalTok{, }\StringTok{"male\_2"}\NormalTok{, }\StringTok{"female\_2"}\NormalTok{, }\StringTok{"total\_2"}\NormalTok{),}
             \AttributeTok{sep =} \StringTok{" "}\NormalTok{, }\CommentTok{\# Just looking for a space. Seems to work fine because the tables are pretty nicely laid out}
             \AttributeTok{remove =} \ConstantTok{TRUE}\NormalTok{,}
             \AttributeTok{fill =} \StringTok{"right"}
\NormalTok{    )}
  
  \CommentTok{\# They are side by side at the moment, need to append to bottom}
\NormalTok{  demography\_data\_long }\OtherTok{\textless{}{-}}
    \FunctionTok{rbind}\NormalTok{(demography\_data }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(age, male, female, total),}
\NormalTok{          demography\_data }\SpecialCharTok{\%\textgreater{}\%}
            \FunctionTok{select}\NormalTok{(age\_2, male\_2, female\_2, total\_2) }\SpecialCharTok{\%\textgreater{}\%}
            \FunctionTok{rename}\NormalTok{(}\AttributeTok{age =}\NormalTok{ age\_2, }\AttributeTok{male =}\NormalTok{ male\_2, }\AttributeTok{female =}\NormalTok{ female\_2, }\AttributeTok{total =}\NormalTok{ total\_2)}
\NormalTok{    )}
  
  \CommentTok{\# There is one row of NAs, so remove it}
\NormalTok{  demography\_data\_long }\OtherTok{\textless{}{-}} 
\NormalTok{    demography\_data\_long }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{    janitor}\SpecialCharTok{::}\FunctionTok{remove\_empty}\NormalTok{(}\AttributeTok{which =} \FunctionTok{c}\NormalTok{(}\StringTok{"rows"}\NormalTok{))}
  
  \CommentTok{\# Add the area and the page}
\NormalTok{  demography\_data\_long}\SpecialCharTok{$}\NormalTok{area }\OtherTok{\textless{}{-}}\NormalTok{ area}
\NormalTok{  demography\_data\_long}\SpecialCharTok{$}\NormalTok{table }\OtherTok{\textless{}{-}}\NormalTok{ type\_of\_table}
\NormalTok{  demography\_data\_long}\SpecialCharTok{$}\NormalTok{page }\OtherTok{\textless{}{-}}\NormalTok{ i}
  
  \FunctionTok{rm}\NormalTok{(just\_page\_i,}
\NormalTok{     i,}
\NormalTok{     area,}
\NormalTok{     type\_of\_table,}
\NormalTok{     just\_page\_i\_no\_header,}
\NormalTok{     just\_page\_i\_no\_header\_no\_footer,}
\NormalTok{     demography\_data)}
  
  \FunctionTok{return}\NormalTok{(demography\_data\_long)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

At this point, I have a function that does what I need to each page of the PDF. I'm going to use the function \texttt{map\_dfr} from the \texttt{purrr} package to apply that function to each page, and then combine all the outputs into one tibble.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Run through each relevant page and get the data}
\NormalTok{pages }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{30}\SpecialCharTok{:}\DecValTok{513}\NormalTok{)}
\NormalTok{all\_tables }\OtherTok{\textless{}{-}} \FunctionTok{map\_dfr}\NormalTok{(pages, get\_data)}
\FunctionTok{rm}\NormalTok{(pages, get\_data, all\_content)}
\end{Highlighting}
\end{Shaded}

\hypertarget{clean-2}{%
\subsection{Clean}\label{clean-2}}

I now need to clean the dataset to make it useful.

\hypertarget{values}{%
\subsubsection{Values}\label{values}}

The first step is to make the numbers into actual numbers, rather than characters. Before I can convert the type I need to remove anything that is not a number otherwise it'll be converted into an NA. I first identify any values that are not numbers so that I can remove them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Need to convert male, female, and total to integers}
\CommentTok{\# First find the characters that should not be in there}
\NormalTok{all\_tables }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(male, female, total) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate\_all}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\FunctionTok{str\_remove\_all}\NormalTok{(., }\StringTok{"[:digit:]"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate\_all}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\FunctionTok{str\_remove\_all}\NormalTok{(., }\StringTok{","}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate\_all}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\FunctionTok{str\_remove\_all}\NormalTok{(., }\StringTok{"\_"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate\_all}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\FunctionTok{str\_remove\_all}\NormalTok{(., }\StringTok{"{-}"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{distinct}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 39 x 3}
\CommentTok{\#\textgreater{}    male    female         total  }
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}   \textless{}chr\textgreater{}          \textless{}chr\textgreater{}  }
\CommentTok{\#\textgreater{}  1  \textless{}NA\textgreater{}    \textless{}NA\textgreater{}           \textless{}NA\textgreater{}  }
\CommentTok{\#\textgreater{}  2 ".:"    "Distribution" "of"   }
\CommentTok{\#\textgreater{}  3 "Male"  "Female"       "Total"}
\CommentTok{\#\textgreater{}  4 ""      ""             ""     }
\CommentTok{\#\textgreater{}  5 "by"    "Age"          "Sex*" }
\CommentTok{\#\textgreater{}  6 "LUNGA"  \textless{}NA\textgreater{}           \textless{}NA\textgreater{}  }
\CommentTok{\#\textgreater{}  7 "NORTH"  \textless{}NA\textgreater{}           \textless{}NA\textgreater{}  }
\CommentTok{\#\textgreater{}  8 "SOUTH"  \textless{}NA\textgreater{}           \textless{}NA\textgreater{}  }
\CommentTok{\#\textgreater{}  9 "RIVER"  \textless{}NA\textgreater{}           \textless{}NA\textgreater{}  }
\CommentTok{\#\textgreater{} 10 "DELTA"  \textless{}NA\textgreater{}           \textless{}NA\textgreater{}  }
\CommentTok{\#\textgreater{} \# ... with 29 more rows}
\CommentTok{\# We clearly need to remove ",", "\_", and "{-}". }
\CommentTok{\# This also highlights a few issues on p. 185 that need to be manually adjusted}
\CommentTok{\# https://twitter.com/RohanAlexander/status/1244337583016022018}
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{male[all\_tables}\SpecialCharTok{$}\NormalTok{male }\SpecialCharTok{==} \StringTok{"23{-}Jun"}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{4923}
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{male[all\_tables}\SpecialCharTok{$}\NormalTok{male }\SpecialCharTok{==} \StringTok{"15{-}Aug"}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{4611}
\end{Highlighting}
\end{Shaded}

While you could use the \texttt{janitor} package here, it is worthwhile at least first looking at what is going on because sometimes there is odd stuff that janitor (and other packages) will deal with, but not in a way that you want. In this case, they've used Excel or similar and this has converted a couple of their entries into dates. If we just took the numbers from the column then we'd have 23 and 15 here, but by inspecting the column we can use Excel to reverse the process and enter the correct values of 4,923 and 4,611, respectively.

Having identified everything that needs to be removed, we can do the actual removal and convert our character column of numbers to integers.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_tables }\OtherTok{\textless{}{-}}
\NormalTok{  all\_tables }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(male, female, total), }\SpecialCharTok{\textasciitilde{}}\FunctionTok{str\_remove\_all}\NormalTok{(., }\StringTok{","}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# First get rid of commas}
  \FunctionTok{mutate\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(male, female, total), }\SpecialCharTok{\textasciitilde{}}\FunctionTok{str\_replace}\NormalTok{(., }\StringTok{"\_"}\NormalTok{, }\StringTok{"0"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(male, female, total), }\SpecialCharTok{\textasciitilde{}}\FunctionTok{str\_replace}\NormalTok{(., }\StringTok{"{-}"}\NormalTok{, }\StringTok{"0"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(male, female, total), }\SpecialCharTok{\textasciitilde{}}\FunctionTok{as.integer}\NormalTok{(.))}
\end{Highlighting}
\end{Shaded}

\hypertarget{areas}{%
\subsubsection{Areas}\label{areas}}

The next thing to clean is the areas. We know that there are 47 counties in Kenya, and a whole bunch of sub-counties. They give us a list on pages 19 to 22 of the PDF (document pages 7 to 10). However, this list is not complete, and there are a few minor issues that we'll deal with later.

In any case, I first need to fix a few inconsistencies.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fix some area names}
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{area[all\_tables}\SpecialCharTok{$}\NormalTok{area }\SpecialCharTok{==} \StringTok{"Taita/ Taveta"}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"Taita/Taveta"}
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{area[all\_tables}\SpecialCharTok{$}\NormalTok{area }\SpecialCharTok{==} \StringTok{"Elgeyo/ Marakwet"}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"Elgeyo/Marakwet"}
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{area[all\_tables}\SpecialCharTok{$}\NormalTok{area }\SpecialCharTok{==} \StringTok{"Nairobi City"}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"Nairobi"}
\end{Highlighting}
\end{Shaded}

Kenya has 47 counties, each of which has sub-counties. The PDF has them arranged as the county data then the sub-counties, without designating which is which. We can use the names, to a certain extent, but in a handful of cases, there is a sub-county that has the same name as a county so we need to first fix that.

The PDF is made-up of three tables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{table }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{table}\NormalTok{()}
\CommentTok{\#\textgreater{} .}
\CommentTok{\#\textgreater{}       }
\CommentTok{\#\textgreater{} 59185}
\end{Highlighting}
\end{Shaded}

So I can first get the names of the counties based on those first two tables and then reconcile them to get a list of the counties.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Get a list of the counties }
\NormalTok{list\_counties }\OtherTok{\textless{}{-}} 
\NormalTok{  all\_tables }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(table }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"Table 2.4a: Distribution of Rural Population by Age, Sex* and County"}\NormalTok{,}
                      \StringTok{"Table 2.4b: Distribution of Urban Population by Age, Sex* and County"}\NormalTok{)}
\NormalTok{         ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(area) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{distinct}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

As I hoped, there are 47 of them. But before I can add a flag based on those names, I need to deal with the sub-counties that share their name. We will do this based on the page, then looking it up and deciding which is the county page and which is the sub-county page.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# The following have the issue of the name being used for both a county and a sub{-}county:}
\NormalTok{all\_tables }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(table }\SpecialCharTok{==} \StringTok{"Table 2.3: Distribution of Population by Age, Sex*, County and Sub{-} County"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(area }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"Busia"}\NormalTok{,}
                     \StringTok{"Garissa"}\NormalTok{,}
                     \StringTok{"Homa Bay"}\NormalTok{,}
                     \StringTok{"Isiolo"}\NormalTok{,}
                     \StringTok{"Kiambu"}\NormalTok{,}
                     \StringTok{"Machakos"}\NormalTok{,}
                     \StringTok{"Makueni"}\NormalTok{,}
                     \StringTok{"Samburu"}\NormalTok{,}
                     \StringTok{"Siaya"}\NormalTok{,}
                     \StringTok{"Tana River"}\NormalTok{,}
                     \StringTok{"Vihiga"}\NormalTok{,}
                     \StringTok{"West Pokot"}\NormalTok{)}
\NormalTok{         ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(area, page) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{distinct}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 0 x 2}
\CommentTok{\#\textgreater{} \# ... with 2 variables: area \textless{}chr\textgreater{}, page \textless{}int\textgreater{}}
\end{Highlighting}
\end{Shaded}

Now we can add the flag for whether the area is a county and adjust for the ones that are troublesome,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add flag for whether it is a county or a sub{-}county}
\NormalTok{all\_tables }\OtherTok{\textless{}{-}} 
\NormalTok{  all\_tables }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{area\_type =} \FunctionTok{if\_else}\NormalTok{(area }\SpecialCharTok{\%in\%}\NormalTok{ list\_counties}\SpecialCharTok{$}\NormalTok{area, }\StringTok{"county"}\NormalTok{, }\StringTok{"sub{-}county"}\NormalTok{))}
\CommentTok{\# Fix the flag for the ones that have their names used twice}
\NormalTok{all\_tables }\OtherTok{\textless{}{-}} 
\NormalTok{  all\_tables }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{area\_type =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{    area }\SpecialCharTok{==} \StringTok{"Samburu"} \SpecialCharTok{\&}\NormalTok{ page }\SpecialCharTok{==} \DecValTok{42} \SpecialCharTok{\textasciitilde{}} \StringTok{"sub{-}county"}\NormalTok{,}
\NormalTok{    area }\SpecialCharTok{==} \StringTok{"Tana River"} \SpecialCharTok{\&}\NormalTok{ page }\SpecialCharTok{==} \DecValTok{56} \SpecialCharTok{\textasciitilde{}} \StringTok{"sub{-}county"}\NormalTok{,}
\NormalTok{    area }\SpecialCharTok{==} \StringTok{"Garissa"} \SpecialCharTok{\&}\NormalTok{ page }\SpecialCharTok{==} \DecValTok{69} \SpecialCharTok{\textasciitilde{}} \StringTok{"sub{-}county"}\NormalTok{,}
\NormalTok{    area }\SpecialCharTok{==} \StringTok{"Isiolo"} \SpecialCharTok{\&}\NormalTok{ page }\SpecialCharTok{==} \DecValTok{100} \SpecialCharTok{\textasciitilde{}} \StringTok{"sub{-}county"}\NormalTok{,}
\NormalTok{    area }\SpecialCharTok{==} \StringTok{"Machakos"} \SpecialCharTok{\&}\NormalTok{ page }\SpecialCharTok{==} \DecValTok{154} \SpecialCharTok{\textasciitilde{}} \StringTok{"sub{-}county"}\NormalTok{,}
\NormalTok{    area }\SpecialCharTok{==} \StringTok{"Makueni"} \SpecialCharTok{\&}\NormalTok{ page }\SpecialCharTok{==} \DecValTok{164} \SpecialCharTok{\textasciitilde{}} \StringTok{"sub{-}county"}\NormalTok{,}
\NormalTok{    area }\SpecialCharTok{==} \StringTok{"Kiambu"} \SpecialCharTok{\&}\NormalTok{ page }\SpecialCharTok{==} \DecValTok{213} \SpecialCharTok{\textasciitilde{}} \StringTok{"sub{-}county"}\NormalTok{,}
\NormalTok{    area }\SpecialCharTok{==} \StringTok{"West Pokot"} \SpecialCharTok{\&}\NormalTok{ page }\SpecialCharTok{==} \DecValTok{233} \SpecialCharTok{\textasciitilde{}} \StringTok{"sub{-}county"}\NormalTok{,}
\NormalTok{    area }\SpecialCharTok{==} \StringTok{"Vihiga"} \SpecialCharTok{\&}\NormalTok{ page }\SpecialCharTok{==} \DecValTok{333} \SpecialCharTok{\textasciitilde{}} \StringTok{"sub{-}county"}\NormalTok{,}
\NormalTok{    area }\SpecialCharTok{==} \StringTok{"Busia"} \SpecialCharTok{\&}\NormalTok{ page }\SpecialCharTok{==} \DecValTok{353} \SpecialCharTok{\textasciitilde{}} \StringTok{"sub{-}county"}\NormalTok{,}
\NormalTok{    area }\SpecialCharTok{==} \StringTok{"Siaya"} \SpecialCharTok{\&}\NormalTok{ page }\SpecialCharTok{==} \DecValTok{360} \SpecialCharTok{\textasciitilde{}} \StringTok{"sub{-}county"}\NormalTok{,}
\NormalTok{    area }\SpecialCharTok{==} \StringTok{"Homa Bay"} \SpecialCharTok{\&}\NormalTok{ page }\SpecialCharTok{==} \DecValTok{375} \SpecialCharTok{\textasciitilde{}} \StringTok{"sub{-}county"}\NormalTok{,}
    \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}}\NormalTok{ area\_type}
\NormalTok{    )}
\NormalTok{  )}

\FunctionTok{rm}\NormalTok{(list\_counties)}
\end{Highlighting}
\end{Shaded}

\hypertarget{ages}{%
\subsubsection{Ages}\label{ages}}

Now we can deal with the ages.

First we need to fix some errors.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Clean up ages}
\FunctionTok{table}\NormalTok{(all\_tables}\SpecialCharTok{$}\NormalTok{age) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}           0   0{-}4     1    10 10{-}14 }
\CommentTok{\#\textgreater{}   392   484   484   484   484   482}
\FunctionTok{unique}\NormalTok{(all\_tables}\SpecialCharTok{$}\NormalTok{age) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] ""        "Table"   "MOMBASA" "Age"     "Total"   "0"}
\CommentTok{\# Looks like there should be 484, so need to follow up on some:}
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{age[all\_tables}\SpecialCharTok{$}\NormalTok{age }\SpecialCharTok{==} \StringTok{"NotStated"}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"Not Stated"}
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{age[all\_tables}\SpecialCharTok{$}\NormalTok{age }\SpecialCharTok{==} \StringTok{"43594"}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"5{-}9"}
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{age[all\_tables}\SpecialCharTok{$}\NormalTok{age }\SpecialCharTok{==} \StringTok{"43752"}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"10{-}14"}
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{age[all\_tables}\SpecialCharTok{$}\NormalTok{age }\SpecialCharTok{==} \StringTok{"9{-}14"}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"5{-}9"}
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{age[all\_tables}\SpecialCharTok{$}\NormalTok{age }\SpecialCharTok{==} \StringTok{"10{-}19"}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"10{-}14"}
\end{Highlighting}
\end{Shaded}

The census has done some of the work of putting together age-groups for us, but we want to make it easy to just focus on the counts by single-year-age. As such I'll add a flag as to the type of age it is: an age group, such as ages 0 to 5, or a single age, such as 1.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add a flag as to whether it\textquotesingle{}s a summary or not}
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{age\_type }\OtherTok{\textless{}{-}} \FunctionTok{if\_else}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(all\_tables}\SpecialCharTok{$}\NormalTok{age, }\FunctionTok{c}\NormalTok{(}\StringTok{"{-}"}\NormalTok{)), }\StringTok{"age{-}group"}\NormalTok{, }\StringTok{"single{-}year"}\NormalTok{)}
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{age\_type }\OtherTok{\textless{}{-}} \FunctionTok{if\_else}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(all\_tables}\SpecialCharTok{$}\NormalTok{age, }\FunctionTok{c}\NormalTok{(}\StringTok{"Total"}\NormalTok{)), }\StringTok{"age{-}group"}\NormalTok{, all\_tables}\SpecialCharTok{$}\NormalTok{age\_type)}
\end{Highlighting}
\end{Shaded}

At the moment, age is a character variable. We have a decision to make here, because we don't want it to be a character variable (because it won't graph properly), but we don't want it to be a numeric, because there is \texttt{total} and also \texttt{100+} in there. For now, we'll just make it into a factor, and at least that will be able to be nicely graphed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{age }\OtherTok{\textless{}{-}} \FunctionTok{as\_factor}\NormalTok{(all\_tables}\SpecialCharTok{$}\NormalTok{age)}
\end{Highlighting}
\end{Shaded}

\hypertarget{check}{%
\subsection{Check}\label{check}}

\hypertarget{gender-sum}{%
\subsubsection{Gender sum}\label{gender-sum}}

Given the format of the data, at this point it is easy to check that \texttt{total} is the sum of \texttt{male} and \texttt{female}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Check the parts and the sums}
\NormalTok{follow\_up }\OtherTok{\textless{}{-}} 
\NormalTok{  all\_tables }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{check\_sum =}\NormalTok{ male }\SpecialCharTok{+}\NormalTok{ female,}
         \AttributeTok{totals\_match =} \FunctionTok{if\_else}\NormalTok{(total }\SpecialCharTok{==}\NormalTok{ check\_sum, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{         ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(totals\_match }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

There is just one that seems wrong.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# There is just one that looks wrong}
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{male[all\_tables}\SpecialCharTok{$}\NormalTok{age }\SpecialCharTok{==} \StringTok{"10"} \SpecialCharTok{\&}\NormalTok{ all\_tables}\SpecialCharTok{$}\NormalTok{page }\SpecialCharTok{==} \DecValTok{187}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{as.integer}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\FunctionTok{rm}\NormalTok{(follow\_up)}
\end{Highlighting}
\end{Shaded}

\hypertarget{rural-urban-split}{%
\subsubsection{Rural-urban split}\label{rural-urban-split}}

The census provides different tables for the total of each county and sub-county; and then within each county, for the number in an urban area in that county, and the number in a urban area in that county. Some counties only have an urban count, but we'd like to make sure that the sum of rural and urban counts equals the total count. This requires reshaping the data from a long to wide format.

First, construct different tables for each of the three. I just do it manually, but I could probably do this a nicer way.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Table 2.3}
\NormalTok{table\_2\_3 }\OtherTok{\textless{}{-}}\NormalTok{ all\_tables }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(table }\SpecialCharTok{==} \StringTok{"Table 2.3: Distribution of Population by Age, Sex*, County and Sub{-} County"}\NormalTok{)}
\NormalTok{table\_2\_4a }\OtherTok{\textless{}{-}}\NormalTok{ all\_tables }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(table }\SpecialCharTok{==} \StringTok{"Table 2.4a: Distribution of Rural Population by Age, Sex* and County"}\NormalTok{)}
\NormalTok{table\_2\_4b }\OtherTok{\textless{}{-}}\NormalTok{ all\_tables }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(table }\SpecialCharTok{==} \StringTok{"Table 2.4b: Distribution of Urban Population by Age, Sex* and County"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Having constructed the constituent parts, I now join then based on age, area, and whether it is a county.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{both\_2\_4s }\OtherTok{\textless{}{-}} \FunctionTok{full\_join}\NormalTok{(table\_2\_4a, table\_2\_4b, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"area"}\NormalTok{, }\StringTok{"area\_type"}\NormalTok{), }\AttributeTok{suffix =} \FunctionTok{c}\NormalTok{(}\StringTok{"\_rural"}\NormalTok{, }\StringTok{"\_urban"}\NormalTok{))}

\NormalTok{all }\OtherTok{\textless{}{-}} \FunctionTok{full\_join}\NormalTok{(table\_2\_3, both\_2\_4s, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"area"}\NormalTok{, }\StringTok{"area\_type"}\NormalTok{), }\AttributeTok{suffix =} \FunctionTok{c}\NormalTok{(}\StringTok{"\_all"}\NormalTok{, }\StringTok{"\_"}\NormalTok{))}

\NormalTok{all }\OtherTok{\textless{}{-}} 
\NormalTok{  all }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{page =}\NormalTok{ glue}\SpecialCharTok{::}\FunctionTok{glue}\NormalTok{(}\StringTok{\textquotesingle{}Total from p. \{page\}, rural from p. \{page\_rural\}, urban from p. \{page\_urban\}\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{page, }\SpecialCharTok{{-}}\NormalTok{page\_rural, }\SpecialCharTok{{-}}\NormalTok{page\_urban,}
         \SpecialCharTok{{-}}\NormalTok{table, }\SpecialCharTok{{-}}\NormalTok{table\_rural, }\SpecialCharTok{{-}}\NormalTok{table\_urban,}
         \SpecialCharTok{{-}}\NormalTok{age\_type\_rural, }\SpecialCharTok{{-}}\NormalTok{age\_type\_urban}
\NormalTok{         )}

\FunctionTok{rm}\NormalTok{(both\_2\_4s, table\_2\_3, table\_2\_4a, table\_2\_4b)}
\end{Highlighting}
\end{Shaded}

We can now check that the sum of rural and urban is the same as the total.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Check that the urban + rural = total}
\NormalTok{follow\_up }\OtherTok{\textless{}{-}} 
\NormalTok{  all }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{total\_from\_bits =}\NormalTok{ total\_rural }\SpecialCharTok{+}\NormalTok{ total\_urban,}
         \AttributeTok{check\_total\_is\_rural\_plus\_urban =} \FunctionTok{if\_else}\NormalTok{(total }\SpecialCharTok{==}\NormalTok{ total\_from\_bits, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{),}
\NormalTok{         total\_from\_bits }\SpecialCharTok{{-}}\NormalTok{ total) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(check\_total\_is\_rural\_plus\_urban }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)}

\FunctionTok{head}\NormalTok{(follow\_up)}
\CommentTok{\#\textgreater{} \# A tibble: 0 x 16}
\CommentTok{\#\textgreater{} \# ... with 16 variables: age \textless{}fct\textgreater{}, male \textless{}int\textgreater{}, female \textless{}int\textgreater{}, total \textless{}int\textgreater{},}
\CommentTok{\#\textgreater{} \#   area \textless{}chr\textgreater{}, area\_type \textless{}chr\textgreater{}, age\_type \textless{}chr\textgreater{}, male\_rural \textless{}int\textgreater{},}
\CommentTok{\#\textgreater{} \#   female\_rural \textless{}int\textgreater{}, total\_rural \textless{}int\textgreater{}, male\_urban \textless{}int\textgreater{},}
\CommentTok{\#\textgreater{} \#   female\_urban \textless{}int\textgreater{}, total\_urban \textless{}int\textgreater{}, total\_from\_bits \textless{}int\textgreater{},}
\CommentTok{\#\textgreater{} \#   check\_total\_is\_rural\_plus\_urban \textless{}dbl\textgreater{}, total\_from\_bits {-} total \textless{}int\textgreater{}}
\FunctionTok{rm}\NormalTok{(follow\_up)}
\end{Highlighting}
\end{Shaded}

There are just a few, but they only have a difference of 1, so I'll just move on.

\hypertarget{ages-sum-to-age-groups}{%
\subsubsection{Ages sum to age-groups}\label{ages-sum-to-age-groups}}

Finally, I want to check that the single age counts sum to the age-groups.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# One last thing to check is that the ages sum to their age{-}groups.}
\NormalTok{follow\_up }\OtherTok{\textless{}{-}} 
\NormalTok{  all }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{groups =} \FunctionTok{case\_when}\NormalTok{(age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"0"}\NormalTok{, }\StringTok{"1"}\NormalTok{, }\StringTok{"2"}\NormalTok{, }\StringTok{"3"}\NormalTok{, }\StringTok{"4"}\NormalTok{, }\StringTok{"0{-}4"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"0{-}4"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"5"}\NormalTok{, }\StringTok{"6"}\NormalTok{, }\StringTok{"7"}\NormalTok{, }\StringTok{"8"}\NormalTok{, }\StringTok{"9"}\NormalTok{, }\StringTok{"5{-}9"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"5{-}9"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"10"}\NormalTok{, }\StringTok{"11"}\NormalTok{, }\StringTok{"12"}\NormalTok{, }\StringTok{"13"}\NormalTok{, }\StringTok{"14"}\NormalTok{, }\StringTok{"10{-}14"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"10{-}14"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"15"}\NormalTok{, }\StringTok{"16"}\NormalTok{, }\StringTok{"17"}\NormalTok{, }\StringTok{"18"}\NormalTok{, }\StringTok{"19"}\NormalTok{, }\StringTok{"15{-}19"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"15{-}19"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"20"}\NormalTok{, }\StringTok{"21"}\NormalTok{, }\StringTok{"22"}\NormalTok{, }\StringTok{"23"}\NormalTok{, }\StringTok{"24"}\NormalTok{, }\StringTok{"20{-}24"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"20{-}24"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"25"}\NormalTok{, }\StringTok{"26"}\NormalTok{, }\StringTok{"27"}\NormalTok{, }\StringTok{"28"}\NormalTok{, }\StringTok{"29"}\NormalTok{, }\StringTok{"25{-}29"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"25{-}29"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"30"}\NormalTok{, }\StringTok{"31"}\NormalTok{, }\StringTok{"32"}\NormalTok{, }\StringTok{"33"}\NormalTok{, }\StringTok{"34"}\NormalTok{, }\StringTok{"30{-}34"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"30{-}34"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"35"}\NormalTok{, }\StringTok{"36"}\NormalTok{, }\StringTok{"37"}\NormalTok{, }\StringTok{"38"}\NormalTok{, }\StringTok{"39"}\NormalTok{, }\StringTok{"35{-}39"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"35{-}39"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"40"}\NormalTok{, }\StringTok{"41"}\NormalTok{, }\StringTok{"42"}\NormalTok{, }\StringTok{"43"}\NormalTok{, }\StringTok{"44"}\NormalTok{, }\StringTok{"40{-}44"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"40{-}44"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"45"}\NormalTok{, }\StringTok{"46"}\NormalTok{, }\StringTok{"47"}\NormalTok{, }\StringTok{"48"}\NormalTok{, }\StringTok{"49"}\NormalTok{, }\StringTok{"45{-}49"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"45{-}49"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"50"}\NormalTok{, }\StringTok{"51"}\NormalTok{, }\StringTok{"52"}\NormalTok{, }\StringTok{"53"}\NormalTok{, }\StringTok{"54"}\NormalTok{, }\StringTok{"50{-}54"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"50{-}54"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"55"}\NormalTok{, }\StringTok{"56"}\NormalTok{, }\StringTok{"57"}\NormalTok{, }\StringTok{"58"}\NormalTok{, }\StringTok{"59"}\NormalTok{, }\StringTok{"55{-}59"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"55{-}59"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"60"}\NormalTok{, }\StringTok{"61"}\NormalTok{, }\StringTok{"62"}\NormalTok{, }\StringTok{"63"}\NormalTok{, }\StringTok{"64"}\NormalTok{, }\StringTok{"60{-}64"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"60{-}64"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"65"}\NormalTok{, }\StringTok{"66"}\NormalTok{, }\StringTok{"67"}\NormalTok{, }\StringTok{"68"}\NormalTok{, }\StringTok{"69"}\NormalTok{, }\StringTok{"65{-}69"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"65{-}69"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"70"}\NormalTok{, }\StringTok{"71"}\NormalTok{, }\StringTok{"72"}\NormalTok{, }\StringTok{"73"}\NormalTok{, }\StringTok{"74"}\NormalTok{, }\StringTok{"70{-}74"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"70{-}74"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"75"}\NormalTok{, }\StringTok{"76"}\NormalTok{, }\StringTok{"77"}\NormalTok{, }\StringTok{"78"}\NormalTok{, }\StringTok{"79"}\NormalTok{, }\StringTok{"75{-}79"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"75{-}79"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"80"}\NormalTok{, }\StringTok{"81"}\NormalTok{, }\StringTok{"82"}\NormalTok{, }\StringTok{"83"}\NormalTok{, }\StringTok{"84"}\NormalTok{, }\StringTok{"80{-}84"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"80{-}84"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"85"}\NormalTok{, }\StringTok{"86"}\NormalTok{, }\StringTok{"87"}\NormalTok{, }\StringTok{"88"}\NormalTok{, }\StringTok{"89"}\NormalTok{, }\StringTok{"85{-}89"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"85{-}89"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"90"}\NormalTok{, }\StringTok{"91"}\NormalTok{, }\StringTok{"92"}\NormalTok{, }\StringTok{"93"}\NormalTok{, }\StringTok{"94"}\NormalTok{, }\StringTok{"90{-}94"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"90{-}94"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"95"}\NormalTok{, }\StringTok{"96"}\NormalTok{, }\StringTok{"97"}\NormalTok{, }\StringTok{"98"}\NormalTok{, }\StringTok{"99"}\NormalTok{, }\StringTok{"95{-}99"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"95{-}99"}\NormalTok{,}
                            \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{"Other"}\NormalTok{)}
\NormalTok{         ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(area\_type, area, groups) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{group\_sum =} \FunctionTok{sum}\NormalTok{(total, }\AttributeTok{na.rm =} \ConstantTok{FALSE}\NormalTok{),}
         \AttributeTok{group\_sum =}\NormalTok{ group\_sum }\SpecialCharTok{/} \DecValTok{2}\NormalTok{,}
         \AttributeTok{difference =}\NormalTok{ total }\SpecialCharTok{{-}}\NormalTok{ group\_sum) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(age }\SpecialCharTok{==}\NormalTok{ groups) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(total }\SpecialCharTok{!=}\NormalTok{ group\_sum) }

\FunctionTok{head}\NormalTok{(follow\_up)}
\CommentTok{\#\textgreater{} \# A tibble: 0 x 16}
\CommentTok{\#\textgreater{} \# ... with 16 variables: age \textless{}fct\textgreater{}, male \textless{}int\textgreater{}, female \textless{}int\textgreater{}, total \textless{}int\textgreater{},}
\CommentTok{\#\textgreater{} \#   area \textless{}chr\textgreater{}, area\_type \textless{}chr\textgreater{}, age\_type \textless{}chr\textgreater{}, male\_rural \textless{}int\textgreater{},}
\CommentTok{\#\textgreater{} \#   female\_rural \textless{}int\textgreater{}, total\_rural \textless{}int\textgreater{}, male\_urban \textless{}int\textgreater{},}
\CommentTok{\#\textgreater{} \#   female\_urban \textless{}int\textgreater{}, total\_urban \textless{}int\textgreater{}, groups \textless{}chr\textgreater{}, group\_sum \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   difference \textless{}dbl\textgreater{}}

\FunctionTok{rm}\NormalTok{(follow\_up)}
\end{Highlighting}
\end{Shaded}

Mt. Kenya Forest, Aberdare Forest, Kakamega Forest are all slightly dodgy. I can't see it in the documentation, but it looks like they have apportioned these between various countries. It's understandable why they'd do this and it's probably not a big deal, so I'll just move on.

\hypertarget{tidy-up}{%
\subsection{Tidy-up}\label{tidy-up}}

Now that we are confident that everything is looking good, we can just convert it to long-format so that it is easy to work with.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all }\OtherTok{\textless{}{-}} 
\NormalTok{  all }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{male\_total =}\NormalTok{ male,}
         \AttributeTok{female\_total =}\NormalTok{ female,}
         \AttributeTok{total\_total =}\NormalTok{ total) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(male\_total, female\_total, total\_total, male\_rural, female\_rural, total\_rural, male\_urban, female\_urban, total\_urban),}
               \AttributeTok{names\_to =} \StringTok{"type"}\NormalTok{,}
               \AttributeTok{values\_to =} \StringTok{"number"}
\NormalTok{               ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{separate}\NormalTok{(}\AttributeTok{col =}\NormalTok{ type, }\AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"gender"}\NormalTok{, }\StringTok{"part\_of\_area"}\NormalTok{), }\AttributeTok{sep =} \StringTok{"\_"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(area, area\_type, part\_of\_area, age, age\_type, gender, number)}

\FunctionTok{write\_csv}\NormalTok{(all, }\AttributeTok{path =} \StringTok{"outputs/data/cleaned\_kenya\_2019\_census.csv"}\NormalTok{)}

\FunctionTok{head}\NormalTok{(all)}
\CommentTok{\#\textgreater{} \# A tibble: 0 x 7}
\CommentTok{\#\textgreater{} \# ... with 7 variables: area \textless{}chr\textgreater{}, area\_type \textless{}chr\textgreater{}, part\_of\_area \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   age \textless{}fct\textgreater{}, age\_type \textless{}chr\textgreater{}, gender \textless{}chr\textgreater{}, number \textless{}int\textgreater{}}
\end{Highlighting}
\end{Shaded}

\hypertarget{make-monicas-dataset}{%
\subsection{Make Monica's dataset}\label{make-monicas-dataset}}

The original purpose of all of this was to make a table for Monica. She needed single-year counts, by gender, for the counties.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{monicas\_dataset }\OtherTok{\textless{}{-}} 
\NormalTok{  all }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(area\_type }\SpecialCharTok{==} \StringTok{"county"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(part\_of\_area }\SpecialCharTok{==} \StringTok{"total"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(age\_type }\SpecialCharTok{==} \StringTok{"single{-}year"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(area, age, gender, number)}

\FunctionTok{head}\NormalTok{(monicas\_dataset)}
\CommentTok{\#\textgreater{} \# A tibble: 0 x 4}
\CommentTok{\#\textgreater{} \# ... with 4 variables: area \textless{}chr\textgreater{}, age \textless{}fct\textgreater{}, gender \textless{}chr\textgreater{}, number \textless{}int\textgreater{}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write\_csv}\NormalTok{(monicas\_dataset, }\StringTok{"outputs/data/monicas\_dataset.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

I'll leave the fancy stats to Monica, but I'll just make a quick graph of Nairobi.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# monicas\_dataset \%\textgreater{}\% }
\CommentTok{\#   filter(area == "Nairobi") \%\textgreater{}\%}
\CommentTok{\#   ggplot() +}
\CommentTok{\#   geom\_col(aes(x = age, y = number, fill = gender), position = "dodge") + }
\CommentTok{\#   scale\_y\_continuous(labels = scales::comma) +}
\CommentTok{\#   scale\_x\_discrete(breaks = c(seq(from = 0, to = 99, by = 5), "100+")) +}
\CommentTok{\#   theme\_classic()+}
\CommentTok{\#   scale\_fill\_brewer(palette = "Set1") +}
\CommentTok{\#   labs(y = "Number",}
\CommentTok{\#        x = "Age",}
\CommentTok{\#        fill = "Gender",}
\CommentTok{\#        title = "Distribution of age and gender in Nairobi in 2019",}
\CommentTok{\#        caption = "Data source: 2019 Kenya Census")}
\end{Highlighting}
\end{Shaded}

\hypertarget{checks-and-tests}{%
\section{Checks and tests}\label{checks-and-tests}}

Robert Caro, the biographer of Lyndon Johnson, spent years tracking down everyone connected to the 36th President of the United States. He went to far as to live in Texas Hill Country for X years so that he could better understand where LBJ was from. When he heard a story that LBJ used to run to the Senate when he worked as a Y, he ran that route multiple times himself to try to understand why LBJ was running. Caro eventually understood it only when he ran the route as the sun was rising, just as LBJ had done, and found that at that time the sun hits the Senate Rotunda and it looks amazing (CITE). This background work enabled him to uncover aspects that no one else knew. For instance, it turns out that LBJ almost surely stole his first election win as a Texas Senator. You need to understand your data to this same extent. Turn every page and go to every extreme.

When we are cleaning data, we are looking for anomalies. We are interested in values that are in there that should not be, but also the opposite situation---values that are missing that should not be. I've I talked fairly generally about checks, tests and other considerations. Here I'd like to be more specific about what I mean. There are four tools that you should use to identify these situations: plots, counts, green/red conditions, targets.

\hypertarget{plots}{%
\subsection{Plots}\label{plots}}

Plots are an invaluable tool when cleaning data, because they show each point in the dataset, in relation to the other points. They are especially useful for identifying when a value doesn't belong. For instance, if a value is expected to be numerical, but it is still a character then it will not plot and a warning will be displayed.

Plots will be especially useful for numerical data, but are still useful for text and categorical data. Let's pretend that we have a situation where we are interested in a person's age, for some youth survey. We have the following data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{raw\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{ages =} \FunctionTok{c}\NormalTok{(}\DecValTok{11}\NormalTok{, }\DecValTok{17}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{150}\NormalTok{))}

\NormalTok{raw\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ ages, }\AttributeTok{x =} \DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{30-clean_and_prepare_files/figure-latex/unnamed-chunk-40-1.pdf}

The graph clearly shows the unexpected value of 150. The most likely explanation is that the data were incorrectly entered with a trailing 0, and should be 15.

We can fix that, and document it, and then redo the graph, so see that everything seems more reasonable now.

\hypertarget{counts}{%
\subsection{Counts}\label{counts}}

We want to focus on getting most of the data right. So we are interested in the counts of unique values. Hopefully a majority of the data are concentrated in the most common counts. But it can also be useful to invert it, and see what is especially uncommon. The extent to which we want to deal with these depends on what we need. Ultimately, each time we fix one we are getting very few additional observations, potentially even just one! Counts are especially useful with text or categorical data, but can be helpful with numerical as well.

Let's see an example.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{raw\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{country =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Australie\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Austrelia\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Australie\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Australie\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Aeustralia\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Austraia\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Australia\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Australia\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Australia\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Australia\textquotesingle{}}
\NormalTok{                  )}
\NormalTok{         )}

\NormalTok{raw\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(country, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 5 x 2}
\CommentTok{\#\textgreater{}   country        n}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}      \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 Australia      4}
\CommentTok{\#\textgreater{} 2 Australie      3}
\CommentTok{\#\textgreater{} 3 Aeustralia     1}
\CommentTok{\#\textgreater{} 4 Austraia       1}
\CommentTok{\#\textgreater{} 5 Austrelia      1}
\end{Highlighting}
\end{Shaded}

The use of this count clearly identifies where we should spend our time - changing `Australie' to `Australia' would almost double our amount of useable data.

\hypertarget{gono-go}{%
\subsection{Go/no-go}\label{gono-go}}

Some things are so important that you require that your cleaned dataset have them. These are go/no-go conditions. They would typically come out of experience, expert knowledge, or the planning and simulation exercises. An example may be that there are no negative numbers in an age column, and no ages above 140.

For these we could specifically require that the condition is met. Other examples include:

\begin{itemize}
\tightlist
\item
  If doing cross-country analysis, then a list of country names that we know should be in our dataset would be useful. Our no-go conditions would then be if there were: 1) values not in that list in our dataset, or, vice versa; 2) countries that we expected to be in there that were not.
\end{itemize}

To have a concrete example, let's consider if we were doing some analysis about the five largest counties in Kenya: `Nairobi', `Kiambu', `Nakuru', `Kakamega', `Bungoma'. Let's create that array first:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{correct\_counties }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Nairobi\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Kiambu\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Nakuru\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Kakamega\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Bungoma\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We begin with the following dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{top\_five\_kenya }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{county =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Nairobi\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Nairob1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Nakuru\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Kakamega\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Nakuru\textquotesingle{}}\NormalTok{, }
                      \StringTok{\textquotesingle{}Kiambu\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Kiambru\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Kabamega\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Bun8oma\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Bungoma\textquotesingle{}}\NormalTok{)}
\NormalTok{  )}

\NormalTok{top\_five\_kenya }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(county, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 9 x 2}
\CommentTok{\#\textgreater{}   county       n}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}    \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 Nakuru       2}
\CommentTok{\#\textgreater{} 2 Bun8oma      1}
\CommentTok{\#\textgreater{} 3 Bungoma      1}
\CommentTok{\#\textgreater{} 4 Kabamega     1}
\CommentTok{\#\textgreater{} 5 Kakamega     1}
\CommentTok{\#\textgreater{} 6 Kiambru      1}
\CommentTok{\#\textgreater{} 7 Kiambu       1}
\CommentTok{\#\textgreater{} 8 Nairob1      1}
\CommentTok{\#\textgreater{} 9 Nairobi      1}
\end{Highlighting}
\end{Shaded}

Based on the count we know that we have to fix some of them and there are two with numbers that are obvious fixes:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{top\_five\_kenya }\OtherTok{\textless{}{-}} 
\NormalTok{  top\_five\_kenya }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{county =} \FunctionTok{str\_replace\_all}\NormalTok{(county, }\StringTok{\textquotesingle{}Nairob1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Nairobi\textquotesingle{}}\NormalTok{),}
         \AttributeTok{county =} \FunctionTok{str\_replace\_all}\NormalTok{(county, }\StringTok{\textquotesingle{}Bun8oma\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Nairobi\textquotesingle{}}\NormalTok{)}
\NormalTok{  )}

\NormalTok{top\_five\_kenya }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(county, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 7 x 2}
\CommentTok{\#\textgreater{}   county       n}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}    \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 Nairobi      3}
\CommentTok{\#\textgreater{} 2 Nakuru       2}
\CommentTok{\#\textgreater{} 3 Bungoma      1}
\CommentTok{\#\textgreater{} 4 Kabamega     1}
\CommentTok{\#\textgreater{} 5 Kakamega     1}
\CommentTok{\#\textgreater{} 6 Kiambru      1}
\CommentTok{\#\textgreater{} 7 Kiambu       1}
\end{Highlighting}
\end{Shaded}

At this point we can use our go/no-go conditions to decide whether we are finished or not.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{top\_five\_kenya}\SpecialCharTok{$}\NormalTok{county }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{unique}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] "Nairobi"  "Nakuru"   "Kakamega" "Kiambu"   "Kiambru"  "Kabamega" "Bungoma"}

\ControlFlowTok{if}\NormalTok{(}\FunctionTok{all}\NormalTok{(top\_five\_kenya}\SpecialCharTok{$}\NormalTok{county }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{unique}\NormalTok{() }\SpecialCharTok{==}\NormalTok{ top\_five\_kenya)) \{}
  \StringTok{"Oh no"}
\NormalTok{\}}
\ControlFlowTok{if}\NormalTok{(}\FunctionTok{all}\NormalTok{(top\_five\_kenya}\SpecialCharTok{==}\NormalTok{top\_five\_kenya}\SpecialCharTok{$}\NormalTok{county }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{unique}\NormalTok{()) ) \{}
  \StringTok{"Oh no"}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

And so it is clear that we still have cleaning to do!

We may also find similar conditions from experts and those with experience in the particular field.

\hypertarget{class-1}{%
\subsection{Class}\label{class-1}}

I can't emphasize this enough, but it is vital that you put in place explicit checks of class because getting this wrong can have a large effect on your analysis. In particular:

\begin{itemize}
\tightlist
\item
  check whether some value should be a number or a factor.
\item
  check that dates are correctly formatted.
\end{itemize}

To understand why it is important to be clear about whether a value is a number or a factor, consider the following situation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{some\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{response =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{),}
         \AttributeTok{group =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Let's start with \texttt{group} as an integer and look at a logistic regression.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{some\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{group =} \FunctionTok{as.integer}\NormalTok{(group)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{lm}\NormalTok{(response}\SpecialCharTok{\textasciitilde{}}\NormalTok{group, }\AttributeTok{data =}\NormalTok{ .) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = response \textasciitilde{} group, data = .)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}    Min     1Q Median     3Q    Max }
\CommentTok{\#\textgreater{}  {-}0.68  {-}0.52   0.32   0.32   0.64 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)}
\CommentTok{\#\textgreater{} (Intercept)   0.8400     0.4495   1.869    0.104}
\CommentTok{\#\textgreater{} group        {-}0.1600     0.2313  {-}0.692    0.511}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 0.5451 on 7 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.064,  Adjusted R{-}squared:  {-}0.06971 }
\CommentTok{\#\textgreater{} F{-}statistic: 0.4786 on 1 and 7 DF,  p{-}value: 0.5113}
\end{Highlighting}
\end{Shaded}

Now we can try it as a factor. The intepretation of the variable is completely different.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{some\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{group =} \FunctionTok{as.factor}\NormalTok{(group)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{lm}\NormalTok{(response}\SpecialCharTok{\textasciitilde{}}\NormalTok{group, }\AttributeTok{data =}\NormalTok{ .) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = response \textasciitilde{} group, data = .)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}0.7500 {-}0.3333  0.2500  0.2500  0.6667 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)  }
\CommentTok{\#\textgreater{} (Intercept)   0.7500     0.2826   2.654   0.0378 *}
\CommentTok{\#\textgreater{} group2       {-}0.4167     0.4317  {-}0.965   0.3717  }
\CommentTok{\#\textgreater{} group3       {-}0.2500     0.4895  {-}0.511   0.6278  }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 0.5652 on 6 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.1375, Adjusted R{-}squared:  {-}0.15 }
\CommentTok{\#\textgreater{} F{-}statistic: 0.4783 on 2 and 6 DF,  p{-}value: 0.6416}
\end{Highlighting}
\end{Shaded}

Another critical aspect is to check the dates. In particular we want to try to make it into the following format: YYYY-MM-DD. There are of course differences of opinion as to what is an appropriate date format in the broader world, and reasonable people can differ on whether 1 July 2010 or July 1, 2020, is better, but YYYY-MM-DD is the format that is generally most appropriate for data.

\hypertarget{naming-things}{%
\section{Naming things}\label{naming-things}}

\begin{quote}
An improved scanning software we developed identified gene name errors in 30.9\% (3,436/11,117) of articles with supplementary Excel gene lists; a figure significantly higher than previously estimated. This is due to gene names being converted not just to dates and floating-point numbers, but also to internal date format (five-digit numbers).

\citet{omggenes}
\end{quote}

\url{https://neverworkintheory.org/2021/08/09/abbreviated-vs-full-names.html}

Names matter. I wrote this book on land that is today named Toronto, which is within a country named Canada, but for a long time before was known as Turtle Island. While not common, these days people will sometimes still refer to themselves as being on Turtle Island. That tells us something about them, and our use of the name Canada tells them something about us. There is a big rock in the centre of the country that I'm from, Australia. For a long time, it was called Uluru, then it was known as Ayers Rock. Today it has a dual name the combines both, and the choice of which name you use tells someone something about you. Even the British Royal Family recognise the power of names. In 1917 they changed from the House of Saxe-Coburg and Gotha to the House of Windsor, due to a feeling that the former was too Germanic given World War I was ongoing. Names matter in everyday life. And they matter in data science too.

The importance of names, and of ignoring existing claims through re-naming was clear in those cases, but we see it in data science as well. We need to be very careful when we name our datasets, our variables, and our functions. There is a tendency, these days, to call the variable `gender' even though it may only have male and female, because we do not want to say the word `sex'. \citep{tukey1962future} essentially defines what we today call data science, but it was popularised by folks in computer science in the 2010s who ignored, either deliberately or through ignorance, what came before them. The past ten years has been characteristic by the renaming of concepts that were well-established in the fields that computer science has recently expanded into. For instance, the use of binary variables in regression, sometimes called `dummy variables', is called one-hot encoding in computer science. Like all fashions, this one will pass also. We most recently saw this through the 1980s through to early 2010s with economics. Economists described themselves as the `queen of the social sciences' and self-described as imperialistic \citep{lazear2000economic}. We are now recognising the costs of this imperialism in social sciences, and in the future we will look back and count the cost of computer science imperialism in data science. The key here is that no area of study is ever \emph{terra nullius}, or nobody's land. It is important to recognise, adopt, and use existing names, and practices.

Names give places meaning, and by ignoring existing names, we ignore what has come before us. \citet[p.~34]{kimmerer2013} describes how `Tahawus is the Algonquin name for Mount Marcy, the highest peak in the Adirondacks. It's called Mount March to commemorate a governor who never set foot on those wild slopes.' She continues that `{[}w{]}hen we call a place by name it is transformed from wilderness to homeland.' She is talking with regard to physical places, but the same is true of our function names, our variable names and our dataset names. When we use gender instead of sex because we don't want to say sex in front of others, we ignore the preferences of those that provided data.

In addition to respecting the nature of the data, names need to satisfy two additional considerations: 1) they need to be machine readable, and 2) they need to be human readable.

Machine readable names is an easier standard to meet, but usually means avoiding spaces and special characters. A space can be replaced with a underbar. Usually, special characters should just be removed because they can be inconsistent between different computers and languages. The names should also be unique within a dataset, and unique within a collection of datasets unless that particular column is being deliberately used as a key to join different datasets.

An especially useful function to use to get closer to machine readable names is \texttt{janitor::clean\_names()} which is from the \texttt{janitor} package \citep{janitor}. This deals with those issues mentioned above as well as a few others. We can see an example.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bad\_names\_good\_names }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}
    \StringTok{\textquotesingle{}First\textquotesingle{}} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{),}
    \StringTok{\textquotesingle{}second name has spaces\textquotesingle{}} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{),}
    \StringTok{\textquotesingle{}weird\#symbol\textquotesingle{}} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{),}
    \StringTok{\textquotesingle{}InCoNsIsTaNtCaPs\textquotesingle{}} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{  )}

\NormalTok{bad\_names\_good\_names}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 4}
\CommentTok{\#\textgreater{}   First \textasciigrave{}second name has spaces\textasciigrave{} \textasciigrave{}weird\#symbol\textasciigrave{} InCoNsIsTaNtCaPs}
\CommentTok{\#\textgreater{}   \textless{}dbl\textgreater{}                    \textless{}dbl\textgreater{}          \textless{}dbl\textgreater{}            \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1     1                        1              1                1}

\NormalTok{bad\_names\_good\_names }\OtherTok{\textless{}{-}} 
\NormalTok{  bad\_names\_good\_names }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  janitor}\SpecialCharTok{::}\FunctionTok{clean\_names}\NormalTok{()}
  
\NormalTok{bad\_names\_good\_names}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 4}
\CommentTok{\#\textgreater{}   first second\_name\_has\_spaces weird\_number\_symbol in\_co\_ns\_is\_ta\_nt\_ca\_ps}
\CommentTok{\#\textgreater{}   \textless{}dbl\textgreater{}                  \textless{}dbl\textgreater{}               \textless{}dbl\textgreater{}                   \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1     1                      1                   1                       1}
\end{Highlighting}
\end{Shaded}

Human readable names require an additional layer! You need to consider other cultures and how they may interpret some of the names that you're using. You also need to consider different experience levels that subsequent users of your dataset may have. This is both in terms of experience with programming and statistics, but also experience with similar datasets. For instance, a column of `flag' is often used to signal that a column contains data that needs to be followed up with or treated carefully in some way. An experienced analyst will know this, but a beginner will not. Try to use meaningful names wherever possible \citep{lin2020ten}.

One interesting feature of R is that in certain cases partial matching on names is possible. For instance:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{never\_use\_partial\_matching }\OtherTok{\textless{}{-}} 
  \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{my\_first\_name =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{),}
    \AttributeTok{another\_name =} \FunctionTok{c}\NormalTok{(}\StringTok{"wow"}\NormalTok{, }\StringTok{"great"}\NormalTok{)}
\NormalTok{  )}

\NormalTok{never\_use\_partial\_matching}\SpecialCharTok{$}\NormalTok{my\_first\_name}
\CommentTok{\#\textgreater{} [1] 1 2}
\NormalTok{never\_use\_partial\_matching}\SpecialCharTok{$}\NormalTok{my}
\CommentTok{\#\textgreater{} [1] 1 2}
\end{Highlighting}
\end{Shaded}

This behaviour is not possible within the \texttt{tidyverse} (for instance if \texttt{data.frame} were replaced with \texttt{tibble} in the above code) and I recommend never using this feature. It makes it more difficult to understand your code after a break, and for others to come to it fresh.

\hypertarget{exercises-and-tutorial-10}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-10}}

\hypertarget{exercises-10}{%
\subsection{Exercises}\label{exercises-10}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Is the following an example of tidy data?
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tibble}\NormalTok{(}\AttributeTok{name =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Anne\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Bethany\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Stephen\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}William\textquotesingle{}}\NormalTok{),}
       \AttributeTok{age\_group =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}18{-}29\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}30{-}44\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}45{-}60\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}60+\textquotesingle{}}\NormalTok{),}
\NormalTok{       )}
\CommentTok{\#\textgreater{} \# A tibble: 4 x 2}
\CommentTok{\#\textgreater{}   name    age\_group}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}   \textless{}chr\textgreater{}    }
\CommentTok{\#\textgreater{} 1 Anne    18{-}29    }
\CommentTok{\#\textgreater{} 2 Bethany 30{-}44    }
\CommentTok{\#\textgreater{} 3 Stephen 45{-}60    }
\CommentTok{\#\textgreater{} 4 William 60+}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  If I am dealing with ages then what is the most likely class for the variable? {[}Select all that apply.{]}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    integer
  \item
    matrix
  \item
    numeric
  \item
    factor
  \end{enumerate}
\end{enumerate}

\hypertarget{tutorial-10}{%
\subsection{Tutorial}\label{tutorial-10}}

With regard to \citet{Jordan2019Artificial}, \citet{datafeminism2020}, Chapter 6, \citet{thatrandyauperson}, and other relevant work, to what extent do you think we should let the data speak for themselves? {[}Please write a page or two.{]}

\hypertarget{storing-and-retrieving-data}{%
\chapter{Storing and retrieving data}\label{storing-and-retrieving-data}}

\textbf{STATUS: Under construction.}

\textbf{Required reading}

\textbf{Recommended reading}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\tightlist
\item
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
\end{itemize}

\textbf{Key functions/etc}

\begin{itemize}
\tightlist
\item
\end{itemize}

\hypertarget{introduction-15}{%
\section{Introduction}\label{introduction-15}}

After you've put together a dataset, an important part of being responsible is storing it appropriately and enabling easy retrieval. While it is certainly possible to be especially concerned about this, and entire careers are based on the storage and retrieval of data, to a certain extent, the baseline here is not onerous. If you can get it off your own computer then you are half-way there! Confirming that someone else can retrieve it and use it, puts you much further than most.

That said, the FAIR principles are especially useful to be more formal about data management. These are \citep{wilkinson2016fair}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Findable. This means that there is one, unchanging, identifier for the dataset and the dataset has high-quality descriptions and explanations.
\item
  Accessible.
\item
  Interoperable.
\item
  Reusable.
\end{enumerate}

It's important to recognise that just because a dataset is FAIR, it is not necessarily an unbiased representation of the world.

\begin{quote}
\textbf{Oh, you think we have good data on that!} One representation of reality that is commonplace is in chess. A chess board (see Figure X - add photo of a chess board) is a 8 x 8 board of alternating black and white squares. The squares are denonated by a unique combination of a letter (A-G) and a number (1-8). And each piece has a unique abbreviation, for instance pawns are X, and knights are Y. A game is recorded by each player noting the move. In this way the entire game can be recreated. The 2021 World Championship was contested by Magnus Carlsen and Ian Nepomniachtchi. Figure X shows a score sheet from Game 6. There were a variety of reasons this game was particularly noteworthy, but one the uncharactertic mistakes that both Carlsen and Nepomniachtchi made. For instance, at Move 32 Carlsen did not exploit an opportunity; and Move 36 a different move would have provided Nepomniachtchi with a promising endgame (CITATION). One reason for this may have been that both players at that point in the game had very little time remaining---they had to decide on their moves very quickly. But there is no sense of that in the representation provided by the game sheet. It is a `correct' representation of what happened in the game, but not necessarily why it happened.
\end{quote}

\hypertarget{plan-3}{%
\section{Plan}\label{plan-3}}

\citet{michener2015ten}

Information Science and libraries

\citet{hart2016ten}

\hypertarget{r-packages-for-data}{%
\section{R Packages for data}\label{r-packages-for-data}}

\hypertarget{documentation}{%
\section{Documentation}\label{documentation}}

Datasheets \citep{gebru2020datasheets} are an increasingly critical aspect of data science. Datasheets are basically nutrition labels for datasets. The process of creating them enables us to think more carefully about what we will feed our model. More importantly, they enable others to better understand what we fed our model. Recently researchers went back and wrote a datasheet for one of the most popular datasets in computer science, and they found that around 30 per cent of the data were duplicated \citep{bandy2021addressing}.

Instead of telling you how unhealthy various foods are, a datasheet tells you things like:

\begin{itemize}
\tightlist
\item
  `Who created the dataset and on behalf of which entity?'
\item
  `Who funded the creation of the dataset?'
\item
  Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?'
\item
  `Is any information missing from individual instances?'
\end{itemize}

If you have done a lot of work to create the dataset that you analyze, then it may make sense to try to publish and share it on its own. But typically a datasheet might live in an appendix to the main work.

\hypertarget{exercises-and-tutorial-11}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-11}}

\hypertarget{exercises-11}{%
\subsection{Exercises}\label{exercises-11}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  According to \citet[p.2]{gebru2020datasheets}, a datasheet should document a dataset's (please select all that apply):

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    composition.
  \item
    recommended uses.
  \item
    motivation.
  \item
    collection process.
  \end{enumerate}
\item
  Following \citet{wilkinson2016fair}, which of the following are FAIR principles (please select all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Findable.
  \item
    Approachable.
  \item
    Interoperable.
  \item
    Reusable.
  \item
    Integrated.
  \item
    Fungible.
  \item
    Reduced.
  \item
    Accessible.
  \end{enumerate}
\end{enumerate}

\hypertarget{tutorial-11}{%
\subsection{Tutorial}\label{tutorial-11}}

Look into how IQ tests are conducted and what goes into them. To what extent do you think they measure intelligence? Some aspects that you may like to think about in answering that question include: Who decides what is intelligence? How is this updated? What is missing from that definition? To what extent is this generalisable? You should write a page or two.

\hypertarget{disseminating-and-protecting-data}{%
\chapter{Disseminating and protecting data}\label{disseminating-and-protecting-data}}

\textbf{STATUS: Under construction.}

\begin{itemize}
\item
  Hawes, M. B. (2020). Implementing Differential Privacy: Seven Lessons From the 2020 United States Census. Harvard Data Science Review. \url{https://doi.org/10.1162/99608f92.353c6f99}
\item
  \url{https://hdsr.mitpress.mit.edu/pub/g9o4z8au/release/2}
\item
  \url{https://www.census.gov/newsroom/blogs/research-matters/2020/02/census_bureau_works.html}
\end{itemize}

\textbf{Required reading}

\textbf{Recommended reading}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\tightlist
\item
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
\end{itemize}

\textbf{Key functions/etc}

\begin{itemize}
\tightlist
\item
\end{itemize}

\hypertarget{introduction-16}{%
\section{Introduction}\label{introduction-16}}

\hypertarget{what-is-personally-identifying-information}{%
\section{What is personally identifying information}\label{what-is-personally-identifying-information}}

\citet{zook2017ten}

\hypertarget{hashing-and-salting}{%
\section{Hashing and salting}\label{hashing-and-salting}}

\hypertarget{gdpr-and-hipaa}{%
\section{GDPR and HIPAA}\label{gdpr-and-hipaa}}

\hypertarget{making-fake-data-to-distribute-when-you-cant-share-the-real-stuff}{%
\section{Making fake data to distribute when you can't share the real stuff}\label{making-fake-data-to-distribute-when-you-cant-share-the-real-stuff}}

\hypertarget{differential-privacy}{%
\section{Differential privacy}\label{differential-privacy}}

\citet{kenny2021impact}

\citet{ruggles2019differential}

\citet{suriyakumar2020chasing}

\hypertarget{exercises-and-tutorial-12}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-12}}

\hypertarget{exercises-12}{%
\subsection{Exercises}\label{exercises-12}}

\hypertarget{tutorial-12}{%
\subsection{Tutorial}\label{tutorial-12}}

\hypertarget{part-modelling}{%
\part{Modelling}\label{part-modelling}}

\hypertarget{exploratory-data-analysis}{%
\chapter{Exploratory data analysis}\label{exploratory-data-analysis}}

\textbf{STATUS: Under construction.}

\textbf{Required reading}

\begin{itemize}
\tightlist
\item
  Barocas, Solon, and Danah Boyd, 2017, `Engaging the ethics of data science in practice', \emph{Communications of the ACM}, 60.11 (2017): 23-25.
\item
  DiCiccio, Thomas J., and Mary E. Thompson, 2004, `A Conversation with Donald A. S. Fraser', \emph{Statistical Science}, 19 (2) pp.~370-386, \url{https://utstat.toronto.edu/craiu/DonFraser_SSInterview.pdf}.
\item
  Jordan, Michael I, 2019, `AI - The revolution hasn't started yet', \emph{Harvard Data Science Review}, 1 July, \url{https://hdsr.mitpress.mit.edu/pub/wot7mkc1}.
\item
  Tukey, John W., 1961, `The Future of Data Analysis', \emph{The annals of mathematical statistics}, Part 1 `General Considerations', \url{https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-33/issue-1/The-Future-of-Data-Analysis/10.1214/aoms/1177704711.full}.
\item
  Wickham, Hadley, and Garrett Grolemund, 2017, \emph{R for Data Science}, Chapters 3 and 7, \url{https://r4ds.had.co.nz/}.
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
  Hall, Megan, 2019, `Exploratory Data Analysis Using Tidyverse', \url{https://hockey-graphs.com/2019/10/08/exploratory-data-analysis-using-tidyverse/}.
\item
  Kommenda, Niko, Helen Pidd and Libby Brooks, 2020, `Revealed: the areas in the UK with one Airbnb for every four homes', \emph{The Guardian}, 20 February, \url{https://www.theguardian.com/technology/2020/feb/20/revealed-the-areas-in-the-uk-with-one-airbnb-for-every-four-homes}.
\item
  Silge, Julia, 2018, `Understanding PCA using Stack Overflow data', \url{https://juliasilge.com/blog/stack-overflow-pca/}.
\item
  Soetewey, Antoine, 2020, `Descriptive statistics in R', \url{https://www.statsandr.com/blog/descriptive-statistics-in-r/}.
\item
  Stodulka, Jiri, 2019, `Toronto Crime and Folium', \url{https://www.jiristodulka.com/post/toronto-crime/}.
\item
  Wong, Julia Carrie, 2020, `One year inside Trump's monumental Facebook campaign', The Guardian, 29 January, \url{https://www.theguardian.com/us-news/2020/jan/28/donald-trump-facebook-ad-campaign-2020-election}.
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\tightlist
\item
  Quickly coming to terms with a new dataset by constructing graphs and tables.
\item
  Understanding the issues and features of the dataset and how this may affect your modelling decisions.
\item
  Thinking about missing values and outliers.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{broom}
\item
  \texttt{ggrepel}
\item
  \texttt{here}
\item
  \texttt{janitor}
\item
  \texttt{lubridate}
\item
  \texttt{opendatatoronto}
\item
  \texttt{tidymodels}
\item
  \texttt{tidyverse}
\item
  \texttt{visdat}
\end{itemize}

\textbf{Key functions/etc}

\begin{itemize}
\tightlist
\item
  \texttt{augment()}
\item
  \texttt{clean\_names()}
\item
  \texttt{coord\_flip()}
\item
  \texttt{count()}
\item
  \texttt{distinct()}
\item
  \texttt{facet\_grid()}
\item
  \texttt{facet\_wrap()}
\item
  \texttt{geom\_bar()}
\item
  \texttt{geom\_col()}
\item
  \texttt{geom\_density()}
\item
  \texttt{geom\_histogram()}
\item
  \texttt{geom\_line()}
\item
  \texttt{geom\_point()}
\item
  \texttt{geom\_smooth()}
\item
  \texttt{geom\_text\_repel()}
\item
  \texttt{get\_dupes()}
\item
  \texttt{glance()}
\item
  \texttt{if\_else()}
\item
  \texttt{ifelse()}
\item
  \texttt{initial\_split()}
\item
  \texttt{left\_join()}
\item
  \texttt{mutate()}
\item
  \texttt{mutate\_all()}
\item
  \texttt{names()}
\item
  \texttt{ncol()}
\item
  \texttt{nrow()}
\item
  \texttt{pivot\_wider()}
\item
  \texttt{scale\_color\_brewer()}
\item
  \texttt{scale\_fill\_brewer()}
\item
  \texttt{scale\_x\_log10()}
\item
  \texttt{scale\_y\_log10()}
\item
  \texttt{str\_detect()}
\item
  \texttt{str\_extract()}
\item
  \texttt{str\_remove()}
\item
  \texttt{str\_split()}
\item
  \texttt{str\_starts()}
\item
  \texttt{summarise()}
\item
  \texttt{summarise\_all()}
\item
  \texttt{theme\_classic()}
\item
  \texttt{theme\_minimal()}
\item
  \texttt{vis\_dat()}
\item
  \texttt{vis\_miss()}
\end{itemize}

\textbf{Quiz}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In your own words what is exploratory data analysis (this will be difficult, but please write only one nuanced paragraph)?
\item
  In Tukey's words, what is exploratory data analysis (please write one paragraph)?
\item
  Who was Tukey (please write a paragraph or two)?
\item
  What is Tukey's link to DoSS (hint: he was an advisor on someone's PhD - who was that person)?
\item
  Can you identify a female equivalent to Tukey who we (as historians of statistics) may have overlooked?
\item
  If you have a dataset called `my\_data', which has two columns: `first\_col' and `second\_col', then could you please write some rough R code that would generate a graph (the type of graph doesn't matter).
\item
  Consider a dataset that has 500 rows and 3 columns, so there are 1,500 cells. If 100 of the cells are missing data for at least one of the columns, then would you remove the whole row your dataset or try to run your analysis on the data as is, or some other procedure? What if your dataset had 10,000 rows instead, but the same number of missing cells?
\item
  Please note three ways of identifying unusual values.
\item
  What is the difference between a categorical and continuous variable?
\item
  What is the difference between a factor and an integer variable?
\item
  How can we think about who is systematically excluded from a dataset?
\item
  Using the \texttt{opendatatoronto} package, download the data on mayoral campaign contributions for 2014. (note: the 2014 file you will get from \texttt{get\_resource}, so just keep the sheet that relates to the Mayor election).

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Clean up the data format (fixing the parsing issue and standardizing the column names using \texttt{janitor})
  \item
    Summarize the variables in the dataset. Are there missing values, and if so, should we be worried about them? Is every variable in the format it should be? If not, create new variable(s) that are in the right format.
  \item
    Visually explore the distribution of values of the contributions. What contributions are notable outliers? Do they share a similar characteristic(s)? It may be useful to plot the distribution of contributions without these outliers to get a better sense of the majority of the data.
  \item
    List the top five candidates in each of these categories: 1) total contributions; 2) mean contribution; and 3) number of contributions.
  \item
    Repeat that process, but without contributions from the candidates themselves.
  \item
    How many contributors gave money to more than one candidate?
  \end{enumerate}
\item
  Name three geoms that produce graphs that have bars on them \texttt{ggplot()}.
\item
  Consider a dataset 10,000 observations and 27 variables. For each observation, there is at least one missing variable. Please discuss, in a paragraph or two, the steps that you would take to understand what is going on.
\item
  Known missing data, are those that leave holes in your dataset. But what about data that were never collected? Please look at McClelland, Alexander, 2019, `\,``Lock This Whore Up'': Legal Violence and Flows of Information Precipitating Personal Violence against People Criminalised for HIV-Related Crimes in Canada', European Journal of Risk Regulation, 10 (1), pp.~132-147. Then look at Policing the Pandemic - \url{https://www.policingthepandemic.ca/}. Look into how they gathered their dataset and what it took to put this together. What is in the dataset and why? What is missing and why? How could this affect the results? How might similar biases enter into other datasets that you have used or read about?
\end{enumerate}

\hypertarget{introduction-17}{%
\section{Introduction}\label{introduction-17}}

\begin{quote}
The future of data analysis can involve great progress, the overcoming of real difficulties, and the provision of a great service to all fields of science and technology. Will it? That remains to us, to our willingness to take up the rocky road of real problems in preference to the smooth road of unreal assumptions, arbitrary criteria, and abstract results without real attachments. Who is for the challenge?

\citet[p.~64]{tukey1962future}.
\end{quote}

Exploratory data analysis is never finished, you just die. It is the active process of exploring and becoming familiar with your data. Like a farmer with their hands in the earth, you need to know every contour and aspect of your data. You need to know how it changes, what it shows, hides, and what are its limits. Exploratory data analysis is the unstructured process of doing this.

That said, exploratory data analysis (EDA) is not something that ends up in your final paper. It is a means to an end and while it will inform your entire paper, especially the data section, it's not typically something that belongs in a final draft. The best way to proceed is to make a separate .Rmd and add code and brief notes as you go. Don't delete previous code, just add to it. When you run out of time, you'll have a useful notebook that captures your exploration. This is a document for you and your collaborators and will guide all the subsequent modelling that you do.

EDA draws on everything that you know as an analyst. Every tool is fair game and should be considered. Look at the raw data, make some tables, some plots, some summary statistics, make some models. The key here is to iterate, move quickly not perfectly, and come to understand your data.

In this chapter we will working with real data that has many issues so that you can understand the main characteristics and potential issues. We will use the \texttt{opendatatoronto} package \citep{citeSharla}, among other sources. There are a lot of options for EDA \citep{staniak2019landscape} and here I focus on a few of them.

\hypertarget{case-study---ttc-subway-delays}{%
\section{Case study - TTC subway delays}\label{case-study---ttc-subway-delays}}

\textbf{This section was written with \href{https://www.monicaalexander.com/}{Monica Alexander}.}

\hypertarget{introduction-18}{%
\subsection{Introduction}\label{introduction-18}}

The \texttt{opendatatoronto} package \citep{citeSharla} provides an interface to all data available on the \href{https://open.toronto.ca/}{Open Data Portal} provided by the City of Toronto. We are going to use that to take a quick look at the subway delays. We're additionally going to especially draw on the \texttt{tidyverse} \citep{citetidyverse}, as well as the \texttt{ggrepel} \citep{citeggrepel}, \texttt{janitor} \citep{janitor}, \texttt{lubridate} \citep{GrolemundWickham2011}, and \texttt{visdat} \citep{citevisdat} packages.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(opendatatoronto)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(ggrepel)}
\FunctionTok{library}\NormalTok{(janitor)}
\FunctionTok{library}\NormalTok{(lubridate)}
\FunctionTok{library}\NormalTok{(visdat)}
\end{Highlighting}
\end{Shaded}

\hypertarget{gather-the-data}{%
\subsection{Gather the data}\label{gather-the-data}}

To begin with, use \texttt{opendatatoronto::list\_packages()} to look at some of the datasets that available.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_data }\OtherTok{\textless{}{-}}\NormalTok{ opendatatoronto}\SpecialCharTok{::}\FunctionTok{list\_packages}\NormalTok{(}\AttributeTok{limit =} \DecValTok{500}\NormalTok{)}
\NormalTok{all\_data}
\CommentTok{\#\textgreater{} \# A tibble: 425 x 11}
\CommentTok{\#\textgreater{}    title   id     topics   civic\_issues   publisher excerpt }
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}   \textless{}chr\textgreater{}  \textless{}chr\textgreater{}    \textless{}chr\textgreater{}          \textless{}chr\textgreater{}     \textless{}chr\textgreater{}   }
\CommentTok{\#\textgreater{}  1 Daily \textasciitilde{} 21c83\textasciitilde{} Communi\textasciitilde{} Affordable ho\textasciitilde{} Shelter,\textasciitilde{} "Daily \textasciitilde{}}
\CommentTok{\#\textgreater{}  2 Munici\textasciitilde{} 57b22\textasciitilde{} Busines\textasciitilde{} \textless{}NA\textgreater{}           Municipa\textasciitilde{} "Some b\textasciitilde{}}
\CommentTok{\#\textgreater{}  3 EarlyO\textasciitilde{} 26196\textasciitilde{} Communi\textasciitilde{} Poverty reduc\textasciitilde{} Children\textasciitilde{} "EarlyO\textasciitilde{}}
\CommentTok{\#\textgreater{}  4 Chemic\textasciitilde{} ae8ee\textasciitilde{} Environ\textasciitilde{} Climate change Toronto \textasciitilde{} "This d\textasciitilde{}}
\CommentTok{\#\textgreater{}  5 Apartm\textasciitilde{} 4ef82\textasciitilde{} Locatio\textasciitilde{} Affordable ho\textasciitilde{} Municipa\textasciitilde{} "This d\textasciitilde{}}
\CommentTok{\#\textgreater{}  6 Addres\textasciitilde{} abedd\textasciitilde{} Locatio\textasciitilde{} Mobility       Informat\textasciitilde{} "This d\textasciitilde{}}
\CommentTok{\#\textgreater{}  7 Proper\textasciitilde{} 1acaa\textasciitilde{} Locatio\textasciitilde{} Mobility       Informat\textasciitilde{} "This d\textasciitilde{}}
\CommentTok{\#\textgreater{}  8 Lobbyi\textasciitilde{} 6a87b\textasciitilde{} City go\textasciitilde{} \textless{}NA\textgreater{}           Lobbyist\textasciitilde{} "The Lo\textasciitilde{}}
\CommentTok{\#\textgreater{}  9 Toront\textasciitilde{} 1d079\textasciitilde{} City go\textasciitilde{} Mobility       Informat\textasciitilde{} "Linear\textasciitilde{}}
\CommentTok{\#\textgreater{} 10 Buildi\textasciitilde{} 8219e\textasciitilde{} Develop\textasciitilde{} \textless{}NA\textgreater{}           Toronto \textasciitilde{} "Provid\textasciitilde{}}
\CommentTok{\#\textgreater{} \# ... with 415 more rows, and 5 more variables:}
\CommentTok{\#\textgreater{} \#   dataset\_category \textless{}chr\textgreater{}, num\_resources \textless{}int\textgreater{},}
\CommentTok{\#\textgreater{} \#   formats \textless{}chr\textgreater{}, refresh\_rate \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   last\_refreshed \textless{}date\textgreater{}}
\end{Highlighting}
\end{Shaded}

We'll download the data on TTC subway delays in 2019. There are multiple files for 2019 so we need to get them all and then make them into one big dataframe.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We know this number based on the \textquotesingle{}id\textquotesingle{} of the interest.}
\NormalTok{ttc\_resources }\OtherTok{\textless{}{-}} 
  \FunctionTok{list\_package\_resources}\NormalTok{(}\StringTok{"996cfe8d{-}fb35{-}40ce{-}b569{-}698d51fc683b"}\NormalTok{)}

\NormalTok{ttc\_resources }\OtherTok{\textless{}{-}} 
\NormalTok{  ttc\_resources }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{year =} \FunctionTok{str\_extract}\NormalTok{(name, }\StringTok{"201.?"}\NormalTok{))}

\NormalTok{delay\_2019\_ids }\OtherTok{\textless{}{-}} 
\NormalTok{  ttc\_resources }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(year}\SpecialCharTok{==}\DecValTok{2019}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(id) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pull}\NormalTok{()}

\NormalTok{delay\_2019 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{()}

\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(delay\_2019\_ids)) \{}
\NormalTok{  delay\_2019 }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(delay\_2019, }\FunctionTok{get\_resource}\NormalTok{(delay\_2019\_ids[i]))}
\NormalTok{\}}

\CommentTok{\# make the column names nicer to work with}
\NormalTok{delay\_2019 }\OtherTok{\textless{}{-}} \FunctionTok{clean\_names}\NormalTok{(delay\_2019)}
\end{Highlighting}
\end{Shaded}

Let's also download the delay code and readme, as reference. You'd probably want to save all this into an `inputs' folder, as this is the raw data that would underpin any analysis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delay\_codes }\OtherTok{\textless{}{-}} \FunctionTok{get\_resource}\NormalTok{(}\StringTok{"fece136b{-}224a{-}412a{-}b191{-}8d31eb00491e"}\NormalTok{)}
\CommentTok{\#\textgreater{} New names:}
\CommentTok{\#\textgreater{} * \textasciigrave{}\textasciigrave{} {-}\textgreater{} ...1}
\CommentTok{\#\textgreater{} * \textasciigrave{}CODE DESCRIPTION\textasciigrave{} {-}\textgreater{} \textasciigrave{}CODE DESCRIPTION...3\textasciigrave{}}
\CommentTok{\#\textgreater{} * \textasciigrave{}\textasciigrave{} {-}\textgreater{} ...4}
\CommentTok{\#\textgreater{} * \textasciigrave{}\textasciigrave{} {-}\textgreater{} ...5}
\CommentTok{\#\textgreater{} * \textasciigrave{}CODE DESCRIPTION\textasciigrave{} {-}\textgreater{} \textasciigrave{}CODE DESCRIPTION...7\textasciigrave{}}

\NormalTok{delay\_data\_codebook }\OtherTok{\textless{}{-}} \FunctionTok{get\_resource}\NormalTok{(}\StringTok{"54247e39{-}5a7d{-}40db{-}a137{-}82b2a9ab0708"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This dataset has a bunch of interesting variables. You can refer to the readme for descriptions. Our outcome of interest is \texttt{min\_delay}, which give the delay in minutes.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(delay\_2019)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 10}
\CommentTok{\#\textgreater{}   date                time  day     station  code  min\_delay}
\CommentTok{\#\textgreater{}   \textless{}dttm\textgreater{}              \textless{}chr\textgreater{} \textless{}chr\textgreater{}   \textless{}chr\textgreater{}    \textless{}chr\textgreater{}     \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 2019{-}01{-}01 00:00:00 01:08 Tuesday YORK MI\textasciitilde{} PUSI          0}
\CommentTok{\#\textgreater{} 2 2019{-}01{-}01 00:00:00 02:14 Tuesday ST ANDR\textasciitilde{} PUMST         0}
\CommentTok{\#\textgreater{} 3 2019{-}01{-}01 00:00:00 02:16 Tuesday JANE ST\textasciitilde{} TUSC          0}
\CommentTok{\#\textgreater{} 4 2019{-}01{-}01 00:00:00 02:27 Tuesday BLOOR S\textasciitilde{} SUO           0}
\CommentTok{\#\textgreater{} 5 2019{-}01{-}01 00:00:00 03:03 Tuesday DUPONT \textasciitilde{} MUATC        11}
\CommentTok{\#\textgreater{} 6 2019{-}01{-}01 00:00:00 03:08 Tuesday EGLINTO\textasciitilde{} EUATC        11}
\CommentTok{\#\textgreater{} \# ... with 4 more variables: min\_gap \textless{}dbl\textgreater{}, bound \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   line \textless{}chr\textgreater{}, vehicle \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

Next we're going to highlight some tools that might be useful when you are getting used to a new dataset. There's no one way to explore, but it's important to keep in mind:

\begin{itemize}
\tightlist
\item
  what should your variables look like (type, values, distribution, etc);
\item
  what would be surprising (outliers etc); and
\item
  what is your end goal (here, it might be understanding factors associated with delays, e.g.~stations, time of year, time of day, etc).
\end{itemize}

In any data analysis project, if it turns out you have data issues, surprising values, missing data etc, it's important you document anything you found and the subsequent steps or assumptions you made before moving onto your data analysis and modeling.

As always:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Start with an end in mind.
\item
  Be as lazy as possible.
\end{enumerate}

\hypertarget{sanity-checks}{%
\subsection{Sanity Checks}\label{sanity-checks}}

We need to check that the variables are what they say they are. If they aren't, the natural next question is to what to do with any issues. Should we recode them, or even remove them? It's important to distinguish between factors and characters, as well as between factors and numerics.

For instance, have a look at the column that claims to be the days of week using \texttt{unique()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{unique}\NormalTok{(delay\_2019}\SpecialCharTok{$}\NormalTok{day)}
\CommentTok{\#\textgreater{} [1] "Tuesday"   "Wednesday" "Thursday"  "Friday"   }
\CommentTok{\#\textgreater{} [5] "Saturday"  "Sunday"    "Monday"}
\end{Highlighting}
\end{Shaded}

Another function that that's useful here is \texttt{table()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(delay\_2019}\SpecialCharTok{$}\NormalTok{day)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}    Friday    Monday  Saturday    Sunday  Thursday   Tuesday }
\CommentTok{\#\textgreater{}      2979      2970      2238      1978      3116      2939 }
\CommentTok{\#\textgreater{} Wednesday }
\CommentTok{\#\textgreater{}      3002}
\end{Highlighting}
\end{Shaded}

Let's now check the lines.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{unique}\NormalTok{(delay\_2019}\SpecialCharTok{$}\NormalTok{line)}
\CommentTok{\#\textgreater{}  [1] "YU"                     "BD"                    }
\CommentTok{\#\textgreater{}  [3] "YU/BD"                  "SHP"                   }
\CommentTok{\#\textgreater{}  [5] "SRT"                    NA                      }
\CommentTok{\#\textgreater{}  [7] "YUS"                    "B/D"                   }
\CommentTok{\#\textgreater{}  [9] "BD LINE"                "999"                   }
\CommentTok{\#\textgreater{} [11] "YU/ BD"                 "YU \& BD"               }
\CommentTok{\#\textgreater{} [13] "BD/YU"                  "YU\textbackslash{}\textbackslash{}BD"                }
\CommentTok{\#\textgreater{} [15] "46 MARTIN GROVE"        "RT"                    }
\CommentTok{\#\textgreater{} [17] "BLOOR{-}DANFORTH"         "YU / BD"               }
\CommentTok{\#\textgreater{} [19] "134 PROGRESS"           "YU {-} BD"               }
\CommentTok{\#\textgreater{} [21] "985 SHEPPARD EAST EXPR" "22 COXWELL"            }
\CommentTok{\#\textgreater{} [23] "100 FLEMINGDON PARK"    "YU LINE"}
\end{Highlighting}
\end{Shaded}

It looks like we have many issues here, and some of them have an obvious re-code, but some do not. Should we drop them? There's no absolute right answer here, it depends on what you're using the data for, but you need to be aware of these issues in the data.

\hypertarget{missing-values}{%
\subsection{Missing values}\label{missing-values}}

Exploring missing data is a course in itself, but the main point is that their presence (or lack thereof) will haunt your analysis. Insert joke about ghostbusters here.

To get started look at known-unknowns, which are the NAs for each variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delay\_2019 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise\_all}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\FunctionTok{sum}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(.))))}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 10}
\CommentTok{\#\textgreater{}    date  time   day station  code min\_delay min\_gap bound}
\CommentTok{\#\textgreater{}   \textless{}int\textgreater{} \textless{}int\textgreater{} \textless{}int\textgreater{}   \textless{}int\textgreater{} \textless{}int\textgreater{}     \textless{}int\textgreater{}   \textless{}int\textgreater{} \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1     0     0     0       0     0         0       0  4380}
\CommentTok{\#\textgreater{} \# ... with 2 more variables: line \textless{}int\textgreater{}, vehicle \textless{}int\textgreater{}}
\end{Highlighting}
\end{Shaded}

The \texttt{visdat} package \citep{citevisdat} is also useful here, particularly to see how missing values are distributed.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{vis\_dat}\NormalTok{(delay\_2019)}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-10-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{vis\_miss}\NormalTok{(delay\_2019)}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-10-2.pdf}

For these known-unknowns, what we are interested in is whether the they are missing at random. We want to, ideally, show that data happened to just drop out. Of course, this is unlikely the case, and so we are looking to see what is systematic about how our data are missing.

\hypertarget{duplicate-rows}{%
\subsection{Duplicate rows}\label{duplicate-rows}}

Sometime data happen to be duplicated. If we didn't notice this then our analysis would be wrong in ways that we'd not be able to consistently expect. There are a variety of ways to look for duplicated rows, but the \texttt{janitor::get\_dupes()} function from the \texttt{janitor} package \citep{janitor} is especially useful.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{janitor}\SpecialCharTok{::}\FunctionTok{get\_dupes}\NormalTok{(delay\_2019)}
\CommentTok{\#\textgreater{} No variable names specified {-} using all columns.}
\CommentTok{\#\textgreater{} \# A tibble: 158 x 11}
\CommentTok{\#\textgreater{}    date                time  day     station code  min\_delay}
\CommentTok{\#\textgreater{}    \textless{}dttm\textgreater{}              \textless{}chr\textgreater{} \textless{}chr\textgreater{}   \textless{}chr\textgreater{}   \textless{}chr\textgreater{}     \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1 2019{-}01{-}01 00:00:00 08:18 Tuesday DONLAN\textasciitilde{} MUESA         5}
\CommentTok{\#\textgreater{}  2 2019{-}01{-}01 00:00:00 08:18 Tuesday DONLAN\textasciitilde{} MUESA         5}
\CommentTok{\#\textgreater{}  3 2019{-}02{-}01 00:00:00 05:51 Friday  SCARB \textasciitilde{} MRTO         10}
\CommentTok{\#\textgreater{}  4 2019{-}02{-}01 00:00:00 05:51 Friday  SCARB \textasciitilde{} MRTO         10}
\CommentTok{\#\textgreater{}  5 2019{-}02{-}01 00:00:00 06:45 Friday  MIDLAN\textasciitilde{} MRWEA         3}
\CommentTok{\#\textgreater{}  6 2019{-}02{-}01 00:00:00 06:45 Friday  MIDLAN\textasciitilde{} MRWEA         3}
\CommentTok{\#\textgreater{}  7 2019{-}02{-}01 00:00:00 06:55 Friday  LAWREN\textasciitilde{} ERDO          0}
\CommentTok{\#\textgreater{}  8 2019{-}02{-}01 00:00:00 06:55 Friday  LAWREN\textasciitilde{} ERDO          0}
\CommentTok{\#\textgreater{}  9 2019{-}02{-}01 00:00:00 07:16 Friday  MCCOWA\textasciitilde{} MRWEA         5}
\CommentTok{\#\textgreater{} 10 2019{-}02{-}01 00:00:00 07:16 Friday  MCCOWA\textasciitilde{} MRWEA         5}
\CommentTok{\#\textgreater{} \# ... with 148 more rows, and 5 more variables:}
\CommentTok{\#\textgreater{} \#   min\_gap \textless{}dbl\textgreater{}, bound \textless{}chr\textgreater{}, line \textless{}chr\textgreater{}, vehicle \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   dupe\_count \textless{}int\textgreater{}}
\end{Highlighting}
\end{Shaded}

Our delays dataset has quite a few duplicates. Again, we're interested in whether there is something systematic going on. Remembering that we're trying to quickly come to terms with dataset, one way forward is to flag this as an issue to come back to and explore later, and to just remove them for now.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delay\_2019 }\OtherTok{\textless{}{-}} 
\NormalTok{  delay\_2019 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{distinct}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{visualizing-distributions}{%
\subsection{Visualizing distributions}\label{visualizing-distributions}}

You need to see your data in its most raw form to understand it, and histograms, barplots, and density plots are your friends here. We're not looking for beauty here, we're looking to get a look at the data as quickly as possible.

Let's look at one outcome of interest: `min\_delay'. First of all let's just look at a histogram of all the data.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Removing the observations that have non{-}standardized lines}
\NormalTok{delay\_2019 }\OtherTok{\textless{}{-}} 
\NormalTok{  delay\_2019 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(line }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"BD"}\NormalTok{, }\StringTok{"YU"}\NormalTok{, }\StringTok{"SHP"}\NormalTok{, }\StringTok{"SRT"}\NormalTok{))}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ delay\_2019) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ min\_delay))}
\CommentTok{\#\textgreater{} \textasciigrave{}stat\_bin()\textasciigrave{} using \textasciigrave{}bins = 30\textasciigrave{}. Pick better value with}
\CommentTok{\#\textgreater{} \textasciigrave{}binwidth\textasciigrave{}.}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-13-1.pdf}

Somewhat concerningly we have some evidence of outliers (given the large x-axis). There are a variety of ways to focus on what is going on, but a quick way is to plot it on a logged scale (remembering that we'd expect any values of 0 to drop away).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ delay\_2019) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ min\_delay)) }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_log10}\NormalTok{()}
\CommentTok{\#\textgreater{} Warning: Transformation introduced infinite values in}
\CommentTok{\#\textgreater{} continuous x{-}axis}
\CommentTok{\#\textgreater{} \textasciigrave{}stat\_bin()\textasciigrave{} using \textasciigrave{}bins = 30\textasciigrave{}. Pick better value with}
\CommentTok{\#\textgreater{} \textasciigrave{}binwidth\textasciigrave{}.}
\CommentTok{\#\textgreater{} Warning: Removed 11944 rows containing non{-}finite values}
\CommentTok{\#\textgreater{} (stat\_bin).}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-14-1.pdf}

This initial exploration is further hinting at an outlying delay time, so let's take a look at the largest delays. We need to join this dataset with the `delay\_codes' dataset to see what the delay is, and this requires some wrangling because of slightly different codes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delay\_2019 }\OtherTok{\textless{}{-}} 
\NormalTok{  delay\_2019 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{left\_join}\NormalTok{(delay\_codes }\SpecialCharTok{\%\textgreater{}\%} 
              \FunctionTok{rename}\NormalTok{(}\AttributeTok{code =} \StringTok{\textasciigrave{}}\AttributeTok{SUB RMENU CODE}\StringTok{\textasciigrave{}}\NormalTok{, }\AttributeTok{code\_desc =} \StringTok{\textasciigrave{}}\AttributeTok{CODE DESCRIPTION...3}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
              \FunctionTok{select}\NormalTok{(code, code\_desc)}
\NormalTok{            )}
\CommentTok{\#\textgreater{} Joining, by = "code"}

\NormalTok{delay\_2019 }\OtherTok{\textless{}{-}} 
\NormalTok{  delay\_2019 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{code\_srt =} \FunctionTok{ifelse}\NormalTok{(line}\SpecialCharTok{==}\StringTok{"SRT"}\NormalTok{, code, }\StringTok{"NA"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{left\_join}\NormalTok{(delay\_codes }\SpecialCharTok{\%\textgreater{}\%} 
              \FunctionTok{rename}\NormalTok{(}\AttributeTok{code\_srt =} \StringTok{\textasciigrave{}}\AttributeTok{SRT RMENU CODE}\StringTok{\textasciigrave{}}\NormalTok{, }\AttributeTok{code\_desc\_srt =} \StringTok{\textasciigrave{}}\AttributeTok{CODE DESCRIPTION...7}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
              \FunctionTok{select}\NormalTok{(code\_srt, code\_desc\_srt)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{code =} \FunctionTok{ifelse}\NormalTok{(code\_srt}\SpecialCharTok{==}\StringTok{"NA"}\NormalTok{, code, code\_srt),}
         \AttributeTok{code\_desc =} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(code\_desc\_srt), code\_desc, code\_desc\_srt)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{code\_srt, }\SpecialCharTok{{-}}\NormalTok{code\_desc\_srt)}
\CommentTok{\#\textgreater{} Joining, by = "code\_srt"}
\end{Highlighting}
\end{Shaded}

And so we can see that the 455 minute delay was due to `Rail Related Problem' and seems to very much be an outlier.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delay\_2019 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{left\_join}\NormalTok{(delay\_codes }\SpecialCharTok{\%\textgreater{}\%} 
              \FunctionTok{rename}\NormalTok{(}\AttributeTok{code =} \StringTok{\textasciigrave{}}\AttributeTok{SUB RMENU CODE}\StringTok{\textasciigrave{}}\NormalTok{, }\AttributeTok{code\_desc =} \StringTok{\textasciigrave{}}\AttributeTok{CODE DESCRIPTION...3}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
              \FunctionTok{select}\NormalTok{(code, code\_desc)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{min\_delay) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(date, time, station, line, min\_delay, code, code\_desc)}
\CommentTok{\#\textgreater{} Joining, by = c("code", "code\_desc")}
\CommentTok{\#\textgreater{} \# A tibble: 18,697 x 7}
\CommentTok{\#\textgreater{}    date                time  station   line  min\_delay code }
\CommentTok{\#\textgreater{}    \textless{}dttm\textgreater{}              \textless{}chr\textgreater{} \textless{}chr\textgreater{}     \textless{}chr\textgreater{}     \textless{}dbl\textgreater{} \textless{}chr\textgreater{}}
\CommentTok{\#\textgreater{}  1 2019{-}06{-}25 00:00:00 18:48 WILSON T\textasciitilde{} YU          455 PUTR }
\CommentTok{\#\textgreater{}  2 2019{-}02{-}12 00:00:00 20:28 LAWRENCE\textasciitilde{} SRT         284 MRWEA}
\CommentTok{\#\textgreater{}  3 2019{-}06{-}05 00:00:00 12:42 UNION TO\textasciitilde{} YU          250 MUPLA}
\CommentTok{\#\textgreater{}  4 2019{-}10{-}22 00:00:00 14:22 LAWRENCE\textasciitilde{} YU          228 PUTS }
\CommentTok{\#\textgreater{}  5 2019{-}09{-}26 00:00:00 11:38 YORK MIL\textasciitilde{} YU          193 MUPR1}
\CommentTok{\#\textgreater{}  6 2019{-}06{-}08 00:00:00 08:51 SPADINA \textasciitilde{} BD          180 MUPLB}
\CommentTok{\#\textgreater{}  7 2019{-}12{-}02 00:00:00 06:59 DUNDAS W\textasciitilde{} BD          176 MUPLB}
\CommentTok{\#\textgreater{}  8 2019{-}01{-}29 00:00:00 05:46 VICTORIA\textasciitilde{} BD          174 MUWEA}
\CommentTok{\#\textgreater{}  9 2019{-}02{-}22 00:00:00 17:32 ELLESMER\textasciitilde{} SRT         168 PRW  }
\CommentTok{\#\textgreater{} 10 2019{-}02{-}10 00:00:00 07:53 BAYVIEW \textasciitilde{} SHP         165 PUSI }
\CommentTok{\#\textgreater{} \# ... with 18,687 more rows, and 1 more variable:}
\CommentTok{\#\textgreater{} \#   code\_desc \textless{}chr\textgreater{}}
\end{Highlighting}
\end{Shaded}

\hypertarget{groups-and-small-counts}{%
\subsection{Groups and small counts}\label{groups-and-small-counts}}

Another thing that we're looking for is various groupings of the data, especially where sub-groups may end up with small numbers of observations in them (because any analysis could be easily influenced by them). A quick way to do this is to group the data by a variable that is of interest, for instance `line', using colour.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ delay\_2019) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ min\_delay, }\AttributeTok{y =}\NormalTok{ ..density.., }\AttributeTok{fill =}\NormalTok{ line), }
                 \AttributeTok{position =} \StringTok{\textquotesingle{}dodge\textquotesingle{}}\NormalTok{, }
                 \AttributeTok{bins =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_log10}\NormalTok{()}
\CommentTok{\#\textgreater{} Warning: Transformation introduced infinite values in}
\CommentTok{\#\textgreater{} continuous x{-}axis}
\CommentTok{\#\textgreater{} Warning: Removed 11944 rows containing non{-}finite values}
\CommentTok{\#\textgreater{} (stat\_bin).}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-17-1.pdf}

That plot uses density so that we can look at the the distributions more comparably, but we should also be aware of differences in frequency. In this case, we'll see that SHP and SRT have much smaller counts.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ delay\_2019) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ min\_delay, }\AttributeTok{fill =}\NormalTok{ line), }
                 \AttributeTok{position =} \StringTok{\textquotesingle{}dodge\textquotesingle{}}\NormalTok{, }
                 \AttributeTok{bins =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_log10}\NormalTok{()}
\CommentTok{\#\textgreater{} Warning: Transformation introduced infinite values in}
\CommentTok{\#\textgreater{} continuous x{-}axis}
\CommentTok{\#\textgreater{} Warning: Removed 11944 rows containing non{-}finite values}
\CommentTok{\#\textgreater{} (stat\_bin).}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-18-1.pdf}

To group by a second variable it can be useful to use facets. They're a little fiddly initially, but once you get used to them, they're both quick and powerful.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ delay\_2019) }\SpecialCharTok{+} 
  \FunctionTok{geom\_density}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ min\_delay, }\AttributeTok{color =}\NormalTok{ day), }
               \AttributeTok{bw =}\NormalTok{ .}\DecValTok{08}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_log10}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{facet\_grid}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{line)}
\CommentTok{\#\textgreater{} Warning: Transformation introduced infinite values in}
\CommentTok{\#\textgreater{} continuous x{-}axis}
\CommentTok{\#\textgreater{} Warning: Removed 11944 rows containing non{-}finite values}
\CommentTok{\#\textgreater{} (stat\_density).}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-19-1.pdf}

As an aside, the station names are a mess. We could try to quickly bring a little order to the chaos by just taking just the first word (or, the first two if it starts with `ST').

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delay\_2019 }\OtherTok{\textless{}{-}} 
\NormalTok{  delay\_2019 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{station\_clean =} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{str\_starts}\NormalTok{(station, }\StringTok{"ST"}\NormalTok{), }\FunctionTok{word}\NormalTok{(station, }\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\FunctionTok{word}\NormalTok{(station, }\DecValTok{1}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

We can now plot the top five stations by mean delay.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delay\_2019 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(line, station\_clean) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean\_delay =} \FunctionTok{mean}\NormalTok{(min\_delay), }\AttributeTok{n\_obs =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(n\_obs}\SpecialCharTok{\textgreater{}}\DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(line, }\SpecialCharTok{{-}}\NormalTok{mean\_delay) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(station\_clean, mean\_delay)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+} 
    \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+} 
    \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{line, }\AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{)}
\CommentTok{\#\textgreater{} \textasciigrave{}summarise()\textasciigrave{} has grouped output by \textquotesingle{}line\textquotesingle{}. You can override using the \textasciigrave{}.groups\textasciigrave{} argument.}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-21-1.pdf}

\hypertarget{visualizing-time-series}{%
\subsection{Visualizing time series}\label{visualizing-time-series}}

Dates are a pain to work with. They always go wrong and give issues, and so they are a critical aspect to explore during EDA. The daily plot of this data is, well, very messy (you can check for yourself). Instead, let's look by week to see if there's any seasonality. The \texttt{lubridate} package \citep{GrolemundWickham2011} has a lot of helpful functions that deal with date variables. It's essentially indispensable.

To get started, let's look at mean delay (of those that were delayed more than zero minutes).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delay\_2019 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(min\_delay}\SpecialCharTok{\textgreater{}}\DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{week =} \FunctionTok{week}\NormalTok{(date)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(week, line) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean\_delay =} \FunctionTok{mean}\NormalTok{(min\_delay)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(week, mean\_delay, }\AttributeTok{color =}\NormalTok{ line)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
    \FunctionTok{geom\_smooth}\NormalTok{() }\SpecialCharTok{+} 
    \FunctionTok{facet\_grid}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{line)}
\CommentTok{\#\textgreater{} \textasciigrave{}summarise()\textasciigrave{} has grouped output by \textquotesingle{}week\textquotesingle{}. You can override using the \textasciigrave{}.groups\textasciigrave{} argument.}
\CommentTok{\#\textgreater{} \textasciigrave{}geom\_smooth()\textasciigrave{} using method = \textquotesingle{}loess\textquotesingle{} and formula \textquotesingle{}y \textasciitilde{} x\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-22-1.pdf}

Now let's look at the proportion of delays that were greater than 10 minutes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delay\_2019 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{week =} \FunctionTok{week}\NormalTok{(date)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(week, line) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{prop\_delay =} \FunctionTok{sum}\NormalTok{(min\_delay}\SpecialCharTok{\textgreater{}}\DecValTok{10}\NormalTok{)}\SpecialCharTok{/}\FunctionTok{n}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(week, prop\_delay, }\AttributeTok{color =}\NormalTok{ line)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
    \FunctionTok{geom\_smooth}\NormalTok{() }\SpecialCharTok{+} 
    \FunctionTok{facet\_grid}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{line)}
\CommentTok{\#\textgreater{} \textasciigrave{}summarise()\textasciigrave{} has grouped output by \textquotesingle{}week\textquotesingle{}. You can override using the \textasciigrave{}.groups\textasciigrave{} argument.}
\CommentTok{\#\textgreater{} \textasciigrave{}geom\_smooth()\textasciigrave{} using method = \textquotesingle{}loess\textquotesingle{} and formula \textquotesingle{}y \textasciitilde{} x\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-23-1.pdf}

Again, and it's important to be clear here. These charts and tables and analyse have no place in a final report, but what they are doing is allowing you to become comfortable with the data. If you were doing this yourself, you should additionally be making notes about each plot and table as you go, noting the warnings and any implications or aspects to return to.

\hypertarget{visualizing-relationships}{%
\subsection{Visualizing relationships}\label{visualizing-relationships}}

We are also interested in looking at the relationship between two variables. Scatter plots are especially useful here for continuous variables, and are a good precursor to modeling. We saw a little of that with `mean\_delay' and `week'.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delay\_2019 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ min\_delay, }\AttributeTok{y =}\NormalTok{ min\_gap)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_log10}\NormalTok{()}
\CommentTok{\#\textgreater{} Warning: Transformation introduced infinite values in}
\CommentTok{\#\textgreater{} continuous x{-}axis}
\CommentTok{\#\textgreater{} Warning: Transformation introduced infinite values in}
\CommentTok{\#\textgreater{} continuous y{-}axis}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-24-1.pdf}

The relationship between categorical variables takes more work, but we could also, for instance, look at the top five reasons for delay by station. In particular, we may be interested in whether they differ, and how any difference could be modelled.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delay\_2019 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(line, code\_desc) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean\_delay =} \FunctionTok{mean}\NormalTok{(min\_delay)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{mean\_delay) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ code\_desc,}
             \AttributeTok{y =}\NormalTok{ mean\_delay)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(line), }
             \AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{,}
             \AttributeTok{nrow =} \DecValTok{4}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{()}
\CommentTok{\#\textgreater{} \textasciigrave{}summarise()\textasciigrave{} has grouped output by \textquotesingle{}line\textquotesingle{}. You can override using the \textasciigrave{}.groups\textasciigrave{} argument.}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-25-1.pdf}

\hypertarget{principal-components-analysis}{%
\subsection{Principal components analysis}\label{principal-components-analysis}}

Principal components analysis (PCA) is another powerful exploratory tool. It allows you to pick up potential clusters and/or outliers that can help to inform model building. To see this let's do a quick (and imperfect) example looking at types of delays by station.

The delay categories are a bit of a mess, and there's hundreds of them. As a simple start, remembering that our task is to come to terms with the dataset as quickly as possible, let's just take the first word.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delay\_2019 }\OtherTok{\textless{}{-}}\NormalTok{ delay\_2019 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{code\_red =} \FunctionTok{case\_when}\NormalTok{(}
    \FunctionTok{str\_starts}\NormalTok{(code\_desc, }\StringTok{"No"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{word}\NormalTok{(code\_desc, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{),}
    \FunctionTok{str\_starts}\NormalTok{(code\_desc, }\StringTok{"Operator"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{word}\NormalTok{(code\_desc, }\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{),}
    \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \FunctionTok{word}\NormalTok{(code\_desc,}\DecValTok{1}\NormalTok{))}
\NormalTok{         )}
\end{Highlighting}
\end{Shaded}

Let's also just restrict the analysis to causes that happen at least 50 times over 2019. To do the PCA, the dataframe also needs to be switched to wide format.

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{dwide }\OtherTok{\textless{}{-}}\NormalTok{ delay\_2019 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(line, station\_clean) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{n\_obs =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(n\_obs}\SpecialCharTok{\textgreater{}}\DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(code\_red) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{tot\_delay =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(tot\_delay) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(tot\_delay}\SpecialCharTok{\textgreater{}}\DecValTok{50}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(line, station\_clean, code\_red) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{n\_delay =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ code\_red, }\AttributeTok{values\_from =}\NormalTok{ n\_delay) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate\_all}\NormalTok{(}\AttributeTok{.funs =} \FunctionTok{funs}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(.), }\DecValTok{0}\NormalTok{, .)))}
\CommentTok{\#\textgreater{} \textasciigrave{}summarise()\textasciigrave{} has grouped output by \textquotesingle{}line\textquotesingle{}, \textquotesingle{}station\_clean\textquotesingle{}. You can override using the \textasciigrave{}.groups\textasciigrave{} argument.}
\CommentTok{\#\textgreater{} \textasciigrave{}mutate\_all()\textasciigrave{} ignored the following grouping variables:}
\CommentTok{\#\textgreater{} Columns \textasciigrave{}line\textasciigrave{}, \textasciigrave{}station\_clean\textasciigrave{}}
\CommentTok{\#\textgreater{} Use \textasciigrave{}mutate\_at(df, vars({-}group\_cols()), myoperation)\textasciigrave{} to silence the message.}
\CommentTok{\#\textgreater{} Warning: \textasciigrave{}funs()\textasciigrave{} was deprecated in dplyr 0.8.0.}
\CommentTok{\#\textgreater{} Please use a list of either functions or lambdas: }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   \# Simple named list: }
\CommentTok{\#\textgreater{}   list(mean = mean, median = median)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   \# Auto named with \textasciigrave{}tibble::lst()\textasciigrave{}: }
\CommentTok{\#\textgreater{}   tibble::lst(mean, median)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   \# Using lambdas}
\CommentTok{\#\textgreater{}   list(\textasciitilde{} mean(., trim = .2), \textasciitilde{} median(., na.rm = TRUE))}
\CommentTok{\#\textgreater{} This warning is displayed once every 8 hours.}
\CommentTok{\#\textgreater{} Call \textasciigrave{}lifecycle::last\_lifecycle\_warnings()\textasciigrave{} to see where this warning was generated.}
\end{Highlighting}
\end{Shaded}

Now we can quickly do some PCA.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delay\_pca }\OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(dwide[,}\DecValTok{3}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(dwide)])}

\NormalTok{df\_out }\OtherTok{\textless{}{-}} \FunctionTok{as\_tibble}\NormalTok{(delay\_pca}\SpecialCharTok{$}\NormalTok{x)}
\NormalTok{df\_out }\OtherTok{\textless{}{-}} \FunctionTok{bind\_cols}\NormalTok{(dwide }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(line, station\_clean), df\_out)}
\FunctionTok{head}\NormalTok{(df\_out)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 40}
\CommentTok{\#\textgreater{} \# Groups:   line, station\_clean [6]}
\CommentTok{\#\textgreater{}   line  station\_clean    PC1     PC2    PC3    PC4    PC5}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{} \textless{}chr\textgreater{}          \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 BD    BATHURST        6.50   26.9   {-}2.71 {-}10.8  {-}8.40 }
\CommentTok{\#\textgreater{} 2 BD    BAY            24.8     7.63  {-}2.19  {-}7.05  0.714}
\CommentTok{\#\textgreater{} 3 BD    BLOOR         {-}62.4  {-}112.    57.3  {-}23.4  {-}5.09 }
\CommentTok{\#\textgreater{} 4 BD    BROADVIEW      {-}6.60   28.1   {-}1.06 {-}14.0  {-}6.49 }
\CommentTok{\#\textgreater{} 5 BD    CASTLE         23.8    11.8   {-}1.31  {-}7.93 {-}3.62 }
\CommentTok{\#\textgreater{} 6 BD    CHESTER        24.6    {-}1.87 {-}18.6    2.75  1.85 }
\CommentTok{\#\textgreater{} \# ... with 33 more variables: PC6 \textless{}dbl\textgreater{}, PC7 \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   PC8 \textless{}dbl\textgreater{}, PC9 \textless{}dbl\textgreater{}, PC10 \textless{}dbl\textgreater{}, PC11 \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   PC12 \textless{}dbl\textgreater{}, PC13 \textless{}dbl\textgreater{}, PC14 \textless{}dbl\textgreater{}, PC15 \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   PC16 \textless{}dbl\textgreater{}, PC17 \textless{}dbl\textgreater{}, PC18 \textless{}dbl\textgreater{}, PC19 \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   PC20 \textless{}dbl\textgreater{}, PC21 \textless{}dbl\textgreater{}, PC22 \textless{}dbl\textgreater{}, PC23 \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   PC24 \textless{}dbl\textgreater{}, PC25 \textless{}dbl\textgreater{}, PC26 \textless{}dbl\textgreater{}, PC27 \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   PC28 \textless{}dbl\textgreater{}, PC29 \textless{}dbl\textgreater{}, PC30 \textless{}dbl\textgreater{}, PC31 \textless{}dbl\textgreater{}, ...}
\end{Highlighting}
\end{Shaded}

We can plot the first two principal components, and add labels to some of the outlying stations.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(df\_out,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{PC1,}\AttributeTok{y=}\NormalTok{PC2,}\AttributeTok{color=}\NormalTok{line )) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_text\_repel}\NormalTok{(}\AttributeTok{data =}\NormalTok{ df\_out }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(PC2}\SpecialCharTok{\textgreater{}}\DecValTok{100}\SpecialCharTok{|}\NormalTok{PC1}\SpecialCharTok{\textless{}}\DecValTok{100}\SpecialCharTok{*{-}}\DecValTok{1}\NormalTok{), }
                  \FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ station\_clean)}
\NormalTok{                  )}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-29-1.pdf}

We could also plot the factor loadings. We se some evidence that perhaps one is to do with the public, compared with another to do with the operator.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_out\_r }\OtherTok{\textless{}{-}} \FunctionTok{as\_tibble}\NormalTok{(delay\_pca}\SpecialCharTok{$}\NormalTok{rotation)}
\NormalTok{df\_out\_r}\SpecialCharTok{$}\NormalTok{feature }\OtherTok{\textless{}{-}} \FunctionTok{colnames}\NormalTok{(dwide[,}\DecValTok{3}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(dwide)])}

\NormalTok{df\_out\_r}
\CommentTok{\#\textgreater{} \# A tibble: 38 x 39}
\CommentTok{\#\textgreater{}         PC1      PC2        PC3       PC4      PC5      PC6}
\CommentTok{\#\textgreater{}       \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{}      \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1 {-}0.0412   0.0638   0.0133    {-}0.0467    0.0246   0.0184 }
\CommentTok{\#\textgreater{}  2 {-}0.0332  {-}0.00469 {-}0.0414    {-}0.00751   0.0201  {-}0.0122 }
\CommentTok{\#\textgreater{}  3 {-}0.135    0.207    0.0237    {-}0.144     0.135   {-}0.0381 }
\CommentTok{\#\textgreater{}  4 {-}0.0652   0.0475  {-}0.0443    {-}0.0251   {-}0.00139 {-}0.0748 }
\CommentTok{\#\textgreater{}  5 {-}0.00443  0.00878 {-}0.0000499 {-}0.000830  0.00967  0.00954}
\CommentTok{\#\textgreater{}  6 {-}0.0268  {-}0.00722 {-}0.00439    0.000534 {-}0.0151  {-}0.0125 }
\CommentTok{\#\textgreater{}  7 {-}0.0813   0.0960  {-}0.0462     0.0479   {-}0.0978  {-}0.0365 }
\CommentTok{\#\textgreater{}  8 {-}0.0117   0.0135   0.00548   {-}0.0294    0.0125   0.0377 }
\CommentTok{\#\textgreater{}  9 {-}0.516    0.655   {-}0.0177    {-}0.162    {-}0.221   {-}0.287  }
\CommentTok{\#\textgreater{} 10 {-}0.151    0.0826   0.0548     0.352    {-}0.397    0.281  }
\CommentTok{\#\textgreater{} \# ... with 28 more rows, and 33 more variables: PC7 \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   PC8 \textless{}dbl\textgreater{}, PC9 \textless{}dbl\textgreater{}, PC10 \textless{}dbl\textgreater{}, PC11 \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   PC12 \textless{}dbl\textgreater{}, PC13 \textless{}dbl\textgreater{}, PC14 \textless{}dbl\textgreater{}, PC15 \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   PC16 \textless{}dbl\textgreater{}, PC17 \textless{}dbl\textgreater{}, PC18 \textless{}dbl\textgreater{}, PC19 \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   PC20 \textless{}dbl\textgreater{}, PC21 \textless{}dbl\textgreater{}, PC22 \textless{}dbl\textgreater{}, PC23 \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   PC24 \textless{}dbl\textgreater{}, PC25 \textless{}dbl\textgreater{}, PC26 \textless{}dbl\textgreater{}, PC27 \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   PC28 \textless{}dbl\textgreater{}, PC29 \textless{}dbl\textgreater{}, PC30 \textless{}dbl\textgreater{}, PC31 \textless{}dbl\textgreater{}, ...}

\FunctionTok{ggplot}\NormalTok{(df\_out\_r,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{PC1,}\AttributeTok{y=}\NormalTok{PC2,}\AttributeTok{label=}\NormalTok{feature )) }\SpecialCharTok{+} \FunctionTok{geom\_text\_repel}\NormalTok{()}
\CommentTok{\#\textgreater{} Warning: ggrepel: 31 unlabeled data points (too many}
\CommentTok{\#\textgreater{} overlaps). Consider increasing max.overlaps}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-30-1.pdf}

\hypertarget{case-study---opinions-about-a-casino-in-toronto}{%
\section{Case study - Opinions about a casino in Toronto}\label{case-study---opinions-about-a-casino-in-toronto}}

\textbf{This was written by \href{https://michael-chong.github.io}{Michael Chong}.}

\hypertarget{data-preparation}{%
\subsection{Data preparation}\label{data-preparation}}

Here we use the \texttt{opendatatoronto} package again. See the previous case study for a deeper explanation of how the code below works.

The dataset I'm extracting below are the results from a survey in 2012 regarding the establishment of a casino in Toronto. More info available by following \href{https://open.toronto.ca/dataset/casino-survey-results/}{this link}. In this analysis, we'll be hoping to address the question: which demographic (age/gender) groups are more likely to be supportive of a new casino in Toronto?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Get the data}
\NormalTok{casino\_resource }\OtherTok{\textless{}{-}} 
  \FunctionTok{search\_packages}\NormalTok{(}\StringTok{"casino survey"}\NormalTok{)}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{list\_package\_resources}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(name }\SpecialCharTok{==} \StringTok{"toronto{-}casino{-}survey{-}results"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{get\_resource}\NormalTok{()}
\CommentTok{\#\textgreater{} New names:}
\CommentTok{\#\textgreater{} * \textasciigrave{}\textasciigrave{} {-}\textgreater{} ...93}
\CommentTok{\#\textgreater{} * \textasciigrave{}\textasciigrave{} {-}\textgreater{} ...94}
\FunctionTok{head}\NormalTok{(casino\_resource)}
\CommentTok{\#\textgreater{} $tblSurvey}
\CommentTok{\#\textgreater{} \# A tibble: 17,766 x 94}
\CommentTok{\#\textgreater{}    SurveyID Q1\_A   Q1\_B1 Q1\_B2 Q1\_B3 Q2\_A  Q2\_B  Q3\_A  Q3\_B }
\CommentTok{\#\textgreater{}       \textless{}dbl\textgreater{} \textless{}chr\textgreater{}  \textless{}chr\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{}}
\CommentTok{\#\textgreater{}  1        1 Stron\textasciitilde{} Do n\textasciitilde{} Do n\textasciitilde{} Do n\textasciitilde{} Does\textasciitilde{} "As \textasciitilde{} Not \textasciitilde{} Very\textasciitilde{}}
\CommentTok{\#\textgreater{}  2        2 Stron\textasciitilde{} Econ\textasciitilde{} Jobs  Arts\textasciitilde{} Fits\textasciitilde{} "Cos\textasciitilde{} Very\textasciitilde{} Very\textasciitilde{}}
\CommentTok{\#\textgreater{}  3        3 Stron\textasciitilde{} Ther\textasciitilde{} If t\textasciitilde{} \textless{}NA\textgreater{}  Fits\textasciitilde{} "Big\textasciitilde{} Very\textasciitilde{} Very\textasciitilde{}}
\CommentTok{\#\textgreater{}  4        4 Somew\textasciitilde{} beli\textasciitilde{} mone\textasciitilde{} evid\textasciitilde{} Does\textasciitilde{} "My \textasciitilde{} Very\textasciitilde{} Very\textasciitilde{}}
\CommentTok{\#\textgreater{}  5        5 Neutr\textasciitilde{} Like\textasciitilde{} Conc\textasciitilde{} \textless{}NA\textgreater{}  Neut\textasciitilde{} "Aga\textasciitilde{} Very\textasciitilde{} Very\textasciitilde{}}
\CommentTok{\#\textgreater{}  6        6 Stron\textasciitilde{} have\textasciitilde{} \textless{}NA\textgreater{}  \textless{}NA\textgreater{}  Does\textasciitilde{} "Tor\textasciitilde{} Not \textasciitilde{} Not \textasciitilde{}}
\CommentTok{\#\textgreater{}  7        7 Stron\textasciitilde{} The \textasciitilde{} Peop\textasciitilde{} We s\textasciitilde{} Does\textasciitilde{} "\#3 \textasciitilde{} Not \textasciitilde{} Not \textasciitilde{}}
\CommentTok{\#\textgreater{}  8        8 Stron\textasciitilde{} It w\textasciitilde{} Mora\textasciitilde{} \textless{}NA\textgreater{}  Does\textasciitilde{} "Cas\textasciitilde{} Very\textasciitilde{} Very\textasciitilde{}}
\CommentTok{\#\textgreater{}  9        9 Stron\textasciitilde{} It\textquotesingle{}s\textasciitilde{} traf\textasciitilde{} heal\textasciitilde{} Does\textasciitilde{} "No \textasciitilde{} Not \textasciitilde{} Very\textasciitilde{}}
\CommentTok{\#\textgreater{} 10       10 Stron\textasciitilde{} Toro\textasciitilde{} Avoi\textasciitilde{} Prov\textasciitilde{} Fits\textasciitilde{} "Tor\textasciitilde{} Very\textasciitilde{} Very\textasciitilde{}}
\CommentTok{\#\textgreater{} \# ... with 17,756 more rows, and 85 more variables:}
\CommentTok{\#\textgreater{} \#   Q3\_C \textless{}chr\textgreater{}, Q3\_D \textless{}chr\textgreater{}, Q3\_E \textless{}chr\textgreater{}, Q3\_F \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   Q3\_G \textless{}chr\textgreater{}, Q3\_H \textless{}chr\textgreater{}, Q3\_I \textless{}chr\textgreater{}, Q3\_J \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   Q3\_K \textless{}chr\textgreater{}, Q3\_L \textless{}chr\textgreater{}, Q3\_M \textless{}chr\textgreater{}, Q3\_N \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   Q3\_O \textless{}chr\textgreater{}, Q3\_P \textless{}chr\textgreater{}, Q3\_Q \textless{}chr\textgreater{}, Q3\_Q\_Other \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   Q3\_Comments \textless{}chr\textgreater{}, Q4\_A \textless{}chr\textgreater{}, Q5 \textless{}chr\textgreater{}, Q6 \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   Q6\_Comments \textless{}chr\textgreater{}, Q7\_A\_StandAlone \textless{}chr\textgreater{}, ...}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Sheet1}
\CommentTok{\#\textgreater{} \# A tibble: 0 x 0}
\end{Highlighting}
\end{Shaded}

The object \texttt{casino\_resource} isn't quite useable yet, because it's (inconveniently) stored as a \texttt{list} of 2 data frames:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Check what kind of object the casino\_resource object is}
\FunctionTok{class}\NormalTok{(casino\_resource)}
\CommentTok{\#\textgreater{} [1] "list"}
\end{Highlighting}
\end{Shaded}

If we just return the object, we can see that the second list item is empty, and we just want to keep the first one:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{casino\_resource}
\CommentTok{\#\textgreater{} $tblSurvey}
\CommentTok{\#\textgreater{} \# A tibble: 17,766 x 94}
\CommentTok{\#\textgreater{}    SurveyID Q1\_A   Q1\_B1 Q1\_B2 Q1\_B3 Q2\_A  Q2\_B  Q3\_A  Q3\_B }
\CommentTok{\#\textgreater{}       \textless{}dbl\textgreater{} \textless{}chr\textgreater{}  \textless{}chr\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{}}
\CommentTok{\#\textgreater{}  1        1 Stron\textasciitilde{} Do n\textasciitilde{} Do n\textasciitilde{} Do n\textasciitilde{} Does\textasciitilde{} "As \textasciitilde{} Not \textasciitilde{} Very\textasciitilde{}}
\CommentTok{\#\textgreater{}  2        2 Stron\textasciitilde{} Econ\textasciitilde{} Jobs  Arts\textasciitilde{} Fits\textasciitilde{} "Cos\textasciitilde{} Very\textasciitilde{} Very\textasciitilde{}}
\CommentTok{\#\textgreater{}  3        3 Stron\textasciitilde{} Ther\textasciitilde{} If t\textasciitilde{} \textless{}NA\textgreater{}  Fits\textasciitilde{} "Big\textasciitilde{} Very\textasciitilde{} Very\textasciitilde{}}
\CommentTok{\#\textgreater{}  4        4 Somew\textasciitilde{} beli\textasciitilde{} mone\textasciitilde{} evid\textasciitilde{} Does\textasciitilde{} "My \textasciitilde{} Very\textasciitilde{} Very\textasciitilde{}}
\CommentTok{\#\textgreater{}  5        5 Neutr\textasciitilde{} Like\textasciitilde{} Conc\textasciitilde{} \textless{}NA\textgreater{}  Neut\textasciitilde{} "Aga\textasciitilde{} Very\textasciitilde{} Very\textasciitilde{}}
\CommentTok{\#\textgreater{}  6        6 Stron\textasciitilde{} have\textasciitilde{} \textless{}NA\textgreater{}  \textless{}NA\textgreater{}  Does\textasciitilde{} "Tor\textasciitilde{} Not \textasciitilde{} Not \textasciitilde{}}
\CommentTok{\#\textgreater{}  7        7 Stron\textasciitilde{} The \textasciitilde{} Peop\textasciitilde{} We s\textasciitilde{} Does\textasciitilde{} "\#3 \textasciitilde{} Not \textasciitilde{} Not \textasciitilde{}}
\CommentTok{\#\textgreater{}  8        8 Stron\textasciitilde{} It w\textasciitilde{} Mora\textasciitilde{} \textless{}NA\textgreater{}  Does\textasciitilde{} "Cas\textasciitilde{} Very\textasciitilde{} Very\textasciitilde{}}
\CommentTok{\#\textgreater{}  9        9 Stron\textasciitilde{} It\textquotesingle{}s\textasciitilde{} traf\textasciitilde{} heal\textasciitilde{} Does\textasciitilde{} "No \textasciitilde{} Not \textasciitilde{} Very\textasciitilde{}}
\CommentTok{\#\textgreater{} 10       10 Stron\textasciitilde{} Toro\textasciitilde{} Avoi\textasciitilde{} Prov\textasciitilde{} Fits\textasciitilde{} "Tor\textasciitilde{} Very\textasciitilde{} Very\textasciitilde{}}
\CommentTok{\#\textgreater{} \# ... with 17,756 more rows, and 85 more variables:}
\CommentTok{\#\textgreater{} \#   Q3\_C \textless{}chr\textgreater{}, Q3\_D \textless{}chr\textgreater{}, Q3\_E \textless{}chr\textgreater{}, Q3\_F \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   Q3\_G \textless{}chr\textgreater{}, Q3\_H \textless{}chr\textgreater{}, Q3\_I \textless{}chr\textgreater{}, Q3\_J \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   Q3\_K \textless{}chr\textgreater{}, Q3\_L \textless{}chr\textgreater{}, Q3\_M \textless{}chr\textgreater{}, Q3\_N \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   Q3\_O \textless{}chr\textgreater{}, Q3\_P \textless{}chr\textgreater{}, Q3\_Q \textless{}chr\textgreater{}, Q3\_Q\_Other \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   Q3\_Comments \textless{}chr\textgreater{}, Q4\_A \textless{}chr\textgreater{}, Q5 \textless{}chr\textgreater{}, Q6 \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   Q6\_Comments \textless{}chr\textgreater{}, Q7\_A\_StandAlone \textless{}chr\textgreater{}, ...}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Sheet1}
\CommentTok{\#\textgreater{} \# A tibble: 0 x 0}
\end{Highlighting}
\end{Shaded}

So, let's only keep the first item by indexing the list with double square brackets:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{casino\_data }\OtherTok{\textless{}{-}}\NormalTok{ casino\_resource[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

Let's check out what the first couple rows of the dataframe looks like. By default, \texttt{head()} returns the first 6 rows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(casino\_data) }
\CommentTok{\#\textgreater{} \# A tibble: 6 x 94}
\CommentTok{\#\textgreater{}   SurveyID Q1\_A   Q1\_B1  Q1\_B2 Q1\_B3 Q2\_A  Q2\_B  Q3\_A  Q3\_B }
\CommentTok{\#\textgreater{}      \textless{}dbl\textgreater{} \textless{}chr\textgreater{}  \textless{}chr\textgreater{}  \textless{}chr\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{} \textless{}chr\textgreater{}}
\CommentTok{\#\textgreater{} 1        1 Stron\textasciitilde{} Do no\textasciitilde{} Do n\textasciitilde{} Do n\textasciitilde{} Does\textasciitilde{} "As \textasciitilde{} Not \textasciitilde{} Very\textasciitilde{}}
\CommentTok{\#\textgreater{} 2        2 Stron\textasciitilde{} Econo\textasciitilde{} Jobs  Arts\textasciitilde{} Fits\textasciitilde{} "Cos\textasciitilde{} Very\textasciitilde{} Very\textasciitilde{}}
\CommentTok{\#\textgreater{} 3        3 Stron\textasciitilde{} There\textasciitilde{} If t\textasciitilde{} \textless{}NA\textgreater{}  Fits\textasciitilde{} "Big\textasciitilde{} Very\textasciitilde{} Very\textasciitilde{}}
\CommentTok{\#\textgreater{} 4        4 Somew\textasciitilde{} belie\textasciitilde{} mone\textasciitilde{} evid\textasciitilde{} Does\textasciitilde{} "My \textasciitilde{} Very\textasciitilde{} Very\textasciitilde{}}
\CommentTok{\#\textgreater{} 5        5 Neutr\textasciitilde{} Like \textasciitilde{} Conc\textasciitilde{} \textless{}NA\textgreater{}  Neut\textasciitilde{} "Aga\textasciitilde{} Very\textasciitilde{} Very\textasciitilde{}}
\CommentTok{\#\textgreater{} 6        6 Stron\textasciitilde{} have \textasciitilde{} \textless{}NA\textgreater{}  \textless{}NA\textgreater{}  Does\textasciitilde{} "Tor\textasciitilde{} Not \textasciitilde{} Not \textasciitilde{}}
\CommentTok{\#\textgreater{} \# ... with 85 more variables: Q3\_C \textless{}chr\textgreater{}, Q3\_D \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   Q3\_E \textless{}chr\textgreater{}, Q3\_F \textless{}chr\textgreater{}, Q3\_G \textless{}chr\textgreater{}, Q3\_H \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   Q3\_I \textless{}chr\textgreater{}, Q3\_J \textless{}chr\textgreater{}, Q3\_K \textless{}chr\textgreater{}, Q3\_L \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   Q3\_M \textless{}chr\textgreater{}, Q3\_N \textless{}chr\textgreater{}, Q3\_O \textless{}chr\textgreater{}, Q3\_P \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   Q3\_Q \textless{}chr\textgreater{}, Q3\_Q\_Other \textless{}chr\textgreater{}, Q3\_Comments \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   Q4\_A \textless{}chr\textgreater{}, Q5 \textless{}chr\textgreater{}, Q6 \textless{}chr\textgreater{}, Q6\_Comments \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   Q7\_A\_StandAlone \textless{}chr\textgreater{}, Q7\_A\_Integrated \textless{}chr\textgreater{}, ...}
\end{Highlighting}
\end{Shaded}

Unfortunately the column names aren't very informative. For simplicity, we'll use the `.pdf' questionnaire that accompanies this dataset from the Toronto Open Data website. Alternatively, we could get and parse the `readme' through the R package. \href{https://ckan0.cf.opendata.inter.prod-toronto.ca/dataset/427ca4cd-168a-4a37-883d-4a574277caf5/resource/ae135d6a-6921-4905-bc79-516fcd428b7b/download/toronto-casino-survey-feedback-form.pdf}{Here's a link to the questionnaire}.

Question 1 indicates the level of support for a casino in Toronto. We'll use this as the response variable.

Concerning potential predictor variables, most of the questions ask respondents about their opinions on different aspects of a potential casino development, which aren't particularly useful towards our cause. The only demographic variables are \texttt{Age} and \texttt{Gender}, so let's choose these.

Here I'm also going to rename the columns so that my resulting data frame has columns `opinion', `age', and `gender'.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Narrow down the dataframe to our variables of interest}
\NormalTok{casino\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  casino\_data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(Q1\_A, Age, Gender) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{opinion =}\NormalTok{ Q1\_A, }\AttributeTok{age =}\NormalTok{ Age, }\AttributeTok{gender =}\NormalTok{ Gender)}

\CommentTok{\# Look at first couple rows:}
\FunctionTok{head}\NormalTok{(casino\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{}   opinion                   age   gender}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}                     \textless{}chr\textgreater{} \textless{}chr\textgreater{} }
\CommentTok{\#\textgreater{} 1 Strongly Opposed          25{-}34 Male  }
\CommentTok{\#\textgreater{} 2 Strongly in Favour        35{-}44 Female}
\CommentTok{\#\textgreater{} 3 Strongly in Favour        55{-}64 Male  }
\CommentTok{\#\textgreater{} 4 Somewhat Opposed          25{-}34 Male  }
\CommentTok{\#\textgreater{} 5 Neutral or Mixed Feelings 25{-}34 Female}
\CommentTok{\#\textgreater{} 6 Strongly Opposed          45{-}54 Female}
\end{Highlighting}
\end{Shaded}

\hypertarget{some-visual-exploration-and-more-cleanup-of-course}{%
\subsection{Some visual exploration (and more cleanup, of course)}\label{some-visual-exploration-and-more-cleanup-of-course}}

Let's first do some quick exploration to get a feel for what's going on in the data. We'll first calculate proportions of casino support for each age-gender combination:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate proportions}
\NormalTok{casino\_summary }\OtherTok{\textless{}{-}}\NormalTok{ casino\_data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(age, gender, opinion) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{n =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# Count the number in each group and response}
  \FunctionTok{group\_by}\NormalTok{(age, gender) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{prop =}\NormalTok{ n}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(n)) }\CommentTok{\# Calculate proportions within each group}
\CommentTok{\#\textgreater{} \textasciigrave{}summarise()\textasciigrave{} has grouped output by \textquotesingle{}age\textquotesingle{}, \textquotesingle{}gender\textquotesingle{}. You can override using the \textasciigrave{}.groups\textasciigrave{} argument.}
\end{Highlighting}
\end{Shaded}

Some notes:

\begin{itemize}
\tightlist
\item
  we use \texttt{geom\_col()} to make a bar chart,
\item
  \texttt{facet\_grid()} modifies the plot so that the plot has panels that correspond only to certain values of discrete variables (in this case, we will ``facet'' by \texttt{age} and \texttt{gender}). This is helpful in this case because we are interested in how the distribution of \texttt{opinion}s changes by \texttt{age} and \texttt{gender}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(casino\_summary) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ opinion, }\AttributeTok{y =}\NormalTok{ prop)) }\SpecialCharTok{+} \CommentTok{\# Specify a histogram of opinion responses}
  \FunctionTok{facet\_grid}\NormalTok{(age}\SpecialCharTok{\textasciitilde{}}\NormalTok{gender) }\SpecialCharTok{+} \CommentTok{\#Facet by age and gender}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle =} \DecValTok{90}\NormalTok{)) }\CommentTok{\# Rotate the x{-}axis labels to be readable}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-38-1.pdf}

Some things to note:

\begin{itemize}
\tightlist
\item
  the x-axis labels are out of order in the sense that they are not in a monotone order of increasing/decreasing support
\item
  there are \texttt{NA} values in \texttt{opinion}, \texttt{age}, and \texttt{gender}, as well as ``Prefer not to disclose'' responses
\end{itemize}

\hypertarget{getting-the-data-into-a-more-model-suitable-format}{%
\subsection{Getting the data into a more model-suitable format}\label{getting-the-data-into-a-more-model-suitable-format}}

First we need to get rid of responses that aren't suitable. For simplicity we'll assume that \texttt{NA} values and ``Prefer not to disclose'' responses occur randomly, and remove them from our dataset (note in reality this assumption might not hold up and we might want to be more careful). Let's check how many rows are in the original dataset:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# nrow() returns the number of rows in a dataframe:}
\FunctionTok{nrow}\NormalTok{(casino\_data)}
\CommentTok{\#\textgreater{} [1] 17766}
\end{Highlighting}
\end{Shaded}

Now let's \texttt{dplyr::filter()} accordingly to omit the responses we don't want. In case you're unfamiliar, I'm going to make use of:

\begin{itemize}
\tightlist
\item
  \texttt{is.na()}, which returns \texttt{TRUE} if the argument is \texttt{NA},
\item
  the \texttt{!} operator, which flips \texttt{TRUE} and \texttt{FALSE}. So for instance, \texttt{!is.na(x)} will return \texttt{TRUE} if \texttt{x} is NOT \texttt{NA}, which is what we want to keep.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{casino\_data }\OtherTok{\textless{}{-}}\NormalTok{ casino\_data }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# Only keep rows with non{-}NA:}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(opinion), }\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(age), }\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(gender)) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# Only keep rows where age and gender are disclosed:}
  \FunctionTok{filter}\NormalTok{(age }\SpecialCharTok{!=} \StringTok{"Prefer not to disclose"}\NormalTok{, gender }\SpecialCharTok{!=} \StringTok{"Prefer not to disclose"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's check how many rows of data we're left with:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{nrow}\NormalTok{(casino\_data)}
\CommentTok{\#\textgreater{} [1] 13658}
\end{Highlighting}
\end{Shaded}

Now we need to convert the response variable into binary. To clean up the first problem (response variables out of order), we might as well take this opportunity to convert these into a format suitable for our model. In a logistic regression, we would like our response variable to be binary, but in this case we have 5 possible categories ranging from ``Strongly Opposed'' to ``Strongly in Favour''. We'll recategorize them into a new `supportive\_or\_not' variable as follows.

\begin{itemize}
\tightlist
\item
  `supportive = 1' if ``Strongly in Favour'' or ``Somewhat in Favour''
\item
  `supportive = 0' if ``Neutral or Mixed Feelings'', ``Somewhat Opposed'', or ``Strongly Opposed''
\end{itemize}

We do this with the \texttt{dplyr::mutate()} function, which creates new columns (possibly as functions of existing columns), and \texttt{dplyr::case\_when()}, which provides a way to assign values conditional on if-statements. The syntax here is a little strange. On the LHS of the \texttt{\textasciitilde{}} is the ``if'' condition, and the RHS of the tilde is the value to return. For example, `x == 0 \textasciitilde{} 3' would return 3 when `x' is 0.

Another commonly used operator here is the \texttt{\%in\%} operator, which checks whether something is an element of a vector, for instance, e.g.:

\begin{itemize}
\tightlist
\item
  \texttt{1\ \%in\%\ c(1,\ 3,\ 4)} returns \texttt{TRUE}
\item
  \texttt{2\ \%in\%\ c(1,\ 3,\ 4)} returns \texttt{FALSE}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Store possible opinions in vectors}
\NormalTok{yes\_opinions }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Strongly in Favour"}\NormalTok{, }\StringTok{"Somewhat in Favour"}\NormalTok{)}
\NormalTok{no\_opinions }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Neutral or Mixed Feelings"}\NormalTok{, }\StringTok{"Somewhat Opposed"}\NormalTok{, }\StringTok{"Strongly Opposed"}\NormalTok{)}

\CommentTok{\# Create \textasciigrave{}supportive\textasciigrave{} column:}
\NormalTok{casino\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  casino\_data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{supportive =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{    opinion }\SpecialCharTok{\%in\%}\NormalTok{ yes\_opinions }\SpecialCharTok{\textasciitilde{}} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# Assign TRUE}
\NormalTok{    opinion }\SpecialCharTok{\%in\%}\NormalTok{ no\_opinions }\SpecialCharTok{\textasciitilde{}} \ConstantTok{FALSE}  \CommentTok{\# Assign FALSE}
\NormalTok{  ))}
\end{Highlighting}
\end{Shaded}

Now we need to convert age to a numeric variable. Age in this survey is given in age groups. Let's instead map it to a numeric variable so that we can more easily talk about trends with age. We'll map the youngest age to \texttt{1}, and so on:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{casino\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  casino\_data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{age\_group =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{    age }\SpecialCharTok{==} \StringTok{"Under 15"} \SpecialCharTok{\textasciitilde{}} \DecValTok{0}\NormalTok{,}
\NormalTok{    age }\SpecialCharTok{==} \StringTok{"15{-}24"} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{,}
\NormalTok{    age }\SpecialCharTok{==} \StringTok{"25{-}34"} \SpecialCharTok{\textasciitilde{}} \DecValTok{2}\NormalTok{,}
\NormalTok{    age }\SpecialCharTok{==} \StringTok{"35{-}44"} \SpecialCharTok{\textasciitilde{}} \DecValTok{3}\NormalTok{,}
\NormalTok{    age }\SpecialCharTok{==} \StringTok{"45{-}54"} \SpecialCharTok{\textasciitilde{}} \DecValTok{4}\NormalTok{,}
\NormalTok{    age }\SpecialCharTok{==} \StringTok{"55{-}64"} \SpecialCharTok{\textasciitilde{}} \DecValTok{5}\NormalTok{,}
\NormalTok{    age }\SpecialCharTok{==} \StringTok{"65 or older"} \SpecialCharTok{\textasciitilde{}} \DecValTok{6}
\NormalTok{  ))}
\end{Highlighting}
\end{Shaded}

Now let's make the same plot again, with our new processed data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{casino\_summary2 }\OtherTok{\textless{}{-}} 
\NormalTok{  casino\_data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(age\_group, gender, supportive) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{n =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# Count the number in each group and response}
  \FunctionTok{group\_by}\NormalTok{(age\_group, gender) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{prop =}\NormalTok{ n}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(n)) }\CommentTok{\# Calculate proportions within each group}
\CommentTok{\#\textgreater{} \textasciigrave{}summarise()\textasciigrave{} has grouped output by \textquotesingle{}age\_group\textquotesingle{}, \textquotesingle{}gender\textquotesingle{}. You can override using the \textasciigrave{}.groups\textasciigrave{} argument.}

\FunctionTok{ggplot}\NormalTok{(casino\_summary2) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{(age\_group }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ supportive, }\AttributeTok{y =}\NormalTok{ prop)) }
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-44-1.pdf}

We can sort of see some difference in the distribution between different panels. To formalize this, we can run a logistic regression.

\hypertarget{logistic-regression}{%
\subsection{Logistic Regression}\label{logistic-regression}}

Now, we're set up to feed it to the regression. We can do this with \texttt{glm()}, which allows us to fit generalized linear models.

We use \texttt{family\ =\ "binomial"} to specify a logistic regression, and our formula is \texttt{supportive\ \textasciitilde{}\ age\_group\ +\ gender}, which indicates that \texttt{supportive} is the (binary) response variable since it's on the LHS, and \texttt{age\_group} and \texttt{gender} are our predictor variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{casino\_glm }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(supportive }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age\_group }\SpecialCharTok{+}\NormalTok{ gender, }\AttributeTok{data =}\NormalTok{ casino\_data, }\AttributeTok{family =} \StringTok{"binomial"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can take a look at the results of running the GLM using \texttt{summary()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(casino\_glm)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = supportive \textasciitilde{} age\_group + gender, family = "binomial", }
\CommentTok{\#\textgreater{}     data = casino\_data)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}1.0107  {-}0.8888  {-}0.6804   1.4249   1.8822  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                     Estimate Std. Error z value Pr(\textgreater{}|z|)}
\CommentTok{\#\textgreater{} (Intercept)         {-}1.10594    0.05863 {-}18.862  \textless{} 2e{-}16}
\CommentTok{\#\textgreater{} age\_group           {-}0.07983    0.01376  {-}5.801 6.59e{-}09}
\CommentTok{\#\textgreater{} genderMale           0.70036    0.04027  17.390  \textless{} 2e{-}16}
\CommentTok{\#\textgreater{} genderTransgendered  0.69023    0.39276   1.757   0.0789}
\CommentTok{\#\textgreater{}                        }
\CommentTok{\#\textgreater{} (Intercept)         ***}
\CommentTok{\#\textgreater{} age\_group           ***}
\CommentTok{\#\textgreater{} genderMale          ***}
\CommentTok{\#\textgreater{} genderTransgendered .  }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  }
\CommentTok{\#\textgreater{} 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for binomial family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 16010  on 13657  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 15653  on 13654  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 15661}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 4}
\end{Highlighting}
\end{Shaded}

Interpretation can be a little tricky. Here are some important things to note about our results:

Firstly, we have a numeric age group variable. Remember that we coded \texttt{age\_group} as numbers 1 to 5. Because we've used age groups instead of age, we have to be careful with how we phrase our conclusion. The coefficient estimate corresponds to the effect of moving up a unit on the age group scale (e.g.~from the 25-34 age group to the 35-44 age group), rather than 1 year in age (e.g.~from age 28 to 29).

Secondly, the results are in log-odds ratios. The effect estimates are on the log-odds scale. This means the effect of -0.07983 for \texttt{age\_group} is interpreted as: `for each unit increase in \texttt{age\_group}, we estimate a 0.07983 decrease in the log-odds of being supportive of a casino'.

We could exponentiate the coefficient estimate to make this at least a little easier to interpret. The number we get is interpreted as a factor for the odds.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.07983}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 0.9232733}
\end{Highlighting}
\end{Shaded}

So our (cleaner) interpretation is: `the odds of an individuals of the same gender being pro-casino are predicted to change by a factor of 0.9232733 for each unit increase in \texttt{age\_group}'

Finally, we have a baseline category. First, note that because we have categorical variables, the \texttt{gender} coefficients are relative to a ``baseline'' category. The value of \texttt{gender} that doesn't appear in the table, \texttt{Female}, is implicitly used as our baseline gender category. Technical note: if the variable is stored as a \texttt{character} class, then \texttt{glm()} will choose the alphabetically first value to use as the baseline.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{exp}\NormalTok{(}\FloatTok{0.70036}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 2.014478}
\end{Highlighting}
\end{Shaded}

So, the interpretation of the \texttt{genderMale} coefficient is: `the odds of a male individual supporting a casino is 2.0144778 times higher than a female individual of the same \texttt{age\_group}.'

We can make estimates in a variety of ways. First we'll look at it manually.

Using the formula found in \citet[4.3.3]{islr}, we can make estimates for an individual of certain characteristics. Suppose we wanted to predict the the probability of supporting a Toronto casino for an individual who was 36 and identified as transgender. Then:

\begin{itemize}
\tightlist
\item
  \texttt{age\_group} takes a value of 3, since they are in the age group of 35-44 coded as 3,
\item
  \texttt{genderTransgendered} takes a value of 1
\end{itemize}

First, let's extract the coefficient estimates as a vector using \texttt{coefficients()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coefs }\OtherTok{\textless{}{-}} \FunctionTok{coefficients}\NormalTok{(casino\_glm)}
\NormalTok{coefs}
\CommentTok{\#\textgreater{}         (Intercept)           age\_group          genderMale }
\CommentTok{\#\textgreater{}         {-}1.10593925         {-}0.07983372          0.70036199 }
\CommentTok{\#\textgreater{} genderTransgendered }
\CommentTok{\#\textgreater{}          0.69022910}
\end{Highlighting}
\end{Shaded}

Since this vector is labeled, we can index it using square brackets and names. For instance:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coefs[}\StringTok{"age\_group"}\NormalTok{]}
\CommentTok{\#\textgreater{}   age\_group }
\CommentTok{\#\textgreater{} {-}0.07983372}
\end{Highlighting}
\end{Shaded}

So first let's evaluate the exponent term \(e^{\beta_0 + \cdots + \beta_p X_p}\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{exp\_term }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(coefs[}\StringTok{"(Intercept)"}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ coefs[}\StringTok{"age\_group"}\NormalTok{]}\SpecialCharTok{*}\DecValTok{3} \SpecialCharTok{+}\NormalTok{ coefs[}\StringTok{"genderTransgendered"}\NormalTok{]}\SpecialCharTok{*}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now evaluate the expression that gives the probability of casino support:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# The unname() command just takes off the label that it "inherited" from the coefs vector.}
\CommentTok{\# (don\textquotesingle{}t worry about it, doesn\textquotesingle{}t affect any functionality)}
\FunctionTok{unname}\NormalTok{(exp\_term }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+}\NormalTok{ exp\_term))}
\CommentTok{\#\textgreater{} [1] 0.3418161}
\end{Highlighting}
\end{Shaded}

That works, but there are more stream-lined ways. Thankfully R comes with a convenient function to make prediction estimates from a \texttt{glm()}. We do this using the \texttt{predict()} function. First, we need to make a dataframe that has the relevant variables and values that we're interested in predicting. We'll use the same values as before:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prediction\_df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{age\_group =} \DecValTok{3}\NormalTok{, }\AttributeTok{gender =} \StringTok{"Transgendered"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The dataframe looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prediction\_df}
\CommentTok{\#\textgreater{}   age\_group        gender}
\CommentTok{\#\textgreater{} 1         3 Transgendered}
\end{Highlighting}
\end{Shaded}

Then we feed it into the \texttt{predict()} function, along with our \texttt{glm} object. To get the probability, we need to specify \texttt{type\ =\ "response"}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{predict}\NormalTok{(casino\_glm, }\AttributeTok{newdata =}\NormalTok{ prediction\_df, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\CommentTok{\#\textgreater{}         1 }
\CommentTok{\#\textgreater{} 0.3418161}
\end{Highlighting}
\end{Shaded}

This matches the probability we got from doing this manually, yay!

\hypertarget{case-study---airbnb-listing-in-toronto}{%
\section{Case study - Airbnb listing in Toronto}\label{case-study---airbnb-listing-in-toronto}}

\hypertarget{essentials}{%
\subsection{Essentials}\label{essentials}}

In this case study we look at Airbnb listings in Toronto.

\hypertarget{set-up-1}{%
\subsection{Set up}\label{set-up-1}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(broom) }\CommentTok{\# Helps with model outputs etc}
\FunctionTok{library}\NormalTok{(here) }\CommentTok{\# Helps with specifying path names}
\FunctionTok{library}\NormalTok{(janitor) }\CommentTok{\# Helps with initial data cleaning and pretty tables}
\FunctionTok{library}\NormalTok{(tidymodels) }\CommentTok{\# Help with modelling}
\FunctionTok{library}\NormalTok{(tidyverse) }
\FunctionTok{library}\NormalTok{(visdat) }\CommentTok{\# Helps check missing values}
\end{Highlighting}
\end{Shaded}

\hypertarget{get-data}{%
\subsection{Get data}\label{get-data}}

The dataset is from Inside Airbnb \citep{airbnbdata}. The \texttt{here} package will help \citep{citehere}.

We can give \texttt{read\_csv()} a link to where the dataset is and it will download it. This helps with reproducibility because the source is clear. But, as that link could change at any time, longer-term reproducibility, as well as wanting to minimise the effect on the Inside Airbnb servers, suggest that we should also save a local copy of the data and then use that. (As the original data is not ours, we should not make that public without first getting written permission.)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# For reproducibility}
\CommentTok{\# airbnb\_data\_reduced \textless{}{-} read\_csv("http://data.insideairbnb.com/canada/on/toronto/2021{-}01{-}02/data/listings.csv.gz", guess\_max = 20000)}
\CommentTok{\# write\_csv(airbnb\_data\_reduced, "week\_6/data/airbnb\_toronto\_2019{-}12{-}07.csv")}

\CommentTok{\# For actual work}
\NormalTok{airbnb\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(here}\SpecialCharTok{::}\FunctionTok{here}\NormalTok{(}\StringTok{"dont\_push/airbnb\_toronto\_2021\_january{-}listings.csv"}\NormalTok{), }\AttributeTok{guess\_max =} \DecValTok{20000}\NormalTok{)}

\CommentTok{\# The guess\_max option in read\_csv helps us avoid having to specify the column types. Usually read\_csv takes a best guess at the column types based on the first few rows. But sometimes those first ones are misleading and so guess\_max forces it to look at a larger number of rows to try to work out what is going on.}
\end{Highlighting}
\end{Shaded}

\hypertarget{clean-data-1}{%
\subsection{Clean data}\label{clean-data-1}}

There are an enormous number of columns, so we'll just select a few.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(airbnb\_data) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{length}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] 74}

\NormalTok{airbnb\_data\_selected }\OtherTok{\textless{}{-}} 
\NormalTok{  airbnb\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(host\_id, }
\NormalTok{         host\_since, }
\NormalTok{         host\_response\_time, }
\NormalTok{         host\_is\_superhost, }
\NormalTok{         host\_listings\_count,}
\NormalTok{         host\_total\_listings\_count,}
\NormalTok{         host\_neighbourhood, }
\NormalTok{         host\_listings\_count, }
\NormalTok{         neighbourhood\_cleansed, }
\NormalTok{         room\_type, }
\NormalTok{         bathrooms, }
\NormalTok{         bedrooms, }
\NormalTok{         price, }
\NormalTok{         number\_of\_reviews, }
\NormalTok{         has\_availability, }
\NormalTok{         review\_scores\_rating, }
\NormalTok{         review\_scores\_accuracy, }
\NormalTok{         review\_scores\_cleanliness, }
\NormalTok{         review\_scores\_checkin, }
\NormalTok{         review\_scores\_communication, }
\NormalTok{         review\_scores\_location, }
\NormalTok{         review\_scores\_value}
\NormalTok{         )}
\end{Highlighting}
\end{Shaded}

We might like to have a brief look at the dataset to see if anything weird is going on. There are a bunch of ways of doing this.

A few things jump out:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  There are a character variables that should probably be numerics or dates/times: \texttt{host\_response\_time}, price, \texttt{weekly\_price}, \texttt{monthly\_price}, \texttt{cleaning\_fee.}
\item
  Weekly and monthly price is missing in an overwhelming number of observations.
\item
  Roughly a fifth of observations are missing a review score and there seems like there is some correlation between those review-type variables.
\item
  There are two variants of the neighbourhood name.
\item
  There are NAs in \texttt{host\_is\_superhost}.
\item
  The reviews seem really skewed.
\item
  There is someone who has 328 properties on Airbnb.
\end{enumerate}

\hypertarget{price}{%
\subsubsection{Price}\label{price}}

First we need to convert to a numeric. This is a common problem, and you need to be a little careful that it doesn't all just convert to NAs. In our case if we just force the price data to be a numeric then it will go to NA because there are a lot of characters that R doesn't know how to convert, e.g.~what is the numeric for `\$'? So we need to remove those characters first.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected}\SpecialCharTok{$}\NormalTok{price }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] "$469.00" "$96.00"  "$64.00"  "$70.00"  "$45.00" }
\CommentTok{\#\textgreater{} [6] "$127.00"}

\CommentTok{\# First work out what is going on}
\NormalTok{airbnb\_data\_selected}\SpecialCharTok{$}\NormalTok{price }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{str\_split}\NormalTok{(}\StringTok{""}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{unlist}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{unique}\NormalTok{()}
\CommentTok{\#\textgreater{}  [1] "$" "4" "6" "9" "." "0" "7" "5" "1" "2" "3" "8" ","}
\CommentTok{\# It\textquotesingle{}s clear that \textquotesingle{}$\textquotesingle{} needs to go. The only odd thing is \textquotesingle{},\textquotesingle{} so take a look at those:}
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(price) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(price, }\StringTok{","}\NormalTok{))}
\CommentTok{\#\textgreater{} \# A tibble: 145 x 1}
\CommentTok{\#\textgreater{}    price    }
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}    }
\CommentTok{\#\textgreater{}  1 $1,724.00}
\CommentTok{\#\textgreater{}  2 $1,000.00}
\CommentTok{\#\textgreater{}  3 $1,100.00}
\CommentTok{\#\textgreater{}  4 $1,450.00}
\CommentTok{\#\textgreater{}  5 $1,019.00}
\CommentTok{\#\textgreater{}  6 $1,000.00}
\CommentTok{\#\textgreater{}  7 $1,300.00}
\CommentTok{\#\textgreater{}  8 $2,142.00}
\CommentTok{\#\textgreater{}  9 $2,000.00}
\CommentTok{\#\textgreater{} 10 $1,200.00}
\CommentTok{\#\textgreater{} \# ... with 135 more rows}
\CommentTok{\# It\textquotesingle{}s clear that the data is just nicely formatted, but we need to remove the comma:}
\NormalTok{airbnb\_data\_selected }\OtherTok{\textless{}{-}} 
\NormalTok{  airbnb\_data\_selected }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{price =} \FunctionTok{str\_remove}\NormalTok{(price, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{$"}\NormalTok{),}
         \AttributeTok{price =} \FunctionTok{str\_remove}\NormalTok{(price, }\StringTok{","}\NormalTok{),}
         \AttributeTok{price =} \FunctionTok{as.integer}\NormalTok{(price)}
\NormalTok{         )}
\end{Highlighting}
\end{Shaded}

Now we can have a look at prices.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Look at distribution of price}
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Price per night"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of properties"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-60-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#  We use bins with a width of 10, so that\textquotesingle{}s going to aggregate prices into 10s so that we don\textquotesingle{}t get overwhelmed with bars.}
\end{Highlighting}
\end{Shaded}

So we have some outliers. Let's zoom in on prices that are more than \$1,000.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Look at distribution of high prices}
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(price }\SpecialCharTok{\textgreater{}} \DecValTok{1000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Price per night"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of properties"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-61-1.pdf}

Let's look in more detail at those with a price more than \$4,999.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(price }\SpecialCharTok{\textgreater{}} \DecValTok{4999}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 11 x 21}
\CommentTok{\#\textgreater{}      host\_id host\_since host\_response\_time host\_is\_superhost}
\CommentTok{\#\textgreater{}        \textless{}dbl\textgreater{} \textless{}date\textgreater{}     \textless{}chr\textgreater{}              \textless{}lgl\textgreater{}            }
\CommentTok{\#\textgreater{}  1   9310264 2013{-}10{-}08 N/A                FALSE            }
\CommentTok{\#\textgreater{}  2  99076885 2016{-}10{-}10 N/A                FALSE            }
\CommentTok{\#\textgreater{}  3 119693302 2017{-}03{-}07 N/A                FALSE            }
\CommentTok{\#\textgreater{}  4 147240941 2017{-}08{-}22 N/A                FALSE            }
\CommentTok{\#\textgreater{}  5 174625477 2018{-}02{-}21 N/A                FALSE            }
\CommentTok{\#\textgreater{}  6  70349386 2016{-}05{-}04 N/A                FALSE            }
\CommentTok{\#\textgreater{}  7 215966560 2018{-}09{-}17 N/A                FALSE            }
\CommentTok{\#\textgreater{}  8  12931053 2014{-}03{-}08 within a few hours TRUE             }
\CommentTok{\#\textgreater{}  9 116137780 2017{-}02{-}12 N/A                FALSE            }
\CommentTok{\#\textgreater{} 10 113826425 2017{-}01{-}29 within a few hours FALSE            }
\CommentTok{\#\textgreater{} 11 184607983 2018{-}04{-}16 a few days or more FALSE            }
\CommentTok{\#\textgreater{} \# ... with 17 more variables: host\_listings\_count \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   host\_total\_listings\_count \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   host\_neighbourhood \textless{}chr\textgreater{}, neighbourhood\_cleansed \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   room\_type \textless{}chr\textgreater{}, bathrooms \textless{}lgl\textgreater{}, bedrooms \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   price \textless{}int\textgreater{}, number\_of\_reviews \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   has\_availability \textless{}lgl\textgreater{}, review\_scores\_rating \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   review\_scores\_accuracy \textless{}dbl\textgreater{}, ...}
\CommentTok{\# It\textquotesingle{}s pretty clear that there is something odd going on with some of these, but some of them seem legit.}
\end{Highlighting}
\end{Shaded}

Let's look at the distribution of prices that are in a `reasonable' range, which until Monica is a full professor, will be defined as a nightly price of less than \$1,000.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(price }\SpecialCharTok{\textless{}} \DecValTok{1000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Price per night"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of properties"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-63-1.pdf}

Interestingly it looks like there is some bunching of prices, possible around numbers ending in zero or nine? Let's just zoom in on prices between \$90 and \$210, out of interest, but change the bins to be smaller.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Look at distribution of price again}
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(price }\SpecialCharTok{\textgreater{}} \DecValTok{90}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(price }\SpecialCharTok{\textless{}} \DecValTok{210}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Price per night"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of properties"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-64-1.pdf}

\hypertarget{superhosts}{%
\subsubsection{Superhosts}\label{superhosts}}

Airbnb \href{https://www.airbnb.ca/help/article/828/what-is-a-superhost}{says} that:

\begin{quote}
Superhosts are experienced hosts who provide a shining example for other hosts, and extraordinary experiences for their guests.

Once a host reaches Superhost status, a badge superhost badge will automatically appear on their listing and profile to help you identify them.

We check Superhosts' activity four times a year, to ensure that the program highlights the people who are most dedicated to providing outstanding hospitality.
\end{quote}

First we'll look at the NAs in \texttt{host\_is\_superhost}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(host\_is\_superhost))}
\CommentTok{\#\textgreater{} \# A tibble: 11 x 21}
\CommentTok{\#\textgreater{}      host\_id host\_since host\_response\_time host\_is\_superhost}
\CommentTok{\#\textgreater{}        \textless{}dbl\textgreater{} \textless{}date\textgreater{}     \textless{}chr\textgreater{}              \textless{}lgl\textgreater{}            }
\CommentTok{\#\textgreater{}  1  23472830 NA         \textless{}NA\textgreater{}               NA               }
\CommentTok{\#\textgreater{}  2  31675651 NA         \textless{}NA\textgreater{}               NA               }
\CommentTok{\#\textgreater{}  3  75779190 NA         \textless{}NA\textgreater{}               NA               }
\CommentTok{\#\textgreater{}  4  47614482 NA         \textless{}NA\textgreater{}               NA               }
\CommentTok{\#\textgreater{}  5 201103629 NA         \textless{}NA\textgreater{}               NA               }
\CommentTok{\#\textgreater{}  6 111820893 NA         \textless{}NA\textgreater{}               NA               }
\CommentTok{\#\textgreater{}  7  23472830 NA         \textless{}NA\textgreater{}               NA               }
\CommentTok{\#\textgreater{}  8 196269219 NA         \textless{}NA\textgreater{}               NA               }
\CommentTok{\#\textgreater{}  9  23472830 NA         \textless{}NA\textgreater{}               NA               }
\CommentTok{\#\textgreater{} 10 266594170 NA         \textless{}NA\textgreater{}               NA               }
\CommentTok{\#\textgreater{} 11 118516038 NA         \textless{}NA\textgreater{}               NA               }
\CommentTok{\#\textgreater{} \# ... with 17 more variables: host\_listings\_count \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   host\_total\_listings\_count \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   host\_neighbourhood \textless{}chr\textgreater{}, neighbourhood\_cleansed \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   room\_type \textless{}chr\textgreater{}, bathrooms \textless{}lgl\textgreater{}, bedrooms \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   price \textless{}int\textgreater{}, number\_of\_reviews \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   has\_availability \textless{}lgl\textgreater{}, review\_scores\_rating \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   review\_scores\_accuracy \textless{}dbl\textgreater{}, ...}
\end{Highlighting}
\end{Shaded}

There are 285 of these and it's clear that there is something odd going on - maybe the host removed the listing or similar?

We'll also want to create a binary variable out of this. It's true/false at the moment, which is fine for the modelling, but there are a handful of situations where it'll be easier if we have a 0/1.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\OtherTok{\textless{}{-}} 
\NormalTok{  airbnb\_data\_selected }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{host\_is\_superhost\_binary =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{    host\_is\_superhost }\SpecialCharTok{==} \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{,}
\NormalTok{    host\_is\_superhost }\SpecialCharTok{==} \ConstantTok{FALSE} \SpecialCharTok{\textasciitilde{}} \DecValTok{0}\NormalTok{,}
    \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \DecValTok{999}
\NormalTok{    )}
\NormalTok{  )}
\NormalTok{airbnb\_data\_selected}\SpecialCharTok{$}\NormalTok{host\_is\_superhost\_binary[airbnb\_data\_selected}\SpecialCharTok{$}\NormalTok{host\_is\_superhost\_binary }\SpecialCharTok{==} \DecValTok{999}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}
\end{Highlighting}
\end{Shaded}

\hypertarget{reviews}{%
\subsubsection{Reviews}\label{reviews}}

Airbnb \href{https://www.airbnb.ca/help/article/1257/how-do-star-ratings-work-for-stays}{says} that:

\begin{quote}
In addition to written reviews, guests can submit an overall star rating and a set of category star ratings for their stay.

Hosts can view their star ratings on their Progress page, under Reviews. Hosts using professional hosting tools can find reviews and quality details on their Performance page, under Quality.

Guests can give ratings on:

\begin{itemize}
\tightlist
\item
  Overall experience. Overall, how was the stay?
\item
  Cleanliness. Did guests feel that the space was clean and tidy?
\item
  Accuracy. How accurately did the listing page represent the space? For example, guests should be able to find up-to-date info and photos in the listing description.
\item
  Value. Did the guest feel that the listing provided good value for the price?
\item
  Communication. How well did you communicate before and during the stay? Guests often care that their host responds quickly, reliably, and frequently to their messages and questions.
\item
  Check-in. How smoothly did check-in go?
\item
  Location. How did guests feel about the neighbourhood? This may mean that there's an accurate description for proximity and access to transportation, shopping centres, city centre, etc., and a description that includes special considerations, like noise, and family safety.
\item
  Amenities. How did guests feel about the amenities that were available during their stay? Guests often care that all the amenities listed are available, working, and in good condition.
\end{itemize}

In each category, hosts are able to see how often they get 5 stars, how guests rated nearby hosts, and, in some cases, tips to help improve the listing.

The number of stars displayed at the top of a listing page is an aggregate of the primary scores guests have given for that listing. At the bottom of a listing page, there's an aggregate for each category rating. A host needs to receive star ratings from at least 3 guests before their aggregate score appears.
\end{quote}

TODO: I don't understand how these review scores are being constructed. Airbnb says it's a star rating, but how are they converting this into the 10 point scale, similarly, how are their constructing the overall one, which seems to be out of 100? There's a lot of clumping around 20, 40, 60, 80, 100 - could they be averaging a five-star scale and then rebasing it to 100?

Now we'll deal with the NAs in \texttt{review\_scores\_rating}. This one is more complicated as there are a lot of them.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(review\_scores\_rating))}
\CommentTok{\#\textgreater{} \# A tibble: 4,368 x 22}
\CommentTok{\#\textgreater{}    host\_id host\_since host\_response\_time host\_is\_superhost}
\CommentTok{\#\textgreater{}      \textless{}dbl\textgreater{} \textless{}date\textgreater{}     \textless{}chr\textgreater{}              \textless{}lgl\textgreater{}            }
\CommentTok{\#\textgreater{}  1   48239 2009{-}10{-}25 N/A                FALSE            }
\CommentTok{\#\textgreater{}  2  187320 2010{-}08{-}01 within a few hours TRUE             }
\CommentTok{\#\textgreater{}  3  188183 2010{-}08{-}01 a few days or more FALSE            }
\CommentTok{\#\textgreater{}  4  187320 2010{-}08{-}01 within a few hours TRUE             }
\CommentTok{\#\textgreater{}  5  304551 2010{-}11{-}29 within an hour     TRUE             }
\CommentTok{\#\textgreater{}  6  545074 2011{-}04{-}29 N/A                FALSE            }
\CommentTok{\#\textgreater{}  7 1210571 2011{-}09{-}26 N/A                FALSE            }
\CommentTok{\#\textgreater{}  8 1411076 2011{-}11{-}15 N/A                FALSE            }
\CommentTok{\#\textgreater{}  9 1409872 2011{-}11{-}15 N/A                FALSE            }
\CommentTok{\#\textgreater{} 10 1664812 2012{-}01{-}28 N/A                FALSE            }
\CommentTok{\#\textgreater{} \# ... with 4,358 more rows, and 18 more variables:}
\CommentTok{\#\textgreater{} \#   host\_listings\_count \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   host\_total\_listings\_count \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   host\_neighbourhood \textless{}chr\textgreater{}, neighbourhood\_cleansed \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   room\_type \textless{}chr\textgreater{}, bathrooms \textless{}lgl\textgreater{}, bedrooms \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   price \textless{}int\textgreater{}, number\_of\_reviews \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   has\_availability \textless{}lgl\textgreater{}, review\_scores\_rating \textless{}dbl\textgreater{}, ...}
\end{Highlighting}
\end{Shaded}

Now see if it's just because they don't have any reviews.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(review\_scores\_rating)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(number\_of\_reviews) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{table}\NormalTok{()}
\CommentTok{\#\textgreater{} .}
\CommentTok{\#\textgreater{}    0    1    2    3    4 }
\CommentTok{\#\textgreater{} 4105  227   23   10    3}
\end{Highlighting}
\end{Shaded}

So it's clear that in almost all these cases they don't have a review yet because they don't have enough reviews. However, it's a large proportion of the total - almost a fifth of properties don't have any reviews (hence an NA in review\_scores\_rating).

We can use \texttt{vis\_miss} from the \texttt{visdat} package \citep{citevisdat} to make sure that all components of the review are missing. If the NAs are being driven by the Airbnb requirement of at least three reviews then we would expect they would all be missing.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We\textquotesingle{}ll just check whether this is the same for all of the different variants of reviews}
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(review\_scores\_rating, }
\NormalTok{         review\_scores\_accuracy, }
\NormalTok{         review\_scores\_cleanliness, }
\NormalTok{         review\_scores\_checkin, }
\NormalTok{         review\_scores\_communication, }
\NormalTok{         review\_scores\_location, }
\NormalTok{         review\_scores\_value) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{vis\_miss}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-69-1.pdf}

It looks pretty convincing that in almost all cases, all the different variants of reviews are missing. So let's just focus on the main review.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(review\_scores\_rating)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ review\_scores\_rating)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Average review score"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of properties"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-70-1.pdf}

It's pretty clear that almost all the reviews are more than 80. Let's just zoom in on that 60 to 80 range to check what the distribution looks like in that range.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(review\_scores\_rating)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(review\_scores\_rating }\SpecialCharTok{\textgreater{}} \DecValTok{60}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(review\_scores\_rating }\SpecialCharTok{\textless{}} \DecValTok{80}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ review\_scores\_rating)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Average review score"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of properties"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-71-1.pdf}

\hypertarget{response-time}{%
\subsubsection{Response time}\label{response-time}}

Airbnb \href{https://www.airbnb.ca/help/article/75/how-much-time-does-a-host-have-to-respond-to-my-reservation-request}{says} that:

\begin{quote}
Hosts have 24 hours to officially accept or decline reservation requests. You'll be updated by email about the status of your request.

More than half of all reservation requests are accepted within one hour of being received. The vast majority of hosts reply within 12 hours.

If a host confirms your request, your payment is processed and collected by Airbnb in full. If a host declines your request or the request expires, we don't process your payment.
\end{quote}

TODO: I don't understand how you can get a response time of NA? It must be related to some other variable.

Looking now at response time:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(airbnb\_data\_selected}\SpecialCharTok{$}\NormalTok{host\_response\_time)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} a few days or more                N/A       within a day }
\CommentTok{\#\textgreater{}                816               8469               1235 }
\CommentTok{\#\textgreater{} within a few hours     within an hour }
\CommentTok{\#\textgreater{}               2062               5672}
\end{Highlighting}
\end{Shaded}

Interestingly it seems like what looks like `NAs' in the \texttt{host\_response\_time} variable are not being coded as proper NAs, but are instead being treated as another category. We'll recode them to be actual NAs.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected}\SpecialCharTok{$}\NormalTok{host\_response\_time[airbnb\_data\_selected}\SpecialCharTok{$}\NormalTok{host\_response\_time }\SpecialCharTok{==} \StringTok{"N/A"}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}
\end{Highlighting}
\end{Shaded}

So here we clearly have issues with NAs. We probably want to filter them away for this example because it's just a quick example, but there are an awful lot of them (more than 20 per cent) so we'll have a quick look at them in relation to the review score.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(host\_response\_time)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ review\_scores\_rating)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{1}\NormalTok{)}
\CommentTok{\#\textgreater{} Warning: Removed 2590 rows containing non{-}finite values}
\CommentTok{\#\textgreater{} (stat\_bin).}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-74-1.pdf}

There seem to be an awful lot that have an overall review of 100. There are also an awful lot that have a review score of NA.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(host\_response\_time)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(review\_scores\_rating))}
\CommentTok{\#\textgreater{} \# A tibble: 2,590 x 22}
\CommentTok{\#\textgreater{}    host\_id host\_since host\_response\_time host\_is\_superhost}
\CommentTok{\#\textgreater{}      \textless{}dbl\textgreater{} \textless{}date\textgreater{}     \textless{}chr\textgreater{}              \textless{}lgl\textgreater{}            }
\CommentTok{\#\textgreater{}  1   48239 2009{-}10{-}25 \textless{}NA\textgreater{}               FALSE            }
\CommentTok{\#\textgreater{}  2  545074 2011{-}04{-}29 \textless{}NA\textgreater{}               FALSE            }
\CommentTok{\#\textgreater{}  3 1210571 2011{-}09{-}26 \textless{}NA\textgreater{}               FALSE            }
\CommentTok{\#\textgreater{}  4 1411076 2011{-}11{-}15 \textless{}NA\textgreater{}               FALSE            }
\CommentTok{\#\textgreater{}  5 1409872 2011{-}11{-}15 \textless{}NA\textgreater{}               FALSE            }
\CommentTok{\#\textgreater{}  6 1664812 2012{-}01{-}28 \textless{}NA\textgreater{}               FALSE            }
\CommentTok{\#\textgreater{}  7 1828773 2012{-}02{-}28 \textless{}NA\textgreater{}               FALSE            }
\CommentTok{\#\textgreater{}  8 1923052 2012{-}03{-}14 \textless{}NA\textgreater{}               FALSE            }
\CommentTok{\#\textgreater{}  9 2432916 2012{-}05{-}22 \textless{}NA\textgreater{}               FALSE            }
\CommentTok{\#\textgreater{} 10 2577688 2012{-}06{-}07 \textless{}NA\textgreater{}               FALSE            }
\CommentTok{\#\textgreater{} \# ... with 2,580 more rows, and 18 more variables:}
\CommentTok{\#\textgreater{} \#   host\_listings\_count \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   host\_total\_listings\_count \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   host\_neighbourhood \textless{}chr\textgreater{}, neighbourhood\_cleansed \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   room\_type \textless{}chr\textgreater{}, bathrooms \textless{}lgl\textgreater{}, bedrooms \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   price \textless{}int\textgreater{}, number\_of\_reviews \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   has\_availability \textless{}lgl\textgreater{}, review\_scores\_rating \textless{}dbl\textgreater{}, ...}
\end{Highlighting}
\end{Shaded}

\hypertarget{host-number-of-listings}{%
\subsubsection{Host number of listings}\label{host-number-of-listings}}

There are two versions of a variable telling you how many properties a host has on Airbnb, so to start just check whether there's a difference.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{listings\_count\_is\_same =} \FunctionTok{if\_else}\NormalTok{(host\_listings\_count }\SpecialCharTok{==}\NormalTok{ host\_total\_listings\_count, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(listings\_count\_is\_same }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 0 x 23}
\CommentTok{\#\textgreater{} \# ... with 23 variables: host\_id \textless{}dbl\textgreater{}, host\_since \textless{}date\textgreater{},}
\CommentTok{\#\textgreater{} \#   host\_response\_time \textless{}chr\textgreater{}, host\_is\_superhost \textless{}lgl\textgreater{},}
\CommentTok{\#\textgreater{} \#   host\_listings\_count \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   host\_total\_listings\_count \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   host\_neighbourhood \textless{}chr\textgreater{}, neighbourhood\_cleansed \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   room\_type \textless{}chr\textgreater{}, bathrooms \textless{}lgl\textgreater{}, bedrooms \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   price \textless{}int\textgreater{}, number\_of\_reviews \textless{}dbl\textgreater{}, ...}
\end{Highlighting}
\end{Shaded}

There are none in this dataset so we can just remove one column for now and have a quick look at the other one.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\OtherTok{\textless{}{-}} 
\NormalTok{  airbnb\_data\_selected }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{host\_listings\_count)}

\NormalTok{airbnb\_data\_selected }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(host\_total\_listings\_count)}
\CommentTok{\#\textgreater{} \# A tibble: 49 x 2}
\CommentTok{\#\textgreater{}    host\_total\_listings\_count     n}
\CommentTok{\#\textgreater{}                        \textless{}dbl\textgreater{} \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{}  1                         0  2128}
\CommentTok{\#\textgreater{}  2                         1  7662}
\CommentTok{\#\textgreater{}  3                         2  2476}
\CommentTok{\#\textgreater{}  4                         3  1384}
\CommentTok{\#\textgreater{}  5                         4   802}
\CommentTok{\#\textgreater{}  6                         5   478}
\CommentTok{\#\textgreater{}  7                         6   385}
\CommentTok{\#\textgreater{}  8                         7   270}
\CommentTok{\#\textgreater{}  9                         8   291}
\CommentTok{\#\textgreater{} 10                         9   170}
\CommentTok{\#\textgreater{} \# ... with 39 more rows}
\end{Highlighting}
\end{Shaded}

So there are a large number who have somewhere in the 2-10 properties range, but the usual long tail. The number with 0 listings is unexpected and worth following up on. And there are a bunch with NA that we'll need to deal with.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(host\_total\_listings\_count }\SpecialCharTok{==} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 21}
\CommentTok{\#\textgreater{}   host\_id host\_since host\_response\_time host\_is\_superhost}
\CommentTok{\#\textgreater{}     \textless{}dbl\textgreater{} \textless{}date\textgreater{}     \textless{}chr\textgreater{}              \textless{}lgl\textgreater{}            }
\CommentTok{\#\textgreater{} 1  140602 2010{-}06{-}08 \textless{}NA\textgreater{}               FALSE            }
\CommentTok{\#\textgreater{} 2 1024550 2011{-}08{-}26 \textless{}NA\textgreater{}               FALSE            }
\CommentTok{\#\textgreater{} 3 2647656 2012{-}06{-}15 \textless{}NA\textgreater{}               FALSE            }
\CommentTok{\#\textgreater{} 4 3783106 2012{-}10{-}06 within an hour     FALSE            }
\CommentTok{\#\textgreater{} 5 3814089 2012{-}10{-}09 within an hour     FALSE            }
\CommentTok{\#\textgreater{} 6 3827668 2012{-}10{-}10 within a day       FALSE            }
\CommentTok{\#\textgreater{} \# ... with 17 more variables:}
\CommentTok{\#\textgreater{} \#   host\_total\_listings\_count \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   host\_neighbourhood \textless{}chr\textgreater{}, neighbourhood\_cleansed \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   room\_type \textless{}chr\textgreater{}, bathrooms \textless{}lgl\textgreater{}, bedrooms \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   price \textless{}int\textgreater{}, number\_of\_reviews \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   has\_availability \textless{}lgl\textgreater{}, review\_scores\_rating \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   review\_scores\_accuracy \textless{}dbl\textgreater{}, ...}
\end{Highlighting}
\end{Shaded}

There's nothing that immediately jumps out as odd about the people with zero listings, but there must be something going on.

Based on this dataset, there's a third way of looking at the number of properties someone has and that's to look at the number of times the unique ID occurs.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(host\_id) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{n) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 2}
\CommentTok{\#\textgreater{}     host\_id     n}
\CommentTok{\#\textgreater{}       \textless{}dbl\textgreater{} \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1  10202618    74}
\CommentTok{\#\textgreater{} 2   1919294    63}
\CommentTok{\#\textgreater{} 3 152088065    59}
\CommentTok{\#\textgreater{} 4 293274089    56}
\CommentTok{\#\textgreater{} 5    785826    54}
\CommentTok{\#\textgreater{} 6    846505    46}
\end{Highlighting}
\end{Shaded}

Again this makes it clear that there are many with multiple properties listed.

\hypertarget{decisions}{%
\subsubsection{Decisions}\label{decisions}}

The purpose of this document is to just give a quick introduction to using real-world data, so we'll just remove anything that is annoying, but if you're using this for research then you'd need to justify these decisions and/or possibly make different ones.

Get rid of prices more than \$999.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_filtered }\OtherTok{\textless{}{-}} 
\NormalTok{  airbnb\_data\_selected }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(price }\SpecialCharTok{\textless{}} \DecValTok{1000}\NormalTok{)}
\FunctionTok{dim}\NormalTok{(airbnb\_data\_filtered)}
\CommentTok{\#\textgreater{} [1] 18120    21}
\end{Highlighting}
\end{Shaded}

Get rid of anyone with an NA for whether they are a super host.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Just remove host\_is\_superhost NAs for now.}
\NormalTok{airbnb\_data\_filtered }\OtherTok{\textless{}{-}} 
\NormalTok{  airbnb\_data\_filtered }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(host\_is\_superhost))}
\FunctionTok{dim}\NormalTok{(airbnb\_data\_filtered)}
\CommentTok{\#\textgreater{} [1] 18109    21}
\end{Highlighting}
\end{Shaded}

Get rid of anyone with an NA in their main review score - this removes roughly 20 per cent of observations.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We\textquotesingle{}ll just get rid of them for now, but this is probably something that deserves more attention {-} possibly in an appendix or similar.}
\NormalTok{airbnb\_data\_filtered }\OtherTok{\textless{}{-}} 
\NormalTok{  airbnb\_data\_filtered }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(review\_scores\_rating))}
\CommentTok{\# There are still some where the rest of the reviews are missing even though there is a main review score}
\CommentTok{\# There seem to be an awful lot that have an overall review of 100. Does that make sense?}
\FunctionTok{dim}\NormalTok{(airbnb\_data\_filtered)}
\CommentTok{\#\textgreater{} [1] 13801    21}
\end{Highlighting}
\end{Shaded}

Get rid of anyone with a main review score less than 70.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We\textquotesingle{}ll just get rid of them for now, but this is probably something that deserves more attention {-} possibly in an appendix or similar.}
\NormalTok{airbnb\_data\_filtered }\OtherTok{\textless{}{-}} 
\NormalTok{  airbnb\_data\_filtered }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(review\_scores\_rating }\SpecialCharTok{\textgreater{}} \DecValTok{69}\NormalTok{)}
\CommentTok{\# There are still some where the rest of the reviews are missing even though there is a main review score}
\CommentTok{\# There seem to be an awful lot that have an overall review of 100. Does that make sense?}
\FunctionTok{dim}\NormalTok{(airbnb\_data\_filtered)}
\CommentTok{\#\textgreater{} [1] 13471    21}
\end{Highlighting}
\end{Shaded}

Get rid of anyone with a NA in their response time - this removes roughly another 20 per cent of the observations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_filtered }\OtherTok{\textless{}{-}} 
\NormalTok{  airbnb\_data\_filtered }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(host\_response\_time))}
\FunctionTok{dim}\NormalTok{(airbnb\_data\_filtered)}
\CommentTok{\#\textgreater{} [1] 7763   21}
\end{Highlighting}
\end{Shaded}

TODO: We don't have to do this next step as we've already got rid of them at some other point. So there's something systematic going on and we should come back and look into it.

Get rid of anyone with a NA in their number of properties.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_filtered }\OtherTok{\textless{}{-}} 
\NormalTok{  airbnb\_data\_filtered }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(host\_total\_listings\_count))}
\FunctionTok{dim}\NormalTok{(airbnb\_data\_filtered)}
\CommentTok{\#\textgreater{} [1] 7763   21}
\end{Highlighting}
\end{Shaded}

Get rid of anyone with a 100 for their review\_scores\_rating.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_filtered }\OtherTok{\textless{}{-}} 
\NormalTok{  airbnb\_data\_filtered }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(review\_scores\_rating }\SpecialCharTok{!=} \DecValTok{100}\NormalTok{)}
\FunctionTok{dim}\NormalTok{(airbnb\_data\_filtered)}
\CommentTok{\#\textgreater{} [1] 5648   21}
\end{Highlighting}
\end{Shaded}

Only keep people with one property:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_filtered }\OtherTok{\textless{}{-}} 
\NormalTok{  airbnb\_data\_filtered }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{add\_count}\NormalTok{(host\_id) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(n }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{n)}
\FunctionTok{dim}\NormalTok{(airbnb\_data\_filtered)}
\CommentTok{\#\textgreater{} [1] 2304   21}
\end{Highlighting}
\end{Shaded}

\hypertarget{explore-data}{%
\subsection{Explore data}\label{explore-data}}

We might like to make some graphs to see if we can any relationships jump out. Some aspects that come to mind is looking at prices and reviews and super hosts, and number of properties and neighbourhood.

\hypertarget{price-and-reviews}{%
\subsubsection{Price and reviews}\label{price-and-reviews}}

Look at the relationship between price and reviews, and whether they are a super-host.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Look at both price and reviews}
\NormalTok{airbnb\_data\_filtered }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ price, }\AttributeTok{y =}\NormalTok{ review\_scores\_rating, }\AttributeTok{color =}\NormalTok{ host\_is\_superhost)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# Make the points smaller and more transparent as they overlap considerably.}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Price per night"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Average review score"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Super host"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# Probably should recode this to more meaningful than TRUE/FALSE.}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-88-1.pdf}

\hypertarget{superhost-and-response-time}{%
\subsubsection{Superhost and response-time}\label{superhost-and-response-time}}

One of the aspects that may make someone a super host is how quickly they respond to enquiries. One could imagine that being a superhost involves quickly saying yes or no to enquiries. Let's look at the data. First, we want to look at the possible values of superhost by their response times.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_filtered }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{tabyl}\NormalTok{(host\_is\_superhost) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{adorn\_totals}\NormalTok{(}\StringTok{"row"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{adorn\_pct\_formatting}\NormalTok{()}
\CommentTok{\#\textgreater{}  host\_is\_superhost    n percent}
\CommentTok{\#\textgreater{}              FALSE 1224   53.1\%}
\CommentTok{\#\textgreater{}               TRUE 1080   46.9\%}
\CommentTok{\#\textgreater{}              Total 2304  100.0\%}
\end{Highlighting}
\end{Shaded}

Fortunately, it looks like when we removed the reviews rows we removed any NAs from whether they were a super host, but if we go back and look into that we may need to check again. The \texttt{tabyl} function within the janitor package (Firke, 2020) would list the NAs if there were any, but in case you don't trust it, another way of check this is to try to filter to just the NAs.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_filtered }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(host\_is\_superhost))}
\CommentTok{\#\textgreater{} \# A tibble: 0 x 21}
\CommentTok{\#\textgreater{} \# ... with 21 variables: host\_id \textless{}dbl\textgreater{}, host\_since \textless{}date\textgreater{},}
\CommentTok{\#\textgreater{} \#   host\_response\_time \textless{}chr\textgreater{}, host\_is\_superhost \textless{}lgl\textgreater{},}
\CommentTok{\#\textgreater{} \#   host\_total\_listings\_count \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   host\_neighbourhood \textless{}chr\textgreater{}, neighbourhood\_cleansed \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   room\_type \textless{}chr\textgreater{}, bathrooms \textless{}lgl\textgreater{}, bedrooms \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   price \textless{}int\textgreater{}, number\_of\_reviews \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   has\_availability \textless{}lgl\textgreater{}, review\_scores\_rating \textless{}dbl\textgreater{}, ...}
\end{Highlighting}
\end{Shaded}

Now let's look at the response time.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_filtered }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{tabyl}\NormalTok{(host\_response\_time) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{adorn\_totals}\NormalTok{(}\StringTok{"row"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{adorn\_pct\_formatting}\NormalTok{()}
\CommentTok{\#\textgreater{}  host\_response\_time    n percent}
\CommentTok{\#\textgreater{}  a few days or more  167    7.2\%}
\CommentTok{\#\textgreater{}        within a day  378   16.4\%}
\CommentTok{\#\textgreater{}  within a few hours  519   22.5\%}
\CommentTok{\#\textgreater{}      within an hour 1240   53.8\%}
\CommentTok{\#\textgreater{}               Total 2304  100.0\%}
\end{Highlighting}
\end{Shaded}

So a vast majority respond within an hour.

Finally, we can look at the cross-tab.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_filtered }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{tabyl}\NormalTok{(host\_response\_time, host\_is\_superhost) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{adorn\_percentages}\NormalTok{(}\StringTok{"row"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{adorn\_pct\_formatting}\NormalTok{(}\AttributeTok{digits =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{adorn\_ns}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{adorn\_title}\NormalTok{()}
\CommentTok{\#\textgreater{}                     host\_is\_superhost          }
\CommentTok{\#\textgreater{}  host\_response\_time             FALSE      TRUE}
\CommentTok{\#\textgreater{}  a few days or more         88\% (147) 12\%  (20)}
\CommentTok{\#\textgreater{}        within a day         67\% (254) 33\% (124)}
\CommentTok{\#\textgreater{}  within a few hours         51\% (266) 49\% (253)}
\CommentTok{\#\textgreater{}      within an hour         45\% (557) 55\% (683)}
\end{Highlighting}
\end{Shaded}

So if someone doesn't respond within an hour then it's unlikely that they are a super host.

\hypertarget{neighbourhood}{%
\subsubsection{Neighbourhood}\label{neighbourhood}}

Finally, let's look at neighbourhood. The data provider has attempted to clean the neighbourhood variable for us, so we'll just use this for now. If we were doing this analysis properly, we'd need to check whether they'd made any mistakes.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We expect something in the order of 100 to 150 neighbourhoods, with the top ten accounting for a large majority of listings.}
\NormalTok{airbnb\_data\_filtered }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{tabyl}\NormalTok{(neighbourhood\_cleansed) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{adorn\_totals}\NormalTok{(}\StringTok{"row"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{adorn\_pct\_formatting}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{nrow}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] 140}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_filtered }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{tabyl}\NormalTok{(neighbourhood\_cleansed) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{adorn\_pct\_formatting}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{n) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(n }\SpecialCharTok{\textgreater{}} \DecValTok{100}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{adorn\_totals}\NormalTok{(}\StringTok{"row"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{}             neighbourhood\_cleansed   n percent}
\CommentTok{\#\textgreater{}  Waterfront Communities{-}The Island 409   17.8\%}
\CommentTok{\#\textgreater{}                            Niagara 101    4.4\%}
\CommentTok{\#\textgreater{}                              Total 510       {-}}
\end{Highlighting}
\end{Shaded}

\hypertarget{model-data}{%
\subsection{Model data}\label{model-data}}

We will now run some models on our dataset. We will first split the data into test/training groups, we do this using functions from the tidymodels package \citep{citeTidymodels} (which like the tidyverse package \citep{citetidyverse} is a package of packages).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{airbnb\_data\_filtered\_split }\OtherTok{\textless{}{-}} 
\NormalTok{  airbnb\_data\_filtered }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{initial\_split}\NormalTok{(}\AttributeTok{prop =} \DecValTok{3}\SpecialCharTok{/}\DecValTok{4}\NormalTok{)}

\NormalTok{airbnb\_train }\OtherTok{\textless{}{-}} \FunctionTok{training}\NormalTok{(airbnb\_data\_filtered\_split)}
\NormalTok{airbnb\_test }\OtherTok{\textless{}{-}} \FunctionTok{testing}\NormalTok{(airbnb\_data\_filtered\_split)}

\FunctionTok{rm}\NormalTok{(airbnb\_data\_filtered\_split)}
\end{Highlighting}
\end{Shaded}

\hypertarget{logistic-regression-1}{%
\subsubsection{Logistic regression}\label{logistic-regression-1}}

We may like to look at whether we can forecast whether someone is a super host, and the factors that go into explaining that. As the dependent variable is binary, this is a good opportunity to look at logistic regression. We expect that better reviews will be associated with faster responses and higher reviews. Specifically, the model that we estimate is:

\[\mbox{Prob(Is super host} = 1) = \beta_0 + \beta_1 \mbox{Response time} + \beta_2 \mbox{Reviews} + \epsilon\]

We estimate the model using \texttt{glm} in the R language \citep{citeR}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{logistic\_reg\_superhost\_response\_review }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(host\_is\_superhost }\SpecialCharTok{\textasciitilde{}} 
\NormalTok{                                                host\_response\_time }\SpecialCharTok{+} 
\NormalTok{                                                review\_scores\_rating,}
                                              \AttributeTok{data =}\NormalTok{ airbnb\_train,}
                                              \AttributeTok{family =}\NormalTok{ binomial}
\NormalTok{                                              )}
\end{Highlighting}
\end{Shaded}

We can have a quick look at the results, for instance, the \texttt{summary} function. We could also use \texttt{tidy} or \texttt{glance} from the \texttt{broom} package \citep{citebroom}. The details should be the same, but the \texttt{broom} functions are tibbles, which means that we can more easily deal with them within a tidy framework.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(logistic\_reg\_superhost\_response\_review)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = host\_is\_superhost \textasciitilde{} host\_response\_time + review\_scores\_rating, }
\CommentTok{\#\textgreater{}     family = binomial, data = airbnb\_train)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}1.9297  {-}0.8873  {-}0.0416   0.8310   4.0657  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                                      Estimate Std. Error}
\CommentTok{\#\textgreater{} (Intercept)                          {-}40.5578     2.4208}
\CommentTok{\#\textgreater{} host\_response\_timewithin a day         1.5351     0.3466}
\CommentTok{\#\textgreater{} host\_response\_timewithin a few hours   1.9520     0.3360}
\CommentTok{\#\textgreater{} host\_response\_timewithin an hour       2.2883     0.3240}
\CommentTok{\#\textgreater{} review\_scores\_rating                   0.4037     0.0248}
\CommentTok{\#\textgreater{}                                      z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)                          {-}16.754  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} host\_response\_timewithin a day         4.429 9.46e{-}06 ***}
\CommentTok{\#\textgreater{} host\_response\_timewithin a few hours   5.809 6.28e{-}09 ***}
\CommentTok{\#\textgreater{} host\_response\_timewithin an hour       7.062 1.65e{-}12 ***}
\CommentTok{\#\textgreater{} review\_scores\_rating                  16.276  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  }
\CommentTok{\#\textgreater{} 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for binomial family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 2392.2  on 1727  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 1750.0  on 1723  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 1760}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 6}
\FunctionTok{tidy}\NormalTok{(logistic\_reg\_superhost\_response\_review)}
\CommentTok{\#\textgreater{} \# A tibble: 5 x 5}
\CommentTok{\#\textgreater{}   term                 estimate std.error statistic  p.value}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}                   \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 (Intercept)           {-}40.6      2.42      {-}16.8  5.31e{-}63}
\CommentTok{\#\textgreater{} 2 host\_response\_timew\textasciitilde{}    1.54     0.347       4.43 9.46e{-} 6}
\CommentTok{\#\textgreater{} 3 host\_response\_timew\textasciitilde{}    1.95     0.336       5.81 6.28e{-} 9}
\CommentTok{\#\textgreater{} 4 host\_response\_timew\textasciitilde{}    2.29     0.324       7.06 1.65e{-}12}
\CommentTok{\#\textgreater{} 5 review\_scores\_rating    0.404    0.0248     16.3  1.45e{-}59}
\FunctionTok{glance}\NormalTok{(logistic\_reg\_superhost\_response\_review)}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 8}
\CommentTok{\#\textgreater{}   null.deviance df.null logLik   AIC   BIC deviance}
\CommentTok{\#\textgreater{}           \textless{}dbl\textgreater{}   \textless{}int\textgreater{}  \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1         2392.    1727  {-}875. 1760. 1787.    1750.}
\CommentTok{\#\textgreater{} \# ... with 2 more variables: df.residual \textless{}int\textgreater{}, nobs \textless{}int\textgreater{}}
\end{Highlighting}
\end{Shaded}

We might like to look at what our model predicts, compared with whether the person was actually a super host. We can do that in a variety of ways, but one way is to use \texttt{augment} from the \texttt{broom} package \citep{citebroom}. This will add the prediction and associated uncertainty to the data. For every row we will then have the probability that our model is estimating that they are a superhost. But ultimately, we need a binary forecast. There are a bunch of different options, but one is to just say that if the model estimates a probability of more than 0.5 then we bin it into a superhost, and other not.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_filtered\_logistic\_fit\_train }\OtherTok{\textless{}{-}} 
  \FunctionTok{augment}\NormalTok{(logistic\_reg\_superhost\_response\_review, }
          \AttributeTok{data =}\NormalTok{ airbnb\_train }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(host\_is\_superhost, }
\NormalTok{                                         host\_is\_superhost\_binary,}
\NormalTok{                                         host\_response\_time,}
\NormalTok{                                         review\_scores\_rating}
\NormalTok{                                         ),}
          \AttributeTok{type.predict =} \StringTok{"response"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# We use the "response" option here so that the function does the work of worrying about the exponential and log odds for us. Our result will be a probability.}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{.hat, }\SpecialCharTok{{-}}\NormalTok{.sigma, }\SpecialCharTok{{-}}\NormalTok{.cooksd, }\SpecialCharTok{{-}}\NormalTok{.std.resid) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{predict\_host\_is\_superhost =} \FunctionTok{if\_else}\NormalTok{(.fitted }\SpecialCharTok{\textgreater{}} \FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{), }\CommentTok{\# How do things change if we change the 0.5 cutoff?}
         \AttributeTok{host\_is\_superhost\_binary =} \FunctionTok{as.factor}\NormalTok{(host\_is\_superhost\_binary),}
         \AttributeTok{predict\_host\_is\_superhost\_binary =} \FunctionTok{as.factor}\NormalTok{(predict\_host\_is\_superhost)}
\NormalTok{         )}
\end{Highlighting}
\end{Shaded}

We can look at how far off the model is. There are a bunch of ways of doing this, but one is to look at what probability the model has given each person.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_filtered\_logistic\_fit\_train }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ .fitted, }\AttributeTok{fill =}\NormalTok{ host\_is\_superhost\_binary)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Estimated probability that host is super host"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Host is super host"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-99-1.pdf}

We can look at how the model probabilities change based on average review score, and their average time to respond.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(airbnb\_data\_filtered\_logistic\_fit\_train, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ review\_scores\_rating, }
           \AttributeTok{y =}\NormalTok{ .fitted, }
           \AttributeTok{color =}\NormalTok{ host\_response\_time)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Average review score"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Predicted probability of being a superhost"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Host response time"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-100-1.pdf}

This nice thing about this graph is that it illustrates nicely the effect of a host having an average response time of, say, `within an hour' compared with `within a few hours'.

We can focus on how the model does in terms of raw classification using \texttt{confusionMatrix} from the caret package (Kuhn, 2020). This also gives a bunch of diagnostics (the help file explains what they are). In general, they suggest this isn't the best model that's ever existed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{caret}\SpecialCharTok{::}\FunctionTok{confusionMatrix}\NormalTok{(}\AttributeTok{data =}\NormalTok{ airbnb\_data\_filtered\_logistic\_fit\_train}\SpecialCharTok{$}\NormalTok{predict\_host\_is\_superhost\_binary,}
                       \AttributeTok{reference =}\NormalTok{ airbnb\_data\_filtered\_logistic\_fit\_train}\SpecialCharTok{$}\NormalTok{host\_is\_superhost\_binary)}
\CommentTok{\#\textgreater{} Confusion Matrix and Statistics}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}           Reference}
\CommentTok{\#\textgreater{} Prediction   0   1}
\CommentTok{\#\textgreater{}          0 619 124}
\CommentTok{\#\textgreater{}          1 283 702}
\CommentTok{\#\textgreater{}                                           }
\CommentTok{\#\textgreater{}                Accuracy : 0.7645          }
\CommentTok{\#\textgreater{}                  95\% CI : (0.7437, 0.7843)}
\CommentTok{\#\textgreater{}     No Information Rate : 0.522           }
\CommentTok{\#\textgreater{}     P{-}Value [Acc \textgreater{} NIR] : \textless{} 2.2e{-}16       }
\CommentTok{\#\textgreater{}                                           }
\CommentTok{\#\textgreater{}                   Kappa : 0.5318          }
\CommentTok{\#\textgreater{}                                           }
\CommentTok{\#\textgreater{}  Mcnemar\textquotesingle{}s Test P{-}Value : 4.811e{-}15       }
\CommentTok{\#\textgreater{}                                           }
\CommentTok{\#\textgreater{}             Sensitivity : 0.6863          }
\CommentTok{\#\textgreater{}             Specificity : 0.8499          }
\CommentTok{\#\textgreater{}          Pos Pred Value : 0.8331          }
\CommentTok{\#\textgreater{}          Neg Pred Value : 0.7127          }
\CommentTok{\#\textgreater{}              Prevalence : 0.5220          }
\CommentTok{\#\textgreater{}          Detection Rate : 0.3582          }
\CommentTok{\#\textgreater{}    Detection Prevalence : 0.4300          }
\CommentTok{\#\textgreater{}       Balanced Accuracy : 0.7681          }
\CommentTok{\#\textgreater{}                                           }
\CommentTok{\#\textgreater{}        \textquotesingle{}Positive\textquotesingle{} Class : 0               }
\CommentTok{\#\textgreater{} }
\end{Highlighting}
\end{Shaded}

In any case, to this point we've been looking at how the model has done on the training set. It's also relevant how it does on the test set. Again, there are a bunch of ways to do this, but one is to again use the augment function, but to include a newdata argument.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_filtered\_logistic\_fit\_test }\OtherTok{\textless{}{-}} 
  \FunctionTok{augment}\NormalTok{(logistic\_reg\_superhost\_response\_review, }
          \AttributeTok{data =}\NormalTok{ airbnb\_train }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(host\_is\_superhost, }
\NormalTok{                                         host\_is\_superhost\_binary,}
\NormalTok{                                         host\_response\_time,}
\NormalTok{                                         review\_scores\_rating}
\NormalTok{                                         ),}
          \AttributeTok{newdata =}\NormalTok{ airbnb\_test }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(host\_is\_superhost, }
\NormalTok{                                         host\_is\_superhost\_binary,}
\NormalTok{                                         host\_response\_time,}
\NormalTok{                                         review\_scores\_rating}
\NormalTok{                                         ), }\CommentTok{\# I\textquotesingle{}m selecting just because the}
          \CommentTok{\# dataset is quite wide, and so this makes it easier to look at.}
          \AttributeTok{type.predict =} \StringTok{"response"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{predict\_host\_is\_superhost =} \FunctionTok{if\_else}\NormalTok{(.fitted }\SpecialCharTok{\textgreater{}} \FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{), }
         \AttributeTok{host\_is\_superhost\_binary =} \FunctionTok{as.factor}\NormalTok{(host\_is\_superhost\_binary),}
         \AttributeTok{predict\_host\_is\_superhost\_binary =} \FunctionTok{as.factor}\NormalTok{(predict\_host\_is\_superhost)}
\NormalTok{         )}
\end{Highlighting}
\end{Shaded}

We would expect the performance to be slightly worse on the test set. But it's actually fairly similar.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{caret}\SpecialCharTok{::}\FunctionTok{confusionMatrix}\NormalTok{(}\AttributeTok{data =}\NormalTok{ airbnb\_data\_filtered\_logistic\_fit\_test}\SpecialCharTok{$}\NormalTok{predict\_host\_is\_superhost\_binary,}
                       \AttributeTok{reference =}\NormalTok{ airbnb\_data\_filtered\_logistic\_fit\_test}\SpecialCharTok{$}\NormalTok{host\_is\_superhost\_binary)}
\CommentTok{\#\textgreater{} Confusion Matrix and Statistics}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}           Reference}
\CommentTok{\#\textgreater{} Prediction   0   1}
\CommentTok{\#\textgreater{}          0 197  36}
\CommentTok{\#\textgreater{}          1 125 218}
\CommentTok{\#\textgreater{}                                           }
\CommentTok{\#\textgreater{}                Accuracy : 0.7205          }
\CommentTok{\#\textgreater{}                  95\% CI : (0.6819, 0.7568)}
\CommentTok{\#\textgreater{}     No Information Rate : 0.559           }
\CommentTok{\#\textgreater{}     P{-}Value [Acc \textgreater{} NIR] : 1.018e{-}15       }
\CommentTok{\#\textgreater{}                                           }
\CommentTok{\#\textgreater{}                   Kappa : 0.4533          }
\CommentTok{\#\textgreater{}                                           }
\CommentTok{\#\textgreater{}  Mcnemar\textquotesingle{}s Test P{-}Value : 4.052e{-}12       }
\CommentTok{\#\textgreater{}                                           }
\CommentTok{\#\textgreater{}             Sensitivity : 0.6118          }
\CommentTok{\#\textgreater{}             Specificity : 0.8583          }
\CommentTok{\#\textgreater{}          Pos Pred Value : 0.8455          }
\CommentTok{\#\textgreater{}          Neg Pred Value : 0.6356          }
\CommentTok{\#\textgreater{}              Prevalence : 0.5590          }
\CommentTok{\#\textgreater{}          Detection Rate : 0.3420          }
\CommentTok{\#\textgreater{}    Detection Prevalence : 0.4045          }
\CommentTok{\#\textgreater{}       Balanced Accuracy : 0.7350          }
\CommentTok{\#\textgreater{}                                           }
\CommentTok{\#\textgreater{}        \textquotesingle{}Positive\textquotesingle{} Class : 0               }
\CommentTok{\#\textgreater{} }
\end{Highlighting}
\end{Shaded}

We could compare the test with the training sets in terms of forecasts.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{training }\OtherTok{\textless{}{-}}\NormalTok{ airbnb\_data\_filtered\_logistic\_fit\_train }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(host\_is\_superhost\_binary, .fitted) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \StringTok{"Training set"}\NormalTok{)}

\NormalTok{test }\OtherTok{\textless{}{-}}\NormalTok{ airbnb\_data\_filtered\_logistic\_fit\_test }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(host\_is\_superhost\_binary, .fitted) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \StringTok{"Test set"}\NormalTok{)}

\NormalTok{both }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(training, test)}
\FunctionTok{rm}\NormalTok{(training, test)}

\NormalTok{both }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ .fitted, }
             \AttributeTok{fill =}\NormalTok{ host\_is\_superhost\_binary)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Estimated probability that host is super host"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Host is super host"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(type),}
             \AttributeTok{nrow =} \DecValTok{2}\NormalTok{,}
             \AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{40-eda_files/figure-latex/unnamed-chunk-104-1.pdf}

\hypertarget{exercises-and-tutorial-13}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-13}}

\hypertarget{exercises-13}{%
\subsection{Exercises}\label{exercises-13}}

\hypertarget{tutorial-13}{%
\subsection{Tutorial}\label{tutorial-13}}

\hypertarget{ijalm}{%
\chapter{It's Just A Linear Model}\label{ijalm}}

\textbf{STATUS: Under construction.}

\textbf{Required reading}

\begin{itemize}
\tightlist
\item
  Greenland, Sander, Stephen J. Senn, Kenneth J. Rothman, John B. Carlin, Charles Poole, Steven N. Goodman, and Douglas G. Altman, 2016, `Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations', \emph{European journal of epidemiology}, 31, no. 4, pp.~337-350.
\item
  James, Gareth, Daniela Witten, Trevor Hastie and Robert Tibshirani, 2017, \emph{An Introduction to Statistical Learning with Applications in R}, 1st Edition, Chapters 3 and 4.1-4.3., \url{https://www.statlearning.com}.
\item
  Obermeyer, Z., Powers, B., Vogeli, C., \& Sendhill, M., 2019, `Dissecting racial bias in an algorithm used to manage the health of populations', \emph{Science}, (366): 447-453.
\item
  Wickham, Hadley, and Garrett Grolemund, 2017, \emph{R for Data Science}, Chapter 23, \url{https://r4ds.had.co.nz/}.
\item
  Zook M, Barocas S, boyd d, Crawford K, Keller E, Gangadharan SP, et al.~(2017) `Ten simple rules for responsible big data research', \emph{PLoS Comput Biol} 13(3): e1005399. \url{https://doi.org/10.1371/journal.pcbi.1005399}
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
  Angrist, Joshua D., and Jörn-Steffen Pischke, 2008, \emph{Mostly harmless econometrics: An empiricist's companion}, Princeton University Press, Chapter 3.4.3.
\item
  Cunningham, Scott, \emph{Causal Inference: The Mixtape}, Chapter 2, Yale University Press, \url{https://mixtape.scunning.com}.
\item
  ElHabr, Tony, 2019, `A Bayesian Approach to Ranking English Premier League Teams (using R)', \url{https://tonyelhabr.rbind.io/post/bayesian-statistics-english-premier-league/}.
\item
  Ioannidis, John PA, 2005, `Why most published research findings are false', \emph{PLoS medicine}, 2, no. 8, e124.
\item
  Pavlik, Kaylin, 2018, `Exploring the Relationship Between Dog Names and Breeds', \url{https://www.kaylinpavlik.com/dog-names-tfidf/}.
\item
  Pavlik, Kaylin, 2019, `Understanding + classifying genres using Spotify audio features', \url{https://www.kaylinpavlik.com/classifying-songs-genres/}.
\item
  Silge, Julia, 2019, `Modeling salary and gender in the tech industry', \url{https://juliasilge.com/blog/salary-gender/}.
\item
  Silge, Julia, 2019, `Opioid prescribing habits in Texas', \url{https://juliasilge.com/blog/texas-opioids/}.
\item
  Silge, Julia, 2019, `Tidymodels', \url{https://juliasilge.com/blog/intro-tidymodels/}.
\item
  Silge, Julia, 2020, `\#TidyTuesday hotel bookings and recipes', \url{https://juliasilge.com/blog/hotels-recipes/}.
\item
  Silge, Julia, 2020, `Hyperparameter tuning and \#TidyTuesday food consumption', \url{https://juliasilge.com/blog/food-hyperparameter-tune/}.
\item
  Taddy, Matt, 2019, \emph{Business Data Science}, Chapters 2 and 4.
\item
  Wasserstein, Ronald L. and Nicole A. Lazar, 2016, `The ASA Statement on p-Values: Context, Process, and Purpose', \emph{The American Statistician}, 70:2, 129-133, DOI: 10.1080/00031305.2016.1154108.
\end{itemize}

\textbf{Fun reading}

\begin{itemize}
\tightlist
\item
  Chellel, Kit, 2018, `The Gambler Who Cracked the Horse-Racing Code', \emph{Bloomberg Businessweek}, 3 May, \url{https://www.bloomberg.com/news/features/2018-05-03/the-gambler-who-cracked-the-horse-racing-code}.
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\tightlist
\item
  Simple and multiple linear regression.
\item
  Logistic and Poisson regression.
\item
  The key role of uncertainty.
\item
  Threats to validity of inferences
\item
  Overfitting.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{broom}
\item
  \texttt{huxtable}
\item
  \texttt{rstanarm}
\item
  \texttt{tidymodels}
\item
  \texttt{tidyverse}
\end{itemize}

\textbf{Key functions}

\begin{itemize}
\tightlist
\item
  \texttt{broom::augment()}
\item
  \texttt{broom::glance()}
\item
  \texttt{broom::tidy()}
\item
  \texttt{glm()}
\item
  \texttt{huxtable::huxreg()}
\item
  \texttt{lm()}
\item
  \texttt{parsnip::fit()}
\item
  \texttt{parsnip::linear\_reg()}
\item
  \texttt{parsnip::logistic\_reg()}
\item
  \texttt{parsnip::set\_engine()}
\item
  \texttt{poissonreg::poisson\_reg()}
\item
  \texttt{rnorm()}
\item
  \texttt{rpois()}
\item
  \texttt{rsample::initial\_split()}
\item
  \texttt{rsample::testing()}
\item
  \texttt{rsample::training()}
\item
  \texttt{sample()}
\item
  \texttt{set.seed()}
\item
  \texttt{summary()}
\end{itemize}

\textbf{Quiz}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Please write a linear relationship between some response variable, Y, and some predictor, X. What is the intercept term? What is the slope term? What would adding a hat to these indicate?
\item
  What is the least squares criterion? Similarly, what is RSS and what are we trying to do when we run least squares regression?
\item
  What is statistical bias?
\item
  If there were three variables: Snow, Temperature, and Wind, please write R code that would fit a simple linear regression to explain Snow as a function of Temperature and Wind. What do you think about another explanatory variable - daily stock market returns - to your model?
\item
  According to \citet{greenland2016statistical}, p-values test (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    All the assumptions about how the data were generated (the entire model), not just the targeted hypothesis it is supposed to test (such as a null hypothesis).
  \item
    Whether the hypothesis targeted for testing is true or not.
  \item
    A dichotomy whereby results can be declared `statistically significant'.
  \end{enumerate}
\item
  According to \citet{greenland2016statistical}, a p-value may be small because (select all)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    The targeted hypothesis is false.
  \item
    The study protocols were violated.
  \item
    It was selected for presentation based on its small size.
  \end{enumerate}
\item
  According to \citet{obermeyer2019dissecting}, why does racial bias occur in an algorithm used to guide health decisions in the US (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    The algorithm uses health costs as a proxy for health needs.
  \item
    The algorithm was trained on Reddit data.
  \end{enumerate}
\item
  When should we use logistic regression (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Continuous dependent variable.
  \item
    Binary dependent variable.
  \item
    Count dependent variable.
  \end{enumerate}
\item
  I am interested in studying how voting intentions in the recent US presidential election vary by an individual's income. I set up a logistic regression model to study this relationship. In my study, one possible dependent variable would be (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Whether the respondent is a US citizen (yes/no)
  \item
    The respondent's personal income (high/low)
  \item
    Whether the respondent is going to vote for Trump (yes/no)
  \item
    Who the respondent voted for in 2016 (Trump/Clinton)
  \end{enumerate}
\item
  I am interested in studying how voting intentions in the recent US presidential election vary by an individual's income. I set up a logistic regression model to study this relationship. In my study, one possible dependent variable would be (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    The race of the respondent (white/not white)
  \item
    The respondent's marital status (married/not)
  \item
    Whether the respondent is registered to vote (yes/no)
  \item
    Whether the respondent is going to vote for Biden (yes/no)
  \end{enumerate}
\item
  Please explain what a p-value is, using only the term itself (i.e.~`p-value') and words that are amongst the 1,000 most common in the English language according to the XKCD Simple Writer - \url{https://xkcd.com/simplewriter/}. (Please write one or two paragraphs.)
\item
  The mean of a Poisson distribution is equal to its?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Median.
  \item
    Standard deviation.
  \item
    Variance.
  \end{enumerate}
\end{enumerate}

\hypertarget{overview-1}{%
\section{Overview}\label{overview-1}}

\begin{quote}
Words! Mere words! How terrible they were! How clear, and vivid, and cruel! One could not escape from them. And yet what a subtle magic there was in them! They seemed to be able to give a plastic form to formless things, and to have a music of their own as sweet as that of viol or of lute. Mere words! Was there anything so real as words?

Oscar Wilde, \emph{The Picture of Dorian Gray}.
\end{quote}

\begin{quote}
Regression will not sort it out. Regression is indeed an oracle, but a cruel one. It speaks in riddles and delights in punishing us for asking bad questions.

\citet[p.~162]{citemcelreath}.
\end{quote}

Linear models have been around for a long time, at least since Galton and many others (some of whom were eugenicists) used linear regression in earnest. The generalized linear model framework came into being, in a formal sense, in the 70s with the seminal folks being Nelder and Wedderburn \citep{nelder1972generalized}. The idea of generalized linear models is that we broaden the types of outcomes that are allowed. You're still modelling things as a linear function, but you're not constrained to an outcome that is normally distributed. The outcome can be anything in the exponential family. A further, well, generalization of generalized linear models is generalized additive models where you're not generalizing anything to do with the outcome, but instead the structure of the explanatory side, as it were. We're still explaining the dependent variable as an additive function of bits, but those bits can be functions. This framework, in this way, came about in the 90s, with Hastie and Tibshirani \citep{hastie1990generalized} (fun fact, Tibshirani did a stats masters at Toronto, and was a professor here from 1985 through to 1998!).

It's important to recognise that when we build models we are not discovering `the truth'. We are using the model to help us explore and understand the data that we have. There is no one best model, there are just useful models that help us learn something about the data that we have and hence, hopefully, something about the world from which the data were generated. Ben Rhodes, who was an Obama staffer, titled his White House memoirs `The World as It Is: A Memoir of the Obama White House'. When we use models, we are similarly trying to understand the world, but as the second part of the title makes clear, there are enormous constraints on the perspective. In the same way that we'd not expect Rhodes to advocate an Australian, Canadian, or even US Republican, perspective about the world, it's silly to expect one model to be universal.

We use models to understand the world. We poke, push, and test them. We build them and rejoice in their beauty, and then seek to understand their limits and ultimately destroy them. It is this process that is important, it is this process that allows us to better understand the world. \citet[p.~19]{citemcelreath} talks about small and large worlds, saying `(a)ll statistical modeling has these two frames: the small world of the model itself and the large world we hope to deploy the model in'. To what extent does a model trained on the experiences of straight, cis, men, speak to the world as it is? It's not worthless, but it's also not unimpeachable. To what extent does the model teach us about the data that we have? To what extent do the data that we have reflect the world for which we would like to draw conclusions? Keep these questions front of mind.

Much of statistics was developed in a vacuum. And that's reasonable because it was developed for situations in which X, Y and Z. The original statisticians were literally able to randomise the order of fields and planting because they literally worked in agricultural stations (CITE). However, almost all subsequent applications have not had those properties. We often teach undergraduates that science proceeds (ADD POINTS ABOUT NULL HYPOTHESIS AND POPPER). If you believe that's how it works, then I have a bridge to sell you. Scientists react to incentives. They dabble, guess, and test, and then follow their guesses and backfill. They apply for grant funding for things that they did last time (because they know that'll work) and then spend the money to conduct other things. All of this is fine. But it's not a world in which a traditional null hypothesis holds, which means p-values and power lose their meaning. While you need to understand the `old world', you also need to be sophisticated enough to understand when you need to move away from it.

In this chapter we\ldots{} It is called `It's Just A Linear Model' after a famous quote by Professor Daniela Witten, who identifies how far we can get with linear models and the huge extent to which they underpin statistics.

\hypertarget{simple-linear-regression}{%
\section{Simple linear regression}\label{simple-linear-regression}}

\begin{figure}
\includegraphics[width=0.7\linewidth]{/Users/rohanalexander/Documents/book/figures/lolregression} \caption{Oh my.}\label{fig:logregression}
\end{figure}

Source: \href{https://twitter.com/mijkenijk/status/1234899588311474176}{Mijke Rhemtulla}, 3 March 2020.

\hypertarget{overview-2}{%
\subsection{Overview}\label{overview-2}}

When we have two continuous variables we use simple linear regression. This is based on the Normal (also `Gaussian') distribution. From \citet[p.~94]{pitman} `The normal distribution with mean \(\mu\) and standard deviation \(\sigma\) is the distribution over the x-axis defined by areas under the normal curve with these parameters. The equation of the normal curve with parameters \(\mu\) and \(\sigma\), can be written as:
\[y = \frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{1}{2}z^2},\]
where \(z = (x - \mu)/\sigma\) measures the number of standard deviations from the mean \(\mu\) to the number \(x\).'

In \texttt{R} we can simulate \(n\) data points from the Normal distribution with \texttt{rnorm()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{20}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}
\CommentTok{\#\textgreater{}  [1] {-}1.03830415  0.45007414 {-}0.92841267  2.07705191}
\CommentTok{\#\textgreater{}  [5]  0.01100518 {-}0.19647894 {-}0.01035069  0.78473863}
\CommentTok{\#\textgreater{}  [9]  0.34374023  0.72619373 {-}1.25329166  0.05990539}
\CommentTok{\#\textgreater{} [13]  0.03453881 {-}0.95807034 {-}0.35676062 {-}0.08239652}
\CommentTok{\#\textgreater{} [17]  0.28501740  0.09357992  0.20207979 {-}0.47891842}
\end{Highlighting}
\end{Shaded}

It will take a few draws before we get the expected shape.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}
\FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{number\_of\_draws =} \FunctionTok{c}\NormalTok{(}
    \FunctionTok{rep.int}\NormalTok{(}\AttributeTok{x =} \StringTok{"2 draws"}\NormalTok{, }\AttributeTok{times =} \DecValTok{2}\NormalTok{),}
    \FunctionTok{rep.int}\NormalTok{(}\AttributeTok{x =} \StringTok{"5 draws"}\NormalTok{, }\AttributeTok{times =} \DecValTok{5}\NormalTok{),}
    \FunctionTok{rep.int}\NormalTok{(}\AttributeTok{x =} \StringTok{"10 draws"}\NormalTok{, }\AttributeTok{times =} \DecValTok{10}\NormalTok{),}
    \FunctionTok{rep.int}\NormalTok{(}\AttributeTok{x =} \StringTok{"50 draws"}\NormalTok{, }\AttributeTok{times =} \DecValTok{50}\NormalTok{),}
    \FunctionTok{rep.int}\NormalTok{(}\AttributeTok{x =} \StringTok{"100 draws"}\NormalTok{, }\AttributeTok{times =} \DecValTok{100}\NormalTok{),}
    \FunctionTok{rep.int}\NormalTok{(}\AttributeTok{x =} \StringTok{"500 draws"}\NormalTok{, }\AttributeTok{times =} \DecValTok{500}\NormalTok{),}
    \FunctionTok{rep.int}\NormalTok{(}\AttributeTok{x =} \StringTok{"1,000 draws"}\NormalTok{, }\AttributeTok{times =} \DecValTok{1000}\NormalTok{),}
    \FunctionTok{rep.int}\NormalTok{(}\AttributeTok{x =} \StringTok{"10,000 draws"}\NormalTok{, }\AttributeTok{times =} \DecValTok{10000}\NormalTok{),}
    \FunctionTok{rep.int}\NormalTok{(}\AttributeTok{x =} \StringTok{"100,000 draws"}\NormalTok{, }\AttributeTok{times =} \DecValTok{100000}\NormalTok{)),}
  \AttributeTok{draws =} \FunctionTok{c}\NormalTok{(}
    \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{2}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
    \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
    \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
    \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{50}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
    \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
    \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{500}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
    \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{1000}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
    \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{10000}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
    \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{100000}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{))}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{number\_of\_draws =} \FunctionTok{as\_factor}\NormalTok{(number\_of\_draws)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ draws)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(number\_of\_draws),}
             \AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}Draw\textquotesingle{}}\NormalTok{,}
       \AttributeTok{y =} \StringTok{\textquotesingle{}Density\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{41-ijalm_files/figure-latex/unnamed-chunk-2-1.pdf}

When we use simple linear regression, we assume that our relationship is characterised by the variables and the parameters, with any difference, often denoted by \(\epsilon\), between the expectation and the reality being normally distributed.

If we have two variables, \(Y\) and \(X\), then we could characterise the relationship between these as:
\[Y \sim \beta_0 + \beta_1 X.\]

There are two coefficients/parameters: the `intercept' is \(\beta_0\), and the `slope' is \(\beta_1\). We are saying that \(Y\) will have some value, \(\beta_0\), even when \(X\) is 0, and that \(Y\) will change by \(\beta_1\) units for every one unit change in \(X\). The language that we use is that `X is being regressed on Y'.

We may then take this relationship to the data that we have about the relationship in order to estimate these coefficients for those particular values that we have:
\[\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x.\]

The hats are used to indicate that these are estimated values. We are saying this is a linear regression because we assume that if \(x\) doubles then \(y\) would also double. Linear regressions considers how the average of a dependent variable changes based on the independent variables.

I want to focus on data, so we'll make this example concrete, by generating some data and then discussing everything in the context of that. The example will be looking at someone's time for running five kilometers, compared with their time for running a marathon.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}
\NormalTok{number\_of\_observations }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{running\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{five\_km\_time =} \FunctionTok{rnorm}\NormalTok{(number\_of\_observations, }\DecValTok{20}\NormalTok{, }\DecValTok{3}\NormalTok{),}
         \AttributeTok{noise =} \FunctionTok{rnorm}\NormalTok{(number\_of\_observations, }\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{),}
         \AttributeTok{marathon\_time =}\NormalTok{ five\_km\_time }\SpecialCharTok{*} \FloatTok{8.4} \SpecialCharTok{+}\NormalTok{ noise,}
         \AttributeTok{was\_raining =} \FunctionTok{sample}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{), }
                              \AttributeTok{size =}\NormalTok{ number\_of\_observations,}
                              \AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }
                              \AttributeTok{prob =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.8}\NormalTok{)) }
\NormalTok{         )}

\NormalTok{running\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ five\_km\_time, }\AttributeTok{y =}\NormalTok{ marathon\_time)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Five{-}kilometer time (minutes)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Marathon time (minutes)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{41-ijalm_files/figure-latex/unnamed-chunk-3-1.pdf}

In this set-up we may like to use \(x\), which is the five-kilometer time, to produce estimates of \(y\), which is the marathon time. This would involve also estimating values of \(\beta_0\) and \(\beta_1\), which is why they have a hat on them.

But how should we estimate the coefficients? Even if we impose a linear relationship there are a lot of options (how many straight lines can you fit on a piece of paper?). But clearly some of the fits are not all that great.

One way we may define being great would be to impose that they be as close as possible to each of the \(x\) and \(y\) combinations that we know. There are a lot of candidates for how we define `as close as possible', but one is to minimise the sum of least squares. To do this we produce our estimates of \(\hat{y}\) based on some estimates of \(\hat{\beta}_0\) and \(\hat{\beta}_1\), given the \(x\), and then work out how `wrong', for every point \(i\), we were:
\[e_i = y_i - \hat{y}_i.\]

The residual sum of squares (RSS) then requires summing across all the points:
\[\mbox{RSS} = e^2_1+ e^2_2 +\dots + e^2_n.\]
This results in one `linear best-fit' line, but it is worth thinking about all of the assumptions and decisions that it took to get us to this point.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{running\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ five\_km\_time, }\AttributeTok{y =}\NormalTok{ marathon\_time)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }
              \AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{, }
              \AttributeTok{color =} \StringTok{"black"}\NormalTok{, }
              \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{,}
              \AttributeTok{formula =} \StringTok{\textquotesingle{}y \textasciitilde{} x\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Five{-}kilometer time (minutes)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Marathon time (minutes)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{41-ijalm_files/figure-latex/unnamed-chunk-4-1.pdf}

With the least squares criterion we want the values of \(\hat{\beta}_0\) and \(\hat{\beta}_1\) that result in the smallest RSS.

\hypertarget{implementation-in-base-r}{%
\subsection{Implementation in base R}\label{implementation-in-base-r}}

Within R, the main function for doing linear regression is \texttt{lm}. This is included in base R, so you don't need to call any packages, but in a moment, we will call a bunch of packages that will surround \texttt{lm} within an environment that we are more familiar with. You specify the relationship with the dependent variable first, then \texttt{\textasciitilde{}}, then the independent variables. Finally, you should specify the dataset (or you could pipe to it as usual).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{data =}\NormalTok{ dataset)}
\end{Highlighting}
\end{Shaded}

In general, you should assign this to an object:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{running\_data\_first\_model }\OtherTok{\textless{}{-}} 
  \FunctionTok{lm}\NormalTok{(marathon\_time }\SpecialCharTok{\textasciitilde{}}\NormalTok{ five\_km\_time, }
     \AttributeTok{data =}\NormalTok{ running\_data)}
\end{Highlighting}
\end{Shaded}

To see the result of your regression you can then call \texttt{summary()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(running\_data\_first\_model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = marathon\_time \textasciitilde{} five\_km\_time, data = running\_data)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}24.763  {-}5.686   0.722   6.650  16.707 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}              Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)    0.4114     6.0610   0.068    0.946    }
\CommentTok{\#\textgreater{} five\_km\_time   8.3617     0.3058  27.343   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  }
\CommentTok{\#\textgreater{} 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 8.474 on 98 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.8841, Adjusted R{-}squared:  0.8829 }
\CommentTok{\#\textgreater{} F{-}statistic: 747.6 on 1 and 98 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

The first part of the result tells us the regression that we called, then information about the residuals, and the estimated coefficients. And then finally some useful diagnostics.

We are considering that there is some relationship between \(X\) and \(Y\), that is: \(Y = f(X) + \epsilon\). We are going to say that function, \(f()\), is linear and so our relationship is:
\[\hat{Y} = \beta_0 + \beta_1 X + \epsilon.\]

There is some `true' relationship between \(X\) and \(Y\), but we don't know what it is. All we can do is use our sample of data to try to estimate it. But because our understanding depends on that sample, for every possible sample, we would get a slightly different relationship (as measured by the coefficients).

That \(\epsilon\) is a measure of our error - what does the model not know? There's going to be plenty that the model doesn't know, but we hope is that the error does not depend on \(X\), and that the error is normally distributed.

The intercept is marathon time that we would expect with a five-kilometer time of 0 minutes. Hopefully this example illustrates the need to carefully interpret the intercept coefficient! The coefficient on five-kilometer run time shows how we expect the marathon time to change if five-kilometer run time changed by one unit. In this case it's about 8.4, which makes sense seeing as a marathon is roughly that many times longer than a five-kilometer run.

\hypertarget{tidy-up-with-broom}{%
\subsection{Tidy up with broom}\label{tidy-up-with-broom}}

While there is nothing wrong with the base approach, I want to introduce the \texttt{broom} package because that will provide us with outputs in a tidy framework \citep{citebroom}. There are three key functions:

\begin{itemize}
\tightlist
\item
  \texttt{broom::tidy()}: Gives the coefficient estimates in a tidy output.
\item
  \texttt{broom::glance()}: Gives the diagnostics.
\item
  \texttt{broom::augment()}: Adds the forecast values, and hence, residuals, to your dataset.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(broom)}
\FunctionTok{tidy}\NormalTok{(running\_data\_first\_model)}
\CommentTok{\#\textgreater{} \# A tibble: 2 x 5}
\CommentTok{\#\textgreater{}   term         estimate std.error statistic  p.value}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}           \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 (Intercept)     0.411     6.06     0.0679 9.46e{-} 1}
\CommentTok{\#\textgreater{} 2 five\_km\_time    8.36      0.306   27.3    1.17e{-}47}
\FunctionTok{glance}\NormalTok{(running\_data\_first\_model)}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 12}
\CommentTok{\#\textgreater{}   r.squared adj.r.squared sigma statistic  p.value    df}
\CommentTok{\#\textgreater{}       \textless{}dbl\textgreater{}         \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1     0.884         0.883  8.47      748. 1.17e{-}47     1}
\CommentTok{\#\textgreater{} \# ... with 6 more variables: logLik \textless{}dbl\textgreater{}, AIC \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   BIC \textless{}dbl\textgreater{}, deviance \textless{}dbl\textgreater{}, df.residual \textless{}int\textgreater{},}
\CommentTok{\#\textgreater{} \#   nobs \textless{}int\textgreater{}}
\end{Highlighting}
\end{Shaded}

Notice how the results are fairly similar to the base summary function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{running\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{augment}\NormalTok{(running\_data\_first\_model,}
          \AttributeTok{data =}\NormalTok{ running\_data)}
\FunctionTok{head}\NormalTok{(running\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 10}
\CommentTok{\#\textgreater{}   five\_km\_time  noise marathon\_time was\_raining .fitted}
\CommentTok{\#\textgreater{}          \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}         \textless{}dbl\textgreater{} \textless{}chr\textgreater{}         \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1         18.9 {-}3.73           155. No             159.}
\CommentTok{\#\textgreater{} 2         19.9  8.42           175. No             167.}
\CommentTok{\#\textgreater{} 3         14.7  4.32           127. No             123.}
\CommentTok{\#\textgreater{} 4         16.6 {-}2.74           137. No             139.}
\CommentTok{\#\textgreater{} 5         17.0 {-}4.89           138. No             142.}
\CommentTok{\#\textgreater{} 6         25.3  0.648          213. No             212.}
\CommentTok{\#\textgreater{} \# ... with 5 more variables: .resid \textless{}dbl\textgreater{}, .hat \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   .sigma \textless{}dbl\textgreater{}, .cooksd \textless{}dbl\textgreater{}, .std.resid \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

We could now make plots of the residuals.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(running\_data, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ .resid)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Number of occurrences"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Residuals"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{41-ijalm_files/figure-latex/unnamed-chunk-10-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{ggplot}\NormalTok{(running\_data, }\FunctionTok{aes}\NormalTok{(five\_km\_time, .resid)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dotted"}\NormalTok{, }\AttributeTok{color =} \StringTok{"grey"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Residuals"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Five{-}kilometer time (minutes)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{41-ijalm_files/figure-latex/unnamed-chunk-10-2.pdf}

When we say our estimate is unbiased, we are trying to say that even though with some sample our estimate might be too high, and with another sample our estimate might be too low, eventually if we have a lot of data then our estimate would be the same as the population. (A pro hockey player may sometimes shoot right of the net, and sometimes left of the net, but we'd hope that on average they'd be right in the middle of the net). In the words of \citet{islr}, `an unbiased estimator does not systematically over- or under-estimate the true parameter.'

But we want to try to speak to the `true' relationship, so we need to try to capture how much we think our understanding depends on the particular sample that we have to analyse. And this is where standard error comes in. It tells us how off our estimate is compared with the actual.

From standard errors, we can compute a confidence interval. A 95 per cent confidence interval means that there is a 0.95 probability that the interval happens to contain the population parameter (which is typically unknown).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{running\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ five\_km\_time, }\AttributeTok{y =}\NormalTok{ marathon\_time)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }
              \AttributeTok{se =} \ConstantTok{TRUE}\NormalTok{, }
              \AttributeTok{color =} \StringTok{"black"}\NormalTok{, }
              \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{,}
              \AttributeTok{formula =} \StringTok{\textquotesingle{}y \textasciitilde{} x\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Five{-}kilometer time (minutes)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Marathon time (minutes)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{41-ijalm_files/figure-latex/unnamed-chunk-11-1.pdf}

There are a bunch of different tests that you can use to understand how your model is performing given this data. One quick way to look at a whole bunch of different aspects is to use the \texttt{performance} package \citep{citePerformance}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(performance)}
\NormalTok{performance}\SpecialCharTok{::}\FunctionTok{check\_model}\NormalTok{(running\_data\_first\_model)}
\end{Highlighting}
\end{Shaded}

\includegraphics{41-ijalm_files/figure-latex/unnamed-chunk-12-1.pdf}

\hypertarget{testing-hypothesis}{%
\subsection{Testing hypothesis}\label{testing-hypothesis}}

Now that we have an interval for which we can say there is a 95 per cent probability it contains the true population parameter we can test claims. For instance, a null hypothesis that there is no relationship between \(X\) and \(Y\) (i.e.~\(\beta_1 = 0\)), compared with an alternative hypothesis that there is some relationship between \(X\) and \(Y\) (i.e.~\(\beta_1 \neq 0\)).

We need to know whether our estimate of \(\beta_1\), which is \(\hat{\beta}_1\), is `far enough' away from zero for us to be comfortable claiming that \(\beta_1 \neq 0\). How far is `far enough'? If we were very confident in our estimate of \(\beta_1\) then it wouldn't have to be far, but if we were not then it would have to be substantial. So it depends on a bunch of things, but essentially the standard error of \(\hat{\beta}_1\).

We compare this standard error with \(\hat{\beta}_1\) to get the t-statistic:
\[t = \frac{\hat{\beta}_1 - 0}{\mbox{SE}(\hat{\beta}_1)}.\]
And we then compare our t-statistic to the t-distribution to compute the probability of getting this absolute t-statistic or a larger one, if \(\beta_1 = 0\). This is the p-value. A small p-value means it is unlikely that we would observe our association due to chance if there wasn't a relationship.

\hypertarget{on-p-values}{%
\subsection{On p-values}\label{on-p-values}}

The p-value is a specific and subtle concept. It is easy to abuse. The main issue is that it embodies, and assumes correct, every assumption of the model. From \citet[p.~339]{greenland2016statistical}: `The p-value is then the probability that the chosen test statistic would have been at least as large as its observed value if every model assumption were correct, including the test hypothesis.'. To provide background on the language used here in case you're unfamiliar, a test hypothesis is typically a `null hypothesis', and a `test statistic' is `the distance between the data and the model prediction' \citep{greenland2016statistical}.

The following quote (minor edits for consistency with above) summarises the situation:

\begin{quote}
It is true that the smaller the p-value, the more unusual the data would be if every single assumption were correct; but a very small p-value does not tell us which assumption is incorrect. For example, the p-value may be very small because the targeted hypothesis is false; but it may instead (or in addition) be very small because the study protocols were violated, or because it was selected for presentation based on its small size. Conversely, a large p-value indicates only that the data are not unusual under the model, but does not imply that the model or any aspect of it (such as the targeted hypothesis) is correct; it may instead (or in addition) be large because (again) the study protocols were violated, or because it was selected for presentation based on its large size.

The general definition of a p-value may help one to understand why statistical tests tell us much less than what many think they do: Not only does a p-value not tell us whether the hypothesis targeted for testing is true or not; it says nothing specifically related to that hypothesis unless we can be completely assured that every other assumption used for its computation is correct---an assurance that is lacking in far too many studies.

\citet[p.~339]{greenland2016statistical}.
\end{quote}

There is nothing inherently wrong about using p-values, but it is important to use them in sophisticated and thoughtful ways.

Typically one application where it's easy to see abuse of p-values is in power analysis. As \citet[p.~438]{gelmanandhill} say, `{[}s{]}ample size is never large enough\ldots. this is not a problem\ldots{} {[}w{]}e are just emphasizing that, just as you never have enough money, because perceived needs increase with resources, your inferential needs with increase with your sample size.' Power refers to the probability of incorrectly failing to reject the null hypothesis. As \citet[p.~303]{Imai2017} says:

\begin{quote}
We use power analysis in order to formalize the degree of informativeness of data in hypothesis tests. The power of a statistical hypothesis test is defined as one minus the probability of type II error:

power = 1-P(type II error)
\end{quote}

In a vacuum, we'd like to have high power and we can achieve that either by having really big effect sizes, or by having a larger number of observations.

\hypertarget{multiple-linear-regression}{%
\section{Multiple linear regression}\label{multiple-linear-regression}}

To this point we've just considered one explanatory variable. But we'll usually have more than one. One approach would be to run separate regressions for each explanatory variable. But compared with separate linear regressions for each, adding more explanatory variables allows us to have a better understanding of the intercept and accounts for interaction. Often the results will be quite different.

\begin{quote}
This slightly counterintuitive result is very common in many real life situations. Consider an absurd example to illustrate the point. Running a regression of shark attacks versus ice cream sales for data collected at a given beach community over a period of time would show a positive relationship, similar to that seen between sales and newspapers. Of course no one (yet) has suggested that ice creams should be banned at beaches to reduce shark attacks. In reality, higher temperatures cause more people to visit the beach, which in turn results in more ice cream sales and more shark attacks. A multiple regression of attacks versus ice cream sales and temperature reveals that, as intuition implies, the former predictor is no longer significant after adjusting for temperature.

\citep[p.~74]{islr}.
\end{quote}

We may also like to consider variables that do not have an inherent ordering. For instance, pregnant or not. When there are only two options then we can use a binary variable which is 0 or 1. If there are more than two levels then use a combination of binary variables, where the `missing' outcome (baseline) gets pushed onto the intercept.

In other languages you may need to explicitly construct dummy variables, but as R was designed as a language to do statistical programming, it does a lot of the work here for you and is fairly forgiving. For instance, if you have a column of character values that only had two values: \texttt{c("Monica",\ "Rohan",\ "Rohan",\ "Monica",\ "Monica",\ "Rohan")}, and you used this as a independent variable in your usual regression set up then R would treat it as a dummy variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{running\_data\_rain\_model }\OtherTok{\textless{}{-}} 
  \FunctionTok{lm}\NormalTok{(marathon\_time }\SpecialCharTok{\textasciitilde{}}\NormalTok{ five\_km\_time }\SpecialCharTok{+}\NormalTok{ was\_raining, }
     \AttributeTok{data =}\NormalTok{ running\_data)}
\FunctionTok{summary}\NormalTok{(running\_data\_rain\_model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = marathon\_time \textasciitilde{} five\_km\_time + was\_raining, data = running\_data)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}24.6239  {-}5.5806   0.8377   6.7636  16.8671 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)      0.1430     6.1476   0.023    0.981    }
\CommentTok{\#\textgreater{} five\_km\_time     8.3689     0.3081  27.166   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} was\_rainingYes   0.7043     2.2220   0.317    0.752    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  }
\CommentTok{\#\textgreater{} 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 8.513 on 97 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.8842, Adjusted R{-}squared:  0.8818 }
\CommentTok{\#\textgreater{} F{-}statistic: 370.4 on 2 and 97 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

The result probably isn't too surprising if we look at a plot of the data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{running\_data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ five\_km\_time, }\AttributeTok{y =}\NormalTok{ marathon\_time, }\AttributeTok{color =}\NormalTok{ was\_raining)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{color =} \StringTok{"black"}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Five{-}kilometer time (minutes)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Marathon time (minutes)"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Was raining"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\CommentTok{\#\textgreater{} \textasciigrave{}geom\_smooth()\textasciigrave{} using formula \textquotesingle{}y \textasciitilde{} x\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

\includegraphics{41-ijalm_files/figure-latex/unnamed-chunk-14-1.pdf}

In addition to wanting to include additional explanatory variables we may think that they are related with one another. For instance, if we were wanting to explain the amount of snowfall in Toronto, then we may be interested in the humidity and the temperature, but those two variables may also interact. We can do this by using \texttt{*} instead of \texttt{+} when we specify the model in R. If you do interact variables, then you should almost always also include the individual variables as well (Figure \ref{fig:trump}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{figures/trump} \caption{Don't leave out the main effects in an interactive model}\label{fig:trump}
\end{figure}

Source: By \href{https://twitter.com/kai_arzheimer/status/1228998718646607876}{Kai Arzheimer}, 16 February 2020.

\hypertarget{threats-to-validity-and-aspects-to-think-about}{%
\subsection{Threats to validity and aspects to think about}\label{threats-to-validity-and-aspects-to-think-about}}

There are a variety of weaknesses and aspects that you should discuss when you use linear regression. A quick list includes \citep[p.~92]{islr}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Non-linearity of the response-predictor relationships.
\item
  Correlation of error terms.
\item
  Non-constant variance of error terms.
\item
  Outliers.
\item
  High-leverage points.
\item
  Collinearity
\end{enumerate}

These are also aspects that you should discuss if you use linear regression. Including plots tends to be handy here to illustrate your points. Other aspects that you may consider discussing include \citep[p.~75]{islr}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Is at least one of the predictors \(X_1, X_2, \dots, X_p\) useful in predicting the response?
\item
  Do all the predictors help to explain \(Y\), or is only a subset of the predictors useful?
\item
  How well does the model fit the data?
\item
  Given a set of predictor values, what response value should we predict, and how accurate is our prediction?
\end{enumerate}

\hypertarget{more-credible-outputs}{%
\subsection{More credible outputs}\label{more-credible-outputs}}

Finally, after creating beautiful graphs and tables you may want your regression output to look just as nice. There are a variety of packages in R that will automatically format your regression outputs. One that is particularly nice is \texttt{huxtable} \citep{citehuxtable}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(huxtable)}
\FunctionTok{huxreg}\NormalTok{(running\_data\_first\_model, running\_data\_rain\_model)}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-15} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} (1) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} (2) \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.411\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.143\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (6.061)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (6.148)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} five\_km\_time \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 8.362 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 8.369 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.306)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.308)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} was\_rainingYes \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.704\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (2.222)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 100\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 100\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.884\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.884\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} logLik \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -354.584\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -354.532\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} AIC \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 715.168\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 717.064\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

\hypertarget{implementation-in-tidymodels}{%
\subsection{\texorpdfstring{Implementation in \texttt{tidymodels}}{Implementation in tidymodels}}\label{implementation-in-tidymodels}}

The reason that we went to all that trouble to do simple regression is that we often want to fit a bunch of models. One way is to copy/paste code a bunch of times. There's nothing wrong with that. And that's the way that most people get started, but you may want to take an approach that scales more easily. We also need to think more carefully about over-fitting, and being able to evaluate our models.

The \texttt{tidymodels} package \citep{citeTidymodels} is what all the cool kids are using these days. It's an attempt to bring some order to the chaos that has been different modelling packages in R. (There have been other attempts in the past and they've crashed and burned, but hopefully this time is different.) The issue is that let's say you want to run a simple linear regression and then run a random forest. The language that you'd use to code these models is fairly different. The \texttt{tidymodels} package is the latest attempt to bring a coherent grammar to this. It's also a package of packages.

We'll create test and training datasets.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}
\FunctionTok{library}\NormalTok{(tidymodels)}

\NormalTok{running\_data\_split }\OtherTok{\textless{}{-}}\NormalTok{ rsample}\SpecialCharTok{::}\FunctionTok{initial\_split}\NormalTok{(running\_data, }\AttributeTok{prop =} \FloatTok{0.80}\NormalTok{)}
\NormalTok{running\_data\_split}
\CommentTok{\#\textgreater{} \textless{}Analysis/Assess/Total\textgreater{}}
\CommentTok{\#\textgreater{} \textless{}80/20/100\textgreater{}}
\end{Highlighting}
\end{Shaded}

So we have 81 points in our training set, 19 in our test set and 100 in total.

We can then make datasets for the test and training samples.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{running\_data\_train }\OtherTok{\textless{}{-}}\NormalTok{ rsample}\SpecialCharTok{::}\FunctionTok{training}\NormalTok{(running\_data\_split)}
\NormalTok{running\_data\_test  }\OtherTok{\textless{}{-}}\NormalTok{  rsample}\SpecialCharTok{::}\FunctionTok{testing}\NormalTok{(running\_data\_split)}
\end{Highlighting}
\end{Shaded}

If we have a look at the dataset that we made we can see that it's got fewer rows. We could have reached the same outcome with something like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{running\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  running\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{magic\_number =} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(running\_data)), }\AttributeTok{size =} \FunctionTok{nrow}\NormalTok{(running\_data), }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{))}

\NormalTok{running\_data\_test }\OtherTok{\textless{}{-}} 
\NormalTok{  running\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(magic\_number }\SpecialCharTok{\textless{}=} \DecValTok{20}\NormalTok{)}

\NormalTok{running\_data\_train }\OtherTok{\textless{}{-}} 
\NormalTok{  running\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(magic\_number }\SpecialCharTok{\textgreater{}} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{first\_go }\OtherTok{\textless{}{-}} 
\NormalTok{  parsnip}\SpecialCharTok{::}\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  parsnip}\SpecialCharTok{::}\FunctionTok{set\_engine}\NormalTok{(}\AttributeTok{engine =} \StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  parsnip}\SpecialCharTok{::}\FunctionTok{fit}\NormalTok{(marathon\_time }\SpecialCharTok{\textasciitilde{}}\NormalTok{ five\_km\_time }\SpecialCharTok{+}\NormalTok{ was\_raining, }
               \AttributeTok{data =}\NormalTok{ running\_data\_train}
\NormalTok{               )}
\end{Highlighting}
\end{Shaded}

\hypertarget{implementation-in-rstanarm}{%
\subsection{\texorpdfstring{Implementation in \texttt{rstanarm}}{Implementation in rstanarm}}\label{implementation-in-rstanarm}}

The \texttt{tidymodels} package will be fine for specific types of tasks. For instance if you are doing machine learning then chances are you are interested in forecasting. That's the kind of thing that \texttt{tidymodels} is really built for. If you want equivalent firepower for explanatory modelling then one option is to use Bayesian approaches more directly. Yes, you can use Bayesian models within the \texttt{tidymodels} ecosystem, but as you start to move away from out-of-the-box solutions, it becomes important to start to understand what is going on under the hood.

There are a variety of ways of getting started, but essentially what you need is a probabilistic programming language. That is one that is specifically designed for this sort of thing, in comparison to \texttt{R}, which is designed for more general statistical computing. We will use \texttt{Stan} in these notes within the context of our familiar R environment. We will interface with \texttt{Stan} using the \texttt{rstanarm} package \citep{citerstanarm}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rstanarm)}

\NormalTok{first\_go\_in\_rstanarm }\OtherTok{\textless{}{-}}
  \FunctionTok{stan\_lm}\NormalTok{(}
\NormalTok{    marathon\_time }\SpecialCharTok{\textasciitilde{}}\NormalTok{ five\_km\_time }\SpecialCharTok{+}\NormalTok{ was\_raining, }
    \AttributeTok{data =}\NormalTok{ running\_data,}
    \AttributeTok{prior =} \ConstantTok{NULL}\NormalTok{,}
    \AttributeTok{seed =} \DecValTok{853}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{first\_go\_in\_rstanarm}
\CommentTok{\#\textgreater{} stan\_lm}
\CommentTok{\#\textgreater{}  family:       gaussian [identity]}
\CommentTok{\#\textgreater{}  formula:      marathon\_time \textasciitilde{} five\_km\_time + was\_raining}
\CommentTok{\#\textgreater{}  observations: 100}
\CommentTok{\#\textgreater{}  predictors:   3}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}                Median MAD\_SD}
\CommentTok{\#\textgreater{} (Intercept)    0.4    6.0   }
\CommentTok{\#\textgreater{} five\_km\_time   8.4    0.3   }
\CommentTok{\#\textgreater{} was\_rainingYes 0.7    2.2   }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Auxiliary parameter(s):}
\CommentTok{\#\textgreater{}               Median MAD\_SD}
\CommentTok{\#\textgreater{} R2            0.9    0.0   }
\CommentTok{\#\textgreater{} log{-}fit\_ratio 0.0    0.0   }
\CommentTok{\#\textgreater{} sigma         8.6    0.6   }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} * For help interpreting the printed output see ?print.stanreg}
\CommentTok{\#\textgreater{} * For info on the priors used see ?prior\_summary.stanreg}
\end{Highlighting}
\end{Shaded}

\hypertarget{logistic-regression-2}{%
\section{Logistic regression}\label{logistic-regression-2}}

\hypertarget{overview-3}{%
\subsection{Overview}\label{overview-3}}

To steal a joke from someone, `it's AI when you're fundraising, machine learning when you're hiring, and logistic regression when you're implementing.'

When the dependent variable is a binary outcome, that is 0 or 1, then instead of linear regression we may like to use logistic regression. Although a binary outcome may sound limiting, there are a lot of circumstances in which your outcome either naturally falls into this situation, or can be adjusted into it (e.g.~a voter supports the liberals or not the liberals).

The reason that we use logistic regression is that we'll be modelling a probability and so it will be bounded between 0 and 1. Whereas with linear regression we may end up with values outside this. In practice it is usually fine to start with linear regression and then move to logistic regression as you build confidence.

This all said, logistic regression, as Daniella Witten teaches us, is just a linear model!

\hypertarget{implementation-in-base}{%
\subsection{Implementation in base}\label{implementation-in-base}}

I'd like to consider a slightly more interesting example, which is a dataset of pearl jewellery, from the Australian retailer Paspaley.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{paspaley\_dataset }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/RohanAlexander/paspaley/master/outputs/data/cleaned\_dataset.csv"}\NormalTok{)}
\CommentTok{\#\textgreater{} Rows: 1289 Columns: 13}
\CommentTok{\#\textgreater{} {-}{-} Column specification {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Delimiter: ","}
\CommentTok{\#\textgreater{} chr (10): product, name, description, availability, sku,...}
\CommentTok{\#\textgreater{} dbl  (2): price, year}
\CommentTok{\#\textgreater{} lgl  (1): keshi}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} i Use \textasciigrave{}spec()\textasciigrave{} to retrieve the full column specification for this data.}
\CommentTok{\#\textgreater{} i Specify the column types or set \textasciigrave{}show\_col\_types = FALSE\textasciigrave{} to quiet this message.}

\NormalTok{paspaley\_dataset}\SpecialCharTok{$}\NormalTok{metal }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{table}\NormalTok{()}
\CommentTok{\#\textgreater{} .}
\CommentTok{\#\textgreater{}       Other    Platinum   Rose gold  White gold Yellow gold }
\CommentTok{\#\textgreater{}         134          23          89         475         568}
\end{Highlighting}
\end{Shaded}

In this case we'll model whether some jewellery is made of white or yellow gold, based on their price and the year (Figure \ref{fig:logisticgraph}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{paspaley\_logistic\_dataset }\OtherTok{\textless{}{-}} 
\NormalTok{  paspaley\_dataset }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(metal }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}White gold\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Yellow gold\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(metal, price, year)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{41-ijalm_files/figure-latex/logisticgraph-1.pdf}
\caption{\label{fig:logisticgraph}Examining the type of gold some jewellery is made from.}
\end{figure}

The graph suggests that we should filter any price higher than \$100,000.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{paspaley\_logistic\_dataset }\OtherTok{\textless{}{-}} 
\NormalTok{  paspaley\_logistic\_dataset }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(price }\SpecialCharTok{\textless{}} \DecValTok{100000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

As with linear regression, logistic regression is built into R, with the \texttt{glm} function. In this case, we'll try to work out if the jewellery was white gold. Although not strictly necessary for this particular function, we'll change it to a binary, that will be 1 if white gold and 0 if not.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{paspaley\_logistic\_dataset }\OtherTok{\textless{}{-}} 
\NormalTok{  paspaley\_logistic\_dataset }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{is\_white\_gold =} \FunctionTok{if\_else}\NormalTok{(metal }\SpecialCharTok{==} \StringTok{"White gold"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))}

\NormalTok{white\_gold\_model }\OtherTok{\textless{}{-}} 
  \FunctionTok{glm}\NormalTok{(is\_white\_gold }\SpecialCharTok{\textasciitilde{}}\NormalTok{ price }\SpecialCharTok{+}\NormalTok{ year, }
    \AttributeTok{data =}\NormalTok{ paspaley\_logistic\_dataset, }
    \AttributeTok{family =} \StringTok{\textquotesingle{}binomial\textquotesingle{}}\NormalTok{)}

\FunctionTok{summary}\NormalTok{(white\_gold\_model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = is\_white\_gold \textasciitilde{} price + year, family = "binomial", }
\CommentTok{\#\textgreater{}     data = paspaley\_logistic\_dataset)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}    Min      1Q  Median      3Q     Max  }
\CommentTok{\#\textgreater{} {-}1.250  {-}1.103  {-}1.015   1.247   1.353  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}               Estimate Std. Error z value Pr(\textgreater{}|z|)  }
\CommentTok{\#\textgreater{} (Intercept)  2.087e+02  8.674e+01   2.406   0.0161 *}
\CommentTok{\#\textgreater{} price        3.832e{-}06  5.405e{-}06   0.709   0.4783  }
\CommentTok{\#\textgreater{} year        {-}1.035e{-}01  4.296e{-}02  {-}2.408   0.0160 *}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  }
\CommentTok{\#\textgreater{} 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for binomial family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 1411.6  on 1023  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 1405.5  on 1021  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 1411.5}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 4}
\end{Highlighting}
\end{Shaded}

One reason that logistic regression can be a bit of a pain initially is because the coefficients take a bit of work to interpret. In particular, our estimate on price is -3.170e-06. This is the odds. So the odds that it was white gold decrease by -3.170e-06 as the price increases. We can have our model make forecasts in terms of a probability, by asking for that.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{paspaley\_logistic\_dataset }\OtherTok{\textless{}{-}} 
\NormalTok{  broom}\SpecialCharTok{::}\FunctionTok{augment}\NormalTok{(white\_gold\_model,}
          \AttributeTok{data =}\NormalTok{ paspaley\_logistic\_dataset,}
          \AttributeTok{type.predict =} \StringTok{"response"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(paspaley\_logistic\_dataset)}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-26} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l l l l l l l l}


\hhline{>{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0.4}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{metal} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{price} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{year} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{is\_white\_gold} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{.fitted} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{.resid} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{.std.resid} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{.hat} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{.sigma} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.4}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{.cooksd} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0.4}}l!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Yellow gold \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.58e+03 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.02e+03 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.505 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -1.19 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -1.19 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.0034\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.17 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.4}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.00116 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.4}}|>{\huxb{0, 0, 0}{0.4}}|}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0.4}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Yellow gold \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.08e+03 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.02e+03 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.504 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -1.18 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -1.19 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.00344 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.17 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.4}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.00118 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.4}}|>{\huxb{0, 0, 0}{0.4}}|}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0.4}}l!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Yellow gold \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 3.08e+03 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.02e+03 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.505 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -1.19 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -1.19 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.00335 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.17 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.4}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.00115 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.4}}|>{\huxb{0, 0, 0}{0.4}}|}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0.4}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} White gold \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 7.38e+03 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.02e+03 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.509 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.16 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.16 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.00313 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.17 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.4}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.00101 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.4}}|>{\huxb{0, 0, 0}{0.4}}|}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0.4}}l!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedright \hspace{6pt} White gold \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 3.08e+03 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.02e+03 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.505 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.17 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.17 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.00335 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.17 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.4}}}{\cellcolor[RGB]{242, 242, 242}\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.0011\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.4}}|>{\huxb{0, 0, 0}{0.4}}|}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0.4}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} White gold \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 3.95e+03 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.02e+03 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.506 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.17 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.17 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.00329 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.17 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.4}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.00108 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

\hypertarget{implementation-in-tidymodels-1}{%
\subsection{\texorpdfstring{Implementation in \texttt{tidymodels}}{Implementation in tidymodels}}\label{implementation-in-tidymodels-1}}

We can use \texttt{tidymodels} to run this if we wanted. In this case, we need it as a factor.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{paspaley\_logistic\_dataset }\OtherTok{\textless{}{-}} 
\NormalTok{  paspaley\_logistic\_dataset }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{is\_white\_gold =} \FunctionTok{as\_factor}\NormalTok{(is\_white\_gold))}

\NormalTok{paspaley\_logistic\_dataset\_split }\OtherTok{\textless{}{-}}\NormalTok{ rsample}\SpecialCharTok{::}\FunctionTok{initial\_split}\NormalTok{(paspaley\_logistic\_dataset, }\AttributeTok{prop =} \FloatTok{0.80}\NormalTok{)}
\NormalTok{paspaley\_logistic\_dataset\_train }\OtherTok{\textless{}{-}}\NormalTok{ rsample}\SpecialCharTok{::}\FunctionTok{training}\NormalTok{(paspaley\_logistic\_dataset\_split)}
\NormalTok{paspaley\_logistic\_dataset\_test  }\OtherTok{\textless{}{-}}\NormalTok{  rsample}\SpecialCharTok{::}\FunctionTok{testing}\NormalTok{(paspaley\_logistic\_dataset\_split)}

\NormalTok{white\_gold\_model\_tidymodels }\OtherTok{\textless{}{-}}
\NormalTok{  parsnip}\SpecialCharTok{::}\FunctionTok{logistic\_reg}\NormalTok{(}\AttributeTok{mode =} \StringTok{"classification"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  parsnip}\SpecialCharTok{::}\FunctionTok{set\_engine}\NormalTok{(}\StringTok{"glm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(is\_white\_gold }\SpecialCharTok{\textasciitilde{}}\NormalTok{ price }\SpecialCharTok{+}\NormalTok{ year, }
      \AttributeTok{data =}\NormalTok{ paspaley\_logistic\_dataset\_train)}

\NormalTok{white\_gold\_model\_tidymodels}
\CommentTok{\#\textgreater{} parsnip model object}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Fit time:  3ms }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:  stats::glm(formula = is\_white\_gold \textasciitilde{} price + year, family = stats::binomial, }
\CommentTok{\#\textgreater{}     data = data)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{} (Intercept)        price         year  }
\CommentTok{\#\textgreater{}   1.832e+02    5.245e{-}06   {-}9.082e{-}02  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Degrees of Freedom: 818 Total (i.e. Null);  816 Residual}
\CommentTok{\#\textgreater{} Null Deviance:       1130 }
\CommentTok{\#\textgreater{} Residual Deviance: 1125  AIC: 1131}
\end{Highlighting}
\end{Shaded}

\hypertarget{implementation-in-rstanarm-1}{%
\subsection{\texorpdfstring{Implementation in \texttt{rstanarm}}{Implementation in rstanarm}}\label{implementation-in-rstanarm-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{paspaley\_in\_rstanarm }\OtherTok{\textless{}{-}}
\NormalTok{  rstanarm}\SpecialCharTok{::}\FunctionTok{stan\_glm}\NormalTok{(}
\NormalTok{    is\_white\_gold }\SpecialCharTok{\textasciitilde{}}\NormalTok{ price }\SpecialCharTok{+}\NormalTok{ year,}
    \AttributeTok{data =}\NormalTok{ paspaley\_logistic\_dataset,}
    \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{),}
    \AttributeTok{prior =} \ConstantTok{NULL}\NormalTok{,}
    \AttributeTok{seed =} \DecValTok{853}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{poisson-regression}{%
\section{Poisson regression}\label{poisson-regression}}

\hypertarget{overview-4}{%
\subsection{Overview}\label{overview-4}}

When we have count data, we use Poisson distribution. From \citet[p.~121]{pitman} 'The Poisson distribution with parameter \(\mu\) or Poisson (\(\mu\)) distribution is the distribution of probabilities \(P_{\mu}(k)\) over \({0, 1, 2, ...}\) defined by:
\[P_{\mu}(k) = e^{-\mu}\mu^k/k!\mbox{, for }k=0,1,2,...\]
We can simulate \(n\) data points from the Poisson distribution with \texttt{rpois()} where \(\lambda\) is the mean and the variance.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rpois}\NormalTok{(}\AttributeTok{n =} \DecValTok{20}\NormalTok{, }\AttributeTok{lambda =} \DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{}  [1] 2 2 3 4 2 2 4 2 4 1 3 2 3 3 2 2 0 1 2 1}
\end{Highlighting}
\end{Shaded}

That \(\lambda\) parameter governs the shape of the distribution.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}
\NormalTok{number\_of\_each }\OtherTok{\textless{}{-}} \DecValTok{1000}
\FunctionTok{tibble}\NormalTok{(}\AttributeTok{lambda =} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, number\_of\_each), }\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, number\_of\_each), }\FunctionTok{rep}\NormalTok{(}\DecValTok{2}\NormalTok{, number\_of\_each), }\FunctionTok{rep}\NormalTok{(}\DecValTok{5}\NormalTok{, number\_of\_each), }\FunctionTok{rep}\NormalTok{(}\DecValTok{10}\NormalTok{, number\_of\_each)),}
       \AttributeTok{draw =} \FunctionTok{c}\NormalTok{(}\FunctionTok{rpois}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_each, }\AttributeTok{lambda =} \DecValTok{0}\NormalTok{), }\FunctionTok{rpois}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_each, }\AttributeTok{lambda =} \DecValTok{1}\NormalTok{), }\FunctionTok{rpois}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_each, }\AttributeTok{lambda =} \DecValTok{2}\NormalTok{), }\FunctionTok{rpois}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_each, }\AttributeTok{lambda =} \DecValTok{5}\NormalTok{), }\FunctionTok{rpois}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_each, }\AttributeTok{lambda =} \DecValTok{10}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ draw)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(lambda)) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{41-ijalm_files/figure-latex/unnamed-chunk-29-1.pdf}

For instance, if we look at the number of A+ grades that are awarded in each university course in a given term then for each course we would have a count.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}
\NormalTok{count\_of\_A\_plus }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{( }
    \CommentTok{\# https://stackoverflow.com/questions/1439513/creating{-}a{-}sequential{-}list{-}of{-}letters{-}with{-}r}
    \AttributeTok{department =} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep.int}\NormalTok{(}\StringTok{"1"}\NormalTok{, }\DecValTok{26}\NormalTok{), }\FunctionTok{rep.int}\NormalTok{(}\StringTok{"2"}\NormalTok{, }\DecValTok{26}\NormalTok{)),}
    \AttributeTok{course =} \FunctionTok{c}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{"DEP\_1\_"}\NormalTok{, letters), }\FunctionTok{paste0}\NormalTok{(}\StringTok{"DEP\_2\_"}\NormalTok{, letters)),}
    \AttributeTok{number\_of\_A\_plus =} \FunctionTok{c}\NormalTok{(}\FunctionTok{sample}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{), }
                              \AttributeTok{size =} \DecValTok{26}\NormalTok{,}
                              \AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{),}
                         \FunctionTok{sample}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{50}\NormalTok{), }
                              \AttributeTok{size =} \DecValTok{26}\NormalTok{,}
                              \AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{    )}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{implementation-in-base-1}{%
\subsection{Implementation in base}\label{implementation-in-base-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grades\_model }\OtherTok{\textless{}{-}} 
  \FunctionTok{glm}\NormalTok{(number\_of\_A\_plus }\SpecialCharTok{\textasciitilde{}}\NormalTok{ department, }
    \AttributeTok{data =}\NormalTok{ count\_of\_A\_plus, }
    \AttributeTok{family =} \StringTok{\textquotesingle{}poisson\textquotesingle{}}\NormalTok{)}

\FunctionTok{summary}\NormalTok{(grades\_model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = number\_of\_A\_plus \textasciitilde{} department, family = "poisson", }
\CommentTok{\#\textgreater{}     data = count\_of\_A\_plus)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}6.7386  {-}1.2102  {-}0.2515   1.3292   3.9520  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)  1.44238    0.09535   15.13   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} department2  1.85345    0.10254   18.07   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  }
\CommentTok{\#\textgreater{} 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for poisson family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 816.08  on 51  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 334.57  on 50  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 545.38}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 5}
\end{Highlighting}
\end{Shaded}

\hypertarget{implementation-in-tidymodels-2}{%
\subsection{\texorpdfstring{Implementation in \texttt{tidymodels}}{Implementation in tidymodels}}\label{implementation-in-tidymodels-2}}

We can use \texttt{tidymodels} to run this if we wanted although we first need to install a helper package \texttt{poissonreg}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("poissonreg")}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{count\_of\_A\_plus\_split }\OtherTok{\textless{}{-}}\NormalTok{ rsample}\SpecialCharTok{::}\FunctionTok{initial\_split}\NormalTok{(count\_of\_A\_plus, }\AttributeTok{prop =} \FloatTok{0.80}\NormalTok{)}
\NormalTok{count\_of\_A\_plus\_train }\OtherTok{\textless{}{-}}\NormalTok{ rsample}\SpecialCharTok{::}\FunctionTok{training}\NormalTok{(count\_of\_A\_plus\_split)}
\NormalTok{count\_of\_A\_plus\_test  }\OtherTok{\textless{}{-}}\NormalTok{  rsample}\SpecialCharTok{::}\FunctionTok{testing}\NormalTok{(count\_of\_A\_plus\_split)}

\NormalTok{a\_plus\_model\_tidymodels }\OtherTok{\textless{}{-}}
\NormalTok{  poissonreg}\SpecialCharTok{::}\FunctionTok{poisson\_reg}\NormalTok{(}\AttributeTok{mode =} \StringTok{"regression"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  parsnip}\SpecialCharTok{::}\FunctionTok{set\_engine}\NormalTok{(}\StringTok{"glm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  parsnip}\SpecialCharTok{::}\FunctionTok{fit}\NormalTok{(number\_of\_A\_plus }\SpecialCharTok{\textasciitilde{}}\NormalTok{ department, }
      \AttributeTok{data =}\NormalTok{ count\_of\_A\_plus\_train)}

\NormalTok{a\_plus\_model\_tidymodels}
\CommentTok{\#\textgreater{} parsnip model object}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Fit time:  3ms }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:  stats::glm(formula = number\_of\_A\_plus \textasciitilde{} department, family = stats::poisson, }
\CommentTok{\#\textgreater{}     data = data)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{} (Intercept)  department2  }
\CommentTok{\#\textgreater{}       1.488        1.867  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Degrees of Freedom: 40 Total (i.e. Null);  39 Residual}
\CommentTok{\#\textgreater{} Null Deviance:       618.6 }
\CommentTok{\#\textgreater{} Residual Deviance: 210.1     AIC: 380.4}
\end{Highlighting}
\end{Shaded}

\hypertarget{exercises-and-tutorial-14}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-14}}

\hypertarget{exercises-14}{%
\subsection{Exercises}\label{exercises-14}}

\hypertarget{tutorial-14}{%
\subsection{Tutorial}\label{tutorial-14}}

\hypertarget{causality}{%
\chapter{Causality from observational data}\label{causality}}

\textbf{STATUS: Under construction.}

\textbf{TODO: Replace the arm matching with \url{https://kosukeimai.github.io/MatchIt/index.html}}

\textbf{Required reading}

\begin{itemize}
\tightlist
\item
  Angelucci, Charles, and Julia Cagé, 2019, `Newspapers in times of low advertising revenues', \emph{American Economic Journal: Microeconomics}, vol.~11, no. 3, pp.~319-364, DOI: 10.1257/mic.20170306, available at: \url{https://www.aeaweb.org/articles?id=10.1257/mic.20170306}.
\item
  Better Evaluation, `Regression Discontinuity', \url{https://www.betterevaluation.org/en/evaluation-options/regressiondiscontinuity}
\item
  Dagan, Noa, Noam Barda, Eldad Kepten, Oren Miron, Shay Perchik, Mark A. Katz, Miguel A. Hernán, Marc Lipsitch, Ben Reis, and Ran D. Balicer, 2021, `BNT162b2 mRNA Covid-19 vaccine in a nationwide mass vaccination setting', \emph{New England Journal of Medicine}, 24 February, \url{https://www.nejm.org/doi/full/10.1056/NEJMoa2101765}.
\item
  Eggers, Andrew C., Anthony Fowler, Jens Hainmueller, Andrew B. Hall, and James M. Snyder Jr, 2015, `On the validity of the regression discontinuity design for estimating electoral effects: New evidence from over 40,000 close races', American Journal of Political Science, 59 (1), pp.~259-274
\item
  Gelman, Andrew, 2019, `Another Regression Discontinuity Disaster and what can we learn from it', 25 June, \url{https://statmodeling.stat.columbia.edu/2019/06/25/another-regression-discontinuity-disaster-and-what-can-we-learn-from-it/}.
\item
  Gelman, Andrew, Jennifer Hill and Aki Vehtari, 2020, Regression and Other Stories, Cambridge University Press, Chs 18 - 21.
\item
  Gertler, Paul, Sebastian Martinez, Patrick Premand, Laura Rawlings, and Christel Vermeersch, `Impact Evaluation in Practice', Chapter 5 - 8.
\item
  McElreath, Richard, 2020, Statistical Rethinking, 2nd Edition, CRC Press, Ch 14.
\item
  Meng, Xiao-Li, 2021, `What Are the Values of Data, Data Science, or Data Scientists?', \emph{Harvard Data Science Review}, \url{https://doi.org/10.1162/99608f92.ee717cf7}, \url{https://hdsr.mitpress.mit.edu/pub/bj2dfcwg/release/2}.
\item
  Riederer, Emily, 2021, `Causal design patterns for data analysts', 30 January, \url{https://emilyriederer.netlify.app/post/causal-design-patterns/}
\item
  Sekhon, Jasjeet and Rocio Titiunik, 2016, `Understanding Regression Discontinuity Designs As Observational Studies', Observational Studies 2 (2016) 174-182, \url{http://sekhon.berkeley.edu/papers/SekhonTitiunik2016-OS.pdf}.
\item
  Wong, Jeffrey, and Colin McFarland, 2020, `Computational Causal Inference at Netflix', Netflix Technology Blog, 11 Aug, \url{https://netflixtechblog.com/computational-causal-inference-at-netflix-293591691c62}.
\end{itemize}

\textbf{Required viewing}

\begin{itemize}
\tightlist
\item
  Gelman, Andrew, 2020 `100 Stories of Causal Inference', 4 August, \url{https://www.youtube.com/watch?v=jnI5KI843Lk}.
\item
  King, Gary, 2020, `Research Designs', Lectures on Quantitative Social Science Methods 1, \url{https://youtu.be/SBwPLwVOb7s}.
\item
  Kuriwaki, Shiro, 2020, `Difference-in-Differences Estimation in R (parts 1 and 2)', 18 April, \url{https://vimeo.com/409267138} and \url{https://vimeo.com/409267190}.
\item
  Kuriwaki, Shiro, 2020, `Instrumental variables in R', 11 April, \url{https://vimeo.com/406629459}.
\item
  Kuriwaki, Shiro, 2020, `Regression Discontinuity in R (parts 1 and 2)', 25 March, \url{https://vimeo.com/400826628} and \url{https://vimeo.com/400826660}.
\item
  Oostrom, Tamar, 2021, `Funding of Clinical Trials and Reported Drug Efficacy', 2 March, \url{https://youtu.be/DdnpWS9Km5U}.
\item
  Riederer, Emily, 2021, `Observational Causal Inference', Toronto Data Workshop, 15 February, \url{https://youtu.be/VP3BBZ7poc0}.
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
  Alexander, Monica, Polimis, Kivan, and Zagheni, Emilio, 2019,' The impact of Hurricane Maria on out-migration from Puerto Rico: Evidence from Facebook data', \emph{Population and Development Review}. (Example of using diff-in-diff to measure the effect of Hurricane Maria.)
\item
  Alexander, Rohan, and Zachary Ward, 2018, `Age at arrival and assimilation during the age of mass migration', \emph{The Journal of Economic History}, 78, no. 3, 904-937. (Example where I used differences between brothers to estimate the effect of education.)
\item
  Angrist, Joshua D., and Jörn-Steffen Pischke, 2008, \emph{Mostly harmless econometrics: An empiricist's companion}, Princeton University Press, Chapter 4.
\item
  Angrist, Joshua D., and Jörn-Steffen Pischke, 2008, \emph{Mostly harmless econometrics: An empiricist's companion}, Princeton University Press, Chapter 6.
\item
  Angrist, Joshua D., and Jörn-Steffen Pischke, 2008, \emph{Mostly harmless econometrics: An empiricist's companion}, Princeton University Press, Chapters 3.3.2 and 5.
\item
  Austin, Peter C., 2011, `An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies', \emph{Multivariate Behavioral Research}, vol.~46, no. 3, pp.399-424. (Broad overview of propensity score matching, with a nice discussion of the comparison to randomised controlled trials.)
\item
  Baker, Andrew, 2019, `Difference-in-Differences Methodology', 25 September, \url{https://andrewcbaker.netlify.app/2019/09/25/difference-in-differences-methodology/}.
\item
  Coppock, Alenxader, and Donald P. Green, 2016, `Is Voting Habit Forming? New Evidence from Experiments and Regression Discontinuities', \emph{American Journal of Political Science}, Volume 60, Issue 4, pp.~1044-1062, available at: \url{https://onlinelibrary.wiley.com/doi/abs/10.1111/ajps.12210}. (Has code and data.)
\item
  Cunningham, Scott, `Causal Inference: The Mixtape', Chapter `Instrumental variables', \url{http://www.scunning.com/causalinference_norap.pdf}.
\item
  Cunningham, Scott, `Causal Inference: The Mixtape', chapter `Regression discontinuity', \url{http://www.scunning.com/causalinference_norap.pdf}.
\item
  Cunningham, Scott, \emph{Causal Inference: The Mixtape}, chapters `Matching and subclassifications' and `Differences-in-differences', \url{http://www.scunning.com/causalinference_norap.pdf}. (Very well-written notes on diff-in-diff.)
\item
  Dell, Melissa, Pablo Querubin, 2018, `Nation Building Through Foreign Intervention: Evidence from Discontinuities in Military Strategies', \emph{The Quarterly Journal of Economics}, Volume 133, Issue 2, pp.~701--764, \url{https://doi.org/10.1093/qje/qjx037}.
\item
  Evans, David, 2013, `Regression Discontinuity Porn', \emph{World Bank Blogs}, 16 November, \url{https://blogs.worldbank.org/impactevaluations/regression-discontinuity-porn}.
\item
  Gelman, Andrew, 2019, `Another Regression Discontinuity Disaster and what can we learn from it', \emph{Statistical Modeling, Causal Inference, and Social Science}, 25 June, \url{https://statmodeling.stat.columbia.edu/2019/06/25/another-regression-discontinuity-disaster-and-what-can-we-learn-from-it/}.
\item
  Gelman, Andrew, and Guido Imbens, 2019, ``Why high-order polynomials should not be used in regression discontinuity designs'', \emph{Journal of Business \& Economic Statistics}, 37, pp.~447-456.
\item
  Gelman, Andrew, and Jennifer Hill, 2007, \emph{Data Analysis Using Regression and Muiltilevel/Hierarchical Models}, Chapter 10, pp.~207-215.
\item
  Grogger, Jeffrey, Andreas Steinmayr, Joachim Winter, 2020, `The Wage Penalty of Regional Accents', NBER Working Paper No.~26719.
\item
  Harris, Rich, Mlacki Migliozzi and Niraj Chokshi, `13,000 Missing Flights: The Global Consequences of the Coronavirus', \emph{New York Times}, 21 February 2020. freely available here (if you make an account): \url{https://www.nytimes.com/interactive/2020/02/21/business/coronavirus-airline-travel.html}.
\item
  Imai, Kosuke, 2017, Quantitative Social Science: An Introduction, Princeton University Press, Ch 2.5.
\item
  Imbens, Guido W., and Thomas Lemieux, 2008, `Regression discontinuity designs: A guide to practice', \emph{Journal of Econometrics}, vol.~142, no. 2, pp.~615-635.
\item
  King, Gary, and Richard Nielsen, 2019, `Why Propensity Scores Should Not Be Used for Matching', \emph{Political Analysis}. (Academic paper on the limits of propensity score matching. Propensity score matching was a big thing in the 90s but everyone knew about these weaknesses and so it died off. Lately, there has been a resurgence because of the CS/ML folks using it without thinking so King and Nielsen wrote a nice paper about the flaws. I mean, you can't say you weren't warned.)
\item
  Myllyvirta, Lauri, 2020, `Analysis: Coronavirus has temporarily reduced China's CO2 emissions by a quarter', \emph{Carbon Brief}, 19 February, \url{https://www.carbonbrief.org/analysis-coronavirus-has-temporarily-reduced-chinas-co2-emissions-by-a-quarter}.
\item
  Saeed, Sahar, Erica E. M. Moodie, Erin C. Strumpf, Marina B. Klein, 2019, `Evaluating the impact of health policies: using a difference-in-differences approach', \emph{International Journal of Public Health}, 64, pp.~637--642, \url{https://doi.org/10.1007/s00038-018-1195-2}.
\item
  Taddy, Matt, 2019, \emph{Business Data Science}, Chapter 5, pp.~146-162.
\item
  Tang, John, 2015, `Pollution havens and the trade in toxic chemicals: evidence from U.S. trade flows', \emph{Ecological Economics}, vol.~112, pp.~150-160. (Example of using diff-in-diff to estimate pollution.)
\item
  Travis, D.J., Carleton, A.M. and Lauritsen, R.G., 2004. `Regional variations in US diurnal temperature range for the 11--14 September 2001 aircraft groundings: Evidence of jet contrail influence on climate', Journal of climate, 17(5), pp.1123-1134.
\item
  Travis, David J., Andrew M. Carleton, and Ryan G. Lauritsen. ``Contrails reduce daily temperature range.'' Nature, 418, no. 6898 (2002): 601-601.
\item
  Valencia Caicedo, Felipe. `The mission: Human capital transmission, economic persistence, and culture in South America.' The Quarterly Journal of Economics 134.1 (2019): 507-556. (Data available at: Valencia Caicedo, Felipe, 2018, ``Replication Data for: `The Mission: Human Capital Transmission, Economic Persistence, and Culture in South America'\,'', \url{https://doi.org/10.7910/DVN/ML1155}, Harvard Dataverse, V1.).
\item
  Zinovyeva, Natalia and Maryna Tverdostup, 2019, `Why are women who earn slightly more than their husbands hard to find?', 10 June, \url{https://blogs.lse.ac.uk/businessreview/2019/06/10/why-are-women-who-earn-slightly-more-than-their-husbands-hard-to-find/}.
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\tightlist
\item
  Essential matching methods.
\item
  Weaknesses of matching.
\item
  Difference-in-differences.
\item
  Identifying opportunities for instrumental variables.
\item
  Implementing instrumental variables.
\item
  Challenges to the validity of instrumental variables.
\item
  Reading in foreign data.
\item
  Difference in differences.
\item
  Replicating work.
\item
  Displaying multiple regression results.
\item
  Discussing results.
\item
  Generating simulated data.
\item
  Understanding regression discontinuity and implementing it both manually and using packages.
\item
  Appreciating the threats to the validity of regression discontinuity.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{broom}
\item
  \texttt{tidyverse}
\item
  \texttt{estimatr}
\item
  \texttt{tidyverse}
\item
  \texttt{haven}
\item
  \texttt{huxtable}
\item
  \texttt{scales}
\item
  \texttt{tidyverse}
\item
  \texttt{broom}
\item
  \texttt{rdrobust}
\item
  \texttt{tidyverse}
\end{itemize}

\textbf{Key functions/etc}

\begin{itemize}
\tightlist
\item
  \texttt{tidy()}
\item
  \texttt{lm()}
\item
  \texttt{iv\_robust()}
\item
  \texttt{dollar\_format()}
\item
  \texttt{hux\_reg()}
\item
  \texttt{lm()}
\item
  \texttt{mutate\_at()}
\item
  \texttt{read\_dta()}
\item
  \texttt{lm()}
\item
  \texttt{tidy()}
\item
  \texttt{rdrobust()()}
\end{itemize}

\textbf{Quiz}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sharla Gelfand has been `(s)haring two \#rstats functions most days - one I know and love, and one that's new to me!'. Please go to Sharla's GitHub page: \url{https://github.com/sharlagelfand/twofunctionsmostdays}. Please find a package that she mentions that you have never used. Please find the relevant website for the package. Please describe what the package does and a context in which it could be useful to you.
\item
  Sharla Gelfand has been `(s)haring two \#rstats functions most days - one I know and love, and one that's new to me!'. Please go to Sharla's GitHub page: \url{https://github.com/sharlagelfand/twofunctionsmostdays}. Please find a function that she mentions that you have never used. Please look at the help file for that function. Please detail the arguments of the function, and a context in which it could be useful to you.
\item
  What is propensity score matching? If you were matching people, then what are some of the features that you would like to match on? What sort of ethical questions does collecting and storing such information raise for you?
\item
  Putting to one side, the ethical issues, what are some statistical weaknesses with propensity score matching?
\item
  What is the key assumption when using diff-in-diff?
\item
  Please read the fascinating article in The Markup about car insurance algorithms: \url{https://themarkup.org/allstates-algorithm/2020/02/25/car-insurance-suckers-list}. Please read the article and tell me what you think. You may wish to focus on ethical, legal, social, statistical, or other, aspects.
\item
  Please go to the GitHub page related to the fascinating article in The Markup about car insurance algorithms: \url{https://github.com/the-markup/investigation-allstates-algorithm}. What is great about their work? What could be improved?
\item
  What are the fundamental features of regression discontinuity design?
\item
  What are the conditions that are needed in order for RDD to be able to be used?
\item
  Can you think of a situation in your own life where RDD may be useful?
\item
  What are some threats to the validity of RDD estimates?
\item
  Please look at the \texttt{performance} package: \url{https://easystats.github.io/performance/index.html}. What are some features of this package that may be useful in your own work?
\item
  What do you think about using COVID-19 in an RDD setting? Statistically? Ethically?
\item
  Please read and reproduce the main findings from Eggers, Fowler, Hainmueller, Hall, Snyder, 2015.
\item
  What is an instrumental variable?
\item
  What are some circumstances in which instrumental variables might be useful?
\item
  What conditions must instrumental variables satisfy?
\item
  Who were some of the early instrumental variable authors?
\item
  Can you please think of and explain an application of instrumental variables in your own life?
\item
  What is the key assumption in difference-in-differences

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Parallel trends.
  \item
    Heteroscedasticity.
  \end{enumerate}
\item
  If you're using regression discontinuity, whare are some aspects to be aware of and think really hard about (select all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Is the cut-off free of manipulation?
  \item
    Is the forcing function continuous?
  \item
    To what extent is the functional form driving the estimate?
  \item
    Would different fitted lines affect the results?
  \end{enumerate}
\item
  What is the main reason that \citet{oostrom2021} finds that the outcome of an RCT can depend on who is funding it (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Publication bias
  \item
    Explicit manipulation
  \item
    Specialisation
  \item
    Larger number of arms
  \end{enumerate}
\item
  What is the key coefficient of interest in Angelucci and Cagé, 2019 (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \(\beta_0\)
  \item
    \(\beta_1\)
  \item
    \(\lambda\)
  \item
    \(\gamma\)
  \end{enumerate}
\item
  The instrumental variable is (please pick all that apply):

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Correlated with the treatment variable.
  \item
    Not correlated with the outcome.
  \item
    Heteroskedastic.
  \end{enumerate}
\item
  Who are the two candidates to have invented instrumental variables?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Sewall Wright
  \item
    Philip G. Wright
  \item
    Sewall Cunningham
  \item
    Philip G. Cunningham
  \end{enumerate}
\item
  What are the two main assumptions of instrumental variables?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Exclusion Restriction.
  \item
    Relevance.
  \item
    Ignorability.
  \item
    Randomization.
  \end{enumerate}
\item
  According to Meng, 2021, `Data science can persuade via\ldots{}' (pick all that apply):

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    the careful establishment of evidence from fair-minded and high-quality data collection
  \item
    processing and analysis
  \item
    the honest interpretation and communication of findings
  \item
    large sample sizes
  \end{enumerate}
\item
  According to Reiderer, 2021, if I have `disjoint treated and untreated groups partitioned by a sharp cut-off' then which method should I use to measure the local treatment effect at the juncture between groups (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    regression discontinuity
  \item
    matching
  \item
    difference-in-differences
  \item
    event study methods
  \end{enumerate}
\item
  According to Reiderer, 2021, `Causal inference requires investment in' (pick all that apply):

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    data management
  \item
    domain knowledge
  \item
    probabilistic reasoning
  \item
    data science
  \end{enumerate}
\item
  I am an Australian 30-39 year old male living in Toronto with one child and a PhD. Which of the following do you think I would match most closely with and why (please explain in a paragraph or two)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    An Australian 30-39 year old male living in Toronto with one child and a bachelors degree
  \item
    A Canadian 30-39 year old male living in Toronto with one child and a PhD
  \item
    An Australian 30-39 year old male living in Ottawa with one child and a PhD
  \item
    A Canadian 18-29 year old male living in Toronto with one child and a PhD
  \end{enumerate}
\item
  In your most disdainful tone (jokes, I love DAGs), what is a DAG (in your own words please)?
\item
  What is a confounder (please select one answer)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A variable, z, that causes both x and y, where x also causes y.
  \item
    A variable, z, that is caused by both x and y, where x also causes y.
  \item
    A variable, z, that causes y and is caused by x, where x also causes y.
  \end{enumerate}
\item
  What is a mediator (please select one answer)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A variable, z, that causes y and is caused by x, where x also causes y.
  \item
    A variable, z, that causes both x and y, where x also causes y.
  \item
    A variable, z, that is caused by both x and y, where x also causes y.
  \end{enumerate}
\item
  What is a collider (please select one answer)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A variable, z, that causes both x and y, where x also causes y.
  \item
    A variable, z, that causes y and is caused by x, where x also causes y.
  \item
    A variable, z, that is caused by both x and y, where x also causes y.
  \end{enumerate}
\item
  Please talk through a brief example of when you may want to be very careful about checking for Simpson's paradox.
\item
  Please talk through a brief example of when you may want to be very careful about checking for Berkson's paradox.
\item
  According to McElreath (2020, 162) `Regression will not sort it out. Regression is indeed an oracle, but a cruel one. It speaks in riddles and delights in punishing us for\ldots{}' (please select one answer)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    overcomplicating models.
  \item
    asking bad questions.
  \item
    using bad data.
  \end{enumerate}
\item
  Is a model that fits the small or large world more important to you, and why?
\item
  In \citet{NoiseKahneman} the authors, including the Nobel Prize winner Daniel Kahneman, say `\ldots{} while correlation does not imply causation, causation does imply correlation. Where there is a causal link, we should find a correlation'. With reference to \citet[Chapter 1]{Cunningham2021}, are they right or wrong, and why?
\end{enumerate}

\hypertarget{introduction-19}{%
\section{Introduction}\label{introduction-19}}

Life it grand when you can conduct experiments to be able to speak to causality. But what if you can only run the survey - you can't run an experiment? Here we begin our discussion of the circumstances and methods that would allow you to nonetheless speak to causality. We use (relatively) simple methods, in sophisticated, well-developed, ways (cf, much of what is done these days) and our applied statistics draw from a variety of social sciences including economics, and political science.

Following the publication of \citet{dagan2021bnt162b2}, one of the authors tweeted (slight edits for formatting):

\begin{quote}
We've just confirmed the effectiveness of the Pfizer-BioNTech vaccine outside of randomized trials. Yes, great news, but let's talk about methodological issues that arise when using observational data to estimate vaccine effectiveness.

A critical concern in observational studies of vaccine effectiveness is confounding: Suppose that people who get vaccinated have, on average, a lower risk of infection/disease than those who don't get vaccinated. Then, even if the vaccine were useless, it'd look beneficial.

To adjust for confounding: We start by identifying potential confounders. For example: Age (vaccination campaigns prioritize older people and older people are more likely to develop severe disease). Then we choose a valid adjustment method. In our paper, we matched on age. After age adjustment, how do we know if there is residual confounding? Here is one way to go about that: We know from the previous randomized trial that the vaccine has no effect in the first few days. So we check whether matching on age suffices to replicate that finding. No, it doesn't. After matching on age (and sex), the curves of infection start to diverge from day 0, which indicates that the vaccinated had a lower risk of infection than the unvaccinated. Conclusion: adjustment for age and sex is insufficient.

We learned that we had to match on other COVID-19 risk factors, e.g., location, comorbidities, healthcare use\ldots{} And we could do so with high-quality data from the Clalit Research Institute, part of a health services organization that covers \textgreater50\% of the Israeli population. As an example, a vaccinated 76 year-old Arab male from a specific neighborhood who received 4 influenza vaccines in the last 5 years and had 2 comorbidities was matched with an unvaccinated Arab male from the same neighborhood, aged 76-77, with 3-4 influenza vaccines and 2 comorbidities. After matching on all those risk factors, the curves of infection start to diverge after day \textasciitilde12, as expected if the vaccinated and the unvaccinated had a comparable risk of infection. Using this ``negative control'', we provide evidence against large residual confounding.

This is a good illustration of how randomized trials and observational studies complement each other for better and more efficient \#causalinference. First, a randomized trial is conducted to estimate the effectiveness of the vaccine to prevent symptomatic infection, but\ldots{} the trial's estimates for severe disease and specific age groups are imprecise. Second, an observational analysis emulates a \#targettrial (an order of magnitude greater) and confirms the vaccine's effectiveness on severe disease and in different age groups. However\ldots{} the observational study needs the trial's findings as a benchmark to guide the data analysis and strengthen the quality of the causal inference.

Randomized trials \& Observational studies working together. The best of both worlds. Let's keep doing it after the pandemic. What a luxury having been able to think about these issues with my colleagues Noa Dagan, Noam Barda, Marc Lipsitch, Ben Reis, and Ran D. Balicer. We hope that our experience is helpful for researchers around the world who use observational data to estimate vaccine effectiveness.

Miguel Hernán, \href{https://twitter.com/_miguelhernan/status/1364700315044438023?s=21}{24 Februrary 2021}.
\end{quote}

This is what this chapter is about. How we can nonetheless be comfortable making causal statements, even when we can't run A/B tests or RCTs. Indeed, in what circumstances may we actually prefer to not run those or to run observational-based approaches in addition to them. We cover three of the major methods that are in popular use these days: difference-in-differences; regression discontinuity; and instrumental variables.

\hypertarget{dags-and-trying-not-to-be-tricked-by-the-data}{%
\section{DAGs and trying not to be tricked by the data}\label{dags-and-trying-not-to-be-tricked-by-the-data}}

\hypertarget{dags-and-confounding}{%
\subsection{DAGs and confounding}\label{dags-and-confounding}}

When we are discussing causality it can help to be very specific about what we mean. It's easy to get caught up in the data that it tricks you. It's important to think really hard. One framework to help with this that has become popular recently is the use of directed acyclic graph (DAG), which is essentially just a fancy name for a flow diagram. A DAG involves drawing arrows between your variables indicating the relationship between them. We will use the \texttt{DiagrammeR} package to draw them \citep{citeDiagrammeR}, because that provides quite a lot of control (Figure \ref{fig:firstdag}). However it can be a little finicky and if you're just looking to do something really quickly then the \texttt{ggdag} package can be useful \citep{citeggdag}. The code to draw these DAGs draws heavily on \citet{citeerik}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(DiagrammeR)}
\NormalTok{DiagrammeR}\SpecialCharTok{::}\FunctionTok{grViz}\NormalTok{(}\StringTok{"}
\StringTok{digraph \{}
\StringTok{  graph [ranksep = 0.2]}
\StringTok{  node [shape = plaintext]}
\StringTok{    x}
\StringTok{    y}
\StringTok{  edge [minlen = 2, arrowhead = vee]}
\StringTok{    x{-}\textgreater{}y}
\StringTok{  \{ rank = same; x; y \}}
\StringTok{\}}
\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\includegraphics[width=0.75\linewidth]{42-causality_from_obs_files/figure-latex/firstdag-1} \caption{Using a DAG to illustrate perceived relationships}\label{fig:firstdag}
\end{figure}

In this example, we claim that \(x\) causes \(y\). We could build another where the situation is less clear. I find all the \(x\) and \(y\) very confusing, so will change to fruits (Figure \ref{fig:carrotasconfounder}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DiagrammeR}\SpecialCharTok{::}\FunctionTok{grViz}\NormalTok{(}\StringTok{"}
\StringTok{digraph \{}
\StringTok{  graph [ranksep = 0.2]}
\StringTok{  node [shape = plaintext]}
\StringTok{    Apple}
\StringTok{    Banana}
\StringTok{    Carrot}
\StringTok{  edge [minlen = 2, arrowhead = vee]}
\StringTok{    Apple{-}\textgreater{}Banana}
\StringTok{    Carrot{-}\textgreater{}Apple}
\StringTok{    Carrot{-}\textgreater{}Banana}
\StringTok{  \{ rank = same; Apple; Banana \}}
\StringTok{\}}
\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\includegraphics[width=0.75\linewidth]{42-causality_from_obs_files/figure-latex/carrotasconfounder-1} \caption{Carrot as a confounder}\label{fig:carrotasconfounder}
\end{figure}

In this case we again think \(apple\) causes \(banana\). But it's also clear that \(carrot\) causes \(banana\), and \(carrot\) also causes \(apple\). That relationship is a `backdoor path', and would create spurious correlation in our analysis. We may think that changes in \(apple\) are causing changes in \(banana\), but it's actually that \(carrot\) is changing them both and hence that variable is called a `confounder'.

There is an excellent discussion by \citet[p.~83]{hernanrobins2020}:

\begin{quote}
Suppose an investigator conducted an observational study to answer the causal question ``does one's looking up to the sky make other pedestrians look up too?'' She found an association between a first pedestrian's looking up and a second one's looking up. However, she also found that pedestrians tend to look up when they hear a thunderous noise above. Thus it was unclear what was making the second pedestrian look up, the first pedestrian's looking up or the thunderous noise? She concluded the effect of one's looking up was confounded by the presence of a thunderous noise.

In randomized experiments treatment is assigned by the flip of a coin, but in observational studies treatment (e.g., a person's looking up) may be determined by many factors (e.g., a thunderous noise). If those factors affect the risk of developing the outcome (e.g., another person's looking up), then the effects of those factors become entangled with the effect of treatment. We then say that there is confounding, which is just a form of lack of exchangeability between the treated and the untreated. Confounding is often viewed as the main shortcoming of observational studies. In the presence of confounding, the old adage ``association is not causation'' holds even if the study population is arbitrarily large.
\end{quote}

If we were interested in causal effects we would need to adjust for \(carrot\), or \(thunder\) and one way is to include it in the regression. However, the validity of this requires a number of assumptions. In particular, \citet[p.~169]{gelmanandhill} warns us our estimate will only correspond to the average causal effect in the sample if: 1) we include `all confounding covariates'; and 2) `the model is correct'. Putting to one side the second requirement, and focusing only on the first, if we don't observe a confounder, then we can't adjust for it. This is where the role of domain experts, experience, and theory, really add a lot to an analysis.

We might have a similar situation, again we think that \(apple\) causes \(banana\), but this time \(apple\) also causes \(carrot\), which itself causes \(banana\) (Figure \ref{fig:carrotasmediator}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DiagrammeR}\SpecialCharTok{::}\FunctionTok{grViz}\NormalTok{(}\StringTok{"}
\StringTok{digraph \{}
\StringTok{  graph [ranksep = 0.2]}
\StringTok{  node [shape = plaintext]}
\StringTok{    Apple}
\StringTok{    Banana}
\StringTok{    Carrot}
\StringTok{  edge [minlen = 2, arrowhead = vee]}
\StringTok{    Apple{-}\textgreater{}Banana}
\StringTok{    Apple{-}\textgreater{}Carrot}
\StringTok{    Carrot{-}\textgreater{}Banana}
\StringTok{  \{ rank = same; Apple; Banana \}}
\StringTok{\}}
\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\includegraphics[width=0.75\linewidth]{42-causality_from_obs_files/figure-latex/carrotasmediator-1} \caption{Carrot as a mediator}\label{fig:carrotasmediator}
\end{figure}

In this case, \(carrot\) is called a `mediator' and we'd not like to adjust for it, because that would affect our estimate of the effect of \(apple\) on \(banana\).

Finally, we might have yet another similar situation, where we again think that \(apple\) causes \(banana\), but this time both \(apple\) and \(banana\) cause \(carrot\) (Figure \ref{fig:carrotascollider}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DiagrammeR}\SpecialCharTok{::}\FunctionTok{grViz}\NormalTok{(}\StringTok{"}
\StringTok{digraph \{}
\StringTok{  graph [ranksep = 0.2]}
\StringTok{  node [shape = plaintext]}
\StringTok{    Apple}
\StringTok{    Banana}
\StringTok{    Carrot}
\StringTok{  edge [minlen = 2, arrowhead = vee]}
\StringTok{    Apple{-}\textgreater{}Banana}
\StringTok{    Apple{-}\textgreater{}Carrot}
\StringTok{    Banana{-}\textgreater{}Carrot}
\StringTok{  \{ rank = same; Apple; Banana \}}
\StringTok{\}}
\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\includegraphics[width=0.75\linewidth]{42-causality_from_obs_files/figure-latex/carrotascollider-1} \caption{Carrot as a collider}\label{fig:carrotascollider}
\end{figure}

In this case, \(carrot\) is called a `collider' and if we were to condition on it we would create a misleading relationship.

I've been circling around this point for a while, but it's time to address it. You will create your DAG - there is nothing that will create it for you. That means that you need to think really carefully about the situation. Because it's one thing to see something in the DAG and then do something about it. But it's another to not know that it's there. \citet[p.~180]{citemcelreath} describes these as haunted DAGs.

DAGs have become fashionable. They are of course helpful, but they are just a tool to help you think really deeply about your situation. As \citet[p.~162]{citemcelreath} says `Regression will not sort it out. Regression is indeed an oracle, but a cruel one. It speaks in riddles and delights in punishing us for asking bad questions.'. The same is true of DAGs, and all the methods that we cover in these notes.

\hypertarget{selection-and-measurement-bias}{%
\subsection{Selection and measurement bias}\label{selection-and-measurement-bias}}

Selection bias occurs when the outcomes are dependent on `the process by which individuals are selected into the analysis' \citep[p.~99]{hernanrobins2020}. We are not going to be able to see this from a DAG (I mean one could draw a DAG that shows it, but you need to know about it in order to draw that DAG is what I mean), or many default diagnostics. One way to go about things is A/A testing in an experimental settings, or comparing the sample with some more general characteristics, for instance age-group, gender, and education. But the fundamental point, as Dr Jill Sheppard says, is that `people who respond to surveys are weird', and this generalises to whatever method you're using to gather your data. Using Facebook ads? People who click on Facebook ads are weird. Going door knocking? People who answer their door are weird. Call people on the phone? Literally who answers their phone anymore.

There is a pernicious aspect of selection bias, which is that it pervades every aspect of your analysis. Even a sample that starts off as perfectly representative, may become selected over time. For instance, the survey panels used for political polling need to be updated from time to time because the folks who don't get anything out of it stop responding - but those people may still vote and so need to be polled.

Another bias to be very aware of is measurement bias, which when `the association between treatment and outcome is weakened or strengthened as a result of the process by which the study data are measured' \citep[p.~113]{hernanrobins2020}. It is implicit in the definition by \citet{hernanrobins2020}, but it is important here that this be systematic. For instance, if I ask people in person what their income is then that is likely to get different answers than if I ask over the phone or via an online form.

\hypertarget{two-common-paradoxes}{%
\subsection{Two common paradoxes}\label{two-common-paradoxes}}

There are two situations where data can trick you that are so common that I'd like to explicitly go through them. These are Simpson's paradox, and Berkson's paradox. Keep these situations in the back of your mind at all times when dealing with data.

Simpson's paradox occurs when we estimate some relationship for subsets of our data, but a different relationship when we consider the entire dataset \citep{simpson1951interpretation}. For instance, it may be that there is a positive relationship between undergraduate grades and performance in graduate school in both statistics and economics when considering each department individually. But if undergraduate grades tended to be higher in statistics than economics while graduate school performance tended to be opposite, we may actually find a negative relationship between undergraduate grades and performance in graduate school.

To see this let's simulate some data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}
\NormalTok{number\_in\_each }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{statistics }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{undergrad =} \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_in\_each, }\AttributeTok{min =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{max =} \FloatTok{0.9}\NormalTok{),}
                     \AttributeTok{noise =} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_in\_each, }\DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.1}\NormalTok{),}
                     \AttributeTok{grad =}\NormalTok{ undergrad }\SpecialCharTok{+}\NormalTok{ noise,}
                     \AttributeTok{type =} \StringTok{"Statistics"}\NormalTok{)}
\NormalTok{economics }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{undergrad =} \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_in\_each, }\AttributeTok{min =} \FloatTok{0.6}\NormalTok{, }\AttributeTok{max =} \FloatTok{0.8}\NormalTok{),}
                    \AttributeTok{noise =} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_in\_each, }\DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.1}\NormalTok{),}
                    \AttributeTok{grad =}\NormalTok{ undergrad }\SpecialCharTok{+}\NormalTok{ noise }\SpecialCharTok{+} \FloatTok{0.3}\NormalTok{,}
                    \AttributeTok{type =} \StringTok{"Economics"}\NormalTok{)}
\NormalTok{both }\OtherTok{=} \FunctionTok{rbind}\NormalTok{(statistics, economics)}

\NormalTok{both }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ undergrad, }\AttributeTok{y =}\NormalTok{ grad)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ type), }\AttributeTok{alpha =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ type), }\AttributeTok{method =} \StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{, }\AttributeTok{formula =} \StringTok{\textquotesingle{}y \textasciitilde{} x\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{, }\AttributeTok{formula =} \StringTok{\textquotesingle{}y \textasciitilde{} x\textquotesingle{}}\NormalTok{, }\AttributeTok{color =} \StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Undergraduate results"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Graduate results"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Type"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{42-causality_from_obs_files/figure-latex/unnamed-chunk-1-1.pdf}

Berkson's paradox occurs when we estimate some relationship based on the dataset that we have, but because the dataset is selected the relationship is different in a more general dataset \citep{citeberkson}. For instance, if we have a dataset of professional cyclists then we would find there is no relationship between their VO2 max and their chance of winning a bike race. But if we had a dataset of the general population then we would find an enormous relationship between their VO2 max and their chance of winning a bike race. The professional dataset has just been so selected that the relationship disappears - you can't become a professional unless you have a high VO2 max.

To see this let's simulate some data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}
\NormalTok{number\_of\_pros }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{number\_of\_public }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{professionals }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{VO2 =} \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_pros, }\AttributeTok{min =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{max =} \FloatTok{0.9}\NormalTok{),}
         \AttributeTok{chance\_of\_winning =} \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_pros, }\AttributeTok{min =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{max =} \FloatTok{0.9}\NormalTok{),}
         \AttributeTok{type =} \StringTok{"Professionals"}\NormalTok{)}
\NormalTok{general\_public }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{VO2 =} \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_public, }\AttributeTok{min =} \FloatTok{0.6}\NormalTok{, }\AttributeTok{max =} \FloatTok{0.8}\NormalTok{),}
         \AttributeTok{noise =} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_public, }\DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.03}\NormalTok{),}
         \AttributeTok{chance\_of\_winning =}\NormalTok{ VO2 }\SpecialCharTok{+}\NormalTok{ noise }\SpecialCharTok{+} \FloatTok{0.1}\NormalTok{,}
         \AttributeTok{type =} \StringTok{"Public"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{noise)}
\NormalTok{both }\OtherTok{=} \FunctionTok{rbind}\NormalTok{(professionals, general\_public)}

\NormalTok{both }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ VO2, }\AttributeTok{y =}\NormalTok{ chance\_of\_winning)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ type), }\AttributeTok{alpha =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ type), }\AttributeTok{method =} \StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{, }\AttributeTok{formula =} \StringTok{\textquotesingle{}y \textasciitilde{} x\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{, }\AttributeTok{formula =} \StringTok{\textquotesingle{}y \textasciitilde{} x\textquotesingle{}}\NormalTok{, }\AttributeTok{color =} \StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"VO2 max"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Chance of winning a bike race"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Type"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{42-causality_from_obs_files/figure-latex/unnamed-chunk-2-1.pdf}

\hypertarget{difference-in-differences}{%
\section{Difference in differences}\label{difference-in-differences}}

\hypertarget{matching-and-difference-in-differences}{%
\subsection{Matching and difference-in-differences}\label{matching-and-difference-in-differences}}

\hypertarget{introduction-20}{%
\subsubsection{Introduction}\label{introduction-20}}

The ideal situation of being able to conduct an experiment is rarely possible in a data science setting. Can we really reasonably expect that Netflix would allow us to change prices. And even if they did once, would they let us do it again, and again, and again? Further, rarely can we explicitly create treatment and control groups. Finally, experiments are really expensive and potentially unethical. Instead, we need to make do with what we have. Rather than our counterfactual coming to us through randomisation, and hence us knowing that the two are the same but for the treatment, we try to identify groups that were similar before the treatment, and hence any differences can be attributed to the treatment. In practice, we tend to even have differences between our two groups before we treat. Provided those pre-treatment differences satisfy some assumptions (basically that they were consistent, and we expect that consistency to continue in the absence of the treatment) -- the `parallel trends' assumption -- then we can look to any difference in the differences as the effect of the treatment. One of the lovely aspects of difference in differences analysis is that we can do it using fairly straight-forward quantitative methods - linear regression with a dummy variable is all that is needed to do a convincing job.

\hypertarget{motivation}{%
\subsubsection{Motivation}\label{motivation}}

Consider us wanting to know the effect of a new tennis racket on serve speed. One way to test this would be to measure the difference between Roger Federer's serve speed without the tennis racket and mine with the tennis racket. Sure, we'd find a difference but how do we know how much to attribute to the tennis racket? Another way would be to consider the difference between my serve speed without the tennis racket and my serve speed with the tennis racket. But what if serves were just getting faster naturally over time? Instead, let's combine the two to look at the difference in the differences!

In this world we measure Federer's serve and compare it to my serve without the new racket. We then measure Federer's serve again and measure my serve with the new racket. That difference in the differences would then be our estimate of the effect of the new racket.

What sorts of assumptions jump out at you that we are going to have to make in order for this analysis to be appropriate?

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Is there something else that may have affected only me, and not Roger that could affect my serve speed? Probably.
\item
  Is it likely that Roger Federer and I have the same trajectory of serve speed improvement? Probably not. This is the `parallel trends' assumption, and it dominates any discussion of difference in differences analysis. Finally, is it likely that the variance of our serve speeds is the same? Probably not.
\end{enumerate}

Why might this be powerful? We don't need the treatment and control group to be the same before the treatment. We just need to have a good idea of how they differ.

\hypertarget{simulated-example}{%
\subsubsection{Simulated example}\label{simulated-example}}

Let's generate some data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(broom)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{diff\_in\_diff\_example\_data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{person =} \FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{1000}\NormalTok{), }\AttributeTok{times =} \DecValTok{2}\NormalTok{),}
                       \AttributeTok{time =} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{times =} \DecValTok{1000}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{times =} \DecValTok{1000}\NormalTok{)),}
                       \AttributeTok{treatment\_group =} \FunctionTok{rep}\NormalTok{(}\FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{1}\NormalTok{, }\AttributeTok{size  =} \DecValTok{1000}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{), }\AttributeTok{times =} \DecValTok{2}\NormalTok{)}
\NormalTok{                       )}
\DocumentationTok{\#\# We want to make the outcome slightly more likely if they were treated than if not.}
\NormalTok{diff\_in\_diff\_example\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  diff\_in\_diff\_example\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rowwise}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{serve\_speed =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{    time }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&}\NormalTok{ treatment\_group }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\textasciitilde{}} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{mean =} \DecValTok{5}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
\NormalTok{    time }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ treatment\_group }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\textasciitilde{}} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{mean =} \DecValTok{6}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
\NormalTok{    time }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&}\NormalTok{ treatment\_group }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{mean =} \DecValTok{8}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
\NormalTok{    time }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ treatment\_group }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{mean =} \DecValTok{14}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
\NormalTok{    )}
\NormalTok{    )}

\FunctionTok{head}\NormalTok{(diff\_in\_diff\_example\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 4}
\CommentTok{\#\textgreater{} \# Rowwise: }
\CommentTok{\#\textgreater{}   person  time treatment\_group serve\_speed}
\CommentTok{\#\textgreater{}    \textless{}int\textgreater{} \textless{}dbl\textgreater{}           \textless{}int\textgreater{}       \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1      1     0               0        4.43}
\CommentTok{\#\textgreater{} 2      2     0               1        6.96}
\CommentTok{\#\textgreater{} 3      3     0               1        7.77}
\CommentTok{\#\textgreater{} 4      4     0               0        5.31}
\CommentTok{\#\textgreater{} 5      5     0               0        4.09}
\CommentTok{\#\textgreater{} 6      6     0               0        4.85}
\end{Highlighting}
\end{Shaded}

Let's make a graph.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diff\_in\_diff\_example\_data}\SpecialCharTok{$}\NormalTok{treatment\_group }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(diff\_in\_diff\_example\_data}\SpecialCharTok{$}\NormalTok{treatment\_group)}
\NormalTok{diff\_in\_diff\_example\_data}\SpecialCharTok{$}\NormalTok{time }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(diff\_in\_diff\_example\_data}\SpecialCharTok{$}\NormalTok{time)}

\NormalTok{diff\_in\_diff\_example\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ time,}
             \AttributeTok{y =}\NormalTok{ serve\_speed,}
             \AttributeTok{color =}\NormalTok{ treatment\_group)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{group =}\NormalTok{ person), }\AttributeTok{alpha =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Time period"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Serve speed"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Person got a new racket"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{42-causality_from_obs_files/figure-latex/unnamed-chunk-4-1.pdf}

As it is a simple example, we could do this manually, by getting the average difference of the differences.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{average\_differences }\OtherTok{\textless{}{-}} 
\NormalTok{  diff\_in\_diff\_example\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ time,}
              \AttributeTok{values\_from =}\NormalTok{ serve\_speed,}
              \AttributeTok{names\_prefix =} \StringTok{"time\_"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{difference =}\NormalTok{ time\_1 }\SpecialCharTok{{-}}\NormalTok{ time\_0) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(treatment\_group) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{average\_difference =} \FunctionTok{mean}\NormalTok{(difference))}

\NormalTok{average\_differences}\SpecialCharTok{$}\NormalTok{average\_difference[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ average\_differences}\SpecialCharTok{$}\NormalTok{average\_difference[}\DecValTok{1}\NormalTok{]}
\CommentTok{\#\textgreater{} [1] 5.058414}
\end{Highlighting}
\end{Shaded}

Let's use OLS to do the same analysis. The general regression equation is:
\[Y_{i,t} = \beta_0 + \beta_1\mbox{Treatment group dummy}_i + \beta_2\mbox{Time dummy}_t + \beta_3(\mbox{Treatment group dummy} \times\mbox{Time dummy})_{i,t} + \epsilon_{i,t}\]

If we use \texttt{*} in the regression then it automatically includes the separate aspects as well as their interaction. It's the estimate of \(\beta_3\) which is of interest.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diff\_in\_diff\_example\_regression }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(serve\_speed }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treatment\_group}\SpecialCharTok{*}\NormalTok{time, }
                         \AttributeTok{data =}\NormalTok{ diff\_in\_diff\_example\_data)}

\FunctionTok{tidy}\NormalTok{(diff\_in\_diff\_example\_regression)}
\CommentTok{\#\textgreater{} \# A tibble: 4 x 5}
\CommentTok{\#\textgreater{}   term                 estimate std.error statistic  p.value}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}                   \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 (Intercept)              4.97    0.0428     116.  0       }
\CommentTok{\#\textgreater{} 2 treatment\_group1         3.03    0.0622      48.7 0       }
\CommentTok{\#\textgreater{} 3 time1                    1.01    0.0605      16.6 2.97e{-}58}
\CommentTok{\#\textgreater{} 4 treatment\_group1:ti\textasciitilde{}     5.06    0.0880      57.5 0}
\end{Highlighting}
\end{Shaded}

Fortunately, our estimates are the same!

\hypertarget{assumptions}{%
\subsubsection{Assumptions}\label{assumptions}}

If we want to use difference in differences, then we need to satisfy the assumptions. There were three that were touched on earlier, but here I want to focus on one: the `parallel trends' assumption. The parallel trends assumption haunts everything to do with diff-in-diff analysis because we can never prove it, we can just be convinced of it.

To see why we can never prove it, consider an example in which we want to know the effect of a new stadium on a professional sports team's wins/loses. To do this we consider two teams: the Warriors and the Raptors. The Warriors changed stadiums at the start of the 2019-20 season (the Raptors did not), so we will consider four time periods: the 2016-17 season, 2017-18 season, 2018-19 season, and finally we will compare the performance with the one after they moved, so in the 2019-20 season. The Raptors here act as our counterfactual. This means that we assume the relationship between the Warriors and the Raptors, in the absence of a new stadium, would have continued to change in a consistent way. But we can never know that for certain. We have to present sufficient evidence to assuage any concerns that a reader may have.

For a variety of reasons, it is worth having tougher than normal requirements around the evidence it would take to convince you of an effect.

There are four main `threats to validity' when you are using difference in differences and you should address all of these (Cunningham, 2020, pp.~272--277):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Non-parallel trends. The treatment and control groups may be based on differences. As such it can be difficult to convincingly argue for parallel trends. In this case, maybe try to find another factor to consider in your model that may adjust for some of that. This may require difference in difference in differences (in the earlier example, perhaps could add in the San Francisco 49ers as they are in the same broad geographic area as the Warriors). Or maybe re-think your analysis to see if you can make a different control group. Adding additional earlier time periods may help but may introduce more issues (see third point).
\item
  Compositional differences. This is a concern when working with repeated cross-sections. What if the composition of those cross-sections change? For instance, if we work at Tik Tok or some other app that is rapidly growing and want to look at the effect of some change. In our initial cross-section, we may have mostly young people, but in a subsequent cross-section, we may have more older people as the demographics of the app usage change. Hence our results may just be an age-effect, not an effect of the change that we are interested in.
\item
  Long-term effects vs.~reliability. As we discussed in the last chapter, there is a trade-off between the length of the analysis that we run. As we run the analysis for longer there is more opportunity for other factors to affect the results. There is also increased chance for someone who was not treated to be treated. But, on the other hand, it can be difficult to convincingly argue that short-term results will continue in the long-term.
\item
  Functional form dependence. This is less of an issue when the outcomes are similar, but if they are different then functional form may be responsible for some aspects of the results.
\end{enumerate}

\hypertarget{matching}{%
\subsubsection{Matching}\label{matching}}

\emph{This section draws on material from Gelman and Hill, 2007, pp.~207-212.}

Difference in differences is a powerful analysis framework. After I learnt about it I began to see opportunities to implement it everywhere. But it can be tough to identify appropriate treatment and control groups. In Alexander and Ward, 2018, we compare migrant brothers - one of whom had most of their education in a different country, and the other who had most of their education in the US. Is this really the best match?

We may be able to match based on observable variables. For instance, age-group or education. At two different times we compare smoking rates in 18-year-olds in one city with smoking rates in 18-year-olds in another city. That is fine, but it is fairly coarse. We know that there are differences between 18-year-olds, even in terms of the variables that we commonly observe, say gender and education. One way to deal with this may be to create sub-groups: 18-year-old males with a high school education, etc. But the sample sizes are likely to quickly become small. How do we deal with continuous variables? And also, is the difference between an 18-year-old and a 19-year-old really so different? Shouldn't we also compare with them?

One way to proceed is to consider a nearest neighbour approach. But there is limited concern for uncertainty in this approach. There is also an issue if you have a large number of variables because you end up with a high-dimension graph. This leads us to propensity score matching.

Propensity score matching involves assigning some probability to each observation. We construct that probability based on the observation's values for the independent variables, at their values before the treatment. That probability is our best guess at the probability of the observation being treated, regardless of whether it was treated or not. For instance, if 18-year-old males were treated but 19-year-old males were not, then as there is not much difference between 18-year-old males and 19-year-old males our assigned probability would be fairly similar. We can then compare the outcomes of observations with similar propensity scores.

One advantage of propensity score matching is that is allows us to easily consider many independent variables at once, and it can be constructed using logistic regression.

Let's generate some data to illustrate propensity score matching. Let's pretend that we work for Amazon. We are going to treat some individuals with free-shipping to see what happens to their average purchase.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_size }\OtherTok{\textless{}{-}} \DecValTok{10000}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{amazon\_purchase\_data }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{unique\_person\_id =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{sample\_size),}
    \AttributeTok{age =} \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =}\NormalTok{ sample\_size,}
                \AttributeTok{min =} \DecValTok{18}\NormalTok{,}
                \AttributeTok{max =} \DecValTok{100}\NormalTok{),}
    \AttributeTok{city =} \FunctionTok{sample}\NormalTok{(}
      \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\StringTok{"Toronto"}\NormalTok{, }\StringTok{"Montreal"}\NormalTok{, }\StringTok{"Calgary"}\NormalTok{),}
      \AttributeTok{size =}\NormalTok{ sample\_size,}
      \AttributeTok{replace =} \ConstantTok{TRUE}
\NormalTok{      ),}
    \AttributeTok{gender =} \FunctionTok{sample}\NormalTok{(}
      \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\StringTok{"Female"}\NormalTok{, }\StringTok{"Male"}\NormalTok{, }\StringTok{"Other/decline"}\NormalTok{),}
      \AttributeTok{size =}\NormalTok{ sample\_size,}
      \AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{,}
      \AttributeTok{prob =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.49}\NormalTok{, }\FloatTok{0.47}\NormalTok{, }\FloatTok{0.02}\NormalTok{)}
\NormalTok{      ),}
    \AttributeTok{income =} \FunctionTok{rlnorm}\NormalTok{(}\AttributeTok{n =}\NormalTok{ sample\_size,}
                    \AttributeTok{meanlog =} \FloatTok{0.5}\NormalTok{, }
                    \AttributeTok{sdlog =} \DecValTok{1}\NormalTok{)}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

Now we need to add some probability of being treated with free shipping, which depends on our variables. Younger, higher-income, male and in Toronto all make it slightly more likely.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{amazon\_purchase\_data }\OtherTok{\textless{}{-}}
\NormalTok{  amazon\_purchase\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{age\_num =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{           age }\SpecialCharTok{\textless{}} \DecValTok{30} \SpecialCharTok{\textasciitilde{}} \DecValTok{3}\NormalTok{,}
\NormalTok{           age }\SpecialCharTok{\textless{}} \DecValTok{50} \SpecialCharTok{\textasciitilde{}} \DecValTok{2}\NormalTok{,}
\NormalTok{           age }\SpecialCharTok{\textless{}} \DecValTok{70} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{,}
           \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \DecValTok{0}\NormalTok{),}
         \AttributeTok{city\_num =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{           city }\SpecialCharTok{==} \StringTok{"Toronto"} \SpecialCharTok{\textasciitilde{}} \DecValTok{3}\NormalTok{,}
\NormalTok{           city }\SpecialCharTok{==} \StringTok{"Montreal"} \SpecialCharTok{\textasciitilde{}} \DecValTok{2}\NormalTok{,}
\NormalTok{           city }\SpecialCharTok{==} \StringTok{"Calgary"} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{,}
           \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \DecValTok{0}\NormalTok{),}
         \AttributeTok{gender\_num =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{           gender }\SpecialCharTok{==} \StringTok{"Male"} \SpecialCharTok{\textasciitilde{}} \DecValTok{3}\NormalTok{,}
\NormalTok{           gender }\SpecialCharTok{==} \StringTok{"Female"} \SpecialCharTok{\textasciitilde{}} \DecValTok{2}\NormalTok{,}
\NormalTok{           gender }\SpecialCharTok{==} \StringTok{"Other/decline"} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{,}
           \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \DecValTok{0}\NormalTok{),}
         \AttributeTok{income\_num =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{           income }\SpecialCharTok{\textgreater{}} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \DecValTok{3}\NormalTok{,}
\NormalTok{           income }\SpecialCharTok{\textgreater{}} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \DecValTok{2}\NormalTok{,}
\NormalTok{           income }\SpecialCharTok{\textgreater{}} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{,}
           \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \DecValTok{0}\NormalTok{)}
\NormalTok{         ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rowwise}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{sum\_num =} \FunctionTok{sum}\NormalTok{(age\_num, city\_num, gender\_num, income\_num),}
         \AttributeTok{softmax\_prob =} \FunctionTok{exp}\NormalTok{(sum\_num)}\SpecialCharTok{/}\FunctionTok{exp}\NormalTok{(}\DecValTok{12}\NormalTok{),}
         \AttributeTok{free\_shipping =} \FunctionTok{sample}\NormalTok{(}
           \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{1}\NormalTok{),}
           \AttributeTok{size =} \DecValTok{1}\NormalTok{,}
           \AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{,}
           \AttributeTok{prob =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{softmax\_prob, softmax\_prob)}
\NormalTok{           )}
\NormalTok{         ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{()}

\NormalTok{amazon\_purchase\_data }\OtherTok{\textless{}{-}}
\NormalTok{  amazon\_purchase\_data }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{age\_num, }\SpecialCharTok{{-}}\NormalTok{city\_num, }\SpecialCharTok{{-}}\NormalTok{gender\_num, }\SpecialCharTok{{-}}\NormalTok{income\_num, }\SpecialCharTok{{-}}\NormalTok{sum\_num, }\SpecialCharTok{{-}}\NormalTok{softmax\_prob)}
\end{Highlighting}
\end{Shaded}

Finally, we need to have some measure of a person's average spend. We want those with free shipping to be slightly higher than those without.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{amazon\_purchase\_data }\OtherTok{\textless{}{-}}
\NormalTok{  amazon\_purchase\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{mean\_spend =} \FunctionTok{if\_else}\NormalTok{(free\_shipping }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\DecValTok{60}\NormalTok{, }\DecValTok{50}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rowwise}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{average\_spend =} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, mean\_spend, }\AttributeTok{sd =} \DecValTok{5}\NormalTok{)}
\NormalTok{    ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{mean\_spend)}

\DocumentationTok{\#\# Fix the class on some}
\NormalTok{amazon\_purchase\_data }\OtherTok{\textless{}{-}}
\NormalTok{  amazon\_purchase\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(city, gender, free\_shipping), }\SpecialCharTok{\textasciitilde{}}\FunctionTok{as.factor}\NormalTok{(.)) }\DocumentationTok{\#\# Change some to factors}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(amazon\_purchase\_data}\SpecialCharTok{$}\NormalTok{free\_shipping)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}    0    1 }
\CommentTok{\#\textgreater{} 9629  371}

\FunctionTok{head}\NormalTok{(amazon\_purchase\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 7}
\CommentTok{\#\textgreater{}   unique\_person\_id   age city    gender income free\_shipping}
\CommentTok{\#\textgreater{}              \textless{}int\textgreater{} \textless{}dbl\textgreater{} \textless{}fct\textgreater{}   \textless{}fct\textgreater{}   \textless{}dbl\textgreater{} \textless{}fct\textgreater{}        }
\CommentTok{\#\textgreater{} 1                1  47.5 Calgary Female  1.72  0            }
\CommentTok{\#\textgreater{} 2                2  27.8 Montre\textasciitilde{} Male    1.54  0            }
\CommentTok{\#\textgreater{} 3                3  57.7 Toronto Female  3.16  0            }
\CommentTok{\#\textgreater{} 4                4  43.9 Toronto Male    0.636 0            }
\CommentTok{\#\textgreater{} 5                5  21.1 Toronto Female  1.43  0            }
\CommentTok{\#\textgreater{} 6                6  51.1 Calgary Male    1.18  0            }
\CommentTok{\#\textgreater{} \# ... with 1 more variable: average\_spend \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

Now we construct a logistic regression model that `explains' whether a person was treated as a function of the variables that we think explain it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{propensity\_score }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(free\_shipping }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ city }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ income, }
                        \AttributeTok{family =}\NormalTok{ binomial,}
                        \AttributeTok{data =}\NormalTok{ amazon\_purchase\_data)}
\end{Highlighting}
\end{Shaded}

We will now add our forecast to our dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{amazon\_purchase\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{augment}\NormalTok{(propensity\_score, }
          \AttributeTok{data =}\NormalTok{ amazon\_purchase\_data,}
          \AttributeTok{type.predict =} \StringTok{"response"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{.resid, }\SpecialCharTok{{-}}\NormalTok{.std.resid, }\SpecialCharTok{{-}}\NormalTok{.hat, }\SpecialCharTok{{-}}\NormalTok{.sigma, }\SpecialCharTok{{-}}\NormalTok{.cooksd) }
\end{Highlighting}
\end{Shaded}

Now we use our forecast to create matches. There are a variety of ways to do this. In a moment I'll step through some code that does it all at once, but as this is a worked example and we only have a small number of possibilities, we can just do it manually.

For every person who was actually treated (given free shipping) we want the untreated person who was considered as similar to them (based on propensity score) as possible.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{amazon\_purchase\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  amazon\_purchase\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(.fitted, free\_shipping)}
\end{Highlighting}
\end{Shaded}

Here we're going to use a matching function from the \texttt{arm} package. This finds which is the closest of the ones that were not treated, to each one that was treated.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{amazon\_purchase\_data}\SpecialCharTok{$}\NormalTok{treated }\OtherTok{\textless{}{-}} \FunctionTok{if\_else}\NormalTok{(amazon\_purchase\_data}\SpecialCharTok{$}\NormalTok{free\_shipping }\SpecialCharTok{==} \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{amazon\_purchase\_data}\SpecialCharTok{$}\NormalTok{treated }\OtherTok{\textless{}{-}} \FunctionTok{as.integer}\NormalTok{(amazon\_purchase\_data}\SpecialCharTok{$}\NormalTok{treated)}

\NormalTok{matches }\OtherTok{\textless{}{-}}\NormalTok{ arm}\SpecialCharTok{::}\FunctionTok{matching}\NormalTok{(}\AttributeTok{z =}\NormalTok{ amazon\_purchase\_data}\SpecialCharTok{$}\NormalTok{treated, }\AttributeTok{score =}\NormalTok{ amazon\_purchase\_data}\SpecialCharTok{$}\NormalTok{.fitted)}

\NormalTok{amazon\_purchase\_data }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(amazon\_purchase\_data, matches)}
\end{Highlighting}
\end{Shaded}

Now we reduce the dataset to just those that are matched. We had 371 treated, so we expect a dataset of 742 observations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{amazon\_purchase\_data\_matched }\OtherTok{\textless{}{-}} 
\NormalTok{  amazon\_purchase\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(match.ind }\SpecialCharTok{!=} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{match.ind, }\SpecialCharTok{{-}}\NormalTok{pairs, }\SpecialCharTok{{-}}\NormalTok{treated)}

\FunctionTok{head}\NormalTok{(amazon\_purchase\_data\_matched)}
\CommentTok{\#\textgreater{}   unique\_person\_id      age     city gender     income}
\CommentTok{\#\textgreater{} 1             5710 81.15636 Montreal Female 0.67505625}
\CommentTok{\#\textgreater{} 2             9458 97.04859 Montreal Female 9.49752179}
\CommentTok{\#\textgreater{} 3             6428 83.21262  Calgary   Male 0.05851482}
\CommentTok{\#\textgreater{} 4             2022 98.97504 Montreal   Male 1.66683768}
\CommentTok{\#\textgreater{} 5             9824 64.61936  Calgary Female 3.35263989}
\CommentTok{\#\textgreater{} 6             1272 97.09546  Toronto Female 0.71813784}
\CommentTok{\#\textgreater{}   free\_shipping average\_spend     .fitted cnts}
\CommentTok{\#\textgreater{} 1             0      47.36258 0.001375987    1}
\CommentTok{\#\textgreater{} 2             1      61.15317 0.001376161    1}
\CommentTok{\#\textgreater{} 3             0      49.90080 0.001560150    1}
\CommentTok{\#\textgreater{} 4             1      57.75673 0.001560418    1}
\CommentTok{\#\textgreater{} 5             1      64.69709 0.002207195    1}
\CommentTok{\#\textgreater{} 6             0      56.64754 0.002207514    1}
\end{Highlighting}
\end{Shaded}

Finally, we can examine the `effect' of being treated on average spend in the `usual' way.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{propensity\_score\_regression }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(average\_spend }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ city }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ free\_shipping, }
                                  \AttributeTok{data =}\NormalTok{ amazon\_purchase\_data\_matched)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{huxtable}\SpecialCharTok{::}\FunctionTok{huxreg}\NormalTok{(propensity\_score\_regression)}
\end{Highlighting}
\end{Shaded}

\label{tab:unnamed-chunk-18}

(1)

(Intercept)

49.694 ***

(0.809)~~~

age

0.005~~~~

(0.011)~~~

cityMontreal

0.169~~~~

(0.734)~~~

cityToronto

0.652~~~~

(0.623)~~~

genderMale

-0.968 *~~

(0.422)~~~

genderOther/decline

-1.973~~~~

(2.621)~~~

income

0.009~~~~

(0.021)~~~

free\_shipping1

10.488 ***

(0.380)~~~

N

742~~~~~~~~

R2

0.513~~~~

logLik

-2267.486~~~~

AIC

4552.971~~~~

*** p \textless{} 0.001; ** p \textless{} 0.01; * p \textless{} 0.05.

I cover propensity score matching here because it is widely used. Hence, you need to know how to use it. People would think it's weird if you didn't, in the same way that we have to cover ANOVA people would think it's weird if we had an entire experimental design course and didn't cover it even though there are more modern ways of looking at differences between two means. But at the same time you need to know that there are flaws with propensity score matching. I will now discuss some of them.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Matching. Propensity score matching cannot match on unobserved variables. This may be fine in a class-room setting, but in more realistic settings it will likely cause issues.
\item
  Modelling. The results tend to be specific to the model that is used. King and Nielsen, 2019, discuss this thoroughly.
\item
  Statistically. We are using the data twice.
\end{enumerate}

\hypertarget{case-study---lower-advertising-revenue-reduced-french-newspaper-prices-between-1960-and-1974}{%
\section{Case study - Lower advertising revenue reduced French newspaper prices between 1960 and 1974}\label{case-study---lower-advertising-revenue-reduced-french-newspaper-prices-between-1960-and-1974}}

\hypertarget{introduction-21}{%
\subsection{Introduction}\label{introduction-21}}

In this case study we introduce Angelucci and Cagé, 2019, and replicate its main findings. Angelucci and Cagé, 2019, is a paper in which difference in differences is used to examine the effect of the reduction in advertising revenues on newspapers' content and prices. They create a dataset of `French newspapers between 1960 and 1974'. They `perform a difference-in-differences analysis' and exploit `the introduction of advertising on television' as this change `affected national newspapers more severely than local ones'. They `find robust evidence of a decrease in the amount of journalistic-intensive content produced and the subscription price.'

In order to conduct this analysis we will use the dataset that they provide alongside their paper. This dataset is available at: \url{https://www.openicpsr.org/openicpsr/project/116438/version/V1/view}. It is available for you to download after registration. As their dataset is in Stata data format, we will use the haven package to read it in (Wickham and Miller, 2019).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(here)}
\FunctionTok{library}\NormalTok{(haven)}
\FunctionTok{library}\NormalTok{(huxtable)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Attaching package: \textquotesingle{}huxtable\textquotesingle{}}
\CommentTok{\#\textgreater{} The following objects are masked from \textquotesingle{}package:ggdag\textquotesingle{}:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     label, label\textless{}{-}}
\CommentTok{\#\textgreater{} The following object is masked from \textquotesingle{}package:dplyr\textquotesingle{}:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     add\_rownames}
\CommentTok{\#\textgreater{} The following object is masked from \textquotesingle{}package:ggplot2\textquotesingle{}:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     theme\_grey}
\FunctionTok{library}\NormalTok{(scales)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Attaching package: \textquotesingle{}scales\textquotesingle{}}
\CommentTok{\#\textgreater{} The following object is masked from \textquotesingle{}package:huxtable\textquotesingle{}:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     number\_format}
\CommentTok{\#\textgreater{} The following object is masked from \textquotesingle{}package:purrr\textquotesingle{}:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     discard}
\CommentTok{\#\textgreater{} The following object is masked from \textquotesingle{}package:readr\textquotesingle{}:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     col\_factor}
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\hypertarget{background-1}{%
\subsection{Background}\label{background-1}}

Newspapers are in trouble. We can probably all think of a local newspaper that has closed recently because of pressure brought on by the internet. But this issue isn't new. When television started, there were similar concerns. In this paper, Angelucci and Cagé use the introduction of television advertising in France, announced in 1967, to examine the effect of decreased advertising revenue on newspapers.

The reason this is important is because it allows us to disentangle a few competing effects. For instance, are newspapers becoming redundant because they can no longer charge high prices for their ads or because consumers prefer to get their news in other ways? Are fewer journalists needed because smartphones and other technology mean they can be more productive? Angelucci and Cagé look at advertising revenue and a few other features, when a new advertising platform arrives, in this case television advertising.

\hypertarget{data-1}{%
\subsection{Data}\label{data-1}}

\begin{quote}
(The) dataset contains annual data on local and national newspapers between 1960 and 1974, as well as detailed information on television content. In 1967, the French government announced it would relax long-standing regulations that prohibited television advertising. We provide evidence that this reform can be plausibly interpreted as an exogenous and negative shock to the advertising side of the newspaper industry\ldots{} {[}I{]}t is likely that the introduction of television advertising constituted a direct shock to the advertising side of the newspaper industry and only an indirect shock to the reader side\ldots{} (O)ur empirical setting constitutes a unique opportunity to isolate the consequences of a decrease in newspapers' advertising revenues on their choices regarding the size of their newsroom, the amount of information to produce, and the prices they charge to both sides of the market.
\end{quote}

The authors' argue that national newspapers were affected by the television advertising change, but local newspapers were not. So the national newspapers are the treatment group and the local newspapers are the control group.

The dataset can be read in using \texttt{read\_dta()}, which is a function within the \texttt{haven} package for reading in Stata dta files. This is equivalent to \texttt{read\_csv()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{newspapers }\OtherTok{\textless{}{-}} \FunctionTok{read\_dta}\NormalTok{(here}\SpecialCharTok{::}\FunctionTok{here}\NormalTok{(}\StringTok{"inputs/data/116438{-}V1/data/dta/Angelucci\_Cage\_AEJMicro\_dataset.dta"}\NormalTok{))}

\FunctionTok{dim}\NormalTok{(newspapers)}
\CommentTok{\#\textgreater{} [1] 1196   52}
\end{Highlighting}
\end{Shaded}

There are 1,196 observations in the dataset and 52 variables. The authors are interested in the 1960-1974 time period which has around 100 newspapers. There are 14 national newspapers at the beginning of the period and 12 at the end.

We just want to replicate their main results, so we don't need all their variables. As such we will just \texttt{select()} the ones that we are interested in and change the \texttt{class()} where needed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{newspapers }\OtherTok{\textless{}{-}} 
\NormalTok{  newspapers }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(year, id\_news, after\_national, local, national, }\DocumentationTok{\#\# Diff in diff variables}
\NormalTok{         ra\_cst, qtotal, ads\_p4\_cst, ads\_s, }\DocumentationTok{\#\# Advertising side dependents}
\NormalTok{         ps\_cst, po\_cst, qtotal, qs\_s, rs\_cst) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\#Reader side dependents}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{ra\_cst\_div\_qtotal =}\NormalTok{ ra\_cst }\SpecialCharTok{/}\NormalTok{ qtotal) }\SpecialCharTok{\%\textgreater{}\%} \DocumentationTok{\#\# An advertising side dependents needs to be built}
  \FunctionTok{mutate\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(id\_news, after\_national, local, national), }\SpecialCharTok{\textasciitilde{}}\FunctionTok{as.factor}\NormalTok{(.)) }\SpecialCharTok{\%\textgreater{}\%} \DocumentationTok{\#\# Change some to factors}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{year =} \FunctionTok{as.integer}\NormalTok{(year))}
\end{Highlighting}
\end{Shaded}

We can now have a look at the main variables of interest for both national (Figure \ref{fig:frenchnewspaperssummarystats}) and local daily newspapers (Figure \ref{fig:frenchlocalnewspaperssummarystats}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{figures/french_national_newspapers_summary_stats} \caption{Angelucci and Cagé, 2019, summary statistics: national daily newspapers}\label{fig:frenchnewspaperssummarystats}
\end{figure}

Source: Angelucci and Cagé, 2019, p.~333.

\begin{figure}
\includegraphics[width=0.9\linewidth]{figures/french_local_newspapers_summary_stats} \caption{Angelucci and Cagé, 2019, summary statistics: local daily newspapers}\label{fig:frenchlocalnewspaperssummarystats}
\end{figure}

Source: Angelucci and Cagé, 2019, p.~334.

Please read this section of their paper to see how they describe their dataset.

We are interested in the change from 1967 onward.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{newspapers }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \FunctionTok{if\_else}\NormalTok{(local }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\StringTok{"Local"}\NormalTok{, }\StringTok{"National"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ year, }\AttributeTok{y =}\NormalTok{ ra\_cst)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{labels =} \FunctionTok{dollar\_format}\NormalTok{(}\AttributeTok{prefix=}\StringTok{"$"}\NormalTok{, }\AttributeTok{suffix =} \StringTok{"M"}\NormalTok{, }\AttributeTok{scale =} \FloatTok{0.000001}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Year"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Advertising revenue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(type),}
               \AttributeTok{nrow =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \FloatTok{1966.5}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{42-causality_from_obs_files/figure-latex/unnamed-chunk-22-1.pdf}

\hypertarget{model-1}{%
\subsection{Model}\label{model-1}}

The model that we are interested in estimating is:
\[\mbox{ln}(y_{n,t}) = \beta_0 + \beta_1(\mbox{National dummy}\times\mbox{1967 onward dummy}) + \lambda_n + \gamma_y + \epsilon.\]
The \(\lambda_n\) is a fixed effect for each newspaper, and the \(\gamma_y\) is a fixed effect for each year. We just use regular linear regression, with a few different dependent variables. It is the \(\beta_1\) coefficient that we are interested in.

\hypertarget{results-1}{%
\subsection{Results}\label{results-1}}

We can run the models using \texttt{lm()}.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Advertising side}
\NormalTok{ad\_revenue }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(ra\_cst) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ after\_national }\SpecialCharTok{+}\NormalTok{ id\_news }\SpecialCharTok{+}\NormalTok{ year, }\AttributeTok{data =}\NormalTok{ newspapers)}
\NormalTok{ad\_revenue\_div\_circulation }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(ra\_cst\_div\_qtotal) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ after\_national }\SpecialCharTok{+}\NormalTok{ id\_news }\SpecialCharTok{+}\NormalTok{ year, }\AttributeTok{data =}\NormalTok{ newspapers)}
\NormalTok{ad\_price }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(ads\_p4\_cst) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ after\_national }\SpecialCharTok{+}\NormalTok{ id\_news }\SpecialCharTok{+}\NormalTok{ year, }\AttributeTok{data =}\NormalTok{ newspapers)}
\NormalTok{ad\_space }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(ads\_s) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ after\_national }\SpecialCharTok{+}\NormalTok{ id\_news }\SpecialCharTok{+}\NormalTok{ year, }\AttributeTok{data =}\NormalTok{ newspapers)}

\DocumentationTok{\#\# Consumer side}
\NormalTok{subscription\_price }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(ps\_cst) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ after\_national }\SpecialCharTok{+}\NormalTok{ id\_news }\SpecialCharTok{+}\NormalTok{ year, }\AttributeTok{data =}\NormalTok{ newspapers)}
\NormalTok{unit\_price }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(po\_cst) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ after\_national }\SpecialCharTok{+}\NormalTok{ id\_news }\SpecialCharTok{+}\NormalTok{ year, }\AttributeTok{data =}\NormalTok{ newspapers)}
\NormalTok{circulation }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(qtotal) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ after\_national }\SpecialCharTok{+}\NormalTok{ id\_news }\SpecialCharTok{+}\NormalTok{ year, }\AttributeTok{data =}\NormalTok{ newspapers)}
\NormalTok{share\_of\_sub }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(qs\_s) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ after\_national }\SpecialCharTok{+}\NormalTok{ id\_news }\SpecialCharTok{+}\NormalTok{ year, }\AttributeTok{data =}\NormalTok{ newspapers)}
\NormalTok{revenue\_from\_sales }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(rs\_cst) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ after\_national }\SpecialCharTok{+}\NormalTok{ id\_news }\SpecialCharTok{+}\NormalTok{ year, }\AttributeTok{data =}\NormalTok{ newspapers)}
\end{Highlighting}
\end{Shaded}

Looking at the advertising-side variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{omit\_me }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"(Intercept)"}\NormalTok{, }\StringTok{"id\_news3"}\NormalTok{, }\StringTok{"id\_news6"}\NormalTok{, }\StringTok{"id\_news7"}\NormalTok{, }\StringTok{"id\_news13"}\NormalTok{, }
             \StringTok{"id\_news16"}\NormalTok{, }\StringTok{"id\_news25"}\NormalTok{, }\StringTok{"id\_news28"}\NormalTok{, }\StringTok{"id\_news34"}\NormalTok{, }\StringTok{"id\_news38"}\NormalTok{, }
             \StringTok{"id\_news44"}\NormalTok{, }\StringTok{"id\_news48"}\NormalTok{, }\StringTok{"id\_news51"}\NormalTok{, }\StringTok{"id\_news53"}\NormalTok{, }\StringTok{"id\_news54"}\NormalTok{, }
             \StringTok{"id\_news57"}\NormalTok{, }\StringTok{"id\_news60"}\NormalTok{, }\StringTok{"id\_news62"}\NormalTok{, }\StringTok{"id\_news66"}\NormalTok{, }\StringTok{"id\_news67"}\NormalTok{, }
             \StringTok{"id\_news70"}\NormalTok{, }\StringTok{"id\_news71"}\NormalTok{, }\StringTok{"id\_news72"}\NormalTok{, }\StringTok{"id\_news80"}\NormalTok{, }\StringTok{"id\_news82"}\NormalTok{, }
             \StringTok{"id\_news88"}\NormalTok{, }\StringTok{"id\_news95"}\NormalTok{, }\StringTok{"id\_news97"}\NormalTok{, }\StringTok{"id\_news98"}\NormalTok{, }\StringTok{"id\_news103"}\NormalTok{, }
             \StringTok{"id\_news105"}\NormalTok{, }\StringTok{"id\_news106"}\NormalTok{, }\StringTok{"id\_news118"}\NormalTok{, }\StringTok{"id\_news119"}\NormalTok{, }\StringTok{"id\_news127"}\NormalTok{, }
             \StringTok{"id\_news136"}\NormalTok{, }\StringTok{"id\_news138"}\NormalTok{, }\StringTok{"id\_news148"}\NormalTok{, }\StringTok{"id\_news151"}\NormalTok{, }\StringTok{"id\_news153"}\NormalTok{, }
             \StringTok{"id\_news154"}\NormalTok{, }\StringTok{"id\_news157"}\NormalTok{, }\StringTok{"id\_news158"}\NormalTok{, }\StringTok{"id\_news161"}\NormalTok{, }\StringTok{"id\_news163"}\NormalTok{, }
             \StringTok{"id\_news167"}\NormalTok{, }\StringTok{"id\_news169"}\NormalTok{, }\StringTok{"id\_news179"}\NormalTok{, }\StringTok{"id\_news184"}\NormalTok{, }\StringTok{"id\_news185"}\NormalTok{, }
             \StringTok{"id\_news187"}\NormalTok{, }\StringTok{"id\_news196"}\NormalTok{, }\StringTok{"id\_news206"}\NormalTok{, }\StringTok{"id\_news210"}\NormalTok{, }\StringTok{"id\_news212"}\NormalTok{, }
             \StringTok{"id\_news213"}\NormalTok{, }\StringTok{"id\_news224"}\NormalTok{, }\StringTok{"id\_news225"}\NormalTok{, }\StringTok{"id\_news234"}\NormalTok{, }\StringTok{"id\_news236"}\NormalTok{, }
             \StringTok{"id\_news245"}\NormalTok{, }\StringTok{"id\_news247"}\NormalTok{, }\StringTok{"id\_news310"}\NormalTok{, }\StringTok{"id\_news452"}\NormalTok{, }\StringTok{"id\_news467"}\NormalTok{, }
             \StringTok{"id\_news469"}\NormalTok{, }\StringTok{"id\_news480"}\NormalTok{, }\StringTok{"id\_news20040"}\NormalTok{, }\StringTok{"id\_news20345"}\NormalTok{, }
             \StringTok{"id\_news20346"}\NormalTok{, }\StringTok{"id\_news20347"}\NormalTok{, }\StringTok{"id\_news20352"}\NormalTok{, }\StringTok{"id\_news20354"}\NormalTok{, }
             \StringTok{"id\_news21006"}\NormalTok{, }\StringTok{"id\_news21025"}\NormalTok{, }\StringTok{"id\_news21173"}\NormalTok{, }\StringTok{"id\_news21176"}\NormalTok{, }
             \StringTok{"id\_news33718"}\NormalTok{, }\StringTok{"id\_news34689"}\NormalTok{, }\StringTok{"id\_news73"}\NormalTok{)}

\FunctionTok{huxreg}\NormalTok{(}\StringTok{"Ad. rev."} \OtherTok{=}\NormalTok{ ad\_revenue, }
       \StringTok{"Ad rev. div. circ."} \OtherTok{=}\NormalTok{ ad\_revenue\_div\_circulation, }
       \StringTok{"Ad price"} \OtherTok{=}\NormalTok{ ad\_price, }
       \StringTok{"Ad space"} \OtherTok{=}\NormalTok{ ad\_space,}
        \AttributeTok{omit\_coefs =}\NormalTok{ omit\_me, }
        \AttributeTok{number\_format =} \DecValTok{2}
\NormalTok{        )}
\end{Highlighting}
\end{Shaded}

\label{tab:unnamed-chunk-24}

Ad. rev.

Ad rev. div. circ.

Ad price

Ad space

after\_national1

-0.23 ***

-0.15 ***

-0.31 ***

0.01~~~~

(0.03)~~~

(0.03)~~~

(0.07)~~~

(0.05)~~~

year

0.05 ***

0.04 ***

0.04 ***

0.02 ***

(0.00)~~~

(0.00)~~~

(0.00)~~~

(0.00)~~~

N

1052~~~~~~~

1048~~~~~~~

809~~~~~~~

1046~~~~~~~

R2

0.99~~~~

0.90~~~~

0.89~~~~

0.72~~~~

logLik

345.34~~~~

449.52~~~~

-277.71~~~~

-164.01~~~~

AIC

-526.68~~~~

-735.05~~~~

705.43~~~~

478.02~~~~

*** p \textless{} 0.001; ** p \textless{} 0.01; * p \textless{} 0.05.

Similarly, we can look at the reader-side variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{omit\_me }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"(Intercept)"}\NormalTok{, }\StringTok{"id\_news3"}\NormalTok{, }\StringTok{"id\_news6"}\NormalTok{, }\StringTok{"id\_news7"}\NormalTok{, }\StringTok{"id\_news13"}\NormalTok{, }
             \StringTok{"id\_news16"}\NormalTok{, }\StringTok{"id\_news25"}\NormalTok{, }\StringTok{"id\_news28"}\NormalTok{, }\StringTok{"id\_news34"}\NormalTok{, }\StringTok{"id\_news38"}\NormalTok{, }
             \StringTok{"id\_news44"}\NormalTok{, }\StringTok{"id\_news48"}\NormalTok{, }\StringTok{"id\_news51"}\NormalTok{, }\StringTok{"id\_news53"}\NormalTok{, }\StringTok{"id\_news54"}\NormalTok{, }
             \StringTok{"id\_news57"}\NormalTok{, }\StringTok{"id\_news60"}\NormalTok{, }\StringTok{"id\_news62"}\NormalTok{, }\StringTok{"id\_news66"}\NormalTok{, }\StringTok{"id\_news67"}\NormalTok{, }
             \StringTok{"id\_news70"}\NormalTok{, }\StringTok{"id\_news71"}\NormalTok{, }\StringTok{"id\_news72"}\NormalTok{, }\StringTok{"id\_news80"}\NormalTok{, }\StringTok{"id\_news82"}\NormalTok{, }
             \StringTok{"id\_news88"}\NormalTok{, }\StringTok{"id\_news95"}\NormalTok{, }\StringTok{"id\_news97"}\NormalTok{, }\StringTok{"id\_news98"}\NormalTok{, }\StringTok{"id\_news103"}\NormalTok{, }
             \StringTok{"id\_news105"}\NormalTok{, }\StringTok{"id\_news106"}\NormalTok{, }\StringTok{"id\_news118"}\NormalTok{, }\StringTok{"id\_news119"}\NormalTok{, }\StringTok{"id\_news127"}\NormalTok{, }
             \StringTok{"id\_news136"}\NormalTok{, }\StringTok{"id\_news138"}\NormalTok{, }\StringTok{"id\_news148"}\NormalTok{, }\StringTok{"id\_news151"}\NormalTok{, }\StringTok{"id\_news153"}\NormalTok{, }
             \StringTok{"id\_news154"}\NormalTok{, }\StringTok{"id\_news157"}\NormalTok{, }\StringTok{"id\_news158"}\NormalTok{, }\StringTok{"id\_news161"}\NormalTok{, }\StringTok{"id\_news163"}\NormalTok{, }
             \StringTok{"id\_news167"}\NormalTok{, }\StringTok{"id\_news169"}\NormalTok{, }\StringTok{"id\_news179"}\NormalTok{, }\StringTok{"id\_news184"}\NormalTok{, }\StringTok{"id\_news185"}\NormalTok{, }
             \StringTok{"id\_news187"}\NormalTok{, }\StringTok{"id\_news196"}\NormalTok{, }\StringTok{"id\_news206"}\NormalTok{, }\StringTok{"id\_news210"}\NormalTok{, }\StringTok{"id\_news212"}\NormalTok{, }
             \StringTok{"id\_news213"}\NormalTok{, }\StringTok{"id\_news224"}\NormalTok{, }\StringTok{"id\_news225"}\NormalTok{, }\StringTok{"id\_news234"}\NormalTok{, }\StringTok{"id\_news236"}\NormalTok{, }
             \StringTok{"id\_news245"}\NormalTok{, }\StringTok{"id\_news247"}\NormalTok{, }\StringTok{"id\_news310"}\NormalTok{, }\StringTok{"id\_news452"}\NormalTok{, }\StringTok{"id\_news467"}\NormalTok{, }
             \StringTok{"id\_news469"}\NormalTok{, }\StringTok{"id\_news480"}\NormalTok{, }\StringTok{"id\_news20040"}\NormalTok{, }\StringTok{"id\_news20345"}\NormalTok{, }
             \StringTok{"id\_news20346"}\NormalTok{, }\StringTok{"id\_news20347"}\NormalTok{, }\StringTok{"id\_news20352"}\NormalTok{, }\StringTok{"id\_news20354"}\NormalTok{, }
             \StringTok{"id\_news21006"}\NormalTok{, }\StringTok{"id\_news21025"}\NormalTok{, }\StringTok{"id\_news21173"}\NormalTok{, }\StringTok{"id\_news21176"}\NormalTok{, }
             \StringTok{"id\_news33718"}\NormalTok{, }\StringTok{"id\_news34689"}\NormalTok{, }\StringTok{"id\_news73"}\NormalTok{)}

\FunctionTok{huxreg}\NormalTok{(}\StringTok{"Subscription price"} \OtherTok{=}\NormalTok{ subscription\_price, }
       \StringTok{"Unit price"} \OtherTok{=}\NormalTok{ unit\_price, }
       \StringTok{"Circulation"} \OtherTok{=}\NormalTok{ circulation, }
       \StringTok{"Share of sub"} \OtherTok{=}\NormalTok{ share\_of\_sub,}
       \StringTok{"Revenue from sales"} \OtherTok{=}\NormalTok{ revenue\_from\_sales,}
       \AttributeTok{omit\_coefs =}\NormalTok{ omit\_me, }
       \AttributeTok{number\_format =} \DecValTok{2}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\label{tab:unnamed-chunk-25}

Subscription price

Unit price

Circulation

Share of sub

Revenue from sales

after\_national1

-0.04 *~~

0.06 **~

-0.06 **~

0.19 ***

-0.06 *~~

(0.02)~~~

(0.02)~~~

(0.02)~~~

(0.03)~~~

(0.03)~~~

year

0.05 ***

0.05 ***

0.01 ***

-0.01 ***

0.05 ***

(0.00)~~~

(0.00)~~~

(0.00)~~~

(0.00)~~~

(0.00)~~~

N

1044~~~~~~~

1063~~~~~~~

1070~~~~~~~

1072~~~~~~~

1046~~~~~~~

R2

0.88~~~~

0.87~~~~

0.99~~~~

0.97~~~~

0.99~~~~

logLik

882.14~~~~

907.28~~~~

759.57~~~~

321.91~~~~

451.11~~~~

AIC

-1600.28~~~~

-1650.57~~~~

-1355.15~~~~

-477.81~~~~

-738.22~~~~

*** p \textless{} 0.001; ** p \textless{} 0.01; * p \textless{} 0.05.

\hypertarget{other-points}{%
\subsection{Other points}\label{other-points}}

\begin{itemize}
\tightlist
\item
  We certainly find that in many cases there appears to be a difference from 1967 onward.
\item
  In general, we are able to obtain results that are similar to Angelucci and Cagé, 2019. If we spent more time, we could probably replicate their findings perfectly. Isn't this great! What else could do?
\item
  Parallel trends: Notice the wonderful way in which they test the `parallel trends' assumption on pp.~350-351.
\item
  Discussion: Look at their wonderful discussion (pp.~353-358) of interpretation, external validity, and robustness.
\end{itemize}

--\textgreater{}

\hypertarget{case-study---funding-of-clinical-trials-and-reported-drug-efficacy}{%
\section{Case study - Funding of Clinical Trials and Reported Drug Efficacy}\label{case-study---funding-of-clinical-trials-and-reported-drug-efficacy}}

\citet{oostrom2021} looks at clinical trials of drugs. These days, of course, we all know a lot more than we may have ever wished to, about clinical trials. But the one thing that we (think) we know is that they are, well, clinical. By that I mean, that it doesn't matter who does the actual trial, the outcome would be the same. \citet{oostrom2021} says this isn't true.

By way of background, clinical trials are needed before a drug can be approved. \citet{oostrom2021} finds that when pharmaceutical firms sponsor a clinical trial, `a drug appears 0.15 standard deviations more effective when the trial is sponsored by that drug's manufacturer, compared with the same drug in the same trial without the drug manufacturer's involvement.' She does this by exploiting the fact that often `the exact same sets of drugs are often compared in different randomized control trials conducted by parties with different financial interests.'.

The main finding is \citep[p.~2]{oostrom2021}:

\begin{quote}
Utilizing dozens of drug combinations across hundreds of clinical trials, I estimate that a drug appears 36 percent more effective (0.15 standard deviations off of a base of 0.42) when the trial is sponsored by that drug's manufacturing or marketing firm, compared with the same drug, evaluated against the same comparators, but without the drug manufacturer's involvement. As in the medical literature, I measure efficacy, in the case of antidepressants, as the share of patients that respond to medication or, in the case of schizophrenia, as the average decline in symptoms.
\end{quote}

Why might this happen? \citet{oostrom2021} looks at a variety of different options, grouped into those that happen before the trial and those that happen after the trial `publication bias'. She finds that `publication bias can explain as much as half of this sponsorship effect. Incorporating data on unpublished clinical trials, I find sponsored trials are less likely to publish non-positive results for their drugs.'

\citet{oostrom2021} focuses on antidepressant and antipsychotic drugs and this allows her to obtain a dataset of trials. An `arm' of a trial refers to `the unit at which randomization occurs. Arms are often unique drugs but occasionally refer to unique drug and dosage combinations.' \citep[ p.~9]{oostrom2021}.

Summary statistics are provided in a summary table (Figure \ref{fig:oostromsummary}) (this approach is common in economics, but not a great idea because it hides the distribution of the data - better to plot the raw data.)

\begin{figure}
\includegraphics[width=1\linewidth]{/Users/rohanalexander/Documents/book/figures/oostrom_1} \caption{Summary statistics from Ooostrom}\label{fig:oostromsummary}
\end{figure}

The model is:

\[y_{ij} = \alpha + \beta \mbox{ Sponsor}_{ij} + X_{ij}\gamma + G_{d(i),s(j)} +\epsilon_{ij}\]

where \(y_{ij}\) is the efficacy for arm \(i\) in trial \(j\). The main coefficient of interest is \(\beta\) which is based on whether \(\mbox{Sponsor}_{ij}\). The outcome is relative to the placebo arm in that trial, or the least effective arm.

`Table 3.3' from the paper is actually the reason that I included this as a case student. If this sounds odd to you then you've not had to read millions of papers that are unclear about their results. `Table 3.3' (republished here as Figure \ref{fig:oostromtable}) is beautiful and I'll allow it to speak for itself.

\begin{figure}
\includegraphics[width=1\linewidth]{/Users/rohanalexander/Documents/book/figures/oostrom_2} \caption{Results}\label{fig:oostromtable}
\end{figure}

The paper is available here: \url{https://www.tamaroostrom.com/research} and I'd recommend a brief read.

\hypertarget{regression-discontinuity-design}{%
\section{Regression discontinuity design}\label{regression-discontinuity-design}}

\hypertarget{introduction-22}{%
\subsection{Introduction}\label{introduction-22}}

Regression discontinuity design (RDD) is a popular way to get causality when there is some continuous variable with cut-offs that determine treatment. Is there a difference between a student who gets 79 per cent and a student who gets 80 per cent? Probably not much, but one gets an A-, while the other gets a B+, and seeing that on a transcript could affect who gets a job which could affect income. In this case the percentage is a `forcing variable' and the cut-off for an A- is a `threshold'. As the treatment is determined by the forcing variable all you need to do is to control for that variable. And, these seemingly arbitrary cut-offs can be seen all the time. Hence, there has been an `explosion' in the use of regression discontinuity design (Figure \ref{fig:johnholbein}).

Please note that I've followed the terminology of Taddy, 2019. Gelman and Hill, 2007, and others use slightly different terminology. For instance, Cunningham refers to the forcing function as the running variable. It doesn't matter what you use so long as you are consistent. If you have a terminology that you are familiar with then please feel free to use it, and to share it with me!

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/johnholbein} \caption{The explosion of regression discontinuity designs in recent years.}\label{fig:johnholbein}
\end{figure}

Source: John Holbein, \href{https://twitter.com/JohnHolbein1/status/1228050675378069504}{13 February 2020}.

The key assumptions are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The cut-off is `known, precise and free of manipulation' (Cunningham, 2020, p.~163).
\item
  The forcing function should be continuous because this means we can say that people on either side of the threshold are the same, other than happening to just fall on either side of the threshold.
\end{enumerate}

\hypertarget{simulated-example-1}{%
\subsection{Simulated example}\label{simulated-example-1}}

Let's generate some data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(broom)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{number\_of\_observation }\OtherTok{\textless{}{-}} \DecValTok{1000}

\NormalTok{rdd\_example\_data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{person =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{number\_of\_observation),}
                           \AttributeTok{grade =} \FunctionTok{runif}\NormalTok{(number\_of\_observation, }\AttributeTok{min =} \DecValTok{78}\NormalTok{, }\AttributeTok{max =} \DecValTok{82}\NormalTok{),}
                           \AttributeTok{income =} \FunctionTok{rnorm}\NormalTok{(number\_of\_observation, }\DecValTok{10}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{                           )}

\DocumentationTok{\#\# We want to make income more likely to be higher if they are have a grade over 80}
\NormalTok{rdd\_example\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  rdd\_example\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{income =} \FunctionTok{if\_else}\NormalTok{(grade }\SpecialCharTok{\textgreater{}} \DecValTok{80}\NormalTok{, income }\SpecialCharTok{+} \DecValTok{2}\NormalTok{, income))}

\FunctionTok{head}\NormalTok{(rdd\_example\_data)}
\end{Highlighting}
\end{Shaded}

\label{tab:unnamed-chunk-26}

person

grade

income

1

79.4

9.43

2

78.5

9.69

3

79.9

10.8~

4

79.3

9.34

5

78.1

10.7~

6

79.6

9.83

Let's make a graph.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rdd\_example\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ grade,}
             \AttributeTok{y =}\NormalTok{ income)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{data =}\NormalTok{ rdd\_example\_data }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(grade }\SpecialCharTok{\textless{}} \DecValTok{80}\NormalTok{), }
              \AttributeTok{method=}\StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{,}
              \AttributeTok{color =} \StringTok{"black"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{data =}\NormalTok{ rdd\_example\_data }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(grade }\SpecialCharTok{\textgreater{}=} \DecValTok{80}\NormalTok{), }
              \AttributeTok{method=}\StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{,}
              \AttributeTok{color =} \StringTok{"black"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Grade"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Income ($)"}\NormalTok{)}
\CommentTok{\#\textgreater{} \textasciigrave{}geom\_smooth()\textasciigrave{} using formula \textquotesingle{}y \textasciitilde{} x\textquotesingle{}}
\CommentTok{\#\textgreater{} \textasciigrave{}geom\_smooth()\textasciigrave{} using formula \textquotesingle{}y \textasciitilde{} x\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

\includegraphics{42-causality_from_obs_files/figure-latex/unnamed-chunk-27-1.pdf}

We can use a dummy variable with linear regression to estimate the effect (we're hoping that it's 2 because that is what we imposed.)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rdd\_example\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  rdd\_example\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{grade\_80\_and\_over =} \FunctionTok{if\_else}\NormalTok{(grade }\SpecialCharTok{\textless{}} \DecValTok{80}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)) }

\FunctionTok{lm}\NormalTok{(income }\SpecialCharTok{\textasciitilde{}}\NormalTok{ grade }\SpecialCharTok{+}\NormalTok{ grade\_80\_and\_over, }\AttributeTok{data =}\NormalTok{ rdd\_example\_data) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{tidy}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\label{tab:unnamed-chunk-28}

term

estimate

std.error

statistic

p.value

(Intercept)

11.7~~

4.24~~

2.76~

0.00585~

grade

-0.021

0.0537

-0.391

0.696~~~

grade\_80\_and\_over

1.99~

0.123~

16.2~~

1.34e-52

There are various caveats to this estimate that we'll get into later, but the essentials are here.

The other great thing about regression discontinuity is that is can almost be as good as an RCT. For instance, (and I thank John Holbein for the pointer) \citet{bloombellreiman2020} compare randomized trials with RDDs and find that the RCTs compare favourably.

\hypertarget{different-slopes}{%
\subsubsection{Different slopes}\label{different-slopes}}

Figure \ref{fig:scotland} shows an example with different slopes.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/scotland} \caption{Effect of minimum unit pricing for alcohol in Scotland.}\label{fig:scotland}
\end{figure}

Source: John Burn-Murdoch, \href{https://twitter.com/jburnmurdoch/status/1225773931342303233}{7 February 2020}.

\hypertarget{overlap}{%
\subsection{Overlap}\label{overlap}}

In the randomised control trial and A/B testing section, because of randomised assignment of the treatment, we imposed that the control and treatment groups were the same but for the treatment. We moved to difference-in-differences, and we assumed that there was a common trend between the treated and control groups. We allowed that the groups could be different, but that we could `difference out' their differences. Finally, we considered matching, and we said that even if we the control and treatment groups seemed quite different we were able to match those who were treated with a group that were similar to them in all ways, apart from the fact that they were not treated.

In regression discontinuity we consider a slightly different setting - the two groups are completely different in terms of the forcing variable - they are on either side of the threshold. So there is no overlap at all. But we know the threshold and believe that those on either side are essentially matched. Let's consider the 2019 NBA Eastern Conference Semifinals - Toronto and the Philadelphia. Game 1: Raptors win 108-95; Game 2: 76ers win 94-89; Game 3: 76ers win 116-95; Game 4: Raptors win 101-96; Game 5: Raptors win 125-89; Game 6: 76ers win 112-101; and finally, Game 7: Raptors win 92-90, because of a ball that win in after bouncing on the rim four times. Was there really that much difference between the teams (Figure \ref{fig:kawai})?

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/torraptors13} \caption{It took four bounces to go in, so how different were the teams...?}\label{fig:kawai}
\end{figure}

Source: Stan Behal / \href{https://nationalpost.com/sports/kawhi-leonard-miracle-shot-toronto-raptors-game-7-scott-stinson-kawhi-leonards-miracle-shot-sends-toronto-raptors-into-game-7-elation}{Postmedia Network}.

\hypertarget{examples}{%
\subsection{Examples}\label{examples}}

As with difference-in-differences, after I learnt about it, I began to see opportunities to implement it everywhere. Frankly, I find it a lot easier to think of legitimate examples of using regression discontinuity than difference-in-differences. But, at the risk of mentioning yet another movie from the 1990s that none of you have seen, when I think of RDD, my first thought is often of Sliding Doors (Figure \ref{fig:slidingdoors}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/sliding_doors} \caption{Nobody expects the Spanish Inquisition.}\label{fig:slidingdoors}
\end{figure}

Source: Mlotek, Haley, 2018, `The Almosts and What-ifs of 'Sliding Doors'', \emph{The Ringer}, 24 April, freely available at: \url{https://www.theringer.com/movies/2018/4/24/17261506/sliding-doors-20th-anniversary}.

Not only did the movie have a great soundtrack and help propel Gwyneth Paltrow to super-stardom, but it features an iconic moment in which Paltrow's character, Helen, arrives at a tube station at which point the movie splits into two. In one version she just makes the train, and arrives home to find her boyfriend cheating on her; and in another she just misses the train and doesn't find out about the boyfriend.

I'd say, spoiler alert, but the movie was released in 1998, so\ldots{} Of course, that `threshold' turns out to be important. In the world in which she gets the train she leaves the boyfriend, cuts her hair, and changes everything about her life. In the world in which she misses the train she doesn't. At least initially. But, and I can't say this any better than Ashley Fetters:

\begin{quote}
At the end of Sliding Doors, the ``bad'' version of Helen's life elides right into the ``good'' version; even in the ``bad'' version, the philandering !@\#\$\%\^{}\& boyfriend eventually gets found out and dumped, the true love eventually gets met-cute, and the MVP friend comes through. According to the Sliding Doors philosophy, in other words, even when our lives take fluky, chaotic detours, ultimately good-hearted people find each other, and the bad boyfriends and home-wreckers of the world get their comeuppance. There's no freak turn of events that allows the cheating boyfriend to just keep cheating, or the well-meaning, morally upright soulmates to just keep floating around in the universe unacquainted.

Fetters, Ashley, 2018, `I Think About This a Lot: The Sliding Doors in Sliding Doors', \emph{The Cut}, 9 April, freely available at: \url{https://www.thecut.com/2018/04/i-think-about-this-a-lot-the-sliding-doors-in-sliding-doors.html}.
\end{quote}

I'm getting off-track here, but the point is, not only does it seem as though we have a `threshold', but it seems as though there's continuity!

Let's see some more legitimate implementations of regression discontinuity. (And thank you to \href{http://www.ryanbedwards.com/}{Ryan Edwards} for pointing me to these.)

\hypertarget{elections}{%
\subsubsection{Elections}\label{elections}}

Elections are a common area of application for regression discontinuity because if the election is close then arguably there's not much difference between the candidates. There are plenty of examples of regression discontinuity in an elections setting, but one recent one is George, Siddharth Eapen, 2019, `Like Father, Like Son? The Effect of Political Dynasties on Economic Development', freely available at: \url{https://www.dropbox.com/s/orhvh3n03wd9ybl/sid_JMP_dynasties_latestdraft.pdf?dl=0}.

In this paper George is interested in political dynasties. But is the child of a politician more likely to be elected because they are the child of a politician, or because they happen to also be similarly skilled at politics? Regression discontinuity can help because in a close election, we can look at differences between places where someone narrowly won with where a similar someone narrowly lost.

In the George, 2019, case he examines:

\begin{quote}
descendant effects using a close elections regression discontinuity (RD) design. We focus on close races between dynastic descendants (i.e.~direct relatives of former officeholders) and non-dynasts, and we compare places where a descendant narrowly won to those where a descendant narrowly lost. In these elections, descendants and non-dynasts have similar demographic and political characteristics, and win in similar places and at similar rates. Nevertheless, we find negative economic effects when a descendant narrowly wins. Villages represented by a descendant have lower asset ownership and public good provision after an electoral term: households are less likely to live in a brick house and to own basic amenities like a refrigerator, mobile phone, or vehicle. Moreover, voters assess descendants to perform worse in office. An additional standard deviation of exposure to descendants lowers a village's wealth rank by 12pp.
\end{quote}

The model that George, 2019, estimates is (p.~19:
\[y_i = \alpha_{\mbox{district}} + \beta \times \mbox{Years descendant rule}_i + f(\mbox{Descendant margin}) + \gamma X_i + \epsilon_{i,t}.\]
In this model, \(y_i\) is various development outcomes in village \(i\); \(\mbox{Years descendant rule}_i\) is the number of years a dynastic descendant has represented village \(i\) in the national or state parliament; \(\mbox{Descendant margin}\) is the vote share difference between the dynastic descendant and non-dynast; and \(\gamma X_i\) is a vector of village-level adjustments.

George, 2019, then conducts a whole bunch of tests of the validity of the regression discontinuity design (p.~19). These are critical in order for the results to be believed. There are a lot of different results but one is shown in Figure \ref{fig:descendantsinindia}.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/descendantsinindia} \caption{George, 2019, descendant effects identified using close elections RD design (p. 41).}\label{fig:descendantsinindia}
\end{figure}

\hypertarget{economic-development}{%
\subsubsection{Economic development}\label{economic-development}}

One of the issues with considering economic development is that a place typically is either subject to some treatment or not. However, sometimes regression discontinuity allows us to compare areas that were just barely treated with those that were just barely not.

One recent paper that does this Esteban Mendez-Chacon and Diana Van Patten, 2020, `Multinationals, monopsony and local development: Evidence from the United Fruit Company' available here: \url{https://www.dianavanpatten.com/}. They are interested in the effect of the United Fruit Company (UFCo), which was given land in Costa Rica between 1889 and 1984. They were given roughly 4 per cent of the national territory or around 4500 acres. They key is that this land assignment was redrawn in 1904 based on a river and hence the re-assignment was essentially random with regard to determinants of growth to that point. They compare areas that were assigned to UFCo with those that were not. They find:

\begin{quote}
We find that the firm had a positive and persistent effect on living standards. Regions within the UFCo were 26 per cent less likely to be poor in 1973 than nearby counterfactual locations, with only 63 per cent of the gap closing over the following three decades. Company documents explain that a key concern at the time was to attract and maintain a sizable workforce, which induced the firm to invest heavily in local amenities that likely account for our result.
\end{quote}

The model is:
\[y_{i,g,t} = \gamma\mbox{UFCo}_g + f(\mbox{geographic location}_g) + X_{i,g,t}\beta + X_g\Gamma + \alpha_t + \epsilon_{i,g,t}.\]
In this model, \(y_{i,g,t}\) is the development outcome for a household \(i\) in census-block \(g\) and year \(t\); \(\gamma\mbox{UFCo}_g\) is an indicator variable as to whether the census-block was in a UFCo area or not; \(f(\mbox{geographic location}_g)\) is a function of the latitude and longitude to adjust for geographic area; \(X_{i,g,t}\) is covariates for household \(i\); \(X_g\) is geographic characteristics for that census-block; and \(\alpha_t\) is a year fixed effect.

Again, there are a lot of different results but one is shown in Figure \ref{fig:ufco}.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/ufco} \caption{George, 2020, UFCo effect on the probability of being poor (p. 17).}\label{fig:ufco}
\end{figure}

\hypertarget{implementation}{%
\subsection{Implementation}\label{implementation}}

Although they are fairly conceptually similar to work that we have done in the past, if you are wanting to use regression discontinuity in your work then you might like to consider a specialised package. The package \texttt{rdrobust} is a one recommendation, although there are others available and you should try those if you are interested. (The \texttt{rdd} package had been the go-to for a while, but seems to have been taken off CRAN recently. If you use RDD, then maybe just follow up to see if it comes back on as that one is pretty nice.)

Let's look at our example using \texttt{rdrobust}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rdrobust)}
\FunctionTok{rdrobust}\NormalTok{(}\AttributeTok{y =}\NormalTok{ rdd\_example\_data}\SpecialCharTok{$}\NormalTok{income, }
         \AttributeTok{x =}\NormalTok{ rdd\_example\_data}\SpecialCharTok{$}\NormalTok{grade, }
         \AttributeTok{c =} \DecValTok{80}\NormalTok{, }\AttributeTok{h =} \DecValTok{2}\NormalTok{, }\AttributeTok{all =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\CommentTok{\#\textgreater{} Call: rdrobust}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Obs.                 1000}
\CommentTok{\#\textgreater{} BW type                      Manual}
\CommentTok{\#\textgreater{} Kernel                   Triangular}
\CommentTok{\#\textgreater{} VCE method                       NN}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Obs.                  497          503}
\CommentTok{\#\textgreater{} Eff. Number of Obs.             497          503}
\CommentTok{\#\textgreater{} Order est. (p)                    1            1}
\CommentTok{\#\textgreater{} Order bias  (q)                   2            2}
\CommentTok{\#\textgreater{} BW est. (h)                   2.000        2.000}
\CommentTok{\#\textgreater{} BW bias (b)                   2.000        2.000}
\CommentTok{\#\textgreater{} rho (h/b)                     1.000        1.000}
\CommentTok{\#\textgreater{} Unique Obs.                     497          503}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} =============================================================================}
\CommentTok{\#\textgreater{}         Method     Coef. Std. Err.         z     P\textgreater{}|z|      [ 95\% C.I. ]       }
\CommentTok{\#\textgreater{} =============================================================================}
\CommentTok{\#\textgreater{}   Conventional     1.974     0.143    13.783     0.000     [1.693 , 2.255]     }
\CommentTok{\#\textgreater{} Bias{-}Corrected     1.977     0.143    13.805     0.000     [1.696 , 2.258]     }
\CommentTok{\#\textgreater{}         Robust     1.977     0.211     9.374     0.000     [1.564 , 2.390]     }
\CommentTok{\#\textgreater{} =============================================================================}
\end{Highlighting}
\end{Shaded}

\hypertarget{fuzzy-rdd}{%
\subsection{Fuzzy RDD}\label{fuzzy-rdd}}

The examples to this point have been `sharp' RDD. That is, the threshold is strict. However, in reality, often the boundary is a little less strict. For instance, consider the drinking age. Although there is a legal drinking age, say 19. If we looked at the number of people who had drank, then it's likely to increase in the few years leading up to that age. Perhaps you went to Australia where the drinking age is 18 and drank. Or perhaps you snuck into a bar when you were 17, etc.

In a sharp RDD setting, if you know the value of the forcing function then you know the outcome. For instance, if you get a grade of 80 then we know that you got an A-, but if you got a grade of 79 then we know that you got a B+. But with fuzzy RDD it is only known with some probability. We can say that a Canadian 19-year-old is more likely to have drunk alcohol than a Canadian 18 year old, but the number of Canadian 18-year-olds who have drunk alcohol is not zero.

It may be possible to deal with fuzzy RDD settings with appropriate choice of model or data. It may also be possible to deal with them using instrumental variables, which we cover in the next section.

\hypertarget{threats-to-validity}{%
\subsection{Threats to validity}\label{threats-to-validity}}

The continuity assumption is fairly important, but we cannot test this as it is based on a counterfactual. Instead we need to convince people of it. Ways to do this include:

\begin{itemize}
\tightlist
\item
  Using a test/train set-up.
\item
  Trying different specifications (and be very careful if your results don't broadly persist when just consider linear or quadratic functions).
\item
  Considering different subsets of the data.
\item
  Consider different windows.
\item
  Be up-front about uncertainty intervals, especially in graphs.
\item
  Discuss and assuage concerns about the possibility of omitted variables.
\end{itemize}

The threshold is also important. For instance, is there an actual shift or is there a non-linear relationship?

We want as `sharp' an effect as possible, but if the thresholds are known, then they will be gamed. For instance, there is a lot of evidence that people run for certain marathon times, and we know that people aim for certain grades. Similarly, from the other side, it is a lot easier for an instructor to just give out As than it is to have to justify Bs. One way to look at this is to consider how `balanced' the sample is on either side of the threshold. Do this by using histograms with appropriate bins, for instance Figure \ref{fig:marathontimes}, which is from \citet{allen2017reference}.

\begin{figure}
\includegraphics[width=0.9\linewidth]{figures/marathontimes} \caption{Bunching around marathon times.}\label{fig:marathontimes}
\end{figure}

You need to really think about the possible effect of the decision around the choice of model. To see this consider the difference between a linear and polynomial.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{some\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{outcome =} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{1}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
         \AttributeTok{running\_variable =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{),}
         \AttributeTok{location =} \StringTok{"before"}\NormalTok{)}

\NormalTok{some\_more\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{outcome =} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{2}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
         \AttributeTok{running\_variable =} \FunctionTok{c}\NormalTok{(}\DecValTok{101}\SpecialCharTok{:}\DecValTok{200}\NormalTok{),}
         \AttributeTok{location =} \StringTok{"after"}\NormalTok{)}

\NormalTok{both }\OtherTok{\textless{}{-}} 
  \FunctionTok{rbind}\NormalTok{(some\_data, some\_more\_data)}

\NormalTok{both }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ running\_variable, }\AttributeTok{y =}\NormalTok{ outcome, }\AttributeTok{color =}\NormalTok{ location)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x, }\AttributeTok{method =} \StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{42-causality_from_obs_files/figure-latex/unnamed-chunk-30-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
  
\NormalTok{both }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ running\_variable, }\AttributeTok{y =}\NormalTok{ outcome, }\AttributeTok{color =}\NormalTok{ location)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(x, }\DecValTok{3}\NormalTok{), }\AttributeTok{method =} \StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{42-causality_from_obs_files/figure-latex/unnamed-chunk-30-2.pdf}

\hypertarget{weaknesses}{%
\subsection{Weaknesses}\label{weaknesses}}

\begin{itemize}
\tightlist
\item
  External validity may be hard - think about the A-/B+ example - do you think the findings generalise to B-/C+?
\item
  The important responses are those that are close to the cut-off. So even if we have a whole bunch of B- and A+ students, they don't really help much. Hence we need a lot of data.
\item
  There is a lot of freedom for the researcher, so open science best practice becomes vital.
\end{itemize}

\hypertarget{case-study---stiers-hooghe-and-dassonneville-2020}{%
\section{Case study - Stiers, Hooghe, and Dassonneville, 2020}\label{case-study---stiers-hooghe-and-dassonneville-2020}}

Paper: Stiers, D., Hooghe, M. and Dassonneville, R., 2020. Voting at 16: Does lowering the voting age lead to more political engagement? Evidence from a quasi-experiment in the city of Ghent (Belgium). Political Science Research and Methods, pp.1-8. Available at: \url{https://www.cambridge.org/core/journals/political-science-research-and-methods/article/voting-at-16-does-lowering-the-voting-age-lead-to-more-political-engagement-evidence-from-a-quasiexperiment-in-the-city-of-ghent-belgium/172A2D9B75ECB66E98C9680787F302AD\#fndtn-information}

Data: \url{https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/J1FQW9}

\hypertarget{case-study---caughey-and-sekhon.-2011}{%
\section{Case study - Caughey, and Sekhon., 2011}\label{case-study---caughey-and-sekhon.-2011}}

Paper: Caughey, Devin, and Jasjeet S. Sekhon. ``Elections and the regression discontinuity design: Lessons from close US house races, 1942--2008.'' Political Analysis 19.4 (2011): 385-408. Available at: \url{https://www.cambridge.org/core/journals/political-analysis/article/elections-and-the-regression-discontinuity-design-lessons-from-close-us-house-races-19422008/E5A69927D29BE682E012CAE9BFD8AEB7}

Data: \url{https://dataverse.harvard.edu/dataset.xhtml?persistentId=hdl:1902.1/16357\&version=1.0}

\hypertarget{instrumental-variables}{%
\section{Instrumental variables}\label{instrumental-variables}}

\hypertarget{introduction-23}{%
\subsection{Introduction}\label{introduction-23}}

Instrumental variables (IV) is an approach that can be handy when we have some type of treatment and control going on, but we have a lot of correlation with other variables and we possibly don't have a variable that actually measures what we are interested in. So adjusting for observables will not be enough to create a good estimate. Instead we find some variable - the eponymous instrumental variable - that is:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  correlated with the treatment variable, but
\item
  not correlated with the outcome.
\end{enumerate}

This solves our problem because the only way the instrumental variable can have an effect is through the treatment variable, and so we are able to adjust our understanding of the effect of the treatment variable appropriately. The trade-off is that instrumental variables must satisfy a bunch of different assumptions, and that, frankly, they are difficult to identify \emph{ex ante}. Nonetheless, when you are able to use them they are a powerful tool for speaking about causality.

The canonical instrumental variables example is smoking. These days we know that smoking causes cancer. But because smoking is correlated with a lot of other variables, for instance, education, it could be that it was actually education that causes cancer. RCTs may be possible, but they are likely to be troublesome in terms of speed and ethics, and so instead we look for some other variable that is correlated with smoking, but not, in and of itself, with lung cancer. In this case, we look to tax rates, and other policy responses, on cigarettes. As the tax rates on cigarettes are correlated with the number of cigarettes that are smoked, but not correlated with lung cancer, other than through their impact on cigarette smoking, through them we can assess the effect of cigarettes smoked on lung cancer.

To implement instrumental variables we first regress tax rates on cigarette smoking to get some coefficient on the instrumental variable, and then (in a separate regression) regress tax rates on lung cancer to again get some coefficient on the instrumental variable. Our estimate is then the ratio of these coefficients. \citep[p.~219]{gelmanandhill} describe this ratio as the `Wald estimate'.

Following the language of \citep[p.~216]{gelmanandhill} when we use instrumental variables we make a variety of assumptions including:

\begin{itemize}
\tightlist
\item
  Ignorability of the instrument.
\item
  Correlation between the instrumental variable and the treatment variable.
\item
  Monotonicity.
\item
  Exclusion restriction.
\end{itemize}

To summarise exactly what instrumental variables is about, I cannot do better than recommend the first few pages of the `Instrumental Variables' chapter in \citet{Cunningham2021}, and this key paragraph in particular (by way of background, Cunningham has explained why it would have been impossible to randomly allocate `clean' and `dirty' water through a randomised controlled trial and then continues\ldots):

\begin{quote}
Snow would need a way to trick the data such that the allocation of clean and dirty water to people was not associated with the other determinants of cholera mortality, such as hygiene and poverty. He just would need for someone or something to be making this treatment assignment for him.

Fortunately for Snow, and the rest of London, that someone or something existed. In the London of the 1800s, there were many different water companies serving different areas of the city. Some were served by more than one company. Several took their water from the Thames, which was heavily polluted by sewage. The service areas of such companies had much higher rates of cholera. The Chelsea water company was an exception, but it had an exceptionally good filtration system. That's when Snow had a major insight. In 1849, Lambeth water company moved the intake point upstream along the Thames, above the main sewage discharge point, giving its customers purer water. Southwark and Vauxhall water company, on the other hand, left their intake point downstream from where the sewage discharged. Insofar as the kinds of people that each company serviced were approximately the same, then comparing the cholera rates between the two houses could be the experiment that Snow so desperately needed to test his hypothesis.
\end{quote}

\hypertarget{history}{%
\subsection{History}\label{history}}

The history of instrumental variables is a rare statistical mystery, and \citet{stock2003retrospectives} provide a brief overview. The method was first published in \citet{wright1928tariff}. This is a book about the effect of tariffs on animal and vegetable oil. So why might instrumental variables be important in a book about tariffs on animal and vegetable oil? The fundamental problem is that the effect of tariffs depends on both supply and demand. But we only know prices and quantities, so we don't know what is driving the effect. We can use instrumental variables to pin down causality.

Where is gets interesting, and becomes something of a mystery, is that the instrumental variables discussion is only in Appendix B. If you made a major statistical break-through would you hide it in an appendix? Further, Philip G. Wright, the book's author, had a son Sewall Wright, who had considerable expertise in statistics and the specific method used in Appendix B. Hence the mystery of Appendix B - did Philip or Sewall write it? Both \citet{Cunningham2021} and \citet{stock2003retrospectives} go into more detail, but on balance feel that it is likely that Philip did actually author the work.

\hypertarget{simulated-example-2}{%
\subsection{Simulated example}\label{simulated-example-2}}

Let's generate some data. We will explore a simulation related to the canonical example of health status, smoking, and tax rates. So we are looking to explain how healthy someone is based on the amount they smoke, via the tax rate on smoking. We are going to generate different tax rates by provinces. My understanding is that the tax rate on cigarettes is now pretty much the same in each of the provinces, but that this is fairly recent. So we'll pretend that Alberta had a low tax, and Nova Scotia had a high tax.

As a reminder, we are simulating data for illustrative purposes, so we need to impose the answer that we want. When you actually use instrumental variables you will be reversing the process.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(broom)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{number\_of\_observation }\OtherTok{\textless{}{-}} \DecValTok{10000}

\NormalTok{iv\_example\_data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{person =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{number\_of\_observation),}
                          \AttributeTok{smoker =} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{1}\NormalTok{),}
                                          \AttributeTok{size =}\NormalTok{ number\_of\_observation, }
                                          \AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{                          )}
\end{Highlighting}
\end{Shaded}

Now we need to relate the number of cigarettes that someone smoked to their health. We'll model health status as a draw from the normal distribution, with either a high or low mean depending on whether the person smokes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iv\_example\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  iv\_example\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{health =} \FunctionTok{if\_else}\NormalTok{(smoker }\SpecialCharTok{==} \DecValTok{0}\NormalTok{,}
                          \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \FunctionTok{n}\NormalTok{(), }\AttributeTok{mean =} \DecValTok{1}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
                          \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \FunctionTok{n}\NormalTok{(), }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}
\NormalTok{                          )}
\NormalTok{         )}
\DocumentationTok{\#\# So health will be one standard deviation higher for people who don\textquotesingle{}t or barely smoke.}
\end{Highlighting}
\end{Shaded}

Now we need a relationship between cigarettes and the province (because in this illustration, the provinces have different tax rates).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iv\_example\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  iv\_example\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rowwise}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{province =} \FunctionTok{case\_when}\NormalTok{(smoker }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\textasciitilde{}} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\StringTok{"Nova Scotia"}\NormalTok{, }\StringTok{"Alberta"}\NormalTok{),}
                                                                       \AttributeTok{size =} \DecValTok{1}\NormalTok{, }
                                                                       \AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{, }
                                                                       \AttributeTok{prob =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1}\SpecialCharTok{/}\DecValTok{2}\NormalTok{)),}
\NormalTok{                              smoker }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\StringTok{"Nova Scotia"}\NormalTok{, }\StringTok{"Alberta"}\NormalTok{),}
                                                                       \AttributeTok{size =} \DecValTok{1}\NormalTok{, }
                                                                       \AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{, }
                                                                       \AttributeTok{prob =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\DecValTok{4}\NormalTok{, }\DecValTok{3}\SpecialCharTok{/}\DecValTok{4}\NormalTok{)))) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{()}

\NormalTok{iv\_example\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  iv\_example\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{tax =} \FunctionTok{case\_when}\NormalTok{(province }\SpecialCharTok{==} \StringTok{"Alberta"} \SpecialCharTok{\textasciitilde{}} \FloatTok{0.3}\NormalTok{,}
\NormalTok{                         province }\SpecialCharTok{==} \StringTok{"Nova Scotia"} \SpecialCharTok{\textasciitilde{}} \FloatTok{0.5}\NormalTok{,}
                         \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \DecValTok{9999999}
\NormalTok{  )}
\NormalTok{  )}

\NormalTok{iv\_example\_data}\SpecialCharTok{$}\NormalTok{tax }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{table}\NormalTok{()}
\CommentTok{\#\textgreater{} .}
\CommentTok{\#\textgreater{}  0.3  0.5 }
\CommentTok{\#\textgreater{} 6206 3794}

\FunctionTok{head}\NormalTok{(iv\_example\_data)}
\end{Highlighting}
\end{Shaded}

\label{tab:unnamed-chunk-33}

person

smoker

health

province

tax

1

0

1.11~~

Alberta

0.3

2

1

-0.0831

Alberta

0.3

3

1

-0.0363

Alberta

0.3

4

0

2.48~~

Alberta

0.3

5

0

0.617~

Alberta

0.3

6

0

0.748~

Nova Scotia

0.5

Now we can look at our data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iv\_example\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{smoker =} \FunctionTok{as\_factor}\NormalTok{(smoker)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ health, }\AttributeTok{fill =}\NormalTok{ smoker)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{, }\AttributeTok{binwidth =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Health rating"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of people"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Smoker"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(province))}
\end{Highlighting}
\end{Shaded}

\includegraphics{42-causality_from_obs_files/figure-latex/unnamed-chunk-34-1.pdf}

Finally, we can use the tax rate as an instrumental variable to estimate the effect of smoking on health.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{health\_on\_tax }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(health }\SpecialCharTok{\textasciitilde{}}\NormalTok{ tax, }\AttributeTok{data =}\NormalTok{ iv\_example\_data)}
\NormalTok{smoker\_on\_tax }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(smoker }\SpecialCharTok{\textasciitilde{}}\NormalTok{ tax, }\AttributeTok{data =}\NormalTok{ iv\_example\_data)}

\FunctionTok{coef}\NormalTok{(health\_on\_tax)[}\StringTok{"tax"}\NormalTok{] }\SpecialCharTok{/} \FunctionTok{coef}\NormalTok{(smoker\_on\_tax)[}\StringTok{"tax"}\NormalTok{]}
\CommentTok{\#\textgreater{}        tax }
\CommentTok{\#\textgreater{} {-}0.8554502}
\end{Highlighting}
\end{Shaded}

So we find, luckily, that if you smoke then your health is likely to be worse than if you don't smoke.

Equivalently, we can think of instrumental variables in a two-stage regression context.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{first\_stage }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(smoker }\SpecialCharTok{\textasciitilde{}}\NormalTok{ tax, }\AttributeTok{data =}\NormalTok{ iv\_example\_data)}
\NormalTok{health\_hat }\OtherTok{\textless{}{-}}\NormalTok{ first\_stage}\SpecialCharTok{$}\NormalTok{fitted.values}
\NormalTok{second\_stage }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(health }\SpecialCharTok{\textasciitilde{}}\NormalTok{ health\_hat, }\AttributeTok{data =}\NormalTok{ iv\_example\_data)}

\FunctionTok{summary}\NormalTok{(second\_stage)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = health \textasciitilde{} health\_hat, data = iv\_example\_data)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}3.9867 {-}0.7600  0.0068  0.7709  4.3293 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)  0.91632    0.04479   20.46   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} health\_hat  {-}0.85545    0.08911   {-}9.60   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  }
\CommentTok{\#\textgreater{} 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 1.112 on 9998 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.009134,   Adjusted R{-}squared:  0.009034 }
\CommentTok{\#\textgreater{} F{-}statistic: 92.16 on 1 and 9998 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

\hypertarget{implementation-1}{%
\subsection{Implementation}\label{implementation-1}}

As with regression discontinuity, although it is possible to use existing functions, it might be worth looking at specialised packages. Instrumental variables has a few moving pieces, so a specialised package can help keep everything organised, and additionally, standard errors need to be adjusted and specialised packages make this easier. The package \texttt{estimatr} is a recommendation, although there are others available and you should try those if you are interested. The \texttt{estimatr} package is from the same team as DeclareDesign.

Let's look at our example using \texttt{iv\_robust()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(estimatr)}
\FunctionTok{iv\_robust}\NormalTok{(health }\SpecialCharTok{\textasciitilde{}}\NormalTok{ smoker }\SpecialCharTok{|}\NormalTok{ tax, }\AttributeTok{data =}\NormalTok{ iv\_example\_data) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} iv\_robust(formula = health \textasciitilde{} smoker | tax, data = iv\_example\_data)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Standard error type:  HC2 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value   Pr(\textgreater{}|t|) CI Lower}
\CommentTok{\#\textgreater{} (Intercept)   0.9163    0.04057   22.59 3.163e{-}110   0.8368}
\CommentTok{\#\textgreater{} smoker       {-}0.8555    0.08047  {-}10.63  2.981e{-}26  {-}1.0132}
\CommentTok{\#\textgreater{}             CI Upper   DF}
\CommentTok{\#\textgreater{} (Intercept)   0.9958 9998}
\CommentTok{\#\textgreater{} smoker       {-}0.6977 9998}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.1971 ,    Adjusted R{-}squared:  0.197 }
\CommentTok{\#\textgreater{} F{-}statistic:   113 on 1 and 9998 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

\hypertarget{assumptions-1}{%
\subsection{Assumptions}\label{assumptions-1}}

As discussed earlier, there are a variety of assumptions that are made when using instrumental variables. The two most important are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Exclusion Restriction. This assumption is that the instrumental variable only affects the dependent variable through the independent variable of interest.
\item
  Relevance. There must actually be a relationship between the instrumental variable and the independent variable.
\end{enumerate}

There is typically a trade-off between these two. There are plenty of variables that

When thinking about potential instrumental variables \citet{Cunningham2021}, p.~211, puts it brilliantly:

\begin{quote}
But, let's say you think you do have a good instrument. How might you defend it as such to someone else? A necessary but not a sufficient condition for having an instrument that can satisfy the exclusion restriction is if people are confused when you tell them about the instrument's relationship to the outcome. Let me explain. No one is going to be confused when you tell them that you think family size will reduce female labor supply. They don't need a Becker model to convince them that women who have more children probably work less than those with fewer children. It's common sense. But, what would they think if you told them that mothers whose first two children were the same gender worked less than those whose children had a balanced sex ratio? They would probably give you a confused look. What does the gender composition of your children have to do with whether a woman works?

It doesn't -- it only matters, in fact, if people whose first two children are the same gender decide to have a third child. Which brings us back to the original point -- people buy that family size can cause women to work less, but they're confused when you say that women work less when their first two kids are the same gender. But if when you point out to them that the two children's gender induces people to have larger families than they would have otherwise, the person
``gets it'', then you might have an excellent instrument.
\end{quote}

Relevance can be tested using regression and other tests for correlation. The exclusion restriction cannot be tested. You need to present evidence and convincing arguments. As \citet{Cunningham2021} p.~225 says `Instruments have a certain ridiculousness to them{[}.{]} That is, you know you have a good instrument if the instrument itself doesn't seem relevant for explaining the outcome of interest because that's what the exclusion restriction implies.'

\hypertarget{conclusion}{%
\subsection{Conclusion}\label{conclusion}}

Instrumental variables is a useful approach because one can obtain causal estimates even without explicit randomisation. Finding instrumental variables used to be a bit of a white whale, especially in academia. However, I will leave the final (and hopefully motivating) word to \citet{taddy2019}, p.~162:

\begin{quote}
As a final point on the importance of IV models and analysis, note that when you are on the inside of a firm---especially on the inside of a modern technology firm---explicitly randomised instruments are everywhere\ldots. But it is often the case that decision-makers want to understand the effects of policies that are not themselves randomised but are rather downstream of the things being AB tested. For example, suppose an algorithm is used to predict the creditworthiness of potential borrowers and assign loans. Even if the process of loan assignment is never itself randomised, if the parameters in the machine learning algorithms used to score credit are AB tested, then those experiments can be used as instruments for the loan assignment treatment. Such `upstream randomisation' is extremely common and IV analysis is your key tool for doing causal inference in that setting.'
\end{quote}

\hypertarget{case-study---effect-of-police-on-crime}{%
\section{Case study - Effect of Police on Crime}\label{case-study---effect-of-police-on-crime}}

\hypertarget{overview-5}{%
\subsection{Overview}\label{overview-5}}

Here we'll use an example of \citet{levitt2002using} that looks at the effect of police on crime. This is interesting because you might think, that more police is associated with lower crime. But, it could actually be the opposite, if more crime causes more police to be hired - how many police would a hypothetical country with no crime need? Hence there is a need to find some sort of instrumental variable that affects crime only through its relationship with the number of police (that is, not in and of itself, related to crime), and yet is also correlated with police numbers. \citet{levitt2002using} suggests the number of firefighters in a city.

\citet{levitt2002using} argues that firefighters are appropriate as an instrument, because `(f)actors such as the power of public sector unions, citizen tastes for government services, affirmative action initiatives, or a mayor's desire to provide spoils might all be expected to jointly influence the number of firefighters and police.'. \citet{levitt2002using} also argues that the relevance assumption is met by showing that `changes in the number of police officers and firefighters within a city are highly correlated over time'.

In terms of satisfying the exclusion restriction, \citet{levitt2002using} argues that the number of firefighters should not have a `direct impact on crime.' However, it may be that there are common factors, and so \citet{levitt2002using} adjusts for this in the regression.

\hypertarget{data-2}{%
\subsection{Data}\label{data-2}}

The dataset is based on 122 US cities between 1975 and 1995. Summary statistics are provided in Figure \ref{fig:levittcrime}.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/levitt_summary_stats} \caption{Summary statistics for Levitt 2002.}\label{fig:levittcrime}
\end{figure}

Source: \citet{levitt2002using} p.~1,246.

\hypertarget{model-2}{%
\subsection{Model}\label{model-2}}

In the first stage \citet{levitt2002using} looks at police as a function of firefighters, and a bunch of adjustment variables:
\[\ln(\mbox{Police}_{ct}) = \gamma \ln(\mbox{Fire}_{ct}) + X'_{ct}\Gamma + \lambda_t + \phi_c + \epsilon_{ct}.\]
The important part of this is the police and firefighters numbers which are on a per capita basis. There are a bunch of adjustment variables in \(X\) which includes things like state prisoners per capita, the unemployment rate, etc, as well as year dummy variables and fixed-effects for each city.

Having established the relationship between police and firefights, \citet{levitt2002using} can then use the estimates of the number of police, based on the number of firefighters, to explain crime rates:
\[\Delta\ln(\mbox{Crime}_{ct}) = \beta_1 \ln(\mbox{Police}_{ct-1}) + X'_{ct}\Gamma + \Theta_c + \mu_{ct}.\]

The typical way to present instrumental variable results is to show both stages. Figure \ref{fig:levittcrimefirst} shows the relationship between police and firefighters.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/levitt_first} \caption{The relationship between firefighters, police and crime.}\label{fig:levittcrimefirst}
\end{figure}

Source: \citet{levitt2002using} p.~1,247.

And then Figure \ref{fig:levittcrimesecond} shows the relationship between police and crime, where is it the IV results that are the ones of interest.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/levitt_second} \caption{The impact of police on crime.}\label{fig:levittcrimesecond}
\end{figure}

Source: \citet{levitt2002using} p.~1,248.

\hypertarget{discussion-1}{%
\subsection{Discussion}\label{discussion-1}}

The key finding of \citet{levitt2002using} is that there is a negative effect of the number of police on the amount of crime.

There are a variety of points that I want to raise in regard to this paper. They will come across as a little negative, but this is mostly just because this a paper from 2002, that I am reading today, and so the standards have changed.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  It's fairly remarkable how reliant on various model specifications the results are. The results bounce around a fair bit and that's just the ones that are reported. Chances are there are a bunch of other results that were not reported, but it would be of interest to see their impact.
\item
  On that note, there is fairly limited model validation. This is probably something that I am more aware of these days, but it seems likely that there is a fair degree of over-fitting here.
\item
  \citet{levitt2002using} is actually a response, after another researcher, \citet{mccrary2002using}, found some issues with the original paper: \citet{levitt87using}. While Levitt appears quite decent about it, it is jarring to see that Levitt was thanked by \citet{mccrary2002using} for providing `both the data and computer code.' What if Levitt had not been decent about providing the data and code? Or what if the code was unintelligible? In some ways it is nice to see how far that we have come - the author of a similar paper these days would be forced to make their code and data available as part of the paper, we wouldn't have to ask them for it. But it reinforces the importance of open data and reproducible science.
\end{enumerate}

\hypertarget{exercises-and-tutorial-15}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-15}}

\hypertarget{exercises-15}{%
\subsection{Exercises}\label{exercises-15}}

\hypertarget{tutorial-15}{%
\subsection{Tutorial}\label{tutorial-15}}

\hypertarget{mrp}{%
\chapter{Multilevel regression with post-stratification}\label{mrp}}

\textbf{STATUS: Under construction.}

\textbf{Required material}

\begin{itemize}
\tightlist
\item
  Read \emph{Analyzing name changes after marriage using a non-representative survey}, \citep{monicanamechanges}.
\item
  Read Chapter 17 of \emph{Regression and Other Stories}, \citep{gelmanhillvehtari2020}.
\item
  Read \emph{An introduction to multilevel regression and post-stratification for estimating constituency opinion}, \citep{hanretty2020}.
\item
  Read the Introduction from \emph{Estimating State Public Opinion With Multi-Level Regression and Poststratification using R}, \citep{kastelleclaxphillips}.
\item
  Read \emph{Using sex and gender in survey adjustment}, \citep{kennedy2020using}.
\item
  Read \emph{MRP with rstanarm}, \citep{kennedygabry2020}.
\item
  Read \emph{Know your population and know your model: Using model-based regression and poststratification to generalize findings beyond the observed sample}, \citep{kennedy2020know}.
\item
  Read \emph{Forecasting elections with non-representative polls}, \citep{wang2015forecasting}.
\item
  Read Chapter 17 of \emph{Sampling Theory and Practice}, \citep{wuandthompson}.
\item
  Watch \emph{Statistical Models of Election Outcomes}, \citep{gelmantalks}.
\item
  Listen to \emph{Episode 248: Are Democrats being irrational? (David Shor)}, \citep{galefandshor}.
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
  Arnold, Jeffrey B., 2018, `Simon Jackman's Bayesian Model Examples in Stan', Ch 13, 7 May, \url{https://jrnold.github.io/bugs-examples-in-stan/campaign.html}.
\item
  Cohn, Nate, 2016, `We Gave Four Good Pollsters the Same Raw Data. They Had Four Different Results', \emph{The New York Times}, The Upshot, 20 September, \url{https://www.nytimes.com/interactive/2016/09/20/upshot/the-error-the-polling-world-rarely-talks-about.html}.
\item
  Edelman, M., Vittert, L., \& Meng, X.-L., 2021, `An Interview with Murray Edelman on the History of the Exit Poll', \emph{Harvard Data Science Review}, \url{https://doi.org/10.1162/99608f92.3a25cd24} \url{https://hdsr.mitpress.mit.edu/pub/fekmqbv4/release/2}.
\item
  Gelman, Andrew, and Julia Azari, 2017, `19 things we learned from the 2016 election', \emph{Statistics and Public Policy}, 4 (1), pp.~1-10.
\item
  Gelman, Andrew, Jessica Hullman, and Christopher Wlezien, 2020, `Information, incentives, and goals in election forecasts', 8 September, available at: \url{http://www.stat.columbia.edu/~gelman/research/unpublished/forecast_incentives3.pdf}
\item
  Gelman, Andrew, Merlin Heidemanns, and Elliott Morris, 2020, `2020 US POTUS model', The Economist, freely available: \url{https://github.com/TheEconomist/us-potus-model}.
\item
  Ghitza, Yair, and Andrew Gelman, 2013, `Deep Interactions with MRP: Election Turnout and Voting Patterns Among Small Electoral Subgroups', \emph{American Journal of Political Science}, 57 (3), pp.~762-776.
\item
  Ghitza, Yair, and Andrew Gelman, 2020, `Voter Registration Databases and MRP: Toward the Use of Large-Scale Databases in Public Opinion Research', \emph{Political Analysis}, pp.~1-25.
\item
  Imai, Kosuke, 2017, Quantitative Social Science: An Introduction, Princeton University Press, Ch 4.1, and 5.3.
\item
  Jackman, Simon, 2005, `Pooling the polls over an election campaign', \emph{Australian Journal of Political Science}, 40 (4), pp.~499-517.
\item
  Jackman, Simon, Shaun Ratcliff and Luke Mansillo, 2019, `Small area estimates of public opinion: Model-assisted post-stratification of data from voter advice applications', 4 January, \url{https://www.cambridge.org/core/membership/services/aop-file-manager/file/5c2f6ebb7cf9ee1118d11c0a/APMM-2019-Simon-Jackman.pdf}.
\item
  Lauderdale, Ben, Delia Bailey, Jack Blumenau, and Doug Rivers, 2020, `Model-based pre-election polling for national and sub-national outcomes in the US and UK', \emph{International Journal of Forecasting}, 36 (2), pp.~399-413.
\item
  Leigh, Andrew, and Justin Wolfers, 2006, `Competing approaches to forecasting elections: Economic models, opinion polling and prediction markets', Economic Record, 82 (258), pp.325-340.
\item
  Nickerson, David W., and Todd Rogers, 2014, `Political campaigns and big data', Journal of Economic Perspectives, 28 (2), pp.~51-74.
\item
  Shirani-Mehr, Houshmand, David Rothschild, Sharad Goel, and Andrew Gelman, 2018, `Disentangling bias and variance in election polls', Journal of the American Statistical Association, 113 (522), pp.~607-614.
\end{itemize}

\textbf{Recommended viewing}

\begin{itemize}
\tightlist
\item
  Jackman, Simon, 2020, `The triumph of the quants?: Model-based poll aggregation for election forecasting', \emph{Ihaka Lecture Series}, \url{https://youtu.be/MvGYsKIsLFs}.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{brms}
\item
  \texttt{broom}
\item
  \texttt{gtsummary}
\item
  \texttt{haven}
\item
  \texttt{labelled}
\item
  \texttt{lme4}
\item
  \texttt{modelsummary}
\item
  \texttt{rstanarm}
\item
  \texttt{tidybayes}
\item
  \texttt{tidyverse}
\end{itemize}

\textbf{Quiz}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Your Mum asked you what you've been learning this term. You decide to tell her about multilevel regression with post-stratification (MRP). Please explain what MRP is. Your Mum has a university-education, but has not necessarily taken any statistics, so you will need to explain any technical terms that you use. {[}Please write two or three paragraphs; strong answers would be clear about both strengths and weaknesses.{]}
\item
  With respect to \citet{wang2015forecasting}: Why is this paper interesting? What do you like about this paper? What do you wish it did better? To what extent can you reproduce this paper? {[}Please write one or two paragraphs about each aspect.{]}
\item
  With respect to \citet{wang2015forecasting}, what is not a feature they mention election forecasts need?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Explainable.
  \item
    Accurate.
  \item
    Cost-effective.
  \item
    Relevant.
  \item
    Timely.
  \end{enumerate}
\item
  With respect to \citet{wang2015forecasting}, what is a weakness of MRP?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Detailed data requirement.
  \item
    Allows use of biased data.
  \item
    Expensive to conduct.
  \end{enumerate}
\item
  With respect to \citet{wang2015forecasting}, what is concerning about the Xbox sample?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Non-representative.
  \item
    Small sample size.
  \item
    Multiple responses from the same respondent.
  \end{enumerate}
\item
  I am interested in studying how voting intentions in the 2020 US presidential election vary by an individual's income. I set up a logistic regression model to study this relationship. In my study, some possible independent variables would be: {[}Please check all that apply.{]}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Whether the respondent is registered to vote (yes/no).
  \item
    Whether the respondent is going to vote for Biden (yes/no).
  \item
    The race of the respondent (white/not white).
  \item
    The respondent's marital status (married/not).
  \end{enumerate}
\item
  Please think about \citet{cohn2016} Why is this type of exercise not carried out more? Why do you think that different groups, even with the same background and level of quantitative sophistication, could have such different estimates even when they use the same data? {[}Please write a paragraph or two about each aspect.{]}
\item
  When we think about multilevel regression with post-stratification, what are the key assumptions that we are making? {[}Please write one or two paragraphs about each aspect.{]}
\item
  I train a model based on a survey, and then post-stratify it using the 2020 ACS dataset. What are some of the practical considerations that I may have to contend when I am doing this? {[}Please write a paragraph each about at least three considerations.{]}
\item
  In a similar manner to \citet{ghitza2020voter} pretend you've got access to a US voter file record from a private company. You train a model on the 2020 US CCES, and post-stratify it, on an individual-basis, based on that voter file.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Could you please put-together a datasheet for the voter file dataset following \citet{gebru2020datasheets}? As a reminder, datasheets accompany datasets and document `motivation, composition, collection process, recommended uses,' among other aspects.
  \item
    Could you also please put together a model card for your model, following \citet{Mitchell_2019}? As a reminder, model cards are deliberately straight-forward one- or two-page documents that report aspects such as: model details; intended use; metrics; training data; ethical considerations; as well as caveats and recommendations \citep{Mitchell_2019}.
  \item
    Could you please discuss three ethical aspects around the features that you are using in your model? {[}Please write a paragraph or two for each point.{]}
  \item
    Could you please detail the protections that you would put in place in terms of the dataset, the model, and the predictions?
  \end{enumerate}
\item
  If I have a model output from \texttt{lm()} called `my\_model\_output' how can I use \texttt{modelsummary} to display the output (assume the package has been loaded) {[}please select all that apply{]}?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{modelsummary::modelsummary(my\_model\_output)}
  \item
    \texttt{modelsummary(my\_model\_output)}
  \item
    \texttt{my\_model\_output\ \%\textgreater{}\%\ modelsummary()}
  \item
    \texttt{my\_model\_output\ \%\textgreater{}\%\ modelsummary(statistic\ =\ NULL)}
  \end{enumerate}
\item
  Which of the following are examples of linear models {[}please select all that apply{]}?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{lm(y\ \textasciitilde{}\ x\_1\ +\ x\_2\ +\ x\_3,\ data\ =\ my\_data)}
  \item
    \texttt{lm(y\ \textasciitilde{}\ x\_1\ +\ x\_2\^{}2\ +\ x\_3,\ data\ =\ my\_data)}
  \item
    \texttt{lm(y\ \textasciitilde{}\ x\_1\ *\ x\_2\ +\ x\_3,\ data\ =\ my\_data)}
  \item
    \texttt{lm(y\ \textasciitilde{}\ x\_1\ +\ x\_1\^{}2\ +\ x\_2\ +\ x\_3,\ data\ =\ my\_data)}
  \end{enumerate}
\item
  Consider a situation in which you have a survey dataset with these age-groups: 18-29; 30-44; 45- 60; and 60+. And a post-stratification dataset with these age-groups: 18-24; 25-29; 30-34; 35-39; 40-44; 45-49; 50-54; 55-59; and 60+. What approach would you take to bringing these together? {[}Please write a paragraph.{]}
\item
  Consider a situation in which you again have a survey dataset with these age-groups: 18-29; 30-44; 45- 60; and 60+. But this time the post-stratification dataset has these age-groups: 18-34; 35-49; 50-64; and 65+. What approach would you take to bringing these together? {[}Please write a paragraph.{]}
\item
  Please consider \citet{kennedy2020using}. What are some statistical facets when considering a survey focused on gender, with a post-stratification survey that is not? {[}Please check all that apply.{]}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Impute all non-male as female
  \item
    Estimate gender using auxiliary information
  \item
    Impute population
  \item
    Impute sample values
  \item
    Model population distribution using auxiliary data
  \item
    Remove all non-binary respondents
  \item
    Remove respondents
  \item
    Assume population distribution
  \end{enumerate}
\item
  Please consider \citet{kennedy2020using}. What are some ethical facets when considering a survey focused on gender, with a post-stratification survey that is not? {[}Please check all that apply.{]}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Impute all non-male as female
  \item
    Estimate gender using auxiliary information
  \item
    Impute population
  \item
    Impute sample values
  \item
    Model population distribution using auxiliary data
  \item
    Remove all non-binary respondents
  \item
    Remove respondents
  \item
    Assume population distribution
  \end{enumerate}
\item
  Please consider \citet{kennedy2020using}. How do they define ethics?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Respecting the perspectives and dignity of individual survey respondents.
  \item
    Generating estimates of the general population and for subpopulations of interest.
  \item
    Using more complicated procedures only when they serve some useful function.
  \end{enumerate}
\end{enumerate}

\hypertarget{introduction-24}{%
\section{Introduction}\label{introduction-24}}

\begin{quote}
{[}The Presidential election of{]} 2016 was the largest analytics failure in US political history.

David Shor, 13 August 2020
\end{quote}

Multilevel regression with post-stratification (MRP) is a popular way to adjust non-representative samples to better analyse opinion and other survey responses.\footnote{I'd like to acknowledge and thank Lauren Kennedy and \href{https://www.monicaalexander.com}{Monica Alexander}, through whose generous sharing of code, data, and countless conversations my thoughts about MRP have developed.} It uses a regression model to relate individual-level survey responses to various characteristics and then rebuilds the sample to better match the population. In this way MRP can not only allow a better understanding of responses, but also allow us to analyse data that may otherwise be unusable. However, it can be a challenge to get started with MRP as the terminology may be unfamiliar, and the data requirements can be onerous.

Let's say that we have a biased survey. Maybe we conducted a survey about computers at an academic seminar, so folks with post-graduate degrees are likely over-represented. We are nonetheless interested in making claims about the population. Let's say that we found 37.5 per cent of our respondents prefer Macs. One way forward is to just ignore the bias and say that `37.5 per cent of people prefer Macs'. Another way is to say, well 50 per cent of our respondents with a post-graduate degree prefer Macs, and of those without a post-graduate degree, 25 per cent prefer Macs. If we knew what proportion of the broader population has post-graduate degree, let's assume 10 per cent, then we could conduct re-weighting, or post-stratification, as follows: \(0.5 * 0.1 + 0.25 * 0.9 = 0.275\), and so our estimate is that 27.5 per cent of people prefer Macs. MRP is a third approach, and uses a model to help do that re-weighting. So we use logistic regression to estimate the relationship between preferring Macs and highest educational attainment in our survey. We then apply that relationship to population dataset.

MRP is a handy approach when dealing with survey data. \citet{hanretty2020} puts it well when he says `MRP is used because the alternatives are either very poor or very expensive.'. Essentially, it trains a model based on the survey, and then applies that trained model to another dataset. There are two main, related, advantages:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  It can allow us to `re-weight' in a way that includes uncertainty front-of-mind and isn't as hamstrung by small samples. The alternative way to deal with having a small sample is to either go and gather more data or throw it away.
\item
  It can allow us to use broad surveys to speak to subsets. As \citet{hanretty2020} says `A poor alternative {[}to using MRP{]} is simply splitting a large sample into (much) smaller geographic subsamples. This is a poor alternative because there is no guarantee that a sample which is representative at the national level will be representative when it is broken down into smaller groups.'.
\end{enumerate}

From a practical perspective, it tends to be less expensive to collect non-probability samples and so there are benefits of being able to use these types of data. That said, it is not a magic-bullet and the laws of statistics still apply. We will have larger uncertainty around our estimates and they will still be subject to all the usual biases. As \href{https://jazzystats.com}{Lauren Kennedy} points out, `MRP has traditionally been used in probability surveys and had potential for non-probability surveys, but we're not sure of the limitations at the moment.' It's an exciting area of research in both academia and industry.

The workflow that you need for MRP is straight-forward, but the details and tiny decisions that have to be made at each step can become overwhelming. The point that you need to keep in mind is that you are trying to create a relationship between two datasets using a statistical model, and so you need to establish similarity between the two datasets in terms of their variables and levels. The steps are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  gather and prepare the survey dataset, thinking about what is needed for coherence with the post-stratification dataset;
\item
  gather and prepare the post-stratification dataset thinking about what is needed for coherence with the survey dataset;
\item
  model the variable of interest from the survey using independent variables and levels that are available in both the survey and the post-stratification datasets;
\item
  apply the model to the post-stratification data.
\end{enumerate}

In these notes, we begin with simulating a situation in which we pretend that we know the features of the population. We then move to a famous example of MRP that used survey data from the Xbox platform and exit poll data to forecast the 2012 US election. We will then move to examples from the Australian political situation. We will then discuss some features to be aware of when conducting MRP.

\hypertarget{simulation---toddler-bedtimes}{%
\section{Simulation - Toddler bedtimes}\label{simulation---toddler-bedtimes}}

\hypertarget{construct-a-population}{%
\subsection{Construct a population}\label{construct-a-population}}

To get started we will simulate some data from a population that has various properties, take a biased sample, and then conduct MRP to demonstrate how we can get those properties back. We are going to have two `explanatory variables' - age-group and toilet-trained - and one dependent variable - bedtime. Bed-time will increase as age-group increases, and will be later for children that are toilet-trained, compared with those that are not. To be clear, in this example we will `know' the `true' features of the population, but this isn't something that occurs when we use real data - it is just to help you understand what MRP is doing. We're going to rely heavily on the \texttt{tidyverse} package \citep{citetidyverse}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Uncomment this (by deleting the \#) if you need to install the packages}
\CommentTok{\# install.packages(\textquotesingle{}tidyverse\textquotesingle{})}
\FunctionTok{library}\NormalTok{(tidyverse)}

\CommentTok{\# This helps reproducibility}
\CommentTok{\# It makes it more likely that you\textquotesingle{}re able to get the same random numbers as in this example.}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\CommentTok{\# One million people in our population.}
\NormalTok{size\_of\_population }\OtherTok{\textless{}{-}} \DecValTok{1000000}

\NormalTok{population\_for\_mrp\_example }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{age\_group =} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{), }\CommentTok{\# Draw from any of 1, 2, 3.}
                            \AttributeTok{size =}\NormalTok{ size\_of\_population,}
                            \AttributeTok{replace =} \ConstantTok{TRUE} \CommentTok{\# After you draw a number, allow that number to be drawn again.}
\NormalTok{                            ),}
         \AttributeTok{toilet\_trained =} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{),}
                                 \AttributeTok{size =}\NormalTok{ size\_of\_population,}
                                 \AttributeTok{replace =} \ConstantTok{TRUE}
\NormalTok{                                 ),}
         \AttributeTok{noise =} \FunctionTok{rnorm}\NormalTok{(size\_of\_population, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{), }
         \AttributeTok{bed\_time =} \DecValTok{5} \SpecialCharTok{+} \FloatTok{0.5} \SpecialCharTok{*}\NormalTok{ age\_group }\SpecialCharTok{+} \DecValTok{1} \SpecialCharTok{*}\NormalTok{ toilet\_trained }\SpecialCharTok{+}\NormalTok{ noise, }\CommentTok{\# Make bedtime a linear function of the variables that we just generated and a intercept (no special reason for that intercept to be five; it could be anything).}
\NormalTok{         ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{noise) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{age\_group =} \FunctionTok{as\_factor}\NormalTok{(age\_group),}
         \AttributeTok{toilet\_trained =} \FunctionTok{as\_factor}\NormalTok{(toilet\_trained)}
\NormalTok{         )}

\NormalTok{population\_for\_mrp\_example }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{}   age\_group toilet\_trained bed\_time}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}     \textless{}fct\textgreater{}             \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 1         0                  5.74}
\CommentTok{\#\textgreater{} 2 2         1                  6.48}
\CommentTok{\#\textgreater{} 3 1         0                  6.53}
\CommentTok{\#\textgreater{} 4 1         1                  5.39}
\CommentTok{\#\textgreater{} 5 1         1                  8.40}
\CommentTok{\#\textgreater{} 6 3         0                  6.54}
\end{Highlighting}
\end{Shaded}

At this point, Figure \ref{fig:bernie} provides invaluable advice (thank you to A Mahfouz).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/book/figures/bernie} \caption{What does Bernie ask us to do?}\label{fig:bernie}
\end{figure}

That is, as always, when we have a dataset, we first try to plot it to better understand what is going on (as there are a million points, I'll just grab the first 1,000 so that it plots nicely).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{population\_for\_mrp\_example }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{1000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{y =}\NormalTok{ bed\_time)) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ toilet\_trained), }
              \AttributeTok{alpha =} \FloatTok{0.4}\NormalTok{, }
              \AttributeTok{width =} \FloatTok{0.1}\NormalTok{, }
              \AttributeTok{height =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age{-}group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Bed time"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Toilet trained"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{43-mrp_files/figure-latex/unnamed-chunk-2-1.pdf}

And we can also work out what the `truth' is for the information that we are interested in (remembering that we'd never actually know this when we move away from simulated examples).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{population\_for\_mrp\_example\_summarised }\OtherTok{\textless{}{-}} 
\NormalTok{  population\_for\_mrp\_example }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(age\_group, toilet\_trained) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{median\_bed\_time =} \FunctionTok{median}\NormalTok{(bed\_time)) }

\NormalTok{population\_for\_mrp\_example\_summarised }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\AttributeTok{digits =} \DecValTok{2}\NormalTok{,}
               \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Age{-}group"}\NormalTok{, }\StringTok{"Is toilet trained"}\NormalTok{, }\StringTok{"Average bed time"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|r}
\hline
Age-group & Is toilet trained & Average bed time\\
\hline
1 & 0 & 5.50\\
\hline
1 & 1 & 6.50\\
\hline
2 & 0 & 6.00\\
\hline
2 & 1 & 7.00\\
\hline
3 & 0 & 6.50\\
\hline
3 & 1 & 7.51\\
\hline
\end{tabular}

\hypertarget{get-a-biased-sample-from-it}{%
\subsection{Get a biased sample from it}\label{get-a-biased-sample-from-it}}

Now we want to pretend that we have some survey that has a biased sample. We'll allow that it over-samples children that are younger and those that are not toilet-trained. For instance, perhaps we gathered our sample based on the records of a paediatrician, so it's more likely that they will see this biased sample of children. We are interested in knowing what proportion of children are toilet-trained at various age-groups.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Thanks to Monica Alexander}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\CommentTok{\# Add a weight for each \textquotesingle{}type\textquotesingle{} (has to sum to one)}
\NormalTok{population\_for\_mrp\_example }\OtherTok{\textless{}{-}} 
\NormalTok{  population\_for\_mrp\_example }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{weight =} 
           \FunctionTok{case\_when}\NormalTok{(toilet\_trained }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&}\NormalTok{ age\_group }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \FloatTok{0.7}\NormalTok{,}
\NormalTok{                     toilet\_trained }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\textasciitilde{}} \FloatTok{0.1}\NormalTok{,}
\NormalTok{                     age\_group }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \FloatTok{0.2}
\NormalTok{                     ),}
         \AttributeTok{id =} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{n}\NormalTok{()}
\NormalTok{         )}

\NormalTok{get\_these }\OtherTok{\textless{}{-}} 
  \FunctionTok{sample}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ population\_for\_mrp\_example}\SpecialCharTok{$}\NormalTok{id,}
    \AttributeTok{size =} \DecValTok{1000}\NormalTok{,}
    \AttributeTok{prob =}\NormalTok{ population\_for\_mrp\_example}\SpecialCharTok{$}\NormalTok{weight}
\NormalTok{    )}

\NormalTok{sample\_for\_mrp\_example }\OtherTok{\textless{}{-}} 
\NormalTok{  population\_for\_mrp\_example }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(id }\SpecialCharTok{\%in\%}\NormalTok{ get\_these) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{weight, }\SpecialCharTok{{-}}\NormalTok{id)}

\CommentTok{\# Clean up}
\NormalTok{poststratification\_dataset }\OtherTok{\textless{}{-}} 
\NormalTok{  population\_for\_mrp\_example }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{weight, }\SpecialCharTok{{-}}\NormalTok{id)}
\end{Highlighting}
\end{Shaded}

And we can plot those also.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_for\_mrp\_example }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{toilet\_trained =} \FunctionTok{as\_factor}\NormalTok{(toilet\_trained)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{y =}\NormalTok{ bed\_time)) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ toilet\_trained), }\AttributeTok{alpha =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{width =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{height =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age{-}group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Bed time"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Toilet trained"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{43-mrp_files/figure-latex/unnamed-chunk-5-1.pdf}

It's pretty clear that our sample has a different bedtime than the overall population, but let's just do the same exercise as before to look at the median, by age and toilet-trained status.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_for\_mrp\_example\_summarized }\OtherTok{\textless{}{-}} 
\NormalTok{  sample\_for\_mrp\_example }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(age\_group, toilet\_trained) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{median\_bed\_time =} \FunctionTok{median}\NormalTok{(bed\_time))}

\NormalTok{sample\_for\_mrp\_example\_summarized }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\AttributeTok{digits =} \DecValTok{2}\NormalTok{,}
               \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Age{-}group"}\NormalTok{, }\StringTok{"Is toilet trained"}\NormalTok{, }\StringTok{"Average bed time"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|r}
\hline
Age-group & Is toilet trained & Average bed time\\
\hline
1 & 0 & 5.41\\
\hline
1 & 1 & 6.35\\
\hline
2 & 0 & 5.89\\
\hline
2 & 1 & 6.85\\
\hline
3 & 0 & 6.49\\
\hline
3 & 1 & 7.62\\
\hline
\end{tabular}

\hypertarget{model-the-sample}{%
\subsection{Model the sample}\label{model-the-sample}}

We will quickly train a model based on the (biased) survey. We'll use \texttt{modelsummary} \citep{citemodelsummary} to format our estimates.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(modelsummary)}

\NormalTok{mrp\_example\_model }\OtherTok{\textless{}{-}} 
\NormalTok{  sample\_for\_mrp\_example }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{lm}\NormalTok{(bed\_time }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age\_group }\SpecialCharTok{+}\NormalTok{ toilet\_trained, }\AttributeTok{data =}\NormalTok{ .)}

\NormalTok{mrp\_example\_model }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  modelsummary}\SpecialCharTok{::}\FunctionTok{modelsummary}\NormalTok{(}\AttributeTok{fmt =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}
\centering
\begin{tabular}[t]{lc}
\toprule
  & Model 1\\
\midrule
(Intercept) & \num{5.47}\\
 & (\num{0.04})\\
age\_group2 & \num{0.54}\\
 & \vphantom{1} (\num{0.09})\\
age\_group3 & \num{1.15}\\
 & (\num{0.09})\\
toilet\_trained1 & \num{0.91}\\
 & (\num{0.07})\\
\midrule
Num.Obs. & \num{1000}\\
R2 & \num{0.373}\\
R2 Adj. & \num{0.371}\\
AIC & \num{2839.3}\\
BIC & \num{2863.8}\\
Log.Lik. & \num{-1414.630}\\
F & \num{197.594}\\
\bottomrule
\end{tabular}
\end{table}

This is the `multilevel regression' part of the MRP (although this isn't really a multilevel model just to keep things simple for now).

\hypertarget{get-a-post-stratification-dataset}{%
\subsection{Get a post-stratification dataset}\label{get-a-post-stratification-dataset}}

Now we will use a post-stratification dataset to get some estimates of the number in each cell. We typically use a larger dataset that may more closely reflection the population. In the US a popular choice is the ACS, while in other countries we typically have to use the census.

In this simulation example, I'll just take a 10 per cent sample from the population and use that as our post-stratification dataset.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{poststratification\_dataset }\OtherTok{\textless{}{-}} 
\NormalTok{  population\_for\_mrp\_example }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{100000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{bed\_time)}

\NormalTok{poststratification\_dataset }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 4}
\CommentTok{\#\textgreater{}   age\_group toilet\_trained weight    id}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}     \textless{}fct\textgreater{}           \textless{}dbl\textgreater{} \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 1         0                 0.7     1}
\CommentTok{\#\textgreater{} 2 2         1                 0.2     2}
\CommentTok{\#\textgreater{} 3 1         0                 0.7     3}
\CommentTok{\#\textgreater{} 4 1         1                 0.2     4}
\CommentTok{\#\textgreater{} 5 1         1                 0.2     5}
\CommentTok{\#\textgreater{} 6 3         0                 0.1     6}
\end{Highlighting}
\end{Shaded}

In an ideal world we have individual-level data in our post-stratification dataset (that's the case above). In that world we can apply our model to each individual. The more likely situation, in reality, is that we just have counts by groups, so we're going to try to construct an estimate for each group.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{poststratification\_dataset\_grouped }\OtherTok{\textless{}{-}} 
\NormalTok{  poststratification\_dataset }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(age\_group, toilet\_trained) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{()}

\NormalTok{poststratification\_dataset\_grouped }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{} \# Groups:   age\_group, toilet\_trained [6]}
\CommentTok{\#\textgreater{}   age\_group toilet\_trained     n}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}     \textless{}fct\textgreater{}          \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 1         0              16766}
\CommentTok{\#\textgreater{} 2 1         1              16649}
\CommentTok{\#\textgreater{} 3 2         0              16801}
\CommentTok{\#\textgreater{} 4 2         1              16617}
\CommentTok{\#\textgreater{} 5 3         0              16625}
\CommentTok{\#\textgreater{} 6 3         1              16542}
\end{Highlighting}
\end{Shaded}

\hypertarget{post-stratify-our-model-estimates}{%
\subsection{Post-stratify our model estimates}\label{post-stratify-our-model-estimates}}

Now we create an estimate for each group, and add some confidence intervals.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{poststratification\_dataset\_grouped }\OtherTok{\textless{}{-}} 
\NormalTok{  mrp\_example\_model }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{predict}\NormalTok{(}\AttributeTok{newdata =}\NormalTok{ poststratification\_dataset\_grouped, }\AttributeTok{interval =} \StringTok{"confidence"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{cbind}\NormalTok{(poststratification\_dataset\_grouped, .) }\CommentTok{\# The dot modifies the behaviour of the pipe; it pipes to the dot instead of the first argument as normal.}
\end{Highlighting}
\end{Shaded}

At this point we can have a look at our MRP estimates (circles) along with their confidence intervals, and compare them the raw estimates from the data (squares). In this case, because we know the truth, we can also compare them to the known truth (triangles) (but that's not something we can do normally).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{poststratification\_dataset\_grouped }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{y =}\NormalTok{ fit)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ population\_for\_mrp\_example\_summarised,}
             \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{y =}\NormalTok{ median\_bed\_time, }\AttributeTok{color =}\NormalTok{ toilet\_trained), }
             \AttributeTok{shape =} \DecValTok{17}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ sample\_for\_mrp\_example\_summarized,}
             \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{y =}\NormalTok{ median\_bed\_time, }\AttributeTok{color =}\NormalTok{ toilet\_trained), }
             \AttributeTok{shape =} \DecValTok{15}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_pointrange}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin=}\NormalTok{lwr, }\AttributeTok{ymax=}\NormalTok{upr, }\AttributeTok{color =}\NormalTok{ toilet\_trained)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age{-}group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Bed time"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Toilet trained"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{43-mrp_files/figure-latex/unnamed-chunk-11-1.pdf}

\hypertarget{case-study---xbox-paper}{%
\section{Case study - Xbox paper}\label{case-study---xbox-paper}}

\hypertarget{overview-6}{%
\subsection{Overview}\label{overview-6}}

One famous MRP example is \citet{wang2015forecasting}. They used data from the Xbox gaming platform to forecast the 2012 US Presidential Election.

Key facts about the set-up:

\begin{itemize}
\tightlist
\item
  Data are from an opt-in poll which was available on the Xbox gaming platform during the 45 days leading up to the 2012 US presidential election (Obama and Romney).
\item
  Each day there were three to five questions, including voter intention: `If the election were held today, who would you vote for?'.
\item
  Respondents were allowed to answer at most once per day.
\item
  First-time respondents were asked to provide information about themselves, including their sex, race, age, education, state, party ID, political ideology, and who they voted for in the 2008 presidential election.
\item
  In total, 750,148 interviews were conducted, with 345,858 unique respondents - over 30,000 of whom completed five or more polls.
\item
  Young men dominate the Xbox population: 18-to-29-year-olds comprise 65 per cent of the Xbox dataset, compared to 19 per cent in the exit poll; and men make up 93 per cent of the Xbox sample but only 47 per cent of the electorate.
\end{itemize}

\hypertarget{model-3}{%
\subsection{Model}\label{model-3}}

Given the structure of the US electorate, they use a two-stage modelling approach. The details don't really matter too much, but essentially they model how likely a respondent is to vote for Obama, given various information such as state, education, sex, etc:

\[
Pr\left(Y_i = \mbox{Obama} | Y_i\in\{\mbox{Obama, Romney}\}\right) = \mbox{logit}^{-1}(\alpha_0 + \alpha_1(\mbox{state last vote share}) + \alpha_{j[i]}^{\mbox{state}} + \alpha_{j[i]}^{\mbox{edu}} + \alpha_{j[i]}^{\mbox{sex}} + ...)
\]

They run this in R using \texttt{glmer()} from `lme4' \citep{citelme}.

\hypertarget{post-stratify}{%
\subsection{Post-stratify}\label{post-stratify}}

Having a trained model that considers the effect of these various independent variables on support for the candidates, they now post-stratify, where each of these `cell-level estimates are weighted by the proportion of the electorate in each cell and aggregated to the appropriate level (i.e., state or national).'

This means that they need cross-tabulated population data. In general, the census would have worked, or one of the other large surveys available in the US, but the difficulty is that the variables need to be available on a cross-tab basis. As such, they use exit polls (not a viable option for most other countries).

They make state-specific estimates by post-stratifying to the features of each state (Figure \ref{fig:states}).

\begin{figure}
\includegraphics[width=1\linewidth]{/Users/rohanalexander/Documents/book/figures/states} \caption{Post-stratified estimates for each state based on the Xbox survey and MRP}\label{fig:states}
\end{figure}

Similarly, they can examine demographic-differences (Figure \ref{fig:demographics}).

\begin{figure}
\includegraphics[width=1\linewidth]{/Users/rohanalexander/Documents/book/figures/demographics} \caption{Post-stratified estimates on a demographic basis based on the Xbox survey and MRP}\label{fig:demographics}
\end{figure}

Finally, they convert their estimates into electoral college estimates (Figure \ref{fig:electoralcollege}).

\begin{figure}
\includegraphics[width=1\linewidth]{/Users/rohanalexander/Documents/book/figures/electoral_college} \caption{Post-stratified estimates of electoral college outcomes based on the Xbox survey and MRP}\label{fig:electoralcollege}
\end{figure}

\hypertarget{simulation---australian-voting}{%
\section{Simulation - Australian voting}\label{simulation---australian-voting}}

\hypertarget{overview-7}{%
\subsection{Overview}\label{overview-7}}

As a reminder, the workflow that we use is:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  read in the poll;
\item
  model the poll;
\item
  read in the post-stratification data; and
\item
  apply the model to the post-stratification data.
\end{enumerate}

In the earlier example, we didn't really do too much in the modelling step, and despite the name `multilevel modelling with post-stratification', we didn't actually use a multilevel model. There's nothing that says you have to use a multilevel model, but a lot of situations will have circumstances such that it's not likely to do any worse. To be clear, this means that although we have individual-level data, there is some grouping of the individuals that we'll take advantage of. For instance, in the case of trying to model elections, usually districts/divisions/electorates/ridings/etc exist within provinces/states so it would likely make sense to, at least, include a coefficient that adjusts the intercept for each province.

In this section we're simulate another dataset and then fit a few different models to it. We're going to draw on the Australian elections set-up. In Australia we have a parliamentary system, with 151 seats in the parliament, one for each electorate. These electorates are grouped within six states and two territories. There are two major parties - the Australian Labor Party (ALP) and the Liberal Party (LP). Somewhat confusingly, the Liberal party are actually the conservative, right-wing party, while the Labor party are the progressive, left-wing, party.

\hypertarget{construct-a-survey}{%
\subsection{Construct a survey}\label{construct-a-survey}}

To move us slightly closer to reality, we are going to simulate a survey (rather than sample from a population as we did earlier) and then post-stratify it using real data. The dependent variable is `supports\_ALP', which is a binary variable - either 0 or 1. We'll just start with three independent variables here:

\begin{itemize}
\tightlist
\item
  `gender', which is either `female' or `male' (as that is what is available from the Australian Bureau of Statistics);
\item
  `age\_group', which is one of four groups: `ages 18 to 29', `ages 30 to 44', `ages 45 to 59', `ages 60 plus';
\item
  `state', which is one of eight integers: 1 - 8 (inclusive).
\end{itemize}

At this point, it's worth briefly discussing the role of sex and gender in survey research, following \citet{kennedy2020using}. Sex is based on biological attributes, while gender is socially constructed. We are likely interested in the effect of gender on our dependent variable. Moving away from a non-binary concept of gender, in terms of official statistics, is only something that has happened recently. As a researcher one of the problems of insisting on a binary is that, as \citet[p.~2]{kennedy2020using} say `\ldots when measuring gender with simply two categories, there is a failure to capture the unique experiences of those who do not identify as either male or female, or for those whose gender does not align with their sex classification.'. A researcher has a variety of ways of proceeding, and \citet{kennedy2020using} discuss these based on: ethics, accuracy, practicality, and flexibility. However, `there is no single good solution that can be applied to all situations. Instead, it is important to recognize that there is a compromise between ethical concerns, statistical concerns, and the most appropriate decision will be reflective of this' {[}p.~16{]}. The most important consideration is to ensure appropriate `respect and consideration for the survey respondent'.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{size\_of\_sample\_for\_australian\_polling }\OtherTok{\textless{}{-}} \DecValTok{2000}

\NormalTok{sample\_for\_australian\_polling }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{age\_group =} 
           \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{3}\NormalTok{), }
                  \AttributeTok{size =}\NormalTok{ size\_of\_sample\_for\_australian\_polling,}
                  \AttributeTok{replace =} \ConstantTok{TRUE}
\NormalTok{                  ),}
         \AttributeTok{gender =} 
           \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{1}\NormalTok{),}
                  \AttributeTok{size =}\NormalTok{ size\_of\_sample\_for\_australian\_polling,}
                  \AttributeTok{replace =} \ConstantTok{TRUE}
\NormalTok{                  ),}
         \AttributeTok{state =} 
           \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{),}
                  \AttributeTok{size =}\NormalTok{ size\_of\_sample\_for\_australian\_polling,}
                  \AttributeTok{replace =} \ConstantTok{TRUE}
\NormalTok{                  ),}
         \AttributeTok{noise =} \FunctionTok{rnorm}\NormalTok{(size\_of\_sample\_for\_australian\_polling, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{), }
         \AttributeTok{support\_alp =} \DecValTok{1} \SpecialCharTok{+} \FloatTok{0.5} \SpecialCharTok{*}\NormalTok{ age\_group }\SpecialCharTok{+} \FloatTok{0.5} \SpecialCharTok{*}\NormalTok{ gender }\SpecialCharTok{+} \FloatTok{0.01} \SpecialCharTok{*}\NormalTok{ state }\SpecialCharTok{+}\NormalTok{ noise}
\NormalTok{         ) }

\CommentTok{\# Normalize the outcome variable}
\NormalTok{sample\_for\_australian\_polling }\OtherTok{\textless{}{-}} 
\NormalTok{  sample\_for\_australian\_polling }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{support\_alp =} 
           \FunctionTok{if\_else}\NormalTok{(support\_alp }\SpecialCharTok{\textgreater{}} \FunctionTok{median}\NormalTok{(support\_alp, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{), }
                   \StringTok{\textquotesingle{}Supports ALP\textquotesingle{}}\NormalTok{, }
                   \StringTok{\textquotesingle{}Does not\textquotesingle{}}\NormalTok{)}
\NormalTok{         )}

\CommentTok{\# Clean up the simulated data}
\NormalTok{sample\_for\_australian\_polling }\OtherTok{\textless{}{-}} 
\NormalTok{  sample\_for\_australian\_polling }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{age\_group =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{      age\_group }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Ages 18 to 29\textquotesingle{}}\NormalTok{,}
\NormalTok{      age\_group }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Ages 30 to 44\textquotesingle{}}\NormalTok{,}
\NormalTok{      age\_group }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Ages 45 to 59\textquotesingle{}}\NormalTok{,}
\NormalTok{      age\_group }\SpecialCharTok{==} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Ages 60 plus\textquotesingle{}}\NormalTok{,}
      \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Problem\textquotesingle{}}
\NormalTok{      ),}
    \AttributeTok{gender =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{      gender }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Male\textquotesingle{}}\NormalTok{,}
\NormalTok{      gender }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Female\textquotesingle{}}\NormalTok{,}
      \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Problem\textquotesingle{}}
\NormalTok{      ),}
    \AttributeTok{state =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{      state }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Queensland\textquotesingle{}}\NormalTok{,}
\NormalTok{      state }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}New South Wales\textquotesingle{}}\NormalTok{,}
\NormalTok{      state }\SpecialCharTok{==} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Australian Capital Territory\textquotesingle{}}\NormalTok{,}
\NormalTok{      state }\SpecialCharTok{==} \DecValTok{4} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Victoria\textquotesingle{}}\NormalTok{,}
\NormalTok{      state }\SpecialCharTok{==} \DecValTok{5} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Tasmania\textquotesingle{}}\NormalTok{,}
\NormalTok{      state }\SpecialCharTok{==} \DecValTok{6} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Northern Territory\textquotesingle{}}\NormalTok{,}
\NormalTok{      state }\SpecialCharTok{==} \DecValTok{7} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}South Australia\textquotesingle{}}\NormalTok{,}
\NormalTok{      state }\SpecialCharTok{==} \DecValTok{8} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Western Australia\textquotesingle{}}\NormalTok{,}
      \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Problem\textquotesingle{}}
\NormalTok{      ),}
    
\NormalTok{    ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{noise)}

\CommentTok{\# Tidy the class}
\NormalTok{sample\_for\_australian\_polling }\OtherTok{\textless{}{-}} 
\NormalTok{  sample\_for\_australian\_polling }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(age\_group, gender, state, support\_alp), as\_factor))}

\NormalTok{sample\_for\_australian\_polling }\SpecialCharTok{\%\textgreater{}\%}   
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 4}
\CommentTok{\#\textgreater{}   age\_group     gender state           support\_alp }
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}         \textless{}fct\textgreater{}  \textless{}fct\textgreater{}           \textless{}fct\textgreater{}       }
\CommentTok{\#\textgreater{} 1 Ages 18 to 29 Female South Australia Supports ALP}
\CommentTok{\#\textgreater{} 2 Ages 60 plus  Male   South Australia Supports ALP}
\CommentTok{\#\textgreater{} 3 Ages 30 to 44 Male   Victoria        Does not    }
\CommentTok{\#\textgreater{} 4 Ages 18 to 29 Male   Tasmania        Does not    }
\CommentTok{\#\textgreater{} 5 Ages 18 to 29 Female Victoria        Does not    }
\CommentTok{\#\textgreater{} 6 Ages 18 to 29 Male   Queensland      Supports ALP}
\end{Highlighting}
\end{Shaded}

Finally, we want our survey to over-sample females, so we'll just get rid of 300 males.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_for\_australian\_polling }\OtherTok{\textless{}{-}} 
\NormalTok{  sample\_for\_australian\_polling }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(gender) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{1700}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{model-the-survey}{%
\subsection{Model the survey}\label{model-the-survey}}

This polling data was generated to make both males and older people less likely to vote for the ALP; and females and younger people more likely to vote for the Labor Party. Females are over-sampled. As such, we should have an ALP skew on the dataset. We're going to use the \texttt{gtsummary} package to quickly make a summary table \citep{citegtsummary}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(gtsummary)}

\NormalTok{sample\_for\_australian\_polling }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  gtsummary}\SpecialCharTok{::}\FunctionTok{tbl\_summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l}
\hline
**Characteristic** & **N = 1,700**\\
\hline
age\_group & \\
\hline
Ages 18 to 29 & 458 (27\%)\\
\hline
Ages 60 plus & 421 (25\%)\\
\hline
Ages 30 to 44 & 401 (24\%)\\
\hline
Ages 45 to 59 & 420 (25\%)\\
\hline
gender & \\
\hline
Female & 1,023 (60\%)\\
\hline
Male & 677 (40\%)\\
\hline
state & \\
\hline
South Australia & 233 (14\%)\\
\hline
Victoria & 189 (11\%)\\
\hline
Tasmania & 229 (13\%)\\
\hline
Queensland & 214 (13\%)\\
\hline
Western Australia & 198 (12\%)\\
\hline
New South Wales & 219 (13\%)\\
\hline
Australian Capital Territory & 237 (14\%)\\
\hline
Northern Territory & 181 (11\%)\\
\hline
support\_alp & \\
\hline
Supports ALP & 896 (53\%)\\
\hline
Does not & 804 (47\%)\\
\hline
\end{tabular}

Now we'd like to see if we can get our results back (we should find females less likely than males to vote for Australian Labor Party and that people are less likely to vote Australian Labor Party as they get older). Our model is:

ADD THE MODEL.

This model says that the probability that some person, \(j\), will vote for the Australian Labor Party depends on their gender and their age-group. Based on our simulated data, we would like older age-groups to be less likely to vote for the Australian Labor Party and for males to be less likely to vote for the Australian Labor Party.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alp\_support }\OtherTok{\textless{}{-}} 
  \FunctionTok{glm}\NormalTok{(support\_alp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ age\_group }\SpecialCharTok{+}\NormalTok{ state, }
      \AttributeTok{data =}\NormalTok{ sample\_for\_australian\_polling,}
      \AttributeTok{family =} \StringTok{"binomial"}
\NormalTok{      )}

\NormalTok{alp\_support }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  modelsummary}\SpecialCharTok{::}\FunctionTok{modelsummary}\NormalTok{(}\AttributeTok{fmt =} \DecValTok{2}\NormalTok{, }\AttributeTok{exponentiate =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}
\centering
\begin{tabular}[t]{lc}
\toprule
  & Model 1\\
\midrule
(Intercept) & \num{1.44}\\
 & (\num{0.18})\\
genderMale & \num{3.22}\\
 & (\num{0.12})\\
age\_groupAges 60 plus & \num{0.07}\\
 & (\num{0.17})\\
age\_groupAges 30 to 44 & \num{0.42}\\
 & \vphantom{1} (\num{0.15})\\
age\_groupAges 45 to 59 & \num{0.17}\\
 & (\num{0.15})\\
stateVictoria & \num{1.65}\\
 & \vphantom{2} (\num{0.22})\\
stateTasmania & \num{1.24}\\
 & \vphantom{2} (\num{0.21})\\
stateQueensland & \num{1.46}\\
 & \vphantom{1} (\num{0.22})\\
stateWestern Australia & \num{1.18}\\
 & (\num{0.22})\\
stateNew South Wales & \num{1.42}\\
 & \vphantom{1} (\num{0.21})\\
stateAustralian Capital Territory & \num{1.73}\\
 & (\num{0.21})\\
stateNorthern Territory & \num{1.49}\\
 & (\num{0.23})\\
\midrule
Num.Obs. & \num{1700}\\
AIC & \num{1959.7}\\
BIC & \num{2024.9}\\
Log.Lik. & \num{-967.836}\\
F & \num{28.555}\\
\bottomrule
\end{tabular}
\end{table}

Essentially we've got our inputs back. Our dependent variable is a binary, and so we used logistic regression so the results are a little more difficult to interpret.

\hypertarget{post-stratify-1}{%
\subsection{Post-stratify}\label{post-stratify-1}}

Now we'd like to see if we can use what we found in the poll to get an estimate for each state based on their demographic features.

First read in some real demographic data, on a state basis, from the ABS.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{post\_strat\_census\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"outputs/data/census\_data.csv"}\NormalTok{)}

\FunctionTok{head}\NormalTok{(post\_strat\_census\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 5}
\CommentTok{\#\textgreater{}   state gender age\_group  number cell\_prop\_of\_division\_total}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{} \textless{}chr\textgreater{}  \textless{}chr\textgreater{}       \textless{}dbl\textgreater{}                       \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 ACT   Female ages18to29  34683                       0.125}
\CommentTok{\#\textgreater{} 2 ACT   Female ages30to44  42980                       0.155}
\CommentTok{\#\textgreater{} 3 ACT   Female ages45to59  33769                       0.122}
\CommentTok{\#\textgreater{} 4 ACT   Female ages60plus  30322                       0.109}
\CommentTok{\#\textgreater{} 5 ACT   Male   ages18to29  34163                       0.123}
\CommentTok{\#\textgreater{} 6 ACT   Male   ages30to44  41288                       0.149}
\end{Highlighting}
\end{Shaded}

At this point, we've got a decision to make because we need the variables to be the same in the survey and the post-stratification dataset, but here the state abbreviations have been used, while in the survey, the full names were used. We'll change the post-stratification dataset because the survey data has already modelled.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{post\_strat\_census\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  post\_strat\_census\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{state =} 
      \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{        state }\SpecialCharTok{==} \StringTok{\textquotesingle{}ACT\textquotesingle{}} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Australian Capital Territory\textquotesingle{}}\NormalTok{,}
\NormalTok{        state }\SpecialCharTok{==} \StringTok{\textquotesingle{}NSW\textquotesingle{}} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}New South Wales\textquotesingle{}}\NormalTok{,}
\NormalTok{        state }\SpecialCharTok{==} \StringTok{\textquotesingle{}NT\textquotesingle{}} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Northern Territory\textquotesingle{}}\NormalTok{,}
\NormalTok{        state }\SpecialCharTok{==} \StringTok{\textquotesingle{}QLD\textquotesingle{}} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Queensland\textquotesingle{}}\NormalTok{,}
\NormalTok{        state }\SpecialCharTok{==} \StringTok{\textquotesingle{}SA\textquotesingle{}} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}South Australia\textquotesingle{}}\NormalTok{,}
\NormalTok{        state }\SpecialCharTok{==} \StringTok{\textquotesingle{}TAS\textquotesingle{}} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Tasmania\textquotesingle{}}\NormalTok{,}
\NormalTok{        state }\SpecialCharTok{==} \StringTok{\textquotesingle{}VIC\textquotesingle{}} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Victoria\textquotesingle{}}\NormalTok{,}
\NormalTok{        state }\SpecialCharTok{==} \StringTok{\textquotesingle{}WA\textquotesingle{}} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Western Australia\textquotesingle{}}\NormalTok{,}
        \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{"Problem"}
\NormalTok{      ),}
    \AttributeTok{age\_group =} 
      \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{        age\_group }\SpecialCharTok{==} \StringTok{\textquotesingle{}ages18to29\textquotesingle{}} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Ages 18 to 29\textquotesingle{}}\NormalTok{,}
\NormalTok{        age\_group }\SpecialCharTok{==} \StringTok{\textquotesingle{}ages30to44\textquotesingle{}} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Ages 30 to 44\textquotesingle{}}\NormalTok{,}
\NormalTok{        age\_group }\SpecialCharTok{==} \StringTok{\textquotesingle{}ages45to59\textquotesingle{}} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Ages 45 to 59\textquotesingle{}}\NormalTok{,}
\NormalTok{        age\_group }\SpecialCharTok{==} \StringTok{\textquotesingle{}ages60plus\textquotesingle{}} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Ages 60 plus\textquotesingle{}}\NormalTok{,}
        \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{"Problem"}
\NormalTok{      )}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

We're just going to do some rough forecasts. For each gender and age-group we want the relevant coefficient in the example data and we can construct the estimates.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{post\_strat\_census\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  alp\_support }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{predict}\NormalTok{(}\AttributeTok{newdata =}\NormalTok{ post\_strat\_census\_data, }\AttributeTok{type =} \StringTok{\textquotesingle{}response\textquotesingle{}}\NormalTok{, }\AttributeTok{se.fit =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{cbind}\NormalTok{(post\_strat\_census\_data, .)}

\NormalTok{post\_strat\_census\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{alp\_predict\_prop =}\NormalTok{ fit}\SpecialCharTok{*}\NormalTok{cell\_prop\_of\_division\_total) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(state) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{alp\_predict =} \FunctionTok{sum}\NormalTok{(alp\_predict\_prop))}
\CommentTok{\#\textgreater{} \# A tibble: 8 x 2}
\CommentTok{\#\textgreater{}   state                        alp\_predict}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}                              \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Australian Capital Territory       0.551}
\CommentTok{\#\textgreater{} 2 New South Wales                    0.487}
\CommentTok{\#\textgreater{} 3 Northern Territory                 0.546}
\CommentTok{\#\textgreater{} 4 Queensland                         0.491}
\CommentTok{\#\textgreater{} 5 South Australia                    0.403}
\CommentTok{\#\textgreater{} 6 Tasmania                           0.429}
\CommentTok{\#\textgreater{} 7 Victoria                           0.521}
\CommentTok{\#\textgreater{} 8 Western Australia                  0.460}
\end{Highlighting}
\end{Shaded}

We now have post-stratified estimates for each state Our model has a fair few weaknesses. For instance, small cell counts are going to be problematic. And our approach ignores uncertainty, but now that we have something working we can complicate it.

\hypertarget{improving-the-model}{%
\subsection{Improving the model}\label{improving-the-model}}

We'd like to address some of the major issues with our approach, specifically being able to deal with small cell counts, and also taking better account of uncertainty. As we are dealing with survey data, prediction intervals or something similar are critical, and it's not appropriate to only report central estimates. To do this we'll use the same broad approach as before, but just improve the model. We're going to change to a Bayesian model and use the \texttt{rstanarm} package \citep{citerstanarm}.

Now, using the same basic model as before, but in a Bayesian setting.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rstanarm)}

\NormalTok{improved\_alp\_support }\OtherTok{\textless{}{-}} 
\NormalTok{  rstanarm}\SpecialCharTok{::}\FunctionTok{stan\_glm}\NormalTok{(support\_alp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ age\_group }\SpecialCharTok{+}\NormalTok{ state,}
                     \AttributeTok{data =}\NormalTok{ sample\_for\_australian\_polling,}
                     \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{),}
                     \AttributeTok{prior =} \FunctionTok{normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                     \AttributeTok{prior\_intercept =} \FunctionTok{normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{),}
                     \AttributeTok{cores =} \DecValTok{2}\NormalTok{, }
                     \AttributeTok{seed =} \DecValTok{12345}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

As before, we'd like an estimate for each state based on their demographic features. We're just going to do some rough forecasts. For each gender and age-group we want the relevant coefficient in the example data and we can construct the estimates (this code is from \href{https://www.monicaalexander.com/posts/2019-08-07-mrp/}{Monica Alexander}). We're going to use \texttt{tidybayes} for this \citep{citetidybayes}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidybayes)}

\NormalTok{post\_stratified\_estimates }\OtherTok{\textless{}{-}} 
\NormalTok{  improved\_alp\_support }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  tidybayes}\SpecialCharTok{::}\FunctionTok{add\_fitted\_draws}\NormalTok{(}\AttributeTok{newdata =}\NormalTok{ post\_strat\_census\_data) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{alp\_predict =}\NormalTok{ .value) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{alp\_predict\_prop =}\NormalTok{ alp\_predict}\SpecialCharTok{*}\NormalTok{cell\_prop\_of\_division\_total) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(state, .draw) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{alp\_predict =} \FunctionTok{sum}\NormalTok{(alp\_predict\_prop)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(state) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(alp\_predict), }
            \AttributeTok{lower =} \FunctionTok{quantile}\NormalTok{(alp\_predict, }\FloatTok{0.025}\NormalTok{), }
            \AttributeTok{upper =} \FunctionTok{quantile}\NormalTok{(alp\_predict, }\FloatTok{0.975}\NormalTok{))}

\NormalTok{post\_stratified\_estimates}
\CommentTok{\#\textgreater{} \# A tibble: 8 x 4}
\CommentTok{\#\textgreater{}   state                         mean lower upper}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}                        \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Australian Capital Territory 0.550 0.494 0.604}
\CommentTok{\#\textgreater{} 2 New South Wales              0.486 0.429 0.544}
\CommentTok{\#\textgreater{} 3 Northern Territory           0.544 0.483 0.607}
\CommentTok{\#\textgreater{} 4 Queensland                   0.491 0.432 0.548}
\CommentTok{\#\textgreater{} 5 South Australia              0.412 0.361 0.464}
\CommentTok{\#\textgreater{} 6 Tasmania                     0.429 0.372 0.487}
\CommentTok{\#\textgreater{} 7 Victoria                     0.519 0.453 0.583}
\CommentTok{\#\textgreater{} 8 Western Australia            0.460 0.401 0.520}
\end{Highlighting}
\end{Shaded}

We now have post-stratified estimates for each division. Our new Bayesian approach will enable us to think more deeply about uncertainty. We could complicate this in a variety of ways including adding more coefficients (but remember that we'd need to get new cell counts), or adding some layers.

One interesting aspect is that our multilevel approach will allow us to deal with small cell counts by borrowing information from other cells. Even if we were to remove most of the, say, 18-to-29-year-old, male respondents from Tasmania our model would still provide estimates. It does this by pooling, in which the effect of these young, male, Tasmanians is partially determined by other cells that do have respondents.

There are many interesting aspects that we may like to communicate to others. For instance, we may like to show how the model is affecting the results. We can make a graph that compares the raw estimate with the model estimate.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{post\_stratified\_estimates }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ mean, }\AttributeTok{x =}\NormalTok{ forcats}\SpecialCharTok{::}\FunctionTok{fct\_inorder}\NormalTok{(state), }\AttributeTok{color =} \StringTok{"MRP estimate"}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_errorbar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ lower, }\AttributeTok{ymax =}\NormalTok{ upper), }\AttributeTok{width =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Proportion ALP support"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"State"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"bottom"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.title =} \FunctionTok{element\_blank}\NormalTok{()) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{43-mrp_files/figure-latex/unnamed-chunk-20-1.pdf}

Similarly, we may like to plot the distribution of the coefficients.\footnote{You can work out which coefficients to be pass to gather\_draws by using tidybayes::get\_variables(model). (In this example I passed `b\_.', but the ones of interest to you may be different.)}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# tidybayes::get\_variables(improved\_alp\_support)}
\CommentTok{\# improved\_alp\_support \%\textgreater{}\%}
\CommentTok{\#   gather\_draws(genderMale) \%\textgreater{}\%}
\CommentTok{\#   ungroup() \%\textgreater{}\%}
\CommentTok{\#   \# mutate(coefficient = stringr::str\_replace\_all(.variable, c("b\_" = ""))) \%\textgreater{}\%}
\CommentTok{\#   mutate(coefficient = forcats::fct\_recode(coefficient,}
\CommentTok{\#                                            Intercept = "Intercept",}
\CommentTok{\#                                            \textasciigrave{}Is male\textasciigrave{} = "genderMale",}
\CommentTok{\#                                            \textasciigrave{}Age 30{-}44\textasciigrave{} = "age\_groupages30to44",}
\CommentTok{\#                                            \textasciigrave{}Age 45{-}59\textasciigrave{} = "age\_groupages45to59",}
\CommentTok{\#                                            \textasciigrave{}Age 60+\textasciigrave{} = "age\_groupages60plus"}
\CommentTok{\#                                            )) \%\textgreater{}\% }
\CommentTok{\# }
\CommentTok{\# \# both \%\textgreater{}\% }
\CommentTok{\#   ggplot(aes(y=fct\_rev(coefficient), x = .value)) + }
\CommentTok{\#   ggridges::geom\_density\_ridges2(aes(height = ..density..),}
\CommentTok{\#                                  rel\_min\_height = 0.01, }
\CommentTok{\#                                  stat = "density",}
\CommentTok{\#                                  scale=1.5) +}
\CommentTok{\#   xlab("Distribution of estimate") +}
\CommentTok{\#   ylab("Coefficient") +}
\CommentTok{\#   scale\_fill\_brewer(name = "Dataset: ", palette = "Set1") +}
\CommentTok{\#   theme\_minimal() +}
\CommentTok{\#   theme(panel.grid.major = element\_blank(),}
\CommentTok{\#         panel.grid.minor = element\_blank()) +}
\CommentTok{\#   theme(legend.position = "bottom")}
\end{Highlighting}
\end{Shaded}

\hypertarget{forecasting-the-2020-us-election}{%
\section{Forecasting the 2020 US election}\label{forecasting-the-2020-us-election}}

The US election has a lot of features that are unique to the US, but the model that we are going to build here is going to be fairly generic and, largely a generalization of the earlier model for the Australian election. One good thing about forecasting the US election is that there is a lot of data around. In this case we can use survey data from the \href{https://www.voterstudygroup.org}{Democracy Fund Voter Study Group}.\footnote{I thank \href{http://www.chriswarshaw.com}{Chris Warshaw} for putting me onto this dataset.} They conducted polling in the lead-up to the US election and make this publicly available after registration. We will use the Integrated Public Use Microdata Series (IPUMS), to access the 2018 American Community Survey (ACS) as a post-stratification dataset. We will use state, age-group, gender, and education as explanatory variables.

\hypertarget{survey-data}{%
\subsection{Survey data}\label{survey-data}}

The first step is that we need to actually get the survey data. Go to their website:\url{https://www.voterstudygroup.org} and then you're looking for `Nationscape' and then a button along the lines of `Get the latest Nationscape data'. To get the dataset, you need to fill out a form, which they will process and then email you. There is a real person on the other side of this form, and so your request could take a few days.

Once you get access you'll want to download the .dta files. Nationscape conducted many surveys and so there are many files. The filename is the reference date, and so `ns20200625' refers to 25 June 2020, which is the one that I'll use here. (This data is not mine to share, which is why I'll refer)

We can read in `.dta' files using the \texttt{haven} package \citep{citehaven}. I've based this code on that written by Alen Mitrovski, Xiaoyan Yang, Matthew Wankiewicz, which is available here: \url{https://github.com/matthewwankiewicz/US_election_forecast}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(haven)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{raw\_nationscape\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_dta}\NormalTok{(here}\SpecialCharTok{::}\FunctionTok{here}\NormalTok{(}\StringTok{"dont\_push/ns20200625.dta"}\NormalTok{))}

\CommentTok{\# The Stata format separates labels so reunite those}
\NormalTok{raw\_nationscape\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  labelled}\SpecialCharTok{::}\FunctionTok{to\_factor}\NormalTok{(raw\_nationscape\_data)}

\CommentTok{\# Just keep relevant variables}
\NormalTok{nationscape\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  raw\_nationscape\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(vote\_2020,}
\NormalTok{         gender,}
\NormalTok{         education,}
\NormalTok{         state,}
\NormalTok{         age)}

\CommentTok{\# For simplicity, remove anyone undecided or planning to vote for someone other than Biden/Trump and make vote a binary variable: 1 for Biden, 0 for Trump.}
\NormalTok{nationscape\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  nationscape\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(vote\_2020 }\SpecialCharTok{==} \StringTok{"Joe Biden"} \SpecialCharTok{|}\NormalTok{ vote\_2020 }\SpecialCharTok{==} \StringTok{"Donald Trump"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{vote\_biden =} \FunctionTok{if\_else}\NormalTok{(vote\_2020 }\SpecialCharTok{==} \StringTok{"Joe Biden"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{vote\_2020)}

\CommentTok{\# Create the dependent variables by grouping the existing variables}
\NormalTok{nationscape\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  nationscape\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{age\_group =} \FunctionTok{case\_when}\NormalTok{( }\CommentTok{\# case\_when works in order and exits when there\textquotesingle{}s a match}
\NormalTok{      age }\SpecialCharTok{\textless{}=} \DecValTok{29} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}age\_18{-}29\textquotesingle{}}\NormalTok{,}
\NormalTok{      age }\SpecialCharTok{\textless{}=} \DecValTok{44} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}age\_30{-}44\textquotesingle{}}\NormalTok{,}
\NormalTok{      age }\SpecialCharTok{\textless{}=} \DecValTok{59} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}age\_45{-}59\textquotesingle{}}\NormalTok{,}
\NormalTok{      age }\SpecialCharTok{\textgreater{}=} \DecValTok{60} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}age\_60\_or\_more\textquotesingle{}}\NormalTok{,}
      \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Trouble\textquotesingle{}}
\NormalTok{      ),}
    \AttributeTok{gender =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{      gender }\SpecialCharTok{==} \StringTok{"Female"} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}female\textquotesingle{}}\NormalTok{,}
\NormalTok{      gender }\SpecialCharTok{==} \StringTok{"Male"} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}male\textquotesingle{}}\NormalTok{,}
      \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Trouble\textquotesingle{}}
\NormalTok{      ),}
    \AttributeTok{education\_level =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{      education }\SpecialCharTok{==} \StringTok{"3rd Grade or less"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      education }\SpecialCharTok{==} \StringTok{"Middle School {-} Grades 4 {-} 8"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      education }\SpecialCharTok{==} \StringTok{"Completed some high school"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      education }\SpecialCharTok{==} \StringTok{"High school graduate"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      education }\SpecialCharTok{==} \StringTok{"Other post high school vocational training"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Some post secondary"}\NormalTok{,}
\NormalTok{      education }\SpecialCharTok{==} \StringTok{"Completed some college, but no degree"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Some post secondary"}\NormalTok{,}
\NormalTok{      education }\SpecialCharTok{==} \StringTok{"Associate Degree"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Post secondary or higher"}\NormalTok{,}
\NormalTok{      education }\SpecialCharTok{==} \StringTok{"College Degree (such as B.A., B.S.)"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Post secondary or higher"}\NormalTok{,}
\NormalTok{      education }\SpecialCharTok{==} \StringTok{"Completed some graduate, but no degree"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Post secondary or higher"}\NormalTok{,}
\NormalTok{      education }\SpecialCharTok{==} \StringTok{"Masters degree"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Graduate degree"}\NormalTok{,}
\NormalTok{      education }\SpecialCharTok{==} \StringTok{"Doctorate degree"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Graduate degree"}\NormalTok{,}
      \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Trouble\textquotesingle{}}
\NormalTok{      )}
\NormalTok{    ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{education, }\SpecialCharTok{{-}}\NormalTok{age)}

\NormalTok{tests }\OtherTok{\textless{}{-}} 
\NormalTok{  nationscape\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{test =}\NormalTok{ stringr}\SpecialCharTok{::}\FunctionTok{str\_detect}\NormalTok{(age\_group, }\StringTok{\textquotesingle{}Trouble\textquotesingle{}}\NormalTok{),}
         \AttributeTok{test =} \FunctionTok{if\_else}\NormalTok{(test }\SpecialCharTok{==} \ConstantTok{TRUE}\NormalTok{, }\ConstantTok{TRUE}\NormalTok{, }
\NormalTok{                        stringr}\SpecialCharTok{::}\FunctionTok{str\_detect}\NormalTok{(education\_level, }\StringTok{\textquotesingle{}Trouble\textquotesingle{}}\NormalTok{)),}
         \AttributeTok{test =} \FunctionTok{if\_else}\NormalTok{(test }\SpecialCharTok{==} \ConstantTok{TRUE}\NormalTok{, }\ConstantTok{TRUE}\NormalTok{, }
\NormalTok{                        stringr}\SpecialCharTok{::}\FunctionTok{str\_detect}\NormalTok{(gender, }\StringTok{\textquotesingle{}Trouble\textquotesingle{}}\NormalTok{))}
\NormalTok{         ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(test }\SpecialCharTok{==} \ConstantTok{TRUE}\NormalTok{)}

\ControlFlowTok{if}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(tests) }\SpecialCharTok{!=} \DecValTok{0}\NormalTok{) \{}
  \FunctionTok{print}\NormalTok{(}\StringTok{"Check nationscape\_data"}\NormalTok{)}
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
    \FunctionTok{rm}\NormalTok{(tests)}
\NormalTok{    \}}

\NormalTok{nationscape\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 5}
\CommentTok{\#\textgreater{}   gender state vote\_biden age\_group      education\_level         }
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}  \textless{}chr\textgreater{}      \textless{}dbl\textgreater{} \textless{}chr\textgreater{}          \textless{}chr\textgreater{}                   }
\CommentTok{\#\textgreater{} 1 female WI             0 age\_45{-}59      Post secondary or higher}
\CommentTok{\#\textgreater{} 2 female VA             0 age\_45{-}59      Post secondary or higher}
\CommentTok{\#\textgreater{} 3 female TX             0 age\_60\_or\_more High school or less     }
\CommentTok{\#\textgreater{} 4 female WA             0 age\_45{-}59      High school or less     }
\CommentTok{\#\textgreater{} 5 female MA             1 age\_18{-}29      Some post secondary     }
\CommentTok{\#\textgreater{} 6 female TX             1 age\_30{-}44      Some post secondary}
\end{Highlighting}
\end{Shaded}

As we've seen, one of the most difficult aspects with MRP is ensuring consistency between the datasets. In this case, we need to do some work to make the variables consistent.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This code is very directly from Alen Mitrovski, Xiaoyan Yang, and Matthew Wankiewicz.}
\CommentTok{\# Format state names so the whole state name is written out, to match IPUMS data}
\NormalTok{states\_names\_and\_abbrevs }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{stateicp =}\NormalTok{ state.name, }\AttributeTok{state =}\NormalTok{ state.abb)}

\NormalTok{nationscape\_data }\OtherTok{\textless{}{-}}
\NormalTok{  nationscape\_data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{left\_join}\NormalTok{(states\_names\_and\_abbrevs)}

\FunctionTok{rm}\NormalTok{(states\_names\_and\_abbrevs)}

\CommentTok{\# Make lowercase to match IPUMS data}
\NormalTok{nationscape\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  nationscape\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{stateicp =} \FunctionTok{tolower}\NormalTok{(stateicp))}

\CommentTok{\# Replace NAs with DC}
\NormalTok{nationscape\_data}\SpecialCharTok{$}\NormalTok{stateicp }\OtherTok{\textless{}{-}} 
  \FunctionTok{replace\_na}\NormalTok{(nationscape\_data}\SpecialCharTok{$}\NormalTok{stateicp, }\StringTok{"district of columbia"}\NormalTok{)}

\CommentTok{\# Tidy the class}
\NormalTok{nationscape\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  nationscape\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(gender, stateicp, education\_level, age\_group), as\_factor))}

\CommentTok{\# Save data}
\FunctionTok{write\_csv}\NormalTok{(nationscape\_data, }\StringTok{"outputs/data/polling\_data.csv"}\NormalTok{)}

\NormalTok{nationscape\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 6}
\CommentTok{\#\textgreater{}   gender state vote\_biden age\_group      education\_level          stateicp     }
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}  \textless{}chr\textgreater{}      \textless{}dbl\textgreater{} \textless{}fct\textgreater{}          \textless{}fct\textgreater{}                    \textless{}fct\textgreater{}        }
\CommentTok{\#\textgreater{} 1 female WI             0 age\_45{-}59      Post secondary or higher wisconsin    }
\CommentTok{\#\textgreater{} 2 female VA             0 age\_45{-}59      Post secondary or higher virginia     }
\CommentTok{\#\textgreater{} 3 female TX             0 age\_60\_or\_more High school or less      texas        }
\CommentTok{\#\textgreater{} 4 female WA             0 age\_45{-}59      High school or less      washington   }
\CommentTok{\#\textgreater{} 5 female MA             1 age\_18{-}29      Some post secondary      massachusetts}
\CommentTok{\#\textgreater{} 6 female TX             1 age\_30{-}44      Some post secondary      texas}
\end{Highlighting}
\end{Shaded}

\hypertarget{post-stratification-data}{%
\subsection{Post-stratification data}\label{post-stratification-data}}

We have a lot of options for a dataset to post-stratify by and there are various considerations. We are after a dataset that is better quality (however that is to be defined), and likely larger. From a strictly data perspective, the best choice would probably be something like the Cooperative Congressional Election Study (CCES), however for whatever reason that is only released after the election and so it's not a reasonable choice. \citet{wang2015forecasting} use exit poll data, but again that's only available after the election.

In most countries we'd be stuck using the census, which is of course quite large, but likely out-of-date. Luckily in the US we have the opportunity to use the American Community Survey (ACS) which asks analogous questions to a census, is conducted every month, and over the course of a year, we end up with a few million responses. In this case we're going to access the ACS through IPUMS.

To do this go to the IPUMS website - \url{https://ipums.org} - and we're looking for something like IPUMS USA and then `get data'. Create an account, if you need to. That'll take a while to process. But once you have an account, go to `Select Samples' and de-select everything apart from the 2019 ACS. Then we need to get the variables that we're interested in. From the household we want `STATEICP', then in person we want `SEX', `AGE', `EDUC'. Once everything is selected, `view cart', and we want to be careful to change the `data format' to `.dta' (there's nothing wrong with the other formats, but we've just already got code earlier to deal with that type). Briefly just check how many rows and columns you're requesting. It should be around a million rows, and around ten to twenty columns. If it's much more than 300MB then maybe just see if you've accidently selected something that you don't need. Submit the request and within a day, you should get an email saying that your data can be downloaded. It should only take 30 minutes or so, but if you don't get an email within a day then check again the size of the dataset, and customize the sample size to reduce the size initially.

In any case let's tidy up the data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Again, closely following code from Alen Mitrovski, Xiaoyan Yang, and Matthew Wankiewicz.}
\FunctionTok{library}\NormalTok{(haven)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{raw\_poststrat\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_dta}\NormalTok{(here}\SpecialCharTok{::}\FunctionTok{here}\NormalTok{(}\StringTok{"dont\_push/usa\_00004.dta"}\NormalTok{))}

\CommentTok{\# The Stata format separates labels so reunite those}
\NormalTok{raw\_poststrat\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  labelled}\SpecialCharTok{::}\FunctionTok{to\_factor}\NormalTok{(raw\_poststrat\_data)}
\FunctionTok{head}\NormalTok{(raw\_poststrat\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 28}
\CommentTok{\#\textgreater{}   year  sample serial cbserial  hhwt cluster region stateicp strata gq    pernum}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{} \textless{}fct\textgreater{}   \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{} \textless{}fct\textgreater{}  \textless{}fct\textgreater{}     \textless{}dbl\textgreater{} \textless{}fct\textgreater{}  \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 2018  2018 \textasciitilde{}      2  2.02e12 392.  2.02e12 east \textasciitilde{} alabama  190001 othe\textasciitilde{}      1}
\CommentTok{\#\textgreater{} 2 2018  2018 \textasciitilde{}      7  2.02e12  94.1 2.02e12 east \textasciitilde{} alabama   40001 grou\textasciitilde{}      1}
\CommentTok{\#\textgreater{} 3 2018  2018 \textasciitilde{}     13  2.02e12  83.7 2.02e12 east \textasciitilde{} alabama  130301 othe\textasciitilde{}      1}
\CommentTok{\#\textgreater{} 4 2018  2018 \textasciitilde{}     18  2.02e12  57.5 2.02e12 east \textasciitilde{} alabama  100001 grou\textasciitilde{}      1}
\CommentTok{\#\textgreater{} 5 2018  2018 \textasciitilde{}     23  2.02e12 157.  2.02e12 east \textasciitilde{} alabama  190001 grou\textasciitilde{}      1}
\CommentTok{\#\textgreater{} 6 2018  2018 \textasciitilde{}     28  2.02e12 157.  2.02e12 east \textasciitilde{} alabama  220001 othe\textasciitilde{}      1}
\CommentTok{\#\textgreater{} \# ... with 17 more variables: perwt \textless{}dbl\textgreater{}, sex \textless{}fct\textgreater{}, age \textless{}fct\textgreater{}, marst \textless{}fct\textgreater{},}
\CommentTok{\#\textgreater{} \#   race \textless{}fct\textgreater{}, raced \textless{}fct\textgreater{}, hispan \textless{}fct\textgreater{}, hispand \textless{}fct\textgreater{}, bpl \textless{}fct\textgreater{},}
\CommentTok{\#\textgreater{} \#   bpld \textless{}fct\textgreater{}, citizen \textless{}fct\textgreater{}, educ \textless{}fct\textgreater{}, educd \textless{}fct\textgreater{}, empstat \textless{}fct\textgreater{},}
\CommentTok{\#\textgreater{} \#   empstatd \textless{}fct\textgreater{}, labforce \textless{}fct\textgreater{}, inctot \textless{}dbl\textgreater{}}

\NormalTok{raw\_poststrat\_data}\SpecialCharTok{$}\NormalTok{age }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(raw\_poststrat\_data}\SpecialCharTok{$}\NormalTok{age)}

\NormalTok{poststrat\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  raw\_poststrat\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(inctot }\SpecialCharTok{\textless{}} \DecValTok{9999999}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(age }\SpecialCharTok{\textgreater{}=} \DecValTok{18}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{gender =}\NormalTok{ sex) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{age\_group =} \FunctionTok{case\_when}\NormalTok{( }\CommentTok{\# case\_when works in order and exits when there\textquotesingle{}s a match}
\NormalTok{      age }\SpecialCharTok{\textless{}=} \DecValTok{29} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}age\_18{-}29\textquotesingle{}}\NormalTok{,}
\NormalTok{      age }\SpecialCharTok{\textless{}=} \DecValTok{44} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}age\_30{-}44\textquotesingle{}}\NormalTok{,}
\NormalTok{      age }\SpecialCharTok{\textless{}=} \DecValTok{59} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}age\_45{-}59\textquotesingle{}}\NormalTok{,}
\NormalTok{      age }\SpecialCharTok{\textgreater{}=} \DecValTok{60} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}age\_60\_or\_more\textquotesingle{}}\NormalTok{,}
      \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Trouble\textquotesingle{}}
\NormalTok{      ),}
    \AttributeTok{education\_level =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"nursery school, preschool"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"kindergarten"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"grade 1"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"grade 2"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"grade 3"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"grade 4"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"grade 5"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"grade 6"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"grade 7"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"grade 8"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"grade 9"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"grade 10"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"grade 11"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"12th grade, no diploma"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"regular high school diploma"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"ged or alternative credential"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"some college, but less than 1 year"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Some post secondary"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"1 or more years of college credit, no degree"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Some post secondary"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"associate\textquotesingle{}s degree, type not specified"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Post secondary or higher"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"bachelor\textquotesingle{}s degree"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Post secondary or higher"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"master\textquotesingle{}s degree"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Graduate degree"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"professional degree beyond a bachelor\textquotesingle{}s degree"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Graduate degree"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"doctoral degree"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Graduate degree"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"no schooling completed"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
      \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Trouble\textquotesingle{}}
\NormalTok{      )}
\NormalTok{    )}

\CommentTok{\# Just keep relevant variables}
\NormalTok{poststrat\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  poststrat\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(gender,}
\NormalTok{         age\_group,}
\NormalTok{         education\_level,}
\NormalTok{         stateicp)}

\CommentTok{\# Tidy the class}
\NormalTok{poststrat\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  poststrat\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(gender, stateicp, education\_level, age\_group), as\_factor))}

\CommentTok{\# Save data}
\FunctionTok{write\_csv}\NormalTok{(poststrat\_data, }\StringTok{"outputs/data/us\_poststrat.csv"}\NormalTok{)}

\NormalTok{poststrat\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 4}
\CommentTok{\#\textgreater{}   gender age\_group      education\_level     stateicp}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}  \textless{}fct\textgreater{}          \textless{}fct\textgreater{}               \textless{}fct\textgreater{}   }
\CommentTok{\#\textgreater{} 1 female age\_18{-}29      Some post secondary alabama }
\CommentTok{\#\textgreater{} 2 female age\_60\_or\_more Some post secondary alabama }
\CommentTok{\#\textgreater{} 3 male   age\_45{-}59      Some post secondary alabama }
\CommentTok{\#\textgreater{} 4 male   age\_30{-}44      High school or less alabama }
\CommentTok{\#\textgreater{} 5 female age\_60\_or\_more High school or less alabama }
\CommentTok{\#\textgreater{} 6 male   age\_30{-}44      High school or less alabama}
\end{Highlighting}
\end{Shaded}

This dataset is on an individual level. So we'll create counts of each sub-cell, and then proportions by state.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{poststrat\_data\_cells }\OtherTok{\textless{}{-}} 
\NormalTok{  poststrat\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(stateicp, gender, age\_group, education\_level) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Now we'd like to add proportions by state.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{poststrat\_data\_cells }\OtherTok{\textless{}{-}} 
\NormalTok{  poststrat\_data\_cells }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(stateicp) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{prop =}\NormalTok{ n}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(n)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{()}

\NormalTok{poststrat\_data\_cells }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 6}
\CommentTok{\#\textgreater{}   stateicp    gender age\_group      education\_level              n    prop}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}       \textless{}fct\textgreater{}  \textless{}fct\textgreater{}          \textless{}fct\textgreater{}                    \textless{}int\textgreater{}   \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 connecticut male   age\_18{-}29      Some post secondary        149 0.0260 }
\CommentTok{\#\textgreater{} 2 connecticut male   age\_18{-}29      High school or less        232 0.0404 }
\CommentTok{\#\textgreater{} 3 connecticut male   age\_18{-}29      Post secondary or higher    96 0.0167 }
\CommentTok{\#\textgreater{} 4 connecticut male   age\_18{-}29      Graduate degree             25 0.00436}
\CommentTok{\#\textgreater{} 5 connecticut male   age\_60\_or\_more Some post secondary        142 0.0248 }
\CommentTok{\#\textgreater{} 6 connecticut male   age\_60\_or\_more High school or less        371 0.0647}
\end{Highlighting}
\end{Shaded}

\hypertarget{model-4}{%
\subsection{Model}\label{model-4}}

We're going to use logistic regression to estimate a model where the binary of support for Biden is explained by gender, age-group, education-level, and state. We're going to do this in a Bayesian framework using \texttt{rstanarm} \citep{citerstanarm}. There are a variety of reasons for using \texttt{rstanarm} here, but the main one is that Stan is pre-compiled which eases some of the computer set-up issues that we may otherwise have. A great further resource about implementing MRP with \texttt{rstanarm} is \citet{kennedygabry2020}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rstanarm)}

\NormalTok{us\_election\_model }\OtherTok{\textless{}{-}} 
\NormalTok{  rstanarm}\SpecialCharTok{::}\FunctionTok{stan\_glmer}\NormalTok{(vote\_biden }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ age\_group }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ stateicp) }\SpecialCharTok{+}\NormalTok{ education\_level,}
                       \AttributeTok{data =}\NormalTok{ nationscape\_data,}
                       \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{),}
                       \AttributeTok{prior =} \FunctionTok{normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                       \AttributeTok{prior\_intercept =} \FunctionTok{normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{),}
                       \AttributeTok{cores =} \DecValTok{2}\NormalTok{, }
                       \AttributeTok{seed =} \DecValTok{853}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

There are a variety of options here that we've largely unthinkingly set, and exploring the effect of these would be a good idea, but for now we can just have a quick look at the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{modelsummary}\SpecialCharTok{::}\FunctionTok{get\_estimates}\NormalTok{(us\_election\_model)}
\CommentTok{\#\textgreater{}                                      term effect    estimate conf.level}
\CommentTok{\#\textgreater{} 1                             (Intercept)  fixed  0.27591651       0.95}
\CommentTok{\#\textgreater{} 2                              gendermale  fixed {-}0.54094841       0.95}
\CommentTok{\#\textgreater{} 3                 age\_groupage\_60\_or\_more  fixed  0.04886803       0.95}
\CommentTok{\#\textgreater{} 4                      age\_groupage\_18{-}29  fixed  0.87817445       0.95}
\CommentTok{\#\textgreater{} 5                      age\_groupage\_30{-}44  fixed  0.12455081       0.95}
\CommentTok{\#\textgreater{} 6      education\_levelHigh school or less  fixed {-}0.35080948       0.95}
\CommentTok{\#\textgreater{} 7      education\_levelSome post secondary  fixed {-}0.14970941       0.95}
\CommentTok{\#\textgreater{} 8          education\_levelGraduate degree  fixed {-}0.21988079       0.95}
\CommentTok{\#\textgreater{} 9 Sigma[stateicp:(Intercept),(Intercept)] random  0.08002654       0.95}
\CommentTok{\#\textgreater{}      conf.low    conf.high      pd rope.percentage      rhat      ess}
\CommentTok{\#\textgreater{} 1  0.10322566  0.447878764 0.99850       0.1281242 1.0002399 2262.704}
\CommentTok{\#\textgreater{} 2 {-}0.65227570 {-}0.430121742 1.00000       0.0000000 0.9996776 5264.121}
\CommentTok{\#\textgreater{} 3 {-}0.10847716  0.200831286 0.72600       0.9765851 0.9998468 3726.472}
\CommentTok{\#\textgreater{} 4  0.69830955  1.061899702 1.00000       0.0000000 0.9997188 3931.709}
\CommentTok{\#\textgreater{} 5 {-}0.02530517  0.284994549 0.94125       0.7729545 0.9995061 3567.152}
\CommentTok{\#\textgreater{} 6 {-}0.51262507 {-}0.204222132 1.00000       0.0000000 0.9997706 3700.321}
\CommentTok{\#\textgreater{} 7 {-}0.29927232  0.006135831 0.96625       0.6771902 1.0005471 4090.628}
\CommentTok{\#\textgreater{} 8 {-}0.38172115 {-}0.036764485 0.99100       0.3249145 0.9999286 4589.040}
\CommentTok{\#\textgreater{} 9  0.02912381  0.160211514 1.00000       1.0000000 1.0031942 1284.436}
\CommentTok{\#\textgreater{}   prior.distribution prior.location prior.scale}
\CommentTok{\#\textgreater{} 1             normal              0           1}
\CommentTok{\#\textgreater{} 2             normal              0           1}
\CommentTok{\#\textgreater{} 3             normal              0           1}
\CommentTok{\#\textgreater{} 4             normal              0           1}
\CommentTok{\#\textgreater{} 5             normal              0           1}
\CommentTok{\#\textgreater{} 6             normal              0           1}
\CommentTok{\#\textgreater{} 7             normal              0           1}
\CommentTok{\#\textgreater{} 8             normal              0           1}
\CommentTok{\#\textgreater{} 9               \textless{}NA\textgreater{}             NA          NA}
\CommentTok{\# The default usage of modelsummary requires statistics that we don\textquotesingle{}t have.}
\CommentTok{\# Uncomment the following line if you want to look at what is available and specify your own:}
\CommentTok{\# modelsummary::get\_estimates(us\_election\_model)}
\NormalTok{modelsummary}\SpecialCharTok{::}\FunctionTok{modelsummary}\NormalTok{(us\_election\_model,}
                            \AttributeTok{statistic =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}conf.low\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}conf.high\textquotesingle{}}\NormalTok{)}
\NormalTok{                            )}
\end{Highlighting}
\end{Shaded}

\begin{table}
\centering
\begin{tabular}[t]{lc}
\toprule
  & Model 1\\
\midrule
(Intercept) & \num{0.276}\\
 & (\num{0.103})\\
 & (\num{0.448})\\
gendermale & \num{-0.541}\\
 & (\num{-0.652})\\
 & (\num{-0.430})\\
age\_groupage\_60\_or\_more & \num{0.049}\\
 & (\num{-0.108})\\
 & (\num{0.201})\\
age\_groupage\_18-29 & \num{0.878}\\
 & (\num{0.698})\\
 & (\num{1.062})\\
age\_groupage\_30-44 & \num{0.125}\\
 & (\num{-0.025})\\
 & (\num{0.285})\\
education\_levelHigh school or less & \num{-0.351}\\
 & (\num{-0.513})\\
 & (\num{-0.204})\\
education\_levelSome post secondary & \num{-0.150}\\
 & (\num{-0.299})\\
 & (\num{0.006})\\
education\_levelGraduate degree & \num{-0.220}\\
 & (\num{-0.382})\\
 & (\num{-0.037})\\
Sigma[stateicp × (Intercept),(Intercept)] & \num{0.080}\\
 & (\num{0.029})\\
 & (\num{0.160})\\
\midrule
Num.Obs. & \num{5200}\\
R2 & \num{0.057}\\
R2 Marg. & \num{0.045}\\
ELPD & \num{-3468.0}\\
ELPD s.e. & \num{16.4}\\
LOOIC & \num{6936.1}\\
LOOIC s.e. & \num{32.8}\\
WAIC & \num{6936.0}\\
RMSE & \num{0.48}\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{post-stratify-2}{%
\subsection{Post-stratify}\label{post-stratify-2}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{biden\_support\_by\_state }\OtherTok{\textless{}{-}} 
\NormalTok{  us\_election\_model }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  tidybayes}\SpecialCharTok{::}\FunctionTok{add\_fitted\_draws}\NormalTok{(}\AttributeTok{newdata=}\NormalTok{poststrat\_data\_cells) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{support\_biden\_predict =}\NormalTok{ .value) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{support\_biden\_predict\_prop =}\NormalTok{ support\_biden\_predict}\SpecialCharTok{*}\NormalTok{prop) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(stateicp, .draw) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{support\_biden\_predict =} \FunctionTok{sum}\NormalTok{(support\_biden\_predict\_prop)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(stateicp) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(support\_biden\_predict), }
            \AttributeTok{lower =} \FunctionTok{quantile}\NormalTok{(support\_biden\_predict, }\FloatTok{0.025}\NormalTok{), }
            \AttributeTok{upper =} \FunctionTok{quantile}\NormalTok{(support\_biden\_predict, }\FloatTok{0.975}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

And we can have a look at our estimates, if we like.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{biden\_support\_by\_state }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ mean, }\AttributeTok{x =}\NormalTok{ stateicp, }\AttributeTok{color =} \StringTok{"MRP estimate"}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_errorbar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ lower, }\AttributeTok{ymax =}\NormalTok{ upper), }\AttributeTok{width =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =} 
\NormalTok{               nationscape\_data }\SpecialCharTok{\%\textgreater{}\%} 
               \FunctionTok{group\_by}\NormalTok{(stateicp, vote\_biden) }\SpecialCharTok{\%\textgreater{}\%}
               \FunctionTok{summarise}\NormalTok{(}\AttributeTok{n =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%} 
               \FunctionTok{group\_by}\NormalTok{(stateicp) }\SpecialCharTok{\%\textgreater{}\%} 
               \FunctionTok{mutate}\NormalTok{(}\AttributeTok{prop =}\NormalTok{ n}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(n)) }\SpecialCharTok{\%\textgreater{}\%} 
               \FunctionTok{filter}\NormalTok{(vote\_biden}\SpecialCharTok{==}\DecValTok{1}\NormalTok{), }
             \FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ prop, }\AttributeTok{x =}\NormalTok{ stateicp, }\AttributeTok{color =} \StringTok{\textquotesingle{}Nationscape raw data\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{linetype =} \StringTok{\textquotesingle{}dashed\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}State\textquotesingle{}}\NormalTok{,}
       \AttributeTok{y =} \StringTok{\textquotesingle{}Estimated proportion support for Biden\textquotesingle{}}\NormalTok{,}
       \AttributeTok{color =} \StringTok{\textquotesingle{}Source\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{\textquotesingle{}Set1\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{43-mrp_files/figure-latex/unnamed-chunk-30-1.pdf}

\hypertarget{concluding-remarks-and-next-steps}{%
\section{Concluding remarks and next steps}\label{concluding-remarks-and-next-steps}}

In general, MRP is a good way to accomplish specific aims, but it's not without trade-offs. If you have a good quality survey, then it may be a way to speak to disaggregated aspects of it. Or if you are concerned about uncertainty then it is a good way to think about that. If you have a biased survey then it's a great place to start, but it's not a panacea.

There's not a lot of work that's been done with it, so there's plenty of scope for exciting work from a variety of approaches:

\begin{itemize}
\tightlist
\item
  From a more statistical perspective, there is a lot of work to do in terms of thinking through how survey design and modelling approaches interact and the extent to which we are underestimating uncertainty. I'm also very interested in thinking through the implications of small samples and uncertainty in the post-stratification dataset. There's an awful lot to do in terms of thinking through what the appropriate model is to use, and how do we even evaluate what `appropriate' means here? Those with statistical interests, should probably go next to \citet{gao2021improving} as well as pretty much anything by \href{http://www-personal.umich.edu/~yajuan/}{Yajuan Si}, but \citet{si2020use} would be a starting point.
\item
  There's a lot to be done from a sociology perspective in terms of survey responses and how we can better design our surveys, knowing they are going to be used for MRP and putting respect for our respondents first.
\item
  From a political science perspective, we just have very little idea of the conditions under which we will have the stable preferences and relationships that are required for MRP to be accurate, and further understanding how this relates to uncertainty in survey design. For those with political science interests, a natural next step would be to go through \citet{lauderdale2020model} or \citet{ghitza2020voter}.
\item
  Economists might be interested to think about how we could use MRP to better understand the inflation and unemployment rates at local levels.
\item
  From a statistical software side of things, we really need to develop better packages around this.
\item
  It's from an information side of things, that I'm most excited about MRP. How do we best store and protect our datasets, yet retain the ability to have them correspond with each other? How do we put the levels together in a way that is meaningful? To what extent do people appreciate uncertainty estimates and how can we better communicate these estimates?
\end{itemize}

More generally, we could pretty much use MRP anywhere we have samples. Determining the conditions under which we actually should, is the work of whole generations.

\hypertarget{exercises-and-tutorial-16}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-16}}

\hypertarget{exercises-16}{%
\subsection{Exercises}\label{exercises-16}}

\hypertarget{tutorial-16}{%
\subsection{Tutorial}\label{tutorial-16}}

\hypertarget{text-as-data}{%
\chapter{Text as data}\label{text-as-data}}

\textbf{STATUS: Under construction.}

\textbf{Required reading}

\begin{itemize}
\tightlist
\item
  Hvitfeldt, Emil, and Julia Silge, 2021, \emph{Supervised Machine Learning for Text Analysis in R}, Chapters 2, 5, 6, 7, \url{https://smltar.com}.
\item
  Silge, Julia, and David Robinson, 2017, \emph{Text Mining with R}, \url{https://www.tidytextmining.com}.
\end{itemize}

\textbf{Required viewing}

\begin{itemize}
\tightlist
\item
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
  Amaka, Ofunne, and Amber Thomas, 2020, `The Naked Truth: How the names of 6,816 complexion products can reveal bias in beauty', The Pudding, March, \url{https://pudding.cool/2021/03/foundation-names/}.
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\tightlist
\item
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
\end{itemize}

\textbf{Key functions/etc}

\begin{itemize}
\tightlist
\item
\end{itemize}

\textbf{Quiz}

\begin{itemize}
\tightlist
\item
\end{itemize}

\hypertarget{introduction-25}{%
\section{Introduction}\label{introduction-25}}

Text can be thought of as an unwieldy, but in generally

\hypertarget{lasso-regression}{%
\section{Lasso regression}\label{lasso-regression}}

\textbf{This subsection, and much of the code that is used, directly draws on Julia Silge's notes, in particular: \url{https://juliasilge.com/blog/tidy-text-classification/} \citep{silge2018}.}

One of the nice aspects of text is that we can adapt our existing methods to use it as an input. Here we are going to use a variation of logistics regression, along with text inputs, to forecast. If you want to learn more about Lasso regression, then you should consider taking Arik's course over the summer, where he will dive into machine learning using Python.

In this section we are going to have two different text inputs, train a model on a sample of text from each of them, and then try to use that model to forecast the text in a training set. Although this is a arbitrary example, you could imagine many real-world applications. For instance, if you work at Twitter then you may want to know if a tweet was likely written by a bot, or by a human. Or similarly, imagine that you work for a political party - you may like to know if an email was likely from an email campaign organised by a group, or from an individual.

First we need to get some data. Julia Silge's example, nicely, uses book text as input. Seeing as I am jointly appointed at a Faculty of Information, that seems especially nice. The wonderful thing about this is that there is an R package - \texttt{gutenbergr} - that makes it easy to get text from Project Gutenberg into R. The key function is \texttt{gutenberg\_download()}, which needs a key for the book that you want. We'll consider Jane Eyre and Alice's Adventures in Wonderland, which have the keys of 1260 and 11, respectively.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(gutenbergr)}
\NormalTok{alice\_and\_jane }\OtherTok{\textless{}{-}}\NormalTok{ gutenbergr}\SpecialCharTok{::}\FunctionTok{gutenberg\_download}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1260}\NormalTok{, }\DecValTok{11}\NormalTok{), }\AttributeTok{meta\_fields =} \StringTok{"title"}\NormalTok{)}

\CommentTok{\# Save the dataset so that we don\textquotesingle{}t need to overwhelm the servers each time}
\FunctionTok{write\_csv}\NormalTok{(alice\_and\_jane, }\StringTok{"inputs/books/alice\_and\_jane.csv"}\NormalTok{)}

\FunctionTok{head}\NormalTok{(alice\_and\_jane)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(gutenbergr)}

\NormalTok{alice\_and\_jane }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"inputs/books/alice\_and\_jane.csv"}\NormalTok{)}

\FunctionTok{head}\NormalTok{(alice\_and\_jane)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{}   gutenberg\_id text                               title                         }
\CommentTok{\#\textgreater{}          \textless{}dbl\textgreater{} \textless{}chr\textgreater{}                              \textless{}chr\textgreater{}                         }
\CommentTok{\#\textgreater{} 1           11 ALICE\textquotesingle{}S ADVENTURES IN WONDERLAND   Alice\textquotesingle{}s Adventures in Wonderl\textasciitilde{}}
\CommentTok{\#\textgreater{} 2           11 \textless{}NA\textgreater{}                               Alice\textquotesingle{}s Adventures in Wonderl\textasciitilde{}}
\CommentTok{\#\textgreater{} 3           11 Lewis Carroll                      Alice\textquotesingle{}s Adventures in Wonderl\textasciitilde{}}
\CommentTok{\#\textgreater{} 4           11 \textless{}NA\textgreater{}                               Alice\textquotesingle{}s Adventures in Wonderl\textasciitilde{}}
\CommentTok{\#\textgreater{} 5           11 THE MILLENNIUM FULCRUM EDITION 3.0 Alice\textquotesingle{}s Adventures in Wonderl\textasciitilde{}}
\CommentTok{\#\textgreater{} 6           11 \textless{}NA\textgreater{}                               Alice\textquotesingle{}s Adventures in Wonderl\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

One of the great things about this is that the dataset is a tibble. So we can just work with all our familiar skills. The package has a lot more functionality, so I'd encourage you to look at the package's website: \url{https://github.com/ropensci/gutenbergr}. Each line of the book is read in as a different row in the dataset. Notice that we have downloaded two books here at once, and so we added the title. The two books are one after each other. You can see that we have both by looking at some summary statistics.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(alice\_and\_jane}\SpecialCharTok{$}\NormalTok{title)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Alice\textquotesingle{}s Adventures in Wonderland      Jane Eyre: An Autobiography }
\CommentTok{\#\textgreater{}                             3339                            20659}
\end{Highlighting}
\end{Shaded}

So it looks like Jane Eyre is much longer than Alice in Wonderland, which isn't a surprise to those who have read them. I don't want to step into Digital Humanities too much, as I don't know anything about it, but looking at things like the broader context of when these books were written, or other books that were written at similar times, is likely a fascinating area.

We'll just get rid of blank lines

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(janitor)}
\CommentTok{\# }\AlertTok{TODO}\CommentTok{ There\textquotesingle{}s a way to do this within janitor, but I forget, need to look it up.}
\NormalTok{alice\_and\_jane }\OtherTok{\textless{}{-}} 
\NormalTok{  alice\_and\_jane }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{blank\_line =} \FunctionTok{if\_else}\NormalTok{(text }\SpecialCharTok{==} \StringTok{""}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(blank\_line }\SpecialCharTok{==} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{blank\_line)}

\FunctionTok{table}\NormalTok{(alice\_and\_jane}\SpecialCharTok{$}\NormalTok{title)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Alice\textquotesingle{}s Adventures in Wonderland      Jane Eyre: An Autobiography }
\CommentTok{\#\textgreater{}                             2481                            16395}
\end{Highlighting}
\end{Shaded}

There's still an overwhelming amount of Jane Eyre in there. So we'll just sample from Jane Eyre to make it more equal.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{alice\_and\_jane}\SpecialCharTok{$}\NormalTok{rows }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(alice\_and\_jane))}
\NormalTok{sample\_from\_me }\OtherTok{\textless{}{-}}\NormalTok{ alice\_and\_jane }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(title }\SpecialCharTok{==} \StringTok{"Jane Eyre: An Autobiography"}\NormalTok{)}
\NormalTok{keep\_me }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =}\NormalTok{ sample\_from\_me}\SpecialCharTok{$}\NormalTok{rows, }\AttributeTok{size =} \DecValTok{2481}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{)}

\NormalTok{alice\_and\_jane }\OtherTok{\textless{}{-}} 
\NormalTok{  alice\_and\_jane }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(title }\SpecialCharTok{==} \StringTok{"Alice\textquotesingle{}s Adventures in Wonderland"} \SpecialCharTok{|}\NormalTok{ rows }\SpecialCharTok{\%in\%}\NormalTok{ keep\_me) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{rows)}

\FunctionTok{table}\NormalTok{(alice\_and\_jane}\SpecialCharTok{$}\NormalTok{title)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Alice\textquotesingle{}s Adventures in Wonderland      Jane Eyre: An Autobiography }
\CommentTok{\#\textgreater{}                             2481                             2481}
\end{Highlighting}
\end{Shaded}

There's a bunch of issues here, for instance, we have the whole of Alice, but we only have random bits of Jane, but nonetheless let's continue and we'll try to do something about that in a moment.

Now we want to get a sample of text from each book. We will use the lines to distinguish these samples. So we use a counter that will add a line number.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alice\_and\_jane }\OtherTok{\textless{}{-}} 
\NormalTok{  alice\_and\_jane }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(title) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{line\_number =} \FunctionTok{paste}\NormalTok{(gutenberg\_id, }\FunctionTok{row\_number}\NormalTok{(), }\AttributeTok{sep =} \StringTok{"\_"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

We now want to separate out the words. We'll just use tidytext, because the focus here is on modelling, but there are a bunch of alternatives and one especially good one is the \texttt{quanteda} package, specifically, the \texttt{tokens()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidytext)}
\NormalTok{alice\_and\_jane\_by\_word }\OtherTok{\textless{}{-}} 
\NormalTok{  alice\_and\_jane }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{unnest\_tokens}\NormalTok{(word, text) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(word) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{n}\NormalTok{() }\SpecialCharTok{\textgreater{}} \DecValTok{10}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Notice here that we removed any word that wasn't used more than 10 times. Nonetheless we still have a lot of unique words. (If we didn't require that the word be used by the author at least 10 times then we end up with more than 6,000 words.)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alice\_and\_jane\_by\_word}\SpecialCharTok{$}\NormalTok{word }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{unique}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{length}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] 585}
\end{Highlighting}
\end{Shaded}

The reason this is relevant is because these are our independent variables. So where you may be used to having something less than 10 explanatory variables, in this case we are going to have 585 As such, we need a model that can handle this.

However, as mentioned before, we are going to have some rows that essentially just had one word. While we could allow that, it might also be nice to give the model at least a few words to work with.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alice\_and\_jane\_by\_word }\OtherTok{\textless{}{-}} 
\NormalTok{  alice\_and\_jane\_by\_word }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(title, line\_number) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{number\_of\_words\_in\_line =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(number\_of\_words\_in\_line }\SpecialCharTok{\textgreater{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{number\_of\_words\_in\_line)}
\end{Highlighting}
\end{Shaded}

We'll create a test/training split, and load in \texttt{tidymodels}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidymodels)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{alice\_and\_jane\_by\_word\_split }\OtherTok{\textless{}{-}} 
\NormalTok{  alice\_and\_jane\_by\_word }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(title, line\_number) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{distinct}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{initial\_split}\NormalTok{(}\AttributeTok{prop =} \DecValTok{3}\SpecialCharTok{/}\DecValTok{4}\NormalTok{, }\AttributeTok{strata =}\NormalTok{ title)}

\CommentTok{\# alice\_and\_jane\_by\_word\_train \textless{}{-} training(alice\_and\_jane\_by\_word\_split) \%\textgreater{}\% select(line\_number)}
\CommentTok{\# alice\_and\_jane\_by\_word\_test \textless{}{-} testing(alice\_and\_jane\_by\_word\_split)}
\CommentTok{\# }
\CommentTok{\# rm(alice\_and\_jane\_by\_word\_split)}
\end{Highlighting}
\end{Shaded}

Now we need to create a document-term matrix.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alice\_and\_jane\_dtm\_training }\OtherTok{\textless{}{-}} 
\NormalTok{  alice\_and\_jane\_by\_word }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(line\_number, word) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{inner\_join}\NormalTok{(}\FunctionTok{training}\NormalTok{(alice\_and\_jane\_by\_word\_split) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(line\_number)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{cast\_dtm}\NormalTok{(}\AttributeTok{term =}\NormalTok{ word, }\AttributeTok{document =}\NormalTok{ line\_number, }\AttributeTok{value =}\NormalTok{ n)}

\FunctionTok{dim}\NormalTok{(alice\_and\_jane\_dtm\_training)}
\CommentTok{\#\textgreater{} [1] 3413  585}
\end{Highlighting}
\end{Shaded}

So we have our independent variables sorted, now we need our binary dependent variable, which is whether the book is Alice in Wonderland or Jane Eyre.

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{response }\OtherTok{\textless{}{-}} 
  \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{id =} \FunctionTok{dimnames}\NormalTok{(alice\_and\_jane\_dtm\_training)[[}\DecValTok{1}\NormalTok{]]) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{separate}\NormalTok{(id, }\AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"book"}\NormalTok{, }\StringTok{"line"}\NormalTok{, }\AttributeTok{sep =} \StringTok{"\_"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{is\_alice =} \FunctionTok{if\_else}\NormalTok{(book }\SpecialCharTok{==} \DecValTok{11}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)) }
  

\NormalTok{predictor }\OtherTok{\textless{}{-}}\NormalTok{ alice\_and\_jane\_dtm\_training[] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.matrix}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Now we can run our model.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(glmnet)}

\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{cv.glmnet}\NormalTok{(}\AttributeTok{x =}\NormalTok{ predictor,}
                   \AttributeTok{y =}\NormalTok{ response}\SpecialCharTok{$}\NormalTok{is\_alice,}
                   \AttributeTok{family =} \StringTok{"binomial"}\NormalTok{,}
                   \AttributeTok{keep =} \ConstantTok{TRUE}
\NormalTok{                   )}

\FunctionTok{save}\NormalTok{(model, }\AttributeTok{file =} \StringTok{"outputs/models/alice\_vs\_jane.rda"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{load}\NormalTok{(}\StringTok{"outputs/models/alice\_vs\_jane.rda"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(glmnet)}
\FunctionTok{library}\NormalTok{(broom)}

\NormalTok{coefs }\OtherTok{\textless{}{-}}\NormalTok{ model}\SpecialCharTok{$}\NormalTok{glmnet.fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tidy}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(lambda }\SpecialCharTok{==}\NormalTok{ model}\SpecialCharTok{$}\NormalTok{lambda}\FloatTok{.1}\NormalTok{se)}

\NormalTok{coefs }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 5}
\CommentTok{\#\textgreater{}   term         step estimate  lambda dev.ratio}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}       \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 (Intercept)    36 {-}0.335   0.00597     0.562}
\CommentTok{\#\textgreater{} 2 in             36 {-}0.144   0.00597     0.562}
\CommentTok{\#\textgreater{} 3 she            36  0.390   0.00597     0.562}
\CommentTok{\#\textgreater{} 4 so             36  0.00249 0.00597     0.562}
\CommentTok{\#\textgreater{} 5 a              36 {-}0.117   0.00597     0.562}
\CommentTok{\#\textgreater{} 6 about          36  0.279   0.00597     0.562}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coefs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(estimate }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{top\_n}\NormalTok{(}\DecValTok{10}\NormalTok{, }\FunctionTok{abs}\NormalTok{(estimate)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\FunctionTok{fct\_reorder}\NormalTok{(term, estimate), estimate, }\AttributeTok{fill =}\NormalTok{ estimate }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Coefficient"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Word"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{44-text_files/figure-latex/unnamed-chunk-15-1.pdf}

Perhaps unsurprisingly, if you mention Alice then it's likely to be a Alice in Wonderland and if you mention Jane then it's likely to be Jane Eyre.

\hypertarget{topic-models}{%
\section{Topic models}\label{topic-models}}

\textbf{A version of these notes was previously circulated as part of \citet{Alexander2020}.}

\hypertarget{overview-8}{%
\subsection{Overview}\label{overview-8}}

Sometimes we have a statement and we want to know what it is about. Sometimes this will be easy, but we don't always have titles for statements, and even when we do, sometimes we do not have titles that define topics in a well-defined and consistent way. One way to get consistent estimates of the topics of each statement is to use topic models. While there are many variants, one way is to use the latent Dirichlet allocation (LDA) method of \citet{Blei2003latent}, as implemented by the R package `topicmodels' by \citet{Grun2011}.

The key assumption behind the LDA method is that each statement, `a document', is made by a person who decides the topics they would like to talk about in that document, and then chooses words, `terms', that are appropriate to those topics. A topic could be thought of as a collection of terms, and a document as a collection of topics. The topics are not specified \emph{ex ante}; they are an outcome of the method. Terms are not necessarily unique to a particular topic, and a document could be about more than one topic. This provides more flexibility than other approaches such as a strict word count method. The goal is to have the words found in documents group themselves to define topics.

\hypertarget{document-generation-process}{%
\subsection{Document generation process}\label{document-generation-process}}

The LDA method considers each statement to be a result of a process where a person first chooses the topics they want to speak about. After choosing the topics, the person then chooses appropriate words to use for each of those topics. More generally, the LDA topic model works by considering each document as having been generated by some probability distribution over topics. For instance, if there were five topics and two documents, then the first document may be comprised mostly of the first few topics; the other document may be mostly about the final few topics (Figure \ref{fig:topicsoverdocuments}).

\includegraphics{44-text_files/figure-latex/topicsoverdocuments-1.pdf} \includegraphics{44-text_files/figure-latex/topicsoverdocuments-2.pdf}

Similarly, each topic could be considered a probability distribution over terms. To choose the terms used in each document the speaker picks terms from each topic in the appropriate proportion. For instance, if there were ten terms, then one topic could be defined by giving more weight to terms related to immigration; and some other topic may give more weight to terms related to the economy (Figure \ref{fig:topicsoverterms}).

\includegraphics{44-text_files/figure-latex/topicsoverterms-1.pdf} \includegraphics{44-text_files/figure-latex/topicsoverterms-2.pdf}

Following \citet{BleiLafferty2009}, \citet{blei2012} and \citet{GriffithsSteyvers2004}, the process by which a document is generated is more formally considered to be:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  There are \(1, 2, \dots, k, \dots, K\) topics and the vocabulary consists of \(1, 2, \dots, V\) terms. For each topic, decide the terms that the topic uses by randomly drawing distributions over the terms. The distribution over the terms for the \(k\)th topic is \(\beta_k\). Typically a topic would be a small number of terms and so the Dirichlet distribution with hyperparameter \(0<\eta<1\) is used: \(\beta_k \sim \mbox{Dirichlet}(\eta)\).\footnote{The Dirichlet distribution is a variation of the beta distribution that is commonly used as a prior for categorical and multinomial variables. If there are just two categories, then the Dirichlet and the beta distributions are the same. In the special case of a symmetric Dirichlet distribution, \(\eta=1\), it is equivalent to a uniform distribution. If \(\eta<1\), then the distribution is sparse and concentrated on a smaller number of the values, and this number decreases as \(\eta\) decreases. A hyperparameter is a parameter of a prior distribution.} Strictly, \(\eta\) is actually a vector of hyperparameters, one for each \(K\), but in practice they all tend to be the same value.
\item
  Decide the topics that each document will cover by randomly drawing distributions over the \(K\) topics for each of the \(1, 2, \dots, d, \dots, D\) documents. The topic distributions for the \(d\)th document are \(\theta_d\), and \(\theta_{d,k}\) is the topic distribution for topic \(k\) in document \(d\). Again, the Dirichlet distribution with the hyperparameter \(0<\alpha<1\) is used here because usually a document would only cover a handful of topics: \(\theta_d \sim \mbox{Dirichlet}(\alpha)\). Again, strictly \(\alpha\) is vector of length \(K\) of hyperparameters, but in practice each is usually the same value.
\item
  If there are \(1, 2, \dots, n, \dots, N\) terms in the \(d\)th document, then to choose the \(n\)th term, \(w_{d, n}\):

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Randomly choose a topic for that term \(n\), in that document \(d\), \(z_{d,n}\), from the multinomial distribution over topics in that document, \(z_{d,n} \sim \mbox{Multinomial}(\theta_d)\).
  \item
    Randomly choose a term from the relevant multinomial distribution over the terms for that topic, \(w_{d,n} \sim \mbox{Multinomial}(\beta_{z_{d,n}})\).
  \end{enumerate}
\end{enumerate}

Given this set-up, the joint distribution for the variables is (\citet{blei2012}, p.6):
\[p(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}) = \prod^{K}_{i=1}p(\beta_i) \prod^{D}_{d=1}p(\theta_d) \left(\prod^N_{n=1}p(z_{d,n}|\theta_d)p\left(w_{d,n}|\beta_{1:K},z_{d,n}\right) \right).\]

Based on this document generation process the analysis problem, discussed in the next section, is to compute a posterior over \(\beta_{1:K}\) and \(\theta_{1:D}\), given \(w_{1:D, 1:N}\). This is intractable directly, but can be approximated (\citet{GriffithsSteyvers2004} and \citet{blei2012}).

\hypertarget{analysis-process}{%
\subsection{Analysis process}\label{analysis-process}}

After the documents are created, they are all that we have to analyse. The term usage in each document, \(w_{1:D, 1:N}\), is observed, but the topics are hidden, or `latent'. We do not know the topics of each document, nor how terms defined the topics. That is, we do not know the probability distributions of Figures \ref{fig:topicsoverdocuments} or \ref{fig:topicsoverterms}. In a sense we are trying to reverse the document generation process -- we have the terms and we would like to discover the topics.

If the earlier process around how the documents were generated is assumed and we observe the terms in each document, then we can obtain estimates of the topics (\citet{SteyversGriffiths2006}). The outcomes of the LDA process are probability distributions and these define the topics. Each term will be given a probability of being a member of a particular topic, and each document will be given a probability of being about a particular topic. That is, we are trying to calculate the posterior distribution of the topics given the terms observed in each document (\citet{blei2012}, p.7):
\[p(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N} | w_{1:D, 1:N}) = \frac{p\left(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}\right)}{p(w_{1:D, 1:N})}.\]

The initial practical step when implementing LDA given a corpus of documents is to remove `stop words'. These are words that are common, but that don't typically help to define topics. There is a general list of stop words such as: ``a''; ``a's''; ``able''; ``about''; ``above''\ldots{} We also remove punctuation and capitalisation. The documents need to then be transformed into a document-term-matrix. This is essentially a table with a column of the number of times each term appears in each document.

After the dataset is ready, the R package `topicmodels' by \citet{Grun2011} can be used to implement LDA and approximate the posterior. It does this using Gibbs sampling or the variational expectation-maximization algorithm. Following \citet{SteyversGriffiths2006} and \citet{Darling2011}, the Gibbs sampling process attempts to find a topic for a particular term in a particular document, given the topics of all other terms for all other documents. Broadly, it does this by first assigning every term in every document to a random topic, specified by Dirichlet priors with \(\alpha = \frac{50}{K}\) and \(\eta = 0.1\) (\citet{SteyversGriffiths2006} recommends \(\eta = 0.01\)), where \(\alpha\) refers to the distribution over topics and \(\eta\) refers to the distribution over terms (\citet{Grun2011}, p.7). It then selects a particular term in a particular document and assigns it to a new topic based on the conditional distribution where the topics for all other terms in all documents are taken as given (\citet{Grun2011}, p.6):
\[p(z_{d, n}=k | w_{1:D, 1:N}, z'_{d, n}) \propto \frac{\lambda'_{n\rightarrow k}+\eta}{\lambda'_{.\rightarrow k}+V\eta} \frac{\lambda'^{(d)}_{n\rightarrow k}+\alpha}{\lambda'^{(d)}_{-i}+K\alpha} \]
where \(z'_{d, n}\) refers to all other topic assignments; \(\lambda'_{n\rightarrow k}\) is a count of how many other times that term has been assigned to topic \(k\); \(\lambda'_{.\rightarrow k}\) is a count of how many other times that any term has been assigned to topic \(k\); \(\lambda'^{(d)}_{n\rightarrow k}\) is a count of how many other times that term has been assigned to topic \(k\) in that particular document; and \(\lambda'^{(d)}_{-i}\) is a count of how many other times that term has been assigned in that document. Once \(z_{d,n}\) has been estimated, then estimates for the distribution of words into topics and topics into documents can be backed out.

This conditional distribution assigns topics depending on how often a term has been assigned to that topic previously, and how common the topic is in that document (\citet{SteyversGriffiths2006}). The initial random allocation of topics means that the results of early passes through the corpus of document are poor, but given enough time the algorithm converges to an appropriate estimate.

\hypertarget{warnings-and-extensions}{%
\subsection{Warnings and extensions}\label{warnings-and-extensions}}

The choice of the number of topics, \emph{k}, affects the results, and must be specified \emph{a priori}. If there is a strong reason for a particular number, then this can be used. Otherwise, one way to choose an appropriate number is to use a test and training set process. Essentially, this means running the process on a variety of possible values for \emph{k} and then picking an appropriate value that performs well.

One weakness of the LDA method is that it considers a `bag of words' where the order of those words does not matter (\citet{blei2012}). It is possible to extend the model to reduce the impact of the bag-of-words assumption and add conditionality to word order. Additionally, alternatives to the Dirichlet distribution can be used to extend the model to allow for correlation. For instance, in Hansard topics related the army may be expected to be more commonly found with topics related to the navy, but less commonly with topics related to banking.

\hypertarget{word-embedding}{%
\section{Word embedding}\label{word-embedding}}

\hypertarget{conclusion-1}{%
\section{Conclusion}\label{conclusion-1}}

Using text as data is exciting because of the quantity and variety of text that is available to us. In general, dealing with text datasets is messy. There is a lot of cleaning and preparation that is typically required. Often text datasets are large. As such, having a workflow in place, in which you work in a reproducible way, simulating data first, and then clearly communicating your findings becomes critical, if only to keep everything organised in your own mind. Nonetheless, it is an exciting area, and I encourage you to regularly use text analysis where possible.

In terms of next steps there are two, related, concerns: data and analysis.

In terms of data there are many places to get large amounts of text data relatively easily, including:

\begin{itemize}
\tightlist
\item
  The r package \texttt{rtweets} makes it easy to get Twitter data (although typically this is going to be looking forward from when you start using it, rather than being able to look back). Plenty of people at U of T work with Twitter data including Jia Xue in the iSchool, and Ludovic Rheault in political science.
\item
  The inside Airbnb dataset that we used earlier provides text from reviews.
\item
  We've seen the \texttt{gutenbergr} package already in these notes, which provides easy access to text from Project Gutenberg.
\item
  We've seen scraping of Wikipedia, but if you are going to do a bit of this then you may find it better to use a package, for instance \texttt{WikipediR}.
\end{itemize}

In terms of analysis:

\begin{itemize}
\tightlist
\item
  Start by going through the tidytext book, \texttt{tidytext}, as it has a lot of nice explanations, code, and examples.
\item
  It would then be worthwhile working through the Quanteda package \texttt{quanteda} tutorials.
\item
  Finally, consider packages such as \texttt{text2vec}, and \texttt{spacyr}.
\end{itemize}

\hypertarget{exercises-and-tutorial-17}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-17}}

\hypertarget{exercises-17}{%
\subsection{Exercises}\label{exercises-17}}

\hypertarget{tutorial-17}{%
\subsection{Tutorial}\label{tutorial-17}}

\hypertarget{part-enrichment}{%
\part{Enrichment}\label{part-enrichment}}

\hypertarget{using-the-cloud}{%
\chapter{Using the cloud}\label{using-the-cloud}}

\textbf{STATUS: Under construction.}

\textbf{Required reading}

\begin{itemize}
\tightlist
\item
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
  Edmondson, Mark, 2020, `googleComputeEngineR documentation', version 0.3.0.9000, freely available at: \url{https://cloudyr.github.io/googleComputeEngineR/}.
\item
  McDermott, Grant R., 2020, `Cloud computing with Google Compute Engine', \emph{Data Science for Economists}, freely available at: \url{https://raw.githack.com/uo-ec607/lectures/master/14-gce/14-gce.html}.
\item
  Morris, Mitzi, 2020, `Stan Notebooks in the Cloud', freely available at: \url{https://mc-stan.org/users/documentation/case-studies/jupyter_colab_notebooks_2020.html}.
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\tightlist
\item
  Benefits/costs of cloud.
\item
  Getting started in the cloud.
\item
  Starting virtual machines with R Studio.
\item
  Stopping virtual machines.
\end{itemize}

\hypertarget{introduction-26}{%
\section{Introduction}\label{introduction-26}}

Cloud benefits:
- Costs can be reduced, or more easily amortized.
- Can scale as you need.
- Many platforms are already sorted out e.g.~R Studio just works.

I stole this from someone and I can't remember who, but the cloud is another name for `someone else's computer'. That's it. Nonetheless, learning to use someone else's computer can be great for a number of reasons including:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Scalability: It can be quite expensive to buy a new computer, especially if you only need it to run something every now and then, but by using someone else's computer, you can just rent for a few hours or days.
\item
  Portability: If you can shift your analysis workflow from your laptop to the cloud, then that suggests that you are likely doing good things in terms of reproducibility and portability. At the very least, your code is capable of running on your laptop and the cloud.
\item
  Set-and-forget: If you are doing something that will take a while, then it can be great to not have to worry about your laptop's fan running overnight, or your partner/baby/pet/housemate/etc accidently closing your computer, or not being able to watch Netflix on that same computer.
\end{enumerate}

When you use the cloud you are running your code on a `virtual machine'. This is a part of a larger bunch of computers that has been designed to act like a computer with specific features. For instance you may specify that your virtual machine has 8 GB RAM, 128 storage, and 4 CPUs. Your VM would then act like a computer with those specifications. The cost to use cloud options increases based on the specifications of the virtual machine that you choose.

There are a few downsides:

\begin{itemize}
\tightlist
\item
  Cost: While most cloud options are cheap, they are rarely free. (While there are free options, they tend to not be very powerful, and so you end up having to pay to get a computer that is better than your laptop.) To give you an idea of cost, when I use AWS, I typically end up spending five to ten dollars for a couple of days. So it's fairly cheap, but it's not nothing. It's also pretty easy to accidently forget about something and run up an unexpected bill, especially initially.
\item
  Public: It is pretty easy to make mistakes and accidently make everything public.
\item
  Time: It takes time to get set-up and comfortable on the cloud.
\end{itemize}

In these notes we are going to introduce the cloud starting with some options that pretty much anyone can (and should) take advantage of: Google Colab; and then moving to more general cloud options including Google Compute Engine, AWS, and Azure, which may be useful to some of you in some cases. If you want to get a job in industry, then the advice of pretty much every speaker from industry at the Toronto Data Workshop is that you learn at least one of those cloud options. For instance, Munich Re is an Azure shop, Receptiviti uses AWS, etc.

\hypertarget{google-colab}{%
\section{Google Colab}\label{google-colab}}

Google Colab is similar to R Studio Cloud, in that it is set-up to allow you to just log in and get started. In this case, you need a Google account. It's better than R Studio because they have more resources to put into its development and you can use GPUs, but but on the other hand it is designed for Python, and while we can use it for R, it's not really focused on that.

To get started you need to tell Google Colab that you want to use R. You can do this by using this: \url{https://colab.research.google.com/notebook\#create=true\&language=r}.

At this point you have a Jupyter notebook open that will run R. (But it is not a R Markdown document.) You can install packages as normal, e.g.~\texttt{install.packages("tidyverse")}, and then call the package e.g.~\texttt{library(tidyverse)}.

Google Colab is a good option if you have a good reason for using the broader capabilities that it has. If you want to go deeper into that then the Morris reading has a bunch of options that you can explore, but as Morris puts it `Colab is a gateway drug - for large-scale processing pipelines you'll need to move up to Google Cloud Platform or one of its competitors AWS, Azure, etc.' and that is what we will do now.

\hypertarget{aws}{%
\section{AWS}\label{aws}}

Amazon Web Services is a cloud service from Amazon. To get started you need an AWS Developer account which you can create here: \url{https://aws.amazon.com/developer/}.

After you have created an account, you need to select a region where the computer that you will access is located. After this, you will want to ``Launch a virtual machine'' (with EC2).

The first step is to choose an Amazon Machine Image (AMI). This provides the details of the computer that you will be using. For instance, your local computer may be a MacBook running Catalina. Helpfully, Louis Aslett provides a bunch of these already set up - \url{http://www.louisaslett.com/RStudio_AMI/}. You can either select the code for the region that you registered for, or you can click on the link. The benefit of this AMI is that they are set-up specifically for R Studio, however the trade-off is that they are a little out-dated, as they were compiled in May 2019.

In the next step you can choose how powerful the computer will be. The free tier has a fairly basic computer, but you can choose better ones when you need them. At this point you can pretty much just launch the instance. If you start using AWS more seriously then you should look into different security settings.

Your instance is now running. You can go to it by pasting the `public DNS' into a browser. The username is `rstudio' and the password is your instance ID.

You should have R Studio running, which is exciting. The first thing to do is probably to change the default password using the instructions in the instance.

You don't need to install, say, the tidyverse, instead you can just call the library and keep going. You can see the list of packages that are installed with \texttt{installed.packages()}. For instance, \texttt{rstan} is already installed. And you can use GPUs if you want.

Perhaps as important as being able to start an AWS instance is being able to stop it (so that you don't get billed). The free tier is pretty great, but you do need to turn it off. To stop an instance, in the AWS instances page, select it, then `Actions -\textgreater{} Instance State -\textgreater{} Terminate'.

\hypertarget{google-compute-engine}{%
\section{Google Compute Engine}\label{google-compute-engine}}

The main R package related to Google Compute Engine seems to be: \texttt{googleComputeEngineR}.

The reading from Grant McDermott is a pretty good walk-through.

\hypertarget{azure}{%
\section{Azure}\label{azure}}

There are a bunch of R packages related to Azure here: \url{https://github.com/Azure/AzureR}.

\hypertarget{exercises-and-tutorial-18}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-18}}

\hypertarget{exercises-18}{%
\subsection{Exercises}\label{exercises-18}}

\hypertarget{tutorial-18}{%
\subsection{Tutorial}\label{tutorial-18}}

\hypertarget{deploying-models}{%
\chapter{Deploying models}\label{deploying-models}}

\textbf{STATUS: Under construction.}

\textbf{Required reading}

\begin{itemize}
\tightlist
\item
  Chip Huyen, 2020, `Machine learning is going real-time', 27 December, \url{https://huyenchip.com/2020/12/27/real-time-machine-learning.html}.
\end{itemize}

\textbf{Required viewing}

\begin{itemize}
\tightlist
\item
  Blair, James, 2019, `Democratizing R with Plumber APIs', RStudio Conference, 24 January, \url{https://www.rstudio.com/resources/rstudioconf-2019/democratizing-r-with-plumber-apis/}.
\item
  Nolis, Heather, and Jacqueline Nolis, `We're hitting R a million times a day so we made a talk about it', RStudio Conference, 30 January, \url{https://www.rstudio.com/resources/rstudioconf-2020/we-re-hitting-r-a-million-times-a-day-so-we-made-a-talk-about-it/}.
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\tightlist
\item
  Putting models into production requires a different set of skills to building a model. We need a familiarity with some cloud provider, APIs, and of course modelling. But the biggest difficulty, for me, is getting things set-up.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{plumber}
\item
  \texttt{shiny}
\end{itemize}

\textbf{Key functions}

\begin{itemize}
\tightlist
\item
\end{itemize}

\textbf{Quiz}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
\end{enumerate}

\hypertarget{introduction-27}{%
\section{Introduction}\label{introduction-27}}

A key troupe against R is that it's not for production. I'm not here to convince you one way or another, however in this section we will go through a bunch of different tools that would allow you to do a lot in R if you wanted. The topics that we cover are:

\begin{itemize}
\tightlist
\item
  SQL databases.
\item
  Docker.
\item
  Plumber and APIs for models.
\item
  Shiny
\item
  Packages
\end{itemize}

The general idea here is that you need to know the whole workflow. To this point, you've been able to scrape some data from a website, bring some order to that chaos, make some charts, appropriately model it, and write this all up. In most academic settings that is more than enough. But in many industry settings we're going to want to use the model to do something. For instance, set-up a website that allows your model to be used to generate an insurance quote given several inputs.

One way to deploy your model is to use Shiny, and we have seen examples of this earlier in the notes. That enables an individual to use your model. But it doesn't really scale very well. For instance, if we wanted to sell our model forecasts to other businesses, then they might have their own way in which they would like users to interact with the results. The general problem is that we want our model results available to other machines and for that we will want to make an APIs.

\hypertarget{packages}{%
\section{Packages}\label{packages}}

To this point we've largely been using R Packages to do things for us. However, another way is to have them loaded

DoSS Toolkit

\hypertarget{shiny-1}{%
\section{Shiny}\label{shiny-1}}

\hypertarget{plumber-and-model-apis}{%
\section{Plumber and model APIs}\label{plumber-and-model-apis}}

\hypertarget{hello-toronto}{%
\subsection{Hello Toronto}\label{hello-toronto}}

The general idea behind the \texttt{plumber} package \citep{citeplumber} is that we can train a model and make it available via an API that we can call when we want a forecast. It's pretty great.

Just to get something working, let's make a function that returns `Hello Toronto' regardless of the output. Open a new R file, add the following, and then save it as `plumber.R' (you may need to install the \texttt{plumber} package if you've not done that yet).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(plumber)}

\CommentTok{\#* @get /print\_toronto}
\NormalTok{print\_toronto }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() \{}
\NormalTok{  result }\OtherTok{\textless{}{-}} \StringTok{"Hello Toronto"}
  \FunctionTok{return}\NormalTok{(result)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

After that is saved, in the top right of the editor you should get a button to `Run API'. Click that, and your API should load. It'll be a `Swagger' application, which provides a GUI around our API. Expand the GET method, and then clik `Try it out' and `Execute'. In the response body, you should get `Toronto'.

To more closely reflect the fact that this is an API designed for computers, you can copy/paste the `request HTML' into a browser and it should return `Hello Toronto'.

\hypertarget{local-model}{%
\subsection{Local model}\label{local-model}}

Now, we're going to update the API so that it serves a model output, given some input. We're going to follow \citet{buhrplumber} fairly closely.

At this point, I'd recommend starting a new R Project. To get started, let's simulate some data and then train a model on it. In this case we're interested in forecasting how long a baby may sleep overnight, given we know how long they slept during their afternoon nap.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{number\_of\_observations }\OtherTok{\textless{}{-}} \DecValTok{1000}

\NormalTok{baby\_sleep }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{afternoon\_nap\_length =} \FunctionTok{rnorm}\NormalTok{(number\_of\_observations, }\DecValTok{120}\NormalTok{, }\DecValTok{5}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{abs}\NormalTok{(),}
         \AttributeTok{noise =} \FunctionTok{rnorm}\NormalTok{(number\_of\_observations, }\DecValTok{0}\NormalTok{, }\DecValTok{120}\NormalTok{),}
         \AttributeTok{night\_sleep\_length =}\NormalTok{ afternoon\_nap\_length }\SpecialCharTok{*} \DecValTok{4} \SpecialCharTok{+}\NormalTok{ noise,}
\NormalTok{         )}

\NormalTok{baby\_sleep }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ afternoon\_nap\_length, }\AttributeTok{y =}\NormalTok{ night\_sleep\_length)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Baby\textquotesingle{}s afternoon nap length (minutes)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Baby\textquotesingle{}s overnight sleep length (minutes)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Let's now use \texttt{tidymodels} to quickly make a dodgy model.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}
\FunctionTok{library}\NormalTok{(tidymodels)}

\NormalTok{baby\_sleep\_split }\OtherTok{\textless{}{-}}\NormalTok{ rsample}\SpecialCharTok{::}\FunctionTok{initial\_split}\NormalTok{(baby\_sleep, }\AttributeTok{prop =} \FloatTok{0.80}\NormalTok{)}
\NormalTok{baby\_sleep\_train }\OtherTok{\textless{}{-}}\NormalTok{ rsample}\SpecialCharTok{::}\FunctionTok{training}\NormalTok{(baby\_sleep\_split)}
\NormalTok{baby\_sleep\_test }\OtherTok{\textless{}{-}}\NormalTok{ rsample}\SpecialCharTok{::}\FunctionTok{testing}\NormalTok{(baby\_sleep\_split)}

\NormalTok{model }\OtherTok{\textless{}{-}} 
\NormalTok{  parsnip}\SpecialCharTok{::}\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  parsnip}\SpecialCharTok{::}\FunctionTok{set\_engine}\NormalTok{(}\AttributeTok{engine =} \StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  parsnip}\SpecialCharTok{::}\FunctionTok{fit}\NormalTok{(night\_sleep\_length }\SpecialCharTok{\textasciitilde{}}\NormalTok{ afternoon\_nap\_length, }
               \AttributeTok{data =}\NormalTok{ baby\_sleep\_train}
\NormalTok{               )}

\FunctionTok{write\_rds}\NormalTok{(}\AttributeTok{x =}\NormalTok{ model, }\AttributeTok{file =} \StringTok{"baby\_sleep.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

At this point, we have a model. One difference from what you might be used to is that we've saved the model as an `.rds' file. We are going to read that in.

Now that we have our model we want to put that into a file that we will use the API to access, again called `plumber.R'. And we also want a file that sets up the API, called `server.R'. So make an R script called `server.R' and add the following content:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(plumber)}

\NormalTok{serve\_model }\OtherTok{\textless{}{-}} \FunctionTok{plumb}\NormalTok{(}\StringTok{"plumber.R"}\NormalTok{)}
\NormalTok{serve\_model}\SpecialCharTok{$}\FunctionTok{run}\NormalTok{(}\AttributeTok{port =} \DecValTok{8000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then in `plumber.R' add the following content:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(plumber)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"baby\_sleep.rds"}\NormalTok{)}

\NormalTok{version\_number }\OtherTok{\textless{}{-}} \StringTok{"0.0.1"}

\NormalTok{variables }\OtherTok{\textless{}{-}} 
  \FunctionTok{list}\NormalTok{(}
    \AttributeTok{afternoon\_nap\_length =} \StringTok{"A value in minutes, likely between 0 and 240."}\NormalTok{,}
    \AttributeTok{night\_sleep\_length =} \StringTok{"A forecast, in minutes, likely between 0 and 1000."}
\NormalTok{  )}

\CommentTok{\#* @param afternoon\_nap\_length}
\CommentTok{\#* @get /survival}
\NormalTok{predict\_sleep }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{afternoon\_nap\_length=}\DecValTok{0}\NormalTok{) \{}
\NormalTok{  afternoon\_nap\_length }\OtherTok{=} \FunctionTok{as.integer}\NormalTok{(afternoon\_nap\_length)}
  
\NormalTok{  payload }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{afternoon\_nap\_length=}\NormalTok{afternoon\_nap\_length)}
  
\NormalTok{  prediction }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(model, payload)}

\NormalTok{  result }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
    \AttributeTok{input =} \FunctionTok{list}\NormalTok{(payload),}
    \AttributeTok{response =} \FunctionTok{list}\NormalTok{(}\StringTok{"estimated\_night\_sleep"} \OtherTok{=}\NormalTok{ prediction),}
    \AttributeTok{status =} \DecValTok{200}\NormalTok{,}
    \AttributeTok{model\_version =}\NormalTok{ version\_number)}

  \FunctionTok{return}\NormalTok{(result)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Again, after you save the `plumber.R' file you should have an option to `Run API'. Click that and you can try out the API locally in the same way as before.

\hypertarget{cloud-model}{%
\subsection{Cloud model}\label{cloud-model}}

To this point, we've got an API working on our own machine, but what we really want to do is to get it working on a computer such that the API can be accessed by anyone. To do this we are going to use DigitalOcean - \url{https://www.digitalocean.com}. It is a charged service, but when you create an account, it will come with \$100 in credit, which will be enough to get started.

This set-up process will be a pain and take some time, but you only need to do it once. Install two additional packages that will assist here:

\begin{itemize}
\tightlist
\item
  \texttt{plumberDeploy} \citep{citeplumberdeploy}.
\item
  \texttt{analogsea} \citep{citeanalogsea}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"plumberDeploy"}\NormalTok{)}
\NormalTok{remotes}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{"sckott/analogsea"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now we need to connect your local computer with your DigitalOcean account. Get started with:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{analogsea}\SpecialCharTok{::}\FunctionTok{account}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Now you need to authenticate the connection and this is done using a SSH public key. You can do this using:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{analogsea}\SpecialCharTok{::}\FunctionTok{key\_create}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

But if this is your first time doing this then it may be more useful to have a visual process, in which case follow the instructions here: \url{https://docs.digitalocean.com/products/droplets/how-to/add-ssh-keys/to-account/}. What you want is to have a `.pub' file on your computer. Then copy the public key aspect in that file, and add it to the SSH keys section in the account security settings. When you have the key on your local computer then you can check this using:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ssh}\SpecialCharTok{::}\FunctionTok{ssh\_key\_info}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Again, this will all take a while to validate. DigitalOcean calls every computer that you start a `droplet'. So if you start three computers, then you'll have started three droplets. You can check the droplets that you have running using:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{analogsea}\SpecialCharTok{::}\FunctionTok{droplets}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

If everything is set-up properly, then this will print the information about all droplets that you have associated with your account (which at this point, is probably none).

To create a droplet, you run:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{id }\OtherTok{\textless{}{-}}\NormalTok{ plumberDeploy}\SpecialCharTok{::}\FunctionTok{do\_provision}\NormalTok{(}\AttributeTok{example =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then you'll get asked for your SSH passphrase and then it'll just set-up a bunch of things. After this we're going to need to install a whole bunch of things onto our droplet:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{analogsea}\SpecialCharTok{::}\FunctionTok{install\_r\_package}\NormalTok{(}\AttributeTok{droplet =}\NormalTok{ id, }\FunctionTok{c}\NormalTok{(}\StringTok{"plumber"}\NormalTok{, }
                                             \StringTok{"remotes"}\NormalTok{, }
                                             \StringTok{"here"}\NormalTok{))}
\NormalTok{analogsea}\SpecialCharTok{::}\FunctionTok{debian\_apt\_get\_install}\NormalTok{(id, }\StringTok{"libssl{-}dev"}\NormalTok{, }
                                  \StringTok{"libsodium{-}dev"}\NormalTok{, }
                                  \StringTok{"libcurl4{-}openssl{-}dev"}\NormalTok{)}
\NormalTok{analogsea}\SpecialCharTok{::}\FunctionTok{debian\_apt\_get\_install}\NormalTok{(id, }
                                  \StringTok{"libxml2{-}dev"}\NormalTok{)}

\NormalTok{analogsea}\SpecialCharTok{::}\FunctionTok{install\_r\_package}\NormalTok{(id, }\FunctionTok{c}\NormalTok{(}\StringTok{"config"}\NormalTok{,}
                                   \StringTok{"httr"}\NormalTok{,}
                                   \StringTok{"urltools"}\NormalTok{,}
                                   \StringTok{"plumber"}\NormalTok{))}

\NormalTok{analogsea}\SpecialCharTok{::}\FunctionTok{install\_r\_package}\NormalTok{(id, }\FunctionTok{c}\NormalTok{(}\StringTok{"xml2"}\NormalTok{))}
\NormalTok{analogsea}\SpecialCharTok{::}\FunctionTok{install\_r\_package}\NormalTok{(id, }\FunctionTok{c}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{))}

\NormalTok{analogsea}\SpecialCharTok{::}\FunctionTok{install\_r\_package}\NormalTok{(id, }\FunctionTok{c}\NormalTok{(}\StringTok{"tidymodels"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

And then when that is finally set-up (it'll seriously take 30 min or so) we can deploy our API!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plumberDeploy}\SpecialCharTok{::}\FunctionTok{do\_deploy\_api}\NormalTok{(}\AttributeTok{droplet =}\NormalTok{ id, }
                             \AttributeTok{path =} \StringTok{"example"}\NormalTok{, }
                             \AttributeTok{localPath =} \FunctionTok{getwd}\NormalTok{(), }
                             \AttributeTok{port =} \DecValTok{8000}\NormalTok{, }
                             \AttributeTok{docs =} \ConstantTok{TRUE}\NormalTok{, }
                             \AttributeTok{overwrite=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{exercises-and-tutorial-19}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-19}}

\hypertarget{exercises-19}{%
\subsection{Exercises}\label{exercises-19}}

\hypertarget{tutorial-19}{%
\subsection{Tutorial}\label{tutorial-19}}

\hypertarget{efficiency}{%
\chapter{Efficiency}\label{efficiency}}

\textbf{STATUS: Under construction.}

\textbf{Required reading}

\begin{itemize}
\tightlist
\item
\end{itemize}

\textbf{Required viewing}

\begin{itemize}
\tightlist
\item
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\tightlist
\item
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
\end{itemize}

\textbf{Key functions}

\begin{itemize}
\tightlist
\item
\end{itemize}

\textbf{Quiz}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
\end{enumerate}

\hypertarget{introduction-28}{%
\section{Introduction}\label{introduction-28}}

\hypertarget{data-efficiency}{%
\section{Data efficiency}\label{data-efficiency}}

\hypertarget{sql}{%
\subsection{SQL}\label{sql}}

\hypertarget{feather}{%
\subsection{Feather}\label{feather}}

\hypertarget{code-efficiency}{%
\section{Code efficiency}\label{code-efficiency}}

By and large, worrying about performance is a waste of time. For the most part you are far better off, just pushing things into the cloud, letting them run for a reasonable time, and using that time to worry about other aspects of your pipeline. However, eventually this becomes unfeasible. For me, this is when something takes more than a day to run because it just becomes a pain. There is rarely a most common area for obvious performance gains. Instead you need to learn to measure and then cut.

Being fast is valuable but it's mostly about you being able to iterate fast not your code running fast. If you find that the speed at which your code completes is a bottle neck then shard. Then throw more machines at it. Then shard again. Then throw more machines at it.

\hypertarget{code-refactoring}{%
\section{Code refactoring}\label{code-refactoring}}

Some baby examples, focused on data science, along the lines of this: \url{https://indrajeetpatil.github.io/Refactoring-ggstatsplot/refactoring-ggstatsplot\#1}

Start with an example of bad code, and then how it gets fixed.

\hypertarget{measure}{%
\subsection{Measure}\label{measure}}

Using \texttt{tic()} and \texttt{tic()}.

Measuring

\hypertarget{experimental-efficiency}{%
\section{Experimental efficiency}\label{experimental-efficiency}}

Multi-armed bandit

\hypertarget{other-languages}{%
\section{Other languages}\label{other-languages}}

\hypertarget{python}{%
\subsection{Python}\label{python}}

\hypertarget{julia}{%
\subsection{Julia}\label{julia}}

\hypertarget{exercises-and-tutorial-20}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-20}}

\hypertarget{exercises-20}{%
\subsection{Exercises}\label{exercises-20}}

\hypertarget{tutorial-20}{%
\subsection{Tutorial}\label{tutorial-20}}

\hypertarget{concludingremarks}{%
\chapter{Concluding remarks, open issues, next steps}\label{concludingremarks}}

\textbf{STATUS: Under construction.}

\hypertarget{concluding-remarks}{%
\section{Concluding remarks}\label{concluding-remarks}}

There's an old saying, something along the lines of `may you live in interesting times'. I'm not sure if every generation feels this, but we sure live in interesting times. In this book, I have tried to convey some essentials that I think would allow you to contribute. But we are just getting started.

I'm 35 and so I am in the `data science didn't exist when I was an undergraduate' generation. In a little over a decade data science has gone from something that barely existed to a defining part of academia and industry. What does that imply for for you? It may imply that one should not just be making decisions that optimize for what data science looks like right now, but also what could happen. While that's a little difficult, that's also one of the things that makes data science so exciting. That might mean choices like:

\begin{itemize}
\tightlist
\item
  taking courses on fundamentals, not just fashionable applications;
\item
  reading books, not just whatever is trending; and
\item
  trying to be at the intersection of at least a few different areas, rather than hyper-specialized.
\end{itemize}

I'm just someone who likes to play with data using R. A decade ago I wouldn't have fit into any particular department. I'm lucky that these days there is space in data science for someone like me. And the nice thing about what we now call data science is that there's space for you as well.

Data science needs diversity. Data science needs your intelligence and enthusiasm. It needs you to be in the room, and able to make contributions. We live in interesting times and it's just such an exciting time to be enthusiastic about data. I can't wait to see what you build.

\hypertarget{some-issues}{%
\section{Some issues}\label{some-issues}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  How do we write unit tests for data science?
\end{enumerate}

\textbf{UPDATE to add in functional tests and stuff}

One thing that working with real computer scientists has taught me is the importance of unit tests. Basically this just means writing down the small checks that we do in our heads all the time. Like if we have a column that purports to the year, then it's unlikely that it's a character, and it's unlikely that it's an integer larger than 2500, and it's unlikely that it's a negative integer. We know all this, but writing unit tests has us write this all down.

In this case it's obvious what the unit test looks like. But more generally, we often have little idea what our results should look like if they're running well. The approach that I've taken is to add simulation---so we simulate reasonable results, write unit tests based on that, and then bring the real data to bear and adjust as necessary. But I really think that we need extensive work in this area because the current state-of-the-art is lacking.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  What happened to the machine learning revolution?
\end{enumerate}

I don't understand what happened to the promised machine learning revolution in social sciences. Specifically, I'm yet to see any convincing application of machine learning methods that are designed for prediction to a social sciences problem where what we care about is understanding. I would like to either see evidence of them or a definitive thesis about why this can't happen. The current situation is untenable where folks, especially those in fields that have been historically female, are made to feel inferior even though their results are no worse.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  How do we think about power?
\end{enumerate}

As someone who learnt statistics from economists, but now is partly in a statistics department, I do think that everyone should learn statistics from statisticians. This isn't anything against economists, but the conversations that I have in the statistics department about what statistical methods are and how they should be used are very different to those that I've had in other departments.

I think the problem is that people outside statistics, treat statistics as a recipe in which they follow various steps and then out comes a cake. With regard to `power'---it turns out that there were a bunch of instructions that no one bothered to check---they turned the oven on to some temperature without checking that it was 180C, and that's fine because whatever mess came out was accepted because the people evaluating the cake didn't know that they needed to check the temperature had been appropriately set. (I'm ditching this analogy right now).

As you know, the issue with power is related to the broader discussion about p-values, which basically no one is taught properly, because it would require changing an awful lot about how we teach statistics i.e.~moving away from the recipe approach.

And so, my specific issue is that people think that statistics is a recipe to be followed. They think that because that's how they are trained especially in social sciences like political science and economics, and that's what is rewarded. But that's not what these methods are. Instead, statistics is a collection of different instruments that let us look at our data in a certain way. I think that we need a revolution here, not a metaphorical tucking in of one's shirt.

\hypertarget{next-steps-1}{%
\section{Next steps}\label{next-steps-1}}

This book has covered much ground, and while we are toward the end of it, as the butler Stevens is told in the novel \emph{The Remains of the Day} \citep{ishiguro}:

\begin{quote}
The evening's the best part of the day. You've done your day's work. Now you can put your feet up and enjoy it.
\end{quote}

Chances are there are aspects that you want to explore further, building on the foundation that you have established. If so, then I've accomplished what I set out to do.

If you were new to data science at the start of this book, then the next step would be to backfill that which I skipped over, and I would recommend \citet{tiffanytimbers}. After that, you should learn more about R in terms of data science by going through \citet{r4ds}. To deepen your understanding of R itself, go next to \citet{advancedr}.

If you're interested in learning more about causality then start with \citet{Cunningham2021} and \citet{theeffect}.

If you're interested to learn more about statistics then begin with \citet{citemcelreath}, and then backfill with \citet{bayesrules} and solidify the foundation with \citet{bda}. You should probably also backfill some of the fundamentals around probability, starting with \citet{wasserman}.

There is only one next natural step if you're interested in learning more about statistical (what's come to be called machine) learning and that's \citet{islr} followed by \citet{esl}.

If you're interested in sampling then the next book to turn to is \citet{lohr}. To deepen your understanding of surveys and experiments, go next to \citet{fieldexperiments} in combination with \citet{kohavi}.

For graphs turn to \citet{healyviz}.

Writing go to\ldots{}

Thinking through production and SQL and things like, a next natural step is\ldots{}

We often hear the phrase let the data speak. Hopefully by this point you understand that never happens. All that we can do is to acknowledge that we are the ones using data to tell stories, and strive and seek to make them worthy.

\cleardoublepage

\hypertarget{appendix-appendix}{%
\appendix \addcontentsline{toc}{chapter}{\appendixname}}


\hypertarget{oh-you-think-and-shoulders-and-datasets}{%
\chapter{Oh you think and shoulders and datasets}\label{oh-you-think-and-shoulders-and-datasets}}

\emph{This is just a holding place for this content while it is being developed.}

\hypertarget{oh-you-think-we-have-good-data-on-that}{%
\section{Oh, you think we have good data on that!}\label{oh-you-think-we-have-good-data-on-that}}

\begin{quote}
\textbf{Oh, you think we have good data on that!} Migration.
\end{quote}

\begin{quote}
\textbf{Oh, you think we have good data on that!} Weather stations
\end{quote}

\begin{quote}
\textbf{Oh, you think we have good data on that!} Olympics events. Who decides on the scoring?. Who does the timing?
\end{quote}

\begin{quote}
\textbf{Oh, you think we have good data on that!} Personality scores. Myers Briggs and Big 5 more generally.
\end{quote}

\begin{quote}
\textbf{Oh, you think we have good data on that!} Cause of death
\end{quote}

\begin{quote}
\textbf{Oh, you think we have good data on that!} City boundaries. What constitues `Atlanta'? Different definitions - metro, X, Y. (also an issue in countries with boundaries changing over time)
\end{quote}

\begin{quote}
\textbf{Oh, you think we have good data on that!} Timing
\end{quote}

\hypertarget{shoulders-of-giants}{%
\section{Shoulders of giants}\label{shoulders-of-giants}}

Chapter 2:

\begin{quote}
\textbf{Shoulders of giants} Robert Gentleman and Ross Ihaka
\end{quote}

Chapter 5:

\begin{quote}
\textbf{Shoulders of giants} Xiao-Li Meng
\end{quote}

\begin{itemize}
\tightlist
\item
  Andrew Gelman
\item
  Barbara Bailar
\item
  Daniela Witten
\item
  Elizabeth Scott
\item
  Evelyn Kitagawa
\item
  Gertrude Mary Cox
\item
  Hadley Wickham
\item
  John Tukey
\item
  Katherine Wallman
\item
  Nancy Reid
\item
  Simon Kuznets
\item
  Stella Cunliffe
\item
  Rob Tibshirani
\item
  Timnit Gebru
\end{itemize}

\hypertarget{possible-datasets}{%
\section{Possible datasets}\label{possible-datasets}}

\begin{itemize}
\tightlist
\item
  \url{https://som.yale.edu/faculty-research/our-centers/international-center-finance/data}
\item
  Alex cookson
\item
  David Andrew's book
\item
  Tidycensus
\item
  \url{https://stat.ethz.ch/R-manual/R-patched/library/datasets/html/00Index.html}
\item
  \url{https://www.historicalstatistics.org/}
\item
  \url{https://data.cityofberkeley.info/browse?limitTo=datasets\&utf8}
\item
  \url{https://data.gov.hk/en-datasets/category/education}
\item
  \url{https://data.rijksmuseum.nl/object-metadata/download/}
\item
  World bank \url{https://data.worldbank.org/} eg development indicators
\item
  South sea bubble
\item
  OECD
\item
  Aer R package and that paper?
\item
  CESr
\item
  Paspaley
\item
  Canlang
\item
  Fred - does that have an api?
\item
  538
\item
  The Economist
\item
  \url{https://pds.nasa.gov/datasearch/subscription-service/SS-Release.shtml}
\item
  \url{https://github.com/BuzzFeedNews/nics-firearm-background-checks}
\item
  The markup
\item
  Tom Cardoso
\item
  List of APIs: \url{https://bookdown.org/paul/apis_for_social_scientists/}
\end{itemize}

\hypertarget{papers}{%
\chapter{Papers}\label{papers}}

\hypertarget{paper-1-1}{%
\section{Paper 1}\label{paper-1-1}}

\hypertarget{task}{%
\subsection{Task}\label{task}}

Working individually and in an entirely reproducible way, please find a dataset of interest on Open Data Toronto -- \url{https://open.toronto.ca} -- and write a short paper telling a story about the data.

\hypertarget{guidance}{%
\subsection{Guidance}\label{guidance}}

\begin{itemize}
\tightlist
\item
  Find a dataset of interest on \href{https://open.toronto.ca}{Open Data Toronto} and download it in a reproducible way using \texttt{opendatatoronto} \citep{citeSharla}.
\item
  Create a folder with appropriate sub-folders, add it to GitHub, and then prepare a PDF using \texttt{R\ Markdown} with these sections (you are welcome to use this starter folder: \url{https://github.com/RohanAlexander/starter_folder}):

  \begin{itemize}
  \tightlist
  \item
    title,
  \item
    author,
  \item
    date,
  \item
    abstract,
  \item
    introduction,
  \item
    data, and
  \item
    references.
  \end{itemize}
\item
  In the data section thoroughly and precisely discuss the source of the data and the bias this brings (ethical, statistical, and otherwise). Comprehensively describe and summarize the data using text and at least one graph and one table. Graphs must be made in \texttt{ggplot2} \citep{citeggplot} and tables must be made using \texttt{knitr} \citep{citeknitr} (with or without \texttt{kableExtra} \citep{citekableextra}). Graphs must show the actual data, or as close to it as possible, not summary statistics. Make sure to cross-reference graphs and tables.
\item
  Add references by using a bib file. Be sure to reference R and any R packages you use, as well as the dataset. Check that you have referenced everything. Strong submissions will draw on related literature and would also reference those. There are various options in R Markdown for references style; just pick one that you are used to.
\item
  Go back and write an introduction. This should be two or three paragraphs. The last paragraph should set out the remainder of the paper.
\item
  Add an abstract. This should be three or four sentences. If your abstract is longer than four sentences, then you need to think a lot about whether it is too long. It may be fine (there are always exceptions) but you should probably have a good reason. Your abstract must tell the reader your top-level finding. What is the one thing that we learn about the world because of your paper?
\item
  Then add a descriptive title. `Paper 1' is not descriptive and there should not be any sign this is a school paper.
\item
  Add a link to your GitHub repo using a footnote.
\item
  Check that your GitHub repo is well-organized, and add an informative README. Comment your code. Make sure that you have got at least one R script in there, in addition, to your R Markdown file.
\item
  Pull this all together as a PDF and check that the paper is well-written and able to be understood by the average reader of, say, FiveThirtyEight. This means that you are allowed to use mathematical notation, but you must explain all of it in plain language. All statistical concepts and terminology must be explained. Your reader is someone with a university education, but not necessarily someone who understands what a p-value is.
\item
  Check there is no evidence that this is a class assignment.
\item
  Via Quercus, submit the PDF.
\end{itemize}

\hypertarget{checks}{%
\subsection{Checks}\label{checks}}

\begin{itemize}
\tightlist
\item
  Check you have not included any R code or raw R output in the final PDF.
\item
  Check that although you will probably have most of your code in the R Markdown, make sure that you have at least one R script in the `scripts' folder.
\item
  Check there is thoroughly commented code that directly creates your PDF. Do not `knit to html' and then save as a PDF. Do not `knit to Word' and then save as a PDF
\item
  Check that your graphs, tables, and text are extremely clear, and of comparable quality to those of FiveThirtyEight.
\item
  Check that the date is updated.
\item
  Check your entire workflow is entirely reproducible.
\item
  Check for typos.
\end{itemize}

\hypertarget{faq}{%
\subsection{FAQ}\label{faq}}

\begin{itemize}
\tightlist
\item
  Can I use a dataset from Kaggle instead? No, because they have done the hard work for you.
\item
  I cannot use code to download my dataset, can I just manually download it? No, because your entire workflow needs to be reproducible. Please fix the download problem or pick a different dataset.
\item
  How much should I write? Most students submit something in the two-to-six-page range, but it's really up to you. Be precise and thorough.
\item
  My data is about apartment blocks/NBA/League of Legends so there's no ethical or bias aspect, what do I do? Please re-read the readings to better understand bias and ethics. If you really cannot think of something, then it might be worth picking a different dataset.
\item
  Can I use Python? No.~If you already know Python then it doesn't hurt to learn another language.
\item
  Why do I need to cite R, when I don't need to cite Word? R is a free statistical programming language with academic origins so it's appropriate to acknowledge the work of others. It's also important for reproducibility.
\end{itemize}

\hypertarget{rubric}{%
\subsection{Rubric}\label{rubric}}

\begin{itemize}
\tightlist
\item
  Go/no-go \#1: R is cited - {[}1 `Yes', 0 `No'{]}

  \begin{itemize}
  \tightlist
  \item
    Both referred to in the main content and included in the reference list.
  \item
    If not, no need to continue marking, just give paper 0 overall.
  \end{itemize}
\item
  Title - {[}2 `Exceptional', 1 `Yes', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    An informative title is included.
  \item
    Tell the reader what your story is, don't waste their time.
  \item
    Ideally tell them what happens at the end of the story.
  \item
    `Problem Set X' is not an informative title. There should be no evidence this is a school paper.
  \end{itemize}
\item
  Author, date, and repo - {[}2 `Yes', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    The author, date of submission, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: `Code and data supporting this analysis is available at: LINK').
  \end{itemize}
\item
  Abstract - {[}4 `Exceptional', 3 `Great', 2 `Fine', 1 `Gets job done', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    An abstract is included and appropriately pitched to a general audience.
  \item
    The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level).
  \end{itemize}
\item
  Introduction - {[}4 `Exceptional', 3 `Great', 2 `Fine', 1 `Gets job done', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    The introduction is self-contained and tells a reader everything they need to know, including putting it into a broader context.
  \item
    Your introduction should provide a bit of broader context to motivate the reader, as well as providing a bit more detail about what you're interested in, what you did, what you found, why it's important, etc.
  \item
    A reader should be able to read only your introduction and have a good idea about the research that you carried out and what you found.
  \item
    It would be rare that you would have tables or figures in your introduction (again there are always exceptions but think deeply about whether yours is one).
  \item
    It must outline the structure of the paper.
  \item
    For instance (and this is just a rough guide) an introduction for a 10 page paper, should probably be about 3 or 4 paragraphs, or 10 per cent, but it depends on specifics.
  \end{itemize}
\item
  Data - {[}10 `Exceptional', 8 `Great', 6 `Good', 4 `Some issues', 2 `Many issues', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    When you discuss the dataset (in the data section) you should make sure to discuss at least:
  \end{itemize}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi})}
  \tightlist
  \item
    The source of the data.
  \item
    The methodology and approach that is used to collect and process the data.
  \item
    The population, the frame, and the sample (as appropriate).
  \item
    Information about how respondents were found. What happened to non-response?
  \item
    What are the key features, strengths, and weaknesses about the source generally.
  \end{enumerate}

  \begin{itemize}
  \tightlist
  \item
    You should thoroughly discuss the variables in the dataset that you use. Are there any that are very similar that you nonetheless don't use? Did you construct any variables by combining various ones?
  \item
    What do the data look like?
  \item
    Plot the actual data that you're using (or as close as you can get to it).
  \item
    Discuss these plots and the other features of these data.

    \begin{itemize}
    \tightlist
    \item
      These are just some of the issues strong submissions will consider. Show off your knowledge. If this becomes too detailed, then you should push some of this to footnotes or an appendix.
    \item
      `Exceptional' means that when I read your submission I learn something about the dataset that I don't learn from any other submission (within a reasonable measure of course).
    \end{itemize}
  \end{itemize}
\item
  Numbering - {[}2 `Yes', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    All figures, tables, equations, etc are numbered and referred to in the text.
  \end{itemize}
\item
  Proofreading - {[}2 `Yes', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    All aspects of submission are free of noticeable typos.
  \end{itemize}
\item
  Graphs/tables/etc - {[}4 `Exceptional', 3 `Great', 2 `Fine', 1 `Gets job done', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    You must include graphs and tables in your paper and they must be to a high standard.
  \item
    They must be well formatted and camera-ready. They should be clear and digestible.
  \item
    They must: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of labels/explanations, etc; and 3) appropriately sized and colored (or appropriate significant figures in the case of stats).
  \end{itemize}
\item
  References - {[}4 `Perfect', 3 `One minor issue', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    All data/software/literature/etc are appropriately noted and cited.
  \item
    You must cite the software and software packages that you use.
  \item
    You must cite the datasets that you use.
  \item
    You must cite literature that you refer to (and you should refer to literature).
  \item
    If you take a small chunk of code from Stack Overflow then add the page in a comment next to the code.
  \item
    If you take a large chunk of code then cite it fully.
  \item
    3 means one minor issue. More than one minor issue receives 0.
  \end{itemize}
\item
  Reproducibility - {[}4 `Exceptional', 3 `Great', 2 `Fine', 1 `Gets job done', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    The paper and analysis must be fully reproducible.
  \item
    A detailed README is included.
  \item
    All code should be thoroughly documented.
  \item
    An R project is used. Do not use \texttt{setwd()}.
  \item
    The code must appropriately read data, prepare it, create plots, conduct analysis, and generate documents. Seeds are used where needed.
  \item
    Code must have a preamble etc.
  \item
    You must appropriately document your scripts such that someone coming in could follow them.
  \item
    Your repo must be thoroughly organized.
  \end{itemize}
\item
  General excellence - {[}3 `Exceptional', 2 `Wow', 1 `Huh, that's interesting', 0 `None'{]}

  \begin{itemize}
  \tightlist
  \item
    There are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.
  \end{itemize}
\end{itemize}

\hypertarget{previous-examples}{%
\subsection{Previous examples}\label{previous-examples}}

Some examples of papers that well in the past include those by: \href{inputs/pdfs/Mandatory_minimums-Amy_Farrow.pdf}{Amy Farrow}, \href{inputs/pdfs/Mandatory_minimums-Morgaine_Westin.pdf}{Morgaine Westin}, and \href{inputs/pdfs/Mandatory_minimums-Rachel_Lam.pdf}{Rachel Lam}.

\newpage

\hypertarget{paper-2}{%
\section{Paper 2}\label{paper-2}}

\hypertarget{task-1}{%
\subsection{Task}\label{task-1}}

\textbf{{[}ADD STUFF aBOUT REPLICATION LAB{]}}

\begin{itemize}
\tightlist
\item
  Working as part of a small team of 1-3 people, and in an entirely reproducible way, please pick a paper to reproduce from an approved list and then write a short paper telling a story based on this. Your story should both talk about the (reproduced) findings, but also (a bit more `meta') about what you learnt from the process.
\end{itemize}

\hypertarget{guidance-1}{%
\subsection{Guidance}\label{guidance-1}}

\begin{itemize}
\tightlist
\item
  Working as part of a team of 1-3 people, prepare a PDF in R Markdown with the following features:

  \begin{itemize}
  \tightlist
  \item
    title,
  \item
    author/s,
  \item
    date,
  \item
    abstract,
  \item
    introduction,
  \item
    data,
  \item
    model,
  \item
    results,
  \item
    discussion, and
  \item
    references.
  \end{itemize}
\item
  In the discussion section and any other relevant section, please be sure to discuss ethics and bias with reference to relevant literature.
\item
  You should reproduce one of the following papers:

  \begin{itemize}
  \tightlist
  \item
    Barari, Soubhik, Christopher Lucas, and Kevin Munger, 2021, `Political Deepfake Videos Misinform the Public, But No More than Other Fake Media', 13 January, \url{https://osf.io/cdfh3/}.
  \item
    Cohn, Alain, Michel André Maréchal, David Tannenbaum, and Christian Lukas Zünd, 2019, `Civic Honesty Around the Globe'.
  \item
    Liran Einav, Amy Finkelstein, Tamar Oostrom, Abigail Ostriker, Heidi Williams, 2020, `Screening and Selection: The Case of Mammograms', \emph{American Economic Review}.
  \item
    Pons, Vincent, 2018, `Will a Five-Minute Discussion Change Your Mind? A Countrywide Experiment on Voter Choice in France' \emph{American Economic Review}.
  \item
    Abramitzky, Ran, Leah Boustan, Katherine Eriksson, and Stephanie Hao. ``Discrimination and the Returns to Cultural Assimilation in the Age of Mass Migration.'' AEA Papers and Proceedings 110 (May 2020): 340--46. \url{https://doi.org/10.1257/pandp.20201090}.
  \item
    Chen, Yan, Ming Jiang, and Erin L. Krupka. ``Hunger and the Gender Gap.'' Experimental Economics 22, no. 4 (December 2019): 885--917. \url{https://doi.org/10.1007/s10683-018-9589-9}.
  \item
    Lise, Jeremy, and Fabien Postel-Vinay. ``Multidimensional Skills, Sorting, and Human Capital Accumulation.'' American Economic Review 110, no. 8 (August 2020): 2328--76. \url{https://doi.org/10.1257/aer.20162002}.
  \item
    Kolev, Julian, Yuly Fuentes-Medel, and Fiona Murray. ``Gender Differences in Scientific Communication and Their Impact on Grant Funding Decisions.'' AEA Papers and Proceedings 110 (May 2020): 245--49. \url{https://doi.org/10.1257/pandp.20201043}.
  \item
    Cowgill, Bo, Fabrizio Dell'Acqua, and Sandra Matz. ``The Managerial Effects of Algorithmic Fairness Activism.'' AEA Papers and Proceedings 110 (May 2020): 85--90. \url{https://doi.org/10.1257/pandp.20201035}.
  \end{itemize}
\item
  You should follow the lead of the author/s of the paper you're reproducing, but thoroughly think about, and discuss, what is being done. Regardless of the particular model that you are using, and the (possibly lack of) extent to which this is done in the paper, your model must be well explained, thoroughly justified, explained as appropriate to the task at hand, and the results must be beautifully described.
\item
  You must include a DAG (probably in the model section).
\item
  You must have a discussion of power and experimental design (probably in the data section)
\item
  Your paper must be well-written, draw on relevant literature, and show your statistical skills by explaining all statistical concepts that you draw on.
\item
  You are welcome to use appendices for supporting, but not critical, material. Your discussion must include sub-sections that focus on three or four interesting points, and also sub-sections on weaknesses and next steps.
\item
  In your report you must provide a link to a GitHub repo that fully contains your analysis. Your code must be entirely reproducible, documented, and readable. Your repo must be well-organised and appropriately use folders.
\item
  Your graphs and tables must be of an incredibly high standard. Graphs and tables should be well formatted and report-ready. They should be clean and digestible. Furthermore, you should label and describe each table/figure.
\item
  When you discuss the dataset (in the data section) you should make sure to discuss (at least):

  \begin{itemize}
  \tightlist
  \item
    Its key features, strengths, and weaknesses generally.
  \item
    A discussion of the questionnaire - what is good and bad about it?
  \item
    A discussion of the methodology including how they find people to take the survey; what their population, frame, and sample were; what sampling approach they took and what some of the trade-offs may be; what they do about non-response; the cost.
  \item
    A discussion of the intervention and experimental design.
  \item
    These are just some of the issues strong submissions will consider. Show off your knowledge. If this becomes too detailed then you should push some of this to footnotes or an appendix.
  \end{itemize}
\item
  When you discuss your model (in the model section), you must be extremely careful to spell out the statistical model that you are using, defining and explaining each aspect and why it is important. (For a Bayesian model, a discussion of priors and regularization is almost always important.) You should mention the software that you used to run the model. You should be clear about model convergence, model checks, and diagnostic issues. How do the sampling and survey aspects that you discussed assert themselves in the modelling decisions that you make? Again, if it becomes too detailed then push some of the details to footnotes or an appendix. You have the original paper to guide you, but you will likely need to go well-beyond what is included.
\item
  You should present model results, graphs, figures, etc, in the results section. This section should strictly relay results. Interpretation of these results and conclusions drawn from the results should be left for the discussion section.
\item
  Your discussion should focus on your model results. Interpret them and explain what they mean. Put them in context. What do we learn about the world having understood your model and its results? What caveats could apply? To what extent does your model represent the small world and the large world (to use the language of McElreath, Ch 2)? What are some weaknesses and opportunities for future work? Additionally, as this is a reproduction you should include a sub-section on differences you found and difficulties that you had.
\item
  Check that you have referenced everything. Strong submissions will draw on related literature in the discussion (and other sections) and would be sure to also reference those. The style of references does not matter, provided it is consistent.
\item
  As a team, via Quercus, submit a PDF of your paper. Again, in your paper you must have a link to the associated GitHub repo. And you must include the R Markdown file that produced the PDF in that repo. And you must include the R Markdown file that produced the PDF in that repo. The repo must be well-organized and have a detailed README.
\item
  A good way to work as a team would be to split up the work, so that one person is doing each section. The people doing the sections that rely on data (such as the analysis and the graphs) could just simulate it while they are waiting for the person putting together the data to finish.
\item
  It is expected that your submission be well written and able to be understood by the average reader of say 538. This means that you are allowed to use mathematical notation, but you must be able to explain it all in plain English. Similarly, you can (and hint: you should) use survey, sampling, observational, and statistical terminology, but again you need to explain it. Your work should have flow and should be easy to follow and understand. To communicate well, anyone at the university level should be able to read your report once and relay back the methodology, overall results, findings, weaknesses and next steps without confusion.
\item
  Everyone in the team receives the same mark.
\item
  There should be no evidence that this is a class assignment.
\end{itemize}

\hypertarget{checks-1}{%
\subsection{Checks}\label{checks-1}}

\begin{itemize}
\tightlist
\item
  We have not just copy-pasted the code from the original paper, but instead have used that as a foundation to work from?
\end{itemize}

\hypertarget{faq-1}{%
\subsection{FAQ}\label{faq-1}}

\begin{itemize}
\tightlist
\item
  Do I have to stay in the same group as the second paper? No.~You're welcome to change. However, it's important that you don't change the second paper group on Quercus - be sure to only change the third paper group.
\item
  Can we switch groups for the third paper? Yes.
\item
  How much should I write? Most students submit something in the 10-to-15-page range, but it's really up to you. Be precise and thorough.
\item
  My paper doesn't have a DAG, what do I do? You need to make the DAG.
\end{itemize}

\hypertarget{rubric-1}{%
\subsection{Rubric}\label{rubric-1}}

\begin{itemize}
\tightlist
\item
  Go/no-go \#1: R is cited - {[}1 `Yes', 0 `No'{]}

  \begin{itemize}
  \tightlist
  \item
    Both referred to in the main content and included in the reference list.
  \item
    If not, no need to continue marking, just give paper 0 overall.
  \end{itemize}
\item
  Title - {[}2 `Exceptional', 1 `Yes', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    An informative title is included.
  \item
    Tell the reader what your story is - don't waste their time.
  \item
    Ideally tell them what happens at the end of the story.
  \item
    `Problem Set X' is not an informative title. There should be no evidence this is a school paper.
  \end{itemize}
\item
  Author, date, and repo - {[}2 `Yes', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    The author, date of submission, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: `Code and data supporting this analysis is available at: LINK').
  \end{itemize}
\item
  Abstract - {[}4 `Exceptional', 3 `Great', 2 `Fine', 1 `Gets job done', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    An abstract is included and appropriately pitched to a general audience.
  \item
    The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level).
  \item
    If your abstract is longer than four sentences then you need to think a lot about whether it is too long. It may be fine (there are always exceptions) but you should probably have a good reason.
  \item
    Your abstract must tell the reader your top-level finding. What is the one thing that we learn about the world because of your paper?
  \end{itemize}
\item
  Introduction - {[}4 `Exceptional', 3 `Great', 2 `Fine', 1 `Gets job done', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    The introduction is self-contained and tells a reader everything they need to know, including putting it into a broader context.
  \item
    Your introduction should provide a bit of broader context to motivate the reader, as well as providing a bit more detail about what you're interested in, what you did, what you found, why it's important, etc.
  \item
    A reader should be able to read only your introduction and have a good idea about the research that you carried out.
  \item
    It would be rare that you would have tables or figures in your introduction (again there are always exceptions but think deeply about whether yours is one).
  \item
    It must outline the structure of the paper.
  \item
    For instance (and this is just a rough guide) an introduction for a 10 page paper, should probably be about 3 or 4 paragraphs, or 10 per cent, but it depends on specifics.
  \end{itemize}
\item
  Data - {[}10 `Exceptional', 8 `Great', 6 `Good', 4 `Some issues', 2 `Many issues', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    You should thoroughly discuss the variables in the dataset that you use. Are there any that are very similar that you nonetheless don't use? Did you construct any variables by combining various ones?
  \item
    What do the data look like?
  \item
    Plot the actual data that you're using (or as close as you can get to it).
  \item
    Discuss these plots and the other features of these data.
  \item
    These are just some of the issues strong submissions will consider. Show off your knowledge. If this becomes too detailed, then you should push some of this to footnotes or an appendix.
  \item
    `Exceptional' means that when I read your submission I learn something about the dataset that I don't learn from any other submission (within a reasonable measure of course).
  \end{itemize}
\item
  Model - {[}10 `Exceptional', 8 `Great', 6 `Good', 4 `Some issues', 2 `Many issues', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    The model is nicely written out, well-explained, justified, and appropriate.
  \item
    When you discuss your model you must be extremely careful to spell out the statistical model that you are using defining and explaining each aspect and why it is important. Failure to do this suggests you don't understand the model.
  \item
    The model is appropriately complex - that is, not too simple, but not unnecessarily complicated.
  \item
    The model has well-defined variables and these correspond to what is discussed in the data section.
  \item
    The model needs to be written out in appropriate mathematical notation but also in plain English.
  \item
    Every aspect of that notation must be defined otherwise the most this section can receive is poor.
  \item
    The model makes sense based on the substantive area, and also the form of the model.
  \item
    If the model is Bayesian, then priors need to be defined and sensible.
  \item
    Discussion needs to occur around how features enter the model and why. For instance, (and these are just examples) why use ages rather than age-groups, why does province have a levels effect, why is gender categorical, etc?
  \item
    In general, in order to be adequate, there needs to be a clear justification that this is the model for the situation.
  \item
    The assumptions underpinning the model are clearly discussed.
  \item
    Alternative models, or variants, must be discussed and strengths and weaknesses made clear. Why was this model chosen?
  \item
    You should mention the software that you used to run the model.
  \item
    There is some evidence of thought about the circumstances in which the model may not be appropriate.
  \item
    There is evidence of model validation and checking, whether that is out of sample or comparison to a straw man or RMSE, test/training, or appropriate sensitivity checks.
  \item
    You should be clear about model convergence, model checks, and diagnostic issues, but if this becomes too detailed then you could push some of this to an appendix.
  \item
    Great answers would discuss things such as, how do the aspects that you discussed in the data section assert themselves in the modelling decisions that you make. Again if it becomes too detailed then push some of the details to footnotes or an appendix.
  \item
    Again, explain what your model is and what is going on with it.
  \end{itemize}
\item
  Results - {[}10 `Exceptional', 8 `Great', 6 `Good', 4 `Some issues', 2 `Many issues', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    Results will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps.
  \item
    To be clear, you should also have text associated with all these aspects.
  \item
    Show the reader the results by plotting them. Talk about them. Explain them. That said, this section should strictly relay results.
  \end{itemize}
\item
  Discussion - {[}10 `Exceptional', 8 `Great', 6 `Good', 4 `Some issues', 2 `Many issues', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    Some questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page):

    \begin{itemize}
    \tightlist
    \item
      What is done in this this paper?
    \item
      What is something that we learn about the world?
    \item
      What is another thing that we learn about the world?
    \item
      What are some weaknesses of what was done?
    \item
      What is left to learn or how should we proceed in the future?
    \end{itemize}
  \end{itemize}
\item
  Numbering - {[}2 `Yes', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    All figures, tables, equations, etc are numbered and referred to in the text.
  \end{itemize}
\item
  Proofreading - {[}2 `Yes', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    All aspects of submission are free of noticeable typos.
  \end{itemize}
\item
  Graphs/tables/etc - {[}4 `Exceptional', 3 `Great', 2 `Fine', 1 `Gets job done', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    You must include graphs and tables in your paper and they must be to a high standard.
  \item
    They must be well formatted and camera-ready They should be clear and digestible.
  \item
    They must: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of labels/explanations, etc; and 3) appropriately sized and coloured (or appropriate significant figures in the case of stats).
  \end{itemize}
\item
  References - {[}4 `Perfect', 3 `One minor issue', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    All data/software/literature/etc are appropriately noted and cited.
  \item
    You must cite the software and software packages that you use.
  \item
    You must cite the datasets that you use.
  \item
    You must cite literature that you refer to (and you should refer to literature).
  \item
    If you take a small chunk of code from Stack Overflow then add the page in a comment next to the code.
  \item
    If you take a large chunk of code then cite it fully.
  \item
    3 means one minor issue. More than one minor issue receives 0.
  \end{itemize}
\item
  Reproducibility - {[}4 `Exceptional', 3 `Great', 2 `Fine', 1 `Gets job done', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    The paper and analysis must be fully reproducible.
  \item
    A detailed README is included.
  \item
    All code should be thoroughly documented.
  \item
    An R project is used. Do not use \texttt{setwd()}.
  \item
    The code must appropriately read data, prepare it, create plots, conduct analysis, generate documents, etc. Seeds are used where needed.
  \item
    Code must have a preamble etc.
  \item
    You must appropriately document your scripts such that someone coming in could follow them.
  \item
    Your repo must be thoroughly organized and not contain extraneous files.
  \end{itemize}
\item
  General excellence - {[}3 `Exceptional', 2 `Wow', 1 `Huh, that's interesting', 0 `None'{]}

  \begin{itemize}
  \tightlist
  \item
    There are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.
  \end{itemize}
\end{itemize}

\hypertarget{these-numbers-mean-dial-it-up}{%
\section{`These numbers mean dial it up'}\label{these-numbers-mean-dial-it-up}}

\hypertarget{task-2}{%
\subsection{Task}\label{task-2}}

Please consider this scenario:

\begin{quote}
You are employed as a junior data scientist at Petit Poll - a Canadian polling company. Petit Poll has a contract with a `client' - an Ontario government department - to provide them with advice. In particular, the client wants to understand the effect of COVID shut-downs on restaurant businesses and has asked Petit Poll to design an experiment about some aspect of this.
\end{quote}

Working as part of a small team of 1-3 people, and in an entirely reproducible way, please decide on an intervention, and some measurement strategies, and then write a short paper telling a story about the effect.

\hypertarget{guidance-2}{%
\subsection{Guidance}\label{guidance-2}}

\begin{itemize}
\tightlist
\item
  Working as part of a team of 1-3 people, prepare a PDF in R Markdown with the following features (you are welcome to use this starter folder: \url{https://github.com/RohanAlexander/starter_folder}):

  \begin{itemize}
  \tightlist
  \item
    title,
  \item
    author/s,
  \item
    date,
  \item
    abstract,
  \item
    introduction,
  \item
    data,
  \item
    discussion,
  \item
    appendix with survey details, and
  \item
    references.
  \end{itemize}
\item
  Decide on an intervention. Some aspects to address include:

  \begin{itemize}
  \tightlist
  \item
    How will it be designed and implemented?
  \item
    What will be random about it?
  \item
    How will you ensure the separation of treatment and non-treatment?
  \item
    How long will it run?
  \end{itemize}
\item
  you will need to run surveys to gather information about your intervention. Decide on a survey methodology. Some aspects to address include:

  \begin{itemize}
  \tightlist
  \item
    What is the population, frame, and sample?
  \item
    What sampling methods will you use and why? What are some of the statistical properties that the method brings to the table?
  \item
    How are you going to reach your desired respondents?
  \item
    How much do you estimate this will cost?
  \item
    What steps will you take to deal with non-response and how will non-response affect your survey?
  \item
    How are you going to protect respondent privacy?
  \item
    Remember to consider all of this in the context of your `client' - for instance, what are they interested in?
  \end{itemize}
\item
  Develop a survey on a platform that was introduced in class, or another that you're familiar with.

  \begin{itemize}
  \tightlist
  \item
    Be sure to test it yourselves. You will want to test this as much as possible, maybe even swap informally with another group?
  \end{itemize}
\item
  Now release the surveys into the (simulated) `field'.

  \begin{itemize}
  \tightlist
  \item
    Please do this by simulating an appropriate number of responses to your survey in R.
  \item
    Don't forget to simulate in relation to the intervention that you proposed.
  \item
    Do you need two, or even more, surveys (and hence multiple sets of simulated results)?
  \end{itemize}
\item
  Show the results and discuss your `findings'. Everything must be entirely reproducible.
\item
  You may wish to scrape some data and/or use open data sources to appropriately parameterize your simulations. Don't forget to cite them when you do this.
\item
  Use R Markdown to write a PDF report about all of this. Discuss your intervention, results and findings, your survey design and motivations, etc - all of it. You are writing a report that will eventually go to the client, so you must set the scene, and use language that demonstrates your command of statistical concepts but brings the reader along with you. Be sure to include graphs and tables and reference them in your discussion. Be sure to be clear about weaknesses and biases, and opportunities for future work.
\item
  Your report must be well written. You are allowed to, and should, use mathematical notation and statistical concepts, but you must explain all of it in plain language. Similarly, you can, and should, use experimental/survey/sampling/observational data terminology, but again, you need to explain it.
\item
  In the data section you should specify the intervention and data gathering methodology. You should also show your data, with tables and graphs as necessary. In addition to summaries, be sure to plot your raw data to the extent possible.
\item
  Your graphs and tables must be of an incredibly high standard. Graphs and tables should be well formatted and report-ready. They should be clean and digestible. Furthermore, you should label and describe each table/figure/equation.
\item
  In the discussion section and any other relevant section, please be sure to discuss ethics and bias with reference to relevant literature. For instance, think about who is and who is not in your dataset. What are the statistical and ethical implications of this?
\item
  Often folks struggle with the Discussion section of reports. For a 10 page report, we're looking for about 2-3 pages of content. Here's an example of sub-sections that you could include:

  \begin{itemize}
  \tightlist
  \item
    A sub-section containing a brief overview of the paper, and also how it fits into the literature and improves existing contributions.
  \item
    Three sub-sections that detail the three main points that we learn about the world from the paper.
  \item
    A sub-section on limitations, but discussed in a sophisticated way. So this means not just stating them, but explaining and justifying.
  \item
    A sub-section on how it could be extended and future directions.
  \end{itemize}
\item
  Your client has stats graduates working for it who need to be impressed by the main content of the report, but also has people who barely know what an average is and these people need to be impressed also.
\item
  Check that you have referenced everything, including R, R packages, and datasets. Strong submissions will draw on related literature and would be sure to also reference those. The style of references does not matter - there are various options in R Markdown - provided it is consistent and that \texttt{bibtex} has been used.
\item
  Via Quercus, submit your PDF report. You must provide a link to the GitHub repo where the code that you used for this assignment lives. Comment. Your. Code. Your entire workflow must be entirely reproducible. Your repo should be clearly organised, and a useful README included. You must include a separate R script that accomplishes something (probably the simulations makes sense). And you must include the R Markdown file that produced the PDF in that repo.
\item
  Please be sure to include a link to your survey/s in your report and screenshots of the survey/s in the appendix of your report.
\item
  Everyone in the team receives the same mark.
\item
  There should be no evidence that this is a class assignment.
\end{itemize}

\hypertarget{checks-2}{%
\subsection{Checks}\label{checks-2}}

\begin{itemize}
\tightlist
\item
  Check you have not included any R code or raw R output in the final PDF.
\item
  Check you have cited R and any R packages used.
\item
  Check that although you will probably have most of your code in the R Markdown, make sure that you have at least one R script in the \texttt{scripts} folder.
\item
  Check there is thoroughly commented code that directly creates your PDF. Do not knit to html and then save as a PDF. Do not knit to Word and then save as a PDF
\item
  Check that your graph and discussion are extremely clear, and of comparable quality to those of FiveThirtyEight.
\item
  Check that the date is updated to the date of submission.
\item
  Check your entire workflow is entirely reproducible.
\item
  Check for typos.
\item
  Check that you have got an appendix that details the survey/s and a link to the live survey.
\end{itemize}

\hypertarget{faq-2}{%
\subsection{FAQ}\label{faq-2}}

\begin{itemize}
\tightlist
\item
  Can I work by myself? Yes. But I recommend forming a group and the workload for the course assumes you will work on the second and third paper as part of a group of four.
\item
  Can we switch groups for the third paper? Yes.
\item
  How can I find a group? I will randomly create groups of four in Quercus. You are welcome to shift out of those groups and form your own groups if you'd like.
\item
  Can I get a different mark to the rest of my group? No.~Everyone in the group gets the same mark.
\item
  I wrote my paper by myself, so can I be graded on a different scale? No.~All papers are graded in the same way.
\item
  How much should I write? Most students submit something in the 10-to-15-page range, but it's really up to you. Be precise and thorough.
\item
  How do students collaborate successfully? Groups that split up the work typically seem to do the best. So one student worries about the survey, one about simulating and analysing data, and another about the write-up. If you're worried about using GitHub to collaborate, then just create different folders in GitHub to place your separate bits of work, and then have one person bring it together at the end.
\item
  What intervention should we use? The intention is that you do something of interest to you. A well-written introduction would make the intervention clear.
\end{itemize}

\hypertarget{rubric-2}{%
\subsection{Rubric}\label{rubric-2}}

\begin{itemize}
\tightlist
\item
  Go/no-go \#1: R is cited - {[}1 `Yes', 0 `No'{]}

  \begin{itemize}
  \tightlist
  \item
    Both referred to in the main content and included in the reference list.
  \item
    If not, no need to continue marking, just give paper 0 overall.
  \end{itemize}
\item
  Title - {[}2 `Exceptional', 1 `Yes', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    An informative title is included.
  \item
    Tell the reader what your story is - don't waste their time.
  \item
    Ideally tell them what happens at the end of the story.
  \item
    `Problem Set X' is not an informative title. There should be no evidence this is a school paper.
  \end{itemize}
\item
  Author, date, and repo - {[}2 `Yes', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    The author, date of submission, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: `Code and data supporting this analysis is available at: LINK').
  \end{itemize}
\item
  Abstract - {[}4 `Exceptional', 3 `Great', 2 `Fine', 1 `Gets job done', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    An abstract is included and appropriately pitched to a general audience.
  \item
    The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level).
  \item
    If your abstract is longer than four sentences then you need to think a lot about whether it is too long. It may be fine (there are always exceptions) but you should probably have a good reason.
  \item
    Your abstract must tell the reader your top-level finding. What is the one thing that we learn about the world because of your paper?
  \end{itemize}
\item
  Introduction - {[}4 `Exceptional', 3 `Great', 2 `Fine', 1 `Gets job done', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    The introduction is self-contained and tells a reader everything they need to know, including putting it into a broader context.
  \item
    Your introduction should provide a bit of broader context to motivate the reader, as well as providing a bit more detail about what you're interested in, what you did, what you found, why it's important, etc.
  \item
    A reader should be able to read only your introduction and have a good idea about the research that you carried out.
  \item
    It would be rare that you would have tables or figures in your introduction (again there are always exceptions but think deeply about whether yours is one).
  \item
    It must outline the structure of the paper.
  \item
    For instance (and this is just a rough guide) an introduction for a 10 page paper, should probably be about 3 or 4 paragraphs, or 10 per cent, but it depends on specifics.
  \end{itemize}
\item
  Data - {[}10 `Exceptional', 8 `Great', 6 `Good', 4 `Some issues', 2 `Many issues', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    The survey has a clear, detailed, and justified methodology that has been thoroughly discussed. The statistical basis for the approach that is used is clear.

    \begin{itemize}
    \tightlist
    \item
      It would achieve the aims of the report.
    \item
      Population, frame, sample, and other key aspects are described.
    \item
      There is a detailed plan for reaching respondents.
    \item
      Cost is discussed and appropriate.
    \item
      Non-response is discussed and a plan for dealing with it clearly articulated.
    \item
      There is clear respect for the survey respondents.
    \end{itemize}
  \item
    When you discuss the dataset (that you have likely largely simulated) you should make sure to discuss at least:

    \begin{itemize}
    \tightlist
    \item
      The source of the data.
    \item
      The methodology and approach that is used to collect and process the data.
    \item
      The population, the frame, and the sample (as appropriate).
    \item
      Information about how respondents were found. What happened to non-response?
    \item
      What are the key features, strengths, and weaknesses about the survey generally.
    \end{itemize}
  \item
    You should thoroughly discuss the variables in the dataset that you use. Are there any that are very similar that you nonetheless don't use? Did you construct any variables by combining various ones?
  \item
    What do the data look like?
  \item
    Plot the actual data that you're using (or as close as you can get to it).
  \item
    Discuss these plots and the other features of these data.
  \item
    These are just some of the issues strong submissions will consider. Show off your knowledge. If this becomes too detailed, then you should push some of this to footnotes or an appendix.
  \item
    `Exceptional' means that when I read your submission I learn something about the dataset that I don't learn from any other submission (within a reasonable measure of course).
  \end{itemize}
\item
  Discussion - {[}10 `Exceptional', 8 `Great', 6 `Good', 4 `Some issues', 2 `Many issues', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    Some questions that a good discussion would cover include:

    \begin{itemize}
    \tightlist
    \item
      What is done in this this paper?
    \item
      What are the main points that we learn about the world?
    \item
      What are some weaknesses of what was done?
    \item
      What is left to learn?
    \end{itemize}
  \end{itemize}
\item
  Details of the survey - {[}6 `Exceptional', 5 `Great', 4 `Good', 3 `Some issues', 2 `Many issues', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    A working link to survey is included in the appendix.
  \item
    Screenshots or at least the survey questions are included in the appendix.

    \begin{itemize}
    \tightlist
    \item
      The survey has been well put together.
    \item
      The number and length of the questions is appropriate.
    \item
      The question type and potential answers are appropriate.
    \item
      It is clear how this survey could accomplish the aims of the report.
    \item
      The questions flow in an appropriate way.
    \end{itemize}
  \end{itemize}
\item
  The simulation of survey responses is appropriate to the survey and the scenario - {[}4 `Exceptional', 3 `Great', 2 `Fine', 1 `Gets job done', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    It has been done in a reproducible way and either contained in a separate R file, or is in the report appendix.
  \end{itemize}
\item
  Numbering - {[}2 `Yes', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    All figures, tables, equations, etc are numbered and referred to in the text.
  \end{itemize}
\item
  Proofreading - {[}2 `Yes', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    All aspects of submission are free of noticeable typos.
  \end{itemize}
\item
  Graphs/tables/etc - {[}4 `Exceptional', 3 `Great', 2 `Fine', 1 `Gets job done', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    You must include graphs and tables in your paper and they must be to a high standard.
  \item
    They must be well formatted and camera-ready They should be clear and digestible.
  \item
    They must: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of labels/explanations, etc; and 3) appropriately sized and coloured (or appropriate significant figures in the case of stats).
  \end{itemize}
\item
  References - {[}4 `Perfect', 3 `One minor issue', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    All data/software/literature/etc are appropriately noted and cited.
  \item
    You must cite the software and software packages that you use.
  \item
    You must cite the datasets that you use.
  \item
    You must cite literature that you refer to (and you should refer to literature).
  \item
    If you take a small chunk of code from Stack Overflow then add the page in a comment next to the code.
  \item
    If you take a large chunk of code then cite it fully.
  \item
    3 means one minor issue. More than one minor issue receives 0.
  \end{itemize}
\item
  Reproducibility - {[}4 `Exceptional', 3 `Great', 2 `Fine', 1 `Gets job done', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    The paper and analysis must be fully reproducible.
  \item
    A detailed README is included.
  \item
    All code should be thoroughly documented.
  \item
    An R project is used. Do not use \texttt{setwd()}.
  \item
    The code must appropriately read data, prepare it, create plots, conduct analysis, generate documents, etc. Seeds are used where needed.
  \item
    Code must have a preamble etc.
  \item
    You must appropriately document your scripts such that someone coming in could follow them.
  \item
    Your repo must be thoroughly organized and not contain extraneous files.
  \end{itemize}
\item
  General excellence - {[}3 `Exceptional', 2 `Wow', 1 `Huh, that's interesting', 0 `None'{]}

  \begin{itemize}
  \tightlist
  \item
    There are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.
  \end{itemize}
\end{itemize}

\hypertarget{two-cathedrals}{%
\section{`Two Cathedrals'}\label{two-cathedrals}}

\hypertarget{task-3}{%
\subsection{Task}\label{task-3}}

\begin{itemize}
\tightlist
\item
  Working individually, please conduct original research that applies methods from statistics to a question that involves an experiment.
\end{itemize}

\hypertarget{guidance-3}{%
\subsection{Guidance}\label{guidance-3}}

You have various options for topics (pick one):

\begin{itemize}
\item
  Develop a research question that is of interest to you and obtain or create a relevant dataset. This option involves developing your own research question based on your own interests, background, and expertise. I encourage you to take this option, but please discuss your plans with me. How does one come up with ideas? One way is to be question-driven, where you keep an informal log of small ideas, questions, and puzzles, that you have as you're reading and working. Often, after dwelling on it for a while you can manage to find some questions of interest. Another way is to be data-driven - try to find some interesting dataset and then work backward. Finally, yet another way, is to be methods-driven - let's say that you happen to understand Gaussian processes, then just apply that expertise.
\item
  A replication exercise, being sure to use the paper as a foundation rather than as a ends-in-itself.
\item
  You should know the expectations by now. If you need a refresher then review the past problem sets. But essentially:

  \begin{itemize}
  \tightlist
  \item
    Everything is entirely reproducible.
  \item
    Your paper must be written in R Markdown.
  \item
    Your paper must have the following sections:

    \begin{itemize}
    \tightlist
    \item
      Title, date, author, keywords, abstract, introduction, data, model, results, discussion, appendix (optional, for supporting, but not critical, material), and a reference list.
    \end{itemize}
  \item
    Your paper must be well-written, draw on relevant literature, and show your statistical skills by explaining all statistical concepts that you draw on.
  \item
    The discussion needs to be substantial. For instance, if the paper were 10 pages long then a discussion should be at least 2.5 pages. In the discussion, the paper must include subsections on weaknesses and next steps - but these must be in proportion.
  \item
    The report must provide a link to a GitHub repo that contains everything (apart from any raw data that you git ignored if it is not yours to share). The code must be entirely reproducible, documented, and readable. The repo must be well-organised and appropriately use folders and README files.
  \end{itemize}
\end{itemize}

\hypertarget{peer-review-submission}{%
\subsection{Peer review submission}\label{peer-review-submission}}

\begin{itemize}
\tightlist
\item
  My expectations for this paper are very high. I'm very excited to read what you submit. To help you achieve this standard, there is an initial `submission' where you can get comments and feedback and then the final, actual, submission.
\item
  Submit initial materials for peer-review.

  \begin{itemize}
  \tightlist
  \item
    As an individual, via Quercus, submit a PDF of your rough draft on Quercus.
  \item
    At a minimum this must include:
  \item
    All top-matter (title, author (you can use a pseudonym if you want), date, keywords, abstract) completely filled out.
  \item
    A fully written Introduction section.
  \end{itemize}
\item
  All other sections must be present in your paper, but don't have to be filled out (e.g.~you must have a `Data' heading, but you don't need to have content in that section).
\item
  To be clear - it is fine to later change any aspect of what you submit at this checkpoint.
\item
  You will be awarded one percentage point just for submitting a draft that meets this minimum.
\item
  The point of this is to get feedback on your work (and to make sure you have at least started thinking about this project) so you are more than welcome to (and so, if it is at all possible) include other sections that you wish to get feedback on.
\item
  There will be no extensions granted for this submission since the following submission is dependent on this date.
\end{itemize}

\hypertarget{conduct-peer-review}{%
\subsection{Conduct peer-review}\label{conduct-peer-review}}

\begin{itemize}
\tightlist
\item
  As an individual, you will randomly be assigned a handful of rough drafts to provide feedback. You have three days to provide feedback to your peers.
\item
  If you provide feedback to one peer you will receive one percentage point, if you provide feedback to two peers you will receive two percentage points, if you provide feedback to three (or more) peers you will receive the full three percentage points.
\item
  Your feedback must include at least five comments (meaningful/useful bullet points). These must be well-written and thoughtful.
\item
  There will be no extensions granted for this submission since the following submission is dependent on this date.
\item
  Please remember that you are providing feedback here to help your colleagues. All comments should be professional and kind. It is challenging to receive criticism. Please remember that your goal here is to help your peers advance their writing/analysis. Any feedback that is inappropriate or not up to standard will receive a 0.
\end{itemize}

\hypertarget{checks-3}{%
\subsection{Checks}\label{checks-3}}

\begin{itemize}
\tightlist
\item
  Do you have a causal story, or at least a sub-section in the discussion that talks about causality and why you cannot speak to it, or what you would do if you could?
\end{itemize}

\hypertarget{faq-3}{%
\subsection{FAQ}\label{faq-3}}

\begin{itemize}
\tightlist
\item
  Can I work as part of a team? No.~It's important that you have some work that is entirely your own. You really need your own work to show off for job applications etc.
\item
  How much should I write? Most students submit something that has 10-to-16-pages of main content, with additional pages devoted to appendices, but it's really up to you. Be precise and thorough.
\end{itemize}

\hypertarget{rubric-3}{%
\subsection{Rubric}\label{rubric-3}}

\begin{itemize}
\tightlist
\item
  Go/no-go \#1: R is cited - {[}1 `Yes', 0 `No'{]}

  \begin{itemize}
  \tightlist
  \item
    Both referred to in the main content and included in the reference list.
  \item
    If not, no need to continue marking, just give paper 0 overall.
  \end{itemize}
\item
  Title - {[}2 `Exceptional', 1 `Yes', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    An informative title is included.
  \item
    Tell the reader what your story is - don't waste their time.
  \item
    Ideally tell them what happens at the end of the story.
  \item
    `Problem Set X' is not an informative title. There should be no evidence this is a school paper.
  \end{itemize}
\item
  Author, date, and repo - {[}2 `Yes', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    The author, date of submission, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: `Code and data supporting this analysis is available at: LINK').
  \end{itemize}
\item
  Abstract - {[}4 `Exceptional', 3 `Great', 2 `Fine', 1 `Gets job done', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    An abstract is included and appropriately pitched to a general audience.
  \item
    The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level).
  \item
    If your abstract is longer than four sentences then you need to think a lot about whether it is too long. It may be fine (there are always exceptions) but you should probably have a good reason.
  \item
    Your abstract must tell the reader your top-level finding. What is the one thing that we learn about the world because of your paper?
  \end{itemize}
\item
  Introduction - {[}4 `Exceptional', 3 `Great', 2 `Fine', 1 `Gets job done', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    The introduction is self-contained and tells a reader everything they need to know, including putting it into a broader context.
  \item
    Your introduction should provide a bit of broader context to motivate the reader, as well as providing a bit more detail about what you're interested in, what you did, what you found, why it's important, etc.
  \item
    A reader should be able to read only your introduction and have a good idea about the research that you carried out.
  \item
    It would be rare that you would have tables or figures in your introduction (again there are always exceptions but think deeply about whether yours is one).
  \item
    It must outline the structure of the paper.
  \item
    For instance (and this is just a rough guide) an introduction for a 10 page paper, should probably be about 3 or 4 paragraphs, or 10 per cent, but it depends on specifics.
  \end{itemize}
\item
  Data - {[}10 `Exceptional', 8 `Great', 6 `Good', 4 `Some issues', 2 `Many issues', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    You should thoroughly discuss the variables in the dataset that you use. Are there any that are very similar that you nonetheless don't use? Did you construct any variables by combining various ones?
  \item
    What do the data look like?
  \item
    Plot the actual data that you're using (or as close as you can get to it).
  \item
    Discuss these plots and the other features of these data.
  \item
    These are just some of the issues strong submissions will consider. Show off your knowledge. If this becomes too detailed, then you should push some of this to footnotes or an appendix.
  \item
    `Exceptional' means that when I read your submission I learn something about the dataset that I don't learn from any other submission (within a reasonable measure of course).
  \end{itemize}
\item
  Model - {[}10 `Exceptional', 8 `Great', 6 `Good', 4 `Some issues', 2 `Many issues', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    The model is nicely written out, well-explained, justified, and appropriate.
  \item
    When you discuss your model you must be extremely careful to spell out the statistical model that you are using defining and explaining each aspect and why it is important. Failure to do this suggests you don't understand the model.
  \item
    The model is appropriately complex - that is, not too simple, but not unnecessarily complicated.
  \item
    The model has well-defined variables and these correspond to what is discussed in the data section.
  \item
    The model needs to be written out in appropriate mathematical notation but also in plain English.
  \item
    Every aspect of that notation must be defined otherwise the most this section can receive is poor.
  \item
    The model makes sense based on the substantive area, and also the form of the model.
  \item
    If the model is Bayesian, then priors need to be defined and sensible.
  \item
    Discussion needs to occur around how features enter the model and why. For instance, (and these are just examples) why use ages rather than age-groups, why does province have a levels effect, why is gender categorical, etc?
  \item
    In general, in order to be adequate, there needs to be a clear justification that this is the model for the situation.
  \item
    The assumptions underpinning the model are clearly discussed.
  \item
    Alternative models, or variants, must be discussed and strengths and weaknesses made clear. Why was this model chosen?
  \item
    You should mention the software that you used to run the model.
  \item
    There is some evidence of thought about the circumstances in which the model may not be appropriate.
  \item
    There is evidence of model validation and checking, whether that is out of sample or comparison to a straw man or RMSE, test/training, or appropriate sensitivity checks.
  \item
    You should be clear about model convergence, model checks, and diagnostic issues, but if this becomes too detailed then you could push some of this to an appendix.
  \item
    Great answers would discuss things such as, how do the aspects that you discussed in the data section assert themselves in the modelling decisions that you make. Again if it becomes too detailed then push some of the details to footnotes or an appendix.
  \item
    Again, explain what your model is and what is going on with it.
  \end{itemize}
\item
  Results - {[}10 `Exceptional', 8 `Great', 6 `Good', 4 `Some issues', 2 `Many issues', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    Results will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps.
  \item
    To be clear, you should also have text associated with all these aspects.
  \item
    Show the reader the results by plotting them. Talk about them. Explain them. That said, this section should strictly relay results.
  \end{itemize}
\item
  Discussion - {[}10 `Exceptional', 8 `Great', 6 `Good', 4 `Some issues', 2 `Many issues', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    Some questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page):

    \begin{itemize}
    \tightlist
    \item
      What is done in this this paper?
    \item
      What is something that we learn about the world?
    \item
      What is another thing that we learn about the world?
    \item
      What are some weaknesses of what was done?
    \item
      What is left to learn or how should we proceed in the future?
    \end{itemize}
  \end{itemize}
\item
  Numbering - {[}2 `Yes', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    All figures, tables, equations, etc are numbered and referred to in the text.
  \end{itemize}
\item
  Proofreading - {[}2 `Yes', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    All aspects of submission are free of noticeable typos.
  \end{itemize}
\item
  Graphs/tables/etc - {[}4 `Exceptional', 3 `Great', 2 `Fine', 1 `Gets job done', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    You must include graphs and tables in your paper and they must be to a high standard.
  \item
    They must be well formatted and camera-ready They should be clear and digestible.
  \item
    They must: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of labels/explanations, etc; and 3) appropriately sized and coloured (or appropriate significant figures in the case of stats).
  \end{itemize}
\item
  References - {[}4 `Perfect', 3 `One minor issue', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    All data/software/literature/etc are appropriately noted and cited.
  \item
    You must cite the software and software packages that you use.
  \item
    You must cite the datasets that you use.
  \item
    You must cite literature that you refer to (and you should refer to literature).
  \item
    If you take a small chunk of code from Stack Overflow then add the page in a comment next to the code.
  \item
    If you take a large chunk of code then cite it fully.
  \item
    3 means one minor issue. More than one minor issue receives 0.
  \end{itemize}
\item
  Reproducibility - {[}4 `Exceptional', 3 `Great', 2 `Fine', 1 `Gets job done', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    The paper and analysis must be fully reproducible.
  \item
    A detailed README is included.
  \item
    All code should be thoroughly documented.
  \item
    An R project is used. Do not use \texttt{setwd()}.
  \item
    The code must appropriately read data, prepare it, create plots, conduct analysis, generate documents, etc. Seeds are used where needed.
  \item
    Code must have a preamble etc.
  \item
    You must appropriately document your scripts such that someone coming in could follow them.
  \item
    Your repo must be thoroughly organized and not contain extraneous files.
  \end{itemize}
\item
  Enhancements - {[}4 `Exceptional', 3 `Great', 2 `Fine', 1 `Gets job done', 0 `Poor or not done'{]}

  \begin{itemize}
  \tightlist
  \item
    You should pick at least one of the following and include it to enhance your submission:

    \begin{itemize}
    \tightlist
    \item
      Datasheets for for your datasets (see: \url{https://arxiv.org/abs/1803.09010}) and model cards for your models (see: \url{https://arxiv.org/pdf/1810.03993.pdf}).
    \item
      Shiny application.
    \item
      R package.
    \item
      API for your model.
    \end{itemize}
  \item
    There are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.
  \end{itemize}
\item
  General excellence - {[}3 `Exceptional', 2 `Wow', 1 `Huh, that's interesting', 0 `None'{]}

  \begin{itemize}
  \tightlist
  \item
    There are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.
  \end{itemize}
\end{itemize}

\hypertarget{previous-examples-1}{%
\subsection{Previous examples}\label{previous-examples-1}}

Some examples of papers that well in the past include those by: \href{inputs/pdfs/Farrow_Amy-replication_paper.pdf}{Amy Farrow}, \href{inputs/pdfs/Cline_Laura-IndigenousWomenInPrison.pdf}{Laura Cline}, \href{inputs/pdfs/Shi_Hong-replication_paper.pdf}{Hong Shi}, \href{inputs/pdfs/Jia_Jia_Ji-effect_of_DLL_ON_spanish_literacy_skills.pdf}{Jia Jia Ji}, and
\href{inputs/pdfs/Lam_Rachael-final_paper.pdf}{Rachael Lam}.

\hypertarget{a-proportional-response}{%
\section{`A Proportional Response'}\label{a-proportional-response}}

\hypertarget{task-4}{%
\subsection{Task}\label{task-4}}

Working in teams of one to four people, please consider this scenario:

\begin{itemize}
\tightlist
\item
  `You are employed as a junior statistician at Petit Poll - a Canadian polling company. Petit Poll has a contract with a Canadian political party to provide them with monthly polling updates.'
\item
  Working as part of a small team of 1-4 people, and in an entirely reproducible way, please write a short paper that tells the client a story about their standing.
\end{itemize}

\hypertarget{recommended-steps}{%
\subsection{Recommended steps}\label{recommended-steps}}

\begin{itemize}
\tightlist
\item
  Please pick a political party that you are `working for', and pick a geographic focus: 1) the overall election, 2) a particular province, or 3) a specific riding.
\item
  Then decide on a survey methodology (hint: p.~13 of Wu \& Thompson provides a handy checklist). Some questions you should address include:
\item
  What is the population, frame, and sample?
\item
  What sampling methods will you use and why (e.g.~you could choose SRSWOR, stratified, etc). What are some of the statistical properties that the method brings to the table (e.g.~for SRSWOR you could discuss Wu \& Thompson, Theorem 2.2, etc, as appropriate)?
\item
  How are you going to reach your desired respondents?
\item
  How much do you estimate this will cost?
\item
  What steps will you take to deal with non-response and how will non-response affect your survey?
\item
  How are you going to protect respondent privacy?
\item
  Remember to consider all of this in the context of your `client' - for instance, who would be more interested in Alberta ridings: Bloc Québécois or the Conservatives? Who likely has more money to spend - the Liberals or the Greens?
\item
  Develop a survey on a platform that was introduced in class. Be sure to test it yourselves. You will want to test this as much as possible, maybe even swap informally with another group?
\item
  Now release the surveys into the (simulated) `field'. Please do this by simulating an appropriate number of responses to your survey in R. Don't forget to simulate in relation to the survey methodology that you proposed. Show the results and discuss your `findings'. Everything must be entirely reproducible. You may like to consider linking your survey `responses' with other data such as the census or GSS.
\item
  Use R Markdown to write a PDF report about all of this. Discuss your results and findings, your survey design and motivations, etc - all of it. You are writing a report that will eventually go to the `client', so you must set the scene, and use language that demonstrates your command of statistical concepts but brings the reader along with you. Be sure to include graphs and tables and reference them in your discussion. Be sure to be clear about weaknesses and biases, and opportunities for future work.
\item
  Your report must be well written. You are allowed to, and should, use mathematical notation, but you must explain all of it in plain english. Similarly, you can, and should, use surveys/sampling/observational data terminology, but again, you need to explain it.
\item
  Your report must include at least the following aspects: title, date, authorship, non-technical executive summary, introduction, survey methodology, results, discussion, appendices that detail the survey, and references. Your `client' has stats graduates working for it who need to be impressed by the main content of the report, but also has people who barely know what an average is and these people need to be impressed also. This is why your report should include a non-technical executive summary. In terms of length, this would typically be roughly 10 per cent of the report. It would be more detailed than an introduction, but still at a high level.
\item
  Your graphs must be of an extremely high standard.
\item
  Check that you have referenced everything. Strong submissions will draw on related literature in the discussion and would be sure to also reference those. The style of references does not matter, provided it is consistent.
\item
  Via Quercus, submit a link to your PDF report which is hosted on GitHub. At some point in the introduction to your report, you must provide a link to the GitHub repo where the code that you used for this assignment lives (Hint: Comment. Your. Code.). Your entire workflow must be entirely reproducible.
\item
  Please be sure to include a link to your survey in your report and screenshots of the survey in the appendix of your report.
\item
  There should be no evidence that this is a class assignment.
\end{itemize}

\hypertarget{checks-4}{%
\subsection{Checks}\label{checks-4}}

\hypertarget{faq-4}{%
\subsection{FAQ}\label{faq-4}}

\hypertarget{mr-willis-of-ohio}{%
\section{`Mr Willis of Ohio'}\label{mr-willis-of-ohio}}

\hypertarget{task-5}{%
\subsection{Task}\label{task-5}}

\begin{itemize}
\tightlist
\item
  Working in teams of one to four people, and in an entirely reproducible way, please use the Canadian General Social Survey (GSS) and a regression model to tell a story.
\end{itemize}

\hypertarget{recommended-steps-1}{%
\subsection{Recommended steps}\label{recommended-steps-1}}

\begin{itemize}
\tightlist
\item
  Depending on your focus and background, you may like to use a Bayesian hierarchical model, but regardless of the particular model that you use it must be well explained, thoroughly justified, appropriate to the task at hand, and the results must be beautifully described.
\item
  You may focus on any year, aspect, or geography that is reasonable given the focus and constraints of the GSS. As a reminder, the GSS `program was designed as a series of independent, annual, cross-sectional surveys, each covering one topic in-depth.' So please consider the topic and the year.
\item
  The GSS is available to University of Toronto students via the library. In order to use it you need to clean and prepare it. Code to do this for one year is being distributed alongside this problem set and was discussed in lectures.
\item
  You are welcome to simply use this code and this year, but the topic of that year will constrain your focus. Naturally, you are welcome to adapt the code to other years. If you use the code exactly as is then you must cite it. If you adapt the code then you don't have to cite it, as it has a MIT license, but it would be appropriate to at least mention and acknowledge it, depending on how close your adaption is.
\item
  Using R Markdown, please write a paper about your analysis and compile it into a PDF.
\item
  Your paper must be well-written, draw on relevant literature, and show your statistical skills by explaining all statistical concepts that you draw on.
\item
  Your paper must have the following sections: title, name/s, and date, abstract, introduction, data, model, results, discussion, and references.
\item
  You are welcome to use appendices for supporting, but not critical, material. Your discussion must include sub-sections on weaknesses and next steps.
\item
  In your report you must provide a link to a GitHub repo that fully contains your analysis. Your code must be entirely reproducible, documented, and readable. Your repo must be well-organised and appropriately use folders.
\item
  Your graphs and tables must be of an incredibly high standard. Graphs and tables should be well formatted and report-ready. They should be clean and digestible. Furthermore, you should label and describe each table/figure.
\item
  When you discuss the dataset (in the data section) you should make sure to discuss (at least):

  \begin{itemize}
  \tightlist
  \item
    Its key features, strengths, and weaknesses generally.
  \item
    A discussion of the questionnaire - what is good and bad about it?
  \item
    A discussion of the methodology including how they find people to take the survey; what their population, frame, and sample were; what sampling approach they took and what some of the trade-offs may be; what they do about non-response; the cost.
  \item
    This is just some of the issues strong submissions will consider. Show off your knowledge. If this becomes too detailed then you should push some of this to footnotes or an appendix.
  \end{itemize}
\item
  When you discuss your model (in the model section), you must be extremely careful to spell out the statistical model that you are using, defining and explaining each aspect and why it is important. (For a Bayesian model, a discussion of priors and regularization is almost always important.) You should mention the software that you used to run the model. You should be clear about model convergence, model checks, and diagnostic issues. How do the sampling and survey aspects that you discussed assert themselves in the modelling decisions that you make? Again, if it becomes too detailed then push some of the details to footnotes or an appendix.
\item
  You should present model results, graphs, figures, etc, in the results section. This section should strictly relay results. Interpretation of these results and conclusions drawn from the results should be left for the discussion section.
\item
  Your discussion should focus on your model results. Interpret them and explain what they mean. Put them in context. What do we learn about the world having understood your model and its results? What caveats could apply? To what extent does your model represent the small world and the large world (to use the language of McElreath, Ch 2)? What are some weaknesses and opportunities for future work?
\item
  Check that you have referenced everything. Strong submissions will draw on related literature in the discussion (and other sections) and would be sure to also reference those. The style of references does not matter, provided it is consistent.
\item
  As a team, via Quercus, submit a PDF of your paper. Again, in your paper you must have a link to the associated GitHub repo in an appendix. And you must include the R Markdown file that produced the PDF in that repo.
\item
  A good way to work as a team would be to split up the work, so that one person is doing each section. The people doing the sections that rely on data (such as the analysis and the graphs) could just simulate it while they are waiting for the person putting together the data to finish.
\item
  It is expected that your submission be well written and able to be understood by the average reader of say 538. This means that you are allowed to use mathematical notation, but you must be able to explain it all in plain English. Similarly, you can (and hint: you should) use survey, sampling, observational, and statistical terminology, but again you need to explain it. Your work should have flow and should be easy to follow and understand. To communicate well, anyone at the university level should be able to read your report once and relay back the methodology, overall results, findings, weaknesses and next steps without confusion.
\end{itemize}

\hypertarget{checks-5}{%
\subsection{Checks}\label{checks-5}}

\begin{itemize}
\tightlist
\item
  It is recommended that you (informally) proofread one another's sections - why not exchange papers with another group?
\item
  Everyone in the team receives the same mark.
\item
  There should be no evidence that this is a class assignment.
\end{itemize}

\hypertarget{faq-5}{%
\subsection{FAQ}\label{faq-5}}

\hypertarget{five-votes-down}{%
\section{`Five Votes Down'}\label{five-votes-down}}

\hypertarget{task-6}{%
\subsection{Task}\label{task-6}}

\begin{itemize}
\tightlist
\item
  The primary goal of this paper is to predict the overall popular vote of the 2020 American presidential election using multilevel regression with post-stratification.
\end{itemize}

\hypertarget{recommended-steps-2}{%
\subsection{Recommended steps}\label{recommended-steps-2}}

\begin{itemize}
\tightlist
\item
  We expect you to work as part of a group of 4 people, but groups of size 1-4 are fine. We have suggested a split of the work based on a 4-person group, but these are just suggestions.
\item
  Individual-level survey data:

  \begin{itemize}
  \tightlist
  \item
    Request access to the Democracy Fund + UCLA Nationscape `Full Data Set': \url{https://www.voterstudygroup.org/publication/nationscape-data-set}. This could take a day or two. Please start early.
  \item
    Given the expense of collecting this data, and the privilege of having access to it, if you don't properly cite this dataset then you will get zero for this problem set.
  \item
    Once you have access then pick a survey of interest. We will use ``ns20200102.dta'' in the example (your number may be different).
  \item
    This will be a large file and is not yours to share. Do not push it to GitHub (use the .gitignore file - see here: \url{https://carpentries-incubator.github.io/git-Rstudio-course/02-ignore/index.html}).
  \item
    Use the example R code to get started preparing this dataset, and then go on cleaning and preparing it based on what you need.
  \item
    Make graphs and tables about the survey data and write beautiful sentences and paragraphs explaining everything.
  \end{itemize}
\item
  Post-stratification data:

  \begin{itemize}
  \tightlist
  \item
    We will use the American Community Surveys (ACS).
  \item
    Please create an account with IPUMS: \url{https://usa.ipums.org/usa/index.shtml}
  \item
    You want the 2018 1-year ACS. Then you need to select some variables. This will depend on what you want to model and the survey data, but some options include: REGION, STATEICP, AGE, SEX, MARST, RACE, HISPAN, BPL, CITIZEN, EDUC, LABFORCE, INCTOT. Have a look around and see what you are interested in, remembering that you will need to establish a correspondence to the survey.
  \item
    Download the relevant post-stratification data (it's probably easiest to change the data format to .dta). Again, this can take some time. Please start this early.
  \item
    This will be a large file and is not yours to share. Do not push it to GitHub (use the .gitignore file - see here: \url{https://carpentries-incubator.github.io/git-Rstudio-course/02-ignore/index.html}).
  \item
    Given the expense of collecting this data, and the privilege of having access to it, if you don't properly cite this dataset then you will get zero for this problem set.
  \item
    Clean and prepare the post-stratification dataset.
  \item
    Remember that you need cell counts for the sub-populations in your model. See examples in the readings.
  \end{itemize}
\item
  (It may be efficient to start with simulated data while waiting for the real data) Modelling.

  \begin{itemize}
  \tightlist
  \item
    You will want to explain vote intention based on a variety of explanatory variables. Construct the vote intention variable so that it is binary (either `supports Trump' or `supports Biden').
  \item
    You are welcome to use lm() but you would need to explain the nuances of this decision in the model section (Hint: start here: \url{https://statmodeling.stat.columbia.edu/2020/01/10/linear-or-logistic-regression-with-binary-outcomes/}).
  \item
    That said, you should probably use logistic regression if it is at all possible for you. If you don't know where to start then look at (in increasing levels of complexity) glm(), lme4::glmer(), or brms::brm(). There are examples of each in the readings.
  \item
    Think very deeply about model fit, diagnostics, and other similar things that you need in order to convince someone that your model is appropriate.
  \item
    You have flexibility of the model that you use, (and hence the cells that you will need to create next). In general, the more cells the better, but you may want fewer cells for simplicity in the writing process and to ensure a decent sample in each cell.
  \item
    Apply your trained model to the post-stratification dataset to make the best estimate of the election result that you can. The specifics will depend on your modelling approach but will likely involve predict(), add\_predicted\_draws(), or similar. See the examples in the readings. We are primarily interested in the distribution of your forecast of the overall Presidential popular vote, and how the explanatory variables affect this. But great submissions would go beyond that. Also, you're taking a statistics course, so if you just gave a central estimate and nothing else, then that would not be great.
  \item
    Create beautiful graphs and tables of your model and results.
  \item
    Create wonderful paragraphs talking about and explaining everything.
  \end{itemize}
\item
  (Again, it's probably efficient to start with simulated data/results while waiting)

  \begin{itemize}
  \tightlist
  \item
    Write up.
  \item
    Using R Markdown, please write a very thorough paper about your analysis and compile it into a PDF.
  \item
    The paper must be well-written, draw on relevant literature, and show your statistical skills by explaining all statistical concepts that you draw on.
  \item
    The paper must have the following sections: title, name/s, and date, abstract and keywords, introduction, data, model, results, discussion, and references.
  \item
    The paper may use appendices for supporting, but not critical, material.
  \item
    The discussion needs to be substantial. For instance, if the paper were 10 pages long then a discussion should be at least 2.5 pages. In the discussion, the paper must include subsections on weaknesses and next steps - but these must be in proportion.
  \item
    The report must provide a link to a GitHub repo that contains everything (apart from the raw data that you git ignored because it is not yours to share). The code must be entirely reproducible, documented, and readable. The repo must be well-organised and appropriately use folders and README files.
  \item
    The graphs and tables must be of an incredibly high standard, well formatted, and report-ready. They should be clean and digestible. Furthermore, you should label and describe each table/figure.
  \item
    When you discuss the datasets (in the data section) (remember there will be at least two datasets to discuss) you should make sure to discuss (at least):

    \begin{itemize}
    \tightlist
    \item
      Their key features, strengths, and weaknesses generally.
    \item
      The survey questionnaire - what is good and bad about it?
    \item
      A discussion of the methodology including how they find people to take the survey; what their population, frame, and sample were; what sampling approach they took and what some of the trade-offs may be; what they do about non-response; the cost.
    \item
      This is just some of the issues strong submissions will consider. Show off your knowledge. If this becomes too detailed then you should push some of this to footnotes or an appendix.
    \end{itemize}
  \item
    The dataset section is probably an appropriate place to include an explanation of what post-stratification is (in non-statistical language) and the strengths and weaknesses of it, although this discussion may fit more naturally in another section. Regardless, be sure to justify the inclusion of each explanatory variable.
  \item
    When you discuss your model (in the model section), you must be extremely careful to spell out the statistical model that you are using, defining and explaining each aspect and why it is important. (For a Bayesian model, a discussion of priors and regularization is almost always important.) You should mention the software that you used to run the model. You should be clear about model convergence, model checks, and diagnostic issues, although you may push the details of this to an appendix depending on how detailed you get. How do the sampling and survey aspects that you discussed assert themselves in the modelling decisions that you make? How can you convince a reader that you have neither overfit nor underfit the data? Again, if it becomes too detailed then push some of the details to footnotes or an appendix.
  \item
    You should present model results, graphs, figures, etc, in the results section. This section should strictly relay results. It must include text explaining all of these and summary statistics and similar. However, interpretation of these results and conclusions drawn from the results should be left for the discussion section.
  \item
    Your discussion should focus on your model results, but this time interpreting them, and explaining what they mean. Put them in context. What do we learn about the world having understood your model and its results? What caveats could apply? To what extent does your model represent the small world and the large world (to use the language of McElreath, Ch 2)? What are some weaknesses and opportunities for future work? Who is going to win the election? How confident are you in that forecast? Do you have a small or large distribution? What could that mean? Are you more confident in certain states? Do certain explanatory variables carry more weight than others? Etc.
  \item
    Check that you have referenced everything. Strong submissions will draw on related literature in the discussion (and other sections) and would be sure to also reference those. The style of references does not matter, but it must be consistent.
  \item
    If you don't cite R then you will get zero for this problem set.
  \item
    As a team, via Quercus, submit a PDF of your paper. Again, in your paper you must have a link to the associated GitHub repo. And you must include the R Markdown file that produced the PDF in that repo.
    The RMarkdown file must exactly produce the PDF. Don't edit it manually ex post - that isn't reproducible.
  \end{itemize}
\item
  A good way to work as a team would be to split up the work, so that one person is doing each section. The people doing the sections that rely on data (such as the analysis and the graphs) could just simulate it while they are waiting for the person putting together the data to finish. We have recommended a split above, but you do what works for you.
\end{itemize}

\hypertarget{checks-6}{%
\subsection{Checks}\label{checks-6}}

\begin{itemize}
\tightlist
\item
  It is expected that your submission be well written and able to be understood by the average reader of say 538. This means that you are allowed to use mathematical notation, but you must be able to explain it all in plain English. Similarly, you can (and hint: you should) use survey, sampling, observational, and statistical terminology, but again you need to explain it. The average person doesn't know what a p-value is nor what a confidence interval is. You need to explain all of this in plain language the first time you use it. Your work should have flow and should be easy to follow and understand. To communicate well, anyone at the university level should be able to read your report once and relay back the methodology, overall results, findings, weaknesses and next steps without confusion.
\item
  It is recommended that you (informally) proofread one another's work - why not exchange papers with another group?
\item
  Everyone in the team receives the same mark.
\item
  There should be no evidence that this is a class assignment.
\end{itemize}

\hypertarget{faq-6}{%
\subsection{FAQ}\label{faq-6}}

\hypertarget{whats-next}{%
\section{`What's next?'}\label{whats-next}}

\hypertarget{task-7}{%
\subsection{Task}\label{task-7}}

Please work individually. In this paper, you will conduct original research that applies methods from statistics to a question involving surveys, sampling or observational data.

\hypertarget{recommended-steps-3}{%
\subsection{Recommended steps}\label{recommended-steps-3}}

You have various options for topics (pick one):
Develop a research question that is of interest to you and obtain or create a relevant dataset. This option involves developing your own research question based on your own interests, background, and expertise. I encourage you to take this option, but please discuss your plans with me. How does one come up with ideas? One way is to be question-driven, where you keep an informal log of small ideas, questions, and puzzles, that you have as you're reading and working. Often, after dwelling on it for a while you can manage to find some questions of interest. Another way is to be data-driven - try to find some interesting dataset and then work backward. Finally, yet another way, is to be methods-driven - let's say that you happen to understand Gaussian processes, then just apply that expertise.
(Thanks to Jack Bailey for this idea.) Build a MRP model based on the CES and a post-stratification dataset that you obtain, to identify how the 2019 Canadian Federal Election would have been different if `everyone' had voted. What do we learn about the importance of turnout based on your model and results? (This option involves logistic regression in either frequentist or Bayesian settings.)
Reproduce a paper. This means that you download the data and then write your own code (using their code and paper as a guide if it's available) to try to get their results and then write up what you did and found. Options include:
Angelucci, Charles, and Julia Cagé, 2019, `Newspapers in times of low advertising revenues', American Economic Journal: Microeconomics, please see: \url{https://www.openicpsr.org/openicpsr/project/116438/version/V1/view}. (This option can be accomplished with just OLS. It is a `safe' pick. I even already provided you with some code in class to get started - see the notes! ).
Bailey, Michael A., Daniel J. Hopkins \& Todd Rogers, 2016, `Unresponsive and Unpersuaded: The Unintended Consequences of a Voter Persuasion Effort', Political Behavior.
Clark, Sam, 2019, `A General Age-Specific Mortality Model With an Example Indexed by Child Mortality or Both Child and Adult Mortality', Demography, please see: \url{https://github.com/sinafala/svd-comp}. (This is an ambiguous pick!)
Skinner, Ben, 2019, `Making the connection: Broadband access and online course enrollment at public open admissions institutions', Research in Higher Education, please see: \url{https://github.com/btskinner/oa_online_broadband_rep}.
Pons, Vincent, 2018, `Will a Five-Minute Discussion Change Your Mind? A Countrywide Experiment on Voter Choice in France' American Economic Review.
Valencia Caicedo, Felipe, 2019, `The Mission: Human Capital Transmission, Economic Persistence, and Culture in South America', The Quarterly Journal of Economics, please see: \url{https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ML1155}.
If you have a favourite paper and want to reproduce it, then please let me know by the end of Week 12 so that I can check that it's appropriate.
Pretend that you work for Upworthy. Request the Upworthy dataset and then use it to evaluate the result of an A/B test. This request could take a week. Please plan ahead if you choose this option.
Critique the following paper: AlShebli, Bedoor, Kinga Makovi \& Talal Rahwan, 2020, `The association between early career informal mentorship in academic collaborations and junior author performance', Nature Communications. You should be able to download the data here: \url{https://github.com/bedoor/Mentorship} and the paper here: \url{https://www.nature.com/articles/s41467-020-19723-8}. For background and starting points for your critique, please see: \url{https://statmodeling.stat.columbia.edu/2020/11/19/are-female-scientists-worse-mentors-this-study-pretends-to-know/} and \url{https://danieleweeks.github.io/Mentorship/\#summary}. (This option involves extensive data exploration and thinking really hard about what they are trying to do and how they are doing it.)
Use this post by Andrew Whitby - \url{https://andrewwhitby.com/2020/11/24/contact-tracing-biased/} - as a starting point to explore biased sampling and its effects on what we know about COVID and how this affects public policy. (This option involves extensive simulation.)
In `Bias Behind Bars', data journalist Tom Cardoso finds that `(a)fter controlling for a number of variables, \ldots{} Black and Indigenous inmates are more likely to get worse scores than white inmates, based solely on their race.'
The main story is here: \url{https://www.theglobeandmail.com/canada/article-investigation-racial-bias-in-canadian-prison-risk-assessments/}
The methodology discussion is here: \url{https://www.theglobeandmail.com/canada/article-investigation-racial-bias-in-canadian-prisons-methodology/}
The observational data is available here: \url{https://www.theglobeandmail.com/files/editorial/News/nw-na-risk-1023/The_Globe_and_Mail_CSC_OMS_2012-2018_20201022235635.zip}
Your task is to follow the methodology that Tom published and attempt to replicate the results. Are you able to replicate them? Do the results change significantly under slightly different assumptions? (This option involves only frequentist logistic regression, although doing everything in a Bayesian setting would be lovely too).
You should know the expectations by now. If you need a refresher then review the past problem sets.
Everything is entirely reproducible.
Your paper must be written in R Markdown.
Your paper must have the following sections:
Title, date, author, keywords, abstract, introduction, data, model, results, discussion, appendix (optional, for supporting, but not critical, material), and a reference list.
Your paper must be well-written, draw on relevant literature, and show your statistical skills by explaining all statistical concepts that you draw on.
The discussion needs to be substantial. For instance, if the paper were 10 pages long then a discussion should be at least 2.5 pages. In the discussion, the paper must include subsections on weaknesses and next steps - but these must be in proportion.
The report must provide a link to a GitHub repo that contains everything (apart from any raw data that you git ignored if it is not yours to share). The code must be entirely reproducible, documented, and readable. The repo must be well-organised and appropriately use folders and README files.
My expectations for this paper are very high. I'm very excited to read what you submit. To help you achieve this standard, there are two initial `submissions' where you can get comments and feedback and then the final, actual, submission.
Due dates:
(Optional) December 9 11:59pm ET
Submit initial materials for peer-review.
As an individual, via Quercus, submit a PDF of your rough draft on Quercus by 11:59pm ET on Wednesday, December 9, 2020.
At a minimum this must include:
All top-matter (title, author (you can use a pseudonym if you want), date, keywords, abstract) completely filled out.
A fully written Introduction section.
All other sections must be present in your paper, but don't have to be filled out (e.g.~you must have a `Data' heading, but you don't need to have content in that section).
To be clear - it is fine to later change any aspect of what you submit at this checkpoint.
You will be awarded 1 percentage point just for submitting a draft that meets this minimum (that is 1 out of the 30 that are available for the final paper). If you don't submit, then the percentage point will be pushed to part d).
The point of this is to get feedback on your work (and to make sure you have at least started thinking about this project) so you are more than welcome to include other sections that you wish to get feedback on.
There will be no extensions granted for this submission since the following submission is dependent on this date.
(Optional) December 12 11:59pm ET
Conduct peer-review.
As an individual, on December 10, you will randomly be assigned a handful of rough drafts to provide feedback. You have until December 12, 2020 11:59pm ET to provide feedback to your peers.
If you provide feedback to one peer you will receive 1 percentage point, if you provide feedback to two peers you will receive 2 percentage points, if you provide feedback to three (or more) peers you will receive the full 3 percentage points.
You may complete this aspect whether or not you submitted something in part a). If you don't complete it then the percentage points will be pushed to part d).
Your feedback must include at least six comments (meaningful/useful bullet points). These must be well-written and thoughtful.
There will be no extensions granted for this submission since the following submission is dependent on this date.
Please remember that you are providing feedback here to help your colleagues. All comments should be professional and kind. It is challenging to receive criticism. Please remember that your goal here is to help your peers advance their writing/analysis. Any feedback that is inappropriate or not up to standard will receive a 0 and cannot be redeemed later.
(Optional) December 16 11:59pm ET
Submit materials for TA review.
Submit a PDF to Quercus. The TA will provide high-level comments on December 17.
At a minimum this must include:
All top-matter.
Fully written Introduction, Data, Model, and Results sections.
All other sections must be present in your paper, but don't have to be filled out (e.g.~you must have a `Discussion' heading, but you don't need to have content in that section).
To be clear - it is fine to later change any aspect of what you submit at this checkpoint.
You receive 1 percentage point for submitting something that meets that minimum. If you don't submit anything then this is pushed to the final paper.
There are no extensions possible on this aspect.
(Compulsory) December 20 11:59pm ET
As an individual, via Quercus, submit a PDF of your paper. Again, in your paper, you must have a link to the associated GitHub repo.
This submission will be graded based on a rubric that will be posted on Quercus and will be worth 25-30 percentage points depending on parts a) - c).

\hypertarget{checks-7}{%
\subsection{Checks}\label{checks-7}}

\hypertarget{faq-7}{%
\subsection{FAQ}\label{faq-7}}

\begin{itemize}
\tightlist
\item
  Do I have to submit an initial paper in order to do the peer-review? Yes.
\end{itemize}

  \bibliography{bibliography.bib}

\backmatter
\printindex

\end{document}
