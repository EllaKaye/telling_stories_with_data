[["index.html", "Telling Stories With Data Chapter 1 Introduction 1.1 Welcome 1.2 Structure 1.3 On telling stories 1.4 Telling stories with data 1.5 Acknowledgements 1.6 Contact", " Telling Stories With Data Rohan Alexander 2021-01-25 Chapter 1 Introduction These notes are being actively developed. If you have comments or suggestions, or find mistakes, then please don’t hesitate to get in touch. Version: 0.0.0.9000. 1.1 Welcome Hi, I’m Rohan Alexander. You can find out more about me here. These are notes that I wrote to support my teaching at the University of Toronto across the Faculty of Information and the Department of Statistical Sciences. The focus is on using quantitative methods to tell stories with data. 1.2 Structure The parts of these notes are: Essentials Introduction Hello world R essentials Workflow Communicate Static communication Interactive communication Hunt and gather Gathering data Hunting data Other Clean Cleaning and preparing Storage and retrieval Dissemination and protection Model Exploratory data analysis Regression essentials Matching and difference-in-differences Instrumental variables Regression discontinuity design Poll of polls Multilevel regression with post-stratification Text Other Cloud Deploy 1.3 On telling stories Like many parents, when our child was born, one of the first things that my wife and I did regularly was read stories to him. In doing so we carried on a tradition that has occurred for millennia. Myths, fables, and fairy tales can be seen and heard all around us. Not only are they entertaining but they enable us to easily learn something about the world. While ‘The Very Hungry Caterpillar’ may seem quite far from the world of quantitative analysis, there are similarities. Both are trying to tell the reader a story. When conducting quantitative analysis we are trying to tell the reader a story that will convince them of something. It may be as exciting as predicting elections, as banal as increasing internet advertising click rates by 0.01 per cent, as serious as finding the cause of some disease, or as fun as forecasting the winner of a basketball game. In any case the key elements are the same. When writing fiction Wikipedia suggests there are five key elements: character, plot, setting, theme, and style. When we are conducting quantitative analysis we have analogous concerns: What is the data? Who generated it and how? What is the data trying to say? How can we let it say this? What is the broader context surrounding the data? Where and when was it generated? Could other data have been generated? What are we hoping others will see from this data? How can you convince them of this? In the past, certain elements of telling stories with quantitative data were easier. For instance, experimental design has a long and robust tradition within traditional applications such as agricultural and medical sciences, physics, and chemistry. Student’s t-distribution was identified by a chemist, William Sealy Gosset, who was working at Guinness and needed to assess the quality of the beer (Raju 2005)! It would have been possible for him to randomly sample the beer and change one aspect at a time. Indeed, many of the fundamental statistical methods that we use today were developed in an agricultural setting. In the settings for which they were developed it was typically possible to establish control groups, randomize, and easily deal with any ethical concerns. In such a setting any subsequent story that is told with the resulting data is likely to be fairly convincing. Unfortunately, such a set-up is rarely possible in modern applied statistics applications. On the other hand, there are many aspects that are easier today. For instance, we have well-developed statistical techniques, easier access to larger datasets, and open source statistical languages such as R. But the lack of ability to conduct traditional experiments means that we must turn to other aspects in order to tell a reader a convincing story about our data. These other aspects allow us to tell convincing stories even in the absence of a traditional experimental set-up. 1.4 Telling stories with data The aim of these notes is to equip you with everything you need to be able to write short(ish), technical, memos, that convince a reader of the story you are telling. These notes encourage research-based, independent learning. This means that you should develop your own questions and answer them to the extent that you can. We focus on methods that can provide convincing stories even when we cannot conduct traditional experiments. Importantly, these approaches do not rely on ‘big data’ – which is widely known by practitioners to not be a panacea (Meng and others 2018) – but instead on better using the data that are available. The purpose of the notes is to allow you to tell convincing stories using data and quantitative analysis. They blend theory and case studies to equip you to with practical skills, a sophisticated workflow, and an appreciation for how more-advanced methods build on what is covered here. Data science is multi-disciplinary. It takes the ‘best’ bits from fields such as statistics, data visualisation, programming, and experimental design (to name a few). As such, data science projects require a blend of these skills. These are hands-on notes in which you will learn these skills by conducting research projects using real-world data. This means that you will: obtain and clean relevant datasets; develop your own research questions; use statistical techniques to answer those questions; and communicate your results in a meaningful way. These notes were developed in collaboration with professional data scientists as well as academics from a variety of fields. They are designed around approaches that are used extensively in academia, government, and industry. Furthermore, they include many aspects, such as data cleaning and communication, that are critical, but rarely taught. However, these notes do not contain everything that you need. Your learning must be ‘active’ when using these notes because that is the way you will continue to learn through the rest of your life and career. You need to seek out additional information, critically evaluate it, and apply it to your situation. The workflow that we follow in these notes is: Research question development. Data collection. Data cleaning. Exploratory data analysis. Statistical modelling. Evaluation. Communication. Reproduce. All of these aspects are critical to being able to convince a reader of your story. Your ability to convince them of your story depends on the quality of all aspects of your workflow. If we were to expand on this workflow then we roughly get the chapters that are covered in these notes, although they are re-ordered as necessary. From the first chapter we will have a workflow (make a graph then write about it convincingly) that allows us to tell a convincing (albeit likely basic) story. In each subsequent chapter we add aspects and depth to our workflow that will allow us to speak with increasing sophistication and credibility. This workflow also aligns nicely with the skills that are sought in data scientists. For instance, Mango Solutions, a UK data science consultancy, describes ‘the six core capabilities of data scientists’ as: 1. communicator; 2. data-wrangler; 3. programmer; 4. technologist; 5. modeller; and 6. visualiser (“Data Science Radar: How to Identify World-Class Data Science Capabilities” 2020). These notes are also designed to enable you to build a portfolio of work that you could show to a potential employer. This is arguably the most important thing that you should be doing. (E. Robinson and Nolis 2020, 55) describe a portfolio as ‘a set of data science projects that you can show to people so they can see what kind of data science work you can do’. They describe this as a ‘step [that] can really help you be successful’. 1.4.1 Software The software that we use in these notes is R (R Core Team 2020). This language was chosen because it is open-source, widely used, general enough to cover the entire workflow, yet specific enough to have plenty of the tools that we need for statistical analysis built in. We do not assume that you have used R before, and so another reason for selecting R for these notes is the community of R users which is, in general, especially welcoming of new-comers and there are a lot of great beginner-friendly materials available. If you don’t have a programming language, then R is a great one to start with. If you have a preferred programming language already, then it wouldn’t hurt to pick up R as well. That said, if you have a good reason to prefer another open source programming language (for instance you use Python daily at work) then you may wish to stick with that. However, all examples in these notes are in R. Please download R and R Studio onto your own computer. You can download R for free here: http://cran.utstat.utoronto.ca/, and you can download R Studio Desktop for free here: https://rstudio.com/products/rstudio/download/#download. Please also create an account on R Studio Cloud: https://rstudio.cloud/. This will allow you to run R in the cloud, which will be helpful when we are getting started. 1.4.2 Assumed background These notes assume familiarity with first-year statistics. For instance, if you have a taken a course or two where you covered hypothesis testing and similar concepts then that should be enough. That said, enthusiasm and interest can take you pretty far, so if you’ve got those then don’t worry about too much else. 1.4.3 Structure These notes are structured around a fairly dense 12-week course. Each chapter contains a list of required reading, as well as a list of recommended reading for those who are interested in the topic and want a starting place for further exploration. All chapters contain a summary of the key concepts and skills that are developed in that chapter. Code and technical chapters additionally contain a list of the main packages and functions that are used in the chapter. Many of the chapters also have a pre-quiz. This is a short quiz that you should complete after doing the required readings, but before going through the chapter to test your knowledge. After completing the chapter, you should go back through the lists and the pre-quiz to make sure that you understand each aspect. There are problem sets throughout these notes. These are opportunities for you to conduct your own research on a topic that is of interest to you. Although the initial problem sets require you to use data from the Toronto Open Data Portal (https://open.toronto.ca/), after those first few you are able to use any appropriate dataset. Although open-ended research may be new to you, the extent to which you are able to develop your own questions, use quantitative methods to explore them, and communicate your story to a reader, is the true measure of the success of these notes. 1.5 Acknowledgements Many people gave generously of their time, code, and data to help develop these notes. Thank you to Monica Alexander, Michael Chong, and Sharla Gelfand for allowing their code to be used. Thank you to Kelly Lyons, Hareem Naveed, and Periklis Andritsos for helpful comments. Thank you to Greg Wilson for giving me a structure to think about teaching. These notes have greatly benefited from the notes and teaching materials of others that are freely available online, especially: Chris Bail - Text as Data; Andrew Heiss - Program Evaluation for Public Service; Grant McDermott - Data Science for Economists; David Mimno - Text Mining for History and Literature Ed Rubin - PhD Econometrics (III) and Introduction to Econometrics (II); Thank you to the following students who identified specific improvements in these notes: Aaron Miller, Amy Farrow, Cesar Villarreal Guzman, Faria Khandaker, Flavia López, Hong Shi, Laura Cline, Mounica Thanam, Wijdan Tariq, and Yang Wu. Finally, thank you to the Winter 2020 and 2021 INF2178 and Fall 2020 Term STA304 students at the University of Toronto, whose feedback greatly improved all aspects. 1.6 Contact Any comments or suggestions on these notes would be welcomed. You can contact me: rohan.alexander@utoronto.ca. References "],["drinking-from-a-fire-hose.html", "Chapter 2 Drinking from a fire hose 2.1 Hello world 2.2 Case study - Canadian elections 2.3 Case study - Toronto homelessness", " Chapter 2 Drinking from a fire hose Required reading Barrett, Malcolm, 2021, ‘Data science as an atomic habit’, 16 January, https://malco.io/2021/01/04/data-science-as-an-atomic-habit/. Gelfand, Sharla, 2019, ‘Cleaning up after the federal election’, https://sharla.party/talk/2019-10-24-uoft-brown-bag/. Hao, Karen, 2019, ’This is how AI bias really happens - and why it’s so hard to fix, MIT Technology Review, 4 February, https://www.technologyreview.com/s/612876/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix/. Keyes, Os, 2019, ‘Counting the Countless’, Real Life, 8 April, freely available at: https://reallifemag.com/counting-the-countless/. Required viewing Register, Yim, 2020, ‘Data Science Ethics in 6 Minutes’, YouTube, 29 December, https://youtu.be/mA4gypAiRYU. Key libraries ggplot2 tidyverse Key concepts/skills/etc R is fun and allows you to accomplish really interesting projects. But like any language it is a slow path to mastery. The way to learn is to start with a really small project in mind, and break down the steps required to achieve it. Look at other people’s code to work out how you might deal with the steps. Copy, paste, and modify someone else’s code to achieve each step. Don’t worry about perfection, just worry about achieving each step. Complete that project and move onto the next project. Rinse and repeat. Each project you’ll get a little better. Key functions %&gt;% ‘pipe’ dplyr::arrange() dplyr::filter() dplyr::group_by() dplyr::mutate() dplyr::select() dplyr::summarise() dplyr::ungroup() ggplot::facet_wrap() ggplot::geom_histogram() Quiz According to Register, 2020, data decisions affect (pick one)? Real people. No one. Those in the training set. Those in the test set. In your own words, what is data science? According to Keyes, 2019, what is perhaps a more accurate accurate definition of data science (pick one)? ‘The inhumane reduction of humanity down to what can be counted.’; ‘The quantitative analysis of large amounts of data for the purpose of decision-making.’; ‘Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data.’ Imagine that you have a job in which including ‘race’ as an explanatory variable improves the performance of your model. What types of issues would you consider when deciding whether to include this variable in production? What if the variable was sexuality? Please be sure to refer to Mullainathan, 2019, in your answer. 2.1 Hello world To jump in we will get some data from the wild, make a graph with it, and then use this to tell a story. Some of the code may be a bit unfamiliar to you if it’s your first-time using R. It’ll all soon be familiar. But the only way to learn how to code is to code. Please try to get this working on your own computer, typing out (not copy/pasting) all the code that you need. It’s important and normal to realise that you’re going to be bad at this for a while. Whenever you’re learning a new tool, for a long time, you’re going to suck… But the good news is that is typical; that’s something that happens to everyone, and it’s only temporary. Hadley Wickham as quoted by Barrett (2021). One of the great things about graphs is that sometimes this is all you need to have a convincing story, as Figure 2.1 from FT Visual &amp; Data Journalism team (2020) show. Figure 2.1: New confirmed cases of Covid-19 in United States, United Kingdom, Canada and Australia, as at 22 December 2020. In this section we are going to focus on making a table and a graph from our data. Although you will be guided thoroughly to achieve this, hopefully by seeing the power of quantitative analysis with R you will be motivated to stick with it when you run into difficulties later on. 2.2 Case study - Canadian elections Figure 2.2: The County Election (1851–52), by George Caleb Bingham (American, 1811 - 1879), as downloaded from https://artvee.com/dl/the-county-election. 2.2.1 Getting started To get started you should open a new R Markdown file (File -&gt; New File -&gt; R Markdown). As this is our first attempt at using R in the wild, we will just have everything in the one R Markdown document. (In later projects we will move to a more robust set-up.) Then you should create a new R code chunk (keyboard shortcut: Command + Option + I) and add some preamble documentation. I like to specify the purpose of the document, who the author is and their contact details, when the file was written or last updated, and pre-requisites that the file relies on. You may also like to include a license, and list outstanding issues or todos. Remember that in R, lines that start with ‘#’ are comments - they won’t run. #### Preamble #### # Purpose: Read in voting data from the 2019 Canadian Election and output a # dataset that can be used for analysis. # Author: Rohan Alexander # Email: rohan.alexander@utoronto.ca # Date: 9 January 2019 # Prerequisites: Need the text file from the Canadian elections website # Issues: # To do: After this I typically set-up my workspace. This usually involves installing and/or reading in any packages, and possibly updating them. Remember that you only need to install a package once for each computer. But you need to call it every time you want to use it. (Here I’ve added excessive comments so that you know what is going on and why - in general I wouldn’t explain what tidyverse is.) #### Workspace set-up #### install.packages(&quot;tidyverse&quot;) # Only need to do this once install.packages(&quot;janitor&quot;) # Only need to do this once install.packages(&quot;here&quot;) In this case we are going to use tidyverse Wickham (2017), janitor Firke (2020), and here Müller (2017a). #### Workspace set-up #### # tidyverse is a collection of packages # Try ?tidyverse to see more library(tidyverse) # Calls the tidyverse - need to do this each time. library(janitor) # janitor helps us clean datasets library(here) # here helps us to know where files are # update.packages() # You can uncomment this if you want to update your packages. 2.2.2 Get the data We read in the dataset from the Elections Canada website. We can actually pass a website to the read_tsv() function, which saves a lot of time. #### Read in the data #### # Read in the data using read_tsv from the readr package (part of the tidyverse) # The &#39;&lt;-&#39; is assigning the output of readr::read_tsv to a object called raw_data. raw_elections_data &lt;- readr::read_tsv(file = &quot;http://enr.elections.ca/DownloadResults.aspx&quot;, skip = 1) # There is some debris on the first line so we skip them. # We have read the data from the Elections Canada website. We may like to save # it just in case something happens and they move it. write_csv(raw_elections_data, here(&quot;inputs/data/canadian_voting.csv&quot;)) (Note that Elections Canada updates this link with the latest elections. When I run this on 31 December 2020, I get the results of a Toronto by-election. While I’ll certainly update these notes from time to time, it may be that there’s been an election between now and when you run these notes, so your specific results may be slightly different.) 2.2.3 Clean the data Now we’d like to clean the data so that we can use it. #### Basic cleaning #### raw_elections_data &lt;- read_csv(here(&quot;inputs/data/canadian_voting.csv&quot;)) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## `Electoral district number - Numéro de la circonscription` = col_character(), ## `Electoral district name` = col_character(), ## `Nom de la circonscription` = col_character(), ## `Type of results*` = col_character(), ## `Type de résultats**` = col_character(), ## `Surname - Nom de famille` = col_character(), ## `Middle name(s) - Autre(s) prénom(s)` = col_logical(), ## `Given name - Prénom` = col_character(), ## `Political affiliation` = col_character(), ## `Appartenance politique` = col_character(), ## `Votes obtained - Votes obtenus` = col_double(), ## `% Votes obtained - Votes obtenus %` = col_double(), ## `Rejected ballots - Bulletins rejetés***` = col_double(), ## `Total number of ballots cast - Nombre total de votes déposés` = col_double() ## ) # If you called the library (as we did) then you don&#39;t need to use this set-up # of janitor::clean_names, you could just use clean_names, but I&#39;m making it # explicit here, but won&#39;t in the future. cleaned_elections_data &lt;- janitor::clean_names(raw_elections_data) # One thing to notice for those who have a Stata background is that we just # overwrote the name - that&#39;s fine in R. # The pipe operator - %&gt;% - pushes the output from one line to be an input to the # next line. cleaned_elections_data &lt;- cleaned_elections_data %&gt;% # Filter to only have certain rows filter(type_of_results == &quot;validated&quot;) %&gt;% # Select only certain columns select(electoral_district_number_numero_de_la_circonscription, electoral_district_name, political_affiliation, surname_nom_de_famille, percent_votes_obtained_votes_obtenus_percent ) %&gt;% # Rename the columns to be a bit shorter rename(riding_number = electoral_district_number_numero_de_la_circonscription, riding = electoral_district_name, party = political_affiliation, surname = surname_nom_de_famille, votes = percent_votes_obtained_votes_obtenus_percent) head(cleaned_elections_data) ## # A tibble: 6 x 5 ## riding_number riding party surname votes ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 35108 Toronto Centre People&#39;s Party - PPC Bawa 1.1 ## 2 35108 Toronto Centre Free Party Canada Cappelletti 0.3 ## 3 35108 Toronto Centre NDP-New Democratic Party Chang 17 ## 4 35108 Toronto Centre Independent Clarke 0.5 ## 5 35108 Toronto Centre Liberal Ien 42 ## 6 35108 Toronto Centre Libertarian Komar 0.5 Finally we may like to save our cleaned dataset. #### Save #### readr::write_csv(cleaned_elections_data, &quot;outputs/data/cleaned_elections_data.csv&quot;) 2.2.4 Make a graph First we need to read in the dataset, we then filter the number of parties to a smaller number. #### Read in the data #### cleaned_elections_data &lt;- readr::read_csv(&quot;outputs/data/cleaned_elections_data.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## riding_number = col_double(), ## riding = col_character(), ## party = col_character(), ## surname = col_character(), ## votes = col_double() ## ) # Make a graph just considers Toronto riding cleaned_elections_data %&gt;% filter(party %in% c(&quot;Bloc Québécois&quot;, &quot;Conservative&quot;, &quot;Liberal&quot;, &quot;NDP-New Democratic Party&quot;) ) %&gt;% ggplot(aes(x = riding, y = votes, color = party)) + geom_point() + theme_minimal() + # Make the theme neater theme(axis.text.x = element_text(angle = 90, hjust = 1)) + # Change the angle labs(x = &quot;Riding&quot;, y = &quot;Votes (%)&quot;, color = &quot;Party&quot;) # Save the graph ggsave(&quot;outputs/figures/toronto_results.pdf&quot;, width = 40, height = 20, units = &quot;cm&quot;) 2.2.5 Make a table There are an awful lot of ways to make a table in R. First we’ll try the built-in function summary(). #### Read in the data #### cleaned_elections_data &lt;- readr::read_csv(&quot;outputs/data/cleaned_elections_data.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## riding_number = col_double(), ## riding = col_character(), ## party = col_character(), ## surname = col_character(), ## votes = col_double() ## ) #### Make some tables #### # Try some different default summary table summary(cleaned_elections_data) ## riding_number riding party surname ## Min. :35108 Length:15 Length:15 Length:15 ## 1st Qu.:35108 Class :character Class :character Class :character ## Median :35108 Mode :character Mode :character Mode :character ## Mean :35112 ## 3rd Qu.:35118 ## Max. :35118 ## votes ## Min. : 0.20 ## 1st Qu.: 0.55 ## Median : 3.60 ## Mean :13.34 ## 3rd Qu.:24.85 ## Max. :45.70 Now we can try a group_by() and summarise(). # Make our own cleaned_elections_data %&gt;% # Using group_by and summarise means that whatever summary statistics we # construct will be on a party basis. We could group_by multiple variables and # similarly, we could create a bunch of different other summary statistics. group_by(party) %&gt;% summarise(min = min(votes), mean = mean(votes), max = max(votes)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 9 x 4 ## party min mean max ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Conservative 5.7 23.8 41.8 ## 2 Free Party Canada 0.3 0.3 0.3 ## 3 Green Party 2.6 17.7 32.7 ## 4 Independent 0.5 0.55 0.6 ## 5 Liberal 42 43.8 45.7 ## 6 Libertarian 0.5 0.5 0.5 ## 7 NDP-New Democratic Party 5.8 11.4 17 ## 8 No Affiliation 0.2 0.2 0.2 ## 9 People&#39;s Party - PPC 1.1 2.35 3.6 2.3 Case study - Toronto homelessness Toronto has a large homeless population, and of course given the pandemic and winter it is critical that there are enough places in shelters. Unfortunately, as we will see in this case study, there are not enough places. However, the one good thing is that we have the data to see this is a problem. In this case study we are going to use data on the number of people in Toronto shelters to make a graph of usage. This will also introduce us to Tidy Tuesday! In my experience, people are most successful at ‘learning R’ when they are learning it to achieve something else. If you’re in a university course then that might be ‘pass the course’, but often it’s nice to have some other projects. TidyTuesday is a weekly event in which the R community comes together around a dataset and shares code and approaches. You can learn more about it here: https://github.com/rfordatascience/tidytuesday. 2.3.1 Getting started Again, open a new R Markdown file: (File -&gt; New File -&gt; R Markdown). Update the details so that they reflect your own. Again, add some top matter with some comments and explanations of the code. #### Preamble #### # Purpose: Read in Toronto homelessness data and output a graph. # Author: Rohan Alexander # Email: rohan.alexander@utoronto.ca # Date: 22 December 2020 # Prerequisites: # Issues: # To do: I want to talk a little about the libraries this time. Libraries are bits of code that other people have written. There are a few common ones that you’ll see regularly, especially the tidyverse. To use a package we first have to install it and then we need to load it. Jenny Bryan has a wonderful analogy of installing a lightbulb - install.packages(\"tidyverse\"). You only need to do this once, but then if you want light then you need to turn on the switch - library(tidyverse). So because we installed everything earlier we won’t need to do it again, we can just call the library. #### Workspace set-up #### library(tidyverse) Given that a lot of people gave up their time to make R and the packages, it’s important to cite them. Luckily, it’s easy to get the information that you need to properly cite them. # To get the citation for R run: citation() ## ## To cite R in publications use: ## ## R Core Team (2020). R: A language and environment for statistical ## computing. R Foundation for Statistical Computing, Vienna, Austria. ## URL https://www.R-project.org/. ## ## A BibTeX entry for LaTeX users is ## ## @Manual{, ## title = {R: A Language and Environment for Statistical Computing}, ## author = {{R Core Team}}, ## organization = {R Foundation for Statistical Computing}, ## address = {Vienna, Austria}, ## year = {2020}, ## url = {https://www.R-project.org/}, ## } ## ## We have invested a lot of time and effort in creating R, please cite it ## when using it for data analysis. See also &#39;citation(&quot;pkgname&quot;)&#39; for ## citing R packages. # And to get the citation for a package run that function with the package name. For instance: citation(&#39;tidyverse&#39;) ## ## Wickham et al., (2019). Welcome to the tidyverse. Journal of Open ## Source Software, 4(43), 1686, https://doi.org/10.21105/joss.01686 ## ## A BibTeX entry for LaTeX users is ## ## @Article{, ## title = {Welcome to the {tidyverse}}, ## author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D&#39;Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani}, ## year = {2019}, ## journal = {Journal of Open Source Software}, ## volume = {4}, ## number = {43}, ## pages = {1686}, ## doi = {10.21105/joss.01686}, ## } # Again, don&#39;t worry too much about the details - we&#39;ll get into them later. 2.3.2 Get the data We are going to grab some data that has been made available about Toronto homeless shelters. Again, don’t worry too much about the details for now, but what we are saying here, is that there’s a CSV that has been made available to us on GitHub and this code downloads it to our own computer. After we download it we can quickly look at the data using head(). toronto_shelters &lt;- readr::read_csv( &#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-12-01/shelters.csv&#39; ) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## id = col_double(), ## occupancy_date = col_datetime(format = &quot;&quot;), ## organization_name = col_character(), ## shelter_name = col_character(), ## shelter_address = col_character(), ## shelter_city = col_character(), ## shelter_province = col_character(), ## shelter_postal_code = col_character(), ## facility_name = col_character(), ## program_name = col_character(), ## sector = col_character(), ## occupancy = col_double(), ## capacity = col_double() ## ) head(toronto_shelters) ## # A tibble: 6 x 13 ## id occupancy_date organization_na… shelter_name shelter_address ## &lt;dbl&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 2017-01-01 00:00:00 COSTI Immigrant… COSTI Recep… 100 Lippincott… ## 2 2 2017-01-01 00:00:00 Christie Ossing… Christie Os… 973 Lansdowne … ## 3 3 2017-01-01 00:00:00 Christie Ossing… Christie Os… 973 Lansdowne … ## 4 4 2017-01-01 00:00:00 Christie Refuge… Christie Re… 43 Christie St… ## 5 5 2017-01-01 00:00:00 City of Toronto Birchmount … 1673 Kingston … ## 6 6 2017-01-01 00:00:00 City of Toronto Birkdale Re… 1229 Ellesmere… ## # … with 8 more variables: shelter_city &lt;chr&gt;, shelter_province &lt;chr&gt;, ## # shelter_postal_code &lt;chr&gt;, facility_name &lt;chr&gt;, program_name &lt;chr&gt;, ## # sector &lt;chr&gt;, occupancy &lt;dbl&gt;, capacity &lt;dbl&gt; 2.3.3 Make a graph The dataset is on a daily basis for each shelter. What I’d like to do is to get an overall picture of the availability of shelter places in Toronto. This code is based on code by Florence Vallée-Dubois1 and Lisa Lendway2. Now, I’d like to first do some data manipulation before we graph it this time. I’m going to introduce a few new functions here that we’ll see a lot more soon. Again, don’t worry if this doesn’t all make sense right now. You’re learning a brand-new language! Just try to focus on picking up a word or two and staying motivated! Finally, because it’s Christmas, we can grab a seasonally-appropriate theme from my colleague Liza Bolton. # In contrast to the earlier packages, which were located in a central repository # of packages called CRAN, Liza&#39;s is available on her GitHub. Again, don&#39;t worry # about the details for now, it&#39;ll all be clarified later. devtools::install_github(&quot;elb0/decemberLB&quot;, ref = &quot;main&quot;) ## Skipping install of &#39;decemberLB&#39; from a github remote, the SHA1 (2cc38a25) has not changed since last install. ## Use `force = TRUE` to force installation # Once it&#39;s installed we call the library as usual. library(decemberLB) toronto_shelters %&gt;% tidyr::drop_na(occupancy, capacity) %&gt;% # We only want rows that have data group_by(occupancy_date, sector) %&gt;% # We want to know the occupancy by date and sector summarise(the_sum = sum(occupancy), the_capacity = sum(capacity), the_usage = the_sum / the_capacity, .groups = &#39;drop&#39;) %&gt;% ggplot(aes(x = occupancy_date, y = the_usage, color = sector)) + geom_smooth(aes(group = sector), se = FALSE) + scale_y_continuous(limits = c(0, NA)) + labs(color = &quot;Type&quot;, x = &quot;Date&quot;, y = &quot;Occupancy rate&quot;, title = &quot;Toronto shelters&quot;, subtitle = &quot;Occupancy per day&quot;) + theme_minimal() + scale_color_december(palette = &quot;xmas&quot;) ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; References "],["r-essentials.html", "Chapter 3 R Essentials 3.1 R essentials 3.2 Social impact 3.3 R, R Studio, and R Studio Cloud 3.4 Tidyverse I 3.5 Base 3.6 ggplot I 3.7 Tidyverse II", " Chapter 3 R Essentials Required reading Bryan, Jennifer and Jim Hester, 2020, What they Forgot to Teach You About R, Chapters 1 to 5, https://rstats.wtf/debugging-r-code.html. Wickham, Hadley, and Garrett Grolemund, 2017, R for Data Science, Chapters 3 - 6, 8, 10, 11, 13, 14, 15, and 18, https://r4ds.had.co.nz/. Required viewing Kuriwaki, Shiro, 2020, ‘Defining Custom Functions in R’, Vimeo, 2 February, https://vimeo.com/388825332. Alternative reading There are a lot of great alternative ‘getting started with R’ type materials. Depending on your background and interests you may find some of the following useful: Arnold, Taylor, and Lauren Tilton, 2015, Humanities Data in R, Springer, Chapters 1 to 5. Hall, Megan, 2019, ‘An Introduction to R With Hockey Data’, https://hockey-graphs.com/2019/12/11/an-introduction-to-r-with-hockey-data/. Hanretty, Chris, 2020, ‘ConveRt’, slides http://chrishanretty.co.uk/conveRt/#1. Phillips, Nathaniel D., 2018, YaRrr! The Pirate’s Guide to R, Chapter 2, https://bookdown.org/ndphillips/YaRrr/started.html. Recommended reading Alexander, Monica, 2019, ‘The concentration and uniqueness of baby names in Australia and the US’, https://www.monicaalexander.com/posts/2019-20-01-babynames/. Hvitfeldt, Emil, 2020, ‘Emoji in ggplot2’, https://www.hvitfeldt.me/blog/real-emojis-in-ggplot2/. Pavlik, Kaylin, 2018, ‘Dairy Queen Deserts in Minnesota’, https://www.kaylinpavlik.com/dairy-queen-deserts/. ‘R Studio Cloud Guide’, https://rstudio.cloud/learn/guide. Scherer, Cédric, 2019, ‘Best TidyTuesday 2019’, https://cedricscherer.netlify.com/2019/12/30/best-tidytuesday-2019/. Silge, Julia, 2019, ‘Reordering and facetting for ggplot2’, https://juliasilge.com/blog/reorder-within/. Smale, David, 2019, ‘Happy Days’, https://davidsmale.netlify.com/portfolio/happy-days/. Key libraries ggplot2 tidyverse Key concepts/skills/etc Tibbles Importing data Joining data Strings Factors Dates Pivot Key functions class() dplyr::case_when() ggplot::facet_wrap() ggplot::geom_density() ggplot::geom_histogram() ggplot::geom_point() janitor::clean_names() skimr::skim() tidyr::pivot_longer() tidyr::pivot_wider() Quiz If I had a dataset with the following columns: name, age and wanted to focus on name, then which verb should I use (pick one)? tidyverse::select(). tidyverse::mutate(). tidyverse::filter(). tidyverse::rename(). If I want to cite R then how do I find a recommended citation (pick one)? cite('R'). cite(). citation('R'). citation(). What are three advantages of R? What are three disadvantages? What is R Studio? An integrated development environment (IDE) A closed source paid program A programming language created by Guido van Rossum A statistical programming language What is R? A open source statistical programming language A programming language created by Guido van Rossum A closed source statistical programming language An integrated development environment (IDE) Which of the following are not tidyverse verbs (pick one)? select(). filter(). arrange(). mutate(). visualize(). If I wanted to make a new column which verb should I use (pick one)? select(). filter(). arrange(). mutate(). visualize(). If I wanted to focus on particular rows which verb should I use (pick one)? select(). filter(). arrange(). mutate(). summarise() If I wanted a summary of the data that gave me the mean by sex, which two verbs should I use (pick one)? summarise(). filter(). arrange(). mutate(). group_by(). What are the three key aspects of the grammar of graphics (select all)? data. aesthetics. type. geom_histogram(). What is not one of the four challenges for mitigating bias mentioned in Hao 2019 (pick one)? Unknown unknowns. Imperfect processes. The definitions of fairness. Lack of social context. Disinterest given profit considerations. What would be the output of class('edward') (pick one)? “numeric”. “character”. “data.frame”. “vector”. How can I simulate 10,000 draws from a normal distribution with a mean of 27 and a standard deviation of 3 (pick one)? rnorm(10000, mean = 27, sd = 3). rnorm(27, mean = 10000, sd = 3). rnorm(3, mean = 10000, sd = 27). rnorm(27, mean = 3, sd = 1000). 3.1 R essentials This section is the basics of using R. Some of it may not make sense at first, but these are commands that we will come back to throughout these notes. You should initially just go through this chapter quickly, noting aspects that you don’t understand. Then start to play around with some of the initial case studies. Then maybe come back to this chapter. That way you will see how the various bits fit into context, and hopefully be more motivated to pick up various aspects. We will come back to everything in this chapter in more detail at some point in these notes. R is an open source language that is useful for statistical programming You can download R for free here: http://cran.utstat.utoronto.ca/, and you can download R Studio Desktop for free here: https://rstudio.com/products/rstudio/download/#download. When you are using R you will run into trouble at some point. To work through that trouble: Look at the help file for the function by putting ? before the function e.g. ?pivot_wider. Check the class of your data, by class(data_set$data_column). Check for typos. Google the error. Google what you are trying to do. Restart R (Session -&gt; Restart R and Clear Output). Try to make a small example and see if you have the same issues. Restart your computer. The past ten years or so of R have been characterised by the rise of the tidyverse. This is ‘… an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.’ Wickham (2020b). There are three distinctions here: the original R language, typically referred to as ‘base’; the ‘tidyverse’ which is a collection of packages that build on top of the R language; and other packages. Pretty much everything that you can do in the tidyverse, you can also do in base. However, as the tidyverse was built especially for modern data science it is usually easier to use the tidyverse, especially when you are setting out. Additionally, pretty much everything that you can do in the tidyverse, you can also do with other packages. However, as the tidyverse is a coherent collection of packages, it is often easier to use the tidyverse, especially when you are setting out. Eventually you will start to see cases where it makes sense to trade-off the convenience and coherence of the tidyverse for some features of base or other packages. Indeed you’ll see that at various points in these notes. For instance, the tidyverse can be slow, and so if you need to import thousands of CSVs then it can make sense to switch away from read_csv(). That is great and the appropriate use of base and non-tidyverse packages, rather than dogmatic insistence on a solution, is a sign of your development as an applied statistician. Get started by loading the tidyverse package. library(tidyverse) The general workflow that we will use involves: Import Tidy Transforming, descriptive Plot Model Repeat 3/4 People like Keyes have tried to tell us this for a long time, but COVID-19 make it very clear to everyone - most of the data that we use will have humans at the heart of it. It’s vitally important that you keep that in mind and grapple with it in everything that you do with R. It can be really easy to forget that almost every point in our dataset is likely a person. 3.2 Social impact “We shouldn’t have to think about the societal impact of our work because it’s hard and other people can do it for us” is a really bad argument. I stopped doing CV research because I saw the impact my work was having. I loved the work but the military applications and privacy concerns eventually became impossible to ignore. But basically all facial recognition work would not get published if we took Broader Impacts sections seriously. There is almost no upside and enormous downside risk. To be fair though i should have a lot of humility here. For most of grad school I bought in to the myth that science is apolitical and research is objectively moral and good no matter what the subject is. Joe Redmon, 20 February 2020. Although the term ‘data science’ is ubiquitous in academia, industry, and even more generally, it is difficult to define. One deliberately antagonistic definition of data science is ‘[t]he inhumane reduction of humanity down to what can be counted’ (Keyes 2019). While purposefully controversial, this definition highlights one reason for the increased demand for data science and quantitative methods over the past decade—individuals and their behaviour are now at the heart of it. Many of the techniques have been around for many decades, but what makes them popular now is this human focus. Unfortunately, even though much of the work may be focused on individuals, issues of privacy and consent, and ethical concerns more broadly, rarely seem front of mind. While there are some exceptions, in general, even at the same time as claiming that AI, machine learning, and data science are going to revolutionise society, consideration of these types of issues appears to have been largely treated as something that would be nice to have, rather than something that we may like to think of before we embrace the revolution. For the most part, these are not new issues. In the sciences, there has been considerable recent ethical consideration around CRISPR technology and gene editing, but in an earlier time similar conversations were had, for instance, about Wernher von Braun being allowed to building rockets for the US. In medicine, of course, these concerns have been front-of-mind for some time. Data science seems determined to have its own Tuskegee syphilis experiment moment rather than think about and deal appropriately with these issues, based on the experiences of other fields, before they occur. That said, there is some evidence that data scientists are beginning to be more concerned about the ethics surrounding the practice. For instance, NeurIPS, the most prestigious machine learning conference, now requires a statement on ethics to accompany all submissions. In order to provide a balanced perspective, authors are required to include a statement of the potential broader impact of their work, including its ethical aspects and future societal consequences. Authors should take care to discuss both positive and negative outcomes. NeurIPS call for papers, as accessed 26 February 2020. Ethical considerations will be mentioned throughout these notes rather than clumped in one easily ignorable part that can be thrown away after ‘ethics week’. The purpose is not to prescriptively rule things in or out, but to provide an opportunity to raise some issues that should be front of mind. The variety of data science applications, the relative youth of the field, and the speed of change, mean that ethical considerations can sometimes be set aside when it comes to data science. This is in contrast to fields such as science, medicine, engineering, and accounting where there is a long history. Nonetheless it can helpful to think through some ethical considerations that you may encounter in the content of a usual data science project. Figure 3.1: Probability, from https://xkcd.com/881/. 3.3 R, R Studio, and R Studio Cloud My colleague Liza Bolton has a lovely analogy here on the relationship between R and R Studio which I really like. R is like a car engine and R Studio is like the car. Although some of us can use a car engine directly, most of us use a car to interact with the engine. 3.3.1 R R - https://www.r-project.org/ - is an open source and free programming language that is focused on general statistics. (Free in this context doesn’t refer to a price of zero, but instead to ‘freedom’, but it also does have a price of zero). This is in contrast with a open source programming language that is designed for general purpose, such as Python, or an open source programming language that is focused on probability, such as Stan. It was created by Ross Ihaka and Robert Gentleman at the University of Auckland in New Zealand. It is maintained by the R Core Team and changes to this ‘base’ of code occur methodically and with concern given to a variety of different priorities. If you are in Canada then you can download R here: http://cran.utstat.utoronto.ca/, if you are in Australia then you can download R here: https://cran.csiro.au/, otherwise you should go here - https://cran.r-project.org/mirrors.html - and find a location that suits you. (It doesn’t really matter where you get it from, it’s just that it may be slightly faster to use a closer option.) Many people build on this stable base, to extend the capabilities of R to better and more quickly suit their needs. They do this by creating packages. Typically, although not always, a package is a collection R code, and this allows you to more easily do things that you want to do. These packages are managed by the Comprehensive R Archive Network (CRAN) - https://cran.r-project.org/, and other repositories. CRAN is built into the download of R that you just got, so you can use it straight away. If you want to use a package then you need to firstly install it in your computer, and then you need to load it when you want to use it. Di Cook, who is a Professor of Business Analytics at Monash University in Australia, describes this as analogous to a lightbulb: if you want light in your house, first you need to screw in the lightbulb, and you need to turn the switch on. You only need to screw in the lightbulb once per house, but you need to turn the switch on every time you want to use the light. To install a package on your computer (again, you’ll need to do this only once per computer) you use the code: install.packages(&quot;tidyverse&quot;) Then when you want to use a package, you need to call it with this code: library(tidyverse) You can open R and use it on your computer. It is primarily designed to be interacted with through the command line. This is how I had to start with R, and it’s fine, but it can be useful to have a richer environment than the command line provides. In particular, it can be useful to install an Integrated Development Environment (IDE), which is an application that brings together various bits and pieces that you’ll use all the time. The one that we will use is R Studio. 3.3.2 R Studio R Studio is distinct to R and they are different entities. R Studio builds on top of R to make it easier for you to use R. This is in the same way that you can use the internet from the command line, but most of us use a browser such as Chrome, Firefox, or Safari. R Studio is free in the sense that you don’t pay anything for it. It is also free in the sense of being able to take the code, modify it, and distribute that code provided others are similarly allowed to take your code and modify it and distribute, etc. However, it is important to recognise that R Studio is an entity and so it is possible that in the future the current situation could change. You can download R Studio here: https://rstudio.com/products/rstudio/download/#download. When you open R Studio it will look like Figure 3.2. Figure 3.2: Opening R Studio for the first time The left pane is a console in which you can type and execute R code line by line. Try it with 2+2 by clicking next to the prompt ‘&gt;’ and typing that out then pressing enter. The code that you type should be: 2 + 2 ## [1] 4 And hopefully you get the same answer printed in the console. The pane on the top right has information about your environment. For instance, when we create variables a list of their names and some properties will appear there. Try to type the following code, replacing my name with your name, next to the prompt, and again press enter: my_name &lt;- &quot;Rohan&quot; You should notice a new value in the environment pane with the variable name and its value. The pane in the bottom right is a file manager. At the moment it should just have two files - an R History file and a R Project file. We’ll get to what these are later, but for now we will create and save a file. Type out the following code (don’t worry too much about the details for now): saveRDS(object = my_name, file = &quot;my_first_file.rds&quot;) And you should see a new ‘.rds’ file in your list of files. 3.3.3 R Studio Cloud While you can download R Studio to your own computer, initially we will us R Studio Cloud, which is an online version that is provided by R Studio. We will use this so that you can focus on getting comfortable with R and R Studio in an environment that is consistent. This way you don’t have to worry about what computer you have or installation permissions while you are still getting used to the basics. The R Studio Cloud - https://rstudio.cloud/ - is as easy as it gets in terms of moving to the cloud. The trade-off is that it is not very powerful and it is sometimes slow, but for the purposes of the initial sections of these notes that will be fine. To get started, go to https://rstudio.cloud/ and create an account. If you are going to be a student for a while then it might be worthwhile using a university email account, because although they don’t yet charge for it, they will probably start charging soon, but with some luck they will offer education discounts. Once you have an account and log in, then it should look something like Figure 3.3. Figure 3.3: Opening R Studio Cloud for the first time (You’ll be in ‘Your Workspace’, and you won’t have a ‘Example Workspace’.) From here you should start a ‘New Project’. You can give the project a name by clicking on ‘Untitled Project’ and replacing it. We can now use R Studio in the cloud. While working line-by-line in the console is fine, it is easier to write out a whole script that can then be executed. We will do this by making an R Script. To do this go to: File -&gt; New File -&gt; R Script, or use the shortcut Command + Shift + N. The console pane will fall to the bottom left and an R Script will open in the top left. Let’s write some code that will grab all of the Australian politicians and then construct a small table about the genders of the prime ministers. (Some of this code won’t make sense at this stage, but just type it all out to get in the habit and then run it, by selecting all of the code and clicking ‘Run’ (or using the keyboard shortcut: Command + Return) # Install the packages that we&#39;ll need install.packages(&quot;devtools&quot;) install.packages(&quot;tidyverse&quot;) # Load the packages that we need to use this time library(devtools) library(tidyverse) # Grab the data on Australian politicians install_github(&quot;RohanAlexander/AustralianPoliticians&quot;) # Make a table of the counts of genders of the prime ministers AustralianPoliticians::all %&gt;% as_tibble() %&gt;% count(gender, wasPrimeMinister) ## # A tibble: 4 x 3 ## gender wasPrimeMinister n ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 female 1 1 ## 2 female NA 235 ## 3 male 1 29 ## 4 male NA 1511 You can save your R Script as ‘my_first_r_script.R’ using File -&gt; Save As (or the keyboard shortcut: Command + S). When you’re done your workspace should look something like Figure 3.4. Figure 3.4: After running an R Script One thing to be aware of is that each R Studio Cloud workspace is essentially a new computer. Because of this, you’ll need to install any package that you want to use for each workspace. For instance, before you can use the tidyverse, you need to install.packages(“tidyverse”). This is in contrast to when you use your own computer. A few final notes on R Studio Cloud for you to keep in the back of your mind: In the Australian politicians example we got our data from the website GitHub, but you can get data into your workspace from your local computer in a variety of ways. One way is to use the ‘upload’ button in the Files panel. R Studio Cloud allows some degree of collaboration. For instance, you can give someone else access to a workspace that you create. This could be useful for collaborating on an assignment, although it is not quite full featured yet and you cannot both be in the workspace at the same time (in contrast to, say, Google Docs). There are a variety of weaknesses of R Studio Cloud, in particular at the moment there is a 1GB limit on RAM. Additionally, it is still under-developed and things break from time to time. The R Studio Community page that is focused on R Studio Cloud can be helpful sometimes: https://community.rstudio.com/c/rstudio-cloud. 3.4 Tidyverse I Aspects of ‘Tidyverse I’ were written with Monica Alexander. One of the key packages that we use in these notes is the tidyverse Wickham, Averick, et al. (2019a). The tidyverse is actually a package of packages (i.e. when you install tidyverse, you are actually installing a whole bunch of different packages). The key package in the tidyverse in terms of manipulating data is dplyr Wickham, François, et al. (2020), and the key package in the tidyverse in terms of creating graphs is ggplot2 Wickham (2016). In this section we are going to cycle through some essentials from the Tidyverse. You’ll come back to the functions in this section regularly. I want to keep this section self-contained, so let’s start by installing the tidyverse (again, to use Di Cook’s analogy, this is the equivalent of screwing in the light-bulb). If you just did it, then you don’t need to do it again. install.packages(&quot;tidyverse&quot;) Now we can load the tidyverse (again, to use Di Cook’s analogy, the equivalent of turning on the light-switch). library(tidyverse) Here we are going to download the data about Australian politicians using the function read_csv(). australian_politicians &lt;- read_csv( file = &quot;https://raw.githubusercontent.com/RohanAlexander/telling_stories_with_data/master/inputs/data/australian_politicians.csv&quot; ) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_character(), ## birthDate = col_date(format = &quot;&quot;), ## birthYear = col_double(), ## deathDate = col_date(format = &quot;&quot;), ## member = col_double(), ## senator = col_double(), ## wasPrimeMinister = col_double() ## ) ## ℹ Use `spec()` for the full column specifications. We will now cover the pipe and six functions that are useful to know and that we will use all the time: select() filter() arrange() mutate() summarise()/summarize() group_by() 3.4.1 The pipe One key tidyverse helper is the ‘pipe’: %&gt;%. Read it as “and then” (keyboard shortcut: Command + Shift + M). This takes the output of a line of code and uses it as an input to the next line of code. You don’t have to use it, but it tends to make your code more readable. The idea of the pipe is that you take your dataset, and then, do something to it. In this case, we will look at the first few lines of our dataset by piping australian_politicians through to the head() function. australian_politicians %&gt;% head() ## # A tibble: 6 x 20 ## uniqueID surname allOtherNames firstName commonName displayName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Abbott1… Abbott Richard Hart… Richard &lt;NA&gt; Abbott, Ri… ## 2 Abbott1… Abbott Percy Phipps Percy &lt;NA&gt; Abbott, Pe… ## 3 Abbott1… Abbott Macartney Macartney Mac Abbott, Mac ## 4 Abbott1… Abbott Charles Lydi… Charles Aubrey Abbott, Au… ## 5 Abbott1… Abbott Joseph Palmer Joseph &lt;NA&gt; Abbott, Jo… ## 6 Abbott1… Abbott Anthony John Anthony Tony Abbott, To… ## # … with 14 more variables: earlierOrLaterNames &lt;chr&gt;, title &lt;chr&gt;, ## # gender &lt;chr&gt;, birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, birthPlace &lt;chr&gt;, ## # deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;, ## # wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;, adb &lt;chr&gt;, comments &lt;chr&gt; 3.4.2 Selecting The select() function is used to get a particular column of a dataset. For instance, we might like to select the first names column. australian_politicians %&gt;% select(firstName) %&gt;% head() ## # A tibble: 6 x 1 ## firstName ## &lt;chr&gt; ## 1 Richard ## 2 Percy ## 3 Macartney ## 4 Charles ## 5 Joseph ## 6 Anthony In R, there are many ways to do things. Another way to get a particular column of a dataset is to use the dollar sign. This is from base R (as opposed to select() which is from the tidyverse package). australian_politicians$firstName %&gt;% head() ## [1] &quot;Richard&quot; &quot;Percy&quot; &quot;Macartney&quot; &quot;Charles&quot; &quot;Joseph&quot; &quot;Anthony&quot; The two are almost equivalent and differ only in the class of what they return (we’ll talk more about class later in the notes). For the sake of completeness, if you combine select() with pull() then you will get the same class of output as if you use the dollar sign. australian_politicians %&gt;% select(firstName) %&gt;% pull() %&gt;% head() ## [1] &quot;Richard&quot; &quot;Percy&quot; &quot;Macartney&quot; &quot;Charles&quot; &quot;Joseph&quot; &quot;Anthony&quot; You can also use select to get rid of columns, by selecting in a negative sense. australian_politicians %&gt;% select(-firstName) ## # A tibble: 1,776 x 19 ## uniqueID surname allOtherNames commonName displayName earlierOrLaterN… title ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Abbott1… Abbott Richard Hart… &lt;NA&gt; Abbott, Ri… &lt;NA&gt; &lt;NA&gt; ## 2 Abbott1… Abbott Percy Phipps &lt;NA&gt; Abbott, Pe… &lt;NA&gt; &lt;NA&gt; ## 3 Abbott1… Abbott Macartney Mac Abbott, Mac &lt;NA&gt; &lt;NA&gt; ## 4 Abbott1… Abbott Charles Lydi… Aubrey Abbott, Au… &lt;NA&gt; &lt;NA&gt; ## 5 Abbott1… Abbott Joseph Palmer &lt;NA&gt; Abbott, Jo… &lt;NA&gt; &lt;NA&gt; ## 6 Abbott1… Abbott Anthony John Tony Abbott, To… &lt;NA&gt; &lt;NA&gt; ## 7 Abel1939 Abel John Arthur &lt;NA&gt; Abel, John &lt;NA&gt; &lt;NA&gt; ## 8 Abetz19… Abetz Eric &lt;NA&gt; Abetz, Eric &lt;NA&gt; &lt;NA&gt; ## 9 Adams19… Adams Judith Anne &lt;NA&gt; Adams, Jud… nee Bird &lt;NA&gt; ## 10 Adams19… Adams Dick Godfrey… &lt;NA&gt; Adams, Dick &lt;NA&gt; &lt;NA&gt; ## # … with 1,766 more rows, and 12 more variables: gender &lt;chr&gt;, ## # birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, birthPlace &lt;chr&gt;, deathDate &lt;date&gt;, ## # member &lt;dbl&gt;, senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;, wikidataID &lt;chr&gt;, ## # wikipedia &lt;chr&gt;, adb &lt;chr&gt;, comments &lt;chr&gt; Finally, you can select, based on conditions. For instance, selecting all all of the columns that start with something, for instance, ‘birth’. australian_politicians %&gt;% select(starts_with(&quot;birth&quot;)) ## # A tibble: 1,776 x 3 ## birthDate birthYear birthPlace ## &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 NA 1859 Bendigo ## 2 1869-05-14 1869 Hobart ## 3 1877-07-03 1877 Murrurundi ## 4 1886-01-04 1886 St Leonards ## 5 1891-10-18 1891 North Sydney ## 6 1957-11-04 1957 London ## 7 1939-06-25 1939 Sydney ## 8 1958-01-25 1958 Stuttgart ## 9 1943-04-11 1943 Picton ## 10 1951-04-29 1951 Launceston ## # … with 1,766 more rows 3.4.3 Filtering The filter() function is used to get particular rows from a dataset. For instance, we might like to filter to only politicians that became prime minister. australian_politicians %&gt;% filter(wasPrimeMinister == 1) ## # A tibble: 30 x 20 ## uniqueID surname allOtherNames firstName commonName displayName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Abbott1… Abbott Anthony John Anthony Tony Abbott, To… ## 2 Barton1… Barton Edmund Edmund &lt;NA&gt; Barton, Ed… ## 3 Bruce18… Bruce Stanley Melb… Stanley &lt;NA&gt; Bruce, Sta… ## 4 Chifley… Chifley Joseph Bened… Joseph Ben Chifley, B… ## 5 Cook1860 Cook Joseph Joseph &lt;NA&gt; Cook, Jose… ## 6 Curtin1… Curtin John Joseph … John &lt;NA&gt; Curtin, Jo… ## 7 Deakin1… Deakin Alfred Alfred &lt;NA&gt; Deakin, Al… ## 8 Fadden1… Fadden Arthur Willi… Arthur Arthur Fadden, Ar… ## 9 Fisher1… Fisher Andrew Andrew &lt;NA&gt; Fisher, An… ## 10 Forde18… Forde Francis Mich… Francis Frank Forde, Fra… ## # … with 20 more rows, and 14 more variables: earlierOrLaterNames &lt;chr&gt;, ## # title &lt;chr&gt;, gender &lt;chr&gt;, birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, ## # birthPlace &lt;chr&gt;, deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;, ## # wasPrimeMinister &lt;dbl&gt;, wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;, adb &lt;chr&gt;, ## # comments &lt;chr&gt; The filter() function also accepts two conditions. For instance, we can look at politicians who were prime minister and were named Joseph. australian_politicians %&gt;% filter(wasPrimeMinister == 1 &amp; firstName == &quot;Joseph&quot;) ## # A tibble: 3 x 20 ## uniqueID surname allOtherNames firstName commonName displayName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Chifley… Chifley Joseph Bened… Joseph Ben Chifley, B… ## 2 Cook1860 Cook Joseph Joseph &lt;NA&gt; Cook, Jose… ## 3 Lyons18… Lyons Joseph Aloys… Joseph &lt;NA&gt; Lyons, Jos… ## # … with 14 more variables: earlierOrLaterNames &lt;chr&gt;, title &lt;chr&gt;, ## # gender &lt;chr&gt;, birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, birthPlace &lt;chr&gt;, ## # deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;, ## # wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;, adb &lt;chr&gt;, comments &lt;chr&gt; We would get the same result if we use a comma instead of an ampersand. australian_politicians %&gt;% filter(wasPrimeMinister == 1, firstName == &quot;Joseph&quot;) ## # A tibble: 3 x 20 ## uniqueID surname allOtherNames firstName commonName displayName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Chifley… Chifley Joseph Bened… Joseph Ben Chifley, B… ## 2 Cook1860 Cook Joseph Joseph &lt;NA&gt; Cook, Jose… ## 3 Lyons18… Lyons Joseph Aloys… Joseph &lt;NA&gt; Lyons, Jos… ## # … with 14 more variables: earlierOrLaterNames &lt;chr&gt;, title &lt;chr&gt;, ## # gender &lt;chr&gt;, birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, birthPlace &lt;chr&gt;, ## # deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;, ## # wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;, adb &lt;chr&gt;, comments &lt;chr&gt; Similarly, we can look at politicians who were named Myles or Ruth. australian_politicians %&gt;% filter(firstName == &quot;Ruth&quot; | firstName == &quot;Myles&quot;) ## # A tibble: 3 x 20 ## uniqueID surname allOtherNames firstName commonName displayName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Coleman… Coleman Ruth Nancy Ruth &lt;NA&gt; Coleman, R… ## 2 Ferrick… Ferric… Myles Aloysi… Myles &lt;NA&gt; Ferricks, … ## 3 Webber1… Webber Ruth Stephan… Ruth &lt;NA&gt; Webber, Ru… ## # … with 14 more variables: earlierOrLaterNames &lt;chr&gt;, title &lt;chr&gt;, ## # gender &lt;chr&gt;, birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, birthPlace &lt;chr&gt;, ## # deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;, ## # wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;, adb &lt;chr&gt;, comments &lt;chr&gt; We can also pipe the results, for instance, pipe from the filter() to select() australian_politicians %&gt;% filter(firstName == &quot;Ruth&quot; | firstName == &quot;Myles&quot;) %&gt;% select(firstName, surname) ## # A tibble: 3 x 2 ## firstName surname ## &lt;chr&gt; &lt;chr&gt; ## 1 Ruth Coleman ## 2 Myles Ferricks ## 3 Ruth Webber Finally, we can filter() to a particular row number, for instance, in this case row 853. australian_politicians %&gt;% filter(row_number() == 853) ## # A tibble: 1 x 20 ## uniqueID surname allOtherNames firstName commonName displayName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Jarman1… Jarman Alan William Alan &lt;NA&gt; Jarman, Al… ## # … with 14 more variables: earlierOrLaterNames &lt;chr&gt;, title &lt;chr&gt;, ## # gender &lt;chr&gt;, birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, birthPlace &lt;chr&gt;, ## # deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;, ## # wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;, adb &lt;chr&gt;, comments &lt;chr&gt; But there is also a dedicated function to do this, which is slice() australian_politicians %&gt;% slice(853) ## # A tibble: 1 x 20 ## uniqueID surname allOtherNames firstName commonName displayName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Jarman1… Jarman Alan William Alan &lt;NA&gt; Jarman, Al… ## # … with 14 more variables: earlierOrLaterNames &lt;chr&gt;, title &lt;chr&gt;, ## # gender &lt;chr&gt;, birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, birthPlace &lt;chr&gt;, ## # deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;, ## # wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;, adb &lt;chr&gt;, comments &lt;chr&gt; 3.4.4 Arranging We can change the order of the dataset based on the values in a particular column using the arrange() function. For instance, we may like to arrange the data by year of birth. australian_politicians %&gt;% arrange(surname) ## # A tibble: 1,776 x 20 ## uniqueID surname allOtherNames firstName commonName displayName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Abbott1… Abbott Richard Hart… Richard &lt;NA&gt; Abbott, Ri… ## 2 Abbott1… Abbott Percy Phipps Percy &lt;NA&gt; Abbott, Pe… ## 3 Abbott1… Abbott Macartney Macartney Mac Abbott, Mac ## 4 Abbott1… Abbott Charles Lydi… Charles Aubrey Abbott, Au… ## 5 Abbott1… Abbott Joseph Palmer Joseph &lt;NA&gt; Abbott, Jo… ## 6 Abbott1… Abbott Anthony John Anthony Tony Abbott, To… ## 7 Abel1939 Abel John Arthur John &lt;NA&gt; Abel, John ## 8 Abetz19… Abetz Eric Eric &lt;NA&gt; Abetz, Eric ## 9 Adams19… Adams Judith Anne Judith &lt;NA&gt; Adams, Jud… ## 10 Adams19… Adams Dick Godfrey… Dick &lt;NA&gt; Adams, Dick ## # … with 1,766 more rows, and 14 more variables: earlierOrLaterNames &lt;chr&gt;, ## # title &lt;chr&gt;, gender &lt;chr&gt;, birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, ## # birthPlace &lt;chr&gt;, deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;, ## # wasPrimeMinister &lt;dbl&gt;, wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;, adb &lt;chr&gt;, ## # comments &lt;chr&gt; We can also use the desc() function to arrange in descending order. australian_politicians %&gt;% arrange(desc(surname)) ## # A tibble: 1,776 x 20 ## uniqueID surname allOtherNames firstName commonName displayName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Zimmerm… Zimmer… Trent Moir Trent &lt;NA&gt; Zimmerman,… ## 2 Zeal1830 Zeal William Aust… William &lt;NA&gt; Zeal, Will… ## 3 Zappia1… Zappia Antonio Antonio Tony Zappia, To… ## 4 Zammit1… Zammit Paul John Paul &lt;NA&gt; Zammit, Pa… ## 5 Zakharo… Zakhar… Alice Olive Alice Olive Zakharov, … ## 6 Zahra19… Zahra Christian Jo… Christian &lt;NA&gt; Zahra, Chr… ## 7 Young19… Young Harold Willi… Harold &lt;NA&gt; Young, Har… ## 8 Young19… Young Michael Jero… Michael Mick Young, Mick ## 9 Young19… Young Terry James Terry &lt;NA&gt; Young, Ter… ## 10 Yates18… Yates George Edwin George Gunner Yates, Gun… ## # … with 1,766 more rows, and 14 more variables: earlierOrLaterNames &lt;chr&gt;, ## # title &lt;chr&gt;, gender &lt;chr&gt;, birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, ## # birthPlace &lt;chr&gt;, deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;, ## # wasPrimeMinister &lt;dbl&gt;, wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;, adb &lt;chr&gt;, ## # comments &lt;chr&gt; We can also arrange based on more than one column. australian_politicians %&gt;% arrange(firstName, surname) ## # A tibble: 1,776 x 20 ## uniqueID surname allOtherNames firstName commonName displayName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Blain18… Blain Adair Macali… Adair &lt;NA&gt; Blain, Ada… ## 2 Armstro… Armstr… Adam Alexand… Adam Bill Armstrong,… ## 3 Bandt19… Bandt Adam Paul Adam &lt;NA&gt; Bandt, Adam ## 4 Dein1889 Dein Adam Kemball Adam Dick Dein, Dick ## 5 Ridgewa… Ridgew… Aden Derek Aden &lt;NA&gt; Ridgeway, … ## 6 Bennett… Bennett Adrian Frank Adrian &lt;NA&gt; Bennett, A… ## 7 Gibson1… Gibson Adrian Adrian &lt;NA&gt; Gibson, Ad… ## 8 Wynne18… Wynne Agar Agar &lt;NA&gt; Wynne, Agar ## 9 Roberts… Robert… Agnes Robert… Agnes &lt;NA&gt; Robertson,… ## 10 Bird1906 Bird Alan Charles Alan &lt;NA&gt; Bird, Alan ## # … with 1,766 more rows, and 14 more variables: earlierOrLaterNames &lt;chr&gt;, ## # title &lt;chr&gt;, gender &lt;chr&gt;, birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, ## # birthPlace &lt;chr&gt;, deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;, ## # wasPrimeMinister &lt;dbl&gt;, wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;, adb &lt;chr&gt;, ## # comments &lt;chr&gt; We can pipe arrange() to another arrange(). australian_politicians %&gt;% arrange(firstName) %&gt;% arrange(surname) ## # A tibble: 1,776 x 20 ## uniqueID surname allOtherNames firstName commonName displayName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Abbott1… Abbott Anthony John Anthony Tony Abbott, To… ## 2 Abbott1… Abbott Charles Lydi… Charles Aubrey Abbott, Au… ## 3 Abbott1… Abbott Joseph Palmer Joseph &lt;NA&gt; Abbott, Jo… ## 4 Abbott1… Abbott Macartney Macartney Mac Abbott, Mac ## 5 Abbott1… Abbott Percy Phipps Percy &lt;NA&gt; Abbott, Pe… ## 6 Abbott1… Abbott Richard Hart… Richard &lt;NA&gt; Abbott, Ri… ## 7 Abel1939 Abel John Arthur John &lt;NA&gt; Abel, John ## 8 Abetz19… Abetz Eric Eric &lt;NA&gt; Abetz, Eric ## 9 Adams19… Adams Dick Godfrey… Dick &lt;NA&gt; Adams, Dick ## 10 Adams19… Adams Judith Anne Judith &lt;NA&gt; Adams, Jud… ## # … with 1,766 more rows, and 14 more variables: earlierOrLaterNames &lt;chr&gt;, ## # title &lt;chr&gt;, gender &lt;chr&gt;, birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, ## # birthPlace &lt;chr&gt;, deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;, ## # wasPrimeMinister &lt;dbl&gt;, wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;, adb &lt;chr&gt;, ## # comments &lt;chr&gt; It is just important to be clear about the precedence of each. australian_politicians %&gt;% arrange(surname, firstName) ## # A tibble: 1,776 x 20 ## uniqueID surname allOtherNames firstName commonName displayName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Abbott1… Abbott Anthony John Anthony Tony Abbott, To… ## 2 Abbott1… Abbott Charles Lydi… Charles Aubrey Abbott, Au… ## 3 Abbott1… Abbott Joseph Palmer Joseph &lt;NA&gt; Abbott, Jo… ## 4 Abbott1… Abbott Macartney Macartney Mac Abbott, Mac ## 5 Abbott1… Abbott Percy Phipps Percy &lt;NA&gt; Abbott, Pe… ## 6 Abbott1… Abbott Richard Hart… Richard &lt;NA&gt; Abbott, Ri… ## 7 Abel1939 Abel John Arthur John &lt;NA&gt; Abel, John ## 8 Abetz19… Abetz Eric Eric &lt;NA&gt; Abetz, Eric ## 9 Adams19… Adams Dick Godfrey… Dick &lt;NA&gt; Adams, Dick ## 10 Adams19… Adams Judith Anne Judith &lt;NA&gt; Adams, Jud… ## # … with 1,766 more rows, and 14 more variables: earlierOrLaterNames &lt;chr&gt;, ## # title &lt;chr&gt;, gender &lt;chr&gt;, birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, ## # birthPlace &lt;chr&gt;, deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;, ## # wasPrimeMinister &lt;dbl&gt;, wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;, adb &lt;chr&gt;, ## # comments &lt;chr&gt; 3.4.5 Grouping We can group variables using the function group_by() and then apply some other function within those groups. For instance, we could arrange by first name within gender, and then get the first three results. australian_politicians %&gt;% group_by(gender) %&gt;% arrange(firstName) %&gt;% slice(1:3) ## # A tibble: 6 x 20 ## # Groups: gender [2] ## uniqueID surname allOtherNames firstName commonName displayName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Roberts… Robert… Agnes Robert… Agnes &lt;NA&gt; Robertson,… ## 2 MacTier… MacTie… Alannah Joan… Alannah &lt;NA&gt; MacTiernan… ## 3 Zakharo… Zakhar… Alice Olive Alice Olive Zakharov, … ## 4 Blain18… Blain Adair Macali… Adair &lt;NA&gt; Blain, Ada… ## 5 Armstro… Armstr… Adam Alexand… Adam Bill Armstrong,… ## 6 Bandt19… Bandt Adam Paul Adam &lt;NA&gt; Bandt, Adam ## # … with 14 more variables: earlierOrLaterNames &lt;chr&gt;, title &lt;chr&gt;, ## # gender &lt;chr&gt;, birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, birthPlace &lt;chr&gt;, ## # deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;, ## # wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;, adb &lt;chr&gt;, comments &lt;chr&gt; 3.4.6 Mutating The mutate() function is used to make a new column. For instance, perhaps we want to make a new column that is 1 if a person was a member and a senator and 0 otherwise. australian_politicians &lt;- australian_politicians %&gt;% mutate(was_both = if_else(member == 1 &amp; senator == 1, 1, 0)) australian_politicians %&gt;% select(member, senator, was_both) %&gt;% head() ## # A tibble: 6 x 3 ## member senator was_both ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0 ## 2 1 1 1 ## 3 0 1 0 ## 4 1 0 0 ## 5 1 0 0 ## 6 1 0 0 3.4.7 Summarise The function summarise() is used to create new summary variables. For instance, looking at the maximum of birth year to find who the most recently born politicians are. australian_politicians %&gt;% summarise(youngest_politicians_birth_year = max(birthYear, na.rm = TRUE)) ## # A tibble: 1 x 1 ## youngest_politicians_birth_year ## &lt;dbl&gt; ## 1 1994 And we can check that using arrange(). australian_politicians %&gt;% arrange(-birthYear) %&gt;% select(uniqueID, surname, allOtherNames, birthYear) %&gt;% slice(1:3) ## # A tibble: 3 x 4 ## uniqueID surname allOtherNames birthYear ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 SteeleJohn1994 Steele-John Jordon Alexander 1994 ## 2 Chandler1990 Chandler Claire 1990 ## 3 Roy1990 Roy Wyatt Beau 1990 The summarise() function is particularly powerful in conjunction with group_by(). For instance, let’s look at the year of birth of the youngest by gender. australian_politicians %&gt;% group_by(gender) %&gt;% summarise(youngest_politician_birth_year = max(birthYear, na.rm = TRUE)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## gender youngest_politician_birth_year ## &lt;chr&gt; &lt;dbl&gt; ## 1 female 1990 ## 2 male 1994 Let’s look at mean of age at death by gender. australian_politicians %&gt;% mutate(days_lived = deathDate - birthDate) %&gt;% filter(!is.na(days_lived)) %&gt;% group_by(gender) %&gt;% summarise(mean_days_lived = round(mean(days_lived), 2)) %&gt;% arrange(-mean_days_lived) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## gender mean_days_lived ## &lt;chr&gt; &lt;drtn&gt; ## 1 female 28857.30 days ## 2 male 27372.89 days We can use group_by() for more than one group for instance, looking again at average number of days lived by gender and by which house. australian_politicians %&gt;% mutate(days_lived = deathDate - birthDate) %&gt;% filter(!is.na(days_lived)) %&gt;% group_by(gender, wasPrimeMinister) %&gt;% summarise(mean_days_lived = round(mean(days_lived), 2)) %&gt;% arrange(-mean_days_lived) ## `summarise()` regrouping output by &#39;gender&#39; (override with `.groups` argument) ## # A tibble: 3 x 3 ## # Groups: gender [2] ## gender wasPrimeMinister mean_days_lived ## &lt;chr&gt; &lt;dbl&gt; &lt;drtn&gt; ## 1 female NA 28857.30 days ## 2 male 1 28446.61 days ## 3 male NA 27345.20 days 3.4.8 Counting We can use the function count() to create counts by groups. For instance, the number of politicians by gender. australian_politicians %&gt;% group_by(gender) %&gt;% count() ## # A tibble: 2 x 2 ## # Groups: gender [2] ## gender n ## &lt;chr&gt; &lt;int&gt; ## 1 female 236 ## 2 male 1540 3.4.9 Proportions Finally, often calculating proportions is a combination of summarise() and mutate() (and group_by()). Let’s calculate the proportion of genders. Note here, that we needed to ungroup() the data before mutating. australian_politicians %&gt;% group_by(gender) %&gt;% count() %&gt;% ungroup() %&gt;% mutate(prop = n/(sum(n))) ## # A tibble: 2 x 3 ## gender n prop ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 female 236 0.133 ## 2 male 1540 0.867 3.5 Base 3.5.1 Class A class is the broader type of object that something is. For instance, your class is probably ‘human’, which is itself a ‘animal’. Similarly, if we create a number in R we can use class() to work out its class, which in this case will be numeric. my_number &lt;- 8 class(my_number) ## [1] &quot;numeric&quot; Or we could make it a character. my_name &lt;- &quot;rohan&quot; class(my_name) ## [1] &quot;character&quot; Finally, we can often coerce classes to be something else. my_number_as_character &lt;- as.character(my_number) class(my_number_as_character) ## [1] &quot;character&quot; There are many ways for your code to not run, but having an issue with the classes is the almost always the first thing to check. 3.5.2 Simulating data Simulating data is a key skill for statistics. We will use the following functions all the time: rnorm(), sample(), and runif(). Arguably the most important function is set.seed(), which we need because while we want our data to be random, we want it to be repeatable. Let’s get 10 observations from the standard normal. set.seed(853) number_of_observations &lt;- 10 simulated_data &lt;- tibble(person = c(1:number_of_observations), observation = rnorm(number_of_observations, mean = 0, sd = 1) ) Then let’s add 10 draws from the uniform distribution between 0 and 10. simulated_data$another_observation &lt;- runif(number_of_observations, min = 0, max = 10) Finally, let’s use sample, which allows use to pick from a list of items, to add a favourite colour to each observation. simulated_data$fav_colour &lt;- sample(x = c(&quot;blue&quot;, &quot; white &quot;), size = number_of_observations, replace = TRUE) We set the option replace to TRUE because we are only choosing between two items, but we want ten outcomes. Depending on the simulation you should think about whether you need it TRUE or FALSE. Also, there is another useful option to adjust the probability with which each item is drawn. In particular, the default is that both options are equally likely, but perhaps we might like to have 10 per cent blue with 90 per cent white. The way to do this is to set the option prob. As always with functions, you can find more in the help with ?sample. 3.5.3 Functions There are a lot of functions in R, and almost any common task that you might need to do is likely already done. But you will need to write your own functions. The way to do this is to define a function and give it a name. Your function will probably have some inputs (note that these inputs can have default values). Your function will then do something with these inputs and then return something. my_function &lt;- function(some_names) { print(some_names) } my_function(c(&quot;rohan&quot;, &quot;monica&quot;)) ## [1] &quot;rohan&quot; &quot;monica&quot; 3.6 ggplot I The ggplot package is the plotting package that is part of the tidyverse collection of packages. In a similar way to piping, it works in layers. But instead of using the pipe (%&gt;%) ggplot uses +. 3.6.1 Main features There are three key aspects: data; aesthetics / mapping; and type. For instances, let’s build up a histogram of age of death with increasing complexity. Starts with a grey box: australian_politicians %&gt;% mutate(days_lived = as.integer(deathDate - birthDate)) %&gt;% filter(!is.na(days_lived)) %&gt;% ggplot(mapping = aes(x = days_lived)) We need to tell it what we want to plot. This is where geom comes in australian_politicians %&gt;% mutate(days_lived = as.integer(deathDate - birthDate)) %&gt;% filter(!is.na(days_lived)) %&gt;% ggplot(mapping = aes(x = days_lived)) + geom_histogram(binwidth = 365) Now let’s color the bars by gender, which means adding an aesthetic. australian_politicians %&gt;% mutate(days_lived = as.integer(deathDate - birthDate)) %&gt;% filter(!is.na(days_lived)) %&gt;% ggplot(mapping = aes(x = days_lived, fill = gender)) + geom_histogram(binwidth = 365) We can add some labels, change the color, and background. australian_politicians %&gt;% mutate(days_lived = as.integer(deathDate - birthDate)) %&gt;% filter(!is.na(days_lived)) %&gt;% ggplot(mapping = aes(x = days_lived, fill = gender)) + geom_histogram(binwidth = 365) + labs(title = &quot;Length of life of Australian politicians&quot;, x = &quot;Age of deaths (days)&quot;, y = &quot;Number&quot;) + theme_classic() + scale_fill_brewer(palette = &quot;Set1&quot;) I forget who said this but, ‘ggplot makes it so easy to have nicely labelled axes, there’s no real excuse not to’. 3.6.2 Facets Facets are subplots and are invaluable because they allow you to add another variable to your plot without having to make a 3D plot. australian_politicians %&gt;% mutate(days_lived = as.integer(deathDate - birthDate)) %&gt;% filter(!is.na(days_lived)) %&gt;% ggplot(mapping = aes(x = days_lived)) + geom_histogram(binwidth = 365) + labs(title = &quot;Length of life of Australian politicians&quot;, x = &quot;Age of deaths (days)&quot;, y = &quot;Number&quot;) + theme_classic() + scale_fill_brewer(palette = &quot;Set1&quot;) + facet_wrap(~gender) 3.7 Tidyverse II 3.7.1 Tibbles A tibble is a data frame, but it is a data frame with some particular changes that make it easier to work with. You should read Chapter 10 of Wickham and Grolemund (2017) for more detail. The main difference is that compared with a dataframe, a tibble doesn’t convert strings to factors, and it prints nicely, including letting you know the class of a column. You can make a tibble manually if you need, for instance this can be handy for simulating data, but usually we will just import data as a tibble. people &lt;- tibble(names = c(&quot;rohan&quot;, &quot;monica&quot;), website = c(&quot;rohanalexander.com&quot;, &quot;monicaalexander.com&quot;), fav_colour = c(&quot;blue&quot;, &quot; white &quot;), ) people ## # A tibble: 2 x 3 ## names website fav_colour ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 rohan rohanalexander.com &quot;blue&quot; ## 2 monica monicaalexander.com &quot; white &quot; 3.7.2 Importing data There are a variety of ways to import data. If you are dealing with CSV files then try read_csv() in the first instance. There were examples of that in earlier sections. 3.7.3 Joining data We can join two datasets together in a variety of ways. The most common join that I use is left_join(), where I have one main dataset and I want to join another to it based on some common column names. Here we’ll join two datasets based on favourite colour. both &lt;- simulated_data %&gt;% left_join(people, by = &quot;fav_colour&quot;) both ## # A tibble: 10 x 6 ## person observation another_observation fav_colour names website ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 -0.360 9.52 &quot;blue&quot; rohan rohanalexander.com ## 2 2 -0.0406 0.586 &quot; white &quot; monica monicaalexander.com ## 3 3 -1.78 2.48 &quot;blue&quot; rohan rohanalexander.com ## 4 4 -1.12 5.80 &quot; white &quot; monica monicaalexander.com ## 5 5 -1.00 5.26 &quot;blue&quot; rohan rohanalexander.com ## 6 6 1.78 4.09 &quot;blue&quot; rohan rohanalexander.com ## 7 7 -1.39 3.97 &quot;blue&quot; rohan rohanalexander.com ## 8 8 -0.497 2.52 &quot; white &quot; monica monicaalexander.com ## 9 9 -0.558 6.29 &quot;blue&quot; rohan rohanalexander.com ## 10 10 -0.824 8.57 &quot;blue&quot; rohan rohanalexander.com 3.7.4 Strings We’ve seen a string earlier, but it is an object that is created with single or double quotes. String manipulation is an entire book in itself, but you should start with the stringr package (Wickham 2019c). I’ll just cover a few essentials: stringr::str_detect(), stringr::str_replace(), stringr::str_squish(). head(people) ## # A tibble: 2 x 3 ## names website fav_colour ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 rohan rohanalexander.com &quot;blue&quot; ## 2 monica monicaalexander.com &quot; white &quot; people &lt;- people %&gt;% mutate(is_rohan = stringr::str_detect(names, &quot;rohan&quot;), make_howlett = stringr::str_replace(website, &quot;alexander&quot;, &quot;howlett&quot;), fav_colour_trim = stringr::str_squish(fav_colour) ) head(people) ## # A tibble: 2 x 6 ## names website fav_colour is_rohan make_howlett fav_colour_trim ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 rohan rohanalexander.com &quot;blue&quot; TRUE rohanhowlett.com blue ## 2 monica monicaalexander.c… &quot; white &quot; FALSE monicahowlett.c… white 3.7.5 Pivot Datasets tend to be either long or wide. Generally, in the tidyverse, and certainly for ggplot, we need long data. To go from one to the other you can use the pivot_longer() and pivot_wider() functions. Let’s see an example with some data on whether red team or blue team won a competition in some year. pivot_example_data &lt;- tibble(year = c(2019, 2020, 2021), blue_team = c(1, 2, 1), red_team = c(2, 1, 2)) head(pivot_example_data) ## # A tibble: 3 x 3 ## year blue_team red_team ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2019 1 2 ## 2 2020 2 1 ## 3 2021 1 2 This dataset is in wide format at the moment. To get it into long format, what we’d like is to have a column that specifies the team, and then another that specifies the result. We’ll use tidyr::pivot_longer. data_pivoted_longer &lt;- pivot_example_data %&gt;% tidyr::pivot_longer(cols = c(&quot;blue_team&quot;, &quot;red_team&quot;), names_to = &quot;team&quot;, values_to = &quot;position&quot;) head(data_pivoted_longer) ## # A tibble: 6 x 3 ## year team position ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2019 blue_team 1 ## 2 2019 red_team 2 ## 3 2020 blue_team 2 ## 4 2020 red_team 1 ## 5 2021 blue_team 1 ## 6 2021 red_team 2 Occasionally, you’ll need to go from long data to wide data. We accomplish this with tidyr::pivot_wider. data_pivoted_wider &lt;- data_pivoted_longer %&gt;% tidyr::pivot_wider(id_cols = c(&quot;year&quot;, &quot;team&quot;), names_from = &quot;team&quot;, values_from = &quot;position&quot;) head(data_pivoted_wider) ## # A tibble: 3 x 3 ## year blue_team red_team ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2019 1 2 ## 2 2020 2 1 ## 3 2021 1 2 3.7.6 Factors A factor is a string that has an inherent ordering. For instance, the days of the week have an order - Monday, Tuesday, Wednesday,… - which is not alphabetical. Factors feature prominently in base, but they often add more complication than they are worth and so the tidyverse gives them a less prominent role. Nonetheless taking advantage of factors is useful in certain circumstances, for instance when plotting the days of the week we probably want them in the usual ordering than in the alphabetical ordering that would result if we had them as characters. The package that we use to deal with factors is forcats (Wickham 2020a). Sometimes you will have a character vector and you will want it ordered in a particular way. The default is that a character vector is ordered alphabetically, but you may not want that, for instance, the days of the week would look strange on a graph if they were alphabetically ordered: Friday, Monday, Saturday, Sunday, Thursday, Tuesday, Wednesday! The way to change the ordering is to change the variable from a character to a factor. I would then use the forcats package to specify an ordering by hand. The help page is here: https://forcats.tidyverse.org/reference/fct_relevel.html. Let’s look at a concrete example. my_data &lt;- tibble(all_names = c(&quot;Rohan&quot;, &quot;Monica&quot;, &quot;Edward&quot;)) If we plotted this then Edward would be first, because it would be alphabetical. But if instead I want to be first as I am the oldest then we could use forcats in the following way. library(forcats) # (BTW you&#39;ll probably have to install that one) library(tidyverse) my_data &lt;- my_data %&gt;% mutate(all_names = factor(all_names), # Change to factor all_names_releveled = fct_relevel(all_names, &quot;Rohan&quot;, &quot;Monica&quot;)) # Change the levels # Then compare the two my_data$all_names ## [1] Rohan Monica Edward ## Levels: Edward Monica Rohan my_data$all_names_releveled ## [1] Rohan Monica Edward ## Levels: Rohan Monica Edward 3.7.7 Cases If you need to write a few conditional statements then case_when is the way to go. Let’s start with a tibble of dates and pretend that we want to group them into ‘pre-1950’, ‘1950-2000’, ‘2000-onwards’ case_when_example &lt;- tibble(some_dates = c(&quot;1909-12-31&quot;, &quot;1919-12-31&quot;, &quot;1929-12-31&quot;, &quot;1939-12-31&quot;, &quot;1949-12-31&quot;, &quot;1959-12-31&quot;, &quot;1969-12-31&quot;, &quot;1979-12-31&quot;, &quot;1989-12-31&quot;, &quot;1999-12-31&quot;, &quot;2009-12-31&quot;) ) case_when_example &lt;- case_when_example %&gt;% mutate(some_dates = lubridate::ymd(some_dates) ) head(case_when_example) ## # A tibble: 6 x 1 ## some_dates ## &lt;date&gt; ## 1 1909-12-31 ## 2 1919-12-31 ## 3 1929-12-31 ## 4 1939-12-31 ## 5 1949-12-31 ## 6 1959-12-31 Now we’ll use dplyr::case_when() to group these. case_when_example &lt;- case_when_example %&gt;% mutate(year_group = case_when( some_dates &lt; lubridate::ymd(&quot;1950-01-01&quot;) ~ &quot;pre-1950&quot;, some_dates &lt; lubridate::ymd(&quot;2000-01-01&quot;) ~ &quot;1950-2000&quot;, some_dates &gt;= lubridate::ymd(&quot;2000-01-01&quot;) ~ &quot;2000-onwards&quot;, TRUE ~ &quot;CHECK ME&quot; ) ) head(case_when_example) ## # A tibble: 6 x 2 ## some_dates year_group ## &lt;date&gt; &lt;chr&gt; ## 1 1909-12-31 pre-1950 ## 2 1919-12-31 pre-1950 ## 3 1929-12-31 pre-1950 ## 4 1939-12-31 pre-1950 ## 5 1949-12-31 pre-1950 ## 6 1959-12-31 1950-2000 We could accomplish this with a series of if_else statements, but case_when is just a bit cleaner. The only thing to be aware of is that statements are evaluated in order. So as soon as something matches it doesn’t continue down the list of conditions. That’s why we have that catch-all at the end - if the date doesn’t fit any of the earlier conditions then we’ve got a problem and want to know about it. References "],["workflow.html", "Chapter 4 Workflow 4.1 Introduction 4.2 R Markdown 4.3 R projects 4.4 Git and GitHub 4.5 Using R in practice 4.6 Developing research questions", " Chapter 4 Workflow Required reading Alexander, Monica, 2019, ‘Reproducibility in demographic research’, https://www.monicaalexander.com/posts/2019-10-20-reproducibility/. Bailey, Jack, 2020, ‘UK Voting Intention Poll Tracker’, https://github.com/jackobailey/poll_tracker. Bryan, Jennifer and Jim Hester, 2020, ‘What they Forgot to Teach You About R’, Chapters 11, 12, and 13, https://rstats.wtf/debugging-r-code.html. Bryan, Jenny, 2020, Happy Git and GitHub for the useR, Chapters 4, 6, 7, 9, https://happygitwithr.com/. Fan, Jean, ‘R Style Guide’, https://jef.works/R-style-guide/. Gelman, Andrew, 2016, ‘What has happened down here is the winds have changed’, https://statmodeling.stat.columbia.edu/2016/09/21/what-has-happened-down-here-is-the-winds-have-changed/. Healy, Kieran, 2020, ‘The Kitchen Counter Observatory’, 21 May, https://kieranhealy.org/blog/archives/2020/05/21/the-kitchen-counter-observatory/. Hill, Alison, 2020, ‘How I Teach R Markdown’, 28 May, https://alison.rbind.io/post/2020-05-28-how-i-teach-r-markdown/. Mostipak, Jesse, 2018, ‘So you’ve been asked to make a reprex’, 24 February, https://www.jessemaegan.com/post/so-you-ve-been-asked-to-make-a-reprex/. Phillips, Nathaniel D., 2018, YaRrr! The Pirate’s Guide to R, Chapter 4.3, https://bookdown.org/ndphillips/YaRrr/a-brief-style-guide-commenting-and-spacing.html. Taback, Nathan, 2019, ‘R Markdown for Class Reports’, https://scidesign.github.io/Rmarkdownforclassreports.html. Tierney, Nicholas, 2020, RMarkdown for Scientists, 9 September, Chapters 7-10, 12-15, https://rmd4sci.njtierney.com. Wickham, Hadley, Advanced R, Chapter on Style, http://adv-r.had.co.nz/Style.html. Wickham, Hadley, and Garrett Grolemund, 2017, R for Data Science, Chapter 27, https://r4ds.had.co.nz/. Wilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, Tracy K. Teal, 2017, ‘Good enough practices in scientific computing’, PLoS Computational Biology, 13(6). Recommended reading AirbnbEng, 2016, ‘Scaling Knowledge at Airbnb’, 25 February, https://medium.com/airbnb-engineering/scaling-knowledge-at-airbnb-875d73eff091. Bik, Elisabeth, 2020, ‘The Tadpole Paper Mill’, Science Integrity Digest, 21 February, https://scienceintegritydigest.com/2020/02/21/the-tadpole-paper-mill/. Chan, Martin, 2020, ‘RStudio Projects and Working Directories: A Beginner’s Guide’, https://martinctc.github.io/blog/rstudio-projects-and-working-directories-a-beginner's-guide/. Daly, Darren, 2020, ‘A brief history of medical statistics and its impact on reproducibility’, 2 February, https://medium.com/@darren_dahly/a-brief-history-of-medical-statistics-and-its-role-in-reproducibility-23e31082f024. Ebert, Philip A. “Bayesian reasoning in avalanche terrain: a theoretical investigation.” Journal of Adventure Education and Outdoor Learning 19, no. 1 (2019): 84-95. Gertler, Paul, Sebastian Martinez, Patrick Premand, Laura Rawlings, and Christel Vermeersch, ‘Impact Evaluation in Practice’, Chapters 1, 2. Guzey, Alexey, 2020, ‘How Life Sciences Actually Work: Findings of a Year-Long Investigation’, https://guzey.com/how-life-sciences-actually-work/. King, Gary, ‘How to Write a Publishable Paper as a Class Project’, https://gking.harvard.edu/papers. King, Gary, 2006, ‘Publication, publication’, PS: Political Science &amp; Politics, vol. 39, pp. 119-125, https://gking.harvard.edu/files/gking/files/paperspub.pdf. Lowndes, Julia S. Stewart, Benjamin D. Best, Courtney Scarborough, Jamie C. Afflerbach, Melanie R. Frazier, Casey C. O’Hara, Ning Jiang, and Benjamin S. Halpern, 2017, ‘Our path to better science in less time using open data science tools’, Nature ecology &amp; evolution, 1(6). Miyakawa, Tsuyoshi, 2020, ‘No raw data, no science: another possible source of the reproducibility crisis’, Molecular Brain, 13, 24. https://doi.org/10.1186/s13041-020-0552-2 Peek, Ryan, 2020, ‘10 tips to souping up rmarkdown’, 20 February, https://ryanpeek.github.io/2020-02-20-10-tips-to-souping-up-rmarkdown/#S8. Riederer, Emily, 2019, ‘Resource Round-Up: Reproducible Research Edition’, 29 August, https://emilyriederer.netlify.com/post/resource-round-up-reproducible-research-edition/. Riederer, Emily, 2019, ‘RMarkdown Driven Development (RmdDD)’, 4 May, https://emilyriederer.netlify.com/post/rmarkdown-driven-development/. Silver, Nate, 2020, ‘We Fixed An Issue With How Our Primary Forecast Was Calculating Candidates’ Demographic Strengths’, 19 February, https://fivethirtyeight.com/features/we-fixed-a-mistake-in-how-our-primary-forecast-was-calculating-candidates-demographic-strengths/. Soetewey, Antoine, 2020, ‘Getting started in R markdown’, 18 February, https://www.statsandr.com/blog/getting-started-in-r-markdown/. Varian, Hal, ‘How to Build an Economic Model in Your Spare Time’, http://people.ischool.berkeley.edu/~hal/Papers/how.pdf. (This paper was written a while ago for economists, please just ignore the economics specific parts.) Wayne, Hillel, 2017, ‘How do we trust our science code?’, 14 August, https://www.hillelwayne.com/post/how-do-we-trust-science-code/. Wayne, Hillel, 2020, ‘This is how science happens’, 9 March, https://www.hillelwayne.com/post/this-is-how-science-happens/. Required viewing Bryan, Jenny, ‘Debugging in R’, https://resources.rstudio.com/rstudio-conf-2020/object-of-type-closure-is-not-subsettable-jenny-bryan. Recommended viewing Gelfand, Sharla, 2020, ‘Don’t repeat yourself, talk to yourself! Repeated reporting in the R universe’, 30 January, talk given at rstudio::conf, San Francisco, https://resources.rstudio.com/rstudio-conf-2020/dont-repeat-yourself-talk-to-yourself-repeated-reporting-in-the-r-universe-sharla-gelfand. Recommended activity ‘First Contributions’ https://github.com/forwards/first-contributions. Fun song The Magnetic Fields, 2016, ‘’86 How I Failed Ethics’, 17 November, https://youtu.be/Hu5dEXZ7DOY (thanks to Paul Hodgetts). Key concepts/skills/etc Restart R often (Session -&gt; Restart R and Clear Output). Debugging is a skill, and you will get better at it with time and practice. Start with reading the error message. Check the class. You may get frustrated at times, this is normal. There are various tools that can help. Google is your friend. Make a small example and try to get the code running on that. Cultivating a tenacious mentality may help. Writing code that future-you can understand. Developing important questions. Reproducibility and replicability The importance of data and code access The importance of version control in a modern scientific workflow. The basics of Git and GitHub, as a solo data scientist. Key GitHub workflow with commands Get the latest changes: git pull. Add your updates: git add -A. Check on everything: git status. Commit your changes: git commit -m \"Short description of changes\". Push your changes to GitHub: git push. Quiz What are three features of a good research question (write a paragraph or two)? What is a counterfactual (pick one)? If-then statements in which the if didn’t happen. If-then statements in which the if happen. Statements that are either true or false. Statements that are neither true or false. How do you hide the warnings in a R Markdown R chunk (pick one)? echo = FALSE include = FALSE eval = FALSE warning = FALSE message = FALSE What is a reprex and why is it important to be able to make one (select all)? A reproducible example that enables your error to be reproduced. A reproducible example that helps others help you. A reproducible example during the construction of which you may solve your own problem. A reproducible example that demonstrates you’ve actually tried to help yourself. Why are R Projects important (select all)? They help with reproducibility. They make it easier to share code. They make your workspace more organized. They are all that needs to be done. Consider this sequence: ‘git pull, git status, ________, git status, git commit -m \"My message\", git push’. What is the missing step (pick one)? git add -A. git status. git pull. git push. 4.1 Introduction Suppose you have cancer and you have to choose between a black box AI surgeon that cannot explain how it works but has a 90% cure rate and a human surgeon with an 80% cure rate. Do you want the AI surgeon to be illegal? Geoffrey Hinton, 20 February 2020. The number one thing to keep in mind about machine learning is that performance is evaluated on samples from one dataset, but the model is used in production on samples that may not necessarily follow the same characteristics… The finance industry has a saying for this: “past performance is no guarantee of future results”. Your model scoring X on your test dataset doesn’t mean it will perform at level X on the next N situations it encounters in the real world. The future may not be like the past. So when asking the question, “would you rather use a model that was evaluated as 90% accurate, or a human that was evaluated as 80% accurate”, the answer depends on whether your data is typical per the evaluation process. Humans are adaptable, models are not. If significant uncertainty is involved, go with the human. They may have inferior pattern recognition capabilities (versus models trained on enormous amounts of data), but they understand what they do, they can reason about it, and they can improvise when faced with novelty If every possible situation is known and you want to prioritize scalability and cost-reduction, go with the model. Models exist to encode and operationalize human cognition in well-understood situations. (“well understood” meaning either that it can be explicitly described by a programmer, or that you can amass a dataset that densely samples the distribution of possible situations – which must be static) François Chollet, 20 February 2020. If science is about systematically building and organising knowledge in terms of testable explanations and predictions, then data science takes this and focuses on data. Fundamentally data science is still science, and as such, building and organising knowledge is a critical aspect. Being able to do something yourself, once, does not achieve this. Hence, the focus on reproducibility and replicability. Alexander (2019) says ‘Research is reproducible if it can be reproduced exactly, given all the materials used in the study.’ ‘[Hence] materials need to be provided!’. ’‘[M]aterials’ usually means data, code and software.’ The minimum requirement is to be able to ‘reproduce the data, methods and results (including figures, tables)’. Similarly, from Gelman (2016) you should have noticed that this has been an issue in other sciences. e.g. psychology. The issue with not being reproducible is that we are not contributing to knowledge. We no longer have any idea was is fact in the field of psychology. (This is coming for other fields too, including the field that I was trained in - economics.) Do this matter? Yes. Some of the examples that Gelman (2016) cites (which turned out to be dodgy) don’t really matter e.g. ESP or the power pose. It doesn’t really matter. But increasingly the same methods are being applied in areas where they do matter e.g. ‘nudge’ units. Similarly, Simpson (2017) makes it clear that it’s a big problem in data science. The ‘gay face’ paper that Simpson (2017) writes about has not released their dataset. We have no way of knowing what is going on with it. They have found a certain set of results based on that dataset, their methods, and what they did, but we have no way of knowing how much that matters. As Simpson (2017) says ‘the paper itself does some things right. It has a detailed discussion of the limitations of the data and the method’. You must do this in everything that you write, but it is not enough. Without the data, we don’t know what their results speak to as we don’t understand how representative the sample is. If the dataset is biased, then that undermines their claims. There’s a reason that while initial medical trials are done on mice, etc, eventually human trials are required. In order to do the study they needed a trained dataset. They trained it using Mechanical Turk. Figure 4.1 is from Mattson (2017). Figure 4.1: Instructions for workers to do classification piecework on the Amazon Mechanical Turk platform, p. 46. From Mattson. Mattson (2017) comments: The problems here are legion: Barack Obama is biracial but simply “Black” by American cultural norms. “Clearly Latino” begs the question “to whom?” Latino is an ethnic category, not a racial one: many Latinos already are Caucasian, and increasingly so. By training their workers according to stereotypical American categories, WAK’s algorithm can only spit out the garbage they put in. ‘WAK’s algorithm can only spit out the garbage they put in.’ I would encourage you to print out that statement and paste it somewhere that you will see it every time you get a data science result. What steps can we take to make our work reproducible? Ensure your entire workflow is documented. How did you get the raw data? Can you save the raw data? Will the raw data always be available? Is the raw data available to others? What steps are you taking to transform the raw data in data that can be analysed? How are you analysing the data? How are you building the report? Try to improve each time. Can you run your entire workflow again? Can ‘another person’ run your entire workflow again? Can a future you run your entire workflow again? Can a future ‘another person’ run your entire workflow again? Each of these requirements is increasingly more onerous. We are going to start with worrying about the first. The way we are going to do this is by using R Markdown. 4.2 R Markdown 4.2.1 Getting started R Markdown is a mark-up language similar to html or LaTeX, in comparison to a WYSIWYG language, such as Word. This means that all of the aspects are consistent, for instance, all ‘main headings’ will look the same. However it means that use symbols to designate how you would like certain aspects to appear, and it is only when you compile it that you get to see it. R Markdown is a variant of regular markdown that is specifically designed to allow R code chunks to be included. The advantage is that you can get a ‘live’ document in which code executes and is then printed to a document. The disadvantage is that it can take a while for the document to compile because all of the code needs to run. You can create a new R Markdown document within R Studio (File -&gt; New File -&gt; R Markdown Document). Another advantage of R Markdown is that very similar code can compile into a variety of documents, including html pages and PDFs. R Markdown also has default options set up for including a title, author, and date sections. 4.2.2 Basic commands If you ever need a reminder of the basics of R Markdown then this is built into R Studio (Help -&gt; Markdown Quick Reference). This provides the code for commonly needed commands: Emphasis: *italic*, **bold**, _italic_, __bold__ Headers (these need to go on their own line with a line before and after): # Header 1, ## Header 2, ### Header 3 Lists: Unordered List * Item 1 * Item 2 + Item 2a + Item 2b Ordered List 1. Item 1 2. Item 2 3. Item 3 + Item 3a + Item 3b URLs: Can just include an address: http://example.com, or can include a [linked phrase](http://example.com). Basic images can just be included either from the internet: ![alt text](http://example.com/logo.png) or from a local file: ![alt text](figures/img.png). In order to create an actual document, once you have these pieces set up, click ‘Knit’. 4.2.3 R chunks You can include R (and a bunch of other languages) code in code chunks within your R Markdown document. Then when you knit your document, the R code will run and be included in your document. To create an R chunk start with three backticks and then within curly braces tell markdown that this is an R chunk. Anything inside this chunk will be considered R code and run as such. library(tidyverse) ggplot(data = diamonds) + geom_point(aes(x = price, y = carat)) There are various evaluation options that are available in chunks. You include these by putting a comma after r and then specifying any options before the closing curly brace. Helpful options include: echo = FALSE: run the code and include the output, but don’t print the code in the document. include = FALSE: run the code but don’t output anything and don’t print the code in the document. eval = FALSE: don’t run the code, and hence don’t include the outputs, but do print the code in the document. warning = FALSE: don’t display warnings. message = FALSE: don’t display messages. 4.2.4 Abstracts and PDF outputs In the default header, you can add a section for a header, so that it would look like this: --- title: My document author: Rohan Alexander date: 5 January 2020 output: html_document abstract: &quot;This is my abstract.&quot; --- Similarly, you can change the output from html_document to pdf_document in order to produce a PDF. This uses LaTeX in the background so you may need to install a bunch of related packages. 4.2.5 References You can reference a bibliography by including one in the preamble and then calling it in the text when you need. --- title: My document author: Rohan Alexander date: 5 January 2020 output: html_document abstract: &quot;This is my abstract.&quot; bibliography: bibliography.bib --- You need to make a separate file called bibliography.bib. In that you need an entry for the item that you want to reference. R and R packages usually provide this for you for instance, if you run citation() then it tells you the entry to put in your bibtex file: @Manual{, title = {R: A Language and Environment for Statistical Computing}, author = {{R Core Team}}, organization = {R Foundation for Statistical Computing}, address = {Vienna, Austria}, year = {2020}, url = {https://www.R-project.org/}, } You need to create a unique key that you’ll refer to it with in the text. This can be anything if it’s unique, but I try to use meaningful ones, so that bibtex entry could become: @Manual{citeR, title = {R: A Language and Environment for Statistical Computing}, author = {{R Core Team}}, organization = {R Foundation for Statistical Computing}, address = {Vienna, Austria}, year = {2020}, url = {https://www.R-project.org/}, } And to cite R you’d then include the following: @citeR, which would put the brackets around the year, like this: R Core Team (2020) or [@citeR], which would put the brackets around the whole thing, like this: (R Core Team 2020). 4.2.6 Cross-references Finally, it can be useful to cross-reference figures, tables and equations. This makes it easier to refer to them in the text. To do this for a figure you refer to the name of the R chunk that creates/contains the figure. For instance, (Figure \\@ref(fig:my_unique_name)) will produce: (Figure @ref(fig:my_unique_name)) as the name of the R chunk is my_unique_name (don’t forget to add the fig in front of the chunk name. Also, super annoyingly you need to have a ‘fig.cap’ in the R chunk, so it looks something like this: ```{r my_unique_name, fig.cap=&quot;More bills of penguins&quot;, echo = TRUE} library(palmerpenguins) ggplot(penguins, aes(x = island, fill = species)) + geom_bar(alpha = 0.8) + scale_fill_manual(values = c(&quot;darkorange&quot;,&quot;purple&quot;,&quot;cyan4&quot;), guide = FALSE) + theme_minimal() + facet_wrap(~species, ncol = 1) + coord_flip() (#fig:my_unique_name)More bills of penguins You can do similar for tables and equations e.g (Table \\@ref(tab:penguinhead)) will produce: (Table 4.1) (again, don’t forget to ad tab in front). penguins %&gt;% select(species, bill_length_mm, bill_depth_mm) %&gt;% slice(1:5) %&gt;% knitr::kable(caption = &quot;A penguin table&quot;) Table 4.1: A penguin table species bill_length_mm bill_depth_mm Adelie 39.1 18.7 Adelie 39.5 17.4 Adelie 40.3 18.0 Adelie NA NA Adelie 36.7 19.3 And finally, you can, and should, cross-reference equations also, but this time you need to add a tag (\\#eq:slope) and then reference that e.g. use Equation \\@ref(eq:slope). to produce Equation (4.1). \\begin{equation} Y = a + b X (\\#eq:slope) \\end{equation} \\[\\begin{equation} Y = a + b X \\tag{4.1} \\end{equation}\\] 4.3 R projects RStudio has the option of creating a project, which allows you to keep all the files (data, analysis, report etc) associated with a particular project together. To create a project, click Click File &gt; New Project, then select empty project, name your project and think about where you want to save it. For example, if you are creating a project for Problem Set 2, you might call it ps2 and save it in a sub-folder called PS2 in your INF2178 folder. Once you have created a project, a new file with the extension .RProj will appear in that file. As an example, download the R help folder. Whenever I work on class materials, I open the project file and work from that. The main advantage of projects is that you don’t have to set the working directory or type the whole file path to read in a file (for example, a data file). So instead of reading a csv from \"~/Documents/toronto/teaching/INF2178/data/\" you can just read it in from data/. To meet even the minimal expected level of reproducibility, you must use R projects. You must not use setwd() because that ties your work to your computer. 4.4 Git and GitHub 4.4.1 Introduction Here we introduce Git and GitHub. These are tools that: enhance the reproducibility of your work by making it easier to share your code and data; make it easier to show off your work; improve your workflow by encouraging you to think systematically about your approach; and (although we won’t take advantage of this) make it easier to work in teams. Git is a version control system. One way you might be used to doing version control is to have various versions of your files: first_go.R, first_go-fixed.R, first_go-fixed-with-mons-edits.R. But this soon becomes cumbersome. Some of you may use dates, for instance: 2020-03-12-analysis.R, 2020-03-13-analysis.R, 2020-03-14-analysis.R, etc. While this keeps a record it can be difficult to search if you need to go back - will you really remember the date of some change in a week? How about a month or a year? It gets unwieldy fairly quickly. Instead of this, Git allows you to only have one version of the file analysis.R and it keeps a record of the changes to that file, and a snapshot of that file at a given point in time. When it takes that snapshot is determined by you. When you want Git to take a snapshot you additionally include a message, saying what changed between this snapshot and the last. In that way, there is only ever one version of the file, but the history can be more easily searched. The issue is that Git was designed for software developers. As such, while it works, it can be a little ungainly for non-developers (Figure 4.2). Figure 4.2: An infamous response to the launch of Dropbox in 2007, trivialising the use-case for Dropbox, and while this actually would work, it wouldn’t for most of us. Hence, GitHub, GitLab, and various other companies offer easier-to-use services that build on Git. GitHub used to be the weapon of choice, but they were sold to Microsoft in 2018 and since then other variants such as GitLab have risen in popularity. We will introduce GitHub here because it remains the most popular, and it is built into RStudio, however you should feel free to explore other options. One of the hardest aspects of Git, and the rest, for me was the terminology. Folders are called ‘repos’. Saving is called a ‘commit’. You’ll get used to it eventually, but just so you know - it’s not you, it’s Git - feeling confused it entirely normal These are brief notes and you should refer to Jenny Bryan’s book for further detail. Frankly, I can’t improve on Bryan’s book and I use them regularly myself. 4.4.2 Git Check if you have Git installed by opening R Studio and then going to the Terminal and typing the following and then return. git --version If you get a version number, then you are done (Figure 4.3). Figure 4.3: How to access the Terminal within R Studio If you have a Mac then Git should come pre-installed, if you have Windows then there’s a chance, and if you have Linux then you probably don’t need this guide. If you don’t get a version number, then you need to install it. Please go to Chapter 5 of J. Bryan (2020) for some instructions based on your operating system. After you have Git, then you need to tell it your username and email. You need this because Git adds this information whenever you take a ‘snapshot’, or to use Git’s language whenever you make a commit. Again, within the Terminal, type the following, replacing the details with yours, and then return after each line. git config --global user.name &#39;Jane Doe&#39; git config --global user.email &#39;jane@example.com&#39; git config --global --list The details that you enter here will be public (there are various ways to hide your email address if you need to do this and GitHub provides instructions about this). Again, if you have issues or need more detailed instructions please go to Chapter 7 of J. Bryan (2020). 4.4.3 GitHub The first step is to create an account on GitHub (https://github.com) (Figure 4.4). Figure 4.4: Sign up screen at GitHub GitHub doesn’t have the most intuitive user experience in the world, but we are now going to make a new folder (which is called a ‘repo’ in Git). You are looking for a plus sign in the top right, and then select ‘New Repository’ (Figure 4.5). Figure 4.5: Create a new repository At this point you can add a sensible name for your repo. Leave it as public (you can delete it later if you want). And check the box to initialize with a readme. In the ‘Add .gitignore’ option you can leave it for now, but if you start using GitHub more regularly then you may like to select the R option here. (That just tells Git to ignore various files.) After that, just click the button to create a new repository (Figure 4.6). Figure 4.6: Create a new repository, really You’ll now be taken to a screen that is fairly empty, but the details that you need are in the green ‘Clone or Download’ button, then click the clipboard (Figure 4.7). Figure 4.7: Get the details of your new repository Now you need to open Terminal, and use cd to get to where you want to save the folder, then: git clone https://github.com/RohanAlexander/test.git At this point, a new folder has been created. We can now interact with it. The first step is almost always to pull the latest changes with git pull (this is slightly pointless in this example because it’s just us but it’s a good habit to get into). We can then make a change to the folder, for instance, update the readme, and then save it as usual. Once this is done, we need to add, commit, and push. As before, use cd to navigate to your folder, then git status to see if there is anything going on (you should see some reference to the change you made). Then git add -A adds the changes to the staging area (this seems pointless, and it is in this context, but this allows you to specify specific files and similar if needed). Then git status to check what has happened. Then git commit -m \"Minor update to readme\", and then git status to check on everything, and finally git push. To summarise (assuming you are in the relevant folder): git pull git status git add -A git status git commit -m &quot;Short commit message&quot; git status git push 4.4.4 Using Git within RStudio I promised that GitHub was built into RStudio, but so far we’ve not really taken advantage of that. The way to do it is to create a new repo in GitHub, and copy the information, as before. At this point, you open RStudio, select Files, New Project, Version Control, Git, and paste the information for the repo. Go through the rest of it, saving the folder somewhere sensible, and clicking ‘Open in new session’. This will then create a new folder on your computer which will be a Git folder that is linked to the GitHub repo that you created. At this point, you’ll have a ‘Git’ tab (Figure 4.8). Figure 4.8: The Git pane in R Studio First pull (click the blue down arrow). Now you want to tick the ‘staged’ box against the files that you want to commit. Then click ‘Commit’. Type a message in the ‘Commit message’ box and then click ‘Commit’. Finally ‘Push’. Again, the details are in J. Bryan (2020), especially Chapter 12. 4.4.5 Next steps We haven’t really taken advantage of GitHub’s features in terms of teams or branches. That is certainly something that you should get into once you have more confidence with these basics. As always, when it comes to Git for data scientists who use R, you should go to the relevant sections of J. Bryan (2020). 4.5 Using R in practice 4.5.1 Introduction This section is what do when your code doesn’t do what you want, discusses a mindset that may help when doing quantitative analysis with R, and finally, some recommendations around how to write your code. 4.5.2 Getting help Programming is hard and everyone struggles sometimes. At some point your code won’t run or will throw an error. This is normal, and it happens to everyone. It happens to me on a daily, sometimes hourly, basis. Everyone gets frustrated. There are a few steps that are worthwhile taking when this happens: Sometimes the error messages in R are useful. Read it carefully and see if there’s anything of use in it. At the very least, if you get the same message in the future, hopefully you might remember how you solved the problem this time! If you’re getting an error then try googling it, (I find it can help to include the term ‘R’ or ‘tidyverse’ or the relevant package name). If there’s a particular function that seems to be giving trouble, have a look at the help file for it. Sometimes you might be putting in the arguments in the wrong order. You can do this with ‘?function’ e.g. for help with select, you would type ‘?select’ and then run that line. Check the class of the object. Sometimes R is a little fussy and converting the class can help. If your code just isn’t running, then try searching for what you are trying to do, e.g. ‘save PDF of graph in R made using ggplot’. Almost always there are relevant blog posts or Stack Overflow answers that will help. Try to restart R and R Studio and load everything again. Try to restart your computer. There are a few small mistakes that I often make and may be worth checking in case you make them too: check the class e.g. class(my_dataset$its_column) to make sure that is what it should be; when you’re using ggplot make sure you use ‘+’ not ‘%&gt;%’; and check whether you are using ‘.’ when you shouldn’t be, or vice versa. It’s almost always helpful to take a break and come back the next day. Asking for help is a skill that you will get better at. Try not to say ‘this doesn’t work’, or ‘I tried everything’ or ‘here’s the error message, what do I do?’. In general, it’s not possible to help based on that because there are too many possibilities. Instead: Provide a small example of your data, and code, and detail what is going wrong. Document what you have tried so far - what Stack Overflow pages have you looked at and why are they not quite what you’re after? What RStudio Community pages have you tried? Be clear about the outcome that you would like. As the RStudio Community welcome page says ‘your job is to make it as easy as possible for others to help you’. To enable that to happen you need to ‘create a minimal reproducible example, or reprex for short’ You can get more information about this here, but basically what is needed is that you code is reproducible (so include things like library(), etc) and that is it minimal - that means making a very simple smaller example and reproducing the error on that small example. Usually doing this actually allows you to solve your own problem. If it doesn’t then it’ll allow someone else a fighting chance as being able to help you. I especially recommend the reprex package (Bryan et al. 2019). There’s almost no chance that you’ve got a problem that someone hasn’t addressed before, it’s just a matter of finding the answer! Try to be tenacious with this and learn how to solve your own problems. 4.5.3 Mentality (Y)ou are a real, valid, competent user and programmer no matter what IDE you develop in or what tools you use to make your work work for you (L)et’s break down the gates, there’s enough room for everyone Sharla Gelfand, 10 March 2020. I’m a little hesitant to make suggestions with regard to mentality. If you write code, then you’re coder regardless of how you do it, what you’re using it for, or who you are. But I want to share a few traits that I have found have been useful to cultivate in myself. That said, entirely, whatever works for you is great, so take or leave this section. Focused: I’ve found that having an aim to ‘learn R’ or something similar tends to be problematic, because there’s no real end point to that. Instead I would recommend smaller, more specific goals, such as ‘make a histogram about the 2019 Canadian Election with ggplot’. That is something that you can focus on and achieve. With more nebulous goals it becomes easier to get lost on tangents, much more difficult to get help, and I’ve noticed that people who have nebulous goals seem to give up. Curious: I’ve found that it’s useful to just have a go. In general, the worst that happens is that you waste your time and have to give up. You can rarely break something irreparably with code. If you want to know what happens if you pass a ‘vector’ instead of a ‘dataframe’ to ggplot then just try it. Pragmatic: At the same time, I’ve found that it’s best to try to stick within the bounds of what I know and just make one small change each time. For instance if you’re wanting to do some regression, and curious about the tidymodels package (Kuhn and Wickham 2020) instead of lm(). Perhaps you could just use one aspect from the tidymodels package initially and then make another change next time. In my opinion ugly code that gets the job done, is better than beautiful code that is never finished. Tenacious: This is a balancing act. I always find there are unexpected problems and issues with every project. On the one hand persevering despite these is a good tendency. But on the other hand I’ve learned that sometimes I need to be prepared to give up on something if it doesn’t seem like a break-through is possible. This is where I have found that mentors can be useful as they tend to have a better idea. This is also where planning comes in. Planned: I have found it is very useful to plan out what you are going to do. For instance, you may want to make a histogram of the 2019 Canadian Election. I find it useful to plan the steps that are needed and even to sketch out how I might implement each step. For instance, the first step is to get the data. What packages might be useful? Where might the data be? What is our back-up plan for if we can’t find the data in that initial spot? Done is better than perfect: We all have various perfectionist tendencies to a certain extent, but I recommend that you try to turn them off to a certain extent when it comes to R. In the first instance just try to write code that works, especially in the early days. You can always come back and improve aspects of it. But it is important to actually ship. 4.5.4 Code comments Comment your code. There is no one way to write code, especially in R. However, there are some general guidelines that will make it easier for you even if you’re just working on your own. Comment your code. Comments in R can be added by including the # symbol. The shortcut on Mac is ‘Command + Shift + m’. You don’t have to put a comment at the start of the line, it can be midway through. In general, you don’t need to comment what every aspect of your code is doing but you should comment parts that are not obvious. For instance, if you read in some value then you may like to comment where it is coming from. Comment your code. You should comment why you are doing something. What are you trying to achieve? Comment your code. You must comment to explain weird things. Like if you’re removing some specific row, say row 27, then why are you removing that row? It may seem obvious in the moment, but future-you in six months won’t remember. Comment your code. I like to break my code into sections. For instance, setting up my workspace, reading in datasets, manipulating and cleaning the dataset, analysing the datasets, and finally producing tables and figures. While it can be difficult to speak generally, I usually separate each of those certainly with comments explaining what is going on, and sometimes into separate files, depending on the length. Comment your code. Additionally, at the top of each file I put basic information, such as the purpose of the file, and pre-requisites or dependencies, the date, the author and contact information, and finally and red-flags, bodies, or todos. Comment your code. At the very least I recommend something like the following for every R script: #### Preamble #### # Purpose: Brief sentence about what this script does # Author: Your name # Data: The date it was written # Contact: Add your email # License: Think about how your code may be used # Pre-requisites: # - Maybe you need some data or some other script to have been run? #### Workspace setup #### # Don&#39;t keep the install.packages line - just comment out if need be # Load libraries library(tidyverse) # Read in the raw data. raw_data &lt;- readr::read_csv(&quot;inputs/data/raw_data.csv&quot;) #### Next section #### ... 4.5.5 Learning more One of the great aspects of R is that there is a friendly community of people who use it. There are a variety of ways that I learn about new tricks, functions, and packages including: R Ladies ‘is a world-wide organization to promote gender diversity in the R community’. Whether that is your cup of tea or not, they tend to share fantastic material on their twitter feeds: https://twitter.com/rladiesglobal and https://twitter.com/WeAreRLadies (where someone takes over the account for the week, so the enthusiasm never wanes). If you’re in Toronto, then the local chapter is: https://rladies.org/canada-rladies/locality/Toronto/ and https://www.meetup.com/rladies-toronto/. R Weekly is a weekly newsletter that highlights new things that have happened recently. A popular R podcast is Not So Standard Deviations. Another great way to learn is by exchanging your code with others. Initially, just have them read it and give you feedback about it. But after you get a bit more confident run each other’s code. The most efficiently I’ve ever improved in my R journey has been by having Monica try to run my code. 4.6 Developing research questions Both qualitative and quantitative approaches have their place, but here we focus on quantitative approaches. (Qualitative research is important as well, and often the most interesting work has a little of both - ‘mixed methods’.) This means that we are subject to issues surrounding data quality, scales, measures, sources, etc. We are especially interested in trying to tease out causality. Broadly there are two ways to go about research: data-first, question-first. If you get a job somewhere typically you will initially be data-first. This means that you will need to work out the questions that you can reasonably answer with the data available to you. After you show some promise, you may be given the latitude to explore specific questions, possibly even gathering data specifically for that purpose. Contrast this with the example of the Behavioural Insights Team, (Gertler et al. 2016, 23) who got to design and then carry out experiments given the remit of the entire British government (as they were spun out of the prime minister’s office). When deciding the questions that you can reasonably answer with the data that are available, you need to think about: Theory: Do you have a reasonable expectation that there is something causal that could be determined? Charting the stock market - maybe, but might be better with haruspex because at least that way you have something you could eat. You need a reasonable theory of how \\(x\\) may be affecting \\(y\\). Importance: There are plenty of trivial questions that you could ask, but it’s important to not waste your time. The importance of a question also helps with motivation when you are on your fourth straight week of cleaning data and de-bugging your code. It also (and this becomes important) makes it easier to get talented people to work with you, or similarly to convince people to fund you or allow you to work on this project. Availability: Can you reasonably expect get more data about this research question in the future or is this the extent of the data that could be gathered? Iteration: Is this something that you can come back to and run often or is this a once-off analysis? The ‘FINER framework’ as a mnemonic device used in medicine.3 This framework reminds us to ask questions that are (Hulley 2007): Feasible: Adequate number of subjects; adequate technical expertise; affordable in time and money; manageable in scope. Interesting: Getting the answer intrigues investigator, peers and community. Novel: Confirms, refutes or extends previous findings Ethical: Amenable to a study that institutional review board will approve. Relevant: To scientific knowledge; to clinical and health policy; to future research. Farrugia et al. (2010) build on this in terms of developing research questions and recommend ‘PICOT’: Population: What specific population are you interested in? Intervention: What is your investigational intervention? Comparison group: What is the main alternative to compare with the intervention? Outcome of interest: What do you intend to accomplish, measure, improve or affect? Time: What is the appropriate follow-up time to assess outcome Often time will be constrained, possibly in interesting ways and these can guide your research. If we are interested in the effect of Trump’s tweets on the stock market, then that can be done just by looking at the minutes (milliseconds?) after he tweets. But what if we are interested in the effect of a cancer drug on long term outcomes? If the effect takes 20 years then we either have to wait a while, or we need to look at people who were treated in 2000, but then we have selection effects and different circumstances to if we give the drug today. Often the only reasonable thing to do is to build a statistical model, but then we need adequate sample sizes, etc. Usually the creation of a counterfactual is crucial. We’ll discuss counterfactuals a lot more later, but briefly, a counterfactual is an if-then statement in which the ‘if’ is false. Consider the example of Humpty Dumpty from Lewis Carroll’s Through the Looking-Glass: Figure 4.9: Humpty Dumpty example Humpty is satisfied with what would happen if he were to fall off, even though he is similarly satisfied that this would never happen. (I won’t ruin the story for you.) The comparison group often determines your results e.g. the relationship between VO2 and athletic outcomes, compared with elite athletic outcomes. Finally, we can often dodge ethics boards in data science, especially once you leave university. Typically, ethics guides from medicine and other fields are focused on ethics boards. But we often don’t have those in data science applications. Even if your intentions are unimpeachable, I want to suggest one additional aspect to think about, and that is Bayes theorem: \\[P(A|B) = \\frac{P(B|A)\\times P(A)}{P(B)}\\] (The probability of A given B depends on the probability of B given A, the probability of A, and the probability of B.) To see why this may be relevant, let’s go to the canonical Bayes example: There is some test for a disease that is 99 per cent accurate both ways (that is, if a person actually has the disease there is a 99 per cent chance the test says positive, and is a person does not have the disease then there is a 99 per cent chance the test says negative). Let’s just say that only 0.005 of the population has the disease. Then if we randomly pick someone from the general population then the chance that they have the disease is outstandingly low. This is even if they test positive: \\[\\frac{0.99\\times0.005}{0.99\\times0.005 + 0.01\\times0.995} \\approx 33.2\\] To see why this may be relevant, consider the example of Google’s AI cancer testing (Shetty and Tse 2020). Basically what they have done is to train a model that can identify breast cancer. They claim ‘greater accuracy, fewer false positives, and fewer false negatives than experts’. I, and many others (Aschwanden 2020), would argue this is probably not where we would want these resources directed at this point. Even when perfectly healthy people go and get screened they tend to find various things that are ‘wrong’ with them. The issue is that they’re perfectly healthy and that we’ve rarely got a good idea as to whether that aspect that was flagged by the test is a big deal or not. Given low prevalence in the community, we probably don’t want wide-spread use of a particular testing regime that only looks at one aspect (i.e. the mammogram in this case). Bayes rule guides us that the danger caused by the unnecessary ‘treatment’ would probably outweigh the benefits. The authors of that Google blog post likely have unimpeachable ethics, but they may not understand Bayes rule. References "],["static-communication.html", "Chapter 5 Static communication 5.1 Introduction 5.2 Graphs 5.3 Tables 5.4 Maps 5.5 Writing", " Chapter 5 Static communication Required reading (The) Economist, 2013, ‘Johnson: Those six little rules’, Prospero, 29 July 2013, available at: https://www.economist.com/prospero/2013/07/29/johnson-those-six-little-rules. Alexander, Monica, 2019, ‘The concentration and uniqueness of baby names in Australia and the US’, https://www.monicaalexander.com/posts/2019-20-01-babynames/. (Look at how Monica explains concepts, especially the Gini coefficient, in a way that you can understand even if you’ve never heard of it before.) Bronner, Laura, 2020, ‘Quant Editing’, http://www.laurabronner.com/quant-editing. (Read these points and evaluate your own writing against them. It’s fine to not comply with them if you have a good reason, but you need to know that you’re not complying with them). Girouard, Dave, 2020, ‘A Founder’s Guide to Writing Well’, First Round Review, 4 August, https://firstround.com/review/a-founders-guide-to-writing-well/. Graham, Paul, 2020, ‘How to Write Usefully’, http://paulgraham.com/useful.html. (Graham is good at writing for a programmer, but if you have a similar background then you may like this.) Healy, Kieran, 2019, Data Visualization: A Practical Introduction, Princeton University Press, Chapters 3 and 4, https://socviz.co/. Hodgetts, Paul, 2020, ‘The ggfortify Package’, 31 December, https://www.hodgettsp.com/posts/r-ggfortify/. Wickham, Hadley, and Garrett Grolemund, 2017, R for Data Science, Chapter 28, https://r4ds.had.co.nz/. Zinsser, William, 1976 [2016], On Writing Well. (Any edition is fine. This book is included because if you’re serious about improving your writing then you should start with this book. It only takes a few hours to read. You’ll go onto other books, but start with this one.) Zinsser, William, 2009, ‘Writing English as a Second Language’, Lecture, Columbia Graduate School of Journalism, 11 August, https://theamericanscholar.org/writing-english-as-a-second-language/. (I’m realistic enough to realise that requiring a book, even though I’ve said it’s great and it’s short, is a bit of a stretch. If you really don’t want to commit to reading the Zinsser, then please at least read this ‘crib notes’ version of it.) Required viewing Kuriwaki, Shiro, 2020, ‘Making maps in R with sf’, 1 March, freely available at: https://vimeo.com/394800836. Recommended reading (The) Economist, 1991 [2014], ‘The Economist Style Guide’, Twelfth edition. (Any edition is fine. Pick a point or two each day and think about how it related to your own writing.) Cochrane, John H., 2005, ‘Writing Tips for Ph. D. Students’, https://faculty.chicagobooth.edu/john.cochrane/research/papers/phd_paper_writing.pdf. (This is aimed at academic research papers, but parts are still broadly relevant. And if you’re going into academia then this is very relevant.) Codrey, Laura, 2013, ‘Churchill’s call for brevity’, 17 October, https://blog.nationalarchives.gov.uk/churchills-call-for-brevity/. Engel, Claudia A, 2019, Using Spatial Data with R, 11 February, Chapter 3 Making Maps in R, freely available at: https://cengel.github.io/R-spatial/mapping.html. Five Thirty Eight, 2020, Pick almost any article in their sports (https://fivethirtyeight.com/sports/) or politics (https://fivethirtyeight.com/politics/) sections. (The people at 538 write beautifully. Look at how their titles tell you exactly what is going on, or what they found. Look at how nicely their first paragraphs motivates you to read the rest of the article. Why am I reading about BYU basketball when I’m indifferent to both BYU and college basketball? Because that title and first paragraph hooked me.) Graham, Paul, 2005, ‘Writing, Briefly’, http://paulgraham.com/writing44.html. Lovelace, Robin, Jakub Nowosad, Jannes Muenchow, 2020, Geocomputation with R, 29 March, Chapter 8, Making maps with R, freely available at: https://geocompr.robinlovelace.net/adv-map.html. Patrick, Cameron, 2019, ‘Plotting multiple variables at once using ggplot2 and tidyr’, 26 November, https://cameronpatrick.com/post/2019/11/plotting-multiple-variables-ggplot2-tidyr/. Patrick, Cameron, 2020, ‘Making beautiful bar charts with ggplot’, 15 March, https://cameronpatrick.com/post/2020/03/beautiful-bar-charts-ggplot/. Shapiro, Jesse M., ‘Four Steps to an Applied Micro Paper’, https://www.brown.edu/Research/Shapiro/pdfs/foursteps.pdf. (This is mostly recommended for the part about ‘the robot’ with regard to your data section.) Shapiro, Julian, ‘Writing Well’, https://www.julian.com/guide/write/intro. Strunk, William Jr., 1959 [2009] ‘The Elements of Style’. (Any edition is fine. Eventually you’ll move beyond this, but it’s important to know the rules before you break them). Vanderplas, Susan, Dianne Cook, and Heike Hofmann, 2020, ‘Testing Statistical Charts: What Makes a Good Graph?’, Annual Review of Statistics and Its Application, https://www.annualreviews.org/doi/abs/10.1146/annurev-statistics-031219-041252 Examples of well-written papers Barron, Alexander TJ, Jenny Huang, Rebecca L. Spang, and Simon DeDeo. “Individuals, institutions, and innovation in the debates of the French Revolution.” Proceedings of the National Academy of Sciences 115, no. 18 (2018): 4607-4612. r Chambliss, Daniel F. “The Mundanity of Excellence: An Ethnographic Report on Stratification and Olympic Swimmers.” Sociological Theory 7, no. 1 (1989): 70-86. doi:10.2307/202063. Joyner, Michael J. “Modeling: optimal marathon performance on the basis of physiological factors.” Journal of Applied Physiology, 70, no. 2 (1991): 683-687. Kharecha, Pushker A., and James E. Hansen, 2013, ‘Prevented mortality and greenhouse gas emissions from historical and projected nuclear power’, Environmental science &amp; technology, 47, no. 9, pp. 4889-4895. Samuel, Arthur L., 1959, ‘Some studies in machine learning using the game of checkers’, IBM Journal of research and development, 3, no. 3, pp. 210-229. Wardrop, Robert L., 1995, ‘Simpson’s paradox and the hot hand in basketball’, The American Statistician, 49, no. 1, 24-28. Key concepts/skills/etc Show the reader your raw data, or as close as you can come to it. Use either geom_point or geom_bar initially. Writing efficiently and effectively is a requirement if you want your work to be convincing. Don’t waste your reader’s time. A good title says what the paper is about, a great title says what the paper found. For a six-page paper, a good abstract is a three to five sentence paragraph. For a longer paper your abstract can be slightly longer. Thinking of maps as a (often fiddly, but strangely enjoyable) variant of a usual ggplot. Broadening the data that we make available via interactive maps, while still telling a clear story. Becoming comfortable with (and excited about) creating static maps. Key libraries ggplot patchwork ggmap maps Key functions/etc ggplot::geom_point() ggplot::geom_bar() canada.cities geom_polygon() ggmap() map() map_data() Quiz I have a dataset that contains measurements of height (in cm) for a sample of 300 penguins, who are either the Adeline or Emperor species. I am interested in visualizing the distribution of heights by species in a graphical way. Please discuss whether a pie chart is an appropriate type of graph to use. What about a box and whisker plot? Finally, what are some considerations if you made a histogram? [Please write a paragraph or two for each aspect.] Assume the dataset and columns exist. Would this code work? data %&gt;% ggplot(aes(x = col_one)) %&gt;% geom_point() (pick one)? Yes No If I have categorical data, which geom should I use to plot it (pick one)? geom_bar() geom_point() geom_abline() geom_boxplot() Why are box plots often inappropriate (pick one)? They hide the full distribution of the data. They are hard to make. They are ugly. The mode is clearly displayed. Which of the following is the best title (pick one)? “Problem Set 1” “Unemployment” “Examining Canada’s Unemployment (2010-2020)” “Canada’s Unemployment Increased between 2010 and 2020” 5.1 Introduction In order to convince someone of your story, your paper must be well-written, well-organized, and easy to follow. It should flow easily from one point to the next. It should have proper sentence structure, spelling, vocabulary, and grammar. Each point should be articulated clearly and completely without being overly verbose. Papers should demonstrate your understanding of the topics you are writing about and your confidence in discussing the terms, techniques and issues that are relevant. References must be included and properly cited because this enhances your credibility. People who need to write: founders, VCs, lawyers, software engineers, designers, painters, data scientists, musicians, filmmakers, creative directors, physical trainers, teachers, writers. Learn to write. Sahil Lavingia. This is great advice. Writing well has done just as much for me as knowing how to code. I’d add that if you’re intimidated by writing, start a blog and write often about something you’re interested in. You’ll get better. At least that’s what I’ve done for the past 10 years. :) Vicki Boykis. This chapter is about writing. By the end of it you will have a better idea of how to write short, detailed, quantitative papers that communicate exactly what you want them to and don’t waste the time of your reader. One critical part of telling stories with data, is that it’s ultimately the data that has to convince them. You’re the medium, but the data are the message. To that end, the easiest way to try to convince someone of your story is to show them the data that allowed you to come to that story. Plot your raw data, or as close to it as possible. While ggplot is a fantastic tool for doing this, there is a lot to that package and so it can be difficult to know where to start. My recommendation is that you start with either a scatter plot or a bar chart. What is critical is that you show the reader your raw data. These notes run through how to do that. It then discusses some more advanced options, but the important thing is that you show the reader your raw data (or as close to it as you can). Students seem to get confused what ‘raw’ means; I’m using it to refer to as close to the original dataset as possible, so no sums, or averages, etc, if possible. Sometimes your data are too disperse for that or you’ve got other constraints, so there needs to be an element of manipulation. The main point is that you, at the very least, need to plot the data that you’re going to be modelling. If you are dealing with larger datasets then just take a 10/1/0.1/etc per cent sample. Figure 5.1: Show me the data! Source: YouTube screenshot. 5.2 Graphs Graphs are critical to tell a compelling story. And the most important thing with your graphs is to plot your raw data. Again: Plot. Your. Raw. Data. Let’s look at a somewhat fun example from the datasauRus package (Locke and D’Agostino McGowan 2018). library(datasauRus) # Code from: https://juliasilge.com/blog/datasaurus-multiclass/ datasaurus_dozen %&gt;% filter(dataset %in% c(&quot;dino&quot;, &quot;star&quot;, &quot;away&quot;, &quot;bullseye&quot;)) %&gt;% group_by(dataset) %&gt;% summarise(across(c(x, y), list(mean = mean, sd = sd)), x_y_cor = cor(x, y) ) %&gt;% ungroup() ## # A tibble: 4 x 6 ## dataset x_mean x_sd y_mean y_sd x_y_cor ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 away 54.3 16.8 47.8 26.9 -0.0641 ## 2 bullseye 54.3 16.8 47.8 26.9 -0.0686 ## 3 dino 54.3 16.8 47.8 26.9 -0.0645 ## 4 star 54.3 16.8 47.8 26.9 -0.0630 And despite these similarities at a summary statistic level, they’re actually very different, well, beasts, when you plot the raw data. datasaurus_dozen %&gt;% filter(dataset %in% c(&quot;dino&quot;, &quot;star&quot;, &quot;away&quot;, &quot;bullseye&quot;)) %&gt;% ggplot(aes(x=x, y=y, colour=dataset)) + geom_point() + theme_minimal() + facet_wrap(vars(dataset), nrow = 2, ncol = 2) + labs(colour = &quot;Dataset&quot;) 5.2.1 Bar chart Bar charts are useful when you have one variable that you want to focus on. Hint: you almost always have one variable that you want to focus on. Hence, you should almost always include at least one (and likely many) bar charts. Bar charts go by a variety of names, depending on their specifics. I recommend the R Studio Data Viz Cheat Sheet. To get started, let’s simulate some data. set.seed(853) number_of_observation &lt;- 10000 example_data &lt;- tibble(person = c(1:number_of_observation), smoker = sample(x = c(&quot;Smoker&quot;, &quot;Non-smoker&quot;), size = number_of_observation, replace = TRUE), age_died = runif(number_of_observation, min = 0, max = 100) %&gt;% round(digits = 0), height = sample(x = c(50:220), size = number_of_observation, replace = TRUE), num_children = sample(x = c(0:5), size = number_of_observation, replace = TRUE, prob = c(0.1, 0.2, 0.40, 0.15, 0.1, 0.05)) ) First, let’s have a look at the data. head(example_data) ## # A tibble: 6 x 5 ## person smoker age_died height num_children ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 1 Smoker 55 80 3 ## 2 2 Non-smoker 54 78 2 ## 3 3 Non-smoker 84 109 1 ## 4 4 Smoker 75 114 4 ## 5 5 Smoker 32 135 1 ## 6 6 Smoker 37 220 0 Now let’s plot the age distribution. Based on our simulated data, we’re expecting a fairly uniform plot. example_data %&gt;% ggplot(mapping = aes(x = age_died)) + geom_bar() Now let’s make it look a little better. There are themes that are built into ggplot, or you can install other themes from other packages, or you can edit aspects yourself. I’d recommend starting with the ggthemes package for some fun ones, but I tend to just use classic or minimal. Remember that you must always refer to your graphs in your text (Figure 5.2). example_data %&gt;% ggplot(mapping = aes(x = age_died)) + geom_bar() + theme_minimal() + labs(x = &quot;Age died&quot;, y = &quot;Number&quot;, title = &quot;Number of people who died at each age&quot;, caption = &quot;Source: Simulated data.&quot;) Figure 5.2: Number of people who died at each age We may want to facet by some variable, in this case whether the person is a smoker (Figure 5.3). example_data %&gt;% ggplot(mapping = aes(x = age_died)) + geom_bar() + theme_minimal() + facet_wrap(vars(smoker)) + labs(x = &quot;Age died&quot;, y = &quot;Number&quot;, title = &quot;Number of people who died at each age, by whether they smoke&quot;, caption = &quot;Source: Simulated data.&quot;) Figure 5.3: Number of people who died at each age, by whether they smoke Alternatively, we may wish to colour by that instead (Figure 5.4). I’ll filter to just a handful of age-groups to keep it tractable. example_data %&gt;% filter(age_died &lt; 25) %&gt;% ggplot(mapping = aes(x = age_died, fill = smoker)) + geom_bar(position = &quot;dodge&quot;) + theme_minimal() + labs(x = &quot;Age died&quot;, y = &quot;Number&quot;, fill = &quot;Smoker&quot;, title = &quot;Number of people who died at each age, by whether they smoke&quot;, caption = &quot;Source: Simulated data.&quot;) Figure 5.4: Number of people who died at each age, by whether they smoke It’s important to recognise that a boxplot hides the full distribution of a variable. Unless you need to communicate the general distribution of many variables at once then you should not use them. The same box plot can apply to very different distributions. 5.2.2 Scatter plot Often, we are also interested in the relationship between two series. We’ll do that with a scatter plot. In this case, let’s simulate some data, say years of education and income. set.seed(853) number_of_observation &lt;- 500 scatter_data &lt;- tibble(years_of_education = runif(n = number_of_observation, min = 10, max = 25), error = rnorm(n= number_of_observation, mean = 0, sd = 10000), ) %&gt;% mutate(income = years_of_education * 5000 + error, income = if_else(income &lt; 0, 0, income)) head(scatter_data) ## # A tibble: 6 x 3 ## years_of_education error income ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 15.4 -13782. 63180. ## 2 11.8 7977. 66985. ## 3 17.3 -9787. 76498. ## 4 14.7 12999. 86689. ## 5 10.6 -1500. 51302. ## 6 16.1 1911. 82202. Now let’s look at income as a function of years of education (Figure 5.5). scatter_data %&gt;% ggplot(mapping = aes(x = years_of_education, y = income)) + geom_point() + theme_minimal() + labs(x = &quot;Years of education&quot;, y = &quot;Income&quot;, title = &quot;Relationship between income and years of education&quot;, caption = &quot;Source: Simulated data.&quot;) Figure 5.5: Relationship between income and years of education 5.2.3 Other 5.2.3.1 Best fit If we’re interested in quickly adding a line of best fit then, continuing with the earlier income example, we can do that with geom_smooth() (Figure 5.6). scatter_data %&gt;% ggplot(mapping = aes(x = years_of_education, y = income)) + geom_point() + geom_smooth(method = lm, color = &quot;black&quot;) + theme_minimal() + labs(x = &quot;Years of education&quot;, y = &quot;Income&quot;, title = &quot;Relationship between income and years of education&quot;, caption = &quot;Source: Simulated data.&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 5.6: Relationship between income and years of education 5.2.3.2 Histogram If we want to get counts by groups then we may want to use a histogram. Figure 5.7 shows the counts for our simulated incomes. scatter_data %&gt;% ggplot(mapping = aes(x = income)) + geom_histogram() + theme_minimal() + labs(x = &quot;Income&quot;, y = &quot;Number&quot;, title = &quot;Distribution of income&quot;, caption = &quot;Source: Simulated data.&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 5.7: Distribution of income 5.2.3.3 Multiple plots Finally, let’s try putting them together. We’re going to use the patchwork package (Pedersen 2020) and the penguins package for data. Don’t forget install.packages(\"palmerpenguins\") as this is probably the first time you’ve used the package. library(patchwork) library(palmerpenguins) p1 &lt;- ggplot(palmerpenguins::penguins) + geom_point(aes(bill_length_mm, bill_depth_mm)) + labs(x = &quot;Bill length (mm)&quot;, y = &quot;Bill depth (mm)&quot;) p2 &lt;- ggplot(palmerpenguins::penguins) + geom_bar(aes(species)) + labs(x = &quot;Species&quot;, y = &quot;Number&quot;) p1 + p2 And we can make things fairly involved fairly quickly. (p1 | p2) / p2 5.3 Tables Tables are also critical to tell a compelling story. We may prefer a table to a graph when there are only a few features that we want to focus on. We’ll use knitr::kable() alongside the ‘kableExtra’ package and also the gt package. Let’s start with the kable package and the summary dinosaur data from earlier. example_data &lt;- datasaurus_dozen %&gt;% filter(dataset %in% c(&quot;dino&quot;, &quot;star&quot;, &quot;away&quot;)) %&gt;% group_by(dataset) %&gt;% summarize( Mean = mean(x), Std_dev = sd(x), ) ## `summarise()` ungrouping output (override with `.groups` argument) example_data %&gt;% knitr::kable() dataset Mean Std_dev away 54.26610 16.76983 dino 54.26327 16.76514 star 54.26734 16.76896 Even the defaults are pretty good, but we can add a few tweaks to make the table better. The first is that this many significant digits is inappropriate, we may also like to add a caption, make the column names consistent, and change the alignment. example_data %&gt;% knitr::kable(digits = 2, caption = &quot;My first table.&quot;, col.names = c(&quot;Dataset&quot;, &quot;Mean&quot;, &quot;Standard deviation&quot;), align = c(&#39;l&#39;, &#39;l&#39;, &#39;l&#39;) ) Table 5.1: My first table. Dataset Mean Standard deviation away 54.27 16.77 dino 54.26 16.77 star 54.27 16.77 The ‘’kableExtra’ package builds extra functionality (Zhu 2020). The gt package (Iannone, Cheng, and Schloerke 2020a) is a newer package that brings a lot of exciting features. However, being newer it sometimes has issues with PDF output. library(gt) example_data %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #xrpdvwsnpk .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #xrpdvwsnpk .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #xrpdvwsnpk .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #xrpdvwsnpk .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #xrpdvwsnpk .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #xrpdvwsnpk .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #xrpdvwsnpk .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #xrpdvwsnpk .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #xrpdvwsnpk .gt_column_spanner_outer:first-child { padding-left: 0; } #xrpdvwsnpk .gt_column_spanner_outer:last-child { padding-right: 0; } #xrpdvwsnpk .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #xrpdvwsnpk .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #xrpdvwsnpk .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #xrpdvwsnpk .gt_from_md > :first-child { margin-top: 0; } #xrpdvwsnpk .gt_from_md > :last-child { margin-bottom: 0; } #xrpdvwsnpk .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #xrpdvwsnpk .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #xrpdvwsnpk .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #xrpdvwsnpk .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #xrpdvwsnpk .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #xrpdvwsnpk .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #xrpdvwsnpk .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #xrpdvwsnpk .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #xrpdvwsnpk .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #xrpdvwsnpk .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #xrpdvwsnpk .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #xrpdvwsnpk .gt_sourcenote { font-size: 90%; padding: 4px; } #xrpdvwsnpk .gt_left { text-align: left; } #xrpdvwsnpk .gt_center { text-align: center; } #xrpdvwsnpk .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #xrpdvwsnpk .gt_font_normal { font-weight: normal; } #xrpdvwsnpk .gt_font_bold { font-weight: bold; } #xrpdvwsnpk .gt_font_italic { font-style: italic; } #xrpdvwsnpk .gt_super { font-size: 65%; } #xrpdvwsnpk .gt_footnote_marks { font-style: italic; font-size: 65%; } dataset Mean Std_dev away 54.26610 16.76982 dino 54.26327 16.76514 star 54.26734 16.76896 We could add sub-titles easily. example_data %&gt;% gt() %&gt;% tab_header( title = &quot;Summary stats can be misleading&quot;, subtitle = &quot;With an example from a dinosaur!&quot; ) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #acmszxjctj .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #acmszxjctj .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #acmszxjctj .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #acmszxjctj .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #acmszxjctj .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #acmszxjctj .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #acmszxjctj .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #acmszxjctj .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #acmszxjctj .gt_column_spanner_outer:first-child { padding-left: 0; } #acmszxjctj .gt_column_spanner_outer:last-child { padding-right: 0; } #acmszxjctj .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #acmszxjctj .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #acmszxjctj .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #acmszxjctj .gt_from_md > :first-child { margin-top: 0; } #acmszxjctj .gt_from_md > :last-child { margin-bottom: 0; } #acmszxjctj .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #acmszxjctj .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #acmszxjctj .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #acmszxjctj .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #acmszxjctj .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #acmszxjctj .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #acmszxjctj .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #acmszxjctj .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #acmszxjctj .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #acmszxjctj .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #acmszxjctj .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #acmszxjctj .gt_sourcenote { font-size: 90%; padding: 4px; } #acmszxjctj .gt_left { text-align: left; } #acmszxjctj .gt_center { text-align: center; } #acmszxjctj .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #acmszxjctj .gt_font_normal { font-weight: normal; } #acmszxjctj .gt_font_bold { font-weight: bold; } #acmszxjctj .gt_font_italic { font-style: italic; } #acmszxjctj .gt_super { font-size: 65%; } #acmszxjctj .gt_footnote_marks { font-style: italic; font-size: 65%; } Summary stats can be misleading With an example from a dinosaur! dataset Mean Std_dev away 54.26610 16.76982 dino 54.26327 16.76514 star 54.26734 16.76896 One common reason for needing a table is to report regression results. You should consider gtsummary, stargazer, and modelsummary. But at the moment, my favourite is modelsummary (Arel-Bundock 2020). library(modelsummary) mod &lt;- lm(y ~ x, datasaurus_dozen) modelsummary(mod) Model 1 (Intercept) 53.590 (2.119) x -0.106 (0.037) Num.Obs. 1846 R2 0.004 R2 Adj. 0.004 AIC 17383.0 BIC 17399.6 Log.Lik. -8688.506 F 8.072 5.4 Maps TODO: Add something about geocoding. In many ways maps can be thought of as a fancy graph, where the x-axis is latitude, the y-axis is longitude, and there is some outline or a background image. We are used to this type of set-up, for instance, in a ggplot setting that is quite familiar. Static maps will be useful for printed output, such as a PDF or Word report, or where there is something in particular that you want to illustrate. ggplot() + geom_polygon( # First draw an outline data = some_data, aes(x = latitude, y = longitude, group = group )) + geom_point( # Then add points of interest data = some_other_data, aes(x = latitude, y = longitude) ) And while there are some small complications, for the most part it is as straight-forward as that. The first step is to get some data. And helpfully, there is some geographic data built into ggplot, and there is some other information built into a package called maps. library(maps) library(tidyverse) canada &lt;- map_data(database = &quot;world&quot;, regions = &quot;canada&quot;) canadian_cities &lt;- maps::canada.cities head(canada) ## long lat group order region subregion ## 1 -59.78760 43.93960 1 1 Canada Sable Island ## 2 -59.92227 43.90391 1 2 Canada Sable Island ## 3 -60.03775 43.90664 1 3 Canada Sable Island ## 4 -60.11426 43.93911 1 4 Canada Sable Island ## 5 -60.11748 43.95337 1 5 Canada Sable Island ## 6 -59.93604 43.93960 1 6 Canada Sable Island head(canadian_cities) ## name country.etc pop lat long capital ## 1 Abbotsford BC BC 157795 49.06 -122.30 0 ## 2 Acton ON ON 8308 43.63 -80.03 0 ## 3 Acton Vale QC QC 5153 45.63 -72.57 0 ## 4 Airdrie AB AB 25863 51.30 -114.02 0 ## 5 Aklavik NT NT 643 68.22 -135.00 0 ## 6 Albanel QC QC 1090 48.87 -72.42 0 With that information in hand we can then create a map of Canada that shows the cities with a population over 1,000. (The geom_polygon() function within ggplot draws shapes, by connecting points within groups. And the coord_map() function adjusts for the fact that we are making something that is 2D map to represent something that is 3D.) ggplot() + geom_polygon(data = canada, aes(x = long, y = lat, group = group), fill = &quot;white&quot;, colour = &quot;grey&quot;) + coord_map(ylim = c(40, 70)) + geom_point(aes(x = canadian_cities$long, y = canadian_cities$lat), alpha = 0.3, color = &quot;black&quot;) + theme_classic() + labs(x = &quot;Longitude&quot;, y = &quot;Latitude&quot;) # If I&#39;m being honest, this &#39;simple example&#39; took me six hours to work out. Firstly # to find Canada and then to find Canadian cities. As is often the case with R, there are many different ways to get started create static maps. We’ve already seen how they can be built using simply ggplot, but here we’ll explore one package that has a bunch of functionality built in that will make things easier: ggmap. There are two essential components to a map: 1) some border or background image (also known as a tile); and 2) something of interest within that border or on top of that tile. In ggmap, we will use an open source option for our tile, Stamen Maps (maps.stamen.com), and we will use plot points based on latitude and longitude. 5.4.1 Australian polling places Like Canada, in Australia people go to specific locations, called booths, to vote. These booths have latitudes and longitudes and so we can plot these. One reason we may like to do this is to notice patterns over geographies. To get started we need to get a tile. We are going to use ggmap to get a tile from Stamen Maps, which builds on OpenStreetMap (openstreetmap.org). The main argument to this function is to specify a bounding box. This requires two latitudes - one for the top of the box and one for the bottom of the box - and two longitudes - one for the left of the box and one for the right of the box. (It can be useful to use Google Maps, or an alternative, to find the values of these that you need.) The bounding box provides the coordinates of the edges that you are interested in. In this case I have provided it with coordinates such that it will be centered around Canberra, Australia (our equivalent of Ottawa - a small city that was created for the purposes of being the capital). library(ggmap) bbox &lt;- c(left = 148.95, bottom = -35.5, right = 149.3, top = -35.1) Once you have defined the bounding box, then the function get_stamenmap() will get the tiles in that area. The number of tiles that it needs to get depends on the zoom, and the type of tiles that it gets depends on the maptype. I’ve chosen the maptype that I like here - the black and white option - but the helpfile specifies a few others that you may like. At this point you can pass your maps to ggmap and it will plot the tile! It will be actively downloading these tiles, so you need an internet connection. canberra_stamen_map &lt;- get_stamenmap(bbox, zoom = 11, maptype = &quot;toner-lite&quot;) ggmap(canberra_stamen_map) Once we have a map then we can use ggmap() to plot it. (That circle in the middle of the map is where the Australian Parliament House is… yes, our parliament is surrounded by circular roads (we call them ‘roundabouts’), actually it’s surrounded by two of them.) Now we want to get some data that we will plot on top of our tiles. We will just plot the location of the polling places, based on which ‘division’ (the Australian equivalent to ‘ridings’ in Canada) it is. This is available here: https://results.aec.gov.au/20499/Website/Downloads/HouseTppByPollingPlaceDownload-20499.csv. (The Australian Electoral Commission (AEC) is the official government agency that is responsible for elections in Australia.) # Read in the booths data for each year booths &lt;- readr::read_csv(&quot;https://results.aec.gov.au/24310/Website/Downloads/GeneralPollingPlacesDownload-24310.csv&quot;, skip = 1, guess_max = 10000) head(booths) ## # A tibble: 6 x 15 ## State DivisionID DivisionNm PollingPlaceID PollingPlaceTyp… PollingPlaceNm ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 ACT 318 Bean 93925 5 Belconnen BEA… ## 2 ACT 318 Bean 93927 5 BLV Bean PPVC ## 3 ACT 318 Bean 11877 1 Bonython ## 4 ACT 318 Bean 11452 1 Calwell ## 5 ACT 318 Bean 8761 1 Chapman ## 6 ACT 318 Bean 8763 1 Chisholm ## # … with 9 more variables: PremisesNm &lt;chr&gt;, PremisesAddress1 &lt;chr&gt;, ## # PremisesAddress2 &lt;chr&gt;, PremisesAddress3 &lt;chr&gt;, PremisesSuburb &lt;chr&gt;, ## # PremisesStateAb &lt;chr&gt;, PremisesPostCode &lt;chr&gt;, Latitude &lt;dbl&gt;, ## # Longitude &lt;dbl&gt; This dataset is for the whole of Australia, but as we are just going to plot the area around Canberra we will filter to that and only to booths that are geographic (the AEC has various options for people who are in hospital, or not able to get to a booth, etc, and these are still ‘booths’ in this dataset). # Reduce the booths data to only rows with that have latitude and longitude booths_reduced &lt;- booths %&gt;% filter(State == &quot;ACT&quot;) %&gt;% select(PollingPlaceID, DivisionNm, Latitude, Longitude) %&gt;% filter(!is.na(Longitude)) %&gt;% # Remove rows that don&#39;t have a geography filter(Longitude &lt; 165) # Remove Norfolk Island Now we can use ggmap in the same way as before to plot our underlying tiles, and then build on that using geom_point() to add our points of interest. ggmap(canberra_stamen_map, extent = &quot;normal&quot;, maprange = FALSE) + geom_point(data = booths_reduced, aes(x = Longitude, y = Latitude, colour = DivisionNm), ) + scale_color_brewer(name = &quot;2019 Division&quot;, palette = &quot;Set1&quot;) + coord_map(projection=&quot;mercator&quot;, xlim=c(attr(map, &quot;bb&quot;)$ll.lon, attr(map, &quot;bb&quot;)$ur.lon), ylim=c(attr(map, &quot;bb&quot;)$ll.lat, attr(map, &quot;bb&quot;)$ur.lat)) + labs(x = &quot;Longitude&quot;, y = &quot;Latitude&quot;) + theme_minimal() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) We may like to save the map so that we don’t have to draw it every time, and we can do that in the same way as any other graph, using ggsave(). ggsave(&quot;outputs/figures/map.pdf&quot;, width = 20, height = 10, units = &quot;cm&quot;) Finally, the reason that I used Stamen Maps and OpenStreetMap is because it is open source, however you can also use Google Maps if you want. This requires you to first register a credit card with Google, and specify a key, but with low usage should be free. The get_googlemap() function with ggmap, brings some nice features that get_stamenmap() does not have. For instance, you can enter a placename and it’ll do it’s best to find it rather than needing to specify a bounding box. 5.4.2 Toronto bike parking Let’s see another example of a static map, this time using Toronto data accessed via the opendatatoronto package. The dataset that we are going to plot is available here: https://open.toronto.ca/dataset/street-furniture-bicycle-parking/. # This code is based on code from: https://open.toronto.ca/dataset/street-furniture-bicycle-parking/. library(opendatatoronto) # (The string identifies the package.) resources &lt;- list_package_resources(&quot;71e6c206-96e1-48f1-8f6f-0e804687e3be&quot;) # In this case there is only one dataset within this resource so just need the first one raw_data &lt;- filter(resources, row_number()==1) %&gt;% get_resource() write_csv(raw_data, &quot;inputs/data/bike_racks.csv&quot;) head(raw_data) Now that we’ve saved a copy of the data, we can use that one. First we need to clean it up a bit. There are some clear errors in the ADDRESSNUMBERTEXT field, but not too many, so we’ll just ignore it. raw_data &lt;- read_csv(&quot;inputs/data/bike_racks.csv&quot;) # We&#39;ll just focus on the data that we want bike_data &lt;- tibble(ward = raw_data$WARD, id = raw_data$ID, status = raw_data$STATUS, street_address = paste(raw_data$ADDRESSNUMBERTEXT, raw_data$ADDRESSSTREET), latitude = raw_data$LATITUDE, longitude = raw_data$LONGITUDE) rm(raw_data) Some of the bike racks were temporary so remove them and also let’s just look at the area around the university, which is Ward 11 # Only keep ones that still exist bike_data &lt;- bike_data %&gt;% filter(status == &quot;Existing&quot;) %&gt;% select(-status) bike_data &lt;- bike_data %&gt;% filter(ward == 11) %&gt;% select(-ward) If you look at the dataset at this point then you’ll notice that there is a row for every bike parking spot. But we don’t really need to know that, because sometimes there are lots right next to each other. Instead we’d just like the one point (we’ll take advantage of this in an interactive graph in a moment). So we want to create a count by address, and then just get one instance per address. bike_data &lt;- bike_data %&gt;% group_by(street_address) %&gt;% mutate(number_of_spots = n(), running_total = row_number() ) %&gt;% ungroup() %&gt;% filter(running_total == 1) %&gt;% select(-id, -running_total) head(bike_data) ## # A tibble: 6 x 4 ## street_address latitude longitude number_of_spots ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 8 Kensington Ave 43.7 -79.4 1 ## 2 87 Avenue Rd 43.7 -79.4 4 ## 3 162 Mc Caul St 43.7 -79.4 1 ## 4 147 Baldwin St 43.7 -79.4 2 ## 5 888 Yonge St 43.7 -79.4 1 ## 6 180 Elizabeth St 43.7 -79.4 10 write_csv(bike_data, &quot;outputs/data/bikes.csv&quot;) Now we can grab our tile, and add our bike rack data onto it. bbox &lt;- c(left = -79.420390, bottom = 43.642658, right = -79.383354, top = 43.672557) toronto_stamen_map &lt;- get_stamenmap(bbox, zoom = 14, maptype = &quot;toner-lite&quot;) ggmap(toronto_stamen_map, maprange = FALSE) + geom_point(data = bike_data, aes(x = longitude, y = latitude), alpha = 0.3 ) + labs(x = &quot;Longitude&quot;, y = &quot;Latitude&quot;) + theme_minimal() 5.5 Writing 5.5.1 Title, abstract, and introduction A title is the first opportunity that you have to tell the reader your story. Ideally you will tell the reader exactly what you found. An effective title is critical in order to get your work read when there are other competing priorities. A title doesn’t have to be ‘cute’ to be great. Good: ‘On the 2019 Canadian Federal Election’. (At least the reader knows what the paper is about.) Better: ‘The Liberal Party performance in the 2019 Canadian Federal Election’. (At least the reader knows what the paper is about more specifically.) Even better: ‘The Liberal Party did poorly in rural areas in the 2019 Canadian Federal Election’. (The reader knows what the paper is about.) You should put your name and the date on the paper because this provides an important context to the paper. For a six-page paper, a good abstract is a three to five sentence paragraph. For a longer paper your abstract can be slightly longer. The abstract should say: What you did, what you found, and why the reader should care. Each of these should just be a sentence or two, so keep it very high level. You should then have an introduction that tells the reader everything they need to know. You are not writing a mystery story - tell the reader the most important points in the introduction. For a six-page paper, your introduction may be two or three paragraphs. Four would likely be too much, but it depends on the context. Your introduction should set the scene and give the reader some background. For instance, you may like to start of a little broader, to provide some context to your paper. You should then describe how your paper fits into that context. Then give some high-level results - provide more detail than you provided in the abstract, but don’t get into the weeds - and finally broadly discuss next steps or glaring weaknesses. With regard to that high-level result: you need to pick one. If you have a bunch of interesting findings, then good for you, but pick one and write your introduction around that. If it’s compelling enough then the reader will end up reading all your other interesting findings in the discussion/results sections. Finally, you should highlight the remainder of the paper. As an example: The Canadian Liberal Party has always struggled in rural ridings. In the past 100 years they have never won more than 25 per cent of them. But even by those standards the 2019 Federal Election was a disappointment with the Liberal Party winning only 2 of the 40 rural ridings. In this paper we look at why the performance of the Liberal Party in this most recent election was so poor. We construct a model in which whether the Liberal Party won the riding is explained by the number of farms in the riding, the average internet connectivity, and the median age. We find that as the median age of a riding increases, the likelihood that a riding was won by the Liberal Party decreases by 14 percentage points. Future work could expand the time horizon that is considered which would allow a more nuanced understanding of these effects. The remainder of this paper is structured as follows: Section 2 discusses the data, Section 3 discusses the model, Section 4 presents the results, and finally Section 5 discusses our findings and some weaknesses. The recommended readings provide some lovely examples of titles, abstracts, and introductions. Please take the time to briefly read these papers. 5.5.2 Figures, tables, equations, and technical terms Figure and tables are a critical aspect of convincing people of your story. In a graph you can show your data and then let people decide for themselves. And in a table you can more easily summarise your data. Figures, tables, equations, etc, should be numbered and then referenced in the text e.g. “Figure 1 shows…” and then have Figure 1. You should make sure that all aspects of your graph are legible. Always label all of the axes. Your graphs should have titles, and the point that you want to communicate should be clear. If you use a technical term, then it should be briefly explained in plain language for readers who might not be familiar with it. A great example of this is this post by Monica Alexander where she explains the Gini coefficient: To look at the concentration of baby names, let’s calculate the Gini coefficient for each country, sex and year. The Gini coefficient measures dispersion or inequality among values of a frequency distribution. It can take any value between 0 and 1. In the case of income distributions, a Gini coefficient of 1 would mean one person has all the income. In this case, a Gini coefficient of 1 would mean that all babies have the same name. In contrast, a Gini coefficient of 0 would mean names are evenly distributed across all babies. 5.5.3 On brevity Figure 5.8: ‘No more than four pages, or he’s never going to read it. Two pages is preferable.’ Source: Shipman, Tim, 2020, \"The prime minister’s vanishing briefs’, The Sunday Times, 23 February, available at: https://www.thetimes.co.uk/article/the-prime-ministers-vanishing-briefs-67mt0bg95 via Sarah Nickson. Insisting on two page briefs is sensible - not ‘government by ADHD’. PM has to be across lots of issues - cannot and should not be across (most of) them in the same depth as secretaries of state. Danger lies in PM trying to take on too much and getting bogged down in detail. This might irk officials who lack a sense of where their issue sits within the PM’s list of priorities - or the writing skills to draft a succinct brief. But there’d be very few occasions when a brief to the PM warrants more than two pages. This is not something peculiar to the current PM - other ministers have raised the same in interviews with @instituteforgov Oliver Letwin complained of ‘huge amount of terrible guff, at huge, colossal, humungous length coming from some departments’ https://www.instituteforgovernment.org.uk/ministers-reflect/person/oliver-letwin/ Letwin sent briefs back and asked they be re-drafted to one quarter of the length. ‘Somewhere along the line the Civil Service had got used to splurge of the meaningless kind’ Similarly, Theresa Villiers talked about the civil service’s ‘frustrating tendency to produce six pages of obscure and rather impenetrable text’ and wishes she’d be firmer in sending documents back for re-drafting: https://www.instituteforgovernment.org.uk/ministers-reflect/person/theresa-villiers/ Sarah Nickson, 23 Feb 2020. Brevity is important. Partly this because you are writing for the reader, not yourself, and your reader has other priorities. But it is also because as the writer it focuses you to consider what your most important points are, how you can best support them, and where your arguments are weakest. If you don’t think that examples from government are persuasive, then please consider Amazon’s 2017 Letter to Shareholders, or other statements about Bezos and memo writing, for instance: Well structured, narrative text is what we’re after rather than just text… The reason writing a 4 page memo is harder than “writing” a 20 page powerpoint is because the narrative structure of a good memo forces better thought and better understanding of what’s more important than what, and how things are related. Jeff Bezos, 9 June 2004. 5.5.4 Other Typos and other grammatical mistakes affect the credibility of your claims. If the reader can’t trust you to use a spell-checker then why should they trust you to use logistic regression? Microsoft Word has a fantastic spell-checker that is much better than what is available for R Markdown: copy/paste your work into there, look for the red lines and fix them in your R Markdown. Then look for the green lines and think about if you need to fix them in your R Markdown. If you don’t have Word then Google Docs is pretty good and so is Apple’s Pages. A few other general tips that I have stolen from various people including the Reserve Bank of Australia’s style guide: Think about what you are writing. Aim to write everything as though it were on the front page of the newspaper, because one day it could be. Be concise. Remove as many words as possible. Be direct. Think about the structure of your story, and identify the key pieces of information and arrange them so that your paper flows logically from one to the next. You should use sub-headings if you need. Be precise. For instance, the stock-market didn’t improve or worsen, it rose or fell. Distinguish levels from rates of change. Be clear. Write simply. Use short sentence where possible. Avoid jargon. You should break these rules when you need to. But the only way to know whether you need to break a rule is to know the rules in the first instance. References "],["interactive-communication.html", "Chapter 6 Interactive communication 6.1 Making a website 6.2 Interactive maps 6.3 Interactive maps 6.4 Shiny", " Chapter 6 Interactive communication THIS ENTIRE CHAPTER NEEDS TO BE UPDATED AND RE-DONE. 6.1 Making a website 6.1.1 Getting started with Blogdown Required reading Hill, Alison, 2017, ‘Up &amp; Running with blogdown’, 12 June, freely available at: https://alison.rbind.io/post/2017-06-12-up-and-running-with-blogdown/. Salmon, Maëlle, 2020, ‘What to know before you adopt Hugo/blogdown’, 29 February, freely available at: https://masalmon.eu/2020/02/29/hugo-maintenance/. Xie, Yihui, Amber Thomas, and Alison Presmanes Hill, 2020, blogdown: Creating Websites with R Markdown’, as at 6 February, freely available at: https://bookdown.org/yihui/blogdown/. Recommended reading Hill, Alison, 2019, ‘A Spoonful of Hugo: Troubleshooting Your Build’, 4 March, freely available at: https://alison.rbind.io/post/2019-03-04-hugo-troubleshooting/. De Leon, Desirée, 2019, ‘Trying out Blogdown’, 4 September, freely available at: http://desiree.rbind.io/post/2019/trying-out-blogdown/. Navarro, Danielle, 2018, ‘Day 1: Getting started with blogdown’, 27 April, freely available at: https://djnavarro.net/post/starting-blogdown/. Key concepts/skills/etc Building a website using blogdown and hugo. Key libraries blogdown tidyverse 6.1.2 Introduction These notes were originally developed for a workshop at the ANU delivered in 2017, and published to my website. Thank you to Minhee Chae and Peter Gibbard for helpful comments. A website is a critical part of communication. If you are searching for a job then it acts as one place to bring everything that you can do together. If you are using R, then you might like a website that makes it easy to share your work. This is where blogdown helps. blogdown is a package that allows you to make websites (not just blogs, notwithstanding its name) largely within R Studio. It builds on Hugo, which is a popular tool for making websites. blogdown lets you freely and quickly get a website up-and-running. It is easy to add content from time-to-time. It integrates with R Markdown which lets you easily share your work. And the separation of content and styling allows you to relatively quickly change your website’s design. That said, using blogdown is more work than Google sites or Squarespace. It requires a little more knowledge than using a basic Wordpress site. And if you want to customise many aspects of your website, or need everything to be ‘just so’ then blogdown may not be for you. blogdown is still under active development and various aspects may break in future releases. That said, the investment of time required to set up a blogdown website is unlikely to be wasted. Even if blogdown were shuttered tomorrow most of the content could be repurposed for a regular Hugo website. This post is a simplified version of the blogdown user-guide and the blog post by Alison Presmanes Hill. It sticks to the basics and doesn’t require much decision-making. The purpose is to allow someone without much experience to use blogdown to get a website up-and-running. Head to those two resources once you’ve got a website working and want to dive a bit deeper. 6.1.3 Foundations To use blogdown you need R and R Studio, but if you have made it this far in the course then you probably know that. We’ll install the blogdown package then use GitHub to create a new folder where we’ll create our website. First install blogdown: install.packages(&quot;blogdown&quot;) Now we want to create a folder in GitHub (because it will be easier to put your website onto the internet if you have a GitHub account). We have seen GitHub earlier in this notes, but if you didn’t do this at that point, then please create a free GitHub account at https://github.com/. Once you have an account, create a new repository by clicking on the plus and call it ‘my_website’ (or whatever you want). Don’t worry about including a readme or gitignore. Once you get to the ‘Quick setup’ page, copy the website address. At this point, we want to get that folder onto our own computer. So open RStudio, select Files, New Project, Version Control, Git, and paste the information for the repo. Go through the rest of it, saving the folder somewhere sensible, and clicking ‘Open in new session’. This will then create a new folder on your computer which will be a Git folder that is linked to the GitHub repo that you created. We can now construct a frame for our website in that folder. 6.1.4 Build the frame Open R Studio and install Hugo via the blogdown package with the following code: blogdown::install_hugo() In R Studio create a new project in the folder that you just created ‘my_website’. To do this click on: File -&gt; New Project -&gt; Existing Directory. Then navigate to the folder ‘my_website’. This will open a new R Studio session. Creating a project just adds a .proj file in the folder that makes it easier to come back to your website later. Using that new R Studio session create your website with the following code: blogdown::new_site(theme = &quot;gcushen/hugo-academic&quot;, theme_example = TRUE) This will: download files into your ‘my_website’ folder; open a R Markdown file that you can close for now; and begin serving the site in your R Studio viewer. The console and viewer of your R Studio session should look like this: Now that we have a frame, we can add our own content. 6.1.5 Add content At this point, the default website is being ‘served’ locally. This means that changes you make will be reflected in the website that you see in your R Studio Viewer. To see the website in a web browser click the ‘show in new window’ button on the top left of the Viewer. This is circled in the above image. That will open the website using the address that the R Studio also tells you. 6.1.5.1 Headshot The first change to make is to update the headshot. In your folder, go to my_website -&gt; static -&gt; img. Replace ‘portrait.jpg’ with your own square headshot jpg. If you do this correctly then when you go back to your website the image will have updated. 6.1.5.2 Personal details, contacts, and main menu To update the biography and other details in that first pane, go to File -&gt; Open File in the R Studio menu and open config.toml which is in my_website -&gt; config.toml. This file will either open in a text editor or in R Studio – it doesn’t matter which. When you save the file the changes will be reflected in the website. Search for ‘title’ or go to line 2. It should say: &#39;title = &quot;Academic&quot;&#39; Change that to: &#39;title = &quot;Your Name&quot;&#39; Search for ‘[params]’ or go to line 21. There you can update parameters such as name, role, and contact details. If you don’t want a particular parameter to show up on your website then set it equal to \"\". (An example of this is on line 33.) Once you’ve updated these parameters, search for ‘[[params.social]]’ or go to line 126. There you can update your contact details, such as email, twitter, etc. Just delete or comment out the full four lines if you don’t want a particular contact type displayed on your website. You can always add more later. Finally, search for ‘[[menu.main]]’ or go to line 152. There you can change the menu items that are displayed on the top right of your website. For instance if you don’t want a blog then delete or comment out the four lines: [[menu.main]] name = &quot;Posts&quot; url = &quot;#posts&quot; weight = 3 If you want to change the order of the items then change the ‘weight’. Ascending values from left to right. 6.1.5.3 Biography In your folder, go to my_website -&gt; content -&gt; home -&gt; about.md. That should open in R Studio or your text editor. Any changes that you save should immediately show up in your website. Search for ‘# List your academic interests.’ or go to line 12. There you can change your academic interests. If you don’t want this to show up on your website then you can just delete or comment out lines 12-18. Search for ‘# List your qualifications (such as academic degrees).’ or go to line 20. There you can change your academic qualification. If you don’t want this to show up on your website then you can just delete or comment out these lines. The ‘year’ is a numeric field. If you’d prefer to include duration (e.g. 2013 – 2017), then replace the ‘2012’ with ‘“2013 – 2017”’ (the \"\" are important). Or similarly, if you are expecting a degree then you could replace the ‘year’ with ‘“Expected month year”’. Search for ‘# Biography’ or go to line 43. There you can add a brief biography. 6.1.5.4 Teaching Most of the other files in my_website -&gt; content -&gt; home just display content from elsewhere. This is because of the setup of the website. The exception is teaching.md. Open that and edit everything after line 15. 6.1.5.5 Publications In your folder, go to my_website -&gt; content -&gt; publication. There are two default publications added there. You can edit those and then copy them to add extra publications. 6.1.5.6 Posts If you want a blog in your website then the content is saved in: my_website -&gt; content -&gt; post. If you don’t want a blog then just delete this folder and comment out the posts menu item from my_website -&gt; config.toml file so it doesn’t show up in the menu. Once your website is working, if you want a new blog post, then you can simply use the R Studio menu bar: Tools -&gt; Addins -&gt; New Post. 6.1.5.7 Etc Go through the different parts and change it as you need. 6.1.5.8 Subsequent editing To come back to editing your website once you’ve closed R Studio, go to the ‘my_website’ folder and then double-click on the Rproj file, ‘blogdown_test.Rproj’. That will open a new instance of R Studio. From there you can type ‘blogdown:::serve_site()’ into the console to serve your site and then continue editing, or you could use the R Studio menu bar: Tools -&gt; Addins -&gt; Serve Site. 6.1.6 Making your website public 6.1.6.1 Commit So far everything has happened on your own computer. The first step to making your website public is to commit these changes to GitHub. To do this open Terminal again and as before use cd and ls to navigate to ‘my_website’. Once there, type each of the following lines (adding your own description) and follow each by ‘return’ git add -A git commit -m &quot;DESCRIBE THE CHANGE YOU ARE ADDING&quot; git push (You may be asked for your GitHub password. Terminal is a bit tricky to type passwords into because you don’t know how many characters you’ve typed, but have a go and follow it by ‘return’.) 6.1.6.2 Netlify There are many ways to make your website public, but one way is to use Netlify. What we are going to do is to link GitHub and Netlify, so that when you make a change in GitHub then Netlify automatically updates. Once you have an account then click “New site from Git” and at that point you can link your GitHub account. Once it has been authorised, then you should select the repo that you want to make public. The publish director is ‘public’. Once this is all specified then you can “Deploy site”. Netlify gives you a default address, but you can change this. However it will still have .netlify.com. To get rid of this you need to purchase a domain, and then go through the custom domains setting in Netlify. 6.2 Interactive maps Required reading Cooley, David, 2020, ‘mapdeck’, freely available at: https://symbolixau.github.io/mapdeck/index.html. Kolb, Jan-Philipp, 2019, ‘Using Web Services to Work with Geodata in R’, The R Journal, 11:2, pages 6-23, freely available at: https://journal.r-project.org/archive/2019/RJ-2019-041/index.html. Gabrielle, 2019, ‘Visualising spatial data using sf and mapdeck - part one’, 4 December, freely available at: https://resources.symbolix.com.au/2019/12/04/mapdeck-1/. ‘Leaflet for R’, freely available at: https://rstudio.github.io/leaflet/. Required viewing Kuriwaki, Shiro, 2020, ‘Making maps in R with sf’, 1 March, freely available at: https://vimeo.com/394800836. Key concepts/skills/etc Thinking of maps as a (often fiddly, but strangely enjoyable) variant of a usual ggplot. Broadening the data that we make available via interactive maps, while still telling a clear story. Becoming comfortable with (and excited about) creating both static and interactive maps. Key libraries ggmap leaflet maps mapdeck Key functions/etc add_arc() addCircleMarkers() addMarkers() addTiles() canada.cities geom_polygon() get_stamenmap() ggmap() leaflet() map() map_data() mapdeck() mapdeck_style() 6.3 Interactive maps The nice thing about interactive maps is that you can let your users decide what they are interested in. Additionally, if there is a lot of information then you may like to leave it to your users as to selectively focus on what they are interested in. For instance, in the case of Canadian politics, some people will be interested in Toronto ridings, while others will be interested in Manitoba, etc. But it would be difficult to present a map that focuses on both of those, so an interactive map is a great option for allowing users to zoom in on what they want. 6.3.0.1 Leaflet The leaflet package is originally a JavaScript library of the same name that has been brought over to R. It makes it easy to make interactive maps. The basics are fairly similar to the ggmap set-up, but of course after that, there are many, many, options. Let’s redo the bike map from earlier, and possibly the interaction will allow us to see what the issue is with the data. In the same way as a graph in ggplot begins with the ggplot() function, a map in the leaflet package begins with a call to the leaflet() function. This allows you to specify data, and a bunch of other options such as width and height. After this, we add ‘layers’, in the same way that we added them in ggplot. The first layer that we’ll add is a tile with the function addTiles(). In this case, the default is from OpenStreeMap. After that we’ll add markers that show the location of each bike parking spot with addMarkers(). library(leaflet) library(tidyverse) bike_data &lt;- read_csv(&quot;outputs/data/bikes.csv&quot;) leaflet(data = bike_data) %&gt;% addTiles() %&gt;% # Add default OpenStreetMap map tiles addMarkers(lng = bike_data$longitude, lat = bike_data$latitude, popup = bike_data$street_address, label = ~as.character(bike_data$number_of_spots)) There are two options here that may not be familiar. The first is popup, and this is what happens when you click on the marker. In this example this is giving the address. The second is label, which is what happens when you hover over the marker. In this example it is given the number of spots. 6.3.0.2 COVID-19 Let’s have another go, this time with Ontario data on COVID-19. We can download the latest data from the Ontario Data Catalogue. This is a fast moving situation in which they are likely to make breaking changes to this dataset. To ensure these notes work, I will save and then use the dataset as at 4 April 2020, but you are able to get the up-to-date dataset using the link and the code. ontario_covid &lt;- read_csv(&quot;https://data.ontario.ca/datastore/dump/455fd63b-603d-4608-8216-7d8647f43350?bom=True&quot;) write_csv(ontario_covid, &quot;inputs/data/ontario_covid_2020-04-04.csv&quot;) ontario_covid &lt;- read_csv(&quot;inputs/data/ontario_covid_2020-04-04.csv&quot;) head(ontario_covid) ## # A tibble: 6 x 14 ## `_id` ROW_ID ACCURATE_EPISODE_D… Age_Group CLIENT_GENDER CASE_ACQUISITIO… ## &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 1 2020-03-07 00:00:00 40s MALE Neither ## 2 2 2 2020-03-08 00:00:00 20s MALE Neither ## 3 3 3 2020-03-10 00:00:00 40s FEMALE Neither ## 4 4 4 2020-03-11 00:00:00 50s FEMALE Neither ## 5 5 5 2020-03-12 00:00:00 30s FEMALE Neither ## 6 6 6 2020-03-15 00:00:00 50s MALE Neither ## # … with 8 more variables: OUTCOME1 &lt;chr&gt;, Reporting_PHU &lt;chr&gt;, ## # Reporting_PHU_Address &lt;chr&gt;, Reporting_PHU_City &lt;chr&gt;, ## # Reporting_PHU_Postal_Code &lt;chr&gt;, Reporting_PHU_Website &lt;chr&gt;, ## # Reporting_PHU_Latitude &lt;dbl&gt;, Reporting_PHU_Longitude &lt;dbl&gt; There is a lot of information here, but we’ll just plot the number of cases, by the reporting area (health areas). So this isn’t the location of the person, but the location of the responsible health unit. Because of this, we’ll add a little bit of noise so that the marker for each person can be seen. We do this with jitter(). ontario_covid &lt;- ontario_covid %&gt;% mutate(Reporting_PHU_Latitude = jitter(Reporting_PHU_Latitude, amount = 0.1), Reporting_PHU_Longitude = jitter(Reporting_PHU_Longitude, amount = 0.1)) We will introduce a different type of marker here, which is circles. This will allow us to use different colours for the outcomes of each case. There are three possible outcomes: the case is resolved, it is not resolved, or it was fatal. library(leaflet) pal &lt;- colorFactor(&quot;Dark2&quot;, domain = ontario_covid$OUTCOME1 %&gt;% unique()) leaflet() %&gt;% addTiles() %&gt;% # Add default OpenStreetMap map tiles addCircleMarkers( data = ontario_covid, lng = ontario_covid$Reporting_PHU_Longitude, lat = ontario_covid$Reporting_PHU_Latitude, color = pal(ontario_covid$OUTCOME1), popup = paste(&quot;&lt;b&gt;Age-group:&lt;/b&gt;&quot;, as.character(ontario_covid$Age_Group), &quot;&lt;br&gt;&quot;, &quot;&lt;b&gt;Gender:&lt;/b&gt;&quot;, as.character(ontario_covid$CLIENT_GENDER), &quot;&lt;br&gt;&quot;, &quot;&lt;b&gt;Acquisition:&lt;/b&gt;&quot;, as.character(ontario_covid$CASE_ACQUISITIONINFO), &quot;&lt;br&gt;&quot;, &quot;&lt;b&gt;Episode date:&lt;/b&gt;&quot;, as.character(ontario_covid$ACCURATE_EPISODE_DATE), &quot;&lt;br&gt;&quot;) ) %&gt;% addLegend(&quot;bottomright&quot;, pal = pal, values = ontario_covid$OUTCOME1 %&gt;% unique(), title = &quot;Case outcome&quot;, opacity = 1 ) 6.3.0.3 mapdeck Thank you to Shaun Ratcliff for introducing me to mapdeck. The package mapdeck is an R package that is built on top of Mapbox (https://www.mapbox.com). It is based on WebGL, which means that your web browser does a lot of work for you. The nice thing is that because of this, it can do a bunch of things that leaflet struggles with, especially dealing with larger datasets. Mapbox is a full-featured application that many businesses that you may have heard of use: https://www.mapbox.com/showcase. To close out these notes on mapping, I want to briefly touch on mapdeck, as it is a newer, but very exciting, package. To this point we have used stamen maps as our tile, but mapdeck uses mapbox - https://www.mapbox.com/ - and so you need to register and get a token for this. (It’s free.) Once you have that token you add it to R using: library(mapdeck) set_token(&quot;asdf&quot;) # replace asdf with your token. mapdeck_tokens() set_token(test$key) (Don’t add it into your script otherwise everyone will be able to take it and use it, especially once you add it to GitHub.) Then we need some data. Here we’re going to just use the example dataset, which is about flights. # Code taken from the example: https://github.com/SymbolixAU/mapdeck library(mapdeck) url &lt;- &#39;https://raw.githubusercontent.com/plotly/datasets/master/2011_february_aa_flight_paths.csv&#39; flights &lt;- read.csv(url) flights$info &lt;- paste0(&quot;&lt;b&gt;&quot;,flights$airport1, &quot; - &quot;, flights$airport2, &quot;&lt;/b&gt;&quot;) head(flights) ## start_lat start_lon end_lat end_lon airline airport1 airport2 cnt ## 1 32.89595 -97.03720 35.04022 -106.60919 AA DFW ABQ 444 ## 2 41.97960 -87.90446 30.19453 -97.66987 AA ORD AUS 166 ## 3 32.89595 -97.03720 41.93887 -72.68323 AA DFW BDL 162 ## 4 18.43942 -66.00183 41.93887 -72.68323 AA SJU BDL 56 ## 5 32.89595 -97.03720 33.56294 -86.75355 AA DFW BHM 168 ## 6 25.79325 -80.29056 36.12448 -86.67818 AA MIA BNA 56 ## info ## 1 &lt;b&gt;DFW - ABQ&lt;/b&gt; ## 2 &lt;b&gt;ORD - AUS&lt;/b&gt; ## 3 &lt;b&gt;DFW - BDL&lt;/b&gt; ## 4 &lt;b&gt;SJU - BDL&lt;/b&gt; ## 5 &lt;b&gt;DFW - BHM&lt;/b&gt; ## 6 &lt;b&gt;MIA - BNA&lt;/b&gt; Finally, we can call the map. Again, this is just the example in the package’s website. mapdeck(style = mapdeck_style(&#39;dark&#39;) ) %&gt;% add_arc( data = flights , origin = c(&quot;start_lon&quot;, &quot;start_lat&quot;) , destination = c(&quot;end_lon&quot;, &quot;end_lat&quot;) , stroke_from = &quot;airport1&quot; , stroke_to = &quot;airport2&quot; , tooltip = &quot;info&quot; , layer_id = &#39;arclayer&#39; ) And this is pretty nice! 6.4 Shiny TBD "],["gather-data.html", "Chapter 7 Gather data 7.1 APIs 7.2 Case study - arXiv 7.3 Case study - rtweet 7.4 Case study - spotifyr 7.5 Scraping 7.6 Case study - Rohan’s books 7.7 Case study - Canadian Prime Ministers 7.8 PDFs 7.9 Case-study: US Total Fertility Rate, by state and year (2000-2018) 7.10 Case-study: Kenyan census data 7.11 Optical Character Recognition 7.12 Text", " Chapter 7 Gather data Required reading Cooksey, Brian, 2014, ‘An Introduction to APIs’, Zapier, 22 April, https://zapier.com/learn/apis/. Gelfand, Sharla, 2019, ‘Crying @ Sephora’, 8 November, https://sharla.party/post/crying-sephora/. Luscombe, Alex, 2020, ‘Getting your .pdfs into R’, 5 August, https://alexluscombe.ca/post/r-pdftools/. Luscombe, Alex, 2020, ‘Parsing your .pdfs in R’, 10 August, https://alexluscombe.ca/post/parsing-pdfs/. Silge, Julia and David Robinson, 2020, Text Mining with R, Chapters 1, 3, and 6, https://www.tidytextmining.com/. Wickham, Hadley, nd, ‘Getting started with httr’, https://cran.r-project.org/web/packages/httr/vignettes/quickstart.html. Wickham, Hadley, 2014, ‘rvest: easy web scraping with R’, 24 November, https://blog.rstudio.com/2014/11/24/rvest-easy-web-scraping-with-r/. Recommended reading Alexander, Rohan, 2019, ‘Gathering and analysing text data’, 3 January, https://rohanalexander.com/posts/2019-01-03-gathering-and-analysing-text-data/. (Example of web-scraping.) Benoit, Kenneth, 2019, ‘Text as data: An overview’, 17 July, https://kenbenoit.net/pdfs/28%20Benoit%20Text%20as%20Data%20draft%202.pdf. Bolton, Liza, 2019, ‘A quick look at museums per capita’, 26 March, http://blog.dataembassy.co.nz/museums-per-capita/. (Example of web-scraping.) Bryan, Jennifer, and Jim Hester, 2020, ‘What They Forgot to Teach You About R’, chapter 7, https://rstats.wtf/index.html. Clavelle, Tyler, 2017, ‘Using R to extract data from web APIs’, 5 June, https://www.tylerclavelle.com/code/2017/randapis/. Dogucu, Mine, and Mine Çetinkaya-Runde, 2020,‘Web Scraping in the Statistics and Data Science Curriculum: Challenges and Opportunities’, 6 May. (Walks through some basics.) Goldman, Shayna, 2019, ‘How Much Do NHL Players Really Make? Part 2: Taxes’, https://hockey-graphs.com/2019/01/08/how-much-do-nhl-players-really-make-part-2-taxes/. (Example of web-scraping.) Graham, Shawn, 2019, ‘Scraping with rvest’, 7 November, https://electricarchaeology.ca/2019/11/07/scraping-with-rvest/. (Example of web-scraping.) Henze, Martin, 2020, ‘Web Scraping with rvest + Astro Throwback’, 23 January, https://heads0rtai1s.github.io/2020/01/23/rvest-intro-astro/. (Example of web-scraping.) Hudon, Caitlin, 2017, ‘’Blue Christmas: A data-driven search for the most depressing Christmas song’, 22 December, https://caitlinhudon.com/2017/12/22/blue-christmas/. Luscombe, Alex, 2020, ‘A Gentle Introduction to Tesseract OCR’, 3 June, https://alexluscombe.ca/post/ocr-tutorial/. Marshall, James, ‘HTML Made Really Easy’, https://www.jmarshall.com/easy/html/. (Primer on HTML.) Marshall, James, ‘HTTP Made Really Easy’, https://www.jmarshall.com/easy/http/. Nakagawara, Ryo, 2020, ‘Intro to {polite} Web Scraping of Soccer Data with R!’, 14 May, https://ryo-n7.github.io/2020-05-14-webscrape-soccer-data-with-R/. (Introduction to the polite package.) Pavlik, Kaylin, 2020, ‘How do fiber types appear together in yarn blends?’, 17 February, https://www.kaylinpavlik.com/ravelry-yarn-fibers/. Silge, Julia, 2017, ‘Scraping CRAN with rvest’, 5 March, https://juliasilge.com/blog/scraping-cran/. (Example of web-scraping.) Silge, Julia, 2018, ‘The game is afoot! Topic modeling of Sherlock Holmes stories’, 25 January, https://juliasilge.com/blog/sherlock-holmes-stm/. Silge, Julia, 2018, ‘Training, evaluating, and interpreting topic models’, 8 September, https://juliasilge.com/blog/evaluating-stm/. Smale, David, 2020, ‘Daniel Johnston’, https://davidsmale.netlify.com/portfolio/daniel-johnston/. Taddy, Matt, 2019, Business Data Science, Chapter 8, pp. 231-259. Wickham, Hadley, ‘Managing Secrets’, https://cran.r-project.org/web/packages/httr/vignettes/secrets.html. Recommended viewing D’Agostino McGowan, Lucy, 2020 ‘Harnessing the Power of the Web via R Clients for Web APIs’, talk at ASA Joint Statistical Meeting 2018, https://www.lucymcgowan.com/talk/asa_joint_statistical_meeting_2018/. Tatman, Rachel, 2018, ‘Character Encoding and You’, 21 February, https://youtu.be/2U9EHYqc59Y. Key concepts/skills/etc Use APIs where possible because the data provider has specified the data they would like to make available to you, and the conditions under which they are making it available. Often R packages have been written to make it easier to use APIs. Use R environments to manage your keys. Using the verb GET (‘a GET request’) means providing a URL and the server will return something, using the verb POST (a POST request’) means providing some data and the server will deal with that data. Cleaning data Graphing data to tell a story Respectfully scraping data Approaching extracting text from PDFs as a workflow. Planning what is needed at the start. Starting small and then iterating. Putting in place checks. Gathering text data. Preparing text datasets. Key libraries babynames broom dplyr ggplot2 glmnet gutenbergr janitor jsonlite pdftools purrr rtweet rvest spotifyr stringi tidymodels tidytext tidyverse usethis Key functions/etc augment() bind_tf_idf() cast_dtm() edit_r_environ() fromJSON() geom_segment() get_artist_audio_features() get_favorites() get_my_top_artists_or_tracks() glance() glmnet() gutenberg_download() html_nodes() html_text() map_dfr() pdf_text() pmap_dfr() read_csv() read_html() safely() search_tweets() SelectorGadget separate() separate_rows() str_detect() str_replace() str_squish() tidy() tokens() unnest_tokens() walk2() Quiz In your own words, what is an API (write a paragraph or two)? Find two APIs and discuss how you could use them to tell interesting stories (write a paragraph or two for each). Find two APIs that have an R packages written around them. How could you use these to tell interesting stories? (Write a paragraph or two for each.) What is the main argument to httr::GET() (pick one)? ‘url’ ‘website’ ‘domain’ ‘location’ Name three reasons why we should be respectful when getting scraping data from websites (write a paragraph or two). What features of a website do we typically take advantage of when we parse the code (select all)? HTML/CSS mark-up. Cookies. Facebook beacons. Code comments. What are three advantages and three disadvantages of scraping compared with using an API (write a paragraph or two)? What are three delimiters that could be useful when trying to bring order to the PDF that you read in as a character vector (write a paragraph or two)? What do I need to put inside “SOMETHING_HERE” if I want to match regular expressions for a full stop i.e. “.” (hint: see the ‘strings’ cheatsheet) (pick one)? . \\. \\.. \\... Name three reasons for sketching out what you want before starting to try to extract data from a PDF (write a paragraph or two for each). If you are interested in demographic data then what are three checks that you might like to do? What are three if you are interested in economic data such as GDP, interest rates, and exchange rates? (Write an explanation for each.) What does the purrr package do (select all)? Enhances R’s functional programming toolkit. Makes loops easier to code and read. Checks the consistency of datasets. Identifies issues in data structures and proposes replacements. Which of these are functions from the purrr package (select all)? map() walk() run() safely() Why should we use safely() when scraping data (pick one)? To protect us from hackers. To avoid side effects of pages with issues. To slow down our scraping to an appropriate speed. What are some principles to follow when scraping (select all)? Avoid it if possible Follow the site’s guidance Slow down Use a scalpel not an axe. What is a robots.txt file (pick one)? The instructions that Frankenstein followed. Notes that web scrapers should follow when scraping. What is the html tag for an item in list (pick one)? li body b em If I have the following text data ‘rohan_alexander’ in a column called ‘names’ and want to split it into first name and surname based on the underbar what function should I use (pick one)? separate() slice() spacing() text_to_columns() 7.1 APIs In everyday language, and for our purposes, an Application Programming Interface (API) is simply a situation in which someone has set up specific files on their computer such that you can follow their instructions to get them. For instance, when you use a gif on Slack, Slack asks Giphy’s server for the appropriate gif, Giphy’s server gives that gif to Slack and then Slack inserts it into your chat. The way in which Slack and Giphy interact is determined by Giphy’s API. More strictly, an API is just an application that runs on a server that we access using the HTTP protocol. In our case, we are going to focus on using APIs for gathering data. I’ll tailor the language that I use toward that: [a]n API is the tool that makes a website’s data digestible for a computer. Through it, a computer can view and edit data, just like a person can by loading pages and submitting forms. Cooksey (2014), Chapter 1. For instance, you could go to Google Maps and then scroll and click and drag to center the map on Canberra, Australia, or you could just paste this into your browser: https://www.google.ca/maps/@-35.2812958,149.1248113,16z. You just used the Google Maps API.4 The result should be a map that looks something like Figure 7.1 . Figure 7.1: Example of Google Maps, as at 25 January 2021. The advantage of using an API is that the data provider specifies exactly the data that they are willing to provide, and the terms under which they will provide it. These terms may include things like rate limits (i.e. how often you can ask for data), and what you can do with the data (e.g. maybe you’re not allowed to use it for commercial purposes, or to republish it, or whatever). Additionally, because the API is being provided specifically for you to use it, it is less likely to be subject to unexpected changes. Because of this it is ethically and legally clear that when an API is available you should try to use it. We’re going to run through some case studies interacting with APIs in R. In the first we will deal directly with an API. That works and is a handy skill to have, but there are a lot of R packages that wrap around APIs making it easier for you to use an API within ‘familiar surroundings’. So I’ll also run through two fun APIs that have R packages built around them. 7.2 Case study - arXiv In this section we introduce GET requests in which we use an API directly. We will use the httr package (Wickham 2019a). A GET request tries to obtain some specific data and the main argument is url. Exactly as before with the Google Maps example! In that case, the specific information was a map and some information about it. For this example we’ll look at arXiv, which is a repository for academic articles before they go through peer-review. I’ll ask arXiv to return some information about a paper that I recently uploaded with a former student. The content that is returned will be a series of information about that paper. # install.packages(&#39;httr&#39;) library(httr) arxiv &lt;- httr::GET(&#39;http://export.arxiv.org/api/query?id_list=2101.05225&#39;) class(arxiv) ## [1] &quot;response&quot; content(arxiv, &quot;text&quot;) %&gt;% cat(&quot;\\n&quot;) ## &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; ## &lt;feed xmlns=&quot;http://www.w3.org/2005/Atom&quot;&gt; ## &lt;link href=&quot;http://arxiv.org/api/query?search_query%3D%26id_list%3D2101.05225%26start%3D0%26max_results%3D10&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/&gt; ## &lt;title type=&quot;html&quot;&gt;ArXiv Query: search_query=&amp;amp;id_list=2101.05225&amp;amp;start=0&amp;amp;max_results=10&lt;/title&gt; ## &lt;id&gt;http://arxiv.org/api/p9UZyl2Vt0cHwPSKinDSThE23qI&lt;/id&gt; ## &lt;updated&gt;2021-01-25T00:00:00-05:00&lt;/updated&gt; ## &lt;opensearch:totalResults xmlns:opensearch=&quot;http://a9.com/-/spec/opensearch/1.1/&quot;&gt;1&lt;/opensearch:totalResults&gt; ## &lt;opensearch:startIndex xmlns:opensearch=&quot;http://a9.com/-/spec/opensearch/1.1/&quot;&gt;0&lt;/opensearch:startIndex&gt; ## &lt;opensearch:itemsPerPage xmlns:opensearch=&quot;http://a9.com/-/spec/opensearch/1.1/&quot;&gt;10&lt;/opensearch:itemsPerPage&gt; ## &lt;entry&gt; ## &lt;id&gt;http://arxiv.org/abs/2101.05225v1&lt;/id&gt; ## &lt;updated&gt;2021-01-13T17:37:07Z&lt;/updated&gt; ## &lt;published&gt;2021-01-13T17:37:07Z&lt;/published&gt; ## &lt;title&gt;On consistency scores in text data with an implementation in R&lt;/title&gt; ## &lt;summary&gt; In this paper, we introduce a reproducible cleaning process for the text ## extracted from PDFs using n-gram models. Our approach compares the originally ## extracted text with the text generated from, or expected by, these models using ## earlier text as stimulus. To guide this process, we introduce the notion of a ## consistency score, which refers to the proportion of text that is expected by ## the model. This is used to monitor changes during the cleaning process, and ## across different corpuses. We illustrate our process on text from the book Jane ## Eyre and introduce both a Shiny application and an R package to make our ## process easier for others to adopt. ## &lt;/summary&gt; ## &lt;author&gt; ## &lt;name&gt;Ke-Li Chiu&lt;/name&gt; ## &lt;/author&gt; ## &lt;author&gt; ## &lt;name&gt;Rohan Alexander&lt;/name&gt; ## &lt;/author&gt; ## &lt;arxiv:comment xmlns:arxiv=&quot;http://arxiv.org/schemas/atom&quot;&gt;13 pages, 0 figures&lt;/arxiv:comment&gt; ## &lt;link href=&quot;http://arxiv.org/abs/2101.05225v1&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot;/&gt; ## &lt;link title=&quot;pdf&quot; href=&quot;http://arxiv.org/pdf/2101.05225v1&quot; rel=&quot;related&quot; type=&quot;application/pdf&quot;/&gt; ## &lt;arxiv:primary_category xmlns:arxiv=&quot;http://arxiv.org/schemas/atom&quot; term=&quot;cs.CL&quot; scheme=&quot;http://arxiv.org/schemas/atom&quot;/&gt; ## &lt;category term=&quot;cs.CL&quot; scheme=&quot;http://arxiv.org/schemas/atom&quot;/&gt; ## &lt;/entry&gt; ## &lt;/feed&gt; ## We get a variety of information about this paper including the title, abstract, and authors. 7.3 Case study - rtweet Twitter is a rich source of text and other data. The Twitter API is the way in which Twitter ask that you interact with Twitter in order to gather these data. The rtweet package (Kearney 2019) is built around this API and allows us to interact with it in ways that are similar to using any other R package. Initially all you need a regular Twitter account. Get started by install the library if you need and then calling it. # install.packages(&#39;rtweet&#39;) library(rtweet) library(tidyverse) To get started we need to authorise rtweet. We start that process by calling a function from the package. get_favorites(user = &quot;RohanAlexander&quot;) This will open a browser on your computer, and you will then have to log into your regular Twitter account as shown in Figure 7.2. Figure 7.2: rtweet authorisation page Once that is done we can actually get my favourites and then save them. rohans_favs &lt;- get_favorites(&quot;RohanAlexander&quot;) saveRDS(rohans_favs, &quot;dont_push/rohans_favs.rds&quot;) And then looking at the most recent favourite, we can see it was when Professor Bolton tweeted about one of the stellar students in ISSC. rohans_favs %&gt;% arrange(desc(created_at)) %&gt;% slice(1) %&gt;% select(screen_name, text) ## # A tibble: 1 x 2 ## screen_name text ## &lt;chr&gt; &lt;chr&gt; ## 1 Liza_Bolton One of our awesome @UofTStatSci students! I 💜 learning about the … Let’s look at who is tweeting about R, using one of the common R hashtags: #rstats. I’ve removed retweets so that we hopefully get some actual interesting projects. rstats_tweets &lt;- search_tweets( q = &quot;#rstats&quot;, include_rts = FALSE ) saveRDS(rstats_tweets, &quot;dont_push/rstats_tweets.rds&quot;) And then have a look at them. names(rstats_tweets) ## [1] &quot;user_id&quot; &quot;status_id&quot; ## [3] &quot;created_at&quot; &quot;screen_name&quot; ## [5] &quot;text&quot; &quot;source&quot; ## [7] &quot;display_text_width&quot; &quot;reply_to_status_id&quot; ## [9] &quot;reply_to_user_id&quot; &quot;reply_to_screen_name&quot; ## [11] &quot;is_quote&quot; &quot;is_retweet&quot; ## [13] &quot;favorite_count&quot; &quot;retweet_count&quot; ## [15] &quot;quote_count&quot; &quot;reply_count&quot; ## [17] &quot;hashtags&quot; &quot;symbols&quot; ## [19] &quot;urls_url&quot; &quot;urls_t.co&quot; ## [21] &quot;urls_expanded_url&quot; &quot;media_url&quot; ## [23] &quot;media_t.co&quot; &quot;media_expanded_url&quot; ## [25] &quot;media_type&quot; &quot;ext_media_url&quot; ## [27] &quot;ext_media_t.co&quot; &quot;ext_media_expanded_url&quot; ## [29] &quot;ext_media_type&quot; &quot;mentions_user_id&quot; ## [31] &quot;mentions_screen_name&quot; &quot;lang&quot; ## [33] &quot;quoted_status_id&quot; &quot;quoted_text&quot; ## [35] &quot;quoted_created_at&quot; &quot;quoted_source&quot; ## [37] &quot;quoted_favorite_count&quot; &quot;quoted_retweet_count&quot; ## [39] &quot;quoted_user_id&quot; &quot;quoted_screen_name&quot; ## [41] &quot;quoted_name&quot; &quot;quoted_followers_count&quot; ## [43] &quot;quoted_friends_count&quot; &quot;quoted_statuses_count&quot; ## [45] &quot;quoted_location&quot; &quot;quoted_description&quot; ## [47] &quot;quoted_verified&quot; &quot;retweet_status_id&quot; ## [49] &quot;retweet_text&quot; &quot;retweet_created_at&quot; ## [51] &quot;retweet_source&quot; &quot;retweet_favorite_count&quot; ## [53] &quot;retweet_retweet_count&quot; &quot;retweet_user_id&quot; ## [55] &quot;retweet_screen_name&quot; &quot;retweet_name&quot; ## [57] &quot;retweet_followers_count&quot; &quot;retweet_friends_count&quot; ## [59] &quot;retweet_statuses_count&quot; &quot;retweet_location&quot; ## [61] &quot;retweet_description&quot; &quot;retweet_verified&quot; ## [63] &quot;place_url&quot; &quot;place_name&quot; ## [65] &quot;place_full_name&quot; &quot;place_type&quot; ## [67] &quot;country&quot; &quot;country_code&quot; ## [69] &quot;geo_coords&quot; &quot;coords_coords&quot; ## [71] &quot;bbox_coords&quot; &quot;status_url&quot; ## [73] &quot;name&quot; &quot;location&quot; ## [75] &quot;description&quot; &quot;url&quot; ## [77] &quot;protected&quot; &quot;followers_count&quot; ## [79] &quot;friends_count&quot; &quot;listed_count&quot; ## [81] &quot;statuses_count&quot; &quot;favourites_count&quot; ## [83] &quot;account_created_at&quot; &quot;verified&quot; ## [85] &quot;profile_url&quot; &quot;profile_expanded_url&quot; ## [87] &quot;account_lang&quot; &quot;profile_banner_url&quot; ## [89] &quot;profile_background_url&quot; &quot;profile_image_url&quot; rstats_tweets %&gt;% select(screen_name, text) %&gt;% head() ## # A tibble: 6 x 2 ## screen_name text ## &lt;chr&gt; &lt;chr&gt; ## 1 CRANberriesFeed CRAN updates: gstat hdme opalr tinytest https://t.co/y5W2NTKS… ## 2 CRANberriesFeed New CRAN package FeatureImpCluster with initial version 0.1.2… ## 3 CRANberriesFeed CRAN updates: crfsuite https://t.co/y5W2NTKSXT #rstats ## 4 CRANberriesFeed CRAN updates: DSI https://t.co/y5W2NTKSXT #rstats ## 5 CRANberriesFeed New CRAN package arcos with initial version 1.1 https://t.co/… ## 6 CRANberriesFeed CRAN updates: COVID19 gaiah mosaic smoothSurv https://t.co/y5… There is a bunch of other things that you can do just using a regular user account, and if you’re interested then you should try the examples in the rtweet package documentation: https://rtweet.info/index.html. But more is available once you register as a developer (https://developer.twitter.com/en/apply-for-access). The Twitter API document is surprisingly readable and you may enjoy some of it: https://developer.twitter.com/en/docs. When I introduced APIs I said that the ‘data provider specifies exactly the data that they are willing to provide…’ and we have certainly been able to take advantage of what they provide But I continued ‘…and the terms under which they will provide it’ and here we haven’t done our part. In particular, I took some tweets and saved them. If I had pushed these to GitHub then it’s possible I may have accidently stored sensitive information if there happened to be some in the tweets. Or if I had taken enough tweets to start to do some reasonable statistical analysis then even if there wasn’t sensitive information I may have violated the terms if I had pushed those saved tweets to GitHub. Finally, I linked a Twitter user name, in this case @Liza_Bolton with Professor Bolton. I happened to ask her if this was okay, but if I hadn’t done that then I would have been violating the Twitter terms of service. If you use Twitter data, please take a moment to look at the terms: https://developer.twitter.com/en/developer-terms/more-on-restricted-use-cases. 7.4 Case study - spotifyr For the next example I will introduce the spotifyr package (Thompson et al. 2020). Again, this is a wrapper that has been developed around an API, in this case the Spotify API. https://www.rcharlie.com/spotifyr/ # devtools::install_github(&#39;charlie86/spotifyr&#39;) library(spotifyr) In order to use this account you need a Spotify Developer Account, which you can set-up here: https://developer.spotify.com/dashboard/. That’ll have you log in with your Spotify details and then accept their terms (it’s worth looking at some of these and I’ll follow up on a few below) as in Figure 7.3. Figure 7.3: rtweet authorisation page What we need from here is a ‘Client ID’ and you can just fill out some basic details. In our case we probably ‘don’t know’ what we’re building, which means that Spotify requires us to use a non-commercial agreement, which is fine. In order to use the Spotify API we need a Client ID and a Client Secret. These are things that you want to keep to yourself. There are a variety of ways of keeping this secret, (and my understanding is that a helpful package is on its way) but we’ll keep them in our System Environment. In this way, when we push to GitHub they won’t be included. To do this we need to be careful about the naming, because spotifyr will look in our environment for specifically named keys. To do this we are going to use the usethis package Wickham and Bryan (2020). So if you don’t have that then please install it. There is a file called ‘.Renviron’ which we will open and add our secrets to. This file also controls things like your default library location and more information is available at Lopp (2017) and Bryan and Hester (2020). usethis::edit_r_environ() When you run that function it will open a file. There you can add your Spotify secrets. SPOTIFY_CLIENT_ID = &#39;PUT_YOUR_CLIENT_ID_HERE&#39; SPOTIFY_CLIENT_SECRET = &#39;PUT_YOUR_SECRET_HERE&#39; Save your ‘.Renviron’ file, and then restart R (Session -&gt; Restart R). You can now draw on that variable when you need. Some functions that require your secrets as arguments will now just work. For instance, we will get information about Radiohead using get_artist_audio_features(). One of the arguments is authorization, but as that is set to default to look at the R Environment, we don’t need to do anything further. radiohead &lt;- get_artist_audio_features(&#39;radiohead&#39;) saveRDS(radiohead, &quot;inputs/radiohead.rds&quot;) radiohead &lt;- readRDS(&quot;inputs/radiohead.rds&quot;) names(radiohead) ## [1] &quot;artist_name&quot; &quot;artist_id&quot; ## [3] &quot;album_id&quot; &quot;album_type&quot; ## [5] &quot;album_images&quot; &quot;album_release_date&quot; ## [7] &quot;album_release_year&quot; &quot;album_release_date_precision&quot; ## [9] &quot;danceability&quot; &quot;energy&quot; ## [11] &quot;key&quot; &quot;loudness&quot; ## [13] &quot;mode&quot; &quot;speechiness&quot; ## [15] &quot;acousticness&quot; &quot;instrumentalness&quot; ## [17] &quot;liveness&quot; &quot;valence&quot; ## [19] &quot;tempo&quot; &quot;track_id&quot; ## [21] &quot;analysis_url&quot; &quot;time_signature&quot; ## [23] &quot;artists&quot; &quot;available_markets&quot; ## [25] &quot;disc_number&quot; &quot;duration_ms&quot; ## [27] &quot;explicit&quot; &quot;track_href&quot; ## [29] &quot;is_local&quot; &quot;track_name&quot; ## [31] &quot;track_preview_url&quot; &quot;track_number&quot; ## [33] &quot;type&quot; &quot;track_uri&quot; ## [35] &quot;external_urls.spotify&quot; &quot;album_name&quot; ## [37] &quot;key_name&quot; &quot;mode_name&quot; ## [39] &quot;key_mode&quot; radiohead %&gt;% select(artist_name, track_name, album_name) %&gt;% head() ## artist_name track_name ## 1 Radiohead Airbag - Remastered ## 2 Radiohead Paranoid Android - Remastered ## 3 Radiohead Subterranean Homesick Alien - Remastered ## 4 Radiohead Exit Music (For a Film) - Remastered ## 5 Radiohead Let Down - Remastered ## 6 Radiohead Karma Police - Remastered ## album_name ## 1 OK Computer OKNOTOK 1997 2017 ## 2 OK Computer OKNOTOK 1997 2017 ## 3 OK Computer OKNOTOK 1997 2017 ## 4 OK Computer OKNOTOK 1997 2017 ## 5 OK Computer OKNOTOK 1997 2017 ## 6 OK Computer OKNOTOK 1997 2017 Let’s just make a quick graph looking at track length over time. radiohead %&gt;% ggplot(aes(x = album_release_year, y = duration_ms)) + geom_point() Just because we can, let’s settle an argument. I’ve always said that Radiohead of quite depressing, but they’re my wife’s favourite band. So let’s see how depressing they are. Spotify provides various information about each track, including ‘valence’, which Spotify define as ‘(a) measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).’ So higher values are happier. Let’s compare someone who we know it likely to be happy - Taylor Swift - with Radiohead. swifty &lt;- get_artist_audio_features(&#39;taylor swift&#39;) saveRDS(swifty, &quot;inputs/swifty.rds&quot;) swifty &lt;- readRDS(&quot;inputs/swifty.rds&quot;) tibble(name = c(swifty$artist_name, radiohead$artist_name), year = c(swifty$album_release_year, radiohead$album_release_year), valence = c(swifty$valence, radiohead$valence) ) %&gt;% ggplot(aes(x = year, y = valence, color = name)) + geom_point() + theme_minimal() + labs(x = &quot;Year&quot;, y = &quot;Valence&quot;, color = &quot;Name&quot;) + scale_color_brewer(palette = &quot;Set1&quot;) Finally, for the sake of embarrassment, let’s look at our most played artists. top_artists &lt;- get_my_top_artists_or_tracks(type = &#39;artists&#39;, time_range = &#39;long_term&#39;, limit = 20) saveRDS(top_artists, &quot;inputs/top_artists.rds&quot;) top_artists &lt;- readRDS(&quot;inputs/top_artists.rds&quot;) top_artists %&gt;% select(name, popularity) ## name popularity ## 1 Radiohead 81 ## 2 Bombay Bicycle Club 66 ## 3 Drake 100 ## 4 Glass Animals 74 ## 5 JAY-Z 85 ## 6 Laura Marling 65 ## 7 Sufjan Stevens 75 ## 8 Vampire Weekend 73 ## 9 Sturgill Simpson 65 ## 10 Nick Drake 66 ## 11 Dire Straits 78 ## 12 Lorde 80 ## 13 Marian Hill 65 ## 14 José González 68 ## 15 Stevie Wonder 79 ## 16 Disclosure 82 ## 17 Ben Folds Five 52 ## 18 Ainslie Wills 40 ## 19 Coldplay 89 ## 20 alt-J 75 So pretty much my wife and I like what everyone else likes, with the exception of Ainslie Wills, who is an Australian and I suspect we used to listen to her when we were homesick. How amazing that we live in a world that all that information is available with very little effort or cost. Again, there is a lot more at the package’s website: https://www.rcharlie.com/spotifyr/. A very nice little application of the Spotify API using some statistical analysis is Pavlik (2019). 7.5 Scraping 7.5.1 Introduction Web-scraping is a way to get data from websites into R. Rather than going to a website ourselves through a browser, we write code that does it for us. This opens up a lot of data to us, but on the other hand, it is not typically data that is being made available for these purposes and so it is important to be respectful of it. While generally not illegal, the specifics with regard to the legality of web-scraping depends on jurisdictions and the specifics of what you’re doing, and so it is also important to be mindful of this. And finally, web-scraping imposes a cost on the website host, and so it is important to reduce this to the extent that it’s possible. That all said, web-scraping is an invaluable source of data. But they are typically a datasets that can be created as a by-product of someone trying to achieve another aim. For instance, a retailer may have a website with their products and their prices. That has not been created deliberately as a source of data, but we can scrape it to create a dataset. As such, the following principles guide my web-scraping. Avoid it. Try to use an API wherever possible. Abide by their desires. Some websites have a file ‘robots.txt’ that contains information about what they are comfortable with scrapers doing, for instance ‘https://www.google.com/robots.txt’. If they have one of these then you should read it and abide by it. Reduce the impact. Firstly, slow down your scraper, for instance, rather than having it visit the website every second, slow it down (using sys.sleep()). If you’re only after a few hundred files then why not just have it visit once a minute, running in the background overnight? Secondly, consider the timing of when you run the scraper. For instance, if it’s a retailer then why not set your script to run from 10pm through to the morning, when fewer customers are likely to need the site? If it’s a government website and they have a big monthly release then why not avoid that day? Take only what you need. For instance, don’t scrape the entire of Wikipedia if all you need is to know the names of the 10 largest cities in Canada. This reduces the impact on their website and allows you to more easily justify what you are doing. Only scrape once. Save everything as you go so that you don’t have to re-collect data. Similarly, once you have the data, you should keep that separate and not modify it. Of course, if you need data over time then you will need to go back, but this is different to needlessly re-scraping a page. Don’t republish the pages that you scraped. (This is in contrast to datasets that you create from it.) Take ownership and ask permission if possible. At a minimum level your scripts should have your contact details in them. Depending on the circumstances, it may be worthwhile asking for permission before you scrape. 7.5.2 Getting started Webscraping is possible by taking advantage of the underlying structure of a webpage. We use patterns in the HTML/CSS to get the data that we want. To look at the underlying HTML/CSS you can either: 1) open a browser, right-click, and choose something like ‘Inspect’; or 2) save the website and then open it with a text editor rather than a browser. HTML/CSS is a markup language comprised of matching tags. So if you want text to be bold then you would use something like: &lt;b&gt;My bold text&lt;/b&gt; Similarly, if you want a list then you start and end the list as well as each item. &lt;ul&gt; &lt;li&gt;Learn webscraping&lt;/li&gt; &lt;li&gt;Do data science&lt;/li&gt; &lt;li&gt;Proft&lt;/li&gt; &lt;/ul&gt; When webscraping we will search for these tags. To get started, this is some HTML/CSS from my website. Let’s say that we want to grab my name from it. We can see that the name is in bold, so we want to probably focus on that feature and extract it. website_extract &lt;- &quot;&lt;p&gt;Hi, I’m &lt;b&gt;Rohan&lt;/b&gt; Alexander.&lt;/p&gt;&quot; We will use the rvest package Wickham (2019b). # install.packages(&quot;rvest&quot;) library(rvest) rohans_data &lt;- read_html(website_extract) rohans_data ## {html_document} ## &lt;html&gt; ## [1] &lt;body&gt;&lt;p&gt;Hi, I’m &lt;b&gt;Rohan&lt;/b&gt; Alexander.&lt;/p&gt;&lt;/body&gt; The language used by rvest to look for tags is ‘node’, so we will focus on bold nodes. By default html_nodes() returns the tags as well. So we can focus on the text that they contain, using html_text(). rohans_data %&gt;% html_nodes(&quot;b&quot;) ## {xml_nodeset (1)} ## [1] &lt;b&gt;Rohan&lt;/b&gt; first_name &lt;- rohans_data %&gt;% html_nodes(&quot;b&quot;) %&gt;% html_text() first_name ## [1] &quot;Rohan&quot; The result is that we learn my first name. 7.6 Case study - Rohan’s books 7.6.1 Introduction In this case study we are going to scrape a list of books that I own, clean it, and look at the distribution of the first letters of author surnames. It is slightly more complicated than the example above, but the underlying approach is the same - download the website, look for the nodes of interest, extract the information, clean it. 7.6.2 Gather Again, the key library that we are using is the rvest library. This makes it easier to download a website, and to then navigate the html to find the aspects that we are interested in. You should create a new project in a new folder (File -&gt; New Project). Within that new folder you should make three new folders: inputs, outputs, and scripts. In the scripts folder you should write and save a script along these lines. This script loads the libraries that we need, then visits my website, and saves a local copy. #### Contact details #### # Title: Get data from rohanalexander.com # Purpose: This script gets data from Rohan&#39;s website about the books that he # owns. It calls his website and then saves the dataset to inputs. # Author: Rohan Alexander # Contact: rohan.alexander@utoronto.ca # Last updated: 20 May 2020 #### Set up workspace #### library(rvest) library(tidyverse) #### Get html #### rohans_data &lt;- read_html(&quot;https://rohanalexander.com/bookshelf.html&quot;) # This takes a website as an input and will read it into R, in the same way that we # can read a, say, CSV into R. write_html(rohans_data, &quot;inputs/my_website/raw_data.html&quot;) # Always save your raw dataset as soon as you get it so that you have a record # of it. This is the equivalent of, say, write_csv() that we have used earlier. 7.6.3 Clean Now we need to navigate the HTML to get the aspects that we want, and to then put them into some sensible structure. I always try to get the data into a tibble as early as possible. While it’s possible to work with the nested data, I move to a tibble so that the usual verbs that I’m used to can be used. In the scripts folder you should write and save a new R script along these lines. First, we need to add the top matter, read in the libraries and the data that we scraped. #### Contact details #### # Title: Clean data from rohanaledander.com # Purpose: This script cleans data that was downloaded in 01-get_data.R. # Author: Rohan Alexander # Contact: rohan.alexander@utoronto.ca # Pre-requisites: Need to have run 01_get_data.R and have saved the data. # Last updated: 20 May 2020 #### Set up workspace #### library(tidyverse) library(rvest) rohans_data &lt;- read_html(&quot;inputs/my_website/raw_data.html&quot;) rohans_data ## {html_document} ## &lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot; lang=&quot;&quot; xml:lang=&quot;&quot;&gt; ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... ## [2] &lt;body&gt;\\n\\n&lt;!--radix_placeholder_front_matter--&gt;\\n\\n&lt;script id=&quot;distill-fr ... Now we need to identify the data that we are interested in using html tags and convert it to a tibble. If you look at the website, then you should notice that we are likely trying to focus on list items (Figure 7.4). Figure 7.4: Some of Rohan’s books Let’s look at the source (Figure 7.5). Figure 7.5: Source code for top of the page There’s a lot of debris, but scrolling down we eventually get to a list (Figure 7.6). Figure 7.6: Source code for list The tag for a list item is ‘li’, so we modify the earlier code to focus on that and to get the text. #### Clean data #### # Identify the lines that have books on them based on the list html tag text_data &lt;- rohans_data %&gt;% html_nodes(&quot;li&quot;) %&gt;% html_text() all_books &lt;- tibble(books = text_data) head(all_books) ## # A tibble: 6 x 1 ## books ## &lt;chr&gt; ## 1 &quot;-“A Little Life”, Hanya Yanighara. Recommended by Lauren.&quot; ## 2 &quot;“The Andromeda Strain”, Michael Crichton.&quot; ## 3 &quot;“Is There Life After Housework”, Don Aslett.\\nGot given this at the Museum o… ## 4 &quot;“The Chosen”, Chaim Potok.&quot; ## 5 &quot;“The Forsyth Saga”, John Galsworthy.&quot; ## 6 &quot;“Freakonomics”, Steven Levitt and Stephen Dubner.&quot; We now need to clean the data. First we want to separate the title and the author # All content is just one string, so need to separate title and author all_books &lt;- all_books %&gt;% separate(books, into = c(&quot;title&quot;, &quot;author&quot;), sep = &quot;”&quot;) # Remove leading comma and clean up the titles a little all_books &lt;- all_books %&gt;% mutate(author = str_remove_all(author, &quot;^, &quot;), author = str_squish(author), title = str_remove(title, &quot;“&quot;), title = str_remove(title, &quot;^-&quot;) ) head(all_books) ## # A tibble: 6 x 2 ## title author ## &lt;chr&gt; &lt;chr&gt; ## 1 A Little Life Hanya Yanighara. Recommended by Lauren. ## 2 The Andromeda Strain Michael Crichton. ## 3 Is There Life After H… Don Aslett. Got given this at the Museum of Clean in P… ## 4 The Chosen Chaim Potok. ## 5 The Forsyth Saga John Galsworthy. ## 6 Freakonomics Steven Levitt and Stephen Dubner. Finally, some specific cleaning is needed. # Some authors have comments after their name, so need to get rid of them, although there are some exceptions that will not work # J. K. Rowling. # M. Mitchell Waldrop. # David A. Price all_books &lt;- all_books %&gt;% mutate(author = str_replace_all(author, c(&quot;J. K. Rowling.&quot; = &quot;J K Rowling.&quot;, &quot;M. Mitchell Waldrop.&quot; = &quot;M Mitchell Waldrop.&quot;, &quot;David A. Price&quot; = &quot;David A Price&quot;) ) ) %&gt;% separate(author, into = c(&quot;author_correct&quot;, &quot;throw_away&quot;), sep = &quot;\\\\.&quot;, extra = &quot;drop&quot;) %&gt;% select(-throw_away) %&gt;% rename(author = author_correct) # Some books have multiple authors, so need to separate them # One has multiple authors: # &quot;Daniela Witten, Gareth James, Robert Tibshirani, and Trevor Hastie&quot; all_books &lt;- all_books %&gt;% mutate(author = str_replace(author, &quot;Daniela Witten, Gareth James, Robert Tibshirani, and Trevor Hastie&quot;, &quot;Daniela Witten and Gareth James and Robert Tibshirani and Trevor Hastie&quot;)) %&gt;% separate(author, into = c(&quot;author_first&quot;, &quot;author_second&quot;, &quot;author_third&quot;, &quot;author_fourth&quot;), sep = &quot; and &quot;, fill = &quot;right&quot;) %&gt;% pivot_longer(cols = starts_with(&quot;author_&quot;), names_to = &quot;author_position&quot;, values_to = &quot;author&quot;) %&gt;% select(-author_position) %&gt;% filter(!is.na(author)) head(all_books) ## # A tibble: 6 x 2 ## title author ## &lt;chr&gt; &lt;chr&gt; ## 1 A Little Life Hanya Yanighara ## 2 The Andromeda Strain Michael Crichton ## 3 Is There Life After Housework Don Aslett ## 4 The Chosen Chaim Potok ## 5 The Forsyth Saga John Galsworthy ## 6 Freakonomics Steven Levitt It looks there is some at the end because I have a best of. I’ll just get rid of those manually because it’s not the focus. all_books &lt;- all_books %&gt;% slice(1:118) 7.6.4 Explore Finally, just because we have the data now, so we may as well try to do something with it, let’s look at the distribution of the first letter of the author names. all_books %&gt;% mutate( first_letter = str_sub(author, 1, 1) ) %&gt;% group_by(first_letter) %&gt;% count() ## # A tibble: 21 x 2 ## # Groups: first_letter [21] ## first_letter n ## &lt;chr&gt; &lt;int&gt; ## 1 &quot;&quot; 1 ## 2 &quot;A&quot; 8 ## 3 &quot;B&quot; 5 ## 4 &quot;C&quot; 4 ## 5 &quot;D&quot; 10 ## 6 &quot;E&quot; 3 ## 7 &quot;F&quot; 1 ## 8 &quot;G&quot; 10 ## 9 &quot;H&quot; 6 ## 10 &quot;I&quot; 1 ## # … with 11 more rows 7.7 Case study - Canadian Prime Ministers 7.7.1 Introduction In this case study we are interested in how long Canadian prime ministers lived, based on the year that they were born. We will scrape data from Wikipedia, clean it, and then make a graph. The key library that we will use for scraping is rvest. This adds a lot of functions that will make life easier. That said, every time you scrape a website things will change. Each scrape will largely be bespoke, even if you can borrow some code from earlier projects that you have completed. It is completely normal to feel frustrated at times. It helps to begin with an end in mind. To that end, let’s generate some simulated data. Ideally, we want a table that has a row for each prime minister, a column for their name, and a column each for the birth and death years. If they are still alive, then that death year can be empty. We know that birth and death years should be somewhere between 1700 and 1990, and that death year should be larger than birth year. Finally, we also know that the years should be integers, and the names should be characters. So, we want something that looks roughly like this: library(babynames) library(tidyverse) simulated_dataset &lt;- tibble(prime_minister = sample(x = babynames %&gt;% filter(prop &gt; 0.01) %&gt;% select(name) %&gt;% unique() %&gt;% unlist(), size = 10, replace = FALSE), birth_year = sample(x = c(1700:1990), size = 10, replace = TRUE), years_lived = sample(x = c(50:100), size = 10, replace = TRUE), death_year = birth_year + years_lived) %&gt;% select(prime_minister, birth_year, death_year, years_lived) %&gt;% arrange(birth_year) head(simulated_dataset) ## # A tibble: 6 x 4 ## prime_minister birth_year death_year years_lived ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Stephen 1716 1809 93 ## 2 Brittany 1784 1860 76 ## 3 Ronald 1804 1892 88 ## 4 Scott 1809 1887 78 ## 5 Linda 1833 1916 83 ## 6 Marie 1841 1897 56 One of the advantages of generating a simulated dataset is that if you are working in groups then one person can start making the graph, using the simulated dataset, while the other person gathers the data. In terms of a graph, we want something like Figure 7.7. Figure 7.7: Sketch of planned graph. 7.7.2 Gather We are starting with a question that is of interest, which how long each Canadian prime minister lived. As such, we need to identify a source of data While there are likely to be plenty of data sources that have the births and deaths of each prime minister, we want one that we can trust, and as we are going to be scraping, we want one that has some structure to it. The Wikipedia page (https://en.wikipedia.org/wiki/List_of_prime_ministers_of_Canada) fits both these criteria. As it is a popular page the information is more likely to be correct, and the data are available in a table. We load the library and then we read in the data from the relevant page. The key function here is read_html(), which you can use in the same way as, say, read_csv(), except that it takes a html page as an input. Once you call read_html() then the page is downloaded to your own computer, and it is usually a good idea to save this, using write_html() as it is your raw data. Saving it also means that we don’t have to keep visiting the website when we want to start again with our cleaning, and so it is part of being polite. However, it is likely not our property (in the case of Wikipedia, we might be okay), and so you should probably not share it. library(rvest) raw_data &lt;- read_html(&quot;https://en.wikipedia.org/wiki/List_of_prime_ministers_of_Canada&quot;) write_html(raw_data, &quot;inputs/wiki/pms.html&quot;) # Note that we save the file as a html file. 7.7.3 Clean Websites are made up of html, which is a markup language. We are looking for patterns in the mark-up that we can use to help us get closer to the data that we want. This is an iterative process and requires a lot of trial and error. Even simple examples will take time. You can look at the html by using a browser, right clicking, and then selecting view page source. Similarly, you could open the html file using a text editor. 7.7.3.1 By inspection We are looking for patterns that we can use to select the information that is of interest - names, birth year, and death year. When we look at the html it looks like there is something going on with &lt;tr&gt;, and then &lt;td&gt; (thanks to Thomas Rosenthal for identifying this). We select those nodes using html_nodes(), which takes the tags as an input. If you only want the first one then there is a singular version, html_node(). # Read in our saved data raw_data &lt;- read_html(&quot;inputs/wiki/pms.html&quot;) # We can parse tags in order parse_data_inspection &lt;- raw_data %&gt;% html_nodes(&quot;tr&quot;) %&gt;% html_nodes(&quot;td&quot;) %&gt;% html_text() # html_text removes any remaining html tags # But this code does exactly the same thing - the nodes are just pushed into # the one function call parse_data_inspection &lt;- raw_data %&gt;% html_nodes(&quot;tr td&quot;) %&gt;% html_text() head(parse_data_inspection) ## [1] &quot;Abbreviation key:&quot; ## [2] &quot;No.: Incumbent number, Min.: Ministry, Refs: References\\n&quot; ## [3] &quot;Colour key:&quot; ## [4] &quot;\\n\\n Liberal Party of Canada\\n \\n Historical Conservative parties (including Liberal-Conservative, Conservative (Historical), Unionist, National Liberal and Conservative, Progressive Conservative) \\n Conservative Party of Canada\\n\\n&quot; ## [5] &quot;Provinces key:&quot; ## [6] &quot;AB: Alberta, BC: British Columbia, MB: Manitoba, NS: Nova Scotia,ON: Ontario, QC: Quebec, SK: Saskatchewan\\n&quot; At this point our data is in a character vector, we want to convert it to a table, and reduce the data down to just the information that we want. The key that is going to allow us to do this is the fact that there seems to be a blank line (which in html is denoted by \\n) before the key information that we need. So, once we identify that line then we can filter to just the line below it! parsed_data &lt;- tibble(raw_text = parse_data_inspection) %&gt;% # Convert the character vector to a table mutate(is_PM = if_else(raw_text == &quot;\\n\\n&quot;, 1, 0), # Look for the blank line that is # above the row that we want is_PM = lag(is_PM, n = 1)) %&gt;% # Identify the actual row that we want filter(is_PM == 1) # Just get the rows that we want head(parsed_data) ## # A tibble: 6 x 2 ## raw_text is_PM ## &lt;chr&gt; &lt;dbl&gt; ## 1 &quot;\\nSir John A. MacDonald(1815–1891)MP for Kingston, ON\\n&quot; 1 ## 2 &quot;\\nAlexander Mackenzie(1822–1892)MP for Lambton, ON\\n&quot; 1 ## 3 &quot;\\nSir John A. MacDonald(1815–1891)MP for Victoria, BC until 1882MP for… 1 ## 4 &quot;\\nSir John Abbott(1821–1893)Senator for Quebec\\n&quot; 1 ## 5 &quot;\\nSir John Thompson(1845–1894)MP for Antigonish, NS\\n&quot; 1 ## 6 &quot;\\nSir Mackenzie Bowell(1823–1917)Senator for Ontario\\n&quot; 1 7.7.3.2 Using the selector gadget If you are comfortable with html then you might be able to see patterns, but one tool that may help is the SelectorGadget: https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html. This allows you to pick and choose the elements that you want, and then gives you the input to give to html_nodes() (Figure 7.8) Figure 7.8: Using the Selector Gadget to identify the tag, as at 13 March 2020. # Read in our saved data raw_data &lt;- read_html(&quot;inputs/wiki/pms.html&quot;) # We can parse tags in order parse_data_selector_gadget &lt;- raw_data %&gt;% html_nodes(&quot;td:nth-child(3)&quot;) %&gt;% html_text() # html_text removes any remaining html tags head(parse_data_selector_gadget) ## [1] &quot;\\nSir John A. MacDonald(1815–1891)MP for Kingston, ON\\n&quot; ## [2] &quot;\\nAlexander Mackenzie(1822–1892)MP for Lambton, ON\\n&quot; ## [3] &quot;\\nSir John A. MacDonald(1815–1891)MP for Victoria, BC until 1882MP for Carleton, ON until 1887MP for Kingston, ON\\n&quot; ## [4] &quot;\\nSir John Abbott(1821–1893)Senator for Quebec\\n&quot; ## [5] &quot;\\nSir John Thompson(1845–1894)MP for Antigonish, NS\\n&quot; ## [6] &quot;\\nSir Mackenzie Bowell(1823–1917)Senator for Ontario\\n&quot; In this case there is one prime minister - Robert Borden - who changed party and we would need to filter away that row: \\nUnionist Party\\n\". 7.7.3.3 Clean data Now that we have the parsed data, we need to clean it to match what we wanted. In particular we want a names column, as well as columns for birth year and death year. We will use separate() to take advantage of the fact that it looks like the dates are distinguished by brackets. initial_clean &lt;- parsed_data %&gt;% separate(raw_text, into = c(&quot;Name&quot;, &quot;not_name&quot;), sep = &quot;\\\\(&quot;, remove = FALSE) %&gt;% # The remove = FALSE option here means that we # keep the original column that we are separating. separate(not_name, into = c(&quot;Date&quot;, &quot;all_the_rest&quot;), sep = &quot;\\\\)&quot;, remove = FALSE) head(initial_clean) ## # A tibble: 6 x 6 ## raw_text Name not_name Date all_the_rest is_PM ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 &quot;\\nSir John A. Ma… &quot;\\nSir … &quot;1815–1891)MP fo… 1815… &quot;MP for Kingston, O… 1 ## 2 &quot;\\nAlexander Mack… &quot;\\nAlex… &quot;1822–1892)MP fo… 1822… &quot;MP for Lambton, ON… 1 ## 3 &quot;\\nSir John A. Ma… &quot;\\nSir … &quot;1815–1891)MP fo… 1815… &quot;MP for Victoria, B… 1 ## 4 &quot;\\nSir John Abbot… &quot;\\nSir … &quot;1821–1893)Senat… 1821… &quot;Senator for Quebec… 1 ## 5 &quot;\\nSir John Thomp… &quot;\\nSir … &quot;1845–1894)MP fo… 1845… &quot;MP for Antigonish,… 1 ## 6 &quot;\\nSir Mackenzie … &quot;\\nSir … &quot;1823–1917)Senat… 1823… &quot;Senator for Ontari… 1 Finally, we need to clean up the columns. cleaned_data &lt;- initial_clean %&gt;% select(Name, Date) %&gt;% separate(Date, into = c(&quot;Birth&quot;, &quot;Died&quot;), sep = &quot;–&quot;, remove = FALSE) %&gt;% # The # PMs who have died have their birth and death years separated by a hyphen, but # you need to be careful with the hyphen as it seems to be a slightly odd type of # hyphen and you need to copy/paste it. mutate(Birth = str_remove(Birth, &quot;b. &quot;)) %&gt;% # Alive PMs have slightly different format select(-Date) %&gt;% mutate(Name = str_remove(Name, &quot;\\n&quot;)) %&gt;% # Remove some html tags that remain mutate_at(vars(Birth, Died), ~as.integer(.)) %&gt;% # Change birth and death to integers mutate(Age_at_Death = Died - Birth) %&gt;% # Add column of the number of years they lived distinct() # Some of the PMs had two goes at it. head(cleaned_data) ## # A tibble: 6 x 4 ## Name Birth Died Age_at_Death ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Sir John A. MacDonald 1815 1891 76 ## 2 Alexander Mackenzie 1822 1892 70 ## 3 Sir John Abbott 1821 1893 72 ## 4 Sir John Thompson 1845 1894 49 ## 5 Sir Mackenzie Bowell 1823 1917 94 ## 6 Sir Charles Tupper 1821 1915 94 7.7.4 Explore At this point we’d like to make a graph that illustrates how long each prime minister lived. If they are still alive then we would like them to run to the end, but we would like to colour them differently. cleaned_data %&gt;% mutate(still_alive = if_else(is.na(Died), &quot;Yes&quot;, &quot;No&quot;), Died = if_else(is.na(Died), as.integer(2020), Died)) %&gt;% mutate(Name = as_factor(Name)) %&gt;% ggplot(aes(x = Birth, xend = Died, y = Name, yend = Name, color = still_alive)) + geom_segment() + labs(x = &quot;Year of birth&quot;, y = &quot;Prime minister&quot;, color = &quot;PM is alive&quot;, title = &quot;Canadian Prime Minister, by year of birth&quot;) + theme_minimal() + scale_color_brewer(palette = &quot;Set1&quot;) 7.8 PDFs 7.8.1 Introduction In contrast to an API, a PDF is usually only produced for human (not computer) consumption. The nice thing about PDFs is that they are static and constant. And it is nice that they make data available at all. But the trade-off is that: It is not overly useful to do larger-scale statistical analysis. We don’t know how the PDF was put together so we don’t know whether we can trust it. We can’t manipulate the data to get results that we are interested in. Indeed, sometimes governments publish data as PDFs because they don’t actually want you to be able to analyse it! Being able to get data from PDFs opens up a large number of dataset for you, some of which we’ll see in this chapter. There are two important aspects to keep in mind when approaching a PDF with a mind to extracting data from it: Begin with an end in mind. Planning and then literally sketching out what you want from a final dataset/graph/paper stops you wasting time and keeps you focused. Start simple, then iterate. The quickest way to make a complicated model is often to first build a simple model and then complicate it. Start with just trying to get one page of the PDF working or even just one line. Then iterate from there. In this chapter we start by walking through several examples and then go through three case studies of varying difficulty. 7.8.2 Getting started Figure 7.9 is a PDF that consists of just the first sentence from Jane Eyre taken from Project Gutenberg Bronte (1847). Figure 7.9: First sentence of Jane Eyre We will use the package pdftools Ooms (2019a) to get the text in this one page PDF into R. # install.packages(&quot;pdftools&quot;) library(pdftools) library(tidyverse) first_example &lt;- pdftools::pdf_text(&quot;inputs/pdfs/first_example.pdf&quot;) first_example ## [1] &quot;There was no possibility of taking a walk that day.&quot; class(first_example) ## [1] &quot;character&quot; We can see that the PDF has been correctly read in, as a character vector. We will now try a slightly more complicated example that consists of the first few paragraphs of Jane Eyre (Figure 7.10). Also notice that now we have the chapter heading as well. Figure 7.10: First few paragraphs of Jane Eyre We use the same function as before. second_example &lt;- pdftools::pdf_text(&quot;inputs/pdfs/second_example.pdf&quot;) second_example ## [1] &quot;CHAPTER I\\nThere was no possibility of taking a walk that day. We had been wandering, indeed, in the\\nleafless shrubbery an hour in the morning; but since dinner (Mrs. Reed, when there was no\\ncompany, dined early) the cold winter wind had brought with it clouds so sombre, and a rain so\\npenetrating, that further out-door exercise was now out of the question.\\nI was glad of it: I never liked long walks, especially on chilly afternoons: dreadful to me was the\\ncoming home in the raw twilight, with nipped fingers and toes, and a heart saddened by the\\nchidings of Bessie, the nurse, and humbled by the consciousness of my physical inferiority to\\nEliza, John, and Georgiana Reed.\\nThe said Eliza, John, and Georgiana were now clustered round their mama in the drawing-room:\\nshe lay reclined on a sofa by the fireside, and with her darlings about her (for the time neither\\nquarrelling nor crying) looked perfectly happy. Me, she had dispensed from joining the group;\\nsaying, “She regretted to be under the necessity of keeping me at a distance; but that until she\\nheard from Bessie, and could discover by her own observation, that I was endeavouring in good\\nearnest to acquire a more sociable and childlike disposition, a more attractive and sprightly\\nmanner—something lighter, franker, more natural, as it were—she really must exclude me from\\nprivileges intended only for contented, happy, little children.”\\n“What does Bessie say I have done?” I asked.\\n“Jane, I don’t like cavillers or questioners; besides, there is something truly forbidding in a child\\ntaking up her elders in that manner. Be seated somewhere; and until you can speak pleasantly,\\nremain silent.”\\nA breakfast-room adjoined the drawing-room, I slipped in there. It contained a bookcase: I soon\\npossessed myself of a volume, taking care that it should be one stored with pictures. I mounted\\ninto the window-seat: gathering up my feet, I sat cross-legged, like a Turk; and, having drawn the\\nred moreen curtain nearly close, I was shrined in double retirement.\\nFolds of scarlet drapery shut in my view to the right hand; to the left were the clear panes of\\nglass, protecting, but not separating me from the drear November day. At intervals, while\\nturning over the leaves of my book, I studied the aspect of that winter afternoon. Afar, it offered\\na pale blank of mist and cloud; near a scene of wet lawn and storm-beat shrub, with ceaseless\\nrain sweeping away wildly before a long and lamentable blast.\\n&quot; class(second_example) ## [1] &quot;character&quot; Again, we have a character vector. The end of each line is signalled by ‘\\n’, but other than that it looks pretty good. Finally, we consider the first two pages. We use the same function as before. third_example &lt;- pdftools::pdf_text(&quot;inputs/pdfs/third_example.pdf&quot;) third_example ## [1] &quot;CHAPTER I\\nThere was no possibility of taking a walk that day. We had been wandering, indeed, in the\\nleafless shrubbery an hour in the morning; but since dinner (Mrs. Reed, when there was no\\ncompany, dined early) the cold winter wind had brought with it clouds so sombre, and a rain so\\npenetrating, that further out-door exercise was now out of the question.\\nI was glad of it: I never liked long walks, especially on chilly afternoons: dreadful to me was the\\ncoming home in the raw twilight, with nipped fingers and toes, and a heart saddened by the\\nchidings of Bessie, the nurse, and humbled by the consciousness of my physical inferiority to\\nEliza, John, and Georgiana Reed.\\nThe said Eliza, John, and Georgiana were now clustered round their mama in the drawing-room:\\nshe lay reclined on a sofa by the fireside, and with her darlings about her (for the time neither\\nquarrelling nor crying) looked perfectly happy. Me, she had dispensed from joining the group;\\nsaying, “She regretted to be under the necessity of keeping me at a distance; but that until she\\nheard from Bessie, and could discover by her own observation, that I was endeavouring in good\\nearnest to acquire a more sociable and childlike disposition, a more attractive and sprightly\\nmanner—something lighter, franker, more natural, as it were—she really must exclude me from\\nprivileges intended only for contented, happy, little children.”\\n“What does Bessie say I have done?” I asked.\\n“Jane, I don’t like cavillers or questioners; besides, there is something truly forbidding in a child\\ntaking up her elders in that manner. Be seated somewhere; and until you can speak pleasantly,\\nremain silent.”\\nA breakfast-room adjoined the drawing-room, I slipped in there. It contained a bookcase: I soon\\npossessed myself of a volume, taking care that it should be one stored with pictures. I mounted\\ninto the window-seat: gathering up my feet, I sat cross-legged, like a Turk; and, having drawn the\\nred moreen curtain nearly close, I was shrined in double retirement.\\nFolds of scarlet drapery shut in my view to the right hand; to the left were the clear panes of\\nglass, protecting, but not separating me from the drear November day. At intervals, while\\nturning over the leaves of my book, I studied the aspect of that winter afternoon. Afar, it offered\\na pale blank of mist and cloud; near a scene of wet lawn and storm-beat shrub, with ceaseless\\nrain sweeping away wildly before a long and lamentable blast.\\nI returned to my book—Bewick’s History of British Birds: the letterpress thereof I cared little\\nfor, generally speaking; and yet there were certain introductory pages that, child as I was, I could\\nnot pass quite as a blank. They were those which treat of the haunts of sea-fowl; of “the solitary\\nrocks and promontories” by them only inhabited; of the coast of Norway, studded with isles from\\nits southern extremity, the Lindeness, or Naze, to the North Cape—\\n“Where the Northern Ocean, in vast whirls,\\nBoils round the naked, melancholy isles\\n&quot; ## [2] &quot;Of farthest Thule; and the Atlantic surge\\nPours in among the stormy Hebrides.”\\nNor could I pass unnoticed the suggestion of the bleak shores of Lapland, Siberia, Spitzbergen,\\nNova Zembla, Iceland, Greenland, with “the vast sweep of the Arctic Zone, and those forlorn\\nregions of dreary space,—that reservoir of frost and snow, where firm fields of ice, the\\naccumulation of centuries of winters, glazed in Alpine heights above heights, surround the pole,\\nand concentre the multiplied rigours of extreme cold.” Of these death-white realms I formed an\\nidea of my own: shadowy, like all the half-comprehended notions that float dim through\\nchildren’s brains, but strangely impressive. The words in these introductory pages connected\\nthemselves with the succeeding vignettes, and gave significance to the rock standing up alone in\\na sea of billow and spray; to the broken boat stranded on a desolate coast; to the cold and ghastly\\nmoon glancing through bars of cloud at a wreck just sinking.\\nI cannot tell what sentiment haunted the quite solitary churchyard, with its inscribed headstone;\\nits gate, its two trees, its low horizon, girdled by a broken wall, and its newly-risen crescent,\\nattesting the hour of eventide.\\nThe two ships becalmed on a torpid sea, I believed to be marine phantoms.\\nThe fiend pinning down the thief’s pack behind him, I passed over quickly: it was an object of\\nterror.\\nSo was the black horned thing seated aloof on a rock, surveying a distant crowd surrounding a\\ngallows.\\nEach picture told a story; mysterious often to my undeveloped understanding and imperfect\\nfeelings, yet ever profoundly interesting: as interesting as the tales Bessie sometimes narrated on\\nwinter evenings, when she chanced to be in good humour; and when, having brought her ironing-\\ntable to the nursery hearth, she allowed us to sit about it, and while she got up Mrs. Reed’s lace\\nfrills, and crimped her nightcap borders, fed our eager attention with passages of love and\\nadventure taken from old fairy tales and other ballads; or (as at a later period I discovered) from\\nthe pages of Pamela, and Henry, Earl of Moreland.\\nWith Bewick on my knee, I was then happy: happy at least in my way. I feared nothing but\\ninterruption, and that came too soon. The breakfast-room door opened.\\n“Boh! Madam Mope!” cried the voice of John Reed; then he paused: he found the room\\napparently empty.\\n“Where the dickens is she!” he continued. “Lizzy! Georgy! (calling to his sisters) Joan is not\\nhere: tell mama she is run out into the rain—bad animal!”\\n“It is well I drew the curtain,” thought I; and I wished fervently he might not discover my hiding-\\nplace: nor would John Reed have found it out himself; he was not quick either of vision or\\nconception; but Eliza just put her head in at the door, and said at once—\\n&quot; class(third_example) ## [1] &quot;character&quot; Now, notice that the first page is the first element of the character vector and the second page is the second element. As we’re most familiar with rectangular data we’ll try to get it into that format as quickly as possible. And then we can use our regular tools to deal with it. First we want to convert the character vector into a tibble. At this point we may like to add page numbers as well. jane_eyre &lt;- tibble(raw_text = third_example, page_number = c(1:2)) We probably now want to separate the lines so that each line is an observation. We can do that by looking for the ‘\\n’ remembering that we need to escape the backslash as it’s a special character. jane_eyre &lt;- separate_rows(jane_eyre, raw_text, sep = &quot;\\\\n&quot;, convert = FALSE) head(jane_eyre) ## # A tibble: 6 x 2 ## raw_text page_number ## &lt;chr&gt; &lt;int&gt; ## 1 CHAPTER I 1 ## 2 There was no possibility of taking a walk that day. We had been w… 1 ## 3 leafless shrubbery an hour in the morning; but since dinner (Mrs.… 1 ## 4 company, dined early) the cold winter wind had brought with it cl… 1 ## 5 penetrating, that further out-door exercise was now out of the qu… 1 ## 6 I was glad of it: I never liked long walks, especially on chilly … 1 7.9 Case-study: US Total Fertility Rate, by state and year (2000-2018) 7.9.1 Introduction If you’re married to a demographer it is not too long until you are asked to look at a US Department of Health and Human Services Vital Statistics Report. In this case we are interested in trying to get the total fertility rate (the average number of births per woman assuming that woman experience the current age-specific fertility rates throughout their reproductive years)5 for each state for nineteen years. Annoyingly, the US persists in only making this data available in PDFs, but it makes a nice case study. In the case of the year 2000 the table that we are interested in is on page 40 of a PDF that is available https://www.cdc.gov/nchs/data/nvsr/nvsr50/nvsr50_05.pdf and it is the column labelled: “Total fertility rate” (Figure 7.11). Figure 7.11: Example Vital Statistics Report, from 2000 7.9.2 Begin with an end in mind The first step when getting data out of a PDF is to sketch out what you eventually want. A PDF typically contains a lot of information, and so it is handy to be very clear about what you need. This helps keep you focused, and prevents scope creep, but it is also helpful when thinking about data checks. Literally write down on paper what you have in mind. In this case, what is needed is a table with a column for state, year and TFR (Figure 7.12). Figure 7.12: Desired output from the PDF 7.9.3 Start simple, then iterate. There are 19 different PDFs and we are interested in a particular column in a particular table in each of them. Unfortunately there is nothing magical about what is coming. This first step requires working out the link for each, and the page and column name that is of interest. In the end, this looks like this. monicas_data &lt;- read_csv(&quot;inputs/tfr_tables_info.csv&quot;) monicas_data %&gt;% select(year, page, table, column_name, url) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #lddgwbunoc .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #lddgwbunoc .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #lddgwbunoc .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #lddgwbunoc .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #lddgwbunoc .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #lddgwbunoc .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #lddgwbunoc .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #lddgwbunoc .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #lddgwbunoc .gt_column_spanner_outer:first-child { padding-left: 0; } #lddgwbunoc .gt_column_spanner_outer:last-child { padding-right: 0; } #lddgwbunoc .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #lddgwbunoc .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #lddgwbunoc .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #lddgwbunoc .gt_from_md > :first-child { margin-top: 0; } #lddgwbunoc .gt_from_md > :last-child { margin-bottom: 0; } #lddgwbunoc .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #lddgwbunoc .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #lddgwbunoc .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #lddgwbunoc .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #lddgwbunoc .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #lddgwbunoc .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #lddgwbunoc .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #lddgwbunoc .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #lddgwbunoc .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #lddgwbunoc .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #lddgwbunoc .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #lddgwbunoc .gt_sourcenote { font-size: 90%; padding: 4px; } #lddgwbunoc .gt_left { text-align: left; } #lddgwbunoc .gt_center { text-align: center; } #lddgwbunoc .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #lddgwbunoc .gt_font_normal { font-weight: normal; } #lddgwbunoc .gt_font_bold { font-weight: bold; } #lddgwbunoc .gt_font_italic { font-style: italic; } #lddgwbunoc .gt_super { font-size: 65%; } #lddgwbunoc .gt_footnote_marks { font-style: italic; font-size: 65%; } year page table column_name url 2000 40 10 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr50/nvsr50_05.pdf 2001 41 10 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr51/nvsr51_02.pdf 2002 46 10 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr52/nvsr52_10.pdf 2003 45 10 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr54/nvsr54_02.pdf 2004 52 11 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr55/nvsr55_01.pdf 2005 52 11 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr56/nvsr56_06.pdf 2006 49 11 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr57/nvsr57_07.pdf 2007 41 11 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr58/nvsr58_24.pdf 2008 43 12 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr59/nvsr59_01.pdf 2009 43 12 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr60/nvsr60_01.pdf 2010 42 12 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr61/nvsr61_01.pdf 2011 40 12 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr62/nvsr62_01.pdf 2012 38 12 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr62/nvsr62_09.pdf 2013 37 12 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr64/nvsr64_01.pdf 2014 38 12 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr64/nvsr64_12.pdf 2015 42 12 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr66/nvsr66_01.pdf 2016 29 8 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr67/nvsr67_01.pdf 2016 30 8 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr67/nvsr67_01.pdf 2017 23 12 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr67/nvsr67_08-508.pdf 2017 24 12 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr67/nvsr67_08-508.pdf 2018 23 12 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr68/nvsr68_13-508.pdf The first step is to get some code that works for one of them. I’ll step through the code in a lot more detail than normal because we’re going to use these pieces a lot. We will choose the year 2000. We first download the data and save it. download.file(url = monicas_data$url[1], destfile = &quot;inputs/pdfs/dhs/year_2000.pdf&quot;) We now want to read the PDF in as a character vector. dhs_2000 &lt;- pdftools::pdf_text(&quot;inputs/pdfs/dhs/year_2000.pdf&quot;) Convert it to a tibble, so that we can use familiar verbs on it. dhs_2000 &lt;- tibble(raw_data = dhs_2000) head(dhs_2000) ## # A tibble: 6 x 1 ## raw_data ## &lt;chr&gt; ## 1 &quot;Volume 50, Number 5 … ## 2 &quot;2 National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\\nH… ## 3 &quot; … ## 4 &quot;4 National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\\nD… ## 5 &quot; … ## 6 &quot;6 National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\\n … Grab the page that is of interest (remembering that each page is a element of the character vector, hence a row in the tibble). dhs_2000 &lt;- dhs_2000 %&gt;% slice(monicas_data$page[1]) head(dhs_2000) ## # A tibble: 1 x 1 ## raw_data ## &lt;chr&gt; ## 1 &quot;40 National Vital Statistics Report, Vol. 50, No. 5, Revised May 15, 20022\\n… Now we want to separate the rows. dhs_2000 &lt;- dhs_2000 %&gt;% separate_rows(raw_data, sep = &quot;\\\\n&quot;, convert = FALSE) head(dhs_2000) ## # A tibble: 6 x 1 ## raw_data ## &lt;chr&gt; ## 1 40 National Vital Statistics Report, Vol. 50, No. 5, Revised May 15, 20022 ## 2 Table 10. Number of births, birth rates, fertility rates, total fertility rat… ## 3 United States, each State and territory, 2000 ## 4 [By place of residence. Birth rates are live births per 1,000 estimated popul… ## 5 estimated in each area; total fertility rates are sums of birth rates for 5-y… ## 6 age group estimated in each area] Now we are searching for patterns that we can use. (If you have a lot of tables that you are interested in grabbing from PDFs then it may also be worthwhile considering the tabulizer package which is specifically designed for that. The issue is that it depends on Java and I always seem to run into trouble when I need to use Java so I avoid it when I can.) Let’s look at the first ten lines of content. dhs_2000[13:22,] ## # A tibble: 10 x 1 ## raw_data ## &lt;chr&gt; ## 1 United States 1 ...................................................... … ## 2 Alabama ............................................................... … ## 3 Alaska ................................................................... … ## 4 Arizona ................................................................. … ## 5 Arkansas ............................................................... … ## 6 California .............................................................. … ## 7 Colorado ............................................................... … ## 8 Connecticut ........................................................... … ## 9 Delaware .............................................................. … ## 10 District of Columbia .............................................. … It doesn’t get much better than this: We have dots separating the states from the data. We have a space between each of the columns. So we can now separate this in to separate columns. First we want to match on when there is at least two dots (remembering that the dot is a special character and so needs to be escaped). dhs_2000 &lt;- dhs_2000 %&gt;% separate(col = raw_data, into = c(&quot;state&quot;, &quot;data&quot;), sep = &quot;\\\\.{2,}&quot;, remove = FALSE, fill = &quot;right&quot; ) head(dhs_2000) ## # A tibble: 6 x 3 ## raw_data state data ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 40 National Vital Statistics Report,… 40 National Vital Statistics Repo… &lt;NA&gt; ## 2 Table 10. Number of births, birth ra… Table 10. Number of births, birth… &lt;NA&gt; ## 3 United States, each State and territ… United States, each State and ter… &lt;NA&gt; ## 4 [By place of residence. Birth rates … [By place of residence. Birth rat… &lt;NA&gt; ## 5 estimated in each area; total fertil… estimated in each area; total fer… &lt;NA&gt; ## 6 age group estimated in each area] age group estimated in each area] &lt;NA&gt; We get the expected warnings about the top and the bottom as they don’t have multiple dots. (Another option here is to use the pdf_data() function which would allow us to use location rather than delimiters.) We can now separate the data based on spaces. There is an inconsistent number of spaces, so we first squish any example of more than one space into just one. dhs_2000 &lt;- dhs_2000 %&gt;% mutate(data = str_squish(data)) %&gt;% tidyr::separate(col = data, into = c(&quot;number_of_births&quot;, &quot;birth_rate&quot;, &quot;fertility_rate&quot;, &quot;TFR&quot;, &quot;teen_births_all&quot;, &quot;teen_births_15_17&quot;, &quot;teen_births_18_19&quot;), sep = &quot;\\\\s&quot;, remove = FALSE ) head(dhs_2000) ## # A tibble: 6 x 10 ## raw_data state data number_of_births birth_rate fertility_rate TFR ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 40 Nati… 40 N… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 Table 1… Tabl… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 United … Unit… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 [By pla… [By … &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 estimat… esti… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 age gro… age … &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## # … with 3 more variables: teen_births_all &lt;chr&gt;, teen_births_15_17 &lt;chr&gt;, ## # teen_births_18_19 &lt;chr&gt; This is all looking fairly great. The only thing left is to clean up. dhs_2000 &lt;- dhs_2000 %&gt;% select(state, TFR) %&gt;% slice(13:69) %&gt;% mutate(year = 2000) dhs_2000 ## # A tibble: 57 x 3 ## state TFR year ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 &quot;United States 1 &quot; 2,130.0 2000 ## 2 &quot;Alabama &quot; 2,021.0 2000 ## 3 &quot;Alaska &quot; 2,437.0 2000 ## 4 &quot;Arizona &quot; 2,652.5 2000 ## 5 &quot;Arkansas &quot; 2,140.0 2000 ## 6 &quot;California &quot; 2,186.0 2000 ## 7 &quot;Colorado &quot; 2,356.5 2000 ## 8 &quot;Connecticut &quot; 1,931.5 2000 ## 9 &quot;Delaware &quot; 2,014.0 2000 ## 10 &quot;District of Columbia &quot; 1,975.5 2000 ## # … with 47 more rows And we’re done for that year. Now we want to take these pieces, put them into a function and then run that function over all 19 years. 7.9.4 Iterating 7.9.4.1 Get the PDFs The first part is downloading each of the 19 PDFs that we need. We’re going to build on the code that we used before. That code was: download.file(url = monicas_data$url[1], destfile = &quot;inputs/pdfs/dhs/year_2000.pdf&quot;) To modify this we need: To have it iterate through each of the lines in the dataset that contains our CSVs (i.e. where it says 1, we want 1, then 2, then 3, etc.). Where it has a filename, we need it to iterate through our desired filenames (i.e. year_2000, then year_2001, then year_2002, etc). We’d like for it to do all of this in a way that is a little robust to errors. For instance, if one of the URLs is wrong or the internet drops out then we’d like it to just move onto the next PDF, and then warn us at the end that it missed one, not to stop. (This doesn’t really matter because it’s only 19 files, but it’s pretty easy to find yourself doing this for thousands of files). We will draw on the purrr package for this Henry and Wickham (2020). library(purrr) monicas_data &lt;- monicas_data %&gt;% mutate(pdf_name = paste0(&quot;inputs/pdfs/dhs/year_&quot;, year, &quot;.pdf&quot;)) purrr::walk2(monicas_data$url, monicas_data$pdf_name, purrr::safely(~download.file(.x , .y))) What this code does it take the function download.file() and give it two arguments: .x and .y. The function walk2() then applies that function to the inputs that we give it, in this case the URLs columns is the .x and the pdf_names column is the .y. Finally, the safely() function means that if there are any failures then it just moves onto the next file instead of throwing an error. We now have each of the PDFs saved and we can move onto getting the data from them. 7.9.4.2 Get data from the PDFs Now we need to get the data from the PDFs. As before, we’re going to build on the code that we used before. That code (overly condensed) was: dhs_2000 &lt;- pdftools::pdf_text(&quot;inputs/pdfs/dhs/year_2000.pdf&quot;) dhs_2000 &lt;- tibble(raw_data = dhs_2000) %&gt;% slice(monicas_data$page[1]) %&gt;% separate_rows(raw_data, sep = &quot;\\\\n&quot;, convert = FALSE) %&gt;% separate(col = raw_data, into = c(&quot;state&quot;, &quot;data&quot;), sep = &quot;\\\\.{2,}&quot;, remove = FALSE) %&gt;% mutate(data = str_squish(data)) %&gt;% separate(col = data, into = c(&quot;number_of_births&quot;, &quot;birth_rate&quot;, &quot;fertility_rate&quot;, &quot;TFR&quot;, &quot;teen_births_all&quot;, &quot;teen_births_15_17&quot;, &quot;teen_births_18_19&quot;), sep = &quot;\\\\s&quot;, remove = FALSE) %&gt;% select(state, TFR) %&gt;% slice(13:69) %&gt;% mutate(year = 2000) dhs_2000 There are a bunch of aspects here that have been hardcoded, but the first thing that we want to iterate is the argument to pdf_text(), then the number in in slice() will also need to change (that is doing the work to get only the page that we are interested in). Two aspects are hardcoded and these may need to be updated. In particular: 1) The separate only works if each of the tables has the same columns in the same order; and 2) the slice (which restricts the data to just the states) only works. Finally, we add the year only at the end, whereas we’d need to bring that up earlier in the process. We’ll start by writing a function that will go through all the files, grab the data, get the page of interest, and then expand the rows. We’ll then use a function from purrr to apply that function to all of the PDFs and to output a tibble. get_pdf_convert_to_tibble &lt;- function(pdf_name, page, year){ dhs_table_of_interest &lt;- tibble(raw_data = pdftools::pdf_text(pdf_name)) %&gt;% slice(page) %&gt;% separate_rows(raw_data, sep = &quot;\\\\n&quot;, convert = FALSE) %&gt;% separate(col = raw_data, into = c(&quot;state&quot;, &quot;data&quot;), sep = &quot;[�|\\\\.]\\\\s+(?=[[:digit:]])&quot;, remove = FALSE) %&gt;% mutate( data = str_squish(data), year_of_data = year) print(paste(&quot;Done with&quot;, year)) return(dhs_table_of_interest) } raw_dhs_data &lt;- purrr::pmap_dfr(monicas_data %&gt;% select(pdf_name, page, year), get_pdf_convert_to_tibble) ## [1] &quot;Done with 2000&quot; ## [1] &quot;Done with 2001&quot; ## [1] &quot;Done with 2002&quot; ## [1] &quot;Done with 2003&quot; ## [1] &quot;Done with 2004&quot; ## [1] &quot;Done with 2005&quot; ## [1] &quot;Done with 2006&quot; ## [1] &quot;Done with 2007&quot; ## [1] &quot;Done with 2008&quot; ## [1] &quot;Done with 2009&quot; ## [1] &quot;Done with 2010&quot; ## [1] &quot;Done with 2011&quot; ## [1] &quot;Done with 2012&quot; ## [1] &quot;Done with 2013&quot; ## [1] &quot;Done with 2014&quot; ## [1] &quot;Done with 2015&quot; ## [1] &quot;Done with 2016&quot; ## [1] &quot;Done with 2016&quot; ## [1] &quot;Done with 2017&quot; ## [1] &quot;Done with 2017&quot; ## [1] &quot;Done with 2018&quot; head(raw_dhs_data) ## # A tibble: 6 x 4 ## raw_data state data year_of_data ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 40 National Vital Statistics … 40 National Vital Statistic… 50, … 2000 ## 2 Table 10. Number of births, b… Table 10. Number of births,… &lt;NA&gt; 2000 ## 3 United States, each State and… United States, each State a… &lt;NA&gt; 2000 ## 4 [By place of residence. Birth… [By place of residence. Bir… &lt;NA&gt; 2000 ## 5 estimated in each area; total… estimated in each area; tot… &lt;NA&gt; 2000 ## 6 age group estimated in each a… age group estimated in each… &lt;NA&gt; 2000 Now we need to clean up the state names and then filter on them. states &lt;- c(&quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;California&quot;, &quot;Colorado&quot;, &quot;Connecticut&quot;, &quot;Delaware&quot;, &quot;Florida&quot;, &quot;Georgia&quot;, &quot;Hawaii&quot;, &quot;Idaho&quot;, &quot;Illinois&quot;, &quot;Indiana&quot;, &quot;Iowa&quot;, &quot;Kansas&quot;, &quot;Kentucky&quot;, &quot;Louisiana&quot;, &quot;Maine&quot;, &quot;Maryland&quot;, &quot;Massachusetts&quot;, &quot;Michigan&quot;, &quot;Minnesota&quot;, &quot;Mississippi&quot;, &quot;Missouri&quot;, &quot;Montana&quot;, &quot;Nebraska&quot;, &quot;Nevada&quot;, &quot;New Hampshire&quot;, &quot;New Jersey&quot;, &quot;New Mexico&quot;, &quot;New York&quot;, &quot;North Carolina&quot;, &quot;North Dakota&quot;, &quot;Ohio&quot;, &quot;Oklahoma&quot;, &quot;Oregon&quot;, &quot;Pennsylvania&quot;, &quot;Rhode Island&quot;, &quot;South Carolina&quot;, &quot;South Dakota&quot;, &quot;Tennessee&quot;, &quot;Texas&quot;, &quot;Utah&quot;, &quot;Vermont&quot;, &quot;Virginia&quot;, &quot;Washington&quot;, &quot;West Virginia&quot;, &quot;Wisconsin&quot;, &quot;Wyoming&quot;, &quot;District of Columbia&quot;) raw_dhs_data &lt;- raw_dhs_data %&gt;% mutate(state = str_remove_all(state, &quot;\\\\.&quot;), state = str_remove_all(state, &quot;�&quot;), state = str_remove_all(state, &quot;\b&quot;), state = str_replace_all(state, &quot;United States 1&quot;, &quot;United States&quot;), state = str_replace_all(state, &quot;United States1&quot;, &quot;United States&quot;), state = str_replace_all(state, &quot;United States 2&quot;, &quot;United States&quot;), state = str_replace_all(state, &quot;United States2&quot;, &quot;United States&quot;), state = str_replace_all(state, &quot;United States²&quot;, &quot;United States&quot;), ) %&gt;% mutate(state = str_squish(state)) %&gt;% filter(state %in% states) head(raw_dhs_data) ## # A tibble: 6 x 4 ## raw_data state data year_of_data ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Alabama ............................… Alabama 63,299 14.4 65.0 2… 2000 ## 2 Alaska .............................… Alaska 9,974 16.0 74.6 2,… 2000 ## 3 Arizona ............................… Arizona 85,273 17.5 84.4 2… 2000 ## 4 Arkansas ...........................… Arkans… 37,783 14.7 69.1 2… 2000 ## 5 California .........................… Califo… 531,959 15.8 70.7 … 2000 ## 6 Colorado ...........................… Colora… 65,438 15.8 73.1 2… 2000 The next step is to separate the data and get the correct column from it. We’re going to separate based on spaces once it is cleaned up. raw_dhs_data &lt;- raw_dhs_data %&gt;% mutate(data = str_remove_all(data, &quot;\\\\*&quot;)) %&gt;% separate(data, into = c(&quot;col_1&quot;, &quot;col_2&quot;, &quot;col_3&quot;, &quot;col_4&quot;, &quot;col_5&quot;, &quot;col_6&quot;, &quot;col_7&quot;, &quot;col_8&quot;, &quot;col_9&quot;, &quot;col_10&quot;), sep = &quot; &quot;, remove = FALSE) head(raw_dhs_data) ## # A tibble: 6 x 14 ## raw_data state data col_1 col_2 col_3 col_4 col_5 col_6 col_7 col_8 col_9 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Alabama… Alab… 63,2… 63,2… 14.4 65.0 2,02… 62.9 37.9 97.3 &lt;NA&gt; &lt;NA&gt; ## 2 Alaska … Alas… 9,97… 9,974 16.0 74.6 2,43… 42.4 23.6 69.4 &lt;NA&gt; &lt;NA&gt; ## 3 Arizona… Ariz… 85,2… 85,2… 17.5 84.4 2,65… 69.1 41.1 111.3 &lt;NA&gt; &lt;NA&gt; ## 4 Arkansa… Arka… 37,7… 37,7… 14.7 69.1 2,14… 68.5 36.7 114.1 &lt;NA&gt; &lt;NA&gt; ## 5 Califor… Cali… 531,… 531,… 15.8 70.7 2,18… 48.5 28.6 75.6 &lt;NA&gt; &lt;NA&gt; ## 6 Colorad… Colo… 65,4… 65,4… 15.8 73.1 2,35… 49.2 28.6 79.8 &lt;NA&gt; &lt;NA&gt; ## # … with 2 more variables: col_10 &lt;chr&gt;, year_of_data &lt;dbl&gt; We can now grab the correct column. tfr_data &lt;- raw_dhs_data %&gt;% mutate(TFR = if_else(year_of_data &lt; 2008, col_4, col_3)) %&gt;% select(state, year_of_data, TFR) %&gt;% rename(year = year_of_data) head(tfr_data) ## # A tibble: 6 x 3 ## state year TFR ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Alabama 2000 2,021.0 ## 2 Alaska 2000 2,437.0 ## 3 Arizona 2000 2,652.5 ## 4 Arkansas 2000 2,140.0 ## 5 California 2000 2,186.0 ## 6 Colorado 2000 2,356.5 Finally, we need to convert the case. head(tfr_data) ## # A tibble: 6 x 3 ## state year TFR ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Alabama 2000 2,021.0 ## 2 Alaska 2000 2,437.0 ## 3 Arizona 2000 2,652.5 ## 4 Arkansas 2000 2,140.0 ## 5 California 2000 2,186.0 ## 6 Colorado 2000 2,356.5 tfr_data &lt;- tfr_data %&gt;% mutate(TFR = str_remove_all(TFR, &quot;,&quot;), TFR = as.numeric(TFR)) head(tfr_data) ## # A tibble: 6 x 3 ## state year TFR ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alabama 2000 2021 ## 2 Alaska 2000 2437 ## 3 Arizona 2000 2652. ## 4 Arkansas 2000 2140 ## 5 California 2000 2186 ## 6 Colorado 2000 2356. And run some checks. # tfr_data %&gt;% # skimr::skim() In particular we want for there to be 51 states and for there to be 19 years. And we’re done. head(tfr_data) ## # A tibble: 6 x 3 ## state year TFR ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alabama 2000 2021 ## 2 Alaska 2000 2437 ## 3 Arizona 2000 2652. ## 4 Arkansas 2000 2140 ## 5 California 2000 2186 ## 6 Colorado 2000 2356. write_csv(tfr_data, &quot;outputs/monicas_tfr.csv&quot;) 7.10 Case-study: Kenyan census data The distribution of population by age, sex, and administrative unit from the 2019 Kenyan census can be downloaded here: https://www.knbs.or.ke/?wpdmpro=2019-kenya-population-and-housing-census-volume-iii-distribution-of-population-by-age-sex-and-administrative-units. And while it is great that they make it easily available, and it is easy to look-up a particular result, it is not overly useful to do larger-scale data analysis, such as building a Bayesian hierarchical model. In this section we will convert a PDF of Kenyan census results of counts, by age and sex, by county and sub-county, into a tidy dataset that can be analysed. I will draw on and introduce a bunch of handy packages including: janitor by Firke (2020), pdftools by Ooms (2019b), tidyverse by Wickham, Averick, et al. (2019a), and stringi by Gagolewski (2020). 7.10.1 Set-up To get started I need to load the necessary packages. library(janitor) library(pdftools) library(tidyverse) library(stringi) And then I need to read in the PDF that I want to convert. # Read in the PDF all_content &lt;- pdftools::pdf_text(&quot;inputs/pdfs/2019_Kenya_census.pdf&quot;) The pdf_text function from pdftools is useful when you have a PDF and you want to read the content into R. For many recently produced PDFs it’ll work pretty well, but there are alternatives. If the PDF is an image, then it won’t work and you’ll need to turn to OCR. You can see a page of the PDF here: knitr::include_graphics(&quot;figures/2020-04-10-screenshot-of-census.png&quot;) 7.10.2 Extract The first challenge is to get the dataset into a format that we can more easily manipulate. The way that I am going to do this is to consider each page of the PDF and extract the relevant parts. To do this, I first write a function that I want to apply to each page. # The function is going to take an input of a page get_data &lt;- function(i){ # Just look at the page of interest # Based on https://stackoverflow.com/questions/47793326/tabulize-function-in-r just_page_i &lt;- stringi::stri_split_lines(all_content[[i]])[[1]] # Grab the name of the location area &lt;- just_page_i[3] %&gt;% str_squish() area &lt;- str_to_title(area) # Grab the type of table type_of_table &lt;- just_page_i[2] %&gt;% str_squish() # Get rid of the top matter just_page_i_no_header &lt;- just_page_i[5:length(just_page_i)] # Just manually for now, but could create some rules if needed # Get rid of the bottom matter just_page_i_no_header_no_footer &lt;- just_page_i_no_header[1:62] # Just manually for now, but could create some rules if needed # Convert into a tibble demography_data &lt;- tibble(all = just_page_i_no_header_no_footer) # # Split columns demography_data &lt;- demography_data %&gt;% mutate(all = str_squish(all)) %&gt;% # Any space more than two spaces is squished down to one mutate(all = str_replace(all, &quot;10 -14&quot;, &quot;10-14&quot;)) %&gt;% mutate(all = str_replace(all, &quot;Not Stated&quot;, &quot;NotStated&quot;)) %&gt;% # Any space more than two spaces is squished down to one separate(col = all, into = c(&quot;age&quot;, &quot;male&quot;, &quot;female&quot;, &quot;total&quot;, &quot;age_2&quot;, &quot;male_2&quot;, &quot;female_2&quot;, &quot;total_2&quot;), sep = &quot; &quot;, # Just looking for a space. Seems to work fine because the tables are pretty nicely laid out remove = TRUE, fill = &quot;right&quot; ) # They are side by side at the moment, need to append to bottom demography_data_long &lt;- rbind(demography_data %&gt;% select(age, male, female, total), demography_data %&gt;% select(age_2, male_2, female_2, total_2) %&gt;% rename(age = age_2, male = male_2, female = female_2, total = total_2) ) # There is one row of NAs, so remove it demography_data_long &lt;- demography_data_long %&gt;% janitor::remove_empty(which = c(&quot;rows&quot;)) # Add the area and the page demography_data_long$area &lt;- area demography_data_long$table &lt;- type_of_table demography_data_long$page &lt;- i rm(just_page_i, i, area, type_of_table, just_page_i_no_header, just_page_i_no_header_no_footer, demography_data) return(demography_data_long) } At this point, I have a function that does what I need to each page of the PDF. I’m going to use the function map_dfr from the purrr package to apply that function to each page, and then combine all the outputs into one tibble. # Run through each relevant page and get the data pages &lt;- c(30:513) all_tables &lt;- map_dfr(pages, get_data) rm(pages, get_data, all_content) 7.10.3 Clean I now need to clean the dataset to make it useful. 7.10.3.1 Values The first step is to make the numbers into actual numbers, rather than characters. Before I can convert the type I need to remove anything that is not a number otherwise it’ll be converted into an NA. So I first identify any values that are not numbers so that I can remove them. # Need to convert male, female, and total to integers # First find the characters that should not be in there all_tables %&gt;% select(male, female, total) %&gt;% mutate_all(~str_remove_all(., &quot;[:digit:]&quot;)) %&gt;% mutate_all(~str_remove_all(., &quot;,&quot;)) %&gt;% mutate_all(~str_remove_all(., &quot;_&quot;)) %&gt;% mutate_all(~str_remove_all(., &quot;-&quot;)) %&gt;% distinct() ## # A tibble: 3 x 3 ## male female total ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &quot;&quot; &quot;&quot; &quot;&quot; ## 2 &quot;Aug&quot; &quot;&quot; &quot;&quot; ## 3 &quot;Jun&quot; &quot;&quot; &quot;&quot; # We clearly need to remove &quot;,&quot;, &quot;_&quot;, and &quot;-&quot;. # This also highlights a few issues on p. 185 that need to be manually adjusted # https://twitter.com/RohanAlexander/status/1244337583016022018 all_tables$male[all_tables$male == &quot;23-Jun&quot;] &lt;- 4923 all_tables$male[all_tables$male == &quot;15-Aug&quot;] &lt;- 4611 While you could use the janitor package here, it is worthwhile at least first looking at what is going on because sometimes there is odd stuff that janitor (and other packages) will deal with, but not in a way that you want. In this case, they’ve used Excel or similar and this has converted a couple of their entries into dates. If we just took the numbers from the column then we’d have 23 and 15 here, but by inspecting the column we can use Excel to reverse the process and enter the correct values of 4,923 and 4,611, respectively. Having identified everything that needs to be removed, we can do the actual removal and convert our character column of numbers to integers. all_tables &lt;- all_tables %&gt;% mutate_at(vars(male, female, total), ~str_remove_all(., &quot;,&quot;)) %&gt;% # First get rid of commas mutate_at(vars(male, female, total), ~str_replace(., &quot;_&quot;, &quot;0&quot;)) %&gt;% mutate_at(vars(male, female, total), ~str_replace(., &quot;-&quot;, &quot;0&quot;)) %&gt;% mutate_at(vars(male, female, total), ~as.integer(.)) 7.10.3.2 Areas The next thing to clean is the areas. We know that there are 47 counties in Kenya, and a whole bunch of sub-counties. They give us a list on pages 19 to 22 of the PDF (document pages 7 to 10). However, this list is not complete, and there are a few minor issues that we’ll deal with later. In any case, I first need to fix a few inconsistencies. # Fix some area names all_tables$area[all_tables$area == &quot;Taita/ Taveta&quot;] &lt;- &quot;Taita/Taveta&quot; all_tables$area[all_tables$area == &quot;Elgeyo/ Marakwet&quot;] &lt;- &quot;Elgeyo/Marakwet&quot; all_tables$area[all_tables$area == &quot;Nairobi City&quot;] &lt;- &quot;Nairobi&quot; Kenya has 47 counties, each of which has sub-counties. The PDF has them arranged as the county data then the sub-counties, without designating which is which. We can use the names, to a certain extent, but in a handful of cases, there is a sub-county that has the same name as a county so we need to first fix that. The PDF is made-up of three tables. all_tables$table %&gt;% table() ## . ## Table 2.3: Distribution of Population by Age, Sex*, County and Sub- County ## 48216 ## Table 2.4a: Distribution of Rural Population by Age, Sex* and County ## 5535 ## Table 2.4b: Distribution of Urban Population by Age, Sex* and County ## 5781 So I can first get the names of the counties based on those first two tables and then reconcile them to get a list of the counties. # Get a list of the counties list_counties &lt;- all_tables %&gt;% filter(table %in% c(&quot;Table 2.4a: Distribution of Rural Population by Age, Sex* and County&quot;, &quot;Table 2.4b: Distribution of Urban Population by Age, Sex* and County&quot;) ) %&gt;% select(area) %&gt;% distinct() As I hoped, there are 47 of them. But before I can add a flag based on those names, I need to deal with the sub-counties that share their name. We will do this based on the page, then looking it up and deciding which is the county page and which is the sub-county page. # The following have the issue of the name being used for both a county and a sub-county: all_tables %&gt;% filter(table == &quot;Table 2.3: Distribution of Population by Age, Sex*, County and Sub- County&quot;) %&gt;% filter(area %in% c(&quot;Busia&quot;, &quot;Garissa&quot;, &quot;Homa Bay&quot;, &quot;Isiolo&quot;, &quot;Kiambu&quot;, &quot;Machakos&quot;, &quot;Makueni&quot;, &quot;Samburu&quot;, &quot;Siaya&quot;, &quot;Tana River&quot;, &quot;Vihiga&quot;, &quot;West Pokot&quot;) ) %&gt;% select(area, page) %&gt;% distinct() ## # A tibble: 24 x 2 ## area page ## &lt;chr&gt; &lt;int&gt; ## 1 Samburu 42 ## 2 Tana River 53 ## 3 Tana River 56 ## 4 Garissa 65 ## 5 Garissa 69 ## 6 Isiolo 98 ## 7 Isiolo 100 ## 8 Machakos 149 ## 9 Machakos 154 ## 10 Makueni 159 ## # … with 14 more rows Now we can add the flag for whether the area is a county and adjust for the ones that are troublesome, # Add flag for whether it is a county or a sub-county all_tables &lt;- all_tables %&gt;% mutate(area_type = if_else(area %in% list_counties$area, &quot;county&quot;, &quot;sub-county&quot;)) # Fix the flag for the ones that have their names used twice all_tables &lt;- all_tables %&gt;% mutate(area_type = case_when( area == &quot;Samburu&quot; &amp; page == 42 ~ &quot;sub-county&quot;, area == &quot;Tana River&quot; &amp; page == 56 ~ &quot;sub-county&quot;, area == &quot;Garissa&quot; &amp; page == 69 ~ &quot;sub-county&quot;, area == &quot;Isiolo&quot; &amp; page == 100 ~ &quot;sub-county&quot;, area == &quot;Machakos&quot; &amp; page == 154 ~ &quot;sub-county&quot;, area == &quot;Makueni&quot; &amp; page == 164 ~ &quot;sub-county&quot;, area == &quot;Kiambu&quot; &amp; page == 213 ~ &quot;sub-county&quot;, area == &quot;West Pokot&quot; &amp; page == 233 ~ &quot;sub-county&quot;, area == &quot;Vihiga&quot; &amp; page == 333 ~ &quot;sub-county&quot;, area == &quot;Busia&quot; &amp; page == 353 ~ &quot;sub-county&quot;, area == &quot;Siaya&quot; &amp; page == 360 ~ &quot;sub-county&quot;, area == &quot;Homa Bay&quot; &amp; page == 375 ~ &quot;sub-county&quot;, TRUE ~ area_type ) ) rm(list_counties) 7.10.3.3 Ages Now we can deal with the ages. First we need to fix some errors. # Clean up ages table(all_tables$age) %&gt;% head() ## ## 0 0-4 1 10 10-14 10-19 ## 484 484 484 484 482 1 unique(all_tables$age) %&gt;% head() ## [1] &quot;Total&quot; &quot;0&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; # Looks like there should be 484, so need to follow up on some: all_tables$age[all_tables$age == &quot;NotStated&quot;] &lt;- &quot;Not Stated&quot; all_tables$age[all_tables$age == &quot;43594&quot;] &lt;- &quot;5-9&quot; all_tables$age[all_tables$age == &quot;43752&quot;] &lt;- &quot;10-14&quot; all_tables$age[all_tables$age == &quot;9-14&quot;] &lt;- &quot;5-9&quot; all_tables$age[all_tables$age == &quot;10-19&quot;] &lt;- &quot;10-14&quot; The census has done some of the work of putting together age-groups for us, but we want to make it easy to just focus on the counts by single-year-age. As such I’ll add a flag as to the type of age it is: an age group, such as ages 0 to 5, or a single age, such as 1. # Add a flag as to whether it&#39;s a summary or not all_tables$age_type &lt;- if_else(str_detect(all_tables$age, c(&quot;-&quot;)), &quot;age-group&quot;, &quot;single-year&quot;) all_tables$age_type &lt;- if_else(str_detect(all_tables$age, c(&quot;Total&quot;)), &quot;age-group&quot;, all_tables$age_type) At the moment, age is a character variable. We have a decision to make here, because we don’t want it to be a character variable (because it won’t graph properly), but we don’t want it to be a numeric, because there is total and also 100+ in there. So for now, we’ll just make it into a factor, and at least that will be able to be nicely graphed. all_tables$age &lt;- as_factor(all_tables$age) 7.10.4 Check 7.10.4.1 Gender sum Given the format of the data, at this point it is easy to check that total is the sum of male and female. # Check the parts and the sums follow_up &lt;- all_tables %&gt;% mutate(check_sum = male + female, totals_match = if_else(total == check_sum, 1, 0) ) %&gt;% filter(totals_match == 0) There is just one that seems wrong. # There is just one that looks wrong all_tables$male[all_tables$age == &quot;10&quot; &amp; all_tables$page == 187] &lt;- as.integer(1) rm(follow_up) 7.10.4.2 Rural-urban split The census provides different tables for the total of each county and sub-county; and then within each county, for the number in an urban area in that county, and the number in a urban area in that county. Some counties only have an urban count, but we’d like to make sure that the sum of rural and urban counts equals the total count. This requires reshaping the data from a long to wide format. First, construct different tables for each of the three. I just do it manually, but I could probably do this a nicer way. # Table 2.3 table_2_3 &lt;- all_tables %&gt;% filter(table == &quot;Table 2.3: Distribution of Population by Age, Sex*, County and Sub- County&quot;) table_2_4a &lt;- all_tables %&gt;% filter(table == &quot;Table 2.4a: Distribution of Rural Population by Age, Sex* and County&quot;) table_2_4b &lt;- all_tables %&gt;% filter(table == &quot;Table 2.4b: Distribution of Urban Population by Age, Sex* and County&quot;) Having constructed the constituent parts, I now join then based on age, area, and whether it is a county. both_2_4s &lt;- full_join(table_2_4a, table_2_4b, by = c(&quot;age&quot;, &quot;area&quot;, &quot;area_type&quot;), suffix = c(&quot;_rural&quot;, &quot;_urban&quot;)) all &lt;- full_join(table_2_3, both_2_4s, by = c(&quot;age&quot;, &quot;area&quot;, &quot;area_type&quot;), suffix = c(&quot;_all&quot;, &quot;_&quot;)) all &lt;- all %&gt;% mutate(page = glue::glue(&#39;Total from p. {page}, rural from p. {page_rural}, urban from p. {page_urban}&#39;)) %&gt;% select(-page, -page_rural, -page_urban, -table, -table_rural, -table_urban, -age_type_rural, -age_type_urban ) rm(both_2_4s, table_2_3, table_2_4a, table_2_4b) We can now check that the sum of rural and urban is the same as the total. # Check that the urban + rural = total follow_up &lt;- all %&gt;% mutate(total_from_bits = total_rural + total_urban, check_total_is_rural_plus_urban = if_else(total == total_from_bits, 1, 0), total_from_bits - total) %&gt;% filter(check_total_is_rural_plus_urban == 0) head(follow_up) ## # A tibble: 3 x 16 ## age male female total area area_type age_type male_rural female_rural ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Not … 31 10 41 Naku… county single-… 8 6 ## 2 Total 434287 441379 875666 Bomet county age-gro… 420119 427576 ## 3 Not … 3 2 5 Bomet county single-… 2 1 ## # … with 7 more variables: total_rural &lt;int&gt;, male_urban &lt;int&gt;, ## # female_urban &lt;int&gt;, total_urban &lt;int&gt;, total_from_bits &lt;int&gt;, ## # check_total_is_rural_plus_urban &lt;dbl&gt;, `total_from_bits - total` &lt;int&gt; rm(follow_up) There are just a few, but they only have a a difference of 1, so I’ll just move on. 7.10.4.3 Ages sum to age-groups Finally, I want to check that the single age counts sum to the age-groups. # One last thing to check is that the ages sum to their age-groups. follow_up &lt;- all %&gt;% mutate(groups = case_when(age %in% c(&quot;0&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;0-4&quot;) ~ &quot;0-4&quot;, age %in% c(&quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;, &quot;5-9&quot;) ~ &quot;5-9&quot;, age %in% c(&quot;10&quot;, &quot;11&quot;, &quot;12&quot;, &quot;13&quot;, &quot;14&quot;, &quot;10-14&quot;) ~ &quot;10-14&quot;, age %in% c(&quot;15&quot;, &quot;16&quot;, &quot;17&quot;, &quot;18&quot;, &quot;19&quot;, &quot;15-19&quot;) ~ &quot;15-19&quot;, age %in% c(&quot;20&quot;, &quot;21&quot;, &quot;22&quot;, &quot;23&quot;, &quot;24&quot;, &quot;20-24&quot;) ~ &quot;20-24&quot;, age %in% c(&quot;25&quot;, &quot;26&quot;, &quot;27&quot;, &quot;28&quot;, &quot;29&quot;, &quot;25-29&quot;) ~ &quot;25-29&quot;, age %in% c(&quot;30&quot;, &quot;31&quot;, &quot;32&quot;, &quot;33&quot;, &quot;34&quot;, &quot;30-34&quot;) ~ &quot;30-34&quot;, age %in% c(&quot;35&quot;, &quot;36&quot;, &quot;37&quot;, &quot;38&quot;, &quot;39&quot;, &quot;35-39&quot;) ~ &quot;35-39&quot;, age %in% c(&quot;40&quot;, &quot;41&quot;, &quot;42&quot;, &quot;43&quot;, &quot;44&quot;, &quot;40-44&quot;) ~ &quot;40-44&quot;, age %in% c(&quot;45&quot;, &quot;46&quot;, &quot;47&quot;, &quot;48&quot;, &quot;49&quot;, &quot;45-49&quot;) ~ &quot;45-49&quot;, age %in% c(&quot;50&quot;, &quot;51&quot;, &quot;52&quot;, &quot;53&quot;, &quot;54&quot;, &quot;50-54&quot;) ~ &quot;50-54&quot;, age %in% c(&quot;55&quot;, &quot;56&quot;, &quot;57&quot;, &quot;58&quot;, &quot;59&quot;, &quot;55-59&quot;) ~ &quot;55-59&quot;, age %in% c(&quot;60&quot;, &quot;61&quot;, &quot;62&quot;, &quot;63&quot;, &quot;64&quot;, &quot;60-64&quot;) ~ &quot;60-64&quot;, age %in% c(&quot;65&quot;, &quot;66&quot;, &quot;67&quot;, &quot;68&quot;, &quot;69&quot;, &quot;65-69&quot;) ~ &quot;65-69&quot;, age %in% c(&quot;70&quot;, &quot;71&quot;, &quot;72&quot;, &quot;73&quot;, &quot;74&quot;, &quot;70-74&quot;) ~ &quot;70-74&quot;, age %in% c(&quot;75&quot;, &quot;76&quot;, &quot;77&quot;, &quot;78&quot;, &quot;79&quot;, &quot;75-79&quot;) ~ &quot;75-79&quot;, age %in% c(&quot;80&quot;, &quot;81&quot;, &quot;82&quot;, &quot;83&quot;, &quot;84&quot;, &quot;80-84&quot;) ~ &quot;80-84&quot;, age %in% c(&quot;85&quot;, &quot;86&quot;, &quot;87&quot;, &quot;88&quot;, &quot;89&quot;, &quot;85-89&quot;) ~ &quot;85-89&quot;, age %in% c(&quot;90&quot;, &quot;91&quot;, &quot;92&quot;, &quot;93&quot;, &quot;94&quot;, &quot;90-94&quot;) ~ &quot;90-94&quot;, age %in% c(&quot;95&quot;, &quot;96&quot;, &quot;97&quot;, &quot;98&quot;, &quot;99&quot;, &quot;95-99&quot;) ~ &quot;95-99&quot;, TRUE ~ &quot;Other&quot;) ) %&gt;% group_by(area_type, area, groups) %&gt;% mutate(group_sum = sum(total, na.rm = FALSE), group_sum = group_sum / 2, difference = total - group_sum) %&gt;% ungroup() %&gt;% filter(age == groups) %&gt;% filter(total != group_sum) head(follow_up) ## # A tibble: 6 x 16 ## age male female total area area_type age_type male_rural female_rural ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 0-4 1 5 6 Mt. … sub-coun… age-gro… NA NA ## 2 5-9 1 2 3 Mt. … sub-coun… age-gro… NA NA ## 3 10-14 6 0 6 Mt. … sub-coun… age-gro… NA NA ## 4 15-19 9 1 10 Mt. … sub-coun… age-gro… NA NA ## 5 20-24 21 4 25 Mt. … sub-coun… age-gro… NA NA ## 6 25-29 59 9 68 Mt. … sub-coun… age-gro… NA NA ## # … with 7 more variables: total_rural &lt;int&gt;, male_urban &lt;int&gt;, ## # female_urban &lt;int&gt;, total_urban &lt;int&gt;, groups &lt;chr&gt;, group_sum &lt;dbl&gt;, ## # difference &lt;dbl&gt; rm(follow_up) Mt. Kenya Forest, Aberdare Forest, Kakamega Forest are all slightly dodgy. I can’t see it in the documentation, but it looks like they have apportioned these between various countries. It’s understandable why they’d do this and it’s probably not a big deal, so I’ll just move on. 7.10.5 Tidy-up Now that we are confident that everything is looking good, we can just convert it to long-format so that it is easy to work with. all &lt;- all %&gt;% rename(male_total = male, female_total = female, total_total = total) %&gt;% pivot_longer(cols = c(male_total, female_total, total_total, male_rural, female_rural, total_rural, male_urban, female_urban, total_urban), names_to = &quot;type&quot;, values_to = &quot;number&quot; ) %&gt;% separate(col = type, into = c(&quot;gender&quot;, &quot;part_of_area&quot;), sep = &quot;_&quot;) %&gt;% select(area, area_type, part_of_area, age, age_type, gender, number) write_csv(all, path = &quot;outputs/data/cleaned_kenya_2019_census.csv&quot;) head(all) ## # A tibble: 6 x 7 ## area area_type part_of_area age age_type gender number ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Mombasa county total Total age-group male 610257 ## 2 Mombasa county total Total age-group female 598046 ## 3 Mombasa county total Total age-group total 1208303 ## 4 Mombasa county rural Total age-group male NA ## 5 Mombasa county rural Total age-group female NA ## 6 Mombasa county rural Total age-group total NA 7.10.6 Make Monica’s dataset The original purpose of all of this was to make a table for Monica. She needed single-year counts, by gender, for the counties. monicas_dataset &lt;- all %&gt;% filter(area_type == &quot;county&quot;) %&gt;% filter(part_of_area == &quot;total&quot;) %&gt;% filter(age_type == &quot;single-year&quot;) %&gt;% select(area, age, gender, number) head(monicas_dataset) ## # A tibble: 6 x 4 ## area age gender number ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 Mombasa 0 male 15111 ## 2 Mombasa 0 female 15009 ## 3 Mombasa 0 total 30120 ## 4 Mombasa 1 male 15805 ## 5 Mombasa 1 female 15308 ## 6 Mombasa 1 total 31113 write_csv(monicas_dataset, &quot;outputs/data/monicas_dataset.csv&quot;) I’ll leave the fancy stats to Monica, but I’ll just make a quick graph of Nairobi. monicas_dataset %&gt;% filter(area == &quot;Nairobi&quot;) %&gt;% ggplot() + geom_col(aes(x = age, y = number, fill = gender), position = &quot;dodge&quot;) + scale_y_continuous(labels = scales::comma) + scale_x_discrete(breaks = c(seq(from = 0, to = 99, by = 5), &quot;100+&quot;)) + theme_classic()+ scale_fill_brewer(palette = &quot;Set1&quot;) + labs(y = &quot;Number&quot;, x = &quot;Age&quot;, fill = &quot;Gender&quot;, title = &quot;Distribution of age and gender in Nairobi in 2019&quot;, caption = &quot;Data source: 2019 Kenya Census&quot;) 7.11 Optical Character Recognition All of the above is predicated on having a PDF that is already ‘digitized’. But what if it is images? In that case you need to first use Optical Character Recognition (OCR). The go-to package is Tesseract (Ooms 2019c). This is a R wrapper around the Tesseract open source OCR engine. Let’s see an example with a scan from the first page of Jane Eyre (Figure 7.13). Figure 7.13: Scan of first page of Jane Eyre. # install.packages(&#39;tesseract&#39;) library(tesseract) # eng &lt;- tesseract(&quot;eng&quot;) text &lt;- tesseract::ocr(here::here(&quot;figures/jane_scan.png&quot;), engine = tesseract(&quot;eng&quot;)) cat(text) ## 1 THERE was no possibility of taking a walk that day. We had ## been wandering, indeed, in the leafless shrubbery an hour in ## the morning; but since dinner (Mrs Reed, when there was no com- ## pany, dined early) the cold winter wind had brought with it clouds ## so sombre, and a rain so penetrating, that further out-door exercise ## ## was now out of the question. ## ## I was glad of it: I never liked long walks, especially on chilly ## afternoons: dreadful to me was the coming home in the raw twi- ## light, with nipped fingers and toes, and a heart saddened by the ## chidings of Bessie, the nurse, and humbled by the consciousness of ## my physical inferiority to Eliza, John, and Georgiana Reed. ## ## The said Eliza, John, and Georgiana were now clustered round ## their mama in the drawing-room: she lay reclined on a sofa by the ## fireside, and with her darlings about her (for the time neither quar- ## relling nor crying) looked perfectly happy. Me, she had dispensed ## from joining the group; saying, ‘She regretted to be under the ## necessity of keeping me at a distance; but that until she heard from ## Bessie, and could discover by her own observation that I was ## endeavouring in good earnest to acquire a more sociable and ## child-like disposition, a more attractive and sprightly manner— ## something lighter, franker, more natural as it were—she really ## must exclude me from privileges intended only for contented, ## happy, littie children.’ ## ## ‘What does Bessie say I have done?’ I asked. ## ## ‘Jane, I don’t like cavillers or questioners: besides, there is ## something truly forbidding in a child taking up her elders in that ## manner. Be seated somewhere; and until you can speak pleasantly, ## remain silent.’ ## ## . Bs aT sae] eae ## ## i; AN TCM TAN | Beal | Sees ## a) } ; | i) ## i i 4 | | A ae | i | eee eek? ## ## a an eames yi | bee ## 1 nea elem | | oe pee ## i i ae BC i i Hale ## oul | ec hi ## pan || i re a al! | ## ## ase } Oty 2 RIES ORT Sata ariel ## SEEN BE — =——_ ## 15 7.12 Text Aspects of this section have been previously published. 7.12.1 Introduction Text data is all around us, and in many cases is some of the earliest types of data that we are exposed to. Recent increases in computational power, the development of new methods, and the enormous availability of text, means that there has been a great deal of interest in using text as data. Initial methods tend to focus, essentially, on converting text into numbers and then analysing them using traditional methods. More recent methods have begun to take advantage of the structure that is inherent in text, to draw additional meaning. The difference is perhaps akin to a child who can group similar colors, compared with a child who knows what objects are; although both crocodiles and trees are green, and you can do something with that knowledge, you can do more by knowing that a crocodile could eat you, and a tree probably won’t. In this section we cover a variety of techniques designed to equip you with the basics of using text as data. One of the great things about text data is that it is typically not generated for the purposes of our analysis. That’s great because it removes one of the unobservable variables that we typically have to worry about. The trade-off is that we typically have to do a bunch more work to get it into a form that we can work with. 7.12.2 Getting text data Text as data is an exciting tool to apply. But many guides assume that you already have a nice dataset. Because we’ve focused on workflow in these notes, we know that’s not likely to be true! In this section we will scrape some text from a website. We’ve already seen examples of scraping, but in general those were focused on exploiting tables in the website. Here we’re going to instead focus on paragraphs of text, hence we’ll focus on different html/css tags. We’re going to us the rvest package to make it easier to scrape data. We’re also going to use the purrr package to apply a function to a bunch of different URLs. For those of you with a little bit of programming, this is an alternative to using a for loop. For those of you with a bit of CS, this is a package that adds functional programming to R. library(rvest) library(tidyverse) # Some websites address_to_visit &lt;- c(&quot;https://www.rba.gov.au/monetary-policy/rba-board-minutes/2020/2020-03-03.html&quot;, &quot;https://www.rba.gov.au/monetary-policy/rba-board-minutes/2020/2020-02-04.html&quot;, &quot;https://www.rba.gov.au/monetary-policy/rba-board-minutes/2019/2019-12-03.html&quot;, &quot;https://www.rba.gov.au/monetary-policy/rba-board-minutes/2019/2019-11-05.html&quot;, &quot;https://www.rba.gov.au/monetary-policy/rba-board-minutes/2019/2019-10-01.html&quot;, &quot;https://www.rba.gov.au/monetary-policy/rba-board-minutes/2019/2019-09-03.html&quot; ) # Save names save_name &lt;- address_to_visit %&gt;% str_remove(&quot;https://www.rba.gov.au/monetary-policy/rba-board-minutes/&quot;) %&gt;% str_remove(&quot;.html&quot;) %&gt;% str_remove(&quot;20[:digit:]{2}/&quot;) %&gt;% str_c(&quot;inputs/rba/&quot;, ., &quot;.csv&quot;) Create the function that will visit address_to_visit and save to save_name files. visit_address_and_save_content &lt;- function(name_of_address_to_visit, name_of_file_to_save_as) { # The function takes two inputs name_of_address_to_visit &lt;- address_to_visit[1] name_of_file_to_save_as &lt;- save_name[1] read_html(name_of_address_to_visit) %&gt;% # Go to the website and read the html html_node(&quot;#content&quot;) %&gt;% # Find the content part html_text() %&gt;% # Extract the text of the content part write_lines(name_of_file_to_save_as) # Save as a text file print(paste(&quot;Done with&quot;, name_of_address_to_visit, &quot;at&quot;, Sys.time())) # Helpful so that you know progress when running it on all the records Sys.sleep(sample(30:60, 1)) # Space out each request by somewhere between # 30 and 60 seconds each so that we don&#39;t overwhelm their server } # If there is an error then ignore it and move to the next one visit_address_and_save_content &lt;- safely(visit_address_and_save_content) We now apply that function to our list of URLs. # Walk through the addresses and apply the function to each walk2(address_to_visit, save_name, ~ visit_address_and_save_content(.x, .y)) The result is a bunch of files with saved text data. In this case we used scraping, but there are, of course, many ways. We may be able to use APIs, for instance, In the case of the Airbnb dataset that we examined earlier in the notes. If you are lucky then it may simply be that there is a column that contains text data in your dataset. 7.12.3 Preparing text datasets This section draws on Sharla Gelfand’s blog post, linked in the required readings. As much as I would like to stick with Australian economics and politics examples, I realise that this is probably only of limited interest to most of you. As such, in this section we will consider a dataset of Sephora reviews. Please read Sharla’s blog post (https://sharla.party/post/crying-sephora/) for another take on this dataset. In this section we assume that there is some text data that you have gathered. At this point we need to change it into a form that we can work with. For some applications this will be counts of words. For others it may be some variant of this. The dataset that we are going to use is from Sephora, was scraped by Connie and I originally became aware of it because of Sharla. First let’s read in the data. # This code is taken from https://sharla.party/post/crying-sephora/ library(dplyr) library(jsonlite) library(tidytext) crying &lt;- fromJSON(&quot;https://raw.githubusercontent.com/everestpipkin/datagardens/master/students/khanniie/5_newDataSet/crying_dataset.json&quot;, simplifyDataFrame = TRUE ) crying &lt;- as_tibble(crying[[&quot;reviews&quot;]]) head(crying) ## # A tibble: 6 x 6 ## date product_info$br… $name $type $url review_body review_title stars ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 29 M… Too Faced Bett… Masc… http… &quot;Now I can… AWESOME 5 st… ## 2 29 S… Too Faced Bett… Masc… http… &quot;This hold… if you&#39;re s… 5 st… ## 3 23 M… Too Faced Bett… Masc… http… &quot;I just bo… Hate it 1 st… ## 4 15 A… Too Faced Bett… Masc… http… &quot;To start … Nearly perf… 5 st… ## 5 21 S… Too Faced Bett… Masc… http… &quot;This masc… Amazing!! 5 st… ## 6 30 M… Too Faced Bett… Masc… http… &quot;Let&#39;s tal… Tricky but … 5 st… ## # … with 1 more variable: userid &lt;dbl&gt; names(crying) ## [1] &quot;date&quot; &quot;product_info&quot; &quot;review_body&quot; &quot;review_title&quot; &quot;stars&quot; ## [6] &quot;userid&quot; We’ll focus on the review_body variable and the number of stars stars that the reviewer gave. Most of them are 5 stars, so we’ll just focus on whether or not the review is five stars. crying &lt;- crying %&gt;% select(review_body, stars) %&gt;% mutate(stars = str_remove(stars, &quot; stars?&quot;), # The question mark at the end means it&#39;l get rid of &#39;star&#39; and &#39;stars&#39;. stars = as.integer(stars) ) %&gt;% mutate(five_stars = if_else(stars == 5, 1, 0)) table(crying$stars) ## ## 1 2 3 4 5 ## 6 2 4 14 79 In this example we are going to split everything into separate words. When we do this it is just searching for a space, and so what other types of elements are going to be considered ‘words’? crying_by_words &lt;- crying %&gt;% unnest_tokens(word, review_body, token = &quot;words&quot;) head(crying_by_words) ## # A tibble: 6 x 3 ## stars five_stars word ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 5 1 now ## 2 5 1 i ## 3 5 1 can ## 4 5 1 cry ## 5 5 1 all ## 6 5 1 i We now want to count the number of times each word is used by each of the star classifications. crying_by_words &lt;- crying_by_words %&gt;% count(stars, word, sort = TRUE) head(crying_by_words) ## # A tibble: 6 x 3 ## stars word n ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 5 i 348 ## 2 5 and 249 ## 3 5 the 239 ## 4 5 it 211 ## 5 5 a 193 ## 6 5 this 178 crying_by_words %&gt;% filter(stars == 1) %&gt;% head() ## # A tibble: 6 x 3 ## stars word n ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 the 39 ## 2 1 i 24 ## 3 1 and 21 ## 4 1 it 21 ## 5 1 to 19 ## 6 1 my 16 So you can see that the most popular word for five star reviews is ‘i’, and that the most popular word for one star reviews is ‘the’. At this point, we can use the data to do a whole bunch of different things, but one nice measure to look at is term frequency e.g. in this case how many times is a word used in reviews with a particular star rating. The issue is that there are a lot of words that are commonly used regardless of context. As such, we may also like to look at the inverse document frequency in which we ‘penalise’ words that occur in many particular star ratings. For instance, ‘the’ probably occurs in both one star and five star reviews and so its idf is lower than ‘hate’ which probably only occurs in one star reviews. The term frequency–inverse document frequency (tf-idf) is then the product of these. We can create this value using the bind_tf_idf() function from the tidytext package, and this will create a bunch of new columns, one for each word and star combination. # This code, and the one in the next block, is from Julia Silge: https://juliasilge.com/blog/sherlock-holmes-stm/ crying_by_words_tf_idf &lt;- crying_by_words %&gt;% bind_tf_idf(word, stars, n) %&gt;% arrange(-tf_idf) head(crying_by_words_tf_idf) ## # A tibble: 6 x 6 ## stars word n tf idf tf_idf ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 below 1 0.00826 1.61 0.0133 ## 2 2 boy 1 0.00826 1.61 0.0133 ## 3 2 choice 1 0.00826 1.61 0.0133 ## 4 2 contrary 1 0.00826 1.61 0.0133 ## 5 2 exceptionally 1 0.00826 1.61 0.0133 ## 6 2 migrates 1 0.00826 1.61 0.0133 crying_by_words_tf_idf %&gt;% group_by(stars) %&gt;% top_n(10) %&gt;% ungroup %&gt;% mutate(word = reorder_within(word, tf_idf, stars)) %&gt;% mutate(stars = as_factor(stars)) %&gt;% filter(stars %in% c(1, 5)) %&gt;% ggplot(aes(word, tf_idf, fill = stars)) + geom_col(show.legend = FALSE) + facet_wrap(vars(stars), scales = &quot;free&quot;) + scale_x_reordered() + coord_flip() + labs(x = &quot;Word&quot;, y = &quot;tf-idf&quot;) + theme_minimal() + scale_fill_brewer(palette = &quot;Set1&quot;) References "],["hunt-data.html", "Chapter 8 Hunt data 8.1 Experiments and randomised controlled trials 8.2 Case study - Fisher’s tea party 8.3 Case study - Tuskegee Syphilis Study 8.4 Case study - The Oregon Health Insurance Experiment 8.5 Case study - Student Coaching: How Far Can Technology Go? 8.6 A/B testing 8.7 Case study - Upworthy 8.8 Sampling and survey essentials 8.9 Implementing surveys 8.10 Next steps", " Chapter 8 Hunt data Required reading Banerjee, Abhijit Vinayak, 2020, ‘Field Experiments and the Practice of Economics’, American Economic Review, Vol. 110, No. 7, pp. 1937-1951. Duflo, Esther, 2020, ‘Field Experiments and the Practice of Policy’, American Economic Review, Vol. 110, No. 7, pp. 1952-1973 (or watch the speech detailed below). Fisher, Ronald, 1935, The Design of Experiments, pp. 20-29, https://archive.org/details/in.ernet.dli.2015.502684/page/n33/mode/2up. Fry, Hanna, 2020, ‘Experiments on Trial’, The New Yorker, 2 March, pp. 61-65, https://www.newyorker.com/magazine/2020/03/02/big-tech-is-testing-you. Gertler, Paul, Sebastian Martinez, Patrick Premand, Laura Rawlings, and Christel Vermeersch, Impact Evaluation in Practice, Chapters 3 and 4, https://www.worldbank.org/en/programs/sief-trust-fund/publication/impact-evaluation-in-practice. Hill, Austin Bradford, 1965, ‘The Environment and Disease: Association or Causation?’, Proceedings of the Royal Society of Medicine, 58, 5, 295-300. Kohavi, Ron and Stefan Thomke, 2017, ‘The Surprising Power of Online Experiments’, Harvard Business Review, September-October, https://hbr.org/2017/09/the-surprising-power-of-online-experiments. Kohavi, Ron, Diane Tang, and Ya Xu, 2020, Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing, Cambridge University Press. (This sounds like a lot, but it’s a light book - it’s more about providing examples of issues to think about.) (Freely available through the U of T library.) Taback, Nathan, 2020, Design of Experiments and Observational Studies, Chapter 8 - Completely Randomized Designs: Comparing More Than Two Treatments, https://scidesign.github.io/designbook/completely-randomized-designs-comparing-more-than-two-treatments.html. Taylor, Sean, Dean Eckles, 2017, ‘Randomized experiments to detect and estimate social influence in networks’, arXiv, https://arxiv.org/abs/1709.09636v1. Wu, Changbao and Mary E. Thompson, 2020, Sampling Theory and Practice, Springer, Chapters 1-3, and 5 (freely available through the U of T library). Required viewing Register, Yim, 2020, ‘Introduction to Sampling and Randomization’, Online Causal Inference Seminar, 14 November, https://youtu.be/U272FFxG8LE. Xu, Ya, 2020, ‘Causal inference challenges in industry, a perspective from experiences at LinkedIn’, Online Causal Inference Seminar, 16 July, https://youtu.be/OoKsLAvyIYA. Recommended reading Angrist, Joshua D., and Jörn-Steffen Pischke, 2008, Mostly harmless econometrics: An empiricist’s companion, Princeton University Press, Chapter 2. Banerjee, Abhijit Vinayak, Esther Duflo, Rachel Glennerster, and Dhruva Kothari, 2010, ‘Improving immunisation coverage in rural India: clustered randomised controlled evaluation of immunisation campaigns with and without incentives’, BMJ, 340, c2220. Beaumont, Jean-François, 2020, ‘Are probability surveys bound to disappear for the production of official statistics?’, Survey Methodology, 46 (1), Statistics Canada, Catalogue No. 12-001-X. Christian, Brian, 2012, ‘The A/B Test: Inside the Technology That’s Changing the Rules of Business’, Wired, 25 April, https://www.wired.com/2012/04/ff-abtesting/. Dablander, Fabian, 2020, “An Introduction to Causal Inference”, PsyArXiv, 13 February, doi:10.31234/osf.io/b3fkw, https://psyarxiv.com/b3fkw. Deaton, Angus, 2010, ‘Instruments, Randomization, and Learning about Development’, Journal of Economic Literature, vol. 48, no. 2, pp. 424-455. Duflo, Esther, Rachel Glennerster, and Michael Kremer, 2007, ‘Using Randomization In Development Economics Research: A Toolkit’, https://economics.mit.edu/files/806. Gordon, Brett R., Florian Zettelmeyer, Neha Bhargava, and Dan Chapsky, 2019, ‘A Comparison of Approaches to Advertising Measurement: Evidence from Big Field Experiments at Facebook’, Marketing Science, Vol. 38, No. 2, March–April, pp. 193–225. Groves, Robert M., 2011, ‘Three Eras of Survey Research’, Public Opinion Quarterly, 75 (5), pp. 861–871, https://doi.org/10.1093/poq/nfr057. Hillygus, D. Sunshine, 2011, ‘The evolution of election polling in the United States’, Public Opinion Quarterly, 75 (5), pp. 962-981. Imai, Kosuke, 2017, Quantitative Social Science: An Introduction, Princeton University Press, Ch 2.3, 2.4, 4.3. Jeffries, Adrianne, Leon Yin, and Surya Mattu, 2020, ‘Swinging the Vote?’, The Markup, 26 February, https://themarkup.org/google-the-giant/2020/02/26/wheres-my-email. Kohavi, Ron, Alex Deng, Brian Frasca, Roger Longbotham, Toby Walker, and Ya Xu. 2012. Trustworthy online controlled experiments: five puzzling outcomes explained. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD ’12). Association for Computing Machinery, New York, NY, USA, 786–794. DOI:https://doi.org/10.1145/2339530.2339653 Landesberg, Eddie, Molly Davies, and Stephanie Yee, 2019, ‘Want to make good business decisions? Learn causality’, MultiThreaded, Stitchfix blog, 19 December, https://multithreaded.stitchfix.com/blog/2019/12/19/good-marketing-decisions/. Levay, Kevin E., Jeremy Freese, and James N. Druckman, 2016, ‘The demographic and political composition of Mechanical Turk samples’, Sage Open, 6 (1), 2158244016636433. Lewis, Randall A., and David H. Reiley, 2014 ‘Online ads and offline sales: Measuring the effects of retail advertising via a controlled experiment on Yahoo!’, Quantitative Marketing and Economics, Vol 12, pp. 235–266. Mullinix, Kevin J., Leeper, Thomas J., Druckman, James N. and Freese, Jeremy, 2015, ‘The generalizability of survey experiments’, Journal of Experimental Political Science, 2 (2), pp. 109-138. Novak, Greg, Sven Schmit, and Dave Spiegel, 2020, Experimentation with resource constraints, 18 November, StitchFix Blog, https://multithreaded.stitchfix.com/blog/2020/11/18/virtual-warehouse/. Prepared for the AAPOR Executive Council by a Task Force operating under the auspices of the AAPOR Standards Committee, with members including:, Reg Baker, Stephen J. Blumberg, J. Michael Brick, Mick P. Couper, Melanie Courtright, J. Michael Dennis, Don Dillman, Martin R. Frankel, Philip Garland, Robert M. Groves, Courtney Kennedy, Jon Krosnick, Paul J. Lavrakas, Sunghee Lee, Michael Link, Linda Piekarski, Kumar Rao, Randall K. Thomas, Dan Zahs, 2010, ‘Research Synthesis: AAPOR Report on Online Panels’, Public Opinion Quarterly, 74 (4), pp. 711–781, https://doi.org/10.1093/poq/nfq048. Ryan, A. C., A. R. MacKenzie, S. Watkins, and R. Timmis, 2012, ‘World War II contrails: a case study of aviation‐induced cloudiness’, International journal of climatology, 32, no. 11, pp. 1745-1753. Said, Chris, 2020, ‘Optimizing sample sizes in A/B testing, Part I: General summary’, 10 January, https://chris-said.io/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-I/. (See also parts 2 and 3). Stolberg, Michael, 2006, ‘Inventing the randomized double-blind trial: the Nuremberg salt test of 1835’, Journal of the Royal Society of Medicine, 99, no. 12, pp. 642-643. Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel, 2019, popular science background, https://www.nobelprize.org/uploads/2019/10/popular-economicsciencesprize2019-2.pdf. Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel, 2019, scientific background, https://www.nobelprize.org/uploads/2019/10/advanced-economicsciencesprize2019.pdf. Taddy, Matt, 2019, Business Data Science, Chapter 5. Urban, Steve, Rangarajan Sreenivasan, and Vineet Kannan, 2016, ‘It’s All A/Bout Testing: The Netflix Experimentation Platform’, Netflix Technology Blog, 29 April, https://netflixtechblog.com/its-all-a-bout-testing-the-netflix-experimentation-platform-4e1ca458c15. VWO, ‘A/B Testing Guide’, https://vwo.com/ab-testing/. Yeager, David S., Jon A. Krosnick, LinChiat Chang, Harold S. Javitz, Matthew S. Levendusky, Alberto Simpser, Rui Wang, 2011, ‘Comparing the Accuracy of RDD Telephone Surveys and Internet Surveys Conducted with Probability and Non-Probability Samples’, Public Opinion Quarterly, 75 (4), pp. 709–747, https://doi.org/10.1093/poq/nfr020. Yin, Xuan and Ercan Yildiz, 2020, ‘The Causal Analysis of Cannibalization in Online Products’, Code as Craft, Etsy blog, 24 February, https://codeascraft.com/2020/02/24/the-causal-analysis-of-cannibalization-in-online-products/. Recommended listening Galef, Julia, 2020, ‘Episode 246: Deaths of despair / Effective altruism (Angus Deaton)’, Rationally Speaking, from 35:30 through to the end, available at: http://rationallyspeakingpodcast.org/show/episode-246-deaths-of-despair-effective-altruism-angus-deato.html. Recommended viewing Duflo, Esther, 2020, ‘Inteview with Esther Duflo’, 12 October, Online Causal Inference Seminar, https://youtu.be/WWW9q3oMYxU. Duflo, Esther, 2019, ‘Nobel Prize Lecture’, 8 December 2019, Stockholm: https://www.nobelprize.org/prizes/economic-sciences/2019/duflo/lecture/. Tipton, Elizabeth, 2020, ‘Will this Intervention Work in this Population? Designing Randomized Trials for Generalization’, Online Causal Inference Seminar, 14 April, https://youtu.be/HYP32wzEZMA. Key concepts/skills/etc Treatment and control groups. Internal and external validity. Average treatment effect. Generating simulated datasets. Defining populations, frames and samples. Distinguishing probability and non-probability sampling Distinguishing strata and clusters. Key libraries broom ggplot2 tidyverse Key functions/etc aov() rnorm() sample() t.test() Quiz In your own words, what is the role of randomisation in constructing a counterfactual (write two or three paragraphs)? What is external validity (pick one)? Findings from an experiment hold in that setting. Findings from an experiment hold outside that setting. Findings from an experiment that has been repeated many times. Findings from an experiment for which code and data are available. What is internal validity (pick one)? Findings from an experiment hold in that setting. Findings from an experiment hold outside that setting. Findings from an experiment that has been repeated many times. Findings from an experiment for which code and data are available. If we have a dataset named ‘netflix_data’, with the columns ‘person’ and ‘tv_show’ and ‘hours’, (person is a character class uniqueID for every person, tv_show is a character class name of a tv show, and hours is double expressing the number of hours that person watched that tv show). Could you please write some code that would randomly assign people into one of two groups? The data looks like this: library(tidyverse) netflix_data &lt;- tibble(person = c(&quot;Rohan&quot;, &quot;Rohan&quot;, &quot;Monica&quot;, &quot;Monica&quot;, &quot;Monica&quot;, &quot;Patricia&quot;, &quot;Patricia&quot;, &quot;Helen&quot;), tv_show = c(&quot;Broadchurch&quot;, &quot;Duty-Shame&quot;, &quot;Broadchurch&quot;, &quot;Duty-Shame&quot;, &quot;Shetland&quot;, &quot;Broadchurch&quot;, &quot;Shetland&quot;, &quot;Duty-Shame&quot;), hours = c(6.8, 8.0, 0.8, 9.2, 3.2, 4.0, 0.2, 10.2) ) In the context of randomisation, what does stratification mean to you (write a paragraph or two)? How could you check that your randomisation had been done appropriately (write two or three paragraphs)? Identify three companies that conduct A/B testing commercially and write a short paper about how they work and the trade-offs of each. Are there any notable Toronto-based or Canadian companies? Why do you think this might be the case? Pretend that you work as a junior analyst for a large consulting firm. Further, pretend that your consulting firm has taken a contract to put together a facial recognition model for the Canada Border Services Agency’s Inland Enforcement branch. Taking a page or two, please discuss your thoughts on this matter. What would you do and why? What are some types of probability sampling, and in what circumstances might you want to implement them (write two or three pages)? There have been some substantial political polling ‘misses’ in recent years (Trump and Brexit come to mind). To what extent do you think non-response bias was the cause of this (write a page or two, being sure to ground your writing with citations)? What is an estimate (pick one)? A rule for calculating an estimate of a given quantity based on observed data. The quantity of interest. The result. Unknown numbers that determine a statistical model. What is an estimator (pick one)? A rule for calculating an estimate of a given quantity based on observed data. The quantity of interest. The result. Unknown numbers that determine a statistical model. What is an estimand (pick one)? A rule for calculating an estimate of a given quantity based on observed data. The quantity of interest. The result. Unknown numbers that determine a statistical model. What is a parameter (pick one)? A rule for calculating an estimate of a given quantity based on observed data. The quantity of interest. The result. Unknown numbers that determine a statistical model. It seems like a lot of businesses have closed in downtown Toronto since the pandemic. To investigate this, I decide to walk along some blocks downtown and count the number of businesses that are closed and open. To decide which blocks to walk, I open a map of Toronto, start at the lake, and then pick every 10th street. This type of sampling is (select all)? Cluster sampling. Systematic sampling. Stratified sampling. Simple random sampling. Convenience sampling. Please name some reasons why you may wish to use cluster sampling (select all)? Balance in responses. Administrative convenience. Efficiency in terms of money. Underlying systematic concerns. Estimation of sub-populations. Please consider Beaumont, 2020, ‘Are probability surveys bound to disappear for the production of official statistics?’. With reference to that paper, do you think that probability surveys will disappear, and why or why not (please write a paragraph or two)? 8.1 Experiments and randomised controlled trials 8.1.1 Introduction First a note on Ronald Fisher and Francis Galton. Fisher and Galton are the intellectual grandfathers of much of the work that we cover. In some cases it is directly their work, in other cases it is work that built on their contributions. Both of these men believed in eugenics, amongst other things that are generally reprehensible. This chapter is about experiments. This is a situation in which we can explicitly control and vary some aspects. The advantage of this is that identification should be clear. There is a treatment group that is treated and a control group that is not. These are randomly split. And so if they end up different then it must be because of the treatment. Unfortunately, life is rarely so smooth. Arguing about how similar the treatment and control groups were tends to carry on indefinitely, because our ability to speak to internal validity affects our ability to speak to external validity. It’s also important to note that the statistics of this were designed in agricultural settings ‘does fertilizer work?’, etc. In those settings you can more easily divide a field into ‘treated’ and ‘non-treated’, and the magnitude of the effect is large. In general, these same statistical approaches are still used today (especially in the social sciences) but often inappropriately. If you hear someone talking about ‘having enough power’ and similar phrases, then it’s not necessarily that they’re not right, but it usually pays to take a step back and really think about what is being done and whether they really know what they’re doing. 8.1.2 Motivation and notation Never forget: if your sampling is in any way non-representative, your observe[d] data is not sufficient for population estimates. You must deal with design, sampling issues, data quality, and misclassification. Otherwise you’ll just be wrong. Dan Simpson, 30 January 2020. When Monica and I moved to San Francisco, the Giants immediately won the baseball, and the Warriors began a historic streak. We moved to Chicago and the Cubs won the baseball for the first time in a hundred years. We then moved to Massachusetts, and the Patriots won the Super Bowl again and again and again. Finally, we moved to Toronto, and the Raptors won the basketball. Should a city pay us to live there or could their funds be better spent elsewhere? One way to get at the answer would be to run an experiment. Make a list of the North American cities with major sports teams, and then roll a dice and send us to live there for a year. If we had enough lifetimes, then we could work it out. The fundamental issue is that we cannot both live in a city and not live in a city. Experiments and randomised controlled trials are circumstances in which we try to randomly allocate some treatment, so as to have a belief that everything else was constant (or at least ignorable). In the words of Hernan and Robins (2020, 3) an action, \\(A\\), is also known ‘as an intervention, an exposure, or a treatment.’ I’ll typically use ‘treated/control’ language, reflecting whether an action was imposed or not. That treatment random variable will typically be binary, that is 0 or 1, ‘treated’ or ‘not treated/control/comparison’. We’ll then typically have some outcome random variable, \\(Y\\), which will typically be binary, able to be made binary, or continuous, although we’ll touch on other options. An example of a binary outcome could be vote choice - ‘Conservative’ vs ‘Not Conservative’ - noticing there that I grouped all the other parties into simply ‘Not Conservative’ to force the binary outcome. Further following Hernan and Robins (2020, 4), but in the notation of Gertler et al. (2016, 48) we describe a treatment as ‘causal’ when \\((Y|a=0)\\neq (Y|a=1)\\). As discussed above, the fundamental problem of causal inference is that we cannot both treat and control the one individual. So when we want to know the effect of the treatment, we need to compare it with the counterfactual, which is what would have happened if the individual were not treated. So causal inference turns out to be fundamentally a missing data problem.6 To quote from Gertler et al. (2016, 48), in the context of evaluating income in response to an intervention program: To put it another way, we would like to measure income at the same point in time for the same unit of observation (a person, in this case), but in two different states of the world. If it were possible to do this, we would be observing how much income the same individual would have had at the same point in time both with and without the program, so that the only possible explanation for any difference in that person’s income would be the program. By comparing the same individual with herself at the same moment, we would have managed to eliminate any outside factors that might also have explained the difference in outcomes. We could then be confident that the relationship between the vocational training program and the change in income is causal… [A] unit either participated in the program or did not participate. The unit cannot be observed simultaneously in two different states (in other words, with and without the program). As we cannot compared treatment and control in one particular individual, we instead compare the average of two groups - all those treated and all those not. We are looking to estimate the counterfactual. We usually consider a default that there’s no effect and we require evidence for us to change our mind. As we’re interested in what is happening in groups, we turn to expectations, and notions of probability to express ourselves. Hence, we’ll make claims that talk, on average. Maybe wearing fun socks really does make you have a lucky day, but on average across the population, it’s probably not the case.7 It’s worth pointing out that we don’t just have to be interested in the average effect. We may consider the median, or variance, or whatever. Nonetheless, if we were interested in the average effect, then one way to proceed would be to divide the dataset into two - treated and not treated - have an effect column of 0s and 1s, sum the column and divide it by the length of the column, and then look at the ratio. This would be an estimator, which is a way of putting together a guess of something of interest. The estimand is the thing of interest, in this case the average effect, and the estimate is whatever our guess turns out to be. To give another example, following Gelman, Hill, and Vehtari (2020): An estimand, or quantity of interest, is some summary of parameters or data that somebody is interested in estimating. For example, in the regression model, \\(y = a + bx + \\epsilon\\), the parameters \\(a\\) and \\(b\\) might be of interest…. We use the data to construct estimates of parameters and other quantities of interest. More broadly, Cunningham (2021) defines causal inference as ‘…the leveraging of theory and deep knowledge of institutional details to estimate the impact of events and choices on a given outcome of interest.’ In the previous chapter we discussed gathering data which we observed about the world. In this chapter we are going to be more active. Cunningham (2021) says that experimental data ‘is collected in something akin to a laboratory environment. In a traditional experiment, the researcher participates actively in the process being recorded.’ That is, if we want to use this data then as researchers we have to go out and hunt it, if you like. 8.1.3 Randomised sampling Correlation can be enough in some settings, but in order to be able to make forecasts when things change and the circumstances are slightly different we need to understand causation. The key is the counterfactual - what would have happened in the absence of the treatment. Ideally we could keep everything else constant, randomly divide the world into two groups, and then treat one and not the other. Then we can be pretty confident that any difference between the two groups is due to that treatment. The reason for this is that if we have some population and we randomly select two groups from it, then our two groups (so long as they are both big enough) should have the same characteristics as the population. Randomised controlled trials (RCTs) and A/B testing attempts to get us as close to this ‘gold standard’ as we can hope. RCTs are often described as the ‘gold standard’, for instance by Athey and Imbens (2017). In doing so, we’re not saying that RCTs are perfect, just that they’re generally better than most of the other options. There is plenty that is wrong with RCTs. Remember that our challenge is (Gertler et al. 2016, 51–52): …to identify a treatment group and a comparison group that are statistically identical, on average, in the absence of the program. If the two groups are identical, with the sole exception that one group participates in the program and the other does not, then we can be sure that any difference in outcomes must be due to the program. Finding such comparison groups is the crux of any impact evaluation, regardless of what type of program is being evaluated. Simply put, without a comparison group that yields an accurate estimate of the counterfactual, the true impact of a program cannot be established. We might be worried about underlying trends (the issues with before/after comparison), or selection bias (the issue with self-selection), either of which would result in biased estimators. Our solution is randomisation. To get started, let’s generate a simulated dataset and then sample from it. In general, this is a good way to approach problems: generate a simulated dataset; do your analysis on the simulated dataset; and take your analysis to the real dataset. The reason this is a good approach is that you know roughly what the outcomes should be in step 2, whereas if you go directly to the real dataset then you don’t know if unexpected outcomes are likely due to your own analysis errors, or actual results. The first time you generate a simulated dataset it will take a while, but after a bit of practice you’ll get good at it. There are also packages that can help, including DeclareDesign (Blair et al. 2019) and survey (Lumley 2020). Another good reason it’s useful to take this approach of simulation is that when you’re working in teams the analysis can get started before the data collection and cleaning is completed. That simulation will also help the collection and cleaning team think about tests they should run on their data. library(tidyverse) set.seed(853) # Construct a population so that 25 per cent of people like blue and 75 per cent # like white. population &lt;- tibble(person = c(1:10000), favourite_color = sample(x = c(&quot;Blue&quot;, &quot;White&quot;), size = 10000, replace = TRUE, prob = c(0.25, 0.75)), supports_the_leafs = sample(x = c(&quot;Yes&quot;, &quot;No&quot;), size = 10000, replace = TRUE, prob = c(0.80, 0.20)), ) %&gt;% mutate(in_frame = sample(x = c(0:1), size = 10000, replace = TRUE)) %&gt;% mutate(group = sample(x = c(1:10), size = 10000, replace = TRUE)) %&gt;% mutate(group = ifelse(in_frame == 1, group, NA)) We’ll get more into this terminology later, but the sampling frame is subset of the population that can actually be sampled, for instance they are listed somewhere. For instance, Lauren Kennedy likes to use the analogy of a city’s population, and the phonebook - almost everyone is in there (or at least they used to be), so the population and the sampling frame are almost the same, but they are not. Now look at the mean for two groups drawn out of the sampling frame. population %&gt;% filter(in_frame == 1) %&gt;% filter(group %in% c(1, 2)) %&gt;% group_by(group, favourite_color) %&gt;% count() ## # A tibble: 4 x 3 ## # Groups: group, favourite_color [4] ## group favourite_color n ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 Blue 114 ## 2 1 White 420 ## 3 2 Blue 105 ## 4 2 White 369 We are probably convinced by looking at it, but to formally test if there is a difference in the two samples, we can use a t-test. library(broom) population &lt;- population %&gt;% mutate(color_as_integer = case_when( favourite_color == &quot;White&quot; ~ 0, favourite_color == &quot;Blue&quot; ~ 1, TRUE ~ 999 )) group_1 &lt;- population %&gt;% filter(group == 1) %&gt;% select(color_as_integer) %&gt;% as.vector() %&gt;% unlist() group_2 &lt;- population %&gt;% filter(group == 2) %&gt;% select(color_as_integer) %&gt;% unlist() t.test(group_1, group_2) ## ## Welch Two Sample t-test ## ## data: group_1 and group_2 ## t = -0.30825, df = 988.57, p-value = 0.758 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.05919338 0.04312170 ## sample estimates: ## mean of x mean of y ## 0.2134831 0.2215190 # We could also use the tidy function in the broom package. tidy(t.test(group_1, group_2)) ## # A tibble: 1 x 10 ## estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.00804 0.213 0.222 -0.308 0.758 989. -0.0592 0.0431 ## # … with 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt; If properly done then not only will we get a ‘representative’ share of people with the favourite color blue, but we should also get a representative share of people who support the Maple Leafs. Why should that happen when we haven’t randomised on these variables? Let’s start by looking at our dataset. population %&gt;% filter(in_frame == 1) %&gt;% filter(group %in% c(1, 2)) %&gt;% group_by(group, supports_the_leafs) %&gt;% count() ## # A tibble: 4 x 3 ## # Groups: group, supports_the_leafs [4] ## group supports_the_leafs n ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 No 102 ## 2 1 Yes 432 ## 3 2 No 81 ## 4 2 Yes 393 This is very exciting. We have a representative share on ‘unobservables’ (in this case we do ‘observe’ them - to illustrate the point - but we didn’t select on them). We get this because they were correlated. But it will breakdown in a number of ways that we will discuss. It also assumes large enough groups - if we sampled in Toronto are we likely to get a ‘representative’ share of people who support the Canadiens? What about F.C. Hansa Rostock? If we want to check that the two groups are the same then what can we do? Exactly what we did above - just check if we can identify a difference between the two groups based on observables (we looked at the mean, but we could look at other aspects as well). 8.1.4 ANOVA Analysis of Variation (ANOVA) was introduced by Fisher while he was working on statistical problems in agriculture. To steal Darren L Dahly’s ‘favorite joke of all time’ (Dahly 2020): Q: “What’s the difference between agricultural and medical research?” A: “The former isn’t conducted by farmers.” We need to cover ANOVA because of its importance historically, but in general you probably shouldn’t actually use ANOVA day-to-day. There’s nothing wrong with it, in the right circumstances, it’s more just that it is a hundred years old and the number of modern use-case where it’s still your best-bet is pretty small. In any case, typically, the null is that all of the groups are from the same distribution. We can run ANOVA with the function built into R - aov(). just_two_groups &lt;- population %&gt;% filter(in_frame == 1) %&gt;% filter(group %in% c(1, 2)) aov(group ~ favourite_color, data = just_two_groups) %&gt;% tidy() ## # A tibble: 2 x 6 ## term df sumsq meansq statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 favourite_color 1 0.0238 0.0238 0.0952 0.758 ## 2 Residuals 1006 251. 0.250 NA NA In this case, we fail to reject the null that the samples are the same. This all said, it’s just linear regression. So I’m not sure why it got a fancy name. lm(group ~ favourite_color, data = just_two_groups) %&gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 1.48 0.0338 43.8 1.67e-235 ## 2 favourite_colorWhite -0.0118 0.0382 -0.308 7.58e- 1 My favourite discussion of ANOVA is Taback (2020 Chapter 8). 8.1.5 Treatment and control If the treated and control groups are the same in all ways and remain that way, then we have internal validity, which is to say that our control will work as a counterfactual and our results can speak to a difference between these groups in that study. In the words of Gertler et al. (2016, 71): Internal validity means that the estimated impact of the program is net of all other potential confounding factors—or, in other words, that the comparison group provides an accurate estimate of the counterfactual, so that we are estimating the true impact of the program. If the group to which we applied our randomisation were representative of the broader population, and the experimental set-up were fairly similar to outside conditions, then we further have external validity. That means that the difference that we find does not just apply in our own experiment, but also in the broader population. Again, in the words of Gertler et al. (2016, 73): External validity means that the evaluation sample accurately represents the population of eligible units. The results of the evaluation can then be generalized to the population of eligible units. We use random sampling to ensure that the evaluation sample accurately reflects the population of eligible units so that impacts identified in the evaluation sample can be extrapolated to the population. But this means we need randomisation twice. How does this trade-off happen and to what extent does it matter? As such, we are interested in the effect of being ‘treated’. This may be that we charge different prices (continuous treatment variable), or that we compare different colours on a website (discrete treatment variable, and a staple of A/B testing). If we consider just discrete treatments (so that we can use dummy variables) then need to make sure that all of the groups are otherwise the same. How can we do this? One way is to ignore the treatment variable and to examine all other variables - can you detect a difference between the groups based on any other variables? In the website example, are there a similar number of: PC/Mac users? Safari/Chrome/Firefox/other users? Mobile/desktop users? Users from certain locations? These are all threats to the validity of our claims. But if done properly, that is if the treatment is truly independent, then we can estimate an ‘average treatment effect’, which in a binary treatment variable setting is: \\[\\mbox{ATE} = \\mbox{E}[y|d=1] - \\mbox{E}[y|d=0].\\] That is, the difference between the treated group, \\(d = 1\\), and the control group, \\(d = 0\\), when measured by the expected value of some outcome variable, \\(y\\). So the mean causal effect is simply the difference between the two expectations! Let’s again get stuck into some code. First we need to generate some data. set.seed(853) example_data &lt;- tibble(person = c(1:1000), treatment = sample(x = 0:1, size = 1000, replace = TRUE) ) # We want to make the outcome slightly more likely if they were treated than if not. example_data &lt;- example_data %&gt;% rowwise() %&gt;% mutate(outcome = if_else(treatment == 0, rnorm(n = 1, mean = 5, sd = 1), rnorm(n = 1, mean = 6, sd = 1) ) ) example_data$treatment &lt;- as.factor(example_data$treatment) example_data %&gt;% ggplot(aes(x = outcome, fill = treatment)) + geom_histogram(position = &quot;dodge&quot;, binwidth = 0.2) + theme_minimal() + labs(x = &quot;Outcome&quot;, y = &quot;Number of people&quot;, fill = &quot;Person was treated&quot;) + scale_fill_brewer(palette = &quot;Set1&quot;) example_regression &lt;- lm(outcome ~ treatment, data = example_data) tidy(example_regression) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 5.00 0.0430 116. 0. ## 2 treatment1 1.01 0.0625 16.1 5.14e-52 But then reality happens. Your experiment cannot run for too long otherwise people may be treated many times, or become inured to the treatment, but it cannot be too short otherwise you can’t measure longer term outcomes. You cannot have a ‘representative’ sample on every cross-tab, but if not then the treatment and control will be different. Practical difficulties may make it difficult to follow up with certain groups. Questions to ask (if they haven’t been answered already) include: How are the participants being selected into the frame for consideration? How are they being selected for treatment? We would hope this is a lottery, but this term is applied to a variety of situations. Additionally, early ‘success’ can lead to pressure to treat everyone. How is treatment being assessed? To what extent is random allocation ethical and fair? Some argue that shortages mean it is reasonable to randomly allocate, but that may depend on how linear the benefits are. It may also be difficult to establish boundaries. If we only want to include people in Ontario then that may be clear, but what about ‘students’ in Ontario - who is a student, and who is making the decision? Bias and other issues are not the end of the world. But you need to think about it carefully. In the famous example, Abraham Wald was given data on the planes that came back to Britain after being shot at in WW2. The question is where to place the armour. One option is to place it over the bullet holes. Wald recognised that there is a selection effect here - these are the planes that made it back - they didn’t need the armour, but instead we should put the armour where there were no bullet holes. To consider an example that may be closer to home - how would the results of a survey differ if I only asked students who completed this course what was difficult about it and not those who dropped out? While, as Dan suggests, we should work to try to make the dataset as good as possible, it may be possible to use the model to control for some of the bias. If there is a variable that is correlated with say, attrition, then we can add it to the model. Either by itself, or as an interaction. What if there is a correlation between the individuals? For instance, what if there were some ‘hidden variable’ that we didn’t know about, such as province, and it turned out that people from the same province were similar? In that case we could use ‘wider’ standard errors. But a better way to deal with this may be to change the experiment. For instance, we discussed stratified sampling - perhaps we should stratify by province? How would we implement this? And of course, these days we’d not really use a 100-year-old method but would instead use Bayes-based approaches. 8.2 Case study - Fisher’s tea party Figure 8.1: Afternoon Tea Party (1890–1891), by Mary Cassatt (American, 1844-1926), as downloaded from https://artvee.com/dl/afternoon-tea-party. Fisher (see note above) introduced a, now, famous example of an experiment designed to see if a person can distinguish between a cup of tea when the milk was added first, or last.8 From Fisher (1935, 13): A lady declares that by tasting a cup of tea made with milk she can discriminate whether the milk or the tea infusion was first added to the cup. We will consider the problem of designing an experiment by means of which this assertion can be tested. Fisher continues: Our experiment consists in mixing eight cups of tea, four in one way and four in the other, and presenting them to the subject for judgment in a random order. The subject has been told in advance of what the test will consist, namely that she will be asked to taste eight cups, that these shall be four of each kind, and that they shall be presented to her in a random order, that is in an order not determined arbitrarily by human choice, but by the actual manipulation of the physical apparatus used in games of chance, cards, dice, roulettes, etc., or, more expeditiously, from a published collection of random sampling-numbers purporting to give the actual results of such manipulation. Her task is to divide the 8 cups into two sets of 4, agreeing, if possible, with the treatments received. To summarize, the set-up is: Eight randomly ordered cups of tea. Four had tea put in first. Four had milk put in first. The person has to choose the four that are the same. The person knows it’s an experiment. We’ll now try this experiment. So brew some tea, grab eight cups, and pour eight cups of tea for a friend that you’re isolating with9 - four where you put the milk in first and four where you put the milk in last. Make sure you use the same amount of tea and milk in each! Don’t forget to randomise the order, possibly even using the following code: sample(c(1:8), size = 8, replace = FALSE) ## [1] 3 7 6 4 1 8 2 5 Then have your friend guess which four you put milk in first and which four you put milk in last! To decide if the person’s choices were likely to have occurred at random or not, we need to think about the probability of this happening by chance. First count the number of successes out of the four that were chosen. Fisher (1935, 14) claims there are: \\({8 \\choose 4} = \\frac{8!}{4!(8-4)!}=70\\) possible outcomes. By chance, there are two ways for the person to be perfectly correct (because we are only asking them to be grouped): correctly identify all the ones that were milk-first (one outcome out of 70) or correctly identify all the ones that were tea-first (one outcome out of 70), so the chance of that is \\(2/70 \\approx 0.028\\). Now, as Fisher (1935, 15) says, ‘[i]t is open to the experimenter to be more or less exacting in respect of the smallness of the probability he would require before he would be willing to admit that his observations have demonstrated a positive result’. You need to decide what evidence it takes for you to be convinced. If there’s no possible evidence that will dissuade you from your view (that there is no difference between milk-first and tea-first) then what is the point of doing an experiment? In any case, if the null is that they can’t distinguish, but they correctly separate them all, then at the five-per-cent level, we reject the null. What if they miss one? Similarly, by chance there are 16 ways for a person to be ‘off-by-one’. Either they think there was one that was milk-first when it was tea-first - there are, \\({4 \\choose 1}\\), four ways this could happen - or they think there was one that was tea-first when it was milk-first - again, there are, \\({4 \\choose 1}\\), four ways this could happen. But these outcomes are independent, so the probability is \\(\\frac{4\\times 4}{70} \\approx 0.228\\). And so on. So, we fail to reject the null. Finally, an aside on this magical ‘5 per cent’. Fisher himself describes this as merely ‘usual and convenient’ (Fisher 1935, 15). Fisher (1935, 16) continues: In order to assert that a natural phenomenon is experimentally demonstrable we need, not an isolated record, but a reliable method of procedure. In relation to the test of significance, we may say that a phenomenon is experimentally demonstrable when we know how to conduct an experiment which will rarely fail to give us a statistically significant result. At the start of these notes, I said that Fisher held views that we would consider reprehensible today. My guess is, were he around today, he would think our use of p-values as discrediting. Do not just go searching for meaning in constellations of stars. Thoroughly interrogate your data and think precisely about the statistical methods you are applying. For conclusions that you want to hold up in the long-run, aim to use as simple, and as understandable, statistical methods as you can. Ensure that you can explain and justify your statistical decisions without recourse to astrology. Source: https://xkcd.com/882/ Figure 8.2: ‘The triumph of wisdom over fortune’ by Otto van Veen (Flemish, 1556 - 1629), as downloaded from https://artvee.com/dl/the-triumph-of-wisdom-over-fortune. 8.3 Case study - Tuskegee Syphilis Study The Tuskegee Syphilis Study is an infamous medical trial in which Black Americans with syphilis (and a ‘control group’ without) were not given appropriate treatment, nor even told they had syphilis, well after standard syphilis treatments were established in the mid-1940s (Alsan and Wanamaker 2018). The study began in 1932 when poor Black Americans in the South were identified and offered compensation including ‘hot meals, the guise of treatment, and burial payments’ (Alsan and Wanamaker 2018). The men were not treated for syphilis. Further, and this is almost unbelievable, some of the men were drafted, told they had syphilis, and ordered to get treatment. This treatment was blocked. By the time the study was stopped, ‘the majority of the study’s victims were deceased, many from syphilis-related causes.’ (Alsan and Wanamaker 2018). The study continued through to 1972, only stopping when it was leaked and published in newspapers. In response the US established requirements for Institutional Review Boards and President Clinton made a formal apology in 1997. Brandt (1978) as quoted by Alsan and Wanamaker (2018) says ‘“In retrospect the Tuskegee Study revealed more about the pathology of racism than the pathology of syphilis; more about the nature of scientific inquiry than the nature of the disease process…. The degree of deception and the damages have been severely underestimated.”’ On the Tuskegee Syphilis Study Professor Monica Alexander says: While it may be illegal to do this exact research these days, it doesn’t mean that unethical research doesn’t still happen, and we see it all the time in ML and health. Just because you can’t explicitly discriminate when you design experiments, doesn’t mean you can’t implicitly discriminate. For an example of this, start with Obermeyer et al. (2019): Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts. 8.4 Case study - The Oregon Health Insurance Experiment The Oregon Health Insurance Experiment involved 74,922 adults in Oregon from 2008 to 2010. The opportunity to apply for health insurance was randomly allocated and then health and earnings evaluated. It was found that (Finkelstein et al. 2012): In the year after random assignment, the treatment group selected by the lottery was about 25 percentage points more likely to have insurance than the control group that was not selected. We find that in this first year, the treatment group had substantively and statistically significantly higher health care utilization (including primary and preventive care as well as hospitalizations), lower out-of-pocket medical expenditures and medical debt (including fewer bills sent to collection), and better self-reported physical and mental health than the control group. A lottery was used to determine which of the 89,824 individuals who signed up would be allowed to apply for Medicaid. This random allocation of insurance allowed the researchers to understand the effect of health insurance. It’s not usually possible to compare those with and without insurance because the type of people that sign up to get health insurance differ to those who don’t - that decision is ‘confounded’ with other variables. They use administrative data, such as hospital discharge data, credit reports that were matched to 68.5 per cent of lottery participants, and mortality records, which will be uncommon. Interestingly this collection of data is actually fairly restrained and so they included a survey conducted via mail. Turning to external validity, the authors restrain themselves and say (Finkelstein et al. 2012): Our estimates of the impact of public health insurance apply to able-bodied uninsured adults below 100 percent of poverty who express interest in insurance coverage. This is a population of considerable policy interest. A lottery was used to allocate 10,000 places in the state-run Medicaid. A lottery was judged fair because ‘the state (correctly) anticipated that the demand for the program among eligible individuals would far exceed the 10,000 available new enrollment slots’ (Finkelstein et al. 2012). People had a month to sign up to enter the draw. The draws were conducted over a six-month period and those who were selected had the opportunity to sign up. 35,169 individuals were selected (the household of those who actually won the draw was given the opportunity) but only 30 per cent of them completed the paperwork and were eligible (typically they earned too much). The insurance lasted indefinitely. The model they consider is (Finkelstein et al. 2012): \\[\\begin{equation} y_{ihj} = \\beta_0 + \\beta_1\\mbox{Lottery} + X_{ih}\\beta+2 + V_{ih}\\beta_3 + \\epsilon_{ihj} \\tag{8.1} \\end{equation}\\] Equation (8.1) explains various \\(j\\) outcomes (such as health) for an individual \\(i\\) in household \\(h\\) as a function of an indicator variable as to whether household \\(h\\) was selected by the lottery. Hence, ‘(t)he coefficient on Lottery, \\(\\beta_1\\), is the main coefficient of interest, and gives the average difference in (adjusted) means between the treatment group (the lottery winners) and the control group (those not selected by the lottery).’ To complete the specification of Equation (8.1), \\(X_{ih}\\) is a set of variables that are correlated with the probability of being treated. These adjust for that impact to a certain extent. An example of that is the number of individuals in a household. And finally, \\(V_{ih}\\) is a set of variables that are not correlated with the lottery. These variables include demographics, hospital discharge and lottery draw. There is a wide range of literature related to this intervention. More papers are available here. 8.5 Case study - Student Coaching: How Far Can Technology Go? There is a general concern about students dropping out of university before they finish their degree. If you work one-on-one with a student then this addresses the issue. But that doesn’t scale. The point of this experiment was to see if technology-based options could be more efficient. The focus was the University of Toronto, and in particular first-year economics courses in Fall 2015. The intervention was administered to students as part of an economics class. Students received 2 per cent of their grade for completing the exercise. The specific exercise depended on the group of the student. The intervention involved three treatments as well as a control group that was just given a Big Five personality traits test. Additional information that was obtained included ‘the highest level of education obtained by students’ parents, the amount of education they expect to obtain, whether they are first-year or international students, and their work and study time plans for the upcoming year.’ (Oreopoulos and Petronijevic 2018, 6). The treatments were (Oreopoulos and Petronijevic 2018, 4): ‘[A] one-time, online exercise completed during the first two weeks of class in the fall’. This exercise was ‘designed to get them thinking about the future they envision and the steps they could take in the upcoming year at U of T to help make that future a reality. They were told that the exercise was designed for their benefit and to take their time while completing it. The online module lasted approximately 60 to 90 minutes and led students through a series of writing exercises in which they wrote about their ideal futures, both at work and at home, what they would like to accomplish in the current year at U of T, how they intend on following certain study strategies to meet their goals, and whether they want to get involved with extracurricular activities at the university’ (Oreopoulos and Petronijevic 2018, 6). ‘[T]he online intervention plus text and email messaging throughout the full academic year’. This involved the students being given ‘the opportunity to provide their phone numbers and participate in a text and email messaging campaign lasting throughout both the fall semester in 2015 and the winter semester in 2016’ (Oreopoulos and Petronijevic 2018, 8). All students in this group got the emails, but only those that provided phone numbers got the messages. They were able to opt out, but ‘few chose to do so’ (Oreopoulos and Petronijevic 2018, 8). This was a two-way interaction in which students could ask questions. Some asked for the ‘locations of certain facilities on campus or how to stay on residence during the holiday break, while others said they need help with English skills or specific courses. Some students expressed relatively deep emotions, such as feeling anxious about family pressure to succeed in school or from doing poorly on an evaluation’ (Oreopoulos and Petronijevic 2018, 9). A response was usually given within an hour. ‘[T]he online intervention plus one-on-one coaching in which students are assigned to upper-year undergraduate coaches’. ‘Coaches were available to meet with students to answer any questions via Skype, phone, or in person, and would send their students regular text and email messages of advice, encouragement, and motivation, much like the You@UofT program described above. In contrast to the messaging program, however, coaches were instructed to be proactive and regularly monitor their students’ progress. Whereas the You@UofT program attempts to “nudge” students in the right direction with academic advice, coaches play a greater “support” role, sensitively guiding students through problems.’ (Oreopoulos and Petronijevic 2018, 11). This coaching program was only available at UTM. ‘Our coaching treatment group was established by randomly drawing twenty-four students from the group of students that were randomly assigned into the text message campaign treatment. At the conclusion of the online exercise, instead of being invited to provide a phone number for the purpose of receiving text messages, these twenty-four students were given the opportunity to participate in a pilot coaching program. A total of seventeen students agreed to participate in the coaching program, while seven students declined.’ Our coaching treatment group was established by randomly drawing twenty-four students from the group of students that were randomly assigned into the text message campaign treatment. At the conclusion of the online exercise, instead of being invited to provide a phone number for the purpose of receiving text messages, these twenty-four students were given the opportunity to participate in a pilot coaching program. A total of seventeen students agreed to participate in the coaching program, while seven students declined. (Oreopoulos and Petronijevic 2018, 14) The model they consider is (Oreopoulos and Petronijevic 2018, 15): \\[\\begin{equation} y_{ij} = \\alpha + \\beta_1\\mbox{Online}_i + \\beta_2\\mbox{Text}_i + + \\beta_3\\mbox{Coach}_i + \\delta_j + \\mu \\mbox{First year}_i + \\epsilon_{ij} \\tag{8.2} \\end{equation}\\] Equation (8.2) explains the outcome of student \\(i\\) at campus \\(j\\) based on ‘indicators for each of the three treatment exercises students were given, campus fixed effects, and a first-year student indicator.’ The main parameters of interest are \\(\\beta_1\\), \\(\\beta_2\\) and \\(\\beta_3\\). The main outcomes were course grades, GPA, credits earned and failed. It was found, that the one-on-one coaching ‘increased grades by approximately 5 percentage points’, while the other treatments had ‘had no detectable impact’. One set of results are summarised in Figure @ref(fig:toronto_intervention). (#fig:toronto_intervention)Example of the results of the intervention. The results are important not only in a teaching context, but also for businesses hoping to retain customers. More papers are available here. 8.6 A/B testing 8.6.1 Introduction The past decade has probably seen the most experiments ever run by several orders of magnitude with the extensive use of A/B testing on websites. Every time you are online you are probably subject to tens, hundreds, or potentially thousands, of different A/B tests. If you use apps like TikTok then this could run to the tens of thousands. While they have several interesting features, which we will discuss, at their heart they are still just surveys that result in data that need to be analysed. In this section of the notes we augment our main textbooks with a less formal book focused on A/B testing that is very popular in industry at the moment. I can’t do much better than to quote their opening example Kohavi, Tang, and Xu (2020, 3). In 2012, an employee working on Bing, Microsoft’s search engine, suggested changing how ad headlines display (Kohavi and Thomke 2017). The idea was to lengthen the title line of ads by combining it with the text from the first line below the title, as shown in Figure 1.1. Nobody thought this simple change, among the hundreds suggested, would be the best revenue-generating idea in Bing’s history! The feature was prioritized low and languished in the backlog for more than six months until a software developer decided to try the change, given how easy it was to code. He implemented the idea and began evaluating the idea on real users, randomly showing some of them the new title layout and others the old one. User interactions with the website were recorded, including ad clicks and the revenue generated from them. This is an example of an A/B test, the simplest type of controlled experiment that compares two variants: A and B, or a Control and a Treatment. A few hours after starting the test, a revenue-too-high alert triggered, indicating that something was wrong with the experiment. The Treatment, that is, the new title layout, was generating too much money from ads. Such “too good to be true” alerts are very useful, as they usually indicate a serious bug, such as cases where revenue was logged twice (double billing) or where only ads displayed, and the rest of the web page was broken. For this experiment, however, the revenue increase was valid. Bing’s revenue increased by a whopping 12%, which at the time translated to over $100M annually in the US alone, without significantly hurting key user-experience metrics. The experiment was replicated multiple times over a long period. The example typifies several key themes in online controlled experiments: It is hard to assess the value of an idea. In this case, a simple change worth over $100M/year was delayed for months. Small changes can have a big impact. A $100M/year return-on-investment (ROI) on a few days’ work for one engineer is about as extreme as it gets. Experiments with big impact are rare. Bing runs over 10,000 experiments a year, but simple features resulting in such a big improvement happen only once every few years. The overhead of running an experiment must be small. Bing’s engineers had access to ExP, Microsoft’s experimentation system, which made it easy to scientifically evaluate the idea. The overall evaluation criterion (OEC, described more later in this chapter) must be clear. In this case, revenue was a key component of the OEC, but revenue alone is insufficient as an OEC. It could lead to plastering the web site with ads, which is known to hurt the user experience. Bing uses an OEC that weighs revenue against user-experience metrics, including Sessions per user (are users abandoning or increasing engagement) and several other components. The key point is that user-experience metrics did not significantly degrade even though revenue increased dramatically. In these notes, I’m going to use A/B testing to strictly refer to the situation in which we’re dealing with a tech firm, and some type of change in code. If we are dealing with the physical world then we’ll stick with RCTs. You may think that it’s easy to go to a workplace and say ‘hey, let’s test stuff before we spend thousands/millions of dollars’. You’d be wrong. The hardest part of A/B testing isn’t the science, it’s the politics. 8.6.2 Unique complications of A/B testing 8.6.2.1 Delivery This is from Chapter 12 of Kohavi, Tang, and Xu (2020). In the case of a RCT it’s fairly obvious how we deliver the treatment - for instance, make them come to a doctor’s clinic and inject them with the drug or a placebo. In the case of A/B testing, it’s less obvious - do you run it ‘server-side’ or ‘client-side’? E.g. do you just change the website - ‘server side’, or do you change an app - ‘client side’. This may seem like a silly issue, but it affects two aspects: Release. Data transmission. In the case of the effect on release, it’s easy and normal to update a website all the time, so small changes can be easily implemented in the case of server-side. However, in the case of client-side, let’s say an app, it’s likely a much bigger deal. It needs to get through an app store (a bigger or lesser deal depending on which one). It need to go through a release cycle (a bigger or lesser deal depending on the specifics of the company and how it ships). Users have the opportunity to not upgrade. Are they likely different to those that do upgrade? (Yes.) Now, in the case of the effect on data transmission, again server-side is less of a big deal - you kind of get the data as part of the user interacting. But in the case of client-side - it’s not necessarily the case that the user will have the internet at the time they’re using your application, and if they do they may have limitations on the data uploads. The phone may limit data transmission depending on its effect on battery, CPU, general performance, etc. So then you decide to cache, but then the user may find it weird that some minor app takes up as much size as their photos. The effect of all this is that you need to plan, and build this into your expectations - don’t promise results the day after a release if you’re evaluating a client-side change. Adjust for the fact that your results are conditional and gather data on those conditions e.g. battery level or whatever. Adjust in your analysis for different devices and platforms, etc. This is a lovely opportunity for multilevel regression. 8.6.2.2 Instrumentation This is from Chapter 13 of Kohavi, Tang, and Xu (2020). I would change the name of this from instrumentation, but I don’t have a good replacement. The point of this is that you need to consider how you are getting your data in the first place. For instance, if we put a cookie on your device then different types of users will remove that at different rates. Using things like beacons can be great (this is when you force the user to ‘download’ some tiny thing they don’t notice so that you know they’ve gone somewhere - see ‘email’ etc). But again, there are practical issues - do we force the beacon before the main content loads - which makes for a worse customer experience; or do we allow the beacon to load after the main content, in which case we may get a biased sample? There are likely different servers and databases for different faces of the product. For instance, Twitter in Australia, compared with Twitter in Canada, compared with Twitter on my phone’s app, compared with Twitter accessed via the browser. Joining these different datasets can be difficult and requires either a unique id or some probabilistic approach. Kohavi, Tang, and Xu (2020, 165) recommend changing the culture of your workplace to ensure instrumentation is normalised, which I mean, yeah. 8.6.2.3 Randomisation unit This is from Chapter 13 of Kohavi, Tang, and Xu (2020). What are we actually randomising over? Okay, again, this is something that’s kind of obvious in normal RCTs, but gets like really interesting in the case of A/B testing. Let’s consider the malaria netting experiments - either a person/village/state gets a net or it doesn’t. Easy (relatively). But in the case of server-side A/B testing - are we randomising the page, the session, or the user? To think about this, let’s think about colour. Let’s say that we change our logo from red to blue on the ‘home’ page. If we’re randomising at the page level, then when the user goes to the ‘about’ page the logo could be back to red. If we’re randomising at the session level, then it’ll be blue while they’re using the website that time, but if they close it and come back then it’ll be red. Finally, if we’re randomising at a user level then it’ll always be red for me, but always blue for my friend. That last bit assumes perfect identity tracking, which might be generally okay if you’re Google or Facebook, but for anyone else is going to be a challenge - what if you visit cbc.ca on your phone and then on your laptop? You’re likely considered a different ‘user’. Does this matter? It’s a trade-off between consistency and importance. 8.7 Case study - Upworthy To see this in action let’s look at the Upworthy dataset Matias et al. (2019). 8.8 Sampling and survey essentials 8.8.1 Introduction Let’s say that we have some data. For instance, a particular toddler goes to sleep at 6:00pm every night. We might be interested to know whether that bed-time is common more generally among all toddlers, or if we have an unusual toddler. We only have one toddler so our ability to use his bed time to speak about all toddlers is limited. But what about if we talk to our friends who also have toddlers? How many friends, and friends of friends, do we have to ask because we can begin to feel comfortable speaking about some underlying truth of toddler bedtime? In the wonderful phrase of Wu and Thompson (2020, 3) ‘[s]tatistics is the science of how to collect and analyze data, and draw statements and conclusions about unknown populations. The term population usually refers to a real or hypothetical set of units with characteristics and attributes which can be modelled by random variables and their respective probability distributions.’. In my own much less wonderful phrasing, ‘statistics involves having some data and trying to say something sensible about it’. I mean, it’s really up to you which one you want to go with. In the case of surveys, our population is a finite set of \\(N\\) labels: ‘person 1’, ‘person 2’, ‘person 3’, …, ‘person \\(N\\)’. It is important here to recognise that there is a difference between the population of interest to a survey and a population in the sense that it is used when we talk of limits and similar infinity concepts in statistics. For instance, from time to time, you hear people who work with census data say that they don’t need to worry about confidence intervals because they have the whole population of the country. Nothing could be further from the truth. Wu and Thompson (2020, 4) have a lovely example of the ambiguity that surrounds the definition of a population. Let’s consider the population of voters. In Canada that means anyone who is 18 or older. Fine. But what if we are interested in consumers - what is the definition of hipsters? I regularly eat avocado toast, (+1), but I’ve never had bullet coffee (-1). Am I in the population or not? More things are formally defined than you may realise. For instance, the idea of a rural area is precisely defined. A property is either in a rural area or not. But then we come to the lovely example of Wu and Thompson (2020, 4) when it comes to whether someone is a smoker. If a 15 year old has had 100 cigarettes then it’s pretty clear that we need to treat them differently than if they have had none. But if a 100 year old has had 100 cigarettes then we consider them to have none. That’s fine, but what is the age at which this changes? Further, think about how this changes over time. At one point, parents used to be worried if children had more than two hours of screen time, now those same children (and possibly even the parents) regularly likely spend more than eight hours in front of a screen if they work in an office job. So we come to some critical terminology: Population: ‘The set of all units covered by the main objective of the study.’ Wu and Thompson (2020, 5). Frame: ‘Lists of sampling units’ Wu and Thompson (2020, 9) where sampling units are either the observational units themselves or the clusters. Sample: Those who complete and return the survey. To be a little more concrete about this, consider that we are trying to conduct a survey about the attitudes Australians who live in Toronto. So the target population is all Australians who live in Toronto, the frame might be all those Australians who live in Toronto who use Facebook, because we are going to use Facebook to choose who to sample. And then finally, if we take that Facebook list of all Australians living in Canada and we gave each one a chance at being surveyed then that would be our sampled population, but if we just picked the ones that I know then it would just be Dan, Monica, and Liza (from New Zealand but we’ll claim her because that’s a thing that Australians do). In that example the target population and the frame will be different because not all Australians who live in Toronto are on Facebook. Similarly, if not everyone that we gave the survey to actually completed the survey then the sample and the frame would be different. Having identified a population of interest and a frame (i.e. a list that gets the closest to that population) At this point we distinguish between probability and non-probability sampling. With probability sampling, every member of the frame has some chance of being sampled. Consider the example of the Australian Election Study - they get a list of all the addresses in Australia, and then randomly choose some to send letters to. The ‘randomista’ and RCT revolution that we discuss later, is needed because of a lack of probability sampling, but when it exists it plays a role here. Importantly it ensure that we are clear about the role of uncertainty (Wu and Thompson 2020, 11). The trade-off is that it is expensive and difficult. Note that each unit in the frame doesn’t have to have the same probability necessarily, it just needs to be determined by a probability measure. In contrast, with non-probability sampling we focus on populations that are ‘readily available’ or convenient, satisfy certain quotas, based on judgement, or those that volunteer. The difference between probability and non-probability sampling is that of degree - we typically cannot force someone to take our survey, and hence, there is almost also as aspect of volunteering. While acknowledging that it is a spectrum, most of statistics was developed based on probability sampling. But much of modern sampling is done using non-probability sampling. In particular, a common approach is to have a bunch of Facebook ads trying to recruit a panel of people in exchange for compensation. This panel is then the group that is sent various surveys as necessary. But think for a moment about the implications of this - what type of people are likely to respond to such an ad? I don’t know who Canada’s richest person is, but are they likely to be in this panel? Is your grandmother likely to respond to that ad? What about you - do you even use Facebook? In some cases it is possible to do a census. Nation-states typically do one every five to ten years. But there is a reason that it is only nation states that do them - they are expensive, time-consuming, and surprisingly, they are sometimes not as accurate as we may hope because of how general they need to be. Hence, the role of surveys. Note, however that censuses will typically have many of the same concerns. When we consider our population, it will typically have some ordering. This may be as simple as a country having states/provinces. We consider a stratified structure to be one in which we can divide the population into mutually exclusive and collectively exhaustive sub-populations, or strata. Examples of strata in Wu and Thompson (2020, 8) include provinces, federal electoral districts, or health regions. But strata need not be geographic, and it may be possible to use different majors. We use stratification to help with the efficiency of sampling or with the balance of the survey. For instance, if we surveyed provinces in proportion to their population, then even a survey of 10,000 responses would only expect to have 10 responses from the Yukon. The other word that is used that takes advantage of the ordering of some population is clusters. Again, these are collectively exhaustive and mutually exclusive. Again, they may be geographically based, but need not be. The difference between stratified sampling and cluster sampling, is that ‘under stratified sampling, sample data are collected from every stratum, (whereas) under cluster sampling, only a portion of the clusters has members in the final sample’ Wu and Thompson (2020, 8). That all said, this difference can become less clear in practice, especially ex post - what if you stratify then randomly sample within that strata, but no one is selected - but in terms of intention the difference is clear. We now turn to the first of our claims, which is that if we have a perfect frame and no non-response, then our sample results will match that of the population. We’d of course be very worried if that weren’t the case, but it’s nice to have it stated. We establish some type of population mean for the study variable, \\(\\mu_y\\), and population means for the auxiliary variables \\(\\mu_x\\), which could be things like age, gender, etc. Remembering that when we do this in the real world, we may have many study variables, and indeed, some overlap. If a variable is an indicator then in this set-up all we have to do is to work out the proportion in order to estimate it, which is \\(P\\). And finally, we get a rule of thumb for large samples whereby the variance in this binary and perfect setting becomes \\(\\sigma_y^2 = P/(1-P)\\) (Wu and Thompson 2020, 11). Finally, we conclude with the steps that you should consider. These are all critical. Strong reports would grapple with all of these. 8.8.2 Simple random sampling TBD 8.8.3 Stratified and cluster sampling TBD 8.9 Implementing surveys 8.9.1 Google 8.9.2 Facebook 8.9.3 Survey Monkey 8.9.4 Mechanical Turk 8.9.5 Prolific 8.9.6 Qualtrics 8.9.7 Other 8.10 Next steps Large scale experiments are happening all around us. These days I feel we all know a lot more about healthcare experiments than perhaps we’d like to know and the AstraZeneca/Oxford situation is especially interesting, for instance, Oxford-AstraZeneca (2020), but see Bastian (2020) for how this is actually possibly more complicated. There are also well-known experiments that tried to see if big government programs are effective, such as: The RAND Health Insurance Experiment randomly gave health insurance to people in the US between 1974 and 1982 (Brook et al. 1984). The Oregon Health Study randomly gave health insurance in Oregon in 2008 (Finkelstein et al. 2012). References "],["farm-data.html", "Chapter 9 Farm data 9.1 Open Government Data 9.2 Electoral Studies", " Chapter 9 Farm data Required reading Recommended reading Key concepts/skills/etc Key libraries Key functions/etc Pre-quiz Key sources of data University of Toronto Dataverse: https://dataverse.scholarsportal.info/dataverse/toronto. Data is Plural structured archive: https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/edit#gid=0. Kaggle Datasets: https://www.kaggle.com/datasets. Figure Eight: https://www.figure-eight.com/data-for-everyone/. Google dataset search: https://datasetsearch.research.google.com/. Awesome Data: https://github.com/awesomedata/awesome-public-datasets. Quiz Please identify three other sources of data that you are interested in and describe where are they available (please include a link if possible)? Please focus on one of those sources. What steps do you have to go through in order to get a dataset that can be analysed in R? Let’s say you take a job at RBC (a Canadian bank) and they already have some quantitative data for you to use. What are some questions that you should explore when deciding whether that data will be useful to you? Please identify three sources of government data in your country and describe where are they available (please include a link if possible)? Please focus on one of those sources. What steps do you have to go through in order to get a dataset such that it can be analysed in R? Please identify three other sources of data that you are interested in and describe where are they available (please include a link if possible)? Please focus on one of those sources. What steps do you have to go through in order to get a dataset that can be analysed in R? Let’s say you take a job at RBC (a Canadian bank) and they already have some quantitative data for you to use. What are some questions that you should explore when deciding whether that data will be useful to you? 9.1 Open Government Data Canadian Government Open Data: https://open.canada.ca/en/open-data. 9.1.1 Canadian Census 9.1.2 City of Toronto Open Data Portal 9.2 Electoral Studies 9.2.1 Canadian Electoral Study 9.2.2 Australian Electoral Study "],["cleaning-and-preparing-data.html", "Chapter 10 Cleaning and preparing data", " Chapter 10 Cleaning and preparing data Required reading Recommended reading Key concepts/skills/etc Key libraries Key functions/etc Quiz "],["storing-and-retrieving-data.html", "Chapter 11 Storing and retrieving data", " Chapter 11 Storing and retrieving data Required reading Recommended reading Key concepts/skills/etc Key libraries Key functions/etc Quiz "],["disseminating-and-protecting-data.html", "Chapter 12 Disseminating and protecting data", " Chapter 12 Disseminating and protecting data Hawes, M. B. (2020). Implementing Differential Privacy: Seven Lessons From the 2020 United States Census. Harvard Data Science Review. https://doi.org/10.1162/99608f92.353c6f99 https://hdsr.mitpress.mit.edu/pub/g9o4z8au/release/2 https://www.census.gov/newsroom/blogs/research-matters/2020/02/census_bureau_works.html Required reading Recommended reading Key concepts/skills/etc Key libraries Key functions/etc Quiz "],["exploratory-data-analysis.html", "Chapter 13 Exploratory data analysis 13.1 Introduction 13.2 A note on packages 13.3 TTC subway delays 13.4 EDA and data viz 13.5 Data checks 13.6 Exercises 13.7 Case study - Opinions about a casino in Toronto 13.8 Case study - Historical Canadian elections 13.9 Case study - Airbnb listing in Toronto", " Chapter 13 Exploratory data analysis The skim function from the skimr package is just a quick way of looking at the data. It can be handy when you get a new dataset to quickly come to terms with it. The p0, p25, p50, etc, values that it provides are percentiles. So for instance, P50 is the 50th percentile which means that 50 per cent of the data are below this, and 50 per cent are above i.e. the median. P0 should be the minimum, and p100 should be the maximum. The middle ones are similarly defined, so p25, means that 25 per cent of the data are below this point and 75 per cent of it is above this point. The general idea of both the percentiles and the histogram is to give you a quick idea of how skewed your data is. This chapter was written with Monica Alexander. Required reading Wickham, Hadley, and Garrett Grolemund, 2017, ‘R for Data Science’, Chapters 3 and 7, freely available here: https://r4ds.had.co.nz/. Recommended reading Hall, Megan, 2019, ‘Exploratory Data Analysis Using Tidyverse’, freely available at: https://hockey-graphs.com/2019/10/08/exploratory-data-analysis-using-tidyverse/. Jordan, Michael I, 2019, ‘AI - The revolution hasn’t started yet’, freely available at: https://hdsr.mitpress.mit.edu/pub/wot7mkc1. Silge, Julia, 2018, ‘Understanding PCA using Stack Overflow data’, freely available at: https://juliasilge.com/blog/stack-overflow-pca/. Soetewey, Antoine, 2020, ‘Descriptive statistics in R’, freely available at: https://www.statsandr.com/blog/descriptive-statistics-in-r/. Stodulka, Jiri, 2019, ‘Toronto Crime and Folium’, freely available at: https://www.jiristodulka.com/post/toronto-crime/. Wong, Julia Carrie, 2020, ‘One year inside Trump’s monumental Facebook campaign’, The Guardian, 29 January, freely available at: https://www.theguardian.com/us-news/2020/jan/28/donald-trump-facebook-ad-campaign-2020-election. Key concepts/skills/etc Quickly coming to terms with a new dataset Key libraries/functions/etc tidyverse ggplot2 Pre-quiz In your own words what is exploratory data analysis? If you have a dataset called ‘my_data’, which has two columns: ‘first_col’ and ‘second_col’, then could you please write some rough R code that would generate a graph (the type of graph doesn’t matter). Consider a dataset that has 500 rows and 3 columns, so there are 1,500 cells. If 100 of the cells are missing data for at least one of the columns, then would you remove the whole row your dataset or try to run your analysis on the data as is, or some other procedure? What if your dataset had 10,000 rows instead, but the same number of missing cells? Please note three ways of identifying unusual values. What is the difference between a categorical and continuous variable? 13.1 Introduction Exploratory data analysis is never finished, you just die. This chapter is about exploratory data analysis (EDA) and data visualization steps in R. The aim is to get you used to working with real data (that has issues) to understand the main characteristics and potential issues. We will be using the opendatatoronto R package, which interfaces with the City of Toronto Open Data Portal. 13.2 A note on packages If you are running this Rmd on your local machine, you may need to install various packages used (using the install.packages function). Load in all the packages we need: library(opendatatoronto) library(tidyverse) library(stringr) library(skimr) library(visdat) library(janitor) library(lubridate) library(ggrepel) 13.3 TTC subway delays This package provides an interface to all data available on the Open Data Portal provided by the City of Toronto. Use the list_packages function to look at what’s available all_data &lt;- list_packages(limit = 500) all_data ## # A tibble: 405 x 11 ## title id topics civic_issues publisher excerpt dataset_category ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Toro… 1d07… City … Mobility Informat… &quot;Linea… Map ## 2 Earl… 2619… Commu… Poverty red… Children… &quot;Early… Map ## 3 Shor… fc41… Permi… Affordable … Municipa… &quot;This … Table ## 4 Poll… 7bce… City … &lt;NA&gt; City Cle… &quot;Polls… Table ## 5 Dail… 8a6e… City … Affordable … Shelter,… &quot;Daily… Table ## 6 Rain… f293… Locat… Climate cha… Toronto … &quot;This … Document ## 7 Stre… 99b1… City … &lt;NA&gt; Transpor… &quot;Infor… Map ## 8 Auto… a154… Trans… Mobility Transpor… &quot;This … Map ## 9 Body… c405… City … &lt;NA&gt; Toronto … &quot;This … Table ## 10 Stre… 1db3… City … Mobility Transpor… &quot;Trans… Map ## # … with 395 more rows, and 4 more variables: num_resources &lt;int&gt;, ## # formats &lt;chr&gt;, refresh_rate &lt;chr&gt;, last_refreshed &lt;date&gt; Let’s download the data on TTC subway delays in 2019. There are multiple files for 2019 so we need to get them all and make them into one big dataframe. res &lt;- list_package_resources(&quot;996cfe8d-fb35-40ce-b569-698d51fc683b&quot;) res &lt;- res %&gt;% mutate(year = str_extract(name, &quot;201.?&quot;)) delay_2019_ids &lt;- res %&gt;% filter(year==2019) %&gt;% select(id) %&gt;% pull() delay_2019 &lt;- c() for(i in 1:length(delay_2019_ids)) { delay_2019 &lt;- bind_rows(delay_2019, get_resource(delay_2019_ids[i])) } # make the column names nicer to work with delay_2019 &lt;- clean_names(delay_2019) Let’s also download the delay code and readme, as reference. delay_codes &lt;- get_resource(&quot;fece136b-224a-412a-b191-8d31eb00491e&quot;) delay_data_codebook &lt;- get_resource(&quot;54247e39-5a7d-40db-a137-82b2a9ab0708&quot;) This dataset has a bunch of interesting variables. You can refer to the readme for descriptions. Our outcome of interest is min_delay, which give the delay in mins. head(delay_2019) ## # A tibble: 6 x 10 ## date time day station code min_delay min_gap bound line ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2019-01-01 00:00:00 01:08 Tues… YORK M… PUSI 0 0 S YU ## 2 2019-01-01 00:00:00 02:14 Tues… ST AND… PUMST 0 0 &lt;NA&gt; YU ## 3 2019-01-01 00:00:00 02:16 Tues… JANE S… TUSC 0 0 W BD ## 4 2019-01-01 00:00:00 02:27 Tues… BLOOR … SUO 0 0 N YU ## 5 2019-01-01 00:00:00 03:03 Tues… DUPONT… MUATC 11 16 N YU ## 6 2019-01-01 00:00:00 03:08 Tues… EGLINT… EUATC 11 16 S YU ## # … with 1 more variable: vehicle &lt;dbl&gt; 13.4 EDA and data viz The following section highlights some tools that might be useful for you when you are getting used to a new dataset. There’s no one way of exploration, but it’s important to always keep in mind: what should your variables look like (type, values, distribution, etc) what would be surprising (outliers etc) what is your end goal (here, it might be understanding factors associated with delays, e.g. stations, time of year, time of day, etc) In any data analysis project, if it turns out you have data issues, surprising values, missing data etc, it’s important you document anything you found and the subsequent steps or assumptions you made before moving onto your data analysis / modeling. As always: Start with an end in mind. Be as lazy as possible. 13.5 Data checks 13.5.1 Sanity Checks We need to check variables should be what they say they are. If they aren’t, the natural next question is to what to do with issues (recode? remove?) E.g. check days of week unique(delay_2019$day) ## [1] &quot;Tuesday&quot; &quot;Wednesday&quot; &quot;Thursday&quot; &quot;Friday&quot; &quot;Saturday&quot; &quot;Sunday&quot; ## [7] &quot;Monday&quot; Check lines: oh no. some issues here. Some have obvious recodes, others, not so much. unique(delay_2019$line) ## [1] &quot;YU&quot; &quot;BD&quot; &quot;YU/BD&quot; ## [4] &quot;SHP&quot; &quot;SRT&quot; NA ## [7] &quot;YUS&quot; &quot;B/D&quot; &quot;BD LINE&quot; ## [10] &quot;999&quot; &quot;YU/ BD&quot; &quot;YU &amp; BD&quot; ## [13] &quot;BD/YU&quot; &quot;YU\\\\BD&quot; &quot;46 MARTIN GROVE&quot; ## [16] &quot;RT&quot; &quot;BLOOR-DANFORTH&quot; &quot;YU / BD&quot; ## [19] &quot;134 PROGRESS&quot; &quot;YU - BD&quot; &quot;985 SHEPPARD EAST EXPR&quot; ## [22] &quot;22 COXWELL&quot; &quot;100 FLEMINGDON PARK&quot; &quot;YU LINE&quot; The skimr package might also be useful here # skim(delay_2019) What are the different values of bound for each line? For simplicity, just keep the correct line labels. # delay_2019 %&gt;% # filter(line %in% c(&quot;BD&quot;, &quot;YU&quot;, &quot;SHP&quot;, &quot;SRT&quot;)) %&gt;% # mutate(bound = as.factor(bound)) %&gt;% # group_by(line) %&gt;% # skim(bound) 13.5.2 Missing values Look to see how many NAs by variable delay_2019 %&gt;% summarise_all(.funs = funs(sum(is.na(.))/nrow(delay_2019)*100)) ## # A tibble: 1 x 10 ## date time day station code min_delay min_gap bound line vehicle ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 0 0 0 0 22.8 0.260 0 The visdat package is also useful here, particularly to see how missing values are distributed. vis_dat(delay_2019) vis_miss(delay_2019) 13.5.3 Duplicates? The get_dupes function from the janitor package is useful for this. get_dupes(delay_2019) ## # A tibble: 158 x 11 ## date time day station code min_delay min_gap bound line ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2019-01-01 00:00:00 08:18 Tues… DONLAN… MUESA 5 10 W BD ## 2 2019-01-01 00:00:00 08:18 Tues… DONLAN… MUESA 5 10 W BD ## 3 2019-02-01 00:00:00 05:51 Frid… SCARB … MRTO 10 15 S SRT ## 4 2019-02-01 00:00:00 05:51 Frid… SCARB … MRTO 10 15 S SRT ## 5 2019-02-01 00:00:00 06:45 Frid… MIDLAN… MRWEA 3 8 S SRT ## 6 2019-02-01 00:00:00 06:45 Frid… MIDLAN… MRWEA 3 8 S SRT ## 7 2019-02-01 00:00:00 06:55 Frid… LAWREN… ERDO 0 0 S SRT ## 8 2019-02-01 00:00:00 06:55 Frid… LAWREN… ERDO 0 0 S SRT ## 9 2019-02-01 00:00:00 07:16 Frid… MCCOWA… MRWEA 5 10 N SRT ## 10 2019-02-01 00:00:00 07:16 Frid… MCCOWA… MRWEA 5 10 N SRT ## # … with 148 more rows, and 2 more variables: vehicle &lt;dbl&gt;, dupe_count &lt;int&gt; There are quite a few duplicates. Remove for now: delay_2019 &lt;- delay_2019 %&gt;% distinct() 13.5.4 Visualizing distributions Histograms, barplots, and density plots are your friends here. Let’s look at the outcome of interest: min_delay. First of all just a histogram of all the data: ## Removing the observations that have non-standardized lines delay_2019 &lt;- delay_2019 %&gt;% filter(line %in% c(&quot;BD&quot;, &quot;YU&quot;, &quot;SHP&quot;, &quot;SRT&quot;)) ggplot(data = delay_2019) + geom_histogram(aes(x = min_delay)) To improve readability, could plot on logged scale: ggplot(data = delay_2019) + geom_histogram(aes(x = min_delay)) + scale_x_log10() Our initial EDA hinted at an outlying delay time, let’s take a look at the largest delays below. Join the delay_codes dataset to see what the delay is. (Have to do some mangling as SRT has different codes). delay_2019 &lt;- delay_2019 %&gt;% left_join(delay_codes %&gt;% rename(code = `SUB RMENU CODE`, code_desc = `CODE DESCRIPTION...3`) %&gt;% select(code, code_desc)) delay_2019 &lt;- delay_2019 %&gt;% mutate(code_srt = ifelse(line==&quot;SRT&quot;, code, &quot;NA&quot;)) %&gt;% left_join(delay_codes %&gt;% rename(code_srt = `SRT RMENU CODE`, code_desc_srt = `CODE DESCRIPTION...7`) %&gt;% select(code_srt, code_desc_srt)) %&gt;% mutate(code = ifelse(code_srt==&quot;NA&quot;, code, code_srt), code_desc = ifelse(is.na(code_desc_srt), code_desc, code_desc_srt)) %&gt;% select(-code_srt, -code_desc_srt) The 455 min delay due to ‘Rail Related Problem’ is an outlier. delay_2019 %&gt;% left_join(delay_codes %&gt;% rename(code = `SUB RMENU CODE`, code_desc = `CODE DESCRIPTION...3`) %&gt;% select(code, code_desc)) %&gt;% arrange(-min_delay) %&gt;% select(date, time, station, line, min_delay, code, code_desc) ## # A tibble: 18,697 x 7 ## date time station line min_delay code code_desc ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2019-06-25 00:00:00 18:48 WILSON TO … YU 455 PUTR Rail Related Pro… ## 2 2019-02-12 00:00:00 20:28 LAWRENCE E… SRT 284 MRWEA Weather Reports … ## 3 2019-06-05 00:00:00 12:42 UNION TO S… YU 250 MUPLA Fire/Smoke Plan A ## 4 2019-10-22 00:00:00 14:22 LAWRENCE S… YU 228 PUTS Structure Relate… ## 5 2019-09-26 00:00:00 11:38 YORK MILLS… YU 193 MUPR1 Priority One - T… ## 6 2019-06-08 00:00:00 08:51 SPADINA BD… BD 180 MUPLB Fire/Smoke Plan … ## 7 2019-12-02 00:00:00 06:59 DUNDAS WES… BD 176 MUPLB Fire/Smoke Plan … ## 8 2019-01-29 00:00:00 05:46 VICTORIA P… BD 174 MUWEA Weather Reports … ## 9 2019-02-22 00:00:00 17:32 ELLESMERE … SRT 168 PRW Rail Defect/Fast… ## 10 2019-02-10 00:00:00 07:53 BAYVIEW ST… SHP 165 PUSI Signals or Relat… ## # … with 18,687 more rows 13.5.4.1 Grouping and small multiples A quick and powerful visualization technique is to group the data by a variable of interest, e.g. line ggplot(data = delay_2019) + geom_histogram(aes(x = min_delay, y = ..density.., fill = line), position = &#39;dodge&#39;, bins = 10) + scale_x_log10() I switched to density above to look at the the distributions more comparably, but we should also be aware of differences in frequency, in particular, SHP and SRT have much smaller counts: ggplot(data = delay_2019) + geom_histogram(aes(x = min_delay, fill = line), position = &#39;dodge&#39;, bins = 10) + scale_x_log10() If you want to group by more than one variable, facets are good: ggplot(data = delay_2019) + geom_density(aes(x = min_delay, color = day), bw = .08) + scale_x_log10() + facet_grid(~line) Side note: the station names are a mess. Try and clean up the station names a bit by taking just the first word (or, the first two if it starts with “ST”): delay_2019 &lt;- delay_2019 %&gt;% mutate(station_clean = ifelse(str_starts(station, &quot;ST&quot;), word(station, 1,2), word(station, 1))) Plot top five stations by mean delay: delay_2019 %&gt;% group_by(line, station_clean) %&gt;% summarise(mean_delay = mean(min_delay), n_obs = n()) %&gt;% filter(n_obs&gt;1) %&gt;% arrange(line, -mean_delay) %&gt;% slice(1:5) %&gt;% ggplot(aes(station_clean, mean_delay)) + geom_col() + coord_flip() + facet_wrap(~line, scales = &quot;free_y&quot;) 13.5.5 Visualizing time series Daily plot is messy (you can check for yourself). Let’s look by week to see if there’s any seasonality. The lubridate package has lots of helpful functions that deal with date variables. First, mean delay (of those that were delayed more than 0 mins): delay_2019 %&gt;% filter(min_delay&gt;0) %&gt;% mutate(week = week(date)) %&gt;% group_by(week, line) %&gt;% summarise(mean_delay = mean(min_delay)) %&gt;% ggplot(aes(week, mean_delay, color = line)) + geom_point() + geom_smooth() + facet_grid(~line) What about proportion of delays that were greater than 10 mins? delay_2019 %&gt;% mutate(week = week(date)) %&gt;% group_by(week, line) %&gt;% summarise(prop_delay = sum(min_delay&gt;10)/n()) %&gt;% ggplot(aes(week, prop_delay, color = line)) + geom_point() + geom_smooth() + facet_grid(~line) 13.5.6 Visualizing relationships Note that scatter plots are a good precursor to modeling, to visualize relationships between continuous variables. Nothing obvious to plot here, but easy to do with geom_point. Look at top five reasons for delay by station. Do they differ? Think about how this could be modeled. delay_2019 %&gt;% group_by(line, code_desc) %&gt;% summarise(mean_delay = mean(min_delay)) %&gt;% arrange(-mean_delay) %&gt;% slice(1:5) %&gt;% ggplot(aes(x = code_desc, y = mean_delay)) + geom_col() + facet_wrap(vars(line), scales = &quot;free_y&quot;, nrow = 4) + coord_flip() 13.5.7 PCA Principal components analysis is a really powerful exploratory tool. It allows you to pick up potential clusters and/or outliers that can help to inform model building. Let’s do a quick (and imperfect) example looking at types of delays by station. The delay categories are a bit of a mess, and there’s hundreds of them. As a simple start, let’s just take the first word: delay_2019 &lt;- delay_2019 %&gt;% mutate(code_red = case_when( str_starts(code_desc, &quot;No&quot;) ~ word(code_desc, 1, 2), str_starts(code_desc, &quot;Operator&quot;) ~ word(code_desc, 1,2), TRUE ~ word(code_desc,1)) ) Let’s also just restrict the analysis to causes that happen at least 50 times over 2019. To do the PCA, the dataframe also needs to be switched to wide format: dwide &lt;- delay_2019 %&gt;% group_by(line, station_clean) %&gt;% mutate(n_obs = n()) %&gt;% filter(n_obs&gt;1) %&gt;% group_by(code_red) %&gt;% mutate(tot_delay = n()) %&gt;% arrange(tot_delay) %&gt;% filter(tot_delay&gt;50) %&gt;% group_by(line, station_clean, code_red) %&gt;% summarise(n_delay = n()) %&gt;% pivot_wider(names_from = code_red, values_from = n_delay) %&gt;% mutate_all(.funs = funs(ifelse(is.na(.), 0, .))) Do the PCA: delay_pca &lt;- prcomp(dwide[,3:ncol(dwide)]) df_out &lt;- as_tibble(delay_pca$x) df_out &lt;- bind_cols(dwide %&gt;% select(line, station_clean), df_out) head(df_out) ## # A tibble: 6 x 40 ## # Groups: line, station_clean [6] ## line station_clean PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BD BATHURST 6.50 26.9 -2.71 -10.8 -8.40 -11.7 -3.33 -4.11 ## 2 BD BAY 24.8 7.63 -2.19 -7.05 0.714 3.90 -2.29 -4.14 ## 3 BD BLOOR -62.4 -112. 57.3 -23.4 -5.09 -14.1 13.7 5.06 ## 4 BD BROADVIEW -6.60 28.1 -1.06 -14.0 -6.49 -8.29 -6.29 -1.40 ## 5 BD CASTLE 23.8 11.8 -1.31 -7.93 -3.62 -3.37 -2.08 -3.48 ## 6 BD CHESTER 24.6 -1.87 -18.6 2.75 1.85 0.0736 3.79 -1.27 ## # … with 30 more variables: PC9 &lt;dbl&gt;, PC10 &lt;dbl&gt;, PC11 &lt;dbl&gt;, PC12 &lt;dbl&gt;, ## # PC13 &lt;dbl&gt;, PC14 &lt;dbl&gt;, PC15 &lt;dbl&gt;, PC16 &lt;dbl&gt;, PC17 &lt;dbl&gt;, PC18 &lt;dbl&gt;, ## # PC19 &lt;dbl&gt;, PC20 &lt;dbl&gt;, PC21 &lt;dbl&gt;, PC22 &lt;dbl&gt;, PC23 &lt;dbl&gt;, PC24 &lt;dbl&gt;, ## # PC25 &lt;dbl&gt;, PC26 &lt;dbl&gt;, PC27 &lt;dbl&gt;, PC28 &lt;dbl&gt;, PC29 &lt;dbl&gt;, PC30 &lt;dbl&gt;, ## # PC31 &lt;dbl&gt;, PC32 &lt;dbl&gt;, PC33 &lt;dbl&gt;, PC34 &lt;dbl&gt;, PC35 &lt;dbl&gt;, PC36 &lt;dbl&gt;, ## # PC37 &lt;dbl&gt;, PC38 &lt;dbl&gt; Plot the first two PCs, and label some outlying stations: ggplot(df_out,aes(x=PC1,y=PC2,color=line )) + geom_point() + geom_text_repel(data = df_out %&gt;% filter(PC2&gt;100|PC1&lt;100*-1), aes(label = station_clean)) Plot the factor loadings. Some evidence of public v operator? df_out_r &lt;- as_tibble(delay_pca$rotation) df_out_r$feature &lt;- colnames(dwide[,3:ncol(dwide)]) df_out_r ## # A tibble: 38 x 39 ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.0412 0.0638 1.33e-2 -4.67e-2 0.0246 0.0184 -0.00363 0.0198 ## 2 -0.0332 -0.00469 -4.14e-2 -7.51e-3 0.0201 -0.0122 -0.0914 -0.0903 ## 3 -0.135 0.207 2.37e-2 -1.44e-1 0.135 -0.0381 -0.00931 -0.320 ## 4 -0.0652 0.0475 -4.43e-2 -2.51e-2 -0.00139 -0.0748 -0.144 -0.428 ## 5 -0.00443 0.00878 -4.99e-5 -8.30e-4 0.00967 0.00954 -0.0160 -0.0144 ## 6 -0.0268 -0.00722 -4.39e-3 5.34e-4 -0.0151 -0.0125 -0.00381 -0.0423 ## 7 -0.0813 0.0960 -4.62e-2 4.79e-2 -0.0978 -0.0365 -0.0766 0.278 ## 8 -0.0117 0.0135 5.48e-3 -2.94e-2 0.0125 0.0377 -0.0790 -0.0321 ## 9 -0.516 0.655 -1.77e-2 -1.62e-1 -0.221 -0.287 -0.184 0.0465 ## 10 -0.151 0.0826 5.48e-2 3.52e-1 -0.397 0.281 0.110 0.477 ## # … with 28 more rows, and 31 more variables: PC9 &lt;dbl&gt;, PC10 &lt;dbl&gt;, ## # PC11 &lt;dbl&gt;, PC12 &lt;dbl&gt;, PC13 &lt;dbl&gt;, PC14 &lt;dbl&gt;, PC15 &lt;dbl&gt;, PC16 &lt;dbl&gt;, ## # PC17 &lt;dbl&gt;, PC18 &lt;dbl&gt;, PC19 &lt;dbl&gt;, PC20 &lt;dbl&gt;, PC21 &lt;dbl&gt;, PC22 &lt;dbl&gt;, ## # PC23 &lt;dbl&gt;, PC24 &lt;dbl&gt;, PC25 &lt;dbl&gt;, PC26 &lt;dbl&gt;, PC27 &lt;dbl&gt;, PC28 &lt;dbl&gt;, ## # PC29 &lt;dbl&gt;, PC30 &lt;dbl&gt;, PC31 &lt;dbl&gt;, PC32 &lt;dbl&gt;, PC33 &lt;dbl&gt;, PC34 &lt;dbl&gt;, ## # PC35 &lt;dbl&gt;, PC36 &lt;dbl&gt;, PC37 &lt;dbl&gt;, PC38 &lt;dbl&gt;, feature &lt;chr&gt; ggplot(df_out_r,aes(x=PC1,y=PC2,label=feature )) + geom_text_repel() 13.6 Exercises Using the opendatatoronto package, download the data on mayoral campaign contributions for 2014. (note: the 2014 file you will get from get_resource, so just keep the sheet that relates to the Mayor election). Clean up the data format (fixing the parsing issue and standardizing the column names using janitor) Summarize the variables in the dataset. Are there missing values, and if so, should we be worried about them? Is every variable in the format it should be? If not, create new variable(s) that are in the right format. Visually explore the distribution of values of the contributions. What contributions are notable outliers? Do they share a similar characteristic(s)? It may be useful to plot the distribution of contributions without these outliers to get a better sense of the majority of the data. List the top five candidates in each of these categories: total contributions mean contribution number of contributions Repeat 5 but without contributions from the candidates themselves. How many contributors gave money to more than one candidate? 13.7 Case study - Opinions about a casino in Toronto This was written by Michael Chong. 13.7.1 Data preparation 13.7.1.1 Getting data from opendatatoronto Here we use the opendatatoronto package again. See the previous example RMarkdown file for a deeper explanation of how the code below works. The dataset I’m extracting below are the results from a survey in 2012 regarding the establishment of a casino in Toronto. More info available by following this link. In this analysis, we’ll be hoping to address the question, which demographic (age/gender) groups are more likely to be supportive of a new casino in Toronto? # Get the data casino_resource &lt;- search_packages(&quot;casino survey&quot;)%&gt;% list_package_resources() %&gt;% filter(name == &quot;toronto-casino-survey-results&quot;) %&gt;% get_resource() 13.7.1.2 Getting the right kind of object The object casino_resource isn’t quite useable yet, because it’s (inconveniently) stored as a list of 2 data frames: # Check what kind of object the casino_resource object is class(casino_resource) ## [1] &quot;list&quot; If we just return the object, we can see that the 2nd list item is empty, and we just want to keep the first one: casino_resource ## $tblSurvey ## # A tibble: 17,766 x 94 ## SurveyID Q1_A Q1_B1 Q1_B2 Q1_B3 Q2_A Q2_B Q3_A Q3_B Q3_C Q3_D Q3_E ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Stro… Do n… Do n… Do n… Does… &quot;As … Not … Very… Not … Not … Not … ## 2 2 Stro… Econ… Jobs Arts… Fits… &quot;Cos… Very… Very… Very… Very… Very… ## 3 3 Stro… Ther… If t… &lt;NA&gt; Fits… &quot;Big… Very… Very… Very… Very… Very… ## 4 4 Some… beli… mone… evid… Does… &quot;My … Very… Very… Some… Some… Very… ## 5 5 Neut… Like… Conc… &lt;NA&gt; Neut… &quot;Aga… Very… Very… Very… Not … Very… ## 6 6 Stro… have… &lt;NA&gt; &lt;NA&gt; Does… &quot;Tor… Not … Not … Not … Not … Not … ## 7 7 Stro… The … Peop… We s… Does… &quot;#3 … Not … Not … Not … Not … Not … ## 8 8 Stro… It w… Mora… &lt;NA&gt; Does… &quot;Cas… Very… Very… Very… Very… Very… ## 9 9 Stro… It&#39;s… traf… heal… Does… &quot;No … Not … Very… Not … Not … Some… ## 10 10 Stro… Toro… Avoi… Prov… Fits… &quot;Tor… Very… Very… Very… Not … Very… ## # … with 17,756 more rows, and 82 more variables: Q3_F &lt;chr&gt;, Q3_G &lt;chr&gt;, ## # Q3_H &lt;chr&gt;, Q3_I &lt;chr&gt;, Q3_J &lt;chr&gt;, Q3_K &lt;chr&gt;, Q3_L &lt;chr&gt;, Q3_M &lt;chr&gt;, ## # Q3_N &lt;chr&gt;, Q3_O &lt;chr&gt;, Q3_P &lt;chr&gt;, Q3_Q &lt;chr&gt;, Q3_Q_Other &lt;chr&gt;, ## # Q3_Comments &lt;chr&gt;, Q4_A &lt;chr&gt;, Q5 &lt;chr&gt;, Q6 &lt;chr&gt;, Q6_Comments &lt;chr&gt;, ## # Q7_A_StandAlone &lt;chr&gt;, Q7_A_Integrated &lt;chr&gt;, Q7_A1 &lt;chr&gt;, Q7_A2 &lt;chr&gt;, ## # Q7_A3 &lt;chr&gt;, Q7_A_A &lt;chr&gt;, Q7_A_B &lt;chr&gt;, Q7_A_C &lt;chr&gt;, Q7_A_D &lt;chr&gt;, ## # Q7_A_E &lt;chr&gt;, Q7_A_F &lt;chr&gt;, Q7_A_G &lt;chr&gt;, Q7_A_H &lt;chr&gt;, Q7_A_I &lt;chr&gt;, ## # Q7_A_J &lt;chr&gt;, Q7_A_J_Other &lt;chr&gt;, Q7_B_StandAlone &lt;chr&gt;, ## # Q7_B_Integrated &lt;chr&gt;, Q7_B1 &lt;chr&gt;, Q7_B2 &lt;chr&gt;, Q7_B3 &lt;chr&gt;, Q7_B_A &lt;chr&gt;, ## # Q7_B_B &lt;chr&gt;, Q7_B_C &lt;chr&gt;, Q7_B_D &lt;chr&gt;, Q7_B_E &lt;chr&gt;, Q7_B_F &lt;chr&gt;, ## # Q7_B_G &lt;chr&gt;, Q7_B_H &lt;chr&gt;, Q7_B_I &lt;chr&gt;, Q7_B_J &lt;chr&gt;, Q7_B_J_Other &lt;chr&gt;, ## # Q7_C_StandAlone &lt;chr&gt;, Q7_C_Integrated &lt;chr&gt;, Q7_C1 &lt;chr&gt;, Q7_C2 &lt;chr&gt;, ## # Q7_C3 &lt;chr&gt;, Q7_C_A &lt;chr&gt;, Q7_C_B &lt;chr&gt;, Q7_C_C &lt;chr&gt;, Q7_C_D &lt;chr&gt;, ## # Q7_C_E &lt;chr&gt;, Q7_C_F &lt;chr&gt;, Q7_C_G &lt;chr&gt;, Q7_C_H &lt;chr&gt;, Q7_C_I &lt;chr&gt;, ## # Q7_C_J &lt;chr&gt;, Q7_C_J_Other &lt;chr&gt;, Q8_A1 &lt;chr&gt;, Q8_A2 &lt;chr&gt;, Q8_B1 &lt;chr&gt;, ## # Q8_B2 &lt;chr&gt;, Q8_B3 &lt;chr&gt;, Q9 &lt;chr&gt;, Q9_Considerations &lt;chr&gt;, Q10 &lt;chr&gt;, ## # Q11 &lt;chr&gt;, Age &lt;chr&gt;, Gender &lt;chr&gt;, PostalCode &lt;chr&gt;, GroupName &lt;chr&gt;, ## # DateCreated &lt;dttm&gt;, ...93 &lt;lgl&gt;, ...94 &lt;lgl&gt; ## ## $Sheet1 ## # A tibble: 0 x 0 So, let’s only keep the first item by indexing the list with double square brackets: casino_data &lt;- casino_resource[[1]] 13.7.1.3 Cleaning up the dataframe Let’s check out what the first couple rows of the dataframe looks like. By default, head() returns the first 6 rows: head(casino_data) ## # A tibble: 6 x 94 ## SurveyID Q1_A Q1_B1 Q1_B2 Q1_B3 Q2_A Q2_B Q3_A Q3_B Q3_C Q3_D Q3_E ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Stro… Do n… Do n… Do n… Does… &quot;As … Not … Very… Not … Not … Not … ## 2 2 Stro… Econ… Jobs Arts… Fits… &quot;Cos… Very… Very… Very… Very… Very… ## 3 3 Stro… Ther… If t… &lt;NA&gt; Fits… &quot;Big… Very… Very… Very… Very… Very… ## 4 4 Some… beli… mone… evid… Does… &quot;My … Very… Very… Some… Some… Very… ## 5 5 Neut… Like… Conc… &lt;NA&gt; Neut… &quot;Aga… Very… Very… Very… Not … Very… ## 6 6 Stro… have… &lt;NA&gt; &lt;NA&gt; Does… &quot;Tor… Not … Not … Not … Not … Not … ## # … with 82 more variables: Q3_F &lt;chr&gt;, Q3_G &lt;chr&gt;, Q3_H &lt;chr&gt;, Q3_I &lt;chr&gt;, ## # Q3_J &lt;chr&gt;, Q3_K &lt;chr&gt;, Q3_L &lt;chr&gt;, Q3_M &lt;chr&gt;, Q3_N &lt;chr&gt;, Q3_O &lt;chr&gt;, ## # Q3_P &lt;chr&gt;, Q3_Q &lt;chr&gt;, Q3_Q_Other &lt;chr&gt;, Q3_Comments &lt;chr&gt;, Q4_A &lt;chr&gt;, ## # Q5 &lt;chr&gt;, Q6 &lt;chr&gt;, Q6_Comments &lt;chr&gt;, Q7_A_StandAlone &lt;chr&gt;, ## # Q7_A_Integrated &lt;chr&gt;, Q7_A1 &lt;chr&gt;, Q7_A2 &lt;chr&gt;, Q7_A3 &lt;chr&gt;, Q7_A_A &lt;chr&gt;, ## # Q7_A_B &lt;chr&gt;, Q7_A_C &lt;chr&gt;, Q7_A_D &lt;chr&gt;, Q7_A_E &lt;chr&gt;, Q7_A_F &lt;chr&gt;, ## # Q7_A_G &lt;chr&gt;, Q7_A_H &lt;chr&gt;, Q7_A_I &lt;chr&gt;, Q7_A_J &lt;chr&gt;, Q7_A_J_Other &lt;chr&gt;, ## # Q7_B_StandAlone &lt;chr&gt;, Q7_B_Integrated &lt;chr&gt;, Q7_B1 &lt;chr&gt;, Q7_B2 &lt;chr&gt;, ## # Q7_B3 &lt;chr&gt;, Q7_B_A &lt;chr&gt;, Q7_B_B &lt;chr&gt;, Q7_B_C &lt;chr&gt;, Q7_B_D &lt;chr&gt;, ## # Q7_B_E &lt;chr&gt;, Q7_B_F &lt;chr&gt;, Q7_B_G &lt;chr&gt;, Q7_B_H &lt;chr&gt;, Q7_B_I &lt;chr&gt;, ## # Q7_B_J &lt;chr&gt;, Q7_B_J_Other &lt;chr&gt;, Q7_C_StandAlone &lt;chr&gt;, ## # Q7_C_Integrated &lt;chr&gt;, Q7_C1 &lt;chr&gt;, Q7_C2 &lt;chr&gt;, Q7_C3 &lt;chr&gt;, Q7_C_A &lt;chr&gt;, ## # Q7_C_B &lt;chr&gt;, Q7_C_C &lt;chr&gt;, Q7_C_D &lt;chr&gt;, Q7_C_E &lt;chr&gt;, Q7_C_F &lt;chr&gt;, ## # Q7_C_G &lt;chr&gt;, Q7_C_H &lt;chr&gt;, Q7_C_I &lt;chr&gt;, Q7_C_J &lt;chr&gt;, Q7_C_J_Other &lt;chr&gt;, ## # Q8_A1 &lt;chr&gt;, Q8_A2 &lt;chr&gt;, Q8_B1 &lt;chr&gt;, Q8_B2 &lt;chr&gt;, Q8_B3 &lt;chr&gt;, Q9 &lt;chr&gt;, ## # Q9_Considerations &lt;chr&gt;, Q10 &lt;chr&gt;, Q11 &lt;chr&gt;, Age &lt;chr&gt;, Gender &lt;chr&gt;, ## # PostalCode &lt;chr&gt;, GroupName &lt;chr&gt;, DateCreated &lt;dttm&gt;, ...93 &lt;lgl&gt;, ## # ...94 &lt;lgl&gt; Unfortunately the column names aren’t very informative. For simplicity, we’ll use the .pdf questionnaire that accompanies this dataset from the Toronto Open Data website. Alternatively, we could get and parse the readme through the R package. Here’s a link to the questionnaire. Question 1 indicates the level of support for a casino in Toronto. We’ll use this as the response variable. Concerning potential predictor variables, most of the questions ask respondents about their opinions on different aspects of a potential casino development, which aren’t particularly useful towards our cause. The only demographic variables are Age and Gender, so let’s choose these. Here I’m also going to rename the columns so that my resulting data frame has columns opinion, age, and gender. # Narrow down the dataframe to our variables of interest casino_data &lt;- casino_data %&gt;% select(Q1_A, Age, Gender) %&gt;% rename(opinion = Q1_A, age = Age, gender = Gender) # Look at first couple rows: head(casino_data) ## # A tibble: 6 x 3 ## opinion age gender ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Strongly Opposed 25-34 Male ## 2 Strongly in Favour 35-44 Female ## 3 Strongly in Favour 55-64 Male ## 4 Somewhat Opposed 25-34 Male ## 5 Neutral or Mixed Feelings 25-34 Female ## 6 Strongly Opposed 45-54 Female 13.7.2 Some visual exploration (and more cleanup, of course) Let’s first do some quick exploration to get a feel for what’s going on in the data. We’ll first calculate proportions of casino support for each age-gender combination: # Calculate proportions casino_summary &lt;- casino_data %&gt;% group_by(age, gender, opinion) %&gt;% summarise(n = n()) %&gt;% # Count the number in each group and response group_by(age, gender) %&gt;% mutate(prop = n/sum(n)) # Calculate proportions within each group Some notes: * we use geom_col() to make a bar chart, * facet_grid() modifies the plot so that the plot has panels that correspond only to certain values of discrete variables (in this case, we will “facet” by age and gender). This is helpful in this case because we are interested in how the distribution of opinions changes by age and gender. ggplot(casino_summary) + geom_col(aes(x = opinion, y = prop)) + # Specify a histogram of opinion responses facet_grid(age~gender) + #Facet by age and gender theme(axis.text.x = element_text(angle = 90)) # Rotate the x-axis labels to be readable Some things to note: the x-axis labels are out of order in the sense that they are not in a monotone order of increasing/decreasing support there are NA values in opinion, age, and gender, as well as “Prefer not to disclose” responses 13.7.2.1 Getting the data into a more model-suitable format 13.7.2.1.1 Get rid of responses that aren’t suitable For simplicity we’ll assume that NA values and “Prefer not to disclose” responses occur randomly, and remove them from our dataset (note in reality this assumption might not hold up and we might want to be more careful). Let’s check how many rows are in the original dataset: # nrow() returns the number of rows in a dataframe: nrow(casino_data) ## [1] 17766 Now let’s filter() accordingly to omit the responses we don’t want. In case you’re unfamiliar, I’m going to make use of: is.na(), which returns TRUE if the argument is NA, the ! operator, which flips TRUE and FALSE. So for instance, !is.na(x) will return TRUE if x is NOT NA, which is what we want to keep. casino_data &lt;- casino_data %&gt;% # Only keep rows with non-NA: filter(!is.na(opinion), !is.na(age), !is.na(gender)) %&gt;% # Only keep rows where age and gender are disclosed: filter(age != &quot;Prefer not to disclose&quot;, gender != &quot;Prefer not to disclose&quot;) Let’s check how many rows of data we’re left with: nrow(casino_data) ## [1] 13658 13.7.2.1.2 Convert response variable into binary To clean up the first problem (response variables out of order), we might as well take this opportunity to convert these into a format suitable for our model. In a logistic regression, we would like our response variable to be binary, but in this case we have 5 possible categories ranging from “Strongly Opposed” to “Strongly in Favour”. We’ll recategorize them into a new supportive_or_not variable as follows. supportive = 1 if “Strongly in Favour” or “Somewhat in Favour” supportive = 0 if “Neutral or Mixed Feelings”, “Somewhat Opposed”, or “Strongly Opposed” We do this with the mutate() function, which creates new columns (possibly as functions of existing columns), and case_when(), which provides a way to assign values conditional on if-statements. The syntax here is a little strange. On the LHS of the ~ is the “if” condition, and the RHS of the tilde is the value to return. For example, x == 0 ~ 3 would return 3 when x is 0. Another commonly used operator here is the %in% operator, which checks whether something is an element of a vector. E.g.: 1 %in% c(1, 3, 4) returns TRUE 2 %in% c(1, 3, 4) returns FALSE # Store possible opinions in vectors yes_opinions &lt;- c(&quot;Strongly in Favour&quot;, &quot;Somewhat in Favour&quot;) no_opinions &lt;- c(&quot;Neutral or Mixed Feelings&quot;, &quot;Somewhat Opposed&quot;, &quot;Strongly Opposed&quot;) # Create `supportive` column: casino_data &lt;- casino_data %&gt;% mutate(supportive = case_when( opinion %in% yes_opinions ~ TRUE, # Assign TRUE opinion %in% no_opinions ~ FALSE # Assign FALSE )) 13.7.2.1.3 Convert age to a numeric variable Age in this survey is given in age groups. Let’s instead treat it map it to a numeric variable so that we can more easily talk about trends with age. We’ll map the youngest age to 1, and so on: casino_data &lt;- casino_data %&gt;% mutate(age_group = case_when( age == &quot;Under 15&quot; ~ 0, age == &quot;15-24&quot; ~ 1, age == &quot;25-34&quot; ~ 2, age == &quot;35-44&quot; ~ 3, age == &quot;45-54&quot; ~ 4, age == &quot;55-64&quot; ~ 5, age == &quot;65 or older&quot; ~ 6 )) Now let’s make the same plot again, with our new processed data: casino_summary2 &lt;- casino_data %&gt;% group_by(age_group, gender, supportive) %&gt;% summarise(n = n()) %&gt;% # Count the number in each group and response group_by(age_group, gender) %&gt;% mutate(prop = n/sum(n)) # Calculate proportions within each group ggplot(casino_summary2) + facet_grid(age_group ~ gender) + geom_col(aes(x = supportive, y = prop)) We can sort of see some difference in the distribution between different panels. To formalize this, we can run a logistic regression. 13.7.3 Logistic Regression Now, we’re set up to feed it to the regression. We can do this with glm(), which allows us to fit generalized linear models. We use family = \"binomial\" to specify a logistic regression, and our formula is supportive ~ age_group + gender, which indicates that supportive is the (binary) response variable since it’s on the LHS, and age_group and gender are our predictor variables. casino_glm &lt;- glm(supportive ~ age_group + gender, data = casino_data, family = &quot;binomial&quot;) We can take a look at the results of running the GLM using summary(): summary(casino_glm) ## ## Call: ## glm(formula = supportive ~ age_group + gender, family = &quot;binomial&quot;, ## data = casino_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.0107 -0.8888 -0.6804 1.4249 1.8822 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.10594 0.05863 -18.862 &lt; 2e-16 *** ## age_group -0.07983 0.01376 -5.801 6.59e-09 *** ## genderMale 0.70036 0.04027 17.390 &lt; 2e-16 *** ## genderTransgendered 0.69023 0.39276 1.757 0.0789 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 16010 on 13657 degrees of freedom ## Residual deviance: 15653 on 13654 degrees of freedom ## AIC: 15661 ## ## Number of Fisher Scoring iterations: 4 13.7.3.1 Interpretation Interpretation can be a little tricky. Here are some important things to note about our results: 13.7.3.1.1 Numeric age group variable Remember that we coded age_group as numbers 1 to 5. Because we’ve used age groups instead of age, we have to be careful with how we phrase our conclusion. The coefficient estimate corresponds to the effect of moving up a unit on the age group scale (e.g. from the 25-34 age group to the 35-44 age group), rather than 1 year in age (e.g. from age 28 to 29). 13.7.3.1.2 log-odds ratios The effect estimates are on the log-odds scale. This means the effect of -0.07983 for age_group is interpreted as: for each unit increase in age_group, we estimate a 0.07983 decrease in the log-odds of being supportive of a casino. We could exponentiate the coefficient estimate to make this at least a little easier to interpret. The number we get is interpreted as a factor for the odds. exp(-0.07983) ## [1] 0.9232733 So our (cleaner) interpretation is: the odds of an individuals of the same gender being pro-casino are predicted to change by a factor of 0.9232733 for each unit increase in age_group 13.7.3.2 Baseline category First, note that because we have categorical variables, the gender coefficients are relative to a “baseline” category. The value of gender that doesn’t appear in the table, Female, is implicitly used as our baseline gender category. Technical note: if the variable is stored as a character class, then glm() will choose the alphabetically first value to use as the baseline. exp(0.70036) ## [1] 2.014478 So, the interpretation of the genderMale coefficient is: the odds of a male individual supporting a casino is 2.0144778 times higher than a female individual of the same age_group. 13.7.3.3 Making estimates 13.7.3.3.1 A manual way Using the formula found in ISLR 4.3.3, we can make estimates for an individual of certain characteristics. Suppose we wanted to predict the the probability of supporting a Toronto casino for an individual who was 36 and identified as transgender. Then: age_group takes a value of 3, since they are in the age group of 35-44 coded as 3, genderTransgendered takes a value of 1 First, let’s extract the coefficient estimates as a vector using coefficients(): coefs &lt;- coefficients(casino_glm) coefs ## (Intercept) age_group genderMale genderTransgendered ## -1.10593925 -0.07983372 0.70036199 0.69022910 Since this vector is labelelled, we can index it using square brackets and names. For instance: coefs[&quot;age_group&quot;] ## age_group ## -0.07983372 So first let’s evaluate the exponent term \\(e^{\\beta_0 + \\cdots + \\beta_p X_p}\\): exp_term &lt;- exp(coefs[&quot;(Intercept)&quot;] + coefs[&quot;age_group&quot;]*3 + coefs[&quot;genderTransgendered&quot;]*1) Now evaluate the expression that gives the probability of casino support: # The unname() command just takes off the label that it &quot;inherited&quot; from the coefs vector. # (don&#39;t worry about it, doesn&#39;t affect any functionality) unname(exp_term / (1 + exp_term)) ## [1] 0.3418161 13.7.3.3.2 A more streamlined way Thankfully R comes with a convenient function to make prediction estimates from a glm(). We do this using the predict() function. First, we need to make a dataframe that has the relevant variables and values that we’re interested in predicting. We’ll use the same values as before: prediction_df &lt;- data.frame(age_group = 3, gender = &quot;Transgendered&quot;) The dataframe looks like this: prediction_df ## age_group gender ## 1 3 Transgendered Then we feed it into the predict() function, along with our glm object. To get the probability, we need to specify type = \"response\". predict(casino_glm, newdata = prediction_df, type = &quot;response&quot;) ## 1 ## 0.3418161 This matches the probability we got from doing this manually, yay! 13.8 Case study - Historical Canadian elections 13.9 Case study - Airbnb listing in Toronto 13.9.1 Essentials In this case study we… Recommended reading Kommenda, Niko, Helen Pidd and Libby Brooks, 2020, ‘Revealed: the areas in the UK with one Airbnb for every four homes’, The Guardian, 20 February, freely available at: https://www.theguardian.com/technology/2020/feb/20/revealed-the-areas-in-the-uk-with-one-airbnb-for-every-four-homes. 13.9.2 Set up library(broom) # Helps with model outputs etc library(here) # Helps with specifying path names library(janitor) # Helps with initial data cleaning and pretty tables library(tidymodels) # Help with modelling library(tidyverse) library(skimr) # Helps with initial data visualisation library(visdat) # Helps check missing values 13.9.3 Get data The dataset is from Inside Airbnb (http://insideairbnb.com). We can give read_csv() a link to where the dataset is and it will download it. This helps with reproducibility because the source is clear. But, as that link could change at any time, longer-term reproducibility, as well as wanting to minimise the effect on the Inside Airbnb servers, suggest that we should also save a local copy of the data and then use that. (As the original data is not ours, we should not make that public without first getting written permission.) # For reproducibility # airbnb_data_reduced &lt;- read_csv(&quot;http://data.insideairbnb.com/canada/on/toronto/2019-12-07/data/listings.csv.gz&quot;, guess_max = 20000) # write_csv(airbnb_data_reduced, &quot;week_6/data/airbnb_toronto_2019-12-07.csv&quot;) # For actual work airbnb_data &lt;- read_csv(here::here(&quot;dont_push/airbnb_toronto_2019-12-07.csv&quot;), guess_max = 20000) # The guess_max option in read_csv helps us avoid having to specify the column types. Usually read_csv takes a best guess at the column types based on the first few rows. But sometimes those first ones are misleading and so guess_max forces it to look at a larger number of rows to try to work out what is going on. 13.9.4 Clean data There are an enormous number of columns, so we’ll just select a few. names(airbnb_data) %&gt;% length() ## [1] 106 airbnb_data_selected &lt;- airbnb_data %&gt;% select(host_id, host_since, host_response_time, host_is_superhost, host_listings_count, host_total_listings_count, host_neighbourhood, host_listings_count, neighbourhood_cleansed, room_type, bathrooms, bedrooms, square_feet, price, number_of_reviews, has_availability, review_scores_rating, review_scores_accuracy, review_scores_cleanliness, review_scores_checkin, review_scores_communication, review_scores_location, review_scores_value ) We might like to have a brief look at the dataset to see if anything weird is going on. There are a bunch of ways of doing this, but one way is ‘skim’ from the skimr package (Waring, Quinn, McNamara, Arino de la Rubia, Zhu and Ellis, 2020). # skim(airbnb_data_selected) A few things jump out: There are a character variables that should probably be numerics or dates/times: host_response_time, price, weekly_price, monthly_price, cleaning_fee. Weekly and monthly price is missing in an overwhelming number of observations. Roughly a fifth of observations are missing a review score and there seems like there is some correlation between those review-type variables. There are two variants of the neighbourhood name. There are NAs in host_is_superhost. The reviews seem really skewed. There is someone who has 328 properties on Airbnb. 13.9.4.1 Price First we need to convert to a numeric. This is a common problem, and you need to be a little careful that it doesn’t all just convert to NAs. In our case if we just force the price data to be a numeric then it will go to NA because there are a lot of characters that R doesn’t know how to convert, e.g. what is the numeric for ‘$’? So we need to remove those characters first. airbnb_data_selected$price %&gt;% head() ## [1] &quot;$469.00&quot; &quot;$99.00&quot; &quot;$66.00&quot; &quot;$72.00&quot; &quot;$199.00&quot; &quot;$54.00&quot; # First work out what is going on airbnb_data_selected$price %&gt;% str_split(&quot;&quot;) %&gt;% unlist() %&gt;% unique() ## [1] &quot;$&quot; &quot;4&quot; &quot;6&quot; &quot;9&quot; &quot;.&quot; &quot;0&quot; &quot;7&quot; &quot;2&quot; &quot;1&quot; &quot;5&quot; &quot;3&quot; &quot;8&quot; &quot;,&quot; # It&#39;s clear that &#39;$&#39; needs to go. The only odd thing is &#39;,&#39; so take a look at those: airbnb_data_selected %&gt;% select(price) %&gt;% filter(str_detect(price, &quot;,&quot;)) ## # A tibble: 87 x 1 ## price ## &lt;chr&gt; ## 1 $1,988.00 ## 2 $1,099.00 ## 3 $2,799.00 ## 4 $1,100.00 ## 5 $1,450.00 ## 6 $1,999.00 ## 7 $1,060.00 ## 8 $1,300.00 ## 9 $2,142.00 ## 10 $1,200.00 ## # … with 77 more rows # It&#39;s clear that the data is just nicely formatted, but we need to remove the comma: airbnb_data_selected &lt;- airbnb_data_selected %&gt;% mutate(price = str_remove(price, &quot;\\\\$&quot;), price = str_remove(price, &quot;,&quot;), price = as.integer(price) ) Now we can have a look at prices. # Look at distribution of price airbnb_data_selected %&gt;% ggplot(aes(x = price)) + geom_histogram(binwidth = 10) + theme_classic() + labs(x = &quot;Price per night&quot;, y = &quot;Number of properties&quot;) # We use bins with a width of 10, so that&#39;s going to aggregate prices into 10s so that we don&#39;t get overwhelmed with bars. So we have some outliers. Let’s zoom in on prices that are more than $1,000. # Look at distribution of high prices airbnb_data_selected %&gt;% filter(price &gt; 1000) %&gt;% ggplot(aes(x = price)) + geom_histogram(binwidth = 10) + theme_classic() + labs(x = &quot;Price per night&quot;, y = &quot;Number of properties&quot;) Let’s look in more detail at those with a price more than $4,999 airbnb_data_selected %&gt;% filter(price &gt; 4999) ## # A tibble: 22 x 22 ## host_id host_since host_response_t… host_is_superho… host_listings_c… ## &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 9.91e7 2016-10-10 N/A FALSE 7 ## 2 5.29e6 2013-03-02 within an hour TRUE 4 ## 3 1.20e8 2017-03-07 N/A FALSE 1 ## 4 8.46e6 2013-08-27 N/A FALSE 2 ## 5 1.47e8 2017-08-22 N/A FALSE 2 ## 6 6.74e7 2016-04-16 N/A FALSE 1 ## 7 2.58e8 2019-04-24 within an hour TRUE 5 ## 8 2.58e8 2019-04-24 within an hour TRUE 5 ## 9 2.71e8 2019-06-24 a few days or m… FALSE 1 ## 10 2.72e8 2019-06-27 within an hour FALSE 4 ## # … with 12 more rows, and 17 more variables: host_total_listings_count &lt;dbl&gt;, ## # host_neighbourhood &lt;chr&gt;, neighbourhood_cleansed &lt;chr&gt;, room_type &lt;chr&gt;, ## # bathrooms &lt;dbl&gt;, bedrooms &lt;dbl&gt;, square_feet &lt;dbl&gt;, price &lt;int&gt;, ## # number_of_reviews &lt;dbl&gt;, has_availability &lt;lgl&gt;, ## # review_scores_rating &lt;dbl&gt;, review_scores_accuracy &lt;dbl&gt;, ## # review_scores_cleanliness &lt;dbl&gt;, review_scores_checkin &lt;dbl&gt;, ## # review_scores_communication &lt;dbl&gt;, review_scores_location &lt;dbl&gt;, ## # review_scores_value &lt;dbl&gt; # It&#39;s pretty clear that there is something odd going on with some of these, but some of them seem legit. Let’s look at the distribution of prices that are in a ‘reasonable’ range, which until Monica is a full professor, will be defined as a nightly price of less than $1,000. airbnb_data_selected %&gt;% filter(price &lt; 1000) %&gt;% ggplot(aes(x = price)) + geom_histogram(binwidth = 10) + theme_classic() + labs(x = &quot;Price per night&quot;, y = &quot;Number of properties&quot;) Interestingly it looks like there is some bunching of prices, possible around numbers ending in zero or nine? Let’s just zoom in on prices between $90 and $210, out of interest, but change the bins to be smaller. # Look at distribution of price again airbnb_data_selected %&gt;% filter(price &gt; 90) %&gt;% filter(price &lt; 210) %&gt;% ggplot(aes(x = price)) + geom_histogram(binwidth = 1) + theme_classic() + labs(x = &quot;Price per night&quot;, y = &quot;Number of properties&quot;) 13.9.4.2 Superhosts Airbnb says that: Superhosts are experienced hosts who provide a shining example for other hosts, and extraordinary experiences for their guests. Once a host reaches Superhost status, a badge superhost badge will automatically appear on their listing and profile to help you identify them. We check Superhosts’ activity four times a year, to ensure that the program highlights the people who are most dedicated to providing outstanding hospitality. First we’ll look at the NAs in host_is_superhost. airbnb_data_selected %&gt;% filter(is.na(host_is_superhost)) ## # A tibble: 285 x 22 ## host_id host_since host_response_t… host_is_superho… host_listings_c… ## &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 2.27e6 NA &lt;NA&gt; NA NA ## 2 2.27e6 NA &lt;NA&gt; NA NA ## 3 2.27e6 NA &lt;NA&gt; NA NA ## 4 5.99e6 NA &lt;NA&gt; NA NA ## 5 8.73e6 NA &lt;NA&gt; NA NA ## 6 1.11e7 NA &lt;NA&gt; NA NA ## 7 1.62e7 NA &lt;NA&gt; NA NA ## 8 1.46e5 NA &lt;NA&gt; NA NA ## 9 1.56e7 NA &lt;NA&gt; NA NA ## 10 1.85e7 NA &lt;NA&gt; NA NA ## # … with 275 more rows, and 17 more variables: host_total_listings_count &lt;dbl&gt;, ## # host_neighbourhood &lt;chr&gt;, neighbourhood_cleansed &lt;chr&gt;, room_type &lt;chr&gt;, ## # bathrooms &lt;dbl&gt;, bedrooms &lt;dbl&gt;, square_feet &lt;dbl&gt;, price &lt;int&gt;, ## # number_of_reviews &lt;dbl&gt;, has_availability &lt;lgl&gt;, ## # review_scores_rating &lt;dbl&gt;, review_scores_accuracy &lt;dbl&gt;, ## # review_scores_cleanliness &lt;dbl&gt;, review_scores_checkin &lt;dbl&gt;, ## # review_scores_communication &lt;dbl&gt;, review_scores_location &lt;dbl&gt;, ## # review_scores_value &lt;dbl&gt; There are 285 of these and it’s clear that there is something odd going on - maybe the host removed the listing or similar? We’ll also want to create a binary variable out of this. It’s true/false at the moment, which is fine for the modelling, but there are a handful of situations where it’ll be easier if we have a 0/1. airbnb_data_selected &lt;- airbnb_data_selected %&gt;% mutate(host_is_superhost_binary = case_when( host_is_superhost == TRUE ~ 1, host_is_superhost == FALSE ~ 0, TRUE ~ 999 ) ) airbnb_data_selected$host_is_superhost_binary[airbnb_data_selected$host_is_superhost_binary == 999] &lt;- NA 13.9.4.3 Reviews Airbnb says that: In addition to written reviews, guests can submit an overall star rating and a set of category star ratings for their stay. Hosts can view their star ratings on their Progress page, under Reviews. Hosts using professional hosting tools can find reviews and quality details on their Performance page, under Quality. Guests can give ratings on: Overall experience. Overall, how was the stay? Cleanliness. Did guests feel that the space was clean and tidy? Accuracy. How accurately did the listing page represent the space? For example, guests should be able to find up-to-date info and photos in the listing description. Value. Did the guest feel that the listing provided good value for the price? Communication. How well did you communicate before and during the stay? Guests often care that their host responds quickly, reliably, and frequently to their messages and questions. Check-in. How smoothly did check-in go? Location. How did guests feel about the neighbourhood? This may mean that there’s an accurate description for proximity and access to transportation, shopping centres, city centre, etc., and a description that includes special considerations, like noise, and family safety. Amenities. How did guests feel about the amenities that were available during their stay? Guests often care that all the amenities listed are available, working, and in good condition. In each category, hosts are able to see how often they get 5 stars, how guests rated nearby hosts, and, in some cases, tips to help improve the listing. The number of stars displayed at the top of a listing page is an aggregate of the primary scores guests have given for that listing. At the bottom of a listing page, there’s an aggregate for each category rating. A host needs to receive star ratings from at least 3 guests before their aggregate score appears. TODO: I don’t understand how these review scores are being constructed. Airbnb says it’s a star rating, but how are they converting this into the 10 point scale, similarly, how are their constructing the overall one, which seems to be out of 100? There’s a lot of clumping around 20, 40, 60, 80, 100 - could they be averaging a five-star scale and then rebasing it to 100? Now we’ll deal with the NAs in review_scores_rating. This one is more complicated as there are a lot of them. airbnb_data_selected %&gt;% filter(is.na(review_scores_rating)) ## # A tibble: 4,676 x 23 ## host_id host_since host_response_t… host_is_superho… host_listings_c… ## &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 48239 2009-10-25 N/A FALSE 1 ## 2 545074 2011-04-29 N/A FALSE 2 ## 3 1210571 2011-09-26 N/A FALSE 1 ## 4 1408862 2011-11-15 N/A FALSE 1 ## 5 1411076 2011-11-15 N/A FALSE 1 ## 6 1409872 2011-11-15 N/A FALSE 1 ## 7 1466410 2011-12-02 within a few ho… FALSE 1 ## 8 1664812 2012-01-28 N/A FALSE 1 ## 9 1828773 2012-02-28 N/A FALSE 1 ## 10 1923052 2012-03-14 N/A FALSE 1 ## # … with 4,666 more rows, and 18 more variables: ## # host_total_listings_count &lt;dbl&gt;, host_neighbourhood &lt;chr&gt;, ## # neighbourhood_cleansed &lt;chr&gt;, room_type &lt;chr&gt;, bathrooms &lt;dbl&gt;, ## # bedrooms &lt;dbl&gt;, square_feet &lt;dbl&gt;, price &lt;int&gt;, number_of_reviews &lt;dbl&gt;, ## # has_availability &lt;lgl&gt;, review_scores_rating &lt;dbl&gt;, ## # review_scores_accuracy &lt;dbl&gt;, review_scores_cleanliness &lt;dbl&gt;, ## # review_scores_checkin &lt;dbl&gt;, review_scores_communication &lt;dbl&gt;, ## # review_scores_location &lt;dbl&gt;, review_scores_value &lt;dbl&gt;, ## # host_is_superhost_binary &lt;dbl&gt; Now see if it’s just because they don’t have any reviews. airbnb_data_selected %&gt;% filter(is.na(review_scores_rating)) %&gt;% select(number_of_reviews) %&gt;% table() ## . ## 0 1 2 3 4 ## 4389 251 26 8 2 So it’s clear that in almost all these cases they don’t have a review yet because they don’t have enough reviews. However, it’s a large proportion of the total - almost a fifth of properties don’t have any reviews (hence an NA in review_scores_rating). We can use vis_miss from the visdat package (Tierney, 2017) to make sure that all components of the review are missing. If the NAs are being driven by the Airbnb requirement of at least three reviews then we would expect they would all be missing. # We&#39;ll just check whether this is the same for all of the different variants of reviews airbnb_data_selected %&gt;% select(review_scores_rating, review_scores_accuracy, review_scores_cleanliness, review_scores_checkin, review_scores_communication, review_scores_location, review_scores_value) %&gt;% vis_miss() It looks pretty convincing that in almost all cases, all the different variants of reviews are missing. So let’s just focus on the main review. airbnb_data_selected %&gt;% filter(!is.na(review_scores_rating)) %&gt;% ggplot(aes(x = review_scores_rating)) + geom_histogram(binwidth = 1) + theme_classic() + labs(x = &quot;Average review score&quot;, y = &quot;Number of properties&quot;) It’s pretty clear that almost all the reviews are more than 80. Let’s just zoom in on that 60 to 80 range to check what the distribution looks like in that range. airbnb_data_selected %&gt;% filter(!is.na(review_scores_rating)) %&gt;% filter(review_scores_rating &gt; 60) %&gt;% filter(review_scores_rating &lt; 80) %&gt;% ggplot(aes(x = review_scores_rating)) + geom_histogram(binwidth = 1) + theme_classic() + labs(x = &quot;Average review score&quot;, y = &quot;Number of properties&quot;) 13.9.4.4 Response time Airbnb says that: Hosts have 24 hours to officially accept or decline reservation requests. You’ll be updated by email about the status of your request. More than half of all reservation requests are accepted within one hour of being received. The vast majority of hosts reply within 12 hours. If a host confirms your request, your payment is processed and collected by Airbnb in full. If a host declines your request or the request expires, we don’t process your payment. TODO: I don’t understand how you can get a response time of NA? It must be related to some other variable. Looking now at response time: table(airbnb_data_selected$host_response_time) ## ## a few days or more N/A within a day within a few hours ## 321 6324 1584 2542 ## within an hour ## 12341 Interestingly it seems like what looks like ‘NAs’ in the host_response_time variable are not being coded as proper NAs, but are instead being treated as another category. We’ll recode them to be actual NAs. airbnb_data_selected$host_response_time[airbnb_data_selected$host_response_time == &quot;N/A&quot;] &lt;- NA So here we clearly have issues with NAs. We probably want to filter them away for this example because it’s just a quick example, but there are an awful lot of them (more than 20 per cent) so we’ll have a quick look at them in relation to the review score. airbnb_data_selected %&gt;% filter(is.na(host_response_time)) %&gt;% ggplot(aes(x = review_scores_rating)) + geom_histogram(binwidth = 1) There seem to be an awful lot that have an overall review of 100. There are also an awful lot that have a review score of NA. airbnb_data_selected %&gt;% filter(is.na(host_response_time)) %&gt;% filter(is.na(review_scores_rating)) ## # A tibble: 2,313 x 23 ## host_id host_since host_response_t… host_is_superho… host_listings_c… ## &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 48239 2009-10-25 &lt;NA&gt; FALSE 1 ## 2 545074 2011-04-29 &lt;NA&gt; FALSE 2 ## 3 1210571 2011-09-26 &lt;NA&gt; FALSE 1 ## 4 1408862 2011-11-15 &lt;NA&gt; FALSE 1 ## 5 1411076 2011-11-15 &lt;NA&gt; FALSE 1 ## 6 1409872 2011-11-15 &lt;NA&gt; FALSE 1 ## 7 1664812 2012-01-28 &lt;NA&gt; FALSE 1 ## 8 1828773 2012-02-28 &lt;NA&gt; FALSE 1 ## 9 1923052 2012-03-14 &lt;NA&gt; FALSE 1 ## 10 2291374 2012-05-04 &lt;NA&gt; FALSE 1 ## # … with 2,303 more rows, and 18 more variables: ## # host_total_listings_count &lt;dbl&gt;, host_neighbourhood &lt;chr&gt;, ## # neighbourhood_cleansed &lt;chr&gt;, room_type &lt;chr&gt;, bathrooms &lt;dbl&gt;, ## # bedrooms &lt;dbl&gt;, square_feet &lt;dbl&gt;, price &lt;int&gt;, number_of_reviews &lt;dbl&gt;, ## # has_availability &lt;lgl&gt;, review_scores_rating &lt;dbl&gt;, ## # review_scores_accuracy &lt;dbl&gt;, review_scores_cleanliness &lt;dbl&gt;, ## # review_scores_checkin &lt;dbl&gt;, review_scores_communication &lt;dbl&gt;, ## # review_scores_location &lt;dbl&gt;, review_scores_value &lt;dbl&gt;, ## # host_is_superhost_binary &lt;dbl&gt; 13.9.4.5 Host number of listings There are two versions of a variable telling you how many properties a host has on Airbnb, so to start just check whether there’s a difference. airbnb_data_selected %&gt;% mutate(listings_count_is_same = if_else(host_listings_count == host_total_listings_count, 1, 0)) %&gt;% filter(listings_count_is_same == 0) ## # A tibble: 0 x 24 ## # … with 24 variables: host_id &lt;dbl&gt;, host_since &lt;date&gt;, ## # host_response_time &lt;chr&gt;, host_is_superhost &lt;lgl&gt;, ## # host_listings_count &lt;dbl&gt;, host_total_listings_count &lt;dbl&gt;, ## # host_neighbourhood &lt;chr&gt;, neighbourhood_cleansed &lt;chr&gt;, room_type &lt;chr&gt;, ## # bathrooms &lt;dbl&gt;, bedrooms &lt;dbl&gt;, square_feet &lt;dbl&gt;, price &lt;int&gt;, ## # number_of_reviews &lt;dbl&gt;, has_availability &lt;lgl&gt;, ## # review_scores_rating &lt;dbl&gt;, review_scores_accuracy &lt;dbl&gt;, ## # review_scores_cleanliness &lt;dbl&gt;, review_scores_checkin &lt;dbl&gt;, ## # review_scores_communication &lt;dbl&gt;, review_scores_location &lt;dbl&gt;, ## # review_scores_value &lt;dbl&gt;, host_is_superhost_binary &lt;dbl&gt;, ## # listings_count_is_same &lt;dbl&gt; There are none in this dataset so we can just remove one column for now and have a quick look at the other one. airbnb_data_selected &lt;- airbnb_data_selected %&gt;% select(-host_listings_count) airbnb_data_selected %&gt;% count(host_total_listings_count) ## # A tibble: 60 x 2 ## host_total_listings_count n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 1750 ## 2 1 9727 ## 3 2 3424 ## 4 3 1889 ## 5 4 1049 ## 6 5 809 ## 7 6 530 ## 8 7 353 ## 9 8 284 ## 10 9 287 ## # … with 50 more rows So there are a large number who have somewhere in the 2-10 properties range, but the usual long tail. The number with 0 listings is unexpected and worth following up on. And there are a bunch with NA that we’ll need to deal with. airbnb_data_selected %&gt;% filter(host_total_listings_count == 0) %&gt;% head() ## # A tibble: 6 x 22 ## host_id host_since host_response_t… host_is_superho… host_total_list… ## &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 140602 2010-06-08 within an hour FALSE 0 ## 2 992557 2011-08-19 a few days or m… FALSE 0 ## 3 2737251 2012-06-25 &lt;NA&gt; FALSE 0 ## 4 3367744 2012-08-25 within a day FALSE 0 ## 5 2139510 2012-04-14 &lt;NA&gt; FALSE 0 ## 6 1786233 2012-02-21 within a few ho… FALSE 0 ## # … with 17 more variables: host_neighbourhood &lt;chr&gt;, ## # neighbourhood_cleansed &lt;chr&gt;, room_type &lt;chr&gt;, bathrooms &lt;dbl&gt;, ## # bedrooms &lt;dbl&gt;, square_feet &lt;dbl&gt;, price &lt;int&gt;, number_of_reviews &lt;dbl&gt;, ## # has_availability &lt;lgl&gt;, review_scores_rating &lt;dbl&gt;, ## # review_scores_accuracy &lt;dbl&gt;, review_scores_cleanliness &lt;dbl&gt;, ## # review_scores_checkin &lt;dbl&gt;, review_scores_communication &lt;dbl&gt;, ## # review_scores_location &lt;dbl&gt;, review_scores_value &lt;dbl&gt;, ## # host_is_superhost_binary &lt;dbl&gt; There’s nothing that immediately jumps out as odd about the people with zero listings, but there must be something going on. Based on this dataset, there’s a third way of looking at the number of properties someone has and that’s to look at the number of times the unique ID occurs. airbnb_data_selected %&gt;% count(host_id) %&gt;% arrange(-n) %&gt;% head() ## # A tibble: 6 x 2 ## host_id n ## &lt;dbl&gt; &lt;int&gt; ## 1 10202618 121 ## 2 1919294 82 ## 3 152088065 53 ## 4 33238402 50 ## 5 63526506 48 ## 6 164320990 48 Again this makes it clear that there are many with multiple properties listed. 13.9.4.6 Decisions The purpose of this document is to just give a quick introduction to using real-world data, so we’ll just remove anything that is annoying, but if you’re using this for research then you’d need to justify these decisions and/or possibly make different ones. Get rid of prices more than $999. airbnb_data_filtered &lt;- airbnb_data_selected %&gt;% filter(price &lt; 1000) dim(airbnb_data_filtered) ## [1] 23310 22 Get rid of anyone with an NA for whether they are a super host. # Just remove host_is_superhost NAs for now. airbnb_data_filtered &lt;- airbnb_data_filtered %&gt;% filter(!is.na(host_is_superhost)) dim(airbnb_data_filtered) ## [1] 23025 22 Get rid of anyone with an NA in their main review score - this removes roughly 20 per cent of observations. # We&#39;ll just get rid of them for now, but this is probably something that deserves more attention - possibly in an appendix or similar. airbnb_data_filtered &lt;- airbnb_data_filtered %&gt;% filter(!is.na(review_scores_rating)) # There are still some where the rest of the reviews are missing even though there is a main review score # There seem to be an awful lot that have an overall review of 100. Does that make sense? dim(airbnb_data_filtered) ## [1] 18485 22 Get rid of anyone with a main review score less than 70. # We&#39;ll just get rid of them for now, but this is probably something that deserves more attention - possibly in an appendix or similar. airbnb_data_filtered &lt;- airbnb_data_filtered %&gt;% filter(review_scores_rating &gt; 69) # There are still some where the rest of the reviews are missing even though there is a main review score # There seem to be an awful lot that have an overall review of 100. Does that make sense? dim(airbnb_data_filtered) ## [1] 18143 22 Get rid of anyone with a NA in their response time - this removes roughly another 20 per cent of the observations. airbnb_data_filtered &lt;- airbnb_data_filtered %&gt;% filter(!is.na(host_response_time)) dim(airbnb_data_filtered) ## [1] 14165 22 (TODO: We don’t have to do this next step as we’ve already got rid of them at some other point. So there’s something systematic going on and we should come back and look into it.) Get rid of anyone with a NA in their number of properties. airbnb_data_filtered &lt;- airbnb_data_filtered %&gt;% filter(!is.na(host_total_listings_count)) dim(airbnb_data_filtered) ## [1] 14165 22 Get rid of anyone with a 100 for their review_scores_rating. airbnb_data_filtered &lt;- airbnb_data_filtered %&gt;% filter(review_scores_rating != 100) dim(airbnb_data_filtered) ## [1] 10857 22 Only keep people with one property: airbnb_data_filtered &lt;- airbnb_data_filtered %&gt;% add_count(host_id) %&gt;% filter(n == 1) %&gt;% select(-n) dim(airbnb_data_filtered) ## [1] 4880 22 13.9.5 Explore data We might like to make some graphs to see if we can any relationships jump out. Some aspects that come to mind is looking at prices and reviews and super hosts, and number of properties and neighbourhood. TODO: Come back and make this paragraph better. 13.9.5.1 Price and reviews Look at the relationship between price and reviews, and whether they are a super-host. # Look at both price and reviews airbnb_data_filtered %&gt;% ggplot(aes(x = price, y = review_scores_rating, color = host_is_superhost)) + geom_point(size = 1, alpha = 0.1) + # Make the points smaller and more transparent as they overlap considerably. theme_classic() + labs(x = &quot;Price per night&quot;, y = &quot;Average review score&quot;, color = &quot;Super host&quot;) + # Probably should recode this to more meaningful than TRUE/FALSE. scale_color_brewer(palette = &quot;Set1&quot;) 13.9.5.2 Superhost and response-time One of the aspects that may make someone a super host is how quickly they respond to enquiries. One could imagine that being a superhost involves quickly saying yes or no to enquiries. Let’s look at the data. First, we want to look at the possible values of superhost by their response times. airbnb_data_filtered %&gt;% tabyl(host_is_superhost) %&gt;% adorn_totals(&quot;row&quot;) %&gt;% adorn_pct_formatting() ## host_is_superhost n percent ## FALSE 2617 53.6% ## TRUE 2263 46.4% ## Total 4880 100.0% Fortunately, it looks like when we removed the reviews rows we removed any NAs from whether they were a super host, but if we go back and look into that we may need to check again. The tabyl function within the janitor package (Firke, 2020) would list the NAs if there were any, but in case you don’t trust it, another way of check this is to try to filter to just the NAs. airbnb_data_filtered %&gt;% filter(is.na(host_is_superhost)) ## # A tibble: 0 x 22 ## # … with 22 variables: host_id &lt;dbl&gt;, host_since &lt;date&gt;, ## # host_response_time &lt;chr&gt;, host_is_superhost &lt;lgl&gt;, ## # host_total_listings_count &lt;dbl&gt;, host_neighbourhood &lt;chr&gt;, ## # neighbourhood_cleansed &lt;chr&gt;, room_type &lt;chr&gt;, bathrooms &lt;dbl&gt;, ## # bedrooms &lt;dbl&gt;, square_feet &lt;dbl&gt;, price &lt;int&gt;, number_of_reviews &lt;dbl&gt;, ## # has_availability &lt;lgl&gt;, review_scores_rating &lt;dbl&gt;, ## # review_scores_accuracy &lt;dbl&gt;, review_scores_cleanliness &lt;dbl&gt;, ## # review_scores_checkin &lt;dbl&gt;, review_scores_communication &lt;dbl&gt;, ## # review_scores_location &lt;dbl&gt;, review_scores_value &lt;dbl&gt;, ## # host_is_superhost_binary &lt;dbl&gt; Now let’s look at the response time. airbnb_data_filtered %&gt;% tabyl(host_response_time) %&gt;% adorn_totals(&quot;row&quot;) %&gt;% adorn_pct_formatting() ## host_response_time n percent ## a few days or more 78 1.6% ## within a day 538 11.0% ## within a few hours 833 17.1% ## within an hour 3431 70.3% ## Total 4880 100.0% So a vast majority respond within an hour. Finally, we can look at the cross-tab. airbnb_data_filtered %&gt;% tabyl(host_response_time, host_is_superhost) %&gt;% adorn_percentages(&quot;row&quot;) %&gt;% adorn_pct_formatting(digits = 0) %&gt;% adorn_ns() %&gt;% adorn_title() ## host_is_superhost ## host_response_time FALSE TRUE ## a few days or more 92% (72) 8% (6) ## within a day 74% (396) 26% (142) ## within a few hours 60% (502) 40% (331) ## within an hour 48% (1647) 52% (1784) So if someone doesn’t respond within an hour then it’s unlikely that they are a super host. 13.9.5.3 Neighbourhood Finally, let’s look at neighbourhood. The data provider has attempted to clean the neighbourhood variable for us, so we’ll just use this for now. If we were doing this analysis properly, we’d need to check whether they’d made any mistakes. # We expect something in the order of 100 to 150 neighbourhoods, with the top ten accounting for a large majority of listings. airbnb_data_filtered %&gt;% tabyl(neighbourhood_cleansed) %&gt;% adorn_totals(&quot;row&quot;) %&gt;% adorn_pct_formatting() %&gt;% nrow() ## [1] 139 airbnb_data_filtered %&gt;% tabyl(neighbourhood_cleansed) %&gt;% adorn_pct_formatting() %&gt;% arrange(-n) %&gt;% filter(n &gt; 100) %&gt;% adorn_totals(&quot;row&quot;) %&gt;% head() ## neighbourhood_cleansed n percent ## Waterfront Communities-The Island 1001 20.5% ## Niagara 273 5.6% ## Church-Yonge Corridor 176 3.6% ## Annex 174 3.6% ## Dovercourt-Wallace Emerson-Junction 163 3.3% ## Bay Street Corridor 130 2.7% 13.9.6 Model data We will now run some models on our dataset. We will first split the data into test/training groups, we do this using functions from the tidymodels package (Kuhn and Wickham, 2019) (which like the tidyverse package (Wickham et al., 2019) is a package of packages). set.seed(853) airbnb_data_filtered_split &lt;- airbnb_data_filtered %&gt;% initial_split(prop = 3/4) airbnb_train &lt;- training(airbnb_data_filtered_split) airbnb_test &lt;- testing(airbnb_data_filtered_split) rm(airbnb_data_filtered_split) 13.9.6.1 Logistic regression We may like to look at whether we can forecast whether someone is a super host, and the factors that go into explaining that. As the dependent variable is binary, this is a good opportunity to look at logistic regression. We expect that better reviews will be associated with faster responses and higher reviews. Specifically, the model that we estimate is: \\[\\mbox{Prob(Is super host} = 1) = \\beta_0 + \\beta_1 \\mbox{Response time} + \\beta_2 \\mbox{Reviews} + \\epsilon\\] We estimate the model using glm in the R language (R Core Team, 2019). logistic_reg_superhost_response_review &lt;- glm(host_is_superhost ~ host_response_time + review_scores_rating, data = airbnb_train, family = binomial ) We can have a quick look at the results, for instance, the summary function. We could also use tidy or glance from the broom package (Robinson and Hayes, 2020). The details should be the same, but the broom functions are tibbles, which means that we can more easily deal with them within a tidy framework. summary(logistic_reg_superhost_response_review) ## ## Call: ## glm(formula = host_is_superhost ~ host_response_time + review_scores_rating, ## family = binomial, data = airbnb_train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9513 -0.7401 -0.0529 0.8562 4.4894 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -49.44238 1.90724 -25.924 &lt; 2e-16 *** ## host_response_timewithin a day 1.13811 0.47746 2.384 0.0171 * ## host_response_timewithin a few hours 1.99356 0.46831 4.257 2.07e-05 *** ## host_response_timewithin an hour 2.42867 0.46035 5.276 1.32e-07 *** ## review_scores_rating 0.49249 0.01905 25.851 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 5051.2 on 3659 degrees of freedom ## Residual deviance: 3583.2 on 3655 degrees of freedom ## AIC: 3593.2 ## ## Number of Fisher Scoring iterations: 6 tidy(logistic_reg_superhost_response_review) ## # A tibble: 5 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -49.4 1.91 -25.9 3.61e-148 ## 2 host_response_timewithin a day 1.14 0.477 2.38 1.71e- 2 ## 3 host_response_timewithin a few hours 1.99 0.468 4.26 2.07e- 5 ## 4 host_response_timewithin an hour 2.43 0.460 5.28 1.32e- 7 ## 5 review_scores_rating 0.492 0.0191 25.9 2.39e-147 glance(logistic_reg_superhost_response_review) ## # A tibble: 1 x 8 ## null.deviance df.null logLik AIC BIC deviance df.residual nobs ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 5051. 3659 -1792. 3593. 3624. 3583. 3655 3660 We might like to look at what our model predicts, compared with whether the person was actually a super host. We can do that in a variety of ways, but one way is to use augment from the broom package (Robinson and Hayes, 2020). This will add the prediction and associated uncertainty to the data. For every row we will then have the probability that our model is estimating that they are a superhost. But ultimately, we need a binary forecast. There are a bunch of different options, but one is to just say that if the model estimates a probability of more than 0.5 then we bin it into a superhost, and other not. airbnb_data_filtered_logistic_fit_train &lt;- augment(logistic_reg_superhost_response_review, data = airbnb_train %&gt;% select(host_is_superhost, host_is_superhost_binary, host_response_time, review_scores_rating ), type.predict = &quot;response&quot;) %&gt;% # We use the &quot;response&quot; option here so that the function does the work of worrying about the exponential and log odds for us. Our result will be a probability. select(-.hat, -.sigma, -.cooksd, -.std.resid) %&gt;% mutate(predict_host_is_superhost = if_else(.fitted &gt; 0.5, 1, 0), # How do things change if we change the 0.5 cutoff? host_is_superhost_binary = as.factor(host_is_superhost_binary), predict_host_is_superhost_binary = as.factor(predict_host_is_superhost) ) We can look at how far off the model is. There are a bunch of ways of doing this, but one is to look at what probability the model has given each person. airbnb_data_filtered_logistic_fit_train %&gt;% ggplot(aes(x = .fitted, fill = host_is_superhost_binary)) + geom_histogram(binwidth = 0.05, position = &quot;dodge&quot;) + theme_classic() + labs(x = &quot;Estimated probability that host is super host&quot;, y = &quot;Number&quot;, fill = &quot;Host is super host&quot;) + scale_fill_brewer(palette = &quot;Set1&quot;) We can look at how the model probabilities change based on average review score, and their average time to respond. ggplot(airbnb_data_filtered_logistic_fit_train, aes(x = review_scores_rating, y = .fitted, color = host_response_time)) + geom_line() + geom_point() + labs(x = &quot;Average review score&quot;, y = &quot;Predicted probability of being a superhost&quot;, color = &quot;Host response time&quot;) + theme_classic() + scale_color_brewer(palette = &quot;Set1&quot;) # What is missing from this graph? This nice thing about this graph is that it illustrates nicely the effect of a host having an average response time of, say, ‘within an hour’ compared with ‘within a few hours’. We can focus on how the model does in terms of raw classification using confusionMatrix from the caret package (Kuhn, 2020). This also gives a bunch of diagnostics (the help file explains what they are). In general, they suggest this isn’t the best model that’s ever existed. caret::confusionMatrix(data = airbnb_data_filtered_logistic_fit_train$predict_host_is_superhost_binary, reference = airbnb_data_filtered_logistic_fit_train$host_is_superhost_binary) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 1502 341 ## 1 472 1345 ## ## Accuracy : 0.7779 ## 95% CI : (0.764, 0.7912) ## No Information Rate : 0.5393 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.5555 ## ## Mcnemar&#39;s Test P-Value : 5.132e-06 ## ## Sensitivity : 0.7609 ## Specificity : 0.7977 ## Pos Pred Value : 0.8150 ## Neg Pred Value : 0.7402 ## Prevalence : 0.5393 ## Detection Rate : 0.4104 ## Detection Prevalence : 0.5036 ## Balanced Accuracy : 0.7793 ## ## &#39;Positive&#39; Class : 0 ## In any case, to this point we’ve been looking at how the model has done on the training set. It’s also relevant how it does on the test set. Again, there are a bunch of ways to do this, but one is to again use the augment function, but to include a newdata argument. airbnb_data_filtered_logistic_fit_test &lt;- augment(logistic_reg_superhost_response_review, data = airbnb_train %&gt;% select(host_is_superhost, host_is_superhost_binary, host_response_time, review_scores_rating ), newdata = airbnb_test %&gt;% select(host_is_superhost, host_is_superhost_binary, host_response_time, review_scores_rating ), # I&#39;m selecting just because the # dataset is quite wide, and so this makes it easier to look at. type.predict = &quot;response&quot;) %&gt;% mutate(predict_host_is_superhost = if_else(.fitted &gt; 0.5, 1, 0), host_is_superhost_binary = as.factor(host_is_superhost_binary), predict_host_is_superhost_binary = as.factor(predict_host_is_superhost) ) We would expect the performance to be slightly worse on the test set. But it’s actually fairly similar. caret::confusionMatrix(data = airbnb_data_filtered_logistic_fit_test$predict_host_is_superhost_binary, reference = airbnb_data_filtered_logistic_fit_test$host_is_superhost_binary) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 501 132 ## 1 142 445 ## ## Accuracy : 0.7754 ## 95% CI : (0.7509, 0.7986) ## No Information Rate : 0.527 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.5499 ## ## Mcnemar&#39;s Test P-Value : 0.5866 ## ## Sensitivity : 0.7792 ## Specificity : 0.7712 ## Pos Pred Value : 0.7915 ## Neg Pred Value : 0.7581 ## Prevalence : 0.5270 ## Detection Rate : 0.4107 ## Detection Prevalence : 0.5189 ## Balanced Accuracy : 0.7752 ## ## &#39;Positive&#39; Class : 0 ## We could compare the test with the training sets in terms of forecasts. training &lt;- airbnb_data_filtered_logistic_fit_train %&gt;% select(host_is_superhost_binary, .fitted) %&gt;% mutate(type = &quot;Training set&quot;) test &lt;- airbnb_data_filtered_logistic_fit_test %&gt;% select(host_is_superhost_binary, .fitted) %&gt;% mutate(type = &quot;Test set&quot;) both &lt;- rbind(training, test) rm(training, test) both %&gt;% ggplot(aes(x = .fitted, fill = host_is_superhost_binary)) + geom_histogram(binwidth = 0.05, position = &quot;dodge&quot;) + theme_minimal() + labs(x = &quot;Estimated probability that host is super host&quot;, y = &quot;Number&quot;, fill = &quot;Host is super host&quot;) + scale_fill_brewer(palette = &quot;Set1&quot;) + facet_wrap(vars(type), nrow = 2, scales = &quot;free_y&quot;) HINT: How many aspects mentioned in the modelling criteria in rubric for PS3 have I gone through? 13.9.7 Next steps Ideas for the future: We could look at the relationship between superhosts and time - how long does it take to become one normally? We could use price. We could use neighbourhood. 13.9.8 References R Core Team (2019). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/. David Robinson and Alex Hayes (2020). broom: Convert Statistical Analysis Objects into Tidy Tibbles. R package version 0.5.4., https://CRAN.R-project.org/package=broom Kirill Müller (2017). here: A Simpler Way to Find Your Files. R package version 0.1. https://CRAN.R-project.org/package=here. Sam Firke (2020). janitor: Simple Tools for Examining and Cleaning Dirty Data. R package version 1.2.1., https://CRAN.R-project.org/package=janitor Max Kuhn and Hadley Wickham (2019). tidymodels: Easily Install and Load the ‘Tidymodels’ Packages. R package, version 0.0.3. https://CRAN.R-project.org/package=tidymodels Wickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686, https://doi.org/10.21105/joss.01686 Elin Waring, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu and Shannon Ellis (2020). skimr: Compact and Flexible Summaries of Data. R package version 2.1. https://CRAN.R-project.org/package=skimr Tierney N (2017). “visdat: Visualising Whole Data Frames.” JOSS, 2(16), 355. doi: 10.21105/joss.00355 (URL: https://doi.org/10.21105/joss.00355), &lt;URL: http://dx.doi.org/10.21105/joss.00355&gt;. Yihui Xie (2020). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.27. Inside Airbnb (2020) Available at: http://insideairbnb.com, as accessed 19 February 2020. Max Kuhn (2020). caret: Classification and Regression Training. R package version 6.0-85. https://CRAN.R-project.org/package=caret "],["regression-essentials.html", "Chapter 14 Regression essentials 14.1 Linear regression 14.2 Classification 14.3 Count data", " Chapter 14 Regression essentials 14.1 Linear regression This chapter is about quantitatively describing a linear relationship. You’re meant to have taken an intro stats course, but will I briefly cover the basics. Then we’ll get stuck into some data and try to see how much of our understanding holds up. Required reading (You are welcome to refer to your favourite linear regression textbook instead of these.) James, Gareth, Daniela Witten, Trevor Hastie and Robert Tibshirani, 2017, An Introduction to Statistical Learning with Applications in R, Chapter 3, freely available at: http://faculty.marshall.usc.edu/gareth-james/ISL/. Wickham, Hadley, and Garrett Grolemund, 2017, R for Data Science, Chapter 23, freely available at: https://r4ds.had.co.nz/. Recommended reading Angrist, Joshua D., and Jörn-Steffen Pischke, 2008, Mostly harmless econometrics: An empiricist’s companion, Princeton University Press, Chapter 3.4.3. Cunningham, Scott, Causal Inference: The Mixtape, Chapters ‘Probability theory and statistics review’ and ‘Properties of Regression’, freely available at: http://www.scunning.com/causalinference_norap.pdf. ElHabr, Tony, 2019, ‘A Bayesian Approach to Ranking English Premier League Teams (using R)’, freely available at: https://tonyelhabr.rbind.io/post/bayesian-statistics-english-premier-league/. Greenland, Sander, Stephen J. Senn, Kenneth J. Rothman, John B. Carlin, Charles Poole, Steven N. Goodman, and Douglas G. Altman. “Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations.” European journal of epidemiology 31, no. 4 (2016): 337-350. Ioannidis, John PA, 2005, ‘Why most published research findings are false’, PLos med 2, no. 8, e124. Ronald L. Wasserstein and Nicole A. Lazar, 2016, ‘The ASA Statement on p-Values: Context, Process, and Purpose’, The American Statistician, 70:2, 129-133, DOI: 10.1080/00031305.2016.1154108. Silge, Julia, 2019, ‘Modeling salary and gender in the tech industry’, freely available at: https://juliasilge.com/blog/salary-gender/. Silge, Julia, 2019, ‘Opioid prescribing habits in Texas’, freely available at: https://juliasilge.com/blog/texas-opioids/. Silge, Julia, 2019, ‘Tidymodels’, freely available at: https://juliasilge.com/blog/intro-tidymodels/. Taddy, Matt, 2019, Business Data Science, Chapter 2. Key concepts/skills/etc Linear regression. Uncertainty. Threats to validity. Key libraries broom ggplot2 modelr tidyverse Key functions augment() glance() lm() tidy() Pre-quiz Please write a linear relationship between some response variable, Y, and some predictor, X. What is the intercept term? What is the slope term? What would adding a hat to these indicate? What is the least squares criterion? Similarly, what is RSS and what are we trying to do when we run least squares regression? What is bias? What is a confidence interval? If there were three variables: Snow, Temperature, and Wind, please write R code that would fit a simple linear regression to explain Snow as a function of Temperature and Wind. What do you think about another explanatory variable - daily stock market returns - to your model? 14.1.1 Introduction I am going to follow a combination of ISLR and R4DS. Sometimes the language that is used for specific terms differs over time and between disciplines. For instance, ISLR is written from a machine learning perspective, R4DS is written from a data science perspective, and if you come from a different discipline then the terminology may be slightly different. You’re welcome to use whatever you are comfortable with, but here I’ll follow ISLR and R4DS. If we have two variables, \\(Y\\) and \\(X\\), then we could characterise the relationship between these as: \\[Y \\sim \\beta_0 + \\beta_1 X.\\] There are two coefficients/parameters, and the intercept is \\(\\beta_0\\), the slope is \\(\\beta_1\\). We are saying that \\(Y\\) will have some value, \\(\\beta_0\\), even when \\(X\\) is 0, and that \\(Y\\) will change by \\(\\beta_1\\) units for every one unit change in \\(X\\). We may then take this relationship to the data that we have about the relationship in order to estimate these coefficients for those particular values that we have: \\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x.\\] 14.1.2 Competing relationships I want to focus on data, so we’ll make this example concrete, by generating some data and then discussing everything in the context of that. The example will be looking at someone’s time for running five kilometers, compared with their time for running a marathon. library(tidyverse) set.seed(853) number_of_observations &lt;- 100 running_data &lt;- tibble(five_km_time = rnorm(number_of_observations, 20, 3), noise = rnorm(number_of_observations, 0, 10), marathon_time = five_km_time * 8.4 + noise, was_raining = sample(c(&quot;Yes&quot;, &quot;No&quot;), size = number_of_observations, replace = TRUE, prob = c(0.2, 0.8)) ) running_data %&gt;% ggplot(aes(x = five_km_time, y = marathon_time)) + geom_point() + labs(x = &quot;Five km time (minutes)&quot;, y = &quot;Marathon time (minutes)&quot;) + theme_classic() In this set-up we may like to use \\(x\\), which is the five-kilometer time, to produce estimates of \\(y\\), which is the marathon time. This would involve also estimating values of \\(\\beta_0\\) and \\(\\beta_1\\), which is why they have a hat on them. But how should we estimate the coefficients? Even if we impose a linear relationship there are a lot of options, but here are some: # This great idea of showing some possible fits is from Wickham, Hadley, and Garrett Grolemund, 2017. set.seed(853) models &lt;- tibble( a1 = runif(750, -40, 400), a2 = runif(750, -16, 16) ) ggplot() + geom_abline(data = models, aes(intercept = a1, slope = a2), alpha = 1/4) + geom_point(data = running_data, aes(x = five_km_time, y = marathon_time)) + labs(x = &quot;five-kilometer time (minutes)&quot;, y = &quot;Marathon time (minutes)&quot;) + theme_classic() Clearly some of these fits are not all that great. One way we may define being great would be to impose that they be as close as possible to each of the \\(x\\) and \\(y\\) combinations that we know. One way is to choose the one that minimises the sum of least squares. To do this we produce our estimates of \\(\\hat{y}\\) based on some estimates of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), given the \\(x\\), and then work out how ‘wrong’, for every point \\(i\\), we were: \\[ e_i = y_i - \\hat{y}_i.\\] The residual sum of squares (RSS) then requires summing across all the points: \\[ \\mbox{RSS} = e^2_1+ e^2_2 +\\dots + e^2_n.\\] This results in one ‘linear best-fit’ line, but it is worth thinking about all of the assumptions and decisions that it took to get us to this point. running_data %&gt;% ggplot(aes(x = five_km_time, y = marathon_time)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;black&quot;, linetype = &quot;dashed&quot;) + labs(x = &quot;five-kilometer time (minutes)&quot;, y = &quot;Marathon time (minutes)&quot;) + theme_classic() With the least squares criterion we want the values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) that result in the smallest RSS. 14.1.3 Implementing this in R Within R, the main function for doing linear regression is lm. This is included in base R, so you don’t need to call any packages, but in a moment we will call a bunch of packages that will surround lm within an environment that we are more familiar with. You specify the relationship with the dependent variable first, then ~, then the independent variables. Finally, you should specify the dataset (or you could pipe to it as usual). lm(y ~ x, data = dataset) In general, you should assign this to an object: running_data_first_model &lt;- lm(marathon_time ~ five_km_time, data = running_data) To see the result of your regression you can then call summary(). summary(running_data_first_model) ## ## Call: ## lm(formula = marathon_time ~ five_km_time, data = running_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -24.763 -5.686 0.722 6.650 16.707 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.4114 6.0610 0.068 0.946 ## five_km_time 8.3617 0.3058 27.343 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.474 on 98 degrees of freedom ## Multiple R-squared: 0.8841, Adjusted R-squared: 0.8829 ## F-statistic: 747.6 on 1 and 98 DF, p-value: &lt; 2.2e-16 The first part of the result tells us the regression that we called, then information about the residuals, and the estimated coefficients. And then finally some useful diagnostics. We are considering that there is some relationship between \\(X\\) and \\(Y\\) - \\(Y = f(X) + \\epsilon\\) - and we are going to say that function is linear and so our relationship is: \\[\\hat{Y} = \\beta_0 + \\beta_1 X + \\epsilon.\\] There is some ‘true’ relationship between \\(X\\) and \\(Y\\), but we don’t know what it is. All we can do is use our sample of data to try to estimate it. But because our understanding depends on that sample, for every possible sample, we would get a slightly different relationship (as measured by the coefficients). That \\(\\epsilon\\) is a measure of our error - what does the model not know? There’s going to be plenty that the model doesn’t know, but we hope is that the error does not depend on \\(X\\). The intercept is marathon time that we would expect with a five-kilometer time of 0 minutes. Hopefully this example illustrates the need to carefully interpret the intercept coefficient! The coefficient on five-kilometer run time shows how we expect the marathon time to change if five-kilometer run time changed by one unit. In this case it’s about 8.4, which makes sense seeing as a marathon is roughly that many times longer than a five-kilometer run. 14.1.4 Tidy up with broom While there is nothing wrong with the base approach, I want to introduce the broom package because that will provide us with outputs in a tidy framework. There are three key functions: tidy: Gives the coefficient estimates in a tidy output. glance: Gives the diagnostics. augment: Adds the forecasted values, and hence, residuals, to your dataset. library(broom) tidy(running_data_first_model) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.411 6.06 0.0679 9.46e- 1 ## 2 five_km_time 8.36 0.306 27.3 1.17e-47 glance(running_data_first_model) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.884 0.883 8.47 748. 1.17e-47 1 -355. 715. 723. ## # … with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; Notice how the results are fairly similar to the base summary function. running_data &lt;- augment(running_data_first_model, data = running_data) We could now make plots of the residuals. ggplot(running_data, aes(x = .resid)) + geom_histogram(binwidth = 1) + theme_classic() + labs(y = &quot;Number of occurrences&quot;, x = &quot;Residuals&quot;) ggplot(running_data, aes(five_km_time, .resid)) + geom_point() + geom_hline(yintercept = 0, linetype = &quot;dotted&quot;, color = &quot;grey&quot;) + theme_classic() + labs(y = &quot;Residuals&quot;, x = &quot;five-kilometer time (minutes)&quot;) When we say our estimate is unbiased we are trying to say that even though with some sample our estimate might be too high, and with another sample our estimate might be too low, eventually if we have a lot of data then our estimate would be the same as the population. (A pro hockey player may sometimes shoot right of the net, and sometimes left of the net, but we’d hope that on average they’d be right in the middle of the net ;)). TODO: Add my favourite graph But we want to try to speak to the ‘true’ relationship, so we need to try to capture how much we think our understanding depends on the particular sample that we have to analyse. And this is where standard error comes in. It tells us how off our estimate is compared with the actual. From standard errors, we can compute a confidence interval. A 95 per cent confidence interval means that there is a 0.95 probability that the interval happens to contain the population parameter (which is typically unknown). running_data %&gt;% ggplot(aes(x = five_km_time, y = marathon_time)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = TRUE, color = &quot;black&quot;, linetype = &quot;dashed&quot;) + labs(x = &quot;five-kilometer time (minutes)&quot;, y = &quot;Marathon time (minutes)&quot;) + theme_classic() 14.1.5 Testing hypothesis Now that we have an interval for which we can say there is a 95 per cent probability it contains the true population parameter we can test claims. For instance, a null hypothesis that there is no relationship between \\(X\\) and \\(Y\\) (i.e. \\(\\beta_1 = 0\\)), compared with an alternative hypothesis that there is some relationship between \\(X\\) and \\(Y\\) (i.e. \\(\\beta_1 \\neq 0\\)). We need to know whether our estimate of \\(\\beta_1\\), which is \\(\\hat{\\beta}_1\\), is ‘far enough’ away from zero for us to be comfortable claiming that \\(\\beta_1 \\neq 0\\). How far is ‘far enough’? If we were very confident in our estimate of \\(\\beta_1\\) then it wouldn’t have to be far, but if we were not then it would have to be substantial. So it depends on a bunch of things, but essentially the standard error of \\(\\hat{\\beta}_1\\). We compare this standard error with \\(\\hat{\\beta}_1\\) to get the t-statistic: \\[t = \\frac{\\hat{\\beta}_1 - 0}{\\mbox{SE}(\\hat{\\beta}_1)}.\\] And we then compare our t-statistic to the t-distribution to compute the probability of getting this absolute t-statistic or a larger one, if \\(\\beta_1 = 0\\). This is the p-value. A small p-value means it is unlikely that we would observe our association due to chance if there wasn’t a relationship. 14.1.6 Adding more and varied explanatory variables To this point we’ve just considered one explanatory variable. But we’ll usually have more than one. One approach would be to run separate regressions for each explanatory variable. But compared with separate linear regressions for each, adding more explanatory variables allows us to have a better understanding of the intercept and accounts for interaction. Often the results will be quite different. This slightly counterintuitive result is very common in many real life situations. Consider an absurd example to illustrate the point. Running a regression of shark attacks versus ice cream sales for data collected at a given beach community over a period of time would show a positive relationship, similar to that seen between sales and newspapers. Of course no one (yet) has suggested that ice creams should be banned at beaches to reduce shark attacks. In reality, higher temperatures cause more people to visit the beach, which in turn results in more ice cream sales and mores hark attacks. A multiple regression of attacks versus ice cream sales and temperature reveals that, as intuition implies, the former predictor is no longer significant after adjusting for temperature. James, Gareth, Daniela Witten, Trevor Hastie and Robert Tibshirani, 2017, p. 74. We may also like to consider variables that do not have an inherent ordering. For instance, pregnant or not. When there are only two options then we can use a binary variable which is 0 or 1. If there are more than two levels then use a combination of binary variables, where the ‘missing’ outcome (baseline) gets pushed onto the intercept. In other languages you may need to explicitly construct dummy variables, but as R was designed as a language to do statistical programming, it does a lot of the work here for you and is fairly forgiving. For instance, if you have a column of character values that only had two values: c(\"Monica\", \"Rohan\", \"Rohan\", \"Monica\", \"Monica\", \"Rohan\"), and you used this as a independent variable in your usual regression set up then R would treat it as a dummy variable. running_data_rain_model &lt;- lm(marathon_time ~ five_km_time + was_raining, data = running_data) summary(running_data_rain_model) The result probably isn’t too surprising if we look at a plot of the data. running_data %&gt;% ggplot(aes(x = five_km_time, y = marathon_time, color = was_raining)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;black&quot;, linetype = &quot;dashed&quot;) + labs(x = &quot;five-kilometer time (minutes)&quot;, y = &quot;Marathon time (minutes)&quot;, color = &quot;Was raining&quot;) + theme_classic() + scale_color_brewer(palette = &quot;Set1&quot;) In addition to wanting to include additional explanatory variables we may think that they are related with one another. For instance, if we were wanting to explain the amount of snowfall in Toronto, then we may be interested in the humidity and the temperature, but those two variables may also interact. We can do this by using * instead of + when we specify the model in R. If you do interact variables, then you should almost always also include the individual variables as well (Figure 14.1). Figure 14.1: Don’t leave out the main effects in an interactive model Source: By Kai Arzheimer, 16 February 2020. 14.1.7 Threats to validity and aspects to think about There are a variety of weaknesses and aspects that you should discuss when you use linear regression. A quick list includes (James, Gareth, Daniela Witten, Trevor Hastie and Robert Tibshirani, 2017, p. 92): Non-linearity of the response-predictor relationships. Correlation of error terms. Non-constant variance of error terms. Outliers. High-leverage points. Collinearity These are also aspects that you should discuss if you use linear regression. Including plots tends to be handy here to illustrate your points. Other aspects that you may consider discussing include (James, Gareth, Daniela Witten, Trevor Hastie and Robert Tibshirani, 2017, p. 75): Is at least one of the predictors \\(X_1, X_2, \\dots, X_p\\) useful in predicting the response? Do all the predictors help to explain \\(Y\\), or is only a subset of the predictors useful? How well does the model fit the data? Given a set of predictor values, what response value should we predict, and how accurate is our prediction? 14.1.8 More credible outputs Finally, after creating beautiful graphs and tables you may want your regression output to look just as nice. There are a variety of packages in R that will automatically format your regression outputs. You should try huxtable. # library(huxtable) # huxreg(running_data_first_model) 14.2 Classification asdf 14.3 Count data 14.3.1 Logistic regression asdf 14.3.2 Poisson regression asdf "],["difference-in-differences.html", "Chapter 15 Difference in differences 15.1 Introduction 15.2 Matching and difference-in-differences 15.3 Case study - Lower advertising revenue reduced French newspaper prices between 1960 and 1974 15.4 Tutorial - Propensity score matching - Lalonde", " Chapter 15 Difference in differences TODO: Replace the arm matching with https://kosukeimai.github.io/MatchIt/index.html Required reading Angelucci, Charles, and Julia Cagé, 2019, ‘Newspapers in times of low advertising revenues’, American Economic Journal: Microeconomics, vol. 11, no. 3, pp. 319-364, DOI: 10.1257/mic.20170306, available at: https://www.aeaweb.org/articles?id=10.1257/mic.20170306. Gertler, Paul, Sebastian Martinez, Patrick Premand, Laura Rawlings, and Christel Vermeersch, ‘Impact Evaluation in Practice’, Chapters 7 and 8. Gelman, Andrew, Jennifer Hill and Aki Vehtari, 2020, Regression and Other Stories, Cambridge University Press, Chs 18 - 21. McElreath, Richard, 2020, Statistical Rethinking, 2nd Edition, CRC Press, Ch 14. Wong, Jeffrey, and Colin McFarland, 2020, ‘Computational Causal Inference at Netflix’, Netflix Technology Blog, 11 Aug, https://netflixtechblog.com/computational-causal-inference-at-netflix-293591691c62 Required viewing Gelman, Andrew, 2020 ‘100 Stories of Causal Inference’, 4 August, https://www.youtube.com/watch?v=jnI5KI843Lk. King, Gary, 2020, ‘Research Designs’, Lectures on Quantitative Social Science Methods 1, freely available: https://youtu.be/27grU_VM5Ps. Kuriwaki, Shiro, 2020, ‘Difference-in-Differences Estimation in R (parts 1 and 2)’, 18 April, https://vimeo.com/409267138 and https://vimeo.com/409267190. Recommended reading Alexander, Monica, Polimis, Kivan, and Zagheni, Emilio, 2019,’ The impact of Hurricane Maria on out-migration from Puerto Rico: Evidence from Facebook data’, Population and Development Review. (Example of using diff-in-diff to measure the effect of Hurricane Maria.) Alexander, Rohan, and Zachary Ward, 2018, ‘Age at arrival and assimilation during the age of mass migration’, The Journal of Economic History, 78, no. 3, 904-937. (Example where I used differences between brothers to estimate the effect of education.) Angrist, Joshua D., and Jörn-Steffen Pischke, 2008, Mostly harmless econometrics: An empiricist’s companion, Princeton University Press, Chapters 3.3.2 and 5. Austin, Peter C., 2011, ‘An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies’, Multivariate Behavioral Research, vol. 46, no. 3, pp.399-424. (Broad overview of propensity score matching, with a nice discussion of the comparison to randomised controlled trials.) Baker, Andrew, 2019, ‘Difference-in-Differences Methodology’, 25 September, https://andrewcbaker.netlify.app/2019/09/25/difference-in-differences-methodology/. Cunningham, Scott, Causal Inference: The Mixtape, chapters ‘Matching and subclassifications’ and ‘Differences-in-differences’, http://www.scunning.com/causalinference_norap.pdf. (Very well-written notes on diff-in-diff.) Gelman, Andrew, and Jennifer Hill, 2007, Data Analysis Using Regression and Muiltilevel/Hierarchical Models, Chapter 10, pp. 207-212. King, Gary, and Richard Nielsen, 2019, ‘Why Propensity Scores Should Not Be Used for Matching’, Political Analysis. (Academic paper on the limits of propensity score matching. Propensity score matching was a big thing in the 90s but everyone knew about these weaknesses and so it died off. Lately, there has been a resurgence because of the CS/ML folks using it without thinking so King and Nielsen wrote a nice paper about the flaws. I mean, you can’t say you weren’t warned.) Saeed, Sahar, Erica E. M. Moodie, Erin C. Strumpf, Marina B. Klein, 2019, ‘Evaluating the impact of health policies: using a difference-in-differences approach’, International Journal of Public Health, 64, pp. 637–642, https://doi.org/10.1007/s00038-018-1195-2. Taddy, Matt, 2019, Business Data Science, Chapter 5. (Some brief notes on diff-in-diff that may appeal to some students.) Tang, John, 2015, ‘Pollution havens and the trade in toxic chemicals: evidence from U.S. trade flows’, Ecological Economics, vol. 112, pp. 150-160. (Example of using diff-in-diff to estimate pollution.) Valencia Caicedo, Felipe. ‘The mission: Human capital transmission, economic persistence, and culture in South America.’ The Quarterly Journal of Economics 134.1 (2019): 507-556. (Data available at: Valencia Caicedo, Felipe, 2018, “Replication Data for: ‘The Mission: Human Capital Transmission, Economic Persistence, and Culture in South America’”, https://doi.org/10.7910/DVN/ML1155, Harvard Dataverse, V1.). Key concepts/skills/etc Difference-in-differences Essential matching methods. Weaknesses of matching. Difference-in-differences. Key libraries broom tidyverse Key functions/etc tidy() lm() Pre-quiz Sharla Gelfand has been ‘(s)haring two #rstats functions most days - one I know and love, and one that’s new to me!’. Please go to Sharla’s GitHub page: https://github.com/sharlagelfand/twofunctionsmostdays. Please find a package that she mentions that you have never used. Please find the relevant website for the package. Please describe what the package does and a context in which it could be useful to you. Sharla Gelfand has been ‘(s)haring two #rstats functions most days - one I know and love, and one that’s new to me!’. Please go to Sharla’s GitHub page: https://github.com/sharlagelfand/twofunctionsmostdays. Please find a function that she mentions that you have never used. Please look at the help file for that function. Please detail the arguments of the function, and a context in which it could be useful to you. What is propensity score matching? If you were matching people, then what are some of the features that you would like to match on? What sort of ethical questions does collecting and storing such information raise for you? Putting to one side, the ethical issues, what are some statistical weaknesses with propensity score matching? What is the key assumption when using diff-in-diff? Please read the fascinating article in The Markup about car insurance algorithms: https://themarkup.org/allstates-algorithm/2020/02/25/car-insurance-suckers-list. Please read the article and tell me what you think. You may wish to focus on ethical, legal, social, statistical, or other, aspects. Please go to the GitHub page related to the fascinating article in The Markup about car insurance algorithms: https://github.com/the-markup/investigation-allstates-algorithm. What is great about their work? What could be improved? 15.1 Introduction Life it grand when you can conduct experiments to be able to speak to causality. But what if you can only run the survey - you can’t run an experiment? Here we begin our discussion of the circumstances and methods that would allow you to nonetheless speak to causality. We use (relatively) simple methods, in sophisticated, well-developed, ways (cf, much of what is done these days) and our applied statistics draw from a variety of social sciences including economics, and political science. 15.2 Matching and difference-in-differences 15.2.1 Introduction The ideal situation, as described in the previous chapter, is rarely possible in a data science setting. Can we really reasonably expect that Netflix would allow us to change prices. And even if they did once, would they let us do it again, and again, and again? Further, rarely can we explicitly create treatment and control groups. Finally, experiments are really expensive and potentially unethical. Instead, we need to make do with what we have. Rather than our counterfactual coming to us through randomisation, and hence us knowing that the two are the same but for the treatment, we try to identify groups that were similar before the treatment, and hence any differences can be attributed to the treatment. In practice, we tend to even have differences between our two groups before we treat. Provided those pre-treatment differences satisfy some assumptions (basically that they were consistent, and we expect that consistency to continue in the absence of the treatment) – the ‘parallel trends’ assumption – then we can look to any difference in the differences as the effect of the treatment. One of the lovely aspects of difference in differences analysis is that we can do it using fairly straight-forward quantitative methods - linear regression with a dummy variable is all that is needed to do a convincing job. 15.2.2 Motivation Consider us wanting to know the effect of a new tennis racket on serve speed. One way to test this would be to measure the difference between Roger Federer’s serve speed without the tennis racket and mine with the tennis racket. Sure, we’d find a difference but how do we know how much to attribute to the tennis racket? Another way would be to consider the difference between my serve speed without the tennis racket and my serve speed with the tennis racket. But what if serves were just getting faster naturally over time? Instead, let’s combine the two to look at the difference in the differences! In this world we measure Federer’s serve and compare it to my serve without the new racket. We then measure Federer’s serve again and measure my serve with the new racket. That difference in the differences would then be our estimate of the effect of the new racket. What sorts of assumptions jump out at you that we are going to have to make in order for this analysis to be appropriate? Is there something else that may have affected only me, and not Roger that could affect my serve speed? Probably. Is it likely that Roger Federer and I have the same trajectory of serve speed improvement? Probably not. This is the ‘parallel trends’ assumption, and it dominates any discussion of difference in differences analysis. Finally, is it likely that the variance of our serve speeds is the same? Probably not. Why might this be powerful? We don’t need the treatment and control group to be the same before the treatment. We just need to have a good idea of how they differ. 15.2.3 Simulated example Let’s generate some data. library(broom) library(tidyverse) set.seed(853) diff_in_diff_example_data &lt;- tibble(person = rep(c(1:1000), times = 2), time = c(rep(0, times = 1000), rep(1, times = 1000)), treatment_group = rep(sample(x = 0:1, size = 1000, replace = TRUE), times = 2) ) # We want to make the outcome slightly more likely if they were treated than if not. diff_in_diff_example_data &lt;- diff_in_diff_example_data %&gt;% rowwise() %&gt;% mutate(serve_speed = case_when( time == 0 &amp; treatment_group == 0 ~ rnorm(n = 1, mean = 5, sd = 1), time == 1 &amp; treatment_group == 0 ~ rnorm(n = 1, mean = 6, sd = 1), time == 0 &amp; treatment_group == 1 ~ rnorm(n = 1, mean = 8, sd = 1), time == 1 &amp; treatment_group == 1 ~ rnorm(n = 1, mean = 14, sd = 1), ) ) head(diff_in_diff_example_data) ## # A tibble: 6 x 4 ## # Rowwise: ## person time treatment_group serve_speed ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 0 0 4.43 ## 2 2 0 1 6.96 ## 3 3 0 1 7.77 ## 4 4 0 0 5.31 ## 5 5 0 0 4.09 ## 6 6 0 0 4.85 Let’s make a graph. diff_in_diff_example_data$treatment_group &lt;- as.factor(diff_in_diff_example_data$treatment_group) diff_in_diff_example_data$time &lt;- as.factor(diff_in_diff_example_data$time) diff_in_diff_example_data %&gt;% ggplot(aes(x = time, y = serve_speed, color = treatment_group)) + geom_point() + geom_line(aes(group = person), alpha = 0.2) + theme_minimal() + labs(x = &quot;Time period&quot;, y = &quot;Serve speed&quot;, color = &quot;Person got a new racket&quot;) + scale_color_brewer(palette = &quot;Set1&quot;) As it is a simple example, we could do this manually, by getting the average difference of the differences. average_differences &lt;- diff_in_diff_example_data %&gt;% pivot_wider(names_from = time, values_from = serve_speed, names_prefix = &quot;time_&quot;) %&gt;% mutate(difference = time_1 - time_0) %&gt;% group_by(treatment_group) %&gt;% summarise(average_difference = mean(difference)) average_differences$average_difference[2] - average_differences$average_difference[1] ## [1] 5.058414 Let’s use OLS to do the same analysis. The general regression equation is: \\[Y_{i,t} = \\beta_0 + \\beta_1\\mbox{Treatment group dummy}_i + \\beta_2\\mbox{Time dummy}_t + \\beta_3(\\mbox{Treatment group dummy} \\times\\mbox{Time dummy})_{i,t} + \\epsilon_{i,t}\\] If we use * in the regression then it automatically includes the separate aspects as well as their interaction. It’s the estimate of \\(\\beta_3\\) which is of interest. diff_in_diff_example_regression &lt;- lm(serve_speed ~ treatment_group*time, data = diff_in_diff_example_data) tidy(diff_in_diff_example_regression) ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 4.97 0.0428 116. 0. ## 2 treatment_group1 3.03 0.0622 48.7 0. ## 3 time1 1.01 0.0605 16.6 2.97e-58 ## 4 treatment_group1:time1 5.06 0.0880 57.5 0. Fortunately, our estimates are the same! 15.2.4 Assumptions If we want to use difference in differences, then we need to satisfy the assumptions. There were three that were touched on earlier, but here I want to focus on one: the ‘parallel trends’ assumption. The parallel trends assumption haunts everything to do with diff-in-diff analysis because we can never prove it, we can just be convinced of it. To see why we can never prove it, consider an example in which we want to know the effect of a new stadium on a professional sports team’s wins/loses. To do this we consider two teams: the Warriors and the Raptors. The Warriors changed stadiums at the start of the 2019-20 season (the Raptors did not), so we will consider four time periods: the 2016-17 season, 2017-18 season, 2018-19 season, and finally we will compare the performance with the one after they moved, so in the 2019-20 season. The Raptors here act as our counterfactual. This means that we assume the relationship between the Warriors and the Raptors, in the absence of a new stadium, would have continued to change in a consistent way. But we can never know that for certain. We have to present sufficient evidence to assuage any concerns that a reader may have. For a variety of reasons, it is worth having tougher than normal requirements around the evidence it would take to convince you of an effect. There are four main ‘threats to validity’ when you are using difference in differences and you should address all of these (Cunningham, 2020, pp. 272–277): Non-parallel trends. The treatment and control groups may be based on differences. As such it can be difficult to convincingly argue for parallel trends. In this case, maybe try to find another factor to consider in your model that may adjust for some of that. This may require difference in difference in differences (in the earlier example, perhaps could add in the San Francisco 49ers as they are in the same broad geographic area as the Warriors). Or maybe re-think your analysis to see if you can make a different control group. Adding additional earlier time periods may help but may introduce more issues (see third point). Compositional differences. This is a concern when working with repeated cross-sections. What if the composition of those cross-sections change? For instance, if we work at Tik Tok or some other app that is rapidly growing and want to look at the effect of some change. In our initial cross-section, we may have mostly young people, but in a subsequent cross-section, we may have more older people as the demographics of the app usage change. Hence our results may just be an age-effect, not an effect of the change that we are interested in. Long-term effects vs. reliability. As we discussed in the last chapter, there is a trade-off between the length of the analysis that we run. As we run the analysis for longer there is more opportunity for other factors to affect the results. There is also increased chance for someone who was not treated to be treated. But, on the other hand, it can be difficult to convincingly argue that short-term results will continue in the long-term. Functional form dependence. This is less of an issue when the outcomes are similar, but if they are different then functional form may be responsible for some aspects of the results. 15.2.5 Matching This section draws on material from Gelman and Hill, 2007, pp. 207-212. Difference in differences is a powerful analysis framework. After I learnt about it I began to see opportunities to implement it everywhere. But it can be tough to identify appropriate treatment and control groups. In Alexander and Ward, 2018, we compare migrant brothers - one of whom had most of their education in a different country, and the other who had most of their education in the US. Is this really the best match? We may be able to match based on observable variables. For instance, age-group or education. At two different times we compare smoking rates in 18-year-olds in one city with smoking rates in 18-year-olds in another city. That is fine, but it is fairly coarse. We know that there are differences between 18-year-olds, even in terms of the variables that we commonly observe, say gender and education. One way to deal with this may be to create sub-groups: 18-year-old males with a high school education, etc. But the sample sizes are likely to quickly become small. How do we deal with continuous variables? And also, is the difference between an 18-year-old and a 19-year-old really so different? Shouldn’t we also compare with them? One way to proceed is to consider a nearest neighbour approach. But there is limited concern for uncertainty in this approach. There is also an issue if you have a large number of variables because you end up with a high-dimension graph. This leads us to propensity score matching. Propensity score matching involves assigning some probability to each observation. We construct that probability based on the observation’s values for the independent variables, at their values before the treatment. That probability is our best guess at the probability of the observation being treated, regardless of whether it was treated or not. For instance, if 18-year-old males were treated but 19-year-old males were not, then as there is not much difference between 18-year-old males and 19-year-old males our assigned probability would be fairly similar. We can then compare the outcomes of observations with similar propensity scores. One advantage of propensity score matching is that is allows us to easily consider many independent variables at once, and it can be constructed using logistic regression. Let’s generate some data to illustrate propensity score matching. Let’s pretend that we work for Amazon. We are going to treat some individuals with free-shipping to see what happens to their average purchase. library(tidyverse) sample_size &lt;- 10000 set.seed(853) amazon_purchase_data &lt;- tibble( unique_person_id = c(1:sample_size), age = runif(n = sample_size, min = 18, max = 100), city = sample( x = c(&quot;Toronto&quot;, &quot;Montreal&quot;, &quot;Calgary&quot;), size = sample_size, replace = TRUE ), gender = sample( x = c(&quot;Female&quot;, &quot;Male&quot;, &quot;Other/decline&quot;), size = sample_size, replace = TRUE, prob = c(0.49, 0.47, 0.02) ), income = rlnorm(n = sample_size, meanlog = 0.5, sdlog = 1) ) Now we need to add some probability of being treated with free shipping, which depends on our variables. Younger, higher-income, male and in Toronto all make it slightly more likely. amazon_purchase_data &lt;- amazon_purchase_data %&gt;% mutate(age_num = case_when( age &lt; 30 ~ 3, age &lt; 50 ~ 2, age &lt; 70 ~ 1, TRUE ~ 0), city_num = case_when( city == &quot;Toronto&quot; ~ 3, city == &quot;Montreal&quot; ~ 2, city == &quot;Calgary&quot; ~ 1, TRUE ~ 0), gender_num = case_when( gender == &quot;Male&quot; ~ 3, gender == &quot;Female&quot; ~ 2, gender == &quot;Other/decline&quot; ~ 1, TRUE ~ 0), income_num = case_when( income &gt; 3 ~ 3, income &gt; 2 ~ 2, income &gt; 1 ~ 1, TRUE ~ 0) ) %&gt;% rowwise() %&gt;% mutate(sum_num = sum(age_num, city_num, gender_num, income_num), softmax_prob = exp(sum_num)/exp(12), free_shipping = sample( x = c(0:1), size = 1, replace = TRUE, prob = c(1-softmax_prob, softmax_prob) ) ) %&gt;% ungroup() amazon_purchase_data &lt;- amazon_purchase_data %&gt;% dplyr::select(-age_num, -city_num, -gender_num, -income_num, -sum_num, -softmax_prob) Finally, we need to have some measure of a person’s average spend. We want those with free shipping to be slightly higher than those without. amazon_purchase_data &lt;- amazon_purchase_data %&gt;% mutate(mean_spend = if_else(free_shipping == 1, 60, 50)) %&gt;% rowwise() %&gt;% mutate(average_spend = rnorm(1, mean_spend, sd = 5) ) %&gt;% ungroup() %&gt;% dplyr::select(-mean_spend) # Fix the class on some amazon_purchase_data &lt;- amazon_purchase_data %&gt;% mutate_at(vars(city, gender, free_shipping), ~as.factor(.)) # Change some to factors table(amazon_purchase_data$free_shipping) ## ## 0 1 ## 9629 371 head(amazon_purchase_data) ## # A tibble: 6 x 7 ## unique_person_id age city gender income free_shipping average_spend ## &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1 47.5 Calgary Female 1.72 0 41.1 ## 2 2 27.8 Montreal Male 1.54 0 55.7 ## 3 3 57.7 Toronto Female 3.16 0 56.5 ## 4 4 43.9 Toronto Male 0.636 0 50.5 ## 5 5 21.1 Toronto Female 1.43 0 44.7 ## 6 6 51.1 Calgary Male 1.18 0 48.8 Now we construct a logistic regression model that ‘explains’ whether a person was treated as a function of the variables that we think explain it. propensity_score &lt;- glm(free_shipping ~ age + city + gender + income, family = binomial, data = amazon_purchase_data) We will now add our forecast to our dataset. amazon_purchase_data &lt;- augment(propensity_score, data = amazon_purchase_data, type.predict = &quot;response&quot;) %&gt;% dplyr::select(-.resid, -.std.resid, -.hat, -.sigma, -.cooksd) Now we use our forecast to create matches. There are a variety of ways to do this. In a moment I’ll step through some code that does it all at once, but as this is a worked example and we only have a small number of possibilities, we can just do it manually. For every person who was actually treated (given free shipping) we want the untreated person who was considered as similar to them (based on propensity score) as possible. amazon_purchase_data &lt;- amazon_purchase_data %&gt;% arrange(.fitted, free_shipping) Here we’re going to use a matching function from the arm package. This finds which is the closest of the ones that were not treated, to each one that was treated. amazon_purchase_data$treated &lt;- if_else(amazon_purchase_data$free_shipping == 0, 0, 1) amazon_purchase_data$treated &lt;- as.integer(amazon_purchase_data$treated) matches &lt;- arm::matching(z = amazon_purchase_data$treated, score = amazon_purchase_data$.fitted) amazon_purchase_data &lt;- cbind(amazon_purchase_data, matches) Now we reduce the dataset to just those that are matched. We had 371 treated, so we expect a dataset of 742 observations. amazon_purchase_data_matched &lt;- amazon_purchase_data %&gt;% filter(match.ind != 0) %&gt;% dplyr::select(-match.ind, -pairs, -treated) head(amazon_purchase_data_matched) ## unique_person_id age city gender income free_shipping ## 1 5710 81.15636 Montreal Female 0.67505625 0 ## 2 9458 97.04859 Montreal Female 9.49752179 1 ## 3 6428 83.21262 Calgary Male 0.05851482 0 ## 4 2022 98.97504 Montreal Male 1.66683768 1 ## 5 9824 64.61936 Calgary Female 3.35263989 1 ## 6 1272 97.09546 Toronto Female 0.71813784 0 ## average_spend .fitted cnts ## 1 47.36258 0.001375987 1 ## 2 61.15317 0.001376161 1 ## 3 49.90080 0.001560150 1 ## 4 57.75673 0.001560418 1 ## 5 64.69709 0.002207195 1 ## 6 56.64754 0.002207514 1 Finally, we can examine the ‘effect’ of being treated on average spend in the ‘usual’ way. propensity_score_regression &lt;- lm(average_spend ~ age + city + gender + income + free_shipping, data = amazon_purchase_data_matched) huxtable::huxreg(propensity_score_regression) Table 15.1: (1) (Intercept)49.694 *** (0.809)&nbsp;&nbsp;&nbsp; age0.005&nbsp;&nbsp;&nbsp;&nbsp; (0.011)&nbsp;&nbsp;&nbsp; cityMontreal0.169&nbsp;&nbsp;&nbsp;&nbsp; (0.734)&nbsp;&nbsp;&nbsp; cityToronto0.652&nbsp;&nbsp;&nbsp;&nbsp; (0.623)&nbsp;&nbsp;&nbsp; genderMale-0.968 *&nbsp;&nbsp; (0.422)&nbsp;&nbsp;&nbsp; genderOther/decline-1.973&nbsp;&nbsp;&nbsp;&nbsp; (2.621)&nbsp;&nbsp;&nbsp; income0.009&nbsp;&nbsp;&nbsp;&nbsp; (0.021)&nbsp;&nbsp;&nbsp; free_shipping110.488 *** (0.380)&nbsp;&nbsp;&nbsp; N742&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.513&nbsp;&nbsp;&nbsp;&nbsp; logLik-2267.486&nbsp;&nbsp;&nbsp;&nbsp; AIC4552.971&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. I cover propensity score matching here because it is widely used. Hence, you need to know how to use it. People would think it’s weird if you didn’t, in the same way that we have to cover ANOVA people would think it’s weird if we had an entire experimental design course and didn’t cover it even though there are more modern ways of looking at differences between two means. But at the same time you need to know that there are flaws with propensity score matching. I will now discuss some of them. Matching. Propensity score matching cannot match on unobserved variables. This may be fine in a class-room setting, but in more realistic settings it will likely cause issues. Modelling. The results tend to be specific to the model that is used. King and Nielsen, 2019, discuss this thoroughly. Statistically. We are using the data twice. 15.3 Case study - Lower advertising revenue reduced French newspaper prices between 1960 and 1974 Required reading Angelucci, Charles, and Julia Cagé, 2019, ‘Newspapers in times of low advertising revenues’, American Economic Journal: Microeconomics, vol. 11, no. 3, pp. 319-364, DOI: 10.1257/mic.20170306, available at: https://www.aeaweb.org/articles?id=10.1257/mic.20170306. Key concepts/skills/etc Reading in foreign data. Difference in differences. Replicating work. Displaying multiple regression results. Discussing results. Key libraries haven huxtable scales tidyverse Key functions/etc dollar_format() hux_reg() lm() mutate_at() read_dta() 15.3.1 Introduction In this case study we introduce Angelucci and Cagé, 2019, and replicate its main findings. Angelucci and Cagé, 2019, is a paper in which difference in differences is used to examine the effect of the reduction in advertising revenues on newspapers’ content and prices. They create a dataset of ‘French newspapers between 1960 and 1974’. They ‘perform a difference-in-differences analysis’ and exploit ‘the introduction of advertising on television’ as this change ‘affected national newspapers more severely than local ones’. They ‘find robust evidence of a decrease in the amount of journalistic-intensive content produced and the subscription price.’ In order to conduct this analysis we will use the dataset that they provide alongside their paper. This dataset is available at: https://www.openicpsr.org/openicpsr/project/116438/version/V1/view. It is available for you to download after registration. As their dataset is in Stata data format, we will use the haven package to read it in (Wickham and Miller, 2019). library(here) library(haven) library(huxtable) library(scales) library(tidyverse) 15.3.2 Background Newspapers are in trouble. We can probably all think of a local newspaper that has closed recently because of pressure brought on by the internet. But this issue isn’t new. When television started, there were similar concerns. In this paper, Angelucci and Cagé use the introduction of television advertising in France, announced in 1967, to examine the effect of decreased advertising revenue on newspapers. The reason this is important is because it allows us to disentangle a few competing effects. For instance, are newspapers becoming redundant because they can no longer charge high prices for their ads or because consumers prefer to get their news in other ways? Are fewer journalists needed because smartphones and other technology mean they can be more productive? Angelucci and Cagé look at advertising revenue and a few other features, when a new advertising platform arrives, in this case television advertising. 15.3.3 Data (The) dataset contains annual data on local and national newspapers between 1960 and 1974, as well as detailed information on television content. In 1967, the French government announced it would relax long-standing regulations that prohibited television advertising. We provide evidence that this reform can be plausibly interpreted as an exogenous and negative shock to the advertising side of the newspaper industry… [I]t is likely that the introduction of television advertising constituted a direct shock to the advertising side of the newspaper industry and only an indirect shock to the reader side… (O)ur empirical setting constitutes a unique opportunity to isolate the consequences of a decrease in newspapers’ advertising revenues on their choices regarding the size of their newsroom, the amount of information to produce, and the prices they charge to both sides of the market. The authors’ argue that national newspapers were affected by the television advertising change, but local newspapers were not. So the national newspapers are the treatment group and the local newspapers are the control group. The dataset can be read in using read_dta(), which is a function within the haven package for reading in Stata dta files. This is equivalent to read_csv(). newspapers &lt;- read_dta(here::here(&quot;inputs/data/116438-V1/data/dta/Angelucci_Cage_AEJMicro_dataset.dta&quot;)) dim(newspapers) ## [1] 1196 52 There are 1,196 observations in the dataset and 52 variables. The authors are interested in the 1960-1974 time period which has around 100 newspapers. There are 14 national newspapers at the beginning of the period and 12 at the end. We just want to replicate their main results, so we don’t need all their variables. As such we will just select() the ones that we are interested in and change the class() where needed. newspapers &lt;- newspapers %&gt;% dplyr::select(year, id_news, after_national, local, national, # Diff in diff variables ra_cst, qtotal, ads_p4_cst, ads_s, # Advertising side dependents ps_cst, po_cst, qtotal, qs_s, rs_cst) %&gt;% #Reader side dependents mutate(ra_cst_div_qtotal = ra_cst / qtotal) %&gt;% # An advertising side dependents needs to be built mutate_at(vars(id_news, after_national, local, national), ~as.factor(.)) %&gt;% # Change some to factors mutate(year = as.integer(year)) We can now have a look at the main variables of interest for both national (Figure 15.1) and local daily newspapers (Figure 15.2). Figure 15.1: Angelucci and Cagé, 2019, summary statistics: national daily newspapers Source: Angelucci and Cagé, 2019, p. 333. Figure 15.2: Angelucci and Cagé, 2019, summary statistics: local daily newspapers Source: Angelucci and Cagé, 2019, p. 334. Please read this section of their paper to see how they describe their dataset. We are interested in the change from 1967 onward. newspapers %&gt;% mutate(type = if_else(local == 1, &quot;Local&quot;, &quot;National&quot;)) %&gt;% ggplot(aes(x = year, y = ra_cst)) + geom_point(alpha = 0.5) + scale_y_continuous(labels = dollar_format(prefix=&quot;$&quot;, suffix = &quot;M&quot;, scale = 0.000001)) + labs(x = &quot;Year&quot;, y = &quot;Advertising revenue&quot;) + facet_wrap(vars(type), nrow = 2) + theme_classic() + geom_vline(xintercept = 1966.5, linetype = &quot;dashed&quot;) 15.3.4 Model The model that we are interested in estimating is: \\[\\mbox{ln}(y_{n,t}) = \\beta_0 + \\beta_1(\\mbox{National dummy}\\times\\mbox{1967 onward dummy}) + \\lambda_n + \\gamma_y + \\epsilon.\\] The \\(\\lambda_n\\) is a fixed effect for each newspaper, and the \\(\\gamma_y\\) is a fixed effect for each year. We just use regular linear regression, with a few different dependent variables. It is the \\(\\beta_1\\) coefficient that we are interested in. 15.3.5 Results We can run the models using lm(). # Advertising side ad_revenue &lt;- lm(log(ra_cst) ~ after_national + id_news + year, data = newspapers) ad_revenue_div_circulation &lt;- lm(log(ra_cst_div_qtotal) ~ after_national + id_news + year, data = newspapers) ad_price &lt;- lm(log(ads_p4_cst) ~ after_national + id_news + year, data = newspapers) ad_space &lt;- lm(log(ads_s) ~ after_national + id_news + year, data = newspapers) # Consumer side subscription_price &lt;- lm(log(ps_cst) ~ after_national + id_news + year, data = newspapers) unit_price &lt;- lm(log(po_cst) ~ after_national + id_news + year, data = newspapers) circulation &lt;- lm(log(qtotal) ~ after_national + id_news + year, data = newspapers) share_of_sub &lt;- lm(log(qs_s) ~ after_national + id_news + year, data = newspapers) revenue_from_sales &lt;- lm(log(rs_cst) ~ after_national + id_news + year, data = newspapers) Looking at the advertising-side variables. omit_me &lt;- c(&quot;(Intercept)&quot;, &quot;id_news3&quot;, &quot;id_news6&quot;, &quot;id_news7&quot;, &quot;id_news13&quot;, &quot;id_news16&quot;, &quot;id_news25&quot;, &quot;id_news28&quot;, &quot;id_news34&quot;, &quot;id_news38&quot;, &quot;id_news44&quot;, &quot;id_news48&quot;, &quot;id_news51&quot;, &quot;id_news53&quot;, &quot;id_news54&quot;, &quot;id_news57&quot;, &quot;id_news60&quot;, &quot;id_news62&quot;, &quot;id_news66&quot;, &quot;id_news67&quot;, &quot;id_news70&quot;, &quot;id_news71&quot;, &quot;id_news72&quot;, &quot;id_news80&quot;, &quot;id_news82&quot;, &quot;id_news88&quot;, &quot;id_news95&quot;, &quot;id_news97&quot;, &quot;id_news98&quot;, &quot;id_news103&quot;, &quot;id_news105&quot;, &quot;id_news106&quot;, &quot;id_news118&quot;, &quot;id_news119&quot;, &quot;id_news127&quot;, &quot;id_news136&quot;, &quot;id_news138&quot;, &quot;id_news148&quot;, &quot;id_news151&quot;, &quot;id_news153&quot;, &quot;id_news154&quot;, &quot;id_news157&quot;, &quot;id_news158&quot;, &quot;id_news161&quot;, &quot;id_news163&quot;, &quot;id_news167&quot;, &quot;id_news169&quot;, &quot;id_news179&quot;, &quot;id_news184&quot;, &quot;id_news185&quot;, &quot;id_news187&quot;, &quot;id_news196&quot;, &quot;id_news206&quot;, &quot;id_news210&quot;, &quot;id_news212&quot;, &quot;id_news213&quot;, &quot;id_news224&quot;, &quot;id_news225&quot;, &quot;id_news234&quot;, &quot;id_news236&quot;, &quot;id_news245&quot;, &quot;id_news247&quot;, &quot;id_news310&quot;, &quot;id_news452&quot;, &quot;id_news467&quot;, &quot;id_news469&quot;, &quot;id_news480&quot;, &quot;id_news20040&quot;, &quot;id_news20345&quot;, &quot;id_news20346&quot;, &quot;id_news20347&quot;, &quot;id_news20352&quot;, &quot;id_news20354&quot;, &quot;id_news21006&quot;, &quot;id_news21025&quot;, &quot;id_news21173&quot;, &quot;id_news21176&quot;, &quot;id_news33718&quot;, &quot;id_news34689&quot;, &quot;id_news73&quot;) huxreg(&quot;Ad. rev.&quot; = ad_revenue, &quot;Ad rev. div. circ.&quot; = ad_revenue_div_circulation, &quot;Ad price&quot; = ad_price, &quot;Ad space&quot; = ad_space, omit_coefs = omit_me, number_format = 2 ) Table 15.2: Ad. rev.Ad rev. div. circ.Ad priceAd space after_national1-0.23 ***-0.15 ***-0.31 ***0.01&nbsp;&nbsp;&nbsp;&nbsp; (0.03)&nbsp;&nbsp;&nbsp;(0.03)&nbsp;&nbsp;&nbsp;(0.07)&nbsp;&nbsp;&nbsp;(0.05)&nbsp;&nbsp;&nbsp; year0.05 ***0.04 ***0.04 ***0.02 *** (0.00)&nbsp;&nbsp;&nbsp;(0.00)&nbsp;&nbsp;&nbsp;(0.00)&nbsp;&nbsp;&nbsp;(0.00)&nbsp;&nbsp;&nbsp; N1052&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1048&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;809&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1046&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.99&nbsp;&nbsp;&nbsp;&nbsp;0.90&nbsp;&nbsp;&nbsp;&nbsp;0.89&nbsp;&nbsp;&nbsp;&nbsp;0.72&nbsp;&nbsp;&nbsp;&nbsp; logLik345.34&nbsp;&nbsp;&nbsp;&nbsp;449.52&nbsp;&nbsp;&nbsp;&nbsp;-277.71&nbsp;&nbsp;&nbsp;&nbsp;-164.01&nbsp;&nbsp;&nbsp;&nbsp; AIC-526.68&nbsp;&nbsp;&nbsp;&nbsp;-735.05&nbsp;&nbsp;&nbsp;&nbsp;705.43&nbsp;&nbsp;&nbsp;&nbsp;478.02&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. Similarly, we can look at the reader-side variables. omit_me &lt;- c(&quot;(Intercept)&quot;, &quot;id_news3&quot;, &quot;id_news6&quot;, &quot;id_news7&quot;, &quot;id_news13&quot;, &quot;id_news16&quot;, &quot;id_news25&quot;, &quot;id_news28&quot;, &quot;id_news34&quot;, &quot;id_news38&quot;, &quot;id_news44&quot;, &quot;id_news48&quot;, &quot;id_news51&quot;, &quot;id_news53&quot;, &quot;id_news54&quot;, &quot;id_news57&quot;, &quot;id_news60&quot;, &quot;id_news62&quot;, &quot;id_news66&quot;, &quot;id_news67&quot;, &quot;id_news70&quot;, &quot;id_news71&quot;, &quot;id_news72&quot;, &quot;id_news80&quot;, &quot;id_news82&quot;, &quot;id_news88&quot;, &quot;id_news95&quot;, &quot;id_news97&quot;, &quot;id_news98&quot;, &quot;id_news103&quot;, &quot;id_news105&quot;, &quot;id_news106&quot;, &quot;id_news118&quot;, &quot;id_news119&quot;, &quot;id_news127&quot;, &quot;id_news136&quot;, &quot;id_news138&quot;, &quot;id_news148&quot;, &quot;id_news151&quot;, &quot;id_news153&quot;, &quot;id_news154&quot;, &quot;id_news157&quot;, &quot;id_news158&quot;, &quot;id_news161&quot;, &quot;id_news163&quot;, &quot;id_news167&quot;, &quot;id_news169&quot;, &quot;id_news179&quot;, &quot;id_news184&quot;, &quot;id_news185&quot;, &quot;id_news187&quot;, &quot;id_news196&quot;, &quot;id_news206&quot;, &quot;id_news210&quot;, &quot;id_news212&quot;, &quot;id_news213&quot;, &quot;id_news224&quot;, &quot;id_news225&quot;, &quot;id_news234&quot;, &quot;id_news236&quot;, &quot;id_news245&quot;, &quot;id_news247&quot;, &quot;id_news310&quot;, &quot;id_news452&quot;, &quot;id_news467&quot;, &quot;id_news469&quot;, &quot;id_news480&quot;, &quot;id_news20040&quot;, &quot;id_news20345&quot;, &quot;id_news20346&quot;, &quot;id_news20347&quot;, &quot;id_news20352&quot;, &quot;id_news20354&quot;, &quot;id_news21006&quot;, &quot;id_news21025&quot;, &quot;id_news21173&quot;, &quot;id_news21176&quot;, &quot;id_news33718&quot;, &quot;id_news34689&quot;, &quot;id_news73&quot;) huxreg(&quot;Subscription price&quot; = subscription_price, &quot;Unit price&quot; = unit_price, &quot;Circulation&quot; = circulation, &quot;Share of sub&quot; = share_of_sub, &quot;Revenue from sales&quot; = revenue_from_sales, omit_coefs = omit_me, number_format = 2 ) Table 15.3: Subscription priceUnit priceCirculationShare of subRevenue from sales after_national1-0.04 *&nbsp;&nbsp;0.06 **&nbsp;-0.06 **&nbsp;0.19 ***-0.06 *&nbsp;&nbsp; (0.02)&nbsp;&nbsp;&nbsp;(0.02)&nbsp;&nbsp;&nbsp;(0.02)&nbsp;&nbsp;&nbsp;(0.03)&nbsp;&nbsp;&nbsp;(0.03)&nbsp;&nbsp;&nbsp; year0.05 ***0.05 ***0.01 ***-0.01 ***0.05 *** (0.00)&nbsp;&nbsp;&nbsp;(0.00)&nbsp;&nbsp;&nbsp;(0.00)&nbsp;&nbsp;&nbsp;(0.00)&nbsp;&nbsp;&nbsp;(0.00)&nbsp;&nbsp;&nbsp; N1044&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1063&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1070&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1072&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1046&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.88&nbsp;&nbsp;&nbsp;&nbsp;0.87&nbsp;&nbsp;&nbsp;&nbsp;0.99&nbsp;&nbsp;&nbsp;&nbsp;0.97&nbsp;&nbsp;&nbsp;&nbsp;0.99&nbsp;&nbsp;&nbsp;&nbsp; logLik882.14&nbsp;&nbsp;&nbsp;&nbsp;907.28&nbsp;&nbsp;&nbsp;&nbsp;759.57&nbsp;&nbsp;&nbsp;&nbsp;321.91&nbsp;&nbsp;&nbsp;&nbsp;451.11&nbsp;&nbsp;&nbsp;&nbsp; AIC-1600.28&nbsp;&nbsp;&nbsp;&nbsp;-1650.57&nbsp;&nbsp;&nbsp;&nbsp;-1355.15&nbsp;&nbsp;&nbsp;&nbsp;-477.81&nbsp;&nbsp;&nbsp;&nbsp;-738.22&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. 15.3.6 Other points We certainly find that in many cases there appears to be a difference from 1967 onward. In general, we are able to obtain results that are similar to Angelucci and Cagé, 2019. If we spent more time, we could probably replicate their findings perfectly. Isn’t this great! What else could do? Parallel trends: Notice the wonderful way in which they test the ‘parallel trends’ assumption on pp. 350-351. Discussion: Look at their wonderful discussion (pp. 353-358) of interpretation, external validity, and robustness. 15.4 Tutorial - Propensity score matching - Lalonde http://sekhon.berkeley.edu/matching/lalonde.html "],["instrumental-variables.html", "Chapter 16 Instrumental variables 16.1 Introduction 16.2 History 16.3 Simulated example 16.4 Implementation 16.5 Assumptions 16.6 Example - Effect of Police on Crime 16.7 Conclusion", " Chapter 16 Instrumental variables Required reading Gertler, Paul, Sebastian Martinez, Patrick Premand, Laura Rawlings, and Christel Vermeersch, ‘Impact Evaluation in Practice’, Chapter 5. Required viewing Kuriwaki, Shiro, 2020, ‘Instrumental variables in R’, 11 April, freely available at: https://vimeo.com/406629459. Recommended reading Angrist, Joshua D., and Jörn-Steffen Pischke, 2008, Mostly harmless econometrics: An empiricist’s companion, Princeton University Press, Chapter 4. Cunningham, Scott, ‘Causal Inference: The Mixtape’, Chapter ‘Instrumental variables’, freely available at: http://www.scunning.com/causalinference_norap.pdf. Grogger, Jeffrey, Andreas Steinmayr, Joachim Winter, 2020, ‘The Wage Penalty of Regional Accents’, NBER Working Paper No. 26719. Taddy, Matt, 2019, Business Data Science, Chapter 5, pp. 152-162. Key concepts/skills/etc Identifying opportunities for instrumental variables. Implementing instrumental variables. Challenges to the validity of instrumental variables. Key libraries estimatr tidyverse Key functions/etc iv_robust() Pre-quiz What is an instrumental variable? What are some circumstances in which instrumental variables might be useful? What conditions must instrumental variables satisfy? Who were some of the early instrumental variable authors? Can you please think of and explain an application of instrumental variables in your own life? 16.1 Introduction Instrumental variables (IV) is an approach that can be handy when we have some type of treatment and control going on, but we have a lot of correlation with other variables and we possibly don’t have a variable that actually measures what we are interested in. So adjusting for observables will not be enough to create a good estimate. Instead we find some variable - the eponymous instrumental variable - that is: correlated with the treatment variable, but not correlated with the outcome. This solves our problem because the only way the instrumental variable can have an effect is through the treatment variable, and so we are able to adjust our understanding of the effect of the treatment variable appropriately. The trade-off is that instrumental variables must satisfy a bunch of different assumptions, and that, frankly, they are difficult to identify ex ante. Nonetheless, when you are able to use them they are a powerful tool for speaking about causality. The canonical instrumental variables example is smoking. These days we know that smoking causes cancer. But because smoking is correlated with a lot of other variables, for instance, education, it could be that it was actually education that causes cancer. RCTs may be possible, but they are likely to be troublesome in terms of speed and ethics, and so instead we look for some other variable that is correlated with smoking, but not, in and of itself, with lung cancer. In this case, we look to tax rates, and other policy responses, on cigarettes. As the tax rates on cigarettes are correlated with the number of cigarettes that are smoked, but not correlated with lung cancer, other than through their impact on cigarette smoking, through them we can assess the effect of cigarettes smoked on lung cancer. To implement instrumental variables we first regress tax rates on cigarette smoking to get some coefficient on the instrumental variable, and then (in a separate regression) regress tax rates on lung cancer to again get some coefficient on the instrumental variable. Our estimate is then the ratio of these coefficients. (Gelman and Hill 2007, 219) describe this ratio as the ‘Wald estimate’. Following the language of (Gelman and Hill 2007, 216) when we use instrumental variables we make a variety of assumptions including: Ignorability of the instrument. Correlation between the instrumental variable and the treatment variable. Monotonicity. Exclusion restriction. To summarise exactly what instrumental variables is about, I cannot do better than recommend the first few pages of the ‘Instrumental Variables’ chapter in Cunningham (2020), and this key paragraph in particular (by way of background, Cunningham has explained why it would have been impossible to randomly allocate ‘clean’ and ‘dirty’ water through a randomised controlled trial and then continues…): Snow would need a way to trick the data such that the allocation of clean and dirty water to people was not associated with the other determinants of cholera mortality, such as hygiene and poverty. He just would need for someone or something to be making this treatment assignment for him. Fortunately for Snow, and the rest of London, that someone or something existed. In the London of the 1800s, there were many different water companies serving different areas of the city. Some were served by more than one company. Several took their water from the Thames, which was heavily polluted by sewage. The service areas of such companies had much higher rates of cholera. The Chelsea water company was an exception, but it had an exceptionally good filtration system. That’s when Snow had a major insight. In 1849, Lambeth water company moved the intake point upstream along the Thames, above the main sewage discharge point, giving its customers purer water. Southwark and Vauxhall water company, on the other hand, left their intake point downstream from where the sewage discharged. Insofar as the kinds of people that each company serviced were approximately the same, then comparing the cholera rates between the two houses could be the experiment that Snow so desperately needed to test his hypothesis. 16.2 History The history of instrumental variables is a rare statistical mystery, and Stock and Trebbi (2003) provide a brief overview. The method was first published in Wright (1928). This is a book about the effect of tariffs on animal and vegetable oil. So why might instrumental variables be important in a book about tariffs on animal and vegetable oil? The fundamental problem is that the effect of tariffs depends on both supply and demand. But we only know prices and quantities, so we don’t know what is driving the effect. We can use instrumental variables to pin down causality. Where is gets interesting, and becomes something of a mystery, is that the instrumental variables discussion is only in Appendix B. If you made a major statistical break-through would you hide it in an appendix? Further, Philip G. Wright, the book’s author, had a son Sewall Wright, who had considerable expertise in statistics and the specific method used in Appendix B. Hence the mystery of Appendix B - did Philip or Sewall write it? Both Cunningham (2020) and Stock and Trebbi (2003) go into more detail, but on balance feel that it is likely that Philip did actually author the work. 16.3 Simulated example Let’s generate some data. We will explore a simulation related to the canonical example of health status, smoking, and tax rates. So we are looking to explain how healthy someone is based on the amount they smoke, via the tax rate on smoking. We are going to generate different tax rates by provinces. My understanding is that the tax rate on cigarettes is now pretty much the same in each of the provinces, but that this is fairly recent. So we’ll pretend that Alberta had a low tax, and Nova Scotia had a high tax. As a reminder, we are simulating data for illustrative purposes, so we need to impose the answer that we want. When you actually use instrumental variables you will be reversing the process. library(broom) library(tidyverse) set.seed(853) number_of_observation &lt;- 10000 iv_example_data &lt;- tibble(person = c(1:number_of_observation), smoker = sample(x = c(0:1), size = number_of_observation, replace = TRUE) ) Now we need to relate the number of cigarettes that someone smoked to their health. We’ll model health status as a draw from the normal distribution, with either a high or low mean depending on whether the person smokes. iv_example_data &lt;- iv_example_data %&gt;% mutate(health = if_else(smoker == 0, rnorm(n = n(), mean = 1, sd = 1), rnorm(n = n(), mean = 0, sd = 1) ) ) # So health will be one standard deviation higher for people who don&#39;t or barely smoke. Now we need a relationship between cigarettes and the province (because in this illustration, the provinces have different tax rates). iv_example_data &lt;- iv_example_data %&gt;% rowwise() %&gt;% mutate(province = case_when(smoker == 0 ~ sample(x = c(&quot;Nova Scotia&quot;, &quot;Alberta&quot;), size = 1, replace = FALSE, prob = c(1/2, 1/2)), smoker == 1 ~ sample(x = c(&quot;Nova Scotia&quot;, &quot;Alberta&quot;), size = 1, replace = FALSE, prob = c(1/4, 3/4)))) %&gt;% ungroup() iv_example_data &lt;- iv_example_data %&gt;% mutate(tax = case_when(province == &quot;Alberta&quot; ~ 0.3, province == &quot;Nova Scotia&quot; ~ 0.5, TRUE ~ 9999999 ) ) iv_example_data$tax %&gt;% table() ## . ## 0.3 0.5 ## 6206 3794 head(iv_example_data) Table 16.1: personsmokerhealthprovincetax 101.11&nbsp;&nbsp;Alberta0.3 21-0.0831Alberta0.3 31-0.0363Alberta0.3 402.48&nbsp;&nbsp;Alberta0.3 500.617&nbsp;Alberta0.3 600.748&nbsp;Nova Scotia0.5 Now we can look at our data. iv_example_data %&gt;% mutate(smoker = as_factor(smoker)) %&gt;% ggplot(aes(x = health, fill = smoker)) + geom_histogram(position = &quot;dodge&quot;, binwidth = 0.2) + theme_minimal() + labs(x = &quot;Health rating&quot;, y = &quot;Number of people&quot;, fill = &quot;Smoker&quot;) + scale_fill_brewer(palette = &quot;Set1&quot;) + facet_wrap(vars(province)) Finally, we can use the tax rate as an instrumental variable to estimate the effect of smoking on health. health_on_tax &lt;- lm(health ~ tax, data = iv_example_data) smoker_on_tax &lt;- lm(smoker ~ tax, data = iv_example_data) coef(health_on_tax)[&quot;tax&quot;] / coef(smoker_on_tax)[&quot;tax&quot;] ## tax ## -0.8554502 So we find, luckily, that if you smoke then your health is likely to be worse than if you don’t smoke. Equivalently, we can think of instrumental variables in a two-stage regression context. first_stage &lt;- lm(smoker ~ tax, data = iv_example_data) health_hat &lt;- first_stage$fitted.values second_stage &lt;- lm(health ~ health_hat, data = iv_example_data) summary(second_stage) ## ## Call: ## lm(formula = health ~ health_hat, data = iv_example_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.9867 -0.7600 0.0068 0.7709 4.3293 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.91632 0.04479 20.46 &lt;2e-16 *** ## health_hat -0.85545 0.08911 -9.60 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.112 on 9998 degrees of freedom ## Multiple R-squared: 0.009134, Adjusted R-squared: 0.009034 ## F-statistic: 92.16 on 1 and 9998 DF, p-value: &lt; 2.2e-16 16.4 Implementation As with regression discontinuity, although it is possible to use existing functions, it might be worth looking at specialised packages. Instrumental variables has a few moving pieces, so a specialised package can help keep everything organised, and additionally, standard errors need to be adjusted and specialised packages make this easier. The package estimatr is a recommendation, although there are others available and you should try those if you are interested. The estimatr package is from the same team as DeclareDesign. Let’s look at our example using iv_robust(). library(estimatr) iv_robust(health ~ smoker | tax, data = iv_example_data) %&gt;% summary() ## ## Call: ## iv_robust(formula = health ~ smoker | tax, data = iv_example_data) ## ## Standard error type: HC2 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper DF ## (Intercept) 0.9163 0.04057 22.59 3.163e-110 0.8368 0.9958 9998 ## smoker -0.8555 0.08047 -10.63 2.981e-26 -1.0132 -0.6977 9998 ## ## Multiple R-squared: 0.1971 , Adjusted R-squared: 0.197 ## F-statistic: 113 on 1 and 9998 DF, p-value: &lt; 2.2e-16 16.5 Assumptions As discussed earlier, there are a variety of assumptions that are made when using instrumental variables. The two most important are: Exclusion Restriction. This assumption is that the instrumental variable only affects the dependent variable through the independent variable of interest. Relevance. There must actually be a relationship between the instrumental variable and the independent variable. There is typically a trade-off between these two. There are plenty of variables that When thinking about potential instrumental variables Cunningham (2020), p. 211, puts it brilliantly: But, let’s say you think you do have a good instrument. How might you defend it as such to someone else? A necessary but not a sufficient condition for having an instrument that can satisfy the exclusion restriction is if people are confused when you tell them about the instrument’s relationship to the outcome. Let me explain. No one is going to be confused when you tell them that you think family size will reduce female labor supply. They don’t need a Becker model to convince them that women who have more children probably work less than those with fewer children. It’s common sense. But, what would they think if you told them that mothers whose first two children were the same gender worked less than those whose children had a balanced sex ratio? They would probably give you a confused look. What does the gender composition of your children have to do with whether a woman works? It doesn’t – it only matters, in fact, if people whose first two children are the same gender decide to have a third child. Which brings us back to the original point – people buy that family size can cause women to work less, but they’re confused when you say that women work less when their first two kids are the same gender. But if when you point out to them that the two children’s gender induces people to have larger families than they would have otherwise, the person “gets it”, then you might have an excellent instrument. Relevance can be tested using regression and other tests for correlation. The exclusion restriction cannot be tested. You need to present evidence and convincing arguments. As Cunningham (2020) p. 225 says ‘Instruments have a certain ridiculousness to them[.] That is, you know you have a good instrument if the instrument itself doesn’t seem relevant for explaining the outcome of interest because that’s what the exclusion restriction implies.’ 16.6 Example - Effect of Police on Crime 16.6.1 Overview Here we’ll use an example of Levitt (2002) that looks at the effect of police on crime. This is interesting because you might think, that more police is associated with lower crime. But, it could actually be the opposite, if more crime causes more police to be hired - how many police would a hypothetical country with no crime need? Hence there is a need to find some sort of instrumental variable that affects crime only through its relationship with the number of police (that is, not in and of itself, related to crime), and yet is also correlated with police numbers. Levitt (2002) suggests the number of firefighters in a city. Levitt (2002) argues that firefighters are appropriate as an instrument, because ‘(f)actors such as the power of public sector unions, citizen tastes for government services, affirmative action initiatives, or a mayor’s desire to provide spoils might all be expected to jointly influence the number of firefighters and police.’. Levitt (2002) also argues that the relevance assumption is met by showing that ‘changes in the number of police officers and firefighters within a city are highly correlated over time’. In terms of satisfying the exclusion restriction, Levitt (2002) argues that the number of firefighters should not have a ‘direct impact on crime.’ However, it may be that there are common factors, and so Levitt (2002) adjusts for this in the regression. 16.6.2 Data The dataset is based on 122 US cities between 1975 and 1995. Summary statistics are provided in Figure 16.1. Figure 16.1: Summary statistics for Levitt 2002. Source: Levitt (2002) p. 1,246. 16.6.3 Model In the first stage Levitt (2002) looks at police as a function of firefighters, and a bunch of adjustment variables: \\[\\ln(\\mbox{Police}_{ct}) = \\gamma \\ln(\\mbox{Fire}_{ct}) + X&#39;_{ct}\\Gamma + \\lambda_t + \\phi_c + \\epsilon_{ct}.\\] The important part of this is the police and firefighters numbers which are on a per capita basis. There are a bunch of adjustment variables in \\(X\\) which includes things like state prisoners per capita, the unemployment rate, etc, as well as year dummy variables and fixed-effects for each city. Having established the relationship between police and firefights, Levitt (2002) can then use the estimates of the number of police, based on the number of firefighters, to explain crime rates: \\[\\Delta\\ln(\\mbox{Crime}_{ct}) = \\beta_1 \\ln(\\mbox{Police}_{ct-1}) + X&#39;_{ct}\\Gamma + \\Theta_c + \\mu_{ct}.\\] The typical way to present instrumental variable results is to show both stages. Figure 16.2 shows the relationship between police and firefighters. Figure 16.2: The relationship between firefighters, police and crime. Source: Levitt (2002) p. 1,247. And then Figure 16.3 shows the relationship between police and crime, where is it the IV results that are the ones of interest. Figure 16.3: The impact of police on crime. Source: Levitt (2002) p. 1,248. 16.6.4 Discussion The key finding of Levitt (2002) is that there is a negative effect of the number of police on the amount of crime. There are a variety of points that I want to raise in regard to this paper. They will come across as a little negative, but this is mostly just because this a paper from 2002, that I am reading today, and so the standards have changed. It’s fairly remarkable how reliant on various model specifications the results are. The results bounce around a fair bit and that’s just the ones that are reported. Chances are there are a bunch of other results that were not reported, but it would be of interest to see their impact. On that note, there is fairly limited model validation. This is probably something that I am more aware of these days, but it seems likely that there is a fair degree of over-fitting here. Levitt (2002) is actually a response, after another researcher, McCrary (2002), found some issues with the original paper: Levitt (1997). While Levitt appears quite decent about it, it is jarring to see that Levitt was thanked by McCrary (2002) for providing ‘both the data and computer code.’ What if Levitt had not been decent about providing the data and code? Or what if the code was unintelligible? In some ways it is nice to see how far that we have come - the author of a similar paper these days would be forced to make their code and data available as part of the paper, we wouldn’t have to ask them for it. But it reinforces the importance of open data and reproducible science. 16.7 Conclusion Instrumental variables is a useful approach because one can obtain causal estimates even without explicit randomisation. Finding instrumental variables used to be a bit of a white whale, especially in academia. However, I will leave the final (and hopefully motivating) word to Taddy (2019), p. 162: As a final point on the importance of IV models and analysis, note that when you are on the inside of a firm—especially on the inside of a modern technology firm—explicitly randomised instruments are everywhere…. But it is often the case that decision-makers want to understand the effects of policies that are not themselves randomised but are rather downstream of the things being AB tested. For example, suppose an algorithm is used to predict the creditworthiness of potential borrowers and assign loans. Even if the process of loan assignment is never itself randomised, if the parameters in the machine learning algorithms used to score credit are AB tested, then those experiments can be used as instruments for the loan assignment treatment. Such ‘upstream randomisation’ is extremely common and IV analysis is your key tool for doing causal inference in that setting.’ References "],["regression-discontinuity-design.html", "Chapter 17 Regression discontinuity design 17.1 Introduction 17.2 Simulated example 17.3 Overlap 17.4 Examples 17.5 Implementation 17.6 Fuzzy RDD 17.7 Threats to validity 17.8 Weaknesses 17.9 Case study - Stiers, Hooghe, and Dassonneville, 2020 17.10 Case study - Caughey, and Sekhon., 2011", " Chapter 17 Regression discontinuity design Required reading Better Evaluation, ‘Regression Discontinuity’, https://www.betterevaluation.org/en/evaluation-options/regressiondiscontinuity Eggers, Andrew C., Anthony Fowler, Jens Hainmueller, Andrew B. Hall, and James M. Snyder Jr, 2015, ‘On the validity of the regression discontinuity design for estimating electoral effects: New evidence from over 40,000 close races’, American Journal of Political Science, 59 (1), pp. 259-274 Gelman, Andrew, 2019, ‘Another Regression Discontinuity Disaster and what can we learn from it’, 25 June, https://statmodeling.stat.columbia.edu/2019/06/25/another-regression-discontinuity-disaster-and-what-can-we-learn-from-it/. Gertler, Paul, Sebastian Martinez, Patrick Premand, Laura Rawlings, and Christel Vermeersch, ‘Impact Evaluation in Practice’, Chapter 6. Sekhon, Jasjeet and Rocio Titiunik, 2016, ‘Understanding Regression Discontinuity Designs As Observational Studies’, Observational Studies 2 (2016) 174-182, http://sekhon.berkeley.edu/papers/SekhonTitiunik2016-OS.pdf. Required viewing Kuriwaki, Shiro, 2020, ‘Regression Discontinuity in R (parts 1 and 2)’, 25 March, freely available at: https://vimeo.com/400826628 and https://vimeo.com/400826660. Recommended reading Angrist, Joshua D., and Jörn-Steffen Pischke, 2008, Mostly harmless econometrics: An empiricist’s companion, Princeton University Press, Chapter 6. Coppock, Alenxader, and Donald P. Green, 2016, ‘Is Voting Habit Forming? New Evidence from Experiments and Regression Discontinuities’, American Journal of Political Science, Volume 60, Issue 4, pp. 1044-1062, available at: https://onlinelibrary.wiley.com/doi/abs/10.1111/ajps.12210. (Has code and data.) Cunningham, Scott, ‘Causal Inference: The Mixtape’, chapter ‘Regression discontinuity’, freely available at: http://www.scunning.com/causalinference_norap.pdf. Dell, Melissa, Pablo Querubin, 2018, ‘Nation Building Through Foreign Intervention: Evidence from Discontinuities in Military Strategies’, The Quarterly Journal of Economics, Volume 133, Issue 2, pp. 701–764, https://doi.org/10.1093/qje/qjx037. Evans, David, 2013, ‘Regression Discontinuity Porn’, World Bank Blogs, 16 November, freely available at: https://blogs.worldbank.org/impactevaluations/regression-discontinuity-porn. Imai, Kosuke, 2017, Quantitative Social Science: An Introduction, Princeton University Press, Ch 2.5. Gelman, Andrew, and Jennifer Hill, 2007, Data Analysis Using Regression and Multilevel/Hierarchical Models, Chapter 10, pp. 212-215. Gelman, Andrew, and Guido Imbens, 2019, “Why high-order polynomials should not be used in regression discontinuity designs”, Journal of Business &amp; Economic Statistics, 37, pp. 447-456. Gelman, Andrew, 2019, ‘Another Regression Discontinuity Disaster and what can we learn from it’, Statistical Modeling, Causal Inference, and Social Science, 25 June, freely available at: https://statmodeling.stat.columbia.edu/2019/06/25/another-regression-discontinuity-disaster-and-what-can-we-learn-from-it/. Harris, Rich, Mlacki Migliozzi and Niraj Chokshi, ‘13,000 Missing Flights: The Global Consequences of the Coronavirus’, New York Times, 21 February 2020. freely available here (if you make an account): https://www.nytimes.com/interactive/2020/02/21/business/coronavirus-airline-travel.html. Imbens, Guido W., and Thomas Lemieux, 2008, ‘Regression discontinuity designs: A guide to practice’, Journal of Econometrics, vol. 142, no. 2, pp. 615-635. Myllyvirta, Lauri, 2020, ‘Analysis: Coronavirus has temporarily reduced China’s CO2 emissions by a quarter’, Carbon Brief, 19 February, freely available at: https://www.carbonbrief.org/analysis-coronavirus-has-temporarily-reduced-chinas-co2-emissions-by-a-quarter. Taddy, Matt, 2019, Business Data Science, Chapter 5, pp. 146-152. Travis, D.J., Carleton, A.M. and Lauritsen, R.G., 2004. ‘Regional variations in US diurnal temperature range for the 11–14 September 2001 aircraft groundings: Evidence of jet contrail influence on climate’, Journal of climate, 17(5), pp.1123-1134. Travis, David J., Andrew M. Carleton, and Ryan G. Lauritsen. “Contrails reduce daily temperature range.” Nature, 418, no. 6898 (2002): 601-601. Zinovyeva, Natalia and Maryna Tverdostup, 2019, ‘Why are women who earn slightly more than their husbands hard to find?’, 10 June, freely available at: https://blogs.lse.ac.uk/businessreview/2019/06/10/why-are-women-who-earn-slightly-more-than-their-husbands-hard-to-find/. Key concepts/skills/etc Generating simulated data. Understanding regression discontinuity and implementing it both manually and using packages. Appreciating the threats to the validity of regression discontinuity. Key libraries broom rdrobust tidyverse Key functions/etc lm() tidy() rdrobust()() Pre-quiz What are the fundamental features of regression discontinuity design? What are the conditions that are needed in order for RDD to be able to be used? Can you think of a situation in your own life where RDD may be useful? What are some threats to the validity of RDD estimates? Please look at the performance package: https://easystats.github.io/performance/index.html. What are some features of this package that may be useful in your own work? What do you think about using COVID-19 in an RDD setting? Statistically? Ethically? Please read and reproduce the main findings from Eggers, Fowler, Hainmueller, Hall, Snyder, 2015. 17.1 Introduction Regression discontinuity design (RDD) is a popular way to get causality when there is some continuous variable with cut-offs that determine treatment. Is there a difference between a student who gets 79 per cent and a student who gets 80 per cent? Probably not much, but one gets an A-, while the other gets a B+, and seeing that on a transcript could affect who gets a job which could affect income. In this case the percentage is a ‘forcing variable’ and the cut-off for an A- is a ‘threshold’. As the treatment is determined by the forcing variable all you need to do is to control for that variable. And, these seemingly arbitrary cut-offs can be seen all the time. Hence, there has been an ‘explosion’ in the use of regression discontinuity design (Figure 17.1). Please note that I’ve followed the terminology of Taddy, 2019. Gelman and Hill, 2007, and others use slightly different terminology. For instance, Cunningham refers to the forcing function as the running variable. It doesn’t matter what you use so long as you are consistent. If you have a terminology that you are familiar with then please feel free to use it, and to share it with me! Figure 17.1: The explosion of regression discontinuity designs in recent years. Source: John Holbein, 13 February 2020. The key assumptions are: The cut-off is ‘known, precise and free of manipulation’ (Cunningham, 2020, p. 163). The forcing function should be continuous because this means we can say that people on either side of the threshold are the same, other than happening to just fall on either side of the threshold. 17.2 Simulated example Let’s generate some data. library(broom) library(tidyverse) set.seed(853) number_of_observation &lt;- 1000 rdd_example_data &lt;- tibble(person = c(1:number_of_observation), grade = runif(number_of_observation, min = 78, max = 82), income = rnorm(number_of_observation, 10, 1) ) # We want to make income more likely to be higher if they are have a grade over 80 rdd_example_data &lt;- rdd_example_data %&gt;% mutate(income = if_else(grade &gt; 80, income + 2, income)) head(rdd_example_data) Table 17.1: persongradeincome 179.49.43 278.59.69 379.910.8&nbsp; 479.39.34 578.110.7&nbsp; 679.69.83 Let’s make a graph. rdd_example_data %&gt;% ggplot(aes(x = grade, y = income)) + geom_point(alpha = 0.2) + geom_smooth(data = rdd_example_data %&gt;% filter(grade &lt; 80), method=&#39;lm&#39;, color = &quot;black&quot;) + geom_smooth(data = rdd_example_data %&gt;% filter(grade &gt;= 80), method=&#39;lm&#39;, color = &quot;black&quot;) + theme_minimal() + labs(x = &quot;Grade&quot;, y = &quot;Income ($)&quot;) We can use a dummy variable with linear regression to estimate the effect (we’re hoping that it’s 2 because that is what we imposed.) rdd_example_data &lt;- rdd_example_data %&gt;% mutate(grade_80_and_over = if_else(grade &lt; 80, 0, 1)) lm(income ~ grade + grade_80_and_over, data = rdd_example_data) %&gt;% tidy() Table 17.2: termestimatestd.errorstatisticp.value (Intercept)11.7&nbsp;&nbsp;4.24&nbsp;&nbsp;2.76&nbsp;0.00585&nbsp; grade-0.0210.0537-0.3910.696&nbsp;&nbsp;&nbsp; grade_80_and_over1.99&nbsp;0.123&nbsp;16.2&nbsp;&nbsp;1.34e-52 There are various caveats to this estimate that we’ll get into later, but the essentials are here. The other great thing about regression discontinuity is that is can almost be as good as an RCT. For instance, (and I thank John Holbein for the pointer) Bloom, Bell, and Reiman (2020) compare randomized trials with RCTs and find that the RCTs compare favourably. 17.2.1 Different slopes Figure 17.2 shows an example with different slopes. Figure 17.2: Effect of minimum unit pricing for alcohol in Scotland. Source: John Burn-Murdoch, 7 February 2020. 17.3 Overlap In the randomised control trial and A/B testing section, because of randomised assignment of the treatment, we imposed that the control and treatment groups were the same but for the treatment. We moved to difference-in-differences, and we assumed that there was a common trend between the treated and control groups. We allowed that the groups could be different, but that we could ‘difference out’ their differences. Finally, we considered matching, and we said that even if we the control and treatment groups seemed quite different we were able to match those who were treated with a group that were similar to them in all ways, apart from the fact that they were not treated. In regression discontinuity we consider a slightly different setting - the two groups are completely different in terms of the forcing variable - they are on either side of the threshold. So there is no overlap at all. But we know the threshold and believe that those on either side are essentially matched. Let’s consider the 2019 NBA Eastern Conference Semifinals - Toronto and the Philadelphia. Game 1: Raptors win 108-95; Game 2: 76ers win 94-89; Game 3: 76ers win 116-95; Game 4: Raptors win 101-96; Game 5: Raptors win 125-89; Game 6: 76ers win 112-101; and finally, Game 7: Raptors win 92-90, because of a ball that win in after bouncing on the rim four times. Was there really that much difference between the teams (Figure 17.3)? Figure 17.3: It took four bounces to go in, so how different were the teams…? Source: Stan Behal / Postmedia Network. 17.4 Examples As with difference-in-differences, after I learnt about it, I began to see opportunities to implement it everywhere. Frankly, I find it a lot easier to think of legitimate examples of using regression discontinuity than difference-in-differences. But, at the risk of mentioning yet another movie from the 1990s that none of you have seen, when I think of RDD, my first thought is often of Sliding Doors (Figure 17.4). Figure 17.4: Nobody expects the Spanish Inquisition. Source: Mlotek, Haley, 2018, ‘The Almosts and What-ifs of ’Sliding Doors’’, The Ringer, 24 April, freely available at: https://www.theringer.com/movies/2018/4/24/17261506/sliding-doors-20th-anniversary. Not only did the movie have a great soundtrack and help propel Gwyneth Paltrow to super-stardom, but it features an iconic moment in which Paltrow’s character, Helen, arrives at a tube station at which point the movie splits into two. In one version she just makes the train, and arrives home to find her boyfriend cheating on her; and in another she just misses the train and doesn’t find out about the boyfriend. I’d say, spoiler alert, but the movie was released in 1998, so… Of course, that ‘threshold’ turns out to be important. In the world in which she gets the train she leaves the boyfriend, cuts her hair, and changes everything about her life. In the world in which she misses the train she doesn’t. At least initially. But, and I can’t say this any better than Ashley Fetters: At the end of Sliding Doors, the “bad” version of Helen’s life elides right into the “good” version; even in the “bad” version, the philandering !@#$%^&amp; boyfriend eventually gets found out and dumped, the true love eventually gets met-cute, and the MVP friend comes through. According to the Sliding Doors philosophy, in other words, even when our lives take fluky, chaotic detours, ultimately good-hearted people find each other, and the bad boyfriends and home-wreckers of the world get their comeuppance. There’s no freak turn of events that allows the cheating boyfriend to just keep cheating, or the well-meaning, morally upright soulmates to just keep floating around in the universe unacquainted. Fetters, Ashley, 2018, ‘I Think About This a Lot: The Sliding Doors in Sliding Doors’, The Cut, 9 April, freely available at: https://www.thecut.com/2018/04/i-think-about-this-a-lot-the-sliding-doors-in-sliding-doors.html. I’m getting off-track here, but the point is, not only does it seem as though we have a ‘threshold’, but it seems as though there’s continuity! Let’s see some more legitimate implementations of regression discontinuity. (And thank you to Ryan Edwards for pointing me to these.) 17.4.1 Elections Elections are a common area of application for regression discontinuity because if the election is close then arguably there’s not much difference between the candidates. There are plenty of examples of regression discontinuity in an elections setting, but one recent one is George, Siddharth Eapen, 2019, ‘Like Father, Like Son? The Effect of Political Dynasties on Economic Development’, freely available at: https://www.dropbox.com/s/orhvh3n03wd9ybl/sid_JMP_dynasties_latestdraft.pdf?dl=0. In this paper George is interested in political dynasties. But is the child of a politician more likely to be elected because they are the child of a politician, or because they happen to also be similarly skilled at politics? Regression discontinuity can help because in a close election, we can look at differences between places where someone narrowly won with where a similar someone narrowly lost. In the George, 2019, case he examines: descendant effects using a close elections regression discontinuity (RD) design. We focus on close races between dynastic descendants (i.e. direct relatives of former officeholders) and non-dynasts, and we compare places where a descendant narrowly won to those where a descendant narrowly lost. In these elections, descendants and non-dynasts have similar demographic and political characteristics, and win in similar places and at similar rates. Nevertheless, we find negative economic effects when a descendant narrowly wins. Villages represented by a descendant have lower asset ownership and public good provision after an electoral term: households are less likely to live in a brick house and to own basic amenities like a refrigerator, mobile phone, or vehicle. Moreover, voters assess descendants to perform worse in office. An additional standard deviation of exposure to descendants lowers a village’s wealth rank by 12pp. The model that George, 2019, estimates is (p. 19: \\[y_i = \\alpha_{\\mbox{district}} + \\beta \\times \\mbox{Years descendant rule}_i + f(\\mbox{Descendant margin}) + \\gamma X_i + \\epsilon_{i,t}.\\] In this model, \\(y_i\\) is various development outcomes in village \\(i\\); \\(\\mbox{Years descendant rule}_i\\) is the number of years a dynastic descendant has represented village \\(i\\) in the national or state parliament; \\(\\mbox{Descendant margin}\\) is the vote share difference between the dynastic descendant and non-dynast; and \\(\\gamma X_i\\) is a vector of village-level adjustments. George, 2019, then conducts a whole bunch of tests of the validity of the regression discontinuity design (p. 19). These are critical in order for the results to be believed. There are a lot of different results but one is shown in Figure 17.5. Figure 17.5: George, 2019, descendant effects identified using close elections RD design (p. 41). 17.4.2 Economic development One of the issues with considering economic development is that a place typically is either subject to some treatment or not. However, sometimes regression discontinuity allows us to compare areas that were just barely treated with those that were just barely not. One recent paper that does this Esteban Mendez-Chacon and Diana Van Patten, 2020, ‘Multinationals, monopsony and local development: Evidence from the United Fruit Company’ available here: https://www.dianavanpatten.com/. They are interested in the effect of the United Fruit Company (UFCo), which was given land in Costa Rica between 1889 and 1984. They were given roughly 4 per cent of the national territory or around 4500 acres. They key is that this land assignment was redrawn in 1904 based on a river and hence the re-assignment was essentially random with regard to determinants of growth to that point. They compare areas that were assigned to UFCo with those that were not. They find: We find that the firm had a positive and persistent effect on living standards. Regions within the UFCo were 26 per cent less likely to be poor in 1973 than nearby counterfactual locations, with only 63 per cent of the gap closing over the following three decades. Company documents explain that a key concern at the time was to attract and maintain a sizable workforce, which induced the firm to invest heavily in local amenities that likely account for our result. The model is: \\[y_{i,g,t} = \\gamma\\mbox{UFCo}_g + f(\\mbox{geographic location}_g) + X_{i,g,t}\\beta + X_g\\Gamma + \\alpha_t + \\epsilon_{i,g,t}.\\] In this model, \\(y_{i,g,t}\\) is the development outcome for a household \\(i\\) in census-block \\(g\\) and year \\(t\\); \\(\\gamma\\mbox{UFCo}_g\\) is an indicator variable as to whether the census-block was in a UFCo area or not; \\(f(\\mbox{geographic location}_g)\\) is a function of the latitude and longitude to adjust for geographic area; \\(X_{i,g,t}\\) is covariates for household \\(i\\); \\(X_g\\) is geographic characteristics for that census-block; and \\(\\alpha_t\\) is a year fixed effect. Again, there are a lot of different results but one is shown in Figure 17.6. Figure 17.6: George, 2020, UFCo effect on the probability of being poor (p. 17). 17.5 Implementation Although they are fairly conceptually similar to work that we have done in the past, if you are wanting to use regression discontinuity in your work then you might like to consider a specialised package. The package rdrobust is a one recommendation, although there are others available and you should try those if you are interested. (The rdd package had been the go-to for a while, but seems to have been taken off CRAN recently. If you use RDD, then maybe just follow up to see if it comes back on as that one is pretty nice.) Let’s look at our example using rdrobust. library(rdrobust) rdrobust(y = rdd_example_data$income, x = rdd_example_data$grade, c = 80, h = 2, all = TRUE) %&gt;% summary() ## Call: rdrobust ## ## Number of Obs. 1000 ## BW type Manual ## Kernel Triangular ## VCE method NN ## ## Number of Obs. 497 503 ## Eff. Number of Obs. 497 503 ## Order est. (p) 1 1 ## Order bias (q) 2 2 ## BW est. (h) 2.000 2.000 ## BW bias (b) 2.000 2.000 ## rho (h/b) 1.000 1.000 ## Unique Obs. 497 503 ## ## ============================================================================= ## Method Coef. Std. Err. z P&gt;|z| [ 95% C.I. ] ## ============================================================================= ## Conventional 1.974 0.143 13.783 0.000 [1.693 , 2.255] ## Bias-Corrected 1.977 0.143 13.805 0.000 [1.696 , 2.258] ## Robust 1.977 0.211 9.374 0.000 [1.564 , 2.390] ## ============================================================================= 17.6 Fuzzy RDD The examples to this point have been ‘sharp’ RDD. That is, the threshold is strict. However, in reality, often the boundary is a little less strict. For instance, consider the drinking age. Although there is a legal drinking age, say 19. If we looked at the number of people who had drank, then it’s likely to increase in the few years leading up to that age. Perhaps you went to Australia where the drinking age is 18 and drank. Or perhaps you snuck into a bar when you were 17, etc. In a sharp RDD setting, if you know the value of the forcing function then you know the outcome. For instance, if you get a grade of 80 then we know that you got an A-, but if you got a grade of 79 then we know that you got a B+. But with fuzzy RDD it is only known with some probability. We can say that a Canadian 19-year-old is more likely to have drunk alcohol than a Canadian 18 year old, but the number of Canadian 18-year-olds who have drunk alcohol is not zero. It may be possible to deal with fuzzy RDD settings with appropriate choice of model or data. It may also be possible to deal with them using instrumental variables, which we cover in the next section. 17.7 Threats to validity The continuity assumption is fairly important, but we cannot test this as it is based on a counterfactual. Instead we need to convince people of it. Ways to do this include: Using a test/train set-up. Trying different specifications (and be very careful if your results don’t broadly persist when just consider linear or quadratic functions). Considering different subsets of the data. Consider different windows. Be up-front about uncertainty intervals, especially in graphs. Discuss and assuage concerns about the possibility of omitted variables. The threshold is also important. For instance, is there an actual shift or is there a non-linear relationship? We want as ‘sharp’ an effect as possible, but if the thresholds are known, then they will be gamed. For instance, there is a lot of evidence that people run for certain marathon times, and we know that people aim for certain grades. Similarly, from the other side, it is a lot easier for an instructor to just give out As than it is to have to justify Bs. One way to look at this is to consider how ‘balanced’ the sample is on either side of the threshold. Do this by using histograms with appropriate bins, for instance Figure 17.7, which is from Allen et al. (2017). Figure 17.7: Bunching around marathon times. 17.8 Weaknesses External validity may be hard - think about the A-/B+ example - do you think the findings generalise to B-/C+? The important responses are those that are close to the cut-off. So even if we have a whole bunch of B- and A+ students, they don’t really help much. Hence we need a lot of data. There is a lot of freedom for the researcher, so open science best practice becomes vital. 17.9 Case study - Stiers, Hooghe, and Dassonneville, 2020 Paper: Stiers, D., Hooghe, M. and Dassonneville, R., 2020. Voting at 16: Does lowering the voting age lead to more political engagement? Evidence from a quasi-experiment in the city of Ghent (Belgium). Political Science Research and Methods, pp.1-8. Available at: https://www.cambridge.org/core/journals/political-science-research-and-methods/article/voting-at-16-does-lowering-the-voting-age-lead-to-more-political-engagement-evidence-from-a-quasiexperiment-in-the-city-of-ghent-belgium/172A2D9B75ECB66E98C9680787F302AD#fndtn-information Data: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/J1FQW9 17.10 Case study - Caughey, and Sekhon., 2011 Paper: Caughey, Devin, and Jasjeet S. Sekhon. “Elections and the regression discontinuity design: Lessons from close US house races, 1942–2008.” Political Analysis 19.4 (2011): 385-408. Available at: https://www.cambridge.org/core/journals/political-analysis/article/elections-and-the-regression-discontinuity-design-lessons-from-close-us-house-races-19422008/E5A69927D29BE682E012CAE9BFD8AEB7 Data: https://dataverse.harvard.edu/dataset.xhtml?persistentId=hdl:1902.1/16357&amp;version=1.0 References "],["poll-of-polls.html", "Chapter 18 Poll of polls 18.1 Introduction", " Chapter 18 Poll of polls Required reading Arnold, Jeffrey B., 2018, ‘Simon Jackman’s Bayesian Model Examples in Stan’, Ch 13, 7 May, https://jrnold.github.io/bugs-examples-in-stan/campaign.html. Gelman, Andrew, Jessica Hullman, and Christopher Wlezien, 2020, ‘Information, incentives, and goals in election forecasts’, 8 September, available at: http://www.stat.columbia.edu/~gelman/research/unpublished/forecast_incentives3.pdf Gelman, Andrew, Merlin Heidemanns, and Elliott Morris, 2020, ‘2020 US POTUS model’, The Economist, freely available: https://github.com/TheEconomist/us-potus-model. Jackman, Simon, 2005, ‘Pooling the polls over an election campaign’, Australian Journal of Political Science, 40 (4), pp. 499-517. Required viewing Jackman, Simon, 2020, ‘The triumph of the quants?: Model-based poll aggregation for election forecasting’, Ihaka Lecture Series, https://youtu.be/MvGYsKIsLFs. Recommended reading Imai, Kosuke, 2017, Quantitative Social Science: An Introduction, Princeton University Press, Ch 4.1, and 5.3. Leigh, Andrew, and Justin Wolfers, 2006, ‘Competing approaches to forecasting elections: Economic models, opinion polling and prediction markets’, Economic Record, 82 (258), pp.325-340. Nickerson, David W., and Todd Rogers, 2014, ‘Political campaigns and big data’, Journal of Economic Perspectives, 28 (2), pp. 51-74. Shirani-Mehr, Houshmand, David Rothschild, Sharad Goel, and Andrew Gelman, 2018, ‘Disentangling bias and variance in election polls’, Journal of the American Statistical Association, 113 (522), pp. 607-614. Key concepts/skills/etc Key libraries Key functions/etc Quiz Lab Following the guidance of the TA, please examine various poll-of-polls models, make changes to them, and then see how their output changes. 18.1 Introduction [The Presidential election of] 2016 was the largest analytics failure in US political history. David Shor, 13 August 2020 In this section we look at poll-of-polls (equally pooling-the-polls or poll aggregation) approaches which use a statistical model to bring together the outcomes of polls. "],["multilevel-modelling-with-post-stratification.html", "Chapter 19 Multilevel modelling with post-stratification 19.1 Introduction 19.2 Hello world 19.3 Your turn! 19.4 Extended example 19.5 Your turn! 19.6 Adding layers 19.7 Communication 19.8 Concluding remarks", " Chapter 19 Multilevel modelling with post-stratification Required reading Alexander, Monica, 2019, ‘Analyzing name changes after marriage using a non-representative survey’, 7 August, https://www.monicaalexander.com/posts/2019-08-07-mrp/. Gelman, Andrew, Jennifer Hill and Aki Vehtari, 2020, Regression and Other Stories, Cambridge University Press, Ch 17. Hanretty, Chris, 2019, ‘An introduction to multilevel regression and post-stratification for estimating constituency opinion’, Political Studies Review, https://doi.org/10.1177/1478929919864773. Kastellec, Jonathan, Jeffrey Lax, and Justin Phillips, 2016, ‘Estimating State Public Opinion With Multi-Level Regression and Poststratification using R’, https://scholar.princeton.edu/sites/default/files/jkastellec/files/mrp_primer.pdf. Kennedy, Lauren and Jonah Gabry, 2019, ‘MRP with rstanarm’, rstanarm vignettes, 8 October, https://mc-stan.org/rstanarm/articles/mrp.html. Kennedy, Lauren, and Andrew Gelman, 2019, ‘Know your population and know your model: Using model-based regression and poststratification to generalize findings beyond the observed sample’, https://arxiv.org/abs/1906.11323. Wang, Wei, David Rothschild, Sharad Goel, and Andrew Gelman, 2015, ‘Forecasting elections with non-representative polls’, International Journal of Forecasting, 31, no. 3, pages 980-991. Wu, Changbao and Mary E. Thompson, 2020, Sampling Theory and Practice, Springer, Ch 17. Required viewing Gelman, Andrew, 2020, ‘Statistical Models of Election Outcomes’, CPSR Summer Program in Quantitative Methods of Social Research, https://youtu.be/7gjDnrbLQ4k. Recommended reading Cohn, Nate, 2016, ‘We Gave Four Good Pollsters the Same Raw Data. They Had Four Different Results’, The New York Times, The Upshot, 20 September, https://www.nytimes.com/interactive/2016/09/20/upshot/the-error-the-polling-world-rarely-talks-about.html. Gelman, Andrew, and Julia Azari, 2017, ‘19 things we learned from the 2016 election’, Statistics and Public Policy, 4 (1), pp. 1-10. Ghitza, Yair, and Andrew Gelman, 2013, ‘Deep Interactions with MRP: Election Turnout and Voting Patterns Among Small Electoral Subgroups’, American Journal of Political Science, 57 (3), pp. 762-776. Ghitza, Yair, and Andrew Gelman, 2020, ‘Voter Registration Databases and MRP: Toward the Use of Large-Scale Databases in Public Opinion Research’, Political Analysis, pp. 1-25. Jackman, Simon, Shaun Ratcliff and Luke Mansillo, 2019, ‘Small area estimates of public opinion: Model-assisted post-stratification of data from voter advice applications’, 4 January, https://www.cambridge.org/core/membership/services/aop-file-manager/file/5c2f6ebb7cf9ee1118d11c0a/APMM-2019-Simon-Jackman.pdf. Lauderdale, Ben, Delia Bailey, Jack Blumenau, and Doug Rivers, 2020, ‘Model-based pre-election polling for national and sub-national outcomes in the US and UK’, International Journal of Forecasting, 36 (2), pp. 399-413. Key libraries brms broom here tidybayes tidyverse Quiz Your Mum asked you what you’ve been learning this term. You decide to tell her about multilevel modelling with post-stratification. Please explain what MRP is. Your Mum has a university-education, but has not necessarily taken any statistics, so you will need to explain any technical terms that you use. [Please write a paragraph or two.] Please consider Wang, Rothschild, Goel, and Gelman, 2015, ‘Forecasting elections with non-representative polls’. Is this the most interesting paper ever written? Why or why not? What do you like about this paper? What do you wish it did better? Can you reproduce this paper? [Please write one or two paragraphs about each aspect.] I am interested in studying how voting intentions in the recent US presidential election vary by an individual’s income. I set up a logistic regression model to study this relationship. In my study, some possible independent variables would be: [Please check all that apply.] 1) Whether the respondent is registered to vote (yes/no). 2) Whether the respondent is going to vote for Biden (yes/no). 3) The race of the respondent (white/not white). 4) The respondent’s marital status (married/not) Please think about Cohn, 2016, ‘We Gave Four Good Pollsters the Same Raw Data. They Had Four Different Results’. Why is this type of exercise not carried out more? Why do you think that different groups, even with the same background and level of quantitative sophistication, could have such different estimates even when they use the same data? [Please write a paragraph or two about each aspect.] Please consider Wang, Rothschild, Goel, and Gelman, 2015, ‘Forecasting elections with non-representative polls’. What is not a feature they mention election forecasts need? 1) Explainable. 2) Accurate. 3) Cost-effective. 4) Relevant. 5) Timely. What is a weakness of MRP? 1) Detailed data requirement. 2) Allows use of biased data. 3) Expensive to conduct. What is concerning about the Xbox sample? 1) Non-representative. 2) Small sample size. 3) Multiple responses from the same respondent. 19.1 Introduction Multilevel regression with post-stratification (MRP) is a popular way to adjust non-representative samples to better analyse opinion and other survey responses. It uses a regression model to relate individual-level survey responses to various characteristics and then rebuilds the sample to better match the population. In this way MRP can not only allow a better understanding of responses, but also allow us to analyse data that may otherwise be unusable. However, it can be a challenge to get started with MRP as the terminology may be unfamiliar, and the data requirements can be onerous. Multilevel regression with post-stratification (MRP) is a handy approach when dealing with survey data. Essentially, it trains a model based on the survey, and then applies that trained model to another dataset. There are two main, related, advantages: It can allow us to ‘re-weight’ in a way that includes uncertainty front-of-mind and isn’t hamstrung by small samples. It can allow us to use broad surveys to speak to subsets. From a practical perspective, it tends to be less expensive to collect non-probability samples and so there are benefits of being able to use these types of data. That said, it is not a magic-bullet and the laws of statistics still apply. We will have larger uncertainty around our estimates and they will still be subject to all the usual biases. As Lauren Kennedy points out, ‘MRP has traditionally been used in probability surveys and had potential for non-probability surveys, but we’re not sure of the limitations at the moment.’ One famous example is Wei Wang, David Rothschild, Sharad Goel, and Andrew Gelman, 2014, ‘Forecasting elections with non-representative polls’, International Journal of Forecasting. They used data from the Xbox gaming platform to forecast the 2012 US Presidential Election. Key facts about the set-up: Data from an opt-in poll which was available on the Xbox gaming platform during the 45 days preceding the 2012 US presidential election. Each day there were three to five questions, including voter intention: “If the election were held today, who would you vote for?” Respondents were allowed to answer at most once per day. First-time respondents were asked to provide information about themselves, including their sex, race, age, education, state, party ID, political ideology, and who they voted for in the 2008 presidential election. In total, 750,148 interviews were conducted, with 345,858 unique respondents - over 30,000 of whom completed five or more polls Young men dominate the Xbox population: 18-to-29-year-olds comprise 65 per cent of the Xbox dataset, compared to 19 per cent in the exit poll; and men make up 93 per cent of the Xbox sample but only 47 per cent of the electorate. Given the US electorate, they use a two-stage modelling approach. The details don’t really matter too much, and essentially they model how likely a respondent is to vote for Obama, given various information such as state, education, sex, etc: \\[ Pr\\left(Y_i = \\mbox{Obama} | Y_i\\in\\{\\mbox{Obama, Romney}\\}\\right) = \\mbox{logit}^{-1}(\\alpha_0 + \\alpha_1(\\mbox{state last vote share}) + \\alpha_{j[i]}^{\\mbox{state}} + \\alpha_{j[i]}^{\\mbox{edu}} + \\alpha_{j[i]}^{\\mbox{sex}}... ) \\] They run this in R using glmer() from lme4. Having a trained model that considers the effect of these various independent variables on support for the candidates, they now post-stratify, where each of these “cell-level estimates are weighted by the proportion of the electorate in each cell and aggregated to the appropriate level (i.e., state or national).” This means that they need cross-tabulated population data. In general, the census would have worked, or one of the other large surveys available in the US, but the difficulty is that the variables need to be available on a cross-tab basis. As such, they use exit polls (not an option for Australia in general). They make state-specific estimates by post-stratifying to the features of each state (Figure ). Figure 19.1: Post-stratified estimates for each state based on the Xbox survey and MRP Similarly, they can examine demographic-differences (Figure ). Figure 19.2: Post-stratified estimates on a demographic basis based on the Xbox survey and MRP Finally, they convert their estimates into electoral college estimates (Figure ). Figure 19.3: Post-stratified estimates of electoral college outcomes based on the Xbox survey and MRP 19.2 Hello world The workflow that we are going to use is: read in the poll; model the poll; read in the post-stratification data; and apply the model to the post-stratification data. We are going to use R (R Core Team 2020). First load the packages: broom (Robinson, Hayes, and Couch 2020), here (Müller 2017b), tidyverse (Wickham, Averick, et al. 2019b). # Uncomment these (by deleting the #) if you need to install the packages # install.packages(&quot;broom&quot;) # install.packages(&quot;here&quot;) # install.packages(&quot;skimr&quot;) # install.packages(&quot;tidyverse&quot;) library(broom) # Helps make the regression results tidier library(here) # Helps make file referencing easier. library(tidyverse) # Helps make programming with R easier Then load some sample polling data to analyse. I have generated this fictitious data so that we have some idea of what to expect from the model. The dependent variable is supports_ALP, which is a binary variable - either 0 or 1. We’ll just use two independent variables here: gender, which is either Female or Male (as that is what is available from the ABS); and age_group, which is one of four groups: ages 18 to 29, ages 30 to 44, ages 45 to 59, ages 60 plus. example_poll &lt;- read_csv(&quot;outputs/data/example_poll.csv&quot;) # Here we read in a # CSV file and assign it to a dataset called &#39;example_poll&#39; head(example_poll) # Displays the first 10 rows (#tab:initial_model_simulate_data) genderage_groupsupports_ALPstate Maleages30to440NSW Femaleages45to590NSW Femaleages60plus1VIC Maleages30to441QLD Femaleages30to441QLD Femaleages18to291VIC # Look at some summary statistics to make sure the data seem reasonable summary(example_poll) ## gender age_group supports_ALP state ## Length:5000 Length:5000 Min. :0.0000 Length:5000 ## Class :character Class :character 1st Qu.:0.0000 Class :character ## Mode :character Mode :character Median :1.0000 Mode :character ## Mean :0.5514 ## 3rd Qu.:1.0000 ## Max. :1.0000 I generated this polling data to make both made males and older people less likely to vote for the Australian Labor Party; and females and younger people more likely to vote for the Labor Party. Females are over-sampled. As such, we should have an ALP skew on the dataset. # The &#39;%&gt;%&#39; is called a &#39;pipe&#39; and it takes whatever the output is of the # command before it, and pipes it to the command after it. example_poll %&gt;% # So we are taking our example_poll dataset and using it as an # input to &#39;summarise&#39;. # summarise reduces the dimensions, so here we will get one number from a column. summarise(raw_ALP_prop = sum(supports_ALP) / nrow(example_poll)) (#tab:summarise_model_simulate_data) raw_ALP_prop 0.551 Now we’d like to see if we can get our results back (we should find females less likely than males to vote for Australian Labor Party and that people are less likely to vote Australian Labor Party as they get older). Our model is: \\[ \\mbox{ALP support}_j = \\mbox{gender}_j + \\mbox{age\\_group}_j + \\epsilon_j \\] This model says that the probability that some person, \\(j\\), will vote for the Australian Labor Party depends on their gender and their age-group. Based on our simulated data, we would like older age-groups to be less likely to vote for the Australian Labor Party and for males to be less likely to vote for the Australian Labor Party. # Here we are running an OLS regression with supports_ALP as the dependent variable # and gender and age_group as the independent variables. The dataset that we are # using is example_poll. We are then saving that OLS regression to a variable called &#39;model&#39;. model &lt;- lm(supports_ALP ~ gender + age_group, data = example_poll ) # broom::tidy just displays the outputs of the regression in a nice table. broom::tidy(model) (#tab:initial_model_analyse_example_polling) termestimatestd.errorstatisticp.value (Intercept)0.9&nbsp;&nbsp;0.013168.80&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; genderMale-0.2050.0142-14.42.69e-46&nbsp; age_groupages30to44-0.1860.0176-10.66.5e-26&nbsp;&nbsp; age_groupages45to59-0.4020.0177-22.78.29e-109 age_groupages60plus-0.5850.0175-33.45.2e-221&nbsp; Essentially we’ve got our inputs back. We just used regular OLS even though our dependent variable is a binary. (It’s usually fine to start with an OLS model and then iterate toward an approach that may be more appropriate such as logistic regression or whatever, but where the results are a little more difficult to interpret.10) If you wanted to do that then the place to start would be glmer() from the R package lme4, and we’ll see that in the next section. Now we’d like to see if we can use what we found in the poll to get an estimate for each state based on their demographic features. First read in some real demographic data, on a seat basis, from the ABS. census_data &lt;- read_csv(&quot;outputs/data/census_data.csv&quot;) head(census_data) (#tab:initial_model_post_stratify_add_coefficients) stategenderage_groupnumbercell_prop_of_division_total ACTFemaleages18to293.47e+040.125 ACTFemaleages30to444.3e+04&nbsp;0.155 ACTFemaleages45to593.38e+040.122 ACTFemaleages60plus3.03e+040.109 ACTMaleages18to293.42e+040.123 ACTMaleages30to444.13e+040.149 We’re just going to do some rough forecasts. For each gender and age-group we want the relevant coefficient in the example_data and we can construct the estimates. # Here we are making predictions using our model with some new data from the # census, and we saving the results of those predictions by adding a new column # to the census_data dataset called &#39;estimate&#39;. census_data$estimate &lt;- model %&gt;% predict(newdata = census_data) census_data %&gt;% mutate(alp_predict_prop = estimate*cell_prop_of_division_total) %&gt;% group_by(state) %&gt;% summarise(alp_predict = sum(alp_predict_prop)) (#tab:initial_model_post_stratify_age_sex_specific) statealp_predict ACT0.525 NSW0.495 NT0.541 QLD0.496 SA0.479 TAS0.464 VIC0.503 WA0.503 We now have post-stratified estimates for each division. Our model has a fair few weaknesses. For instance small cell counts are going to be problematic. And our approach ignores uncertainty, but now that we have something working we can complicate it. 19.3 Your turn! We’re going to go through this all again, but this time with you doing it. If you run into issues then I am happy to help point you in the right direction. As a reminder, our workflow is: read in the poll; model the poll; read in the post-stratification data; apply your model to the post-stratification data. Get started by opening a Rproj file and opening a new R script. 19.4 Extended example We’d like to address some of the major issues with our approach, specifically being able to deal with small cell counts, and also taking better account of uncertainty. As we are dealing with survey data, prediction intervals or something similar are crticial, and it’s not appropriate to only report central estimates. To do this we’ll use the same broad approach as before, but just improving bits of our workflow. First load the packages (you don’t need to reload the earlier ones - I just do it here so that each section is self-contained in case people are lost). We additionally need: brms (Bürkner 2018) and tidybayes (Kay 2020). # Uncomment these if you need to install the packages # install.packages(&quot;broom&quot;) # install.packages(&quot;brms&quot;) # install.packages(&quot;here&quot;) # install.packages(&quot;tidybayes&quot;) # install.packages(&quot;tidyverse&quot;) library(broom) library(brms) # Used for the modelling library(here) library(tidybayes) # Used to help understand the modelling estimates library(tidyverse) As before, read in the polling dataset. example_poll &lt;- read_csv(&quot;outputs/data/example_poll.csv&quot;) head(example_poll) (#tab:brms_model_simulate_data) genderage_groupsupports_ALPstate Maleages30to440NSW Femaleages45to590NSW Femaleages60plus1VIC Maleages30to441QLD Femaleages30to441QLD Femaleages18to291VIC Now, using the same basic model as before, but we move it to a setting that acknowledges the dependent variable as being binary, and in a Bayesian setting. model &lt;- brm(supports_ALP ~ gender + age_group, data = example_poll, family = bernoulli(), file = &quot;outputs/model/brms_model&quot; ) model &lt;- read_rds(&quot;outputs/model/brms_model.rds&quot;) summary(model) ## Family: bernoulli ## Links: mu = logit ## Formula: supports_ALP ~ gender + age_group ## Data: example_poll (Number of observations: 5000) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 2.07 0.09 1.91 2.23 1.00 2240 2194 ## genderMale -1.06 0.07 -1.20 -0.91 1.00 3403 2595 ## age_groupages30to44 -1.10 0.10 -1.29 -0.91 1.00 2483 2805 ## age_groupages45to59 -2.04 0.10 -2.23 -1.85 1.00 2521 3061 ## age_groupages60plus -2.88 0.10 -3.09 -2.68 1.00 2517 2858 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We’ve moved to the Bernoulli distribution, so we have to do a bit more work to understand our results, but we are broadly getting back what we’d expect. As before, we’d like an estimate for each state based on their demographic features and start by reading in the data. census_data &lt;- read_csv(&quot;outputs/data/census_data.csv&quot;) head(census_data) (#tab:brms_model_post_stratify_add_coefficients) stategenderage_groupnumbercell_prop_of_division_total ACTFemaleages18to293.47e+040.125 ACTFemaleages30to444.3e+04&nbsp;0.155 ACTFemaleages45to593.38e+040.122 ACTFemaleages60plus3.03e+040.109 ACTMaleages18to293.42e+040.123 ACTMaleages30to444.13e+040.149 We’re just going to do some rough forecasts. For each gender and age_group we want the relevant coefficient in the example_data and we can construct the estimates (this code is from Monica Alexander). post_stratified_estimates &lt;- model %&gt;% tidybayes::add_predicted_draws(newdata = census_data) %&gt;% rename(alp_predict = .prediction) %&gt;% mutate(alp_predict_prop = alp_predict*cell_prop_of_division_total) %&gt;% group_by(state, .draw) %&gt;% summarise(alp_predict = sum(alp_predict_prop)) %&gt;% group_by(state) %&gt;% summarise(mean = mean(alp_predict), lower = quantile(alp_predict, 0.025), upper = quantile(alp_predict, 0.975)) post_stratified_estimates (#tab:brms_model_post_stratify_age_sex_specific) statemeanlowerupper ACT0.53&nbsp;0.2450.791 NSW0.4940.2140.767 NT0.5440.2530.852 QLD0.4980.2150.768 SA0.4820.2010.75&nbsp; TAS0.4650.1830.757 VIC0.5020.2240.769 WA0.5030.2190.765 We now have post-stratified estimates for each division. Our new Bayesian approach will enable us to think more deeply about uncertainty. We could complicate this in a variety of ways including adding more coefficients (but remember that we’d need to get new cell counts), or adding some layers. 19.5 Your turn! We’re going to go through this all again, but this time with you doing it. If you run into issues then I am happy to help point you in the right direction. As a reminder, our workflow is: read in the poll; model the poll; read in the post-stratification data; apply your model to the post-stratification data. 19.6 Adding layers We may like to try to add some layers to our model. For instance, we may like a different intercept for each state. model_states &lt;- brm(supports_ALP ~ gender + age_group + (1|state), data = example_poll, family = bernoulli(), file = &quot;outputs/model/brms_model_states&quot;, control = list(adapt_delta = 0.90) ) summary(model_states) ## Family: bernoulli ## Links: mu = logit ## Formula: supports_ALP ~ gender + age_group + (1 | state) ## Data: example_poll (Number of observations: 5000) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~state (Number of levels: 8) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.06 0.05 0.00 0.20 1.00 1553 2072 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 2.07 0.09 1.90 2.26 1.00 1660 2273 ## genderMale -1.06 0.08 -1.21 -0.91 1.00 4106 2833 ## age_groupages30to44 -1.10 0.10 -1.30 -0.90 1.00 2110 2566 ## age_groupages45to59 -2.04 0.10 -2.24 -1.84 1.00 2058 2347 ## age_groupages60plus -2.89 0.10 -3.10 -2.69 1.00 2201 2581 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). # broom::tidy(model_states, par_type = &quot;varying&quot;) # broom::tidy(model_states, par_type = &quot;non-varying&quot;, robust = TRUE) One interesting aspect is that our multilevel approach will allow us to deal with small cell counts by borrowing information from other cells. example_poll %&gt;% count(state) (#tab:brms_model_analyse_extended_state_counts) staten ACT107 NSW1622 NT50 QLD982 SA359 TAS105 VIC1285 WA490 At the moment we have 50 respondents in the Northern Territory, 105 in Tasmania, and 107 in the ACT. Even if we were to remove most of the, say, 18 to 29 year old, male respondents from Tasmania our model would still provide estimates. It does this by pooling, in which the effect of these young, male, Tasmanians is partially determined by other cells that do have respondents. 19.7 Communication There are many interesting aspects that we may like to communicate to others. For instance, we may like to show how the model is affecting the results. We can make a graph that compares the raw estimate with the model estimate. post_stratified_estimates %&gt;% ggplot(aes(y = mean, x = forcats::fct_inorder(state), color = &quot;MRP estimate&quot;)) + geom_point() + geom_errorbar(aes(ymin = lower, ymax = upper), width = 0) + ylab(&quot;Proportion ALP support&quot;) + xlab(&quot;State&quot;) + geom_point(data = example_poll %&gt;% group_by(state, supports_ALP) %&gt;% summarise(n = n()) %&gt;% group_by(state) %&gt;% mutate(prop = n/sum(n)) %&gt;% filter(supports_ALP==1), aes(state, prop, color = &quot;Raw data&quot;)) + theme_minimal() + scale_color_brewer(palette = &quot;Set1&quot;) + theme(legend.position = &quot;bottom&quot;) + theme(legend.title = element_blank()) Similarly, we may like to plot the distribution of the coefficients.11 model %&gt;% gather_draws(`b_.*`, regex=TRUE) %&gt;% ungroup() %&gt;% mutate(coefficient = stringr::str_replace_all(.variable, c(&quot;b_&quot; = &quot;&quot;))) %&gt;% mutate(coefficient = forcats::fct_recode(coefficient, Intercept = &quot;Intercept&quot;, `Is male` = &quot;genderMale&quot;, `Age 30-44` = &quot;age_groupages30to44&quot;, `Age 45-59` = &quot;age_groupages45to59&quot;, `Age 60+` = &quot;age_groupages60plus&quot; )) %&gt;% # both %&gt;% ggplot(aes(y=fct_rev(coefficient), x = .value)) + ggridges::geom_density_ridges2(aes(height = ..density..), rel_min_height = 0.01, stat = &quot;density&quot;, scale=1.5) + xlab(&quot;Distribution of estimate&quot;) + ylab(&quot;Coefficient&quot;) + scale_fill_brewer(name = &quot;Dataset: &quot;, palette = &quot;Set1&quot;) + theme_minimal() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + theme(legend.position = &quot;bottom&quot;) 19.8 Concluding remarks In general, MRP is a good way to accomplish specific aims, but it’s not without trade-offs. If you have a good quality survey, then it may be a way to speak to disaggregated aspects of it. Or if you are concerned about uncertainty then it is a good way to think about that. If you have a biased survey then it’s a great place to start, but it’s not a panacea. There’s not a lot of work that’s been done using Canadian data, so there’s plenty of scope for exciting work. I look forward to seeing what you do with it! References "],["text-as-data.html", "Chapter 20 Text as data 20.1 Introduction 20.2 Lasso regression 20.3 Topic models 20.4 Word embedding 20.5 Conclusion", " Chapter 20 Text as data Required reading Required viewing Recommended reading Key concepts/skills/etc Key libraries Key functions/etc Pre-quiz 20.1 Introduction TBD 20.2 Lasso regression This subsection, and much of the code that is used, directly draws on Julia Silge’s notes, in particular: https://juliasilge.com/blog/tidy-text-classification/ (Silge 2018). One of the nice aspects of text is that we can adapt our existing methods to use it as an input. Here we are going to use a variation of logistics regression, along with text inputs, to forecast. If you want to learn more about Lasso regression, then you should consider taking Arik’s course over the summer, where he will dive into machine learning using Python. In this section we are going to have two different text inputs, train a model on a sample of text from each of them, and then try to use that model to forecast the text in a training set. Although this is a arbitrary example, you could imagine many real-world applications. For instance, if you work at Twitter then you may want to know if a tweet was likely written by a bot, or by a human. Or similarly, imagine that you work for a political party - you may like to know if an email was likely from an email campaign organised by a group, or from an individual. First we need to get some data. Julia Silge’s example, nicely, uses book text as input. Seeing as I am jointly appointed at a Faculty of Information, that seems especially nice. The wonderful thing about this is that there is an R package - gutenbergr - that makes it easy to get text from Project Gutenberg into R. The key function is gutenberg_download(), which needs a key for the book that you want. We’ll consider Jane Eyre and Alice’s Adventures in Wonderland, which have the keys of 1260 and 11, respectively. library(gutenbergr) alice_and_jane &lt;- gutenberg_download(c(1260, 11), meta_fields = &quot;title&quot;) # Save the dataset so that we don&#39;t need to overwhelm the servers each time write_csv(alice_and_jane, &quot;inputs/books/alice_and_jane.csv&quot;) head(alice_and_jane) library(gutenbergr) alice_and_jane &lt;- read_csv(&quot;inputs/books/alice_and_jane.csv&quot;) head(alice_and_jane) Table 20.1: gutenberg_idtexttitle 11ALICE'S ADVENTURES IN WONDERLANDAlice's Adventures in Wonderland 11Alice's Adventures in Wonderland 11Lewis CarrollAlice's Adventures in Wonderland 11Alice's Adventures in Wonderland 11THE MILLENNIUM FULCRUM EDITION 3.0Alice's Adventures in Wonderland 11Alice's Adventures in Wonderland One of the great things about this is that the dataset is a tibble. So we can just work with all our familiar skills. The package has a lot more functionality, so I’d encourage you to look at the package’s website: https://github.com/ropensci/gutenbergr. Each line of the book is read in as a different row in the dataset. Notice that we have downloaded two books here at once, and so we added the title. The two books are one after each other. You can see that we have both by looking at some summary statistics. table(alice_and_jane$title) ## ## Alice&#39;s Adventures in Wonderland Jane Eyre: An Autobiography ## 3339 20659 So it looks like Jane Eyre is much longer than Alice in Wonderland, which isn’t a surprise to those who have read them. I don’t want to step into Digital Humanities too much, as I don’t know anything about it, but looking at things like the broader context of when these books were written, or other books that were written at similar times, is likely a fascinating area. We’ll just get rid of blank lines library(janitor) # TODO There&#39;s a way to do this within janitor, but I forget, need to look it up. alice_and_jane &lt;- alice_and_jane %&gt;% mutate(blank_line = if_else(text == &quot;&quot;, 1, 0)) %&gt;% filter(blank_line == 0) %&gt;% select(-blank_line) table(alice_and_jane$title) ## ## Alice&#39;s Adventures in Wonderland Jane Eyre: An Autobiography ## 2481 16395 There’s still an overwhelming amount of Jane Eyre in there. So we’ll just sample from Jane Eyre to make it more equal. set.seed(853) alice_and_jane$rows &lt;- c(1:nrow(alice_and_jane)) sample_from_me &lt;- alice_and_jane %&gt;% filter(title == &quot;Jane Eyre: An Autobiography&quot;) keep_me &lt;- sample(x = sample_from_me$rows, size = 2481, replace = FALSE) alice_and_jane &lt;- alice_and_jane %&gt;% filter(title == &quot;Alice&#39;s Adventures in Wonderland&quot; | rows %in% keep_me) %&gt;% select(-rows) table(alice_and_jane$title) ## ## Alice&#39;s Adventures in Wonderland Jane Eyre: An Autobiography ## 2481 2481 There’s a bunch of issues here, for instance, we have the whole of Alice, but we only have random bits of Jane, but nonetheless let’s continue and we’ll try to do something about that in a moment. Now we want to get a sample of text from each book. We will use the lines to distinguish these samples. So we use a counter that will add a line number. alice_and_jane &lt;- alice_and_jane %&gt;% group_by(title) %&gt;% mutate(line_number = paste(gutenberg_id, row_number(), sep = &quot;_&quot;)) %&gt;% ungroup() We now want to sepearate out the words. We’ll just use tidytext, because the focus here is on modelling, but there are a bunch of alternatives and one especially good one is the quanteda package, specifically, the tokens() function. library(tidytext) alice_and_jane_by_word &lt;- alice_and_jane %&gt;% unnest_tokens(word, text) %&gt;% group_by(word) %&gt;% filter(n() &gt; 10) %&gt;% ungroup() Notice here that we removed any word that wasn’t used more than 10 times. Nonetheless we still have a lot of unique words. (If we didn’t require that the word be used by the author at least 10 times then we end up with more than 6,000 words.) alice_and_jane_by_word$word %&gt;% unique() %&gt;% length() ## [1] 585 The reason this is relevant is because these are our independent variables. So where you may be used to having something less than 10 explanatory variables, in this case we are going to have 585 As such, we need a model that can handle this. However, as mentioned before, we are going to have some rows that essentially just had one word. While we could allow that, it might also be nice to give the model at least a few words to work with. alice_and_jane_by_word &lt;- alice_and_jane_by_word %&gt;% group_by(title, line_number) %&gt;% mutate(number_of_words_in_line = n()) %&gt;% ungroup() %&gt;% filter(number_of_words_in_line &gt; 2) %&gt;% select(-number_of_words_in_line) We’ll create a test/training split, and load in tidymodels. library(tidymodels) set.seed(853) alice_and_jane_by_word_split &lt;- alice_and_jane_by_word %&gt;% select(title, line_number) %&gt;% distinct() %&gt;% initial_split(prop = 3/4, strata = title) # alice_and_jane_by_word_train &lt;- training(alice_and_jane_by_word_split) %&gt;% select(line_number) # alice_and_jane_by_word_test &lt;- testing(alice_and_jane_by_word_split) # # rm(alice_and_jane_by_word_split) Now we need to create a document-term matrix. alice_and_jane_dtm_training &lt;- alice_and_jane_by_word %&gt;% count(line_number, word) %&gt;% inner_join(training(alice_and_jane_by_word_split) %&gt;% select(line_number)) %&gt;% cast_dtm(term = word, document = line_number, value = n) dim(alice_and_jane_dtm_training) ## [1] 3415 585 So we have our independent variables sorted, now we need our binary dependent variable, which is whether the book is Alice in Wonderland or Jane Eyre. response &lt;- data.frame(id = dimnames(alice_and_jane_dtm_training)[[1]]) %&gt;% separate(id, into = c(&quot;book&quot;, &quot;line&quot;, sep = &quot;_&quot;)) %&gt;% mutate(is_alice = if_else(book == 11, 1, 0)) predictor &lt;- alice_and_jane_dtm_training[] %&gt;% as.matrix() Now we can run our model. library(glmnet) model &lt;- cv.glmnet(x = predictor, y = response$is_alice, family = &quot;binomial&quot;, keep = TRUE ) save(model, file = &quot;outputs/models/alice_vs_jane.rda&quot;) load(&quot;outputs/models/alice_vs_jane.rda&quot;) library(glmnet) library(broom) coefs &lt;- model$glmnet.fit %&gt;% tidy() %&gt;% filter(lambda == model$lambda.1se) coefs %&gt;% head() Table 20.2: termstepestimatelambdadev.ratio (Intercept)36-0.335&nbsp;&nbsp;0.005970.562 in36-0.144&nbsp;&nbsp;0.005970.562 she360.39&nbsp;&nbsp;&nbsp;0.005970.562 so360.002490.005970.562 a36-0.117&nbsp;&nbsp;0.005970.562 about360.279&nbsp;&nbsp;0.005970.562 coefs %&gt;% group_by(estimate &gt; 0) %&gt;% top_n(10, abs(estimate)) %&gt;% ungroup() %&gt;% ggplot(aes(fct_reorder(term, estimate), estimate, fill = estimate &gt; 0)) + geom_col(alpha = 0.8, show.legend = FALSE) + coord_flip() + theme_minimal() + labs(x = &quot;Coefficient&quot;, y = &quot;Word&quot;) + scale_fill_brewer(palette = &quot;Set1&quot;) Perhaps unsurprisingly, if you mention Alice then it’s likely to be a Alice in Wonderland and if you mention Jane then it’s likely to be Jane Eyre. 20.3 Topic models A version of these notes was previously circulated as part of Alexander and Alexander (2020). 20.3.1 Overview Sometimes we have a statement and we want to know what it is about. Sometimes this will be easy, but we don’t always have titles for statements, and even when we do, sometimes we do not have titles that define topics in a well-defined and consistent way. One way to get consistent estimates of the topics of each statement is to use topic models. While there are many variants, one way is to use the latent Dirichlet allocation (LDA) method of Blei, Ng, and Jordan (2003), as implemented by the R package ‘topicmodels’ by Grün and Hornik (2011). The key assumption behind the LDA method is that each statement, ‘a document’, is made by a person who decides the topics they would like to talk about in that document, and then chooses words, ‘terms’, that are appropriate to those topics. A topic could be thought of as a collection of terms, and a document as a collection of topics. The topics are not specified ex ante; they are an outcome of the method. Terms are not necessarily unique to a particular topic, and a document could be about more than one topic. This provides more flexibility than other approaches such as a strict word count method. The goal is to have the words found in documents group themselves to define topics. 20.3.2 Document generation process The LDA method considers each statement to be a result of a process where a person first chooses the topics they want to speak about. After choosing the topics, the person then chooses appropriate words to use for each of those topics. More generally, the LDA topic model works by considering each document as having been generated by some probability distribution over topics. For instance, if there were five topics and two documents, then the first document may be comprised mostly of the first few topics; the other document may be mostly about the final few topics (Figure 20.1). Figure 20.1: Probability distributions over topics Similarly, each topic could be considered a probability distribution over terms. To choose the terms used in each document the speaker picks terms from each topic in the appropriate proportion. For instance, if there were ten terms, then one topic could be defined by giving more weight to terms related to immigration; and some other topic may give more weight to terms related to the economy (Figure 20.2). Figure 20.2: Probability distributions over terms Following Blei and Lafferty (2009), Blei (2012) and Griffiths and Steyvers (2004), the process by which a document is generated is more formally considered to be: There are \\(1, 2, \\dots, k, \\dots, K\\) topics and the vocabulary consists of \\(1, 2, \\dots, V\\) terms. For each topic, decide the terms that the topic uses by randomly drawing distributions over the terms. The distribution over the terms for the \\(k\\)th topic is \\(\\beta_k\\). Typically a topic would be a small number of terms and so the Dirichlet distribution with hyperparameter \\(0&lt;\\eta&lt;1\\) is used: \\(\\beta_k \\sim \\mbox{Dirichlet}(\\eta)\\).12 Strictly, \\(\\eta\\) is actually a vector of hyperparameters, one for each \\(K\\), but in practice they all tend to be the same value. Decide the topics that each document will cover by randomly drawing distributions over the \\(K\\) topics for each of the \\(1, 2, \\dots, d, \\dots, D\\) documents. The topic distributions for the \\(d\\)th document are \\(\\theta_d\\), and \\(\\theta_{d,k}\\) is the topic distribution for topic \\(k\\) in document \\(d\\). Again, the Dirichlet distribution with the hyperparameter \\(0&lt;\\alpha&lt;1\\) is used here because usually a document would only cover a handful of topics: \\(\\theta_d \\sim \\mbox{Dirichlet}(\\alpha)\\). Again, strictly \\(\\alpha\\) is vector of length \\(K\\) of hyperparameters, but in practice each is usually the same value. If there are \\(1, 2, \\dots, n, \\dots, N\\) terms in the \\(d\\)th document, then to choose the \\(n\\)th term, \\(w_{d, n}\\): Randomly choose a topic for that term \\(n\\), in that document \\(d\\), \\(z_{d,n}\\), from the multinomial distribution over topics in that document, \\(z_{d,n} \\sim \\mbox{Multinomial}(\\theta_d)\\). Randomly choose a term from the relevant multinomial distribution over the terms for that topic, \\(w_{d,n} \\sim \\mbox{Multinomial}(\\beta_{z_{d,n}})\\). Given this set-up, the joint distribution for the variables is (Blei (2012), p.6): \\[p(\\beta_{1:K}, \\theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}) = \\prod^{K}_{i=1}p(\\beta_i) \\prod^{D}_{d=1}p(\\theta_d) \\left(\\prod^N_{n=1}p(z_{d,n}|\\theta_d)p\\left(w_{d,n}|\\beta_{1:K},z_{d,n}\\right) \\right).\\] Based on this document generation process the analysis problem, discussed in the next section, is to compute a posterior over \\(\\beta_{1:K}\\) and \\(\\theta_{1:D}\\), given \\(w_{1:D, 1:N}\\). This is intractable directly, but can be approximated (Griffiths and Steyvers (2004) and Blei (2012)). 20.3.3 Analysis process After the documents are created, they are all that we have to analyse. The term usage in each document, \\(w_{1:D, 1:N}\\), is observed, but the topics are hidden, or ‘latent’. We do not know the topics of each document, nor how terms defined the topics. That is, we do not know the probability distributions of Figures 20.1 or 20.2. In a sense we are trying to reverse the document generation process – we have the terms and we would like to discover the topics. If the earlier process around how the documents were generated is assumed and we observe the terms in each document, then we can obtain estimates of the topics (Steyvers and Griffiths (2006)). The outcomes of the LDA process are probability distributions and these define the topics. Each term will be given a probability of being a member of a particular topic, and each document will be given a probability of being about a particular topic. That is, we are trying to calculate the posterior distribution of the topics given the terms observed in each document (Blei (2012), p.7): \\[p(\\beta_{1:K}, \\theta_{1:D}, z_{1:D, 1:N} | w_{1:D, 1:N}) = \\frac{p\\left(\\beta_{1:K}, \\theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}\\right)}{p(w_{1:D, 1:N})}.\\] The initial practical step when implementing LDA given a corpus of documents is to remove ‘stop words’. These are words that are common, but that don’t typically help to define topics. There is a general list of stop words such as: “a”; “a’s”; “able”; “about”; “above”… We also remove punctuation and capitalisation. The documents need to then be transformed into a document-term-matrix. This is essentially a table with a column of the number of times each term appears in each document. After the dataset is ready, the R package ‘topicmodels’ by Grün and Hornik (2011) can be used to implement LDA and approximate the posterior. It does this using Gibbs sampling or the variational expectation-maximization algorithm. Following Steyvers and Griffiths (2006) and Darling (2011), the Gibbs sampling process attempts to find a topic for a particular term in a particular document, given the topics of all other terms for all other documents. Broadly, it does this by first assigning every term in every document to a random topic, specified by Dirichlet priors with \\(\\alpha = \\frac{50}{K}\\) and \\(\\eta = 0.1\\) (Steyvers and Griffiths (2006) recommends \\(\\eta = 0.01\\)), where \\(\\alpha\\) refers to the distribution over topics and \\(\\eta\\) refers to the distribution over terms (Grün and Hornik (2011), p.7). It then selects a particular term in a particular document and assigns it to a new topic based on the conditional distribution where the topics for all other terms in all documents are taken as given (Grün and Hornik (2011), p.6): \\[p(z_{d, n}=k | w_{1:D, 1:N}, z&#39;_{d, n}) \\propto \\frac{\\lambda&#39;_{n\\rightarrow k}+\\eta}{\\lambda&#39;_{.\\rightarrow k}+V\\eta} \\frac{\\lambda&#39;^{(d)}_{n\\rightarrow k}+\\alpha}{\\lambda&#39;^{(d)}_{-i}+K\\alpha} \\] where \\(z&#39;_{d, n}\\) refers to all other topic assignments; \\(\\lambda&#39;_{n\\rightarrow k}\\) is a count of how many other times that term has been assigned to topic \\(k\\); \\(\\lambda&#39;_{.\\rightarrow k}\\) is a count of how many other times that any term has been assigned to topic \\(k\\); \\(\\lambda&#39;^{(d)}_{n\\rightarrow k}\\) is a count of how many other times that term has been assigned to topic \\(k\\) in that particular document; and \\(\\lambda&#39;^{(d)}_{-i}\\) is a count of how many other times that term has been assigned in that document. Once \\(z_{d,n}\\) has been estimated, then estimates for the distribution of words into topics and topics into documents can be backed out. This conditional distribution assigns topics depending on how often a term has been assigned to that topic previously, and how common the topic is in that document (Steyvers and Griffiths (2006)). The initial random allocation of topics means that the results of early passes through the corpus of document are poor, but given enough time the algorithm converges to an appropriate estimate. 20.3.4 Warnings and extensions The choice of the number of topics, k, affects the results, and must be specified a priori. If there is a strong reason for a particular number, then this can be used. Otherwise, one way to choose an appropriate number is to use a test and training set process. Essentially, this means running the process on a variety of possible values for k and then picking an appropriate value that performs well. One weakness of the LDA method is that it considers a ‘bag of words’ where the order of those words does not matter (Blei (2012)). It is possible to extend the model to reduce the impact of the bag-of-words assumption and add conditionality to word order. Additionally, alternatives to the Dirichlet distribution can be used to extend the model to allow for correlation. For instance, in Hansard topics related the army may be expected to be more commonly found with topics related to the navy, but less commonly with topics related to banking. 20.4 Word embedding 20.5 Conclusion Using text as data is exciting because of the quantity and variety of text that is available to us. In general, dealing with text datasets is messy. There is a lot of cleaning and preparation that is typically required. Often text datasets are large. As such, having a workflow in place, in which you work in a reproducible way, simulating data first, and then clearly communicating your findings becomes critical, if only to keep everything organised in your own mind. Nonetheless, it is an exciting area, and I encourage you to regularly use text analysis where possible. In terms of next steps there are two, related, concerns: data and analysis. In terms of data there are many places to get large amounts of text data relatively easily, including: The r package rtweets makes it easy to get Twitter data (although typically this is going to be looking forward from when you start using it, rather than being able to look back). Plenty of people at U of T work with Twitter data including Jia Xue in the iSchool, and Ludovic Rheault in political science. The inside Airbnb dataset that we used earlier provides text from reviews. We’ve seen the gutenbergr package already in these notes, which provides easy access to text from Project Gutenberg. We’ve seen scraping of Wikipedia, but if you are going to do a bit of this then you may find it better to use a package, for instance WikipediR. In terms of analysis: Start by going through the tidytext book, tidytext, as it has a lot of nice explanations, code, and examples. It would then be worthwhile working through the Quanteda package quanteda tutorials. Finally, consider packages such as text2vec, and spacyr. References "],["cloud.html", "Chapter 21 Cloud 21.1 Introduction 21.2 Google Colab 21.3 AWS 21.4 Google Compute Engine 21.5 Azure", " Chapter 21 Cloud Required reading Recommended reading Edmondson, Mark, 2020, ‘googleComputeEngineR documentation’, version 0.3.0.9000, freely available at: https://cloudyr.github.io/googleComputeEngineR/. McDermott, Grant R., 2020, ‘Cloud computing with Google Compute Engine’, Data Science for Economists, freely available at: https://raw.githack.com/uo-ec607/lectures/master/14-gce/14-gce.html. Morris, Mitzi, 2020, ‘Stan Notebooks in the Cloud’, freely available at: https://mc-stan.org/users/documentation/case-studies/jupyter_colab_notebooks_2020.html. Key concepts/skills/etc Benefits/costs of cloud. Getting started in the cloud. Starting virtual machines with R Studio. Stopping virtual machines. 21.1 Introduction Cloud benefits: - Costs can be reduced, or more easily amortized. - Can scale as you need. - Many platforms are already sorted out e.g. R Studio just works. I stole this from someone and I can’t remember who, but the cloud is another name for ‘someone else’s computer’. That’s it. Nonetheless, learning to use someone else’s computer can be great for a number of reasons including: Scalability: It can be quite expensive to buy a new computer, especially if you only need it to run something every now and then, but by using someone else’s computer, you can just rent for a few hours or days. Portability: If you can shift your analysis workflow from your laptop to the cloud, then that suggests that you are likely doing good things in terms of reproducibility and portability. At the very least, your code is capable of running on your laptop and the cloud. Set-and-forget: If you are doing something that will take a while, then it can be great to not have to worry about your laptop’s fan running overnight, or your partner/baby/pet/housemate/etc accidently closing your computer, or not being able to watch Netflix on that same computer. When you use the cloud you are running your code on a ‘virtual machine’. This is a part of a larger bunch of computers that has been designed to act like a computer with specific features. For instance you may specify that your virtual machine has 8 GB RAM, 128 storage, and 4 CPUs. Your VM would then act like a computer with those specifications. The cost to use cloud options increases based on the specifications of the virtual machine that you choose. There are a few downsides: Cost: While most cloud options are cheap, they are rarely free. (While there are free options, they tend to not be very powerful, and so you end up having to pay to get a computer that is better than your laptop.) To give you an idea of cost, when I use AWS, I typically end up spending five to ten dollars for a couple of days. So it’s fairly cheap, but it’s not nothing. It’s also pretty easy to accidently forget about something and run up an unexpected bill, especially initially. Public: It is pretty easy to make mistakes and accidently make everything public. Time: It takes time to get set-up and comfortable on the cloud. In these notes we are going to introduce the cloud starting with some options that pretty much anyone can (and should) take advantage of: Google Colab; and then moving to more general cloud options including Google Compute Engine, AWS, and Azure, which may be useful to some of you in some cases. If you want to get a job in industry, then the advice of pretty much every speaker from industry at the Toronto Data Workshop is that you learn at least one of those cloud options. For instance, Munich Re is an Azure shop, Receptiviti uses AWS, etc. 21.2 Google Colab Google Colab is similar to R Studio Cloud, in that it is set-up to allow you to just log in and get started. In this case, you need a Google account. It’s better than R Studio because they have more resources to put into its development and you can use GPUs, but but on the other hand it is designed for Python, and while we can use it for R, it’s not really focused on that. To get started you need to tell Google Colab that you want to use R. You can do this by using this: https://colab.research.google.com/notebook#create=true&amp;language=r. At this point you have a Jupyter notebook open that will run R. (But it is not a R Markdown document.) You can install packages as normal, e.g. install.packages(\"tidyverse\"), and then call the package e.g. library(tidyverse). Google Colab is a good option if you have a good reason for using the broader capabilities that it has. If you want to go deeper into that then the Morris reading has a bunch of options that you can explore, but as Morris puts it ‘Colab is a gateway drug - for large-scale processing pipelines you’ll need to move up to Google Cloud Platform or one of its competitors AWS, Azure, etc.’ and that is what we will do now. 21.3 AWS Amazon Web Services is a cloud service from Amazon. To get started you need an AWS Developer account which you can create here: https://aws.amazon.com/developer/. After you have created an account, you need to select a region where the computer that you will access is located. After this, you will want to “Launch a virtual machine” (with EC2). The first step is to choose an Amazon Machine Image (AMI). This provides the details of the computer that you will be using. For instance, your local computer may be a MacBook running Catalina. Helpfully, Louis Aslett provides a bunch of these already set up - http://www.louisaslett.com/RStudio_AMI/. You can either select the code for the region that you registered for, or you can click on the link. The benefit of this AMI is that they are set-up specifically for R Studio, however the trade-off is that they are a little out-dated, as they were compiled in May 2019. In the next step you can choose how powerful the computer will be. The free tier has a fairly basic computer, but you can choose better ones when you need them. At this point you can pretty much just launch the instance. If you start using AWS more seriously then you should look into different security settings. Your instance is now running. You can go to it by pasting the ‘public DNS’ into a browser. The username is ‘rstudio’ and the password is your instance ID. You should have R Studio running, which is exciting. The first thing to do is probably to change the default password using the instructions in the instance. You don’t need to install, say, the tidyverse, instead you can just call the library and keep going. You can see the list of packages that are installed with installed.packages(). For instance, rstan is already installed. And you can use GPUs if you want. Perhaps as important as being able to start an AWS instance is being able to stop it (so that you don’t get billed). The free tier is pretty great, but you do need to turn it off. To stop an instance, in the AWS instances page, select it, then ‘Actions -&gt; Instance State -&gt; Terminate’. 21.4 Google Compute Engine The main R package related to Google Compute Engine seems to be: googleComputeEngineR. The reading from Grant McDermott is a pretty good walk-through. 21.5 Azure There are a bunch of R packages related to Azure here: https://github.com/Azure/AzureR. "],["deploy.html", "Chapter 22 Deploy 22.1 Introduction", " Chapter 22 Deploy Required reading Recommended reading Key concepts/skills/etc 22.1 Introduction "],["papers.html", "Chapter 23 Papers 23.1 ‘Mandatory Minimums’ 23.2 ‘These numbers mean dial it up’ 23.3 ‘The Short List’ 23.4 ‘Two Cathedrals’ 23.5 ‘A Proportional Response’ 23.6 ‘Mr Willis of Ohio’ 23.7 ‘Five Votes Down’ 23.8 ‘What’s next?’", " Chapter 23 Papers 23.1 ‘Mandatory Minimums’ 23.1.1 Task Working individually and in an entirely reproducible way, please find a dataset of interest on Open Data Toronto and write a short paper telling a story about the data. 23.1.2 Guidance Find a dataset of interest on Open Data Toronto and download it in a reproducible way using the R package opendatatoronto (Gelfand 2020). Create a folder with appropriate sub-folders, add it to GitHub, and then prepare a PDF using R Markdown with these sections (you are welcome to use this starter folder: https://github.com/RohanAlexander/starter_folder): title, author, date, abstract, introduction, data, and references. In the data section thoroughly and precisely discuss the source of the data and the bias this brings (ethical, statistical, and otherwise). Comprehensively describe and summarize the data using text and at least one graph and one table. Graphs must be made in ggplot (Wickham 2016) and tables must be made using knitr::kable() (with or without kableExtra) or gt (Iannone, Cheng, and Schloerke 2020b). Make sure to cross-reference graphs and tables. Use bibtex to add references. Be sure to reference R and any R packages you use, as well as the dataset. Check that you have referenced everything. Strong submissions will draw on related literature and would be sure to also reference those. There are various options in R Markdown for references style; just pick one that you are used to. Go back and write an introduction. This should be two or three paragraphs. The last paragraph should set out the remainder of the paper. Add an abstract. This should be three or four sentences. And then add a descriptive title (Hint: ‘Paper 1’ is not descriptive.) Add a link to your GitHub repo via a footnote. Check that your GitHub repo is well-organized, and add an informative README. (Hint: Comment. Your. Code.). Make sure that you’ve got at least one R script in there, in addition, to your R Markdown file. Pull this all together as a PDF and check that the paper is well-written and able to be understood by the average reader of, say, FiveThirtyEight. This means that you are allowed to use mathematical notation, but you must explain all of it in plain language. All statistical concepts and terminology must be explained. Your reader is someone with a university education, but not necessarily someone who understands what a p-value is - explain everything that you use. Check there is no evidence that this is a class assignment. Via Quercus, submit the PDF. 23.1.3 Check offs points Check you’ve not included any R code or raw R output in the final PDF. Check that although you’ll probably have most of your code in the R Markdown, make sure that you have at least one R script in the scripts folder. Check there is thoroughly commented code that directly creates your PDF. Do not knit to html and then save as a PDF. Do not knit to Word and then save as a PDF Check that your graph and discussion are extremely clear, and of comparable quality to those of FiveThirtyEight. Check that the date is updated. Check your entire workflow is entirely reproducible. Check for typos. 23.1.4 FAQ Can I use a dataset from Kaggle instead? No, because too many people use Kaggle datasets so employers are sick of them. I can’t use code to download my dataset, can I just manually download it? No, because your entire workflow needs to be reproducible. Please fix the download problem or pick a different dataset. How much should I write? Most students submit something in the two-to-six-page range, but it’s really up to you. Be precise and thorough. My data is about apartment blocks/NBA/League of Legends so there’s no ethical or bias aspect, what do I do? Please re-read the readings to better understand bias and ethics. If you really can’t think of something, then it might be worth picking a different dataset. Can I use Python? No. If you already know Python then it doesn’t hurt to learn another language. Why do I need to cite R, when I don’t need to cite Word? R is a free statistical programming language with academic origins so it’s appropriate to acknowledge the work of others. It’s also important for reproducibility. 23.1.5 Rubric Go/no-go #1: R is cited - [1 ‘Yes’, 0 ‘No’] Both referred to in the main content and included in the reference list. If not, no need to continue marking, just give paper 0 overall. Title - [2 ‘Exceptional’, 1 ‘Yes’, 0 ‘Poor or not done’] An informative title is included. Tell the reader what your story is - don’t waste their time. Ideally tell them what happens at the end of the story. ‘Problem Set X’ is not an informative title. There should be no evidence this is a school paper. Author, date, and repo - [2 ‘Yes’, 0 ‘Poor or not done’] The author, date of submission, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: ‘Code and data supporting this analysis is available at: LINK’). Abstract - [4 ‘Exceptional’, 3 ‘Great’, 2 ‘Fine’, 1 ‘Gets job done’, 0 ‘Poor or not done’] An abstract is included and appropriately pitched to a general audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). If your abstract is longer than four sentences then you need to think a lot about whether it is too long. It may be fine (there are always exceptions) but you should probably have a good reason. Your abstract must tell the reader your top-level finding. What is the one thing that we learn about the world because of your paper? Introduction - [4 ‘Exceptional’, 3 ‘Great’, 2 ‘Fine’, 1 ‘Gets job done’, 0 ‘Poor or not done’] The introduction is self-contained and tells a reader everything they need to know, including putting it into a broader context. Your introduction should provide a bit of broader context to motivate the reader, as well as providing a bit more detail about what you’re interested in, what you did, what you found, why it’s important, etc. A reader should be able to read only your introduction and have a good idea about the research that you carried out. It would be rare that you would have tables or figures in your introduction (again there are always exceptions but think deeply about whether yours is one). It must outline the structure of the paper. For instance (and this is just a rough guide) an introduction for a 10 page paper, should probably be about 3 or 4 paragraphs, or 10 per cent, but it depends on specifics. Data - [10 ‘Exceptional’, 8 ‘Great’, 6 ‘Good’, 4 ‘Some issues’, 2 ‘Many issues’, 0 ‘Poor or not done’] When you discuss the dataset (in the data section) you should make sure to discuss at least: The source of the data. The methodology and approach that is used to collect and process the data. The population, the frame, and the sample (as appropriate). Information about how respondents were found. What happened to non-response? What are its key features, strengths, and weaknesses about the survey generally. You should thoroughly discuss the variables in the dataset that you use. Are there any that are very similar that you nonetheless don’t use? Did you construct any variables by combining various ones? What do the data look like? Plot the actual data that you’re using (or as close as you can get to it). Discuss these plots and the other features of these data. These are just some of the issues strong submissions will consider. Show off your knowledge. If this becomes too detailed, then you should push some of this to footnotes or an appendix. ‘Exceptional’ means that when I read your submission I learn something about the dataset that I don’t learn from any other submission (within a reasonable measure of course). Numbering - [2 ‘Yes’, 0 ‘Poor or not done’] All figures, tables, equations, etc are numbered and referred to in the text. Proofreading - [2 ‘Yes’, 0 ‘Poor or not done’] All aspects of submission are free of noticeable typos. Graphs/tables/etc - [4 ‘Exceptional’, 3 ‘Great’, 2 ‘Fine’, 1 ‘Gets job done’, 0 ‘Poor or not done’] You must include graphs and tables in your paper and they must be to a high standard. They must be well formatted and camera-ready They should be clear and digestible. They must: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of labels/explanations, etc; and 3) appropriately sized and coloured (or appropriate significant figures in the case of stats). References - [4 ‘Perfect’, 3 ‘One minor issue’, 0 ‘Poor or not done’] All data/software/literature/etc are appropriately noted and cited. You must cite the software and software packages that you use. You must cite the datasets that you use. You must cite literature that you refer to (and you should refer to literature). If you take a small chunk of code from Stack Overflow then add the page in a comment next to the code. If you take a large chunk of code then cite it fully. 3 means one minor issue. More than one minor issues receives 0. Reproducibility - [4 ‘Exceptional’, 3 ‘Great’, 2 ‘Fine’, 1 ‘Gets job done’, 0 ‘Poor or not done’] The paper and analysis must be fully reproducible. A detailed README is included. All code should be thoroughly documented. An R project is used. Do not use setwd(). The code must appropriately reads data, prepares it, creates plots, conducts analysis, and generate documents. Seeds are used where needed. Code must have a preamble etc. You must appropriately document your scripts such that someone coming in could follow them. Your repo must be thoroughly organized. General excellence - [3 ‘Exceptional’, 2 ‘Wow’, 1 ‘Huh, that’s interesting’, 0 ‘None’] There are always students that excel in a way that is not anticipated in the rubric. This item accounts for that. 23.2 ‘These numbers mean dial it up’ 23.2.1 Task Please consider this scenario: ‘You are employed as a junior data scientist at Petit Poll - a Canadian polling company. Petit Poll has a contract with a ’client’ - an Ontario government department - to provide them with advice. In particular, the client wants to understand the effect of COVID shut-downs on restaurant businesses and has asked Petit Poll to design an experiment where some restaurants are shutdown.’ Working as part of a small team of 1-3 people, and in an entirely reproducible way, please decide on an intervention, and some measurement strategies, and then write a short paper telling a story about the effect of shut-downs on restaurants. 23.2.2 Guidance Working as part of a team of 1-3 people, prepare a PDF in R Markdown with the following features (you are welcome to use this starter folder: https://github.com/RohanAlexander/starter_folder): title, author/s, date, abstract, introduction, data, discussion, and references. In the data section you should specify the intervention and data gathering methodology, In the discussion section and any other relevant section, please be sure to discuss ethics and bias with reference to relevant literature. Decide on an intervention. Some aspects to address include: How will it be designed and implemented? What will be random about it? How will you ensure the separation of treatment and non-treatment? How long will it run? Decide on a survey methodology. Some aspects to address include: What is the population, frame, and sample? What sampling methods will you use and why? What are some of the statistical properties that the method brings to the table? How are you going to reach your desired respondents? How much do you estimate this will cost? What steps will you take to deal with non-response and how will non-response affect your survey? How are you going to protect respondent privacy? Remember to consider all of this in the context of your ‘client’ - for instance, what are they interested in? Develop a survey on a platform that was introduced in class. Be sure to test it yourselves. You will want to test this as much as possible, maybe even swap informally with another group? Now release the surveys into the (simulated) ‘field’. Please do this by simulating an appropriate number of responses to your survey in R. Don’t forget to simulate in relation to the intervention that you proposed. Do you need two, or even more, surveys? Show the results and discuss your ‘findings’. Everything must be entirely reproducible. You may wish to scrape some data and/or use open data sources to appropriately parameterize your simulations. Don’t forget to cite them when you do this. Use R Markdown to write a PDF report about all of this. Discuss your intervention, results and findings, your survey design and motivations, etc - all of it. You are writing a report that will eventually go to the client, so you must set the scene, and use language that demonstrates your command of statistical concepts but brings the reader along with you. Be sure to include graphs and tables and reference them in your discussion. Be sure to be clear about weaknesses and biases, and opportunities for future work. Your report must be well written. You are allowed to, and should, use mathematical notation, but you must explain all of it in plain language. Similarly, you can, and should, use experimental/survey/sampling/observational data terminology, but again, you need to explain it. Your graphs and tables must be of an incredibly high standard. Graphs and tables should be well formatted and report-ready. They should be clean and digestible. Furthermore, you should label and describe each table/figure. Your client has stats graduates working for it who need to be impressed by the main content of the report, but also has people who barely know what an average is and these people need to be impressed also. Your graphs must be of an extremely high standard. Check that you have referenced everything, including R, R packages, and datasets. Strong submissions will draw on related literature and would be sure to also reference those. The style of references does not matter, provided it is consistent. Via Quercus, submit your PDF report. You must provide a link to the GitHub repo where the code that you used for this assignment lives (hint: Comment. Your. Code.). Your entire workflow must be entirely reproducible. Your repo should be clearly organised and a useful README included. And you must include the R Markdown file that produced the PDF in that repo. Please be sure to include a link to your survey/s in your report and screenshots of the survey/s in the appendix of your report. Everyone in the team receives the same mark. There should be no evidence that this is a class assignment. 23.2.3 Check offs points 23.2.4 FAQ Can I work by myself? Yes. But I recommend forming a group and the workload for the course assumes you’ll work on the second and third paper as part of a group of four. Can we switch groups for the third paper? Yes. How can I find a group? I will randomly create groups of four in Quercus. You are welcome to shift out of those groups and form your own groups if you’d like. Can I get a different mark to the rest of my group? No. Everyone in the group gets the same mark. I wrote my paper by myself, so can I be graded on a different scale? No. All papers are graded in the same way. How much should I write? Most students submit something in the 10-to-15-page range, but it’s really up to you. Be precise and thorough. 23.3 ‘The Short List’ 23.3.1 Task Working as part of a small team of 1-3 people, and in an entirely reproducible way, please pick a paper to reproduce from an approved list and then write a short paper telling a story based on this. Your story should both talk about the (reproduced) findings, but also (a bit more ‘meta’) about what you learnt from the process. 23.3.2 Guidance Working as part of a team of 1-3 people, prepare a PDF in R Markdown with the following features: title, author/s, date, abstract, introduction, data, model, results, discussion, and references. In the discussion section and any other relevant section, please be sure to discuss ethics and bias with reference to relevant literature. You should reproduce one of the following papers: Liran Einav, Amy Finkelstein, Tamar Oostrom, Abigail Ostriker, Heidi Williams, 2020, ‘Screening and Selection: The Case of Mammograms’, American Economic Review. Pons, Vincent, 2018, ‘Will a Five-Minute Discussion Change Your Mind? A Countrywide Experiment on Voter Choice in France’ American Economic Review. Barari, Soubhik, Christopher Lucas, and Kevin Munger, 2021, ‘Political Deepfake Videos Misinform the Public, But No More than Other Fake Media’, 13 January, https://osf.io/cdfh3/. Others TBD If you have a favourite paper and want to reproduce it, then please submit it to me for consideration before Reading Week. You should follow the lead of the author/s of the paper you’re reproducing, but thoroughly think about, and discuss, what is being done. Regardless of the particular model that you are using, and the (possibly lack of) extent to which this is done in the paper, your model must be well explained, thoroughly justified, explained as appropriate to the task at hand, and the results must be beautifully described. You must include a DAG (probably in the model section). You must have a discussion of power and experimental design (probably in the data section) Your paper must be well-written, draw on relevant literature, and show your statistical skills by explaining all statistical concepts that you draw on. You are welcome to use appendices for supporting, but not critical, material. Your discussion must include sub-sections that focus on three or four interesting points, and also sub-sections on weaknesses and next steps. In your report you must provide a link to a GitHub repo that fully contains your analysis. Your code must be entirely reproducible, documented, and readable. Your repo must be well-organised and appropriately use folders. Your graphs and tables must be of an incredibly high standard. Graphs and tables should be well formatted and report-ready. They should be clean and digestible. Furthermore, you should label and describe each table/figure. When you discuss the dataset (in the data section) you should make sure to discuss (at least): Its key features, strengths, and weaknesses generally. A discussion of the questionnaire - what is good and bad about it? A discussion of the methodology including how they find people to take the survey; what their population, frame, and sample were; what sampling approach they took and what some of the trade-offs may be; what they do about non-response; the cost. A discussion of the intervention and experimental design. These are just some of the issues strong submissions will consider. Show off your knowledge. If this becomes too detailed then you should push some of this to footnotes or an appendix. When you discuss your model (in the model section), you must be extremely careful to spell out the statistical model that you are using, defining and explaining each aspect and why it is important. (For a Bayesian model, a discussion of priors and regularization is almost always important.) You should mention the software that you used to run the model. You should be clear about model convergence, model checks, and diagnostic issues. How do the sampling and survey aspects that you discussed assert themselves in the modelling decisions that you make? Again, if it becomes too detailed then push some of the details to footnotes or an appendix. You have the original paper to guide you, but you’ll likely need to go well-beyond what is included. You should present model results, graphs, figures, etc, in the results section. This section should strictly relay results. Interpretation of these results and conclusions drawn from the results should be left for the discussion section. Your discussion should focus on your model results. Interpret them and explain what they mean. Put them in context. What do we learn about the world having understood your model and its results? What caveats could apply? To what extent does your model represent the small world and the large world (to use the language of McElreath, Ch 2)? What are some weaknesses and opportunities for future work? Additionally, as this is a reproduction you should include a sub-section on differences you found and difficulties that you had. Check that you have referenced everything. Strong submissions will draw on related literature in the discussion (and other sections) and would be sure to also reference those. The style of references does not matter, provided it is consistent. As a team, via Quercus, submit a PDF of your paper. Again, in your paper you must have a link to the associated GitHub repo. And you must include the R Markdown file that produced the PDF in that repo. And you must include the R Markdown file that produced the PDF in that repo. The repo must be well-organized and have a detailed README. A good way to work as a team would be to split up the work, so that one person is doing each section. The people doing the sections that rely on data (such as the analysis and the graphs) could just simulate it while they are waiting for the person putting together the data to finish. It is expected that your submission be well written and able to be understood by the average reader of say 538. This means that you are allowed to use mathematical notation, but you must be able to explain it all in plain English. Similarly, you can (and hint: you should) use survey, sampling, observational, and statistical terminology, but again you need to explain it. Your work should have flow and should be easy to follow and understand. To communicate well, anyone at the university level should be able to read your report once and relay back the methodology, overall results, findings, weaknesses and next steps without confusion. Everyone in the team receives the same mark. There should be no evidence that this is a class assignment. 23.3.3 Check offs points 23.3.4 FAQ Do I have to stay in the same group as the second paper? No. You’re welcome to change. However, it’s important that you don’t change the second paper group on Quercus - be sure to only change the third paper group. Can we switch groups for the third paper? Yes. How much should I write? Most students submit something in the 10-to-15-page range, but it’s really up to you. Be precise and thorough. My paper doesn’t have a DAG, what do I do? You need to make the DAG. 23.4 ‘Two Cathedrals’ 23.4.1 Task Working individually, please conduct original research that applies methods from statistics to a question that involves an experiment. 23.4.2 Guidance You have various options for topics (pick one): - Develop a research question that is of interest to you and obtain or create a relevant dataset. This option involves developing your own research question based on your own interests, background, and expertise. I encourage you to take this option, but please discuss your plans with me. How does one come up with ideas? One way is to be question-driven, where you keep an informal log of small ideas, questions, and puzzles, that you have as you’re reading and working. Often, after dwelling on it for a while you can manage to find some questions of interest. Another way is to be data-driven - try to find some interesting dataset and then work backward. Finally, yet another way, is to be methods-driven - let’s say that you happen to understand Gaussian processes, then just apply that expertise. - Others TBA You should know the expectations by now. If you need a refresher then review the past problem sets. But essentially: Everything is entirely reproducible. Your paper must be written in R Markdown. Your paper must have the following sections: Title, date, author, keywords, abstract, introduction, data, model, results, discussion, appendix (optional, for supporting, but not critical, material), and a reference list. Your paper must be well-written, draw on relevant literature, and show your statistical skills by explaining all statistical concepts that you draw on. The discussion needs to be substantial. For instance, if the paper were 10 pages long then a discussion should be at least 2.5 pages. In the discussion, the paper must include subsections on weaknesses and next steps - but these must be in proportion. The report must provide a link to a GitHub repo that contains everything (apart from any raw data that you git ignored if it is not yours to share). The code must be entirely reproducible, documented, and readable. The repo must be well-organised and appropriately use folders and README files. 23.4.3 Peer review submission My expectations for this paper are very high. I’m very excited to read what you submit. To help you achieve this standard, there is an initial ‘submission’ where you can get comments and feedback and then the final, actual, submission. Submit initial materials for peer-review. As an individual, via Quercus, submit a PDF of your rough draft on Quercus. At a minimum this must include: All top-matter (title, author (you can use a pseudonym if you want), date, keywords, abstract) completely filled out. A fully written Introduction section. All other sections must be present in your paper, but don’t have to be filled out (e.g. you must have a ‘Data’ heading, but you don’t need to have content in that section). To be clear - it is fine to later change any aspect of what you submit at this check-point. You will be awarded two percentage points just for submitting a draft that meets this minimum. The point of this is to get feedback on your work (and to make sure you have at least started thinking about this project) so you are more than welcome to include other sections that you wish to get feedback on. There will be no extensions granted for this submission since the following submission is dependent on this date. 23.4.4 Conduct peer-review As an individual, you will randomly be assigned a handful of rough drafts to provide feedback. You have three days to provide feedback to your peers. If you provide feedback to one peer you will receive one percentage point, if you provide feedback to two peers you will receive two percentage points, if you provide feedback to three (or more) peers you will receive the full three percentage points. Your feedback must include at least five comments (meaningful/useful bullet points). These must be well-written and thoughtful. There will be no extensions granted for this submission since the following submission is dependent on this date. Please remember that you are providing feedback here to help your colleagues. All comments should be professional and kind. It is challenging to receive criticism. Please remember that your goal here is to help your peers advance their writing/analysis. Any feedback that is inappropriate or not up to standard will receive a 0 and cannot be redeemed later. 23.4.5 Check offs points 23.4.6 FAQ Can I work as part of a team? No. It’s important that you have some work that is entirely your own. You really need your own work to show off for job applications etc. How much should I write? Most students submit something in the 10-to-15-page range, but it’s really up to you. Be precise and thorough. 23.5 ‘A Proportional Response’ 23.5.1 Task Working in teams of one to four people, please consider this scenario: ‘You are employed as a junior statistician at Petit Poll - a Canadian polling company. Petit Poll has a contract with a Canadian political party to provide them with monthly polling updates.’ Working as part of a small team of 1-4 people, and in an entirely reproducible way, please write a short paper that tells the client a story about their standing. 23.5.2 Recommended steps Please pick a political party that you are ‘working for’, and pick a geographic focus: 1) the overall election, 2) a particular province, or 3) a specific riding. Then decide on a survey methodology (hint: p. 13 of Wu &amp; Thompson provides a handy checklist). Some questions you should address include: What is the population, frame, and sample? What sampling methods will you use and why (e.g. you could choose SRSWOR, stratified, etc). What are some of the statistical properties that the method brings to the table (e.g. for SRSWOR you could discuss Wu &amp; Thompson, Theorem 2.2, etc, as appropriate)? How are you going to reach your desired respondents? How much do you estimate this will cost? What steps will you take to deal with non-response and how will non-response affect your survey? How are you going to protect respondent privacy? Remember to consider all of this in the context of your ‘client’ - for instance, who would be more interested in Alberta ridings: Bloc Québécois or the Conservatives? Who likely has more money to spend - the Liberals or the Greens? Develop a survey on a platform that was introduced in class. Be sure to test it yourselves. You will want to test this as much as possible, maybe even swap informally with another group? Now release the surveys into the (simulated) ‘field’. Please do this by simulating an appropriate number of responses to your survey in R. Don’t forget to simulate in relation to the survey methodology that you proposed. Show the results and discuss your ‘findings’. Everything must be entirely reproducible. You may like to consider linking your survey ‘responses’ with other data such as the census or GSS. Use R Markdown to write a PDF report about all of this. Discuss your results and findings, your survey design and motivations, etc - all of it. You are writing a report that will eventually go to the ‘client’, so you must set the scene, and use language that demonstrates your command of statistical concepts but brings the reader along with you. Be sure to include graphs and tables and reference them in your discussion. Be sure to be clear about weaknesses and biases, and opportunities for future work. Your report must be well written. You are allowed to, and should, use mathematical notation, but you must explain all of it in plain english. Similarly, you can, and should, use surveys/sampling/observational data terminology, but again, you need to explain it. Your report must include at least the following aspects: title, date, authorship, non-technical executive summary, introduction, survey methodology, results, discussion, appendices that detail the survey, and references. Your ‘client’ has stats graduates working for it who need to be impressed by the main content of the report, but also has people who barely know what an average is and these people need to be impressed also. This is why your report should include a non-technical executive summary. In terms of length, this would typically be roughly 10 per cent of the report. It would be more detailed than an introduction, but still at a high level. Your graphs must be of an extremely high standard. Check that you have referenced everything. Strong submissions will draw on related literature in the discussion and would be sure to also reference those. The style of references does not matter, provided it is consistent. Via Quercus, submit a link to your PDF report which is hosted on GitHub. At some point in the introduction to your report, you must provide a link to the GitHub repo where the code that you used for this assignment lives (Hint: Comment. Your. Code.). Your entire workflow must be entirely reproducible. Please be sure to include a link to your survey in your report and screenshots of the survey in the appendix of your report. There should be no evidence that this is a class assignment. 23.5.3 Check offs points 23.5.4 FAQ 23.6 ‘Mr Willis of Ohio’ 23.6.1 Task Working in teams of one to four people, and in an entirely reproducible way, please use the Canadian General Social Survey (GSS) and a regression model to tell a story. 23.6.2 Recommended steps Depending on your focus and background, you may like to use a Bayesian hierarchical model, but regardless of the particular model that you use it must be well explained, thoroughly justified, appropriate to the task at hand, and the results must be beautifully described. You may focus on any year, aspect, or geography that is reasonable given the focus and constraints of the GSS. As a reminder, the GSS ‘program was designed as a series of independent, annual, cross-sectional surveys, each covering one topic in-depth.’ So please consider the topic and the year. The GSS is available to University of Toronto students via the library. In order to use it you need to clean and prepare it. Code to do this for one year is being distributed alongside this problem set and was discussed in lectures. You are welcome to simply use this code and this year, but the topic of that year will constrain your focus. Naturally, you are welcome to adapt the code to other years. If you use the code exactly as is then you must cite it. If you adapt the code then you don’t have to cite it, as it has a MIT license, but it would be appropriate to at least mention and acknowledge it, depending on how close your adaption is. Using R Markdown, please write a paper about your analysis and compile it into a PDF. Your paper must be well-written, draw on relevant literature, and show your statistical skills by explaining all statistical concepts that you draw on. Your paper must have the following sections: title, name/s, and date, abstract, introduction, data, model, results, discussion, and references. You are welcome to use appendices for supporting, but not critical, material. Your discussion must include sub-sections on weaknesses and next steps. In your report you must provide a link to a GitHub repo that fully contains your analysis. Your code must be entirely reproducible, documented, and readable. Your repo must be well-organised and appropriately use folders. Your graphs and tables must be of an incredibly high standard. Graphs and tables should be well formatted and report-ready. They should be clean and digestible. Furthermore, you should label and describe each table/figure. When you discuss the dataset (in the data section) you should make sure to discuss (at least): Its key features, strengths, and weaknesses generally. A discussion of the questionnaire - what is good and bad about it? A discussion of the methodology including how they find people to take the survey; what their population, frame, and sample were; what sampling approach they took and what some of the trade-offs may be; what they do about non-response; the cost. This is just some of the issues strong submissions will consider. Show off your knowledge. If this becomes too detailed then you should push some of this to footnotes or an appendix. When you discuss your model (in the model section), you must be extremely careful to spell out the statistical model that you are using, defining and explaining each aspect and why it is important. (For a Bayesian model, a discussion of priors and regularization is almost always important.) You should mention the software that you used to run the model. You should be clear about model convergence, model checks, and diagnostic issues. How do the sampling and survey aspects that you discussed assert themselves in the modelling decisions that you make? Again, if it becomes too detailed then push some of the details to footnotes or an appendix. You should present model results, graphs, figures, etc, in the results section. This section should strictly relay results. Interpretation of these results and conclusions drawn from the results should be left for the discussion section. Your discussion should focus on your model results. Interpret them and explain what they mean. Put them in context. What do we learn about the world having understood your model and its results? What caveats could apply? To what extent does your model represent the small world and the large world (to use the language of McElreath, Ch 2)? What are some weaknesses and opportunities for future work? Check that you have referenced everything. Strong submissions will draw on related literature in the discussion (and other sections) and would be sure to also reference those. The style of references does not matter, provided it is consistent. As a team, via Quercus, submit a PDF of your paper. Again, in your paper you must have a link to the associated GitHub repo in an appendix. And you must include the R Markdown file that produced the PDF in that repo. A good way to work as a team would be to split up the work, so that one person is doing each section. The people doing the sections that rely on data (such as the analysis and the graphs) could just simulate it while they are waiting for the person putting together the data to finish. It is expected that your submission be well written and able to be understood by the average reader of say 538. This means that you are allowed to use mathematical notation, but you must be able to explain it all in plain English. Similarly, you can (and hint: you should) use survey, sampling, observational, and statistical terminology, but again you need to explain it. Your work should have flow and should be easy to follow and understand. To communicate well, anyone at the university level should be able to read your report once and relay back the methodology, overall results, findings, weaknesses and next steps without confusion. 23.6.3 Check offs points It is recommended that you (informally) proofread one another’s sections - why not exchange papers with another group? Everyone in the team receives the same mark. There should be no evidence that this is a class assignment. 23.6.4 FAQ 23.7 ‘Five Votes Down’ 23.7.1 Task The primary goal of this paper is to predict the overall popular vote of the 2020 American presidential election using multilevel regression with post-stratification. 23.7.2 Recommended steps We expect you to work as part of a group of 4 people, but groups of size 1-4 are fine. We have suggested a split of the work based on a 4-person group, but these are just suggestions. Individual-level survey data: Request access to the Democracy Fund + UCLA Nationscape ‘Full Data Set’: https://www.voterstudygroup.org/publication/nationscape-data-set. This could take a day or two. Please start early. Given the expense of collecting this data, and the privilege of having access to it, if you don’t properly cite this dataset then you will get zero for this problem set. Once you have access then pick a survey of interest. We will use “ns20200102.dta” in the example (your number may be different). This will be a large file and is not yours to share. Do not push it to GitHub (use the .gitignore file - see here: https://carpentries-incubator.github.io/git-Rstudio-course/02-ignore/index.html). Use the example R code to get started preparing this dataset, and then go on cleaning and preparing it based on what you need. Make graphs and tables about the survey data and write beautiful sentences and paragraphs explaining everything. Post-stratification data: We will use the American Community Surveys (ACS). Please create an account with IPUMS: https://usa.ipums.org/usa/index.shtml You want the 2018 1-year ACS. Then you need to select some variables. This will depend on what you want to model and the survey data, but some options include: REGION, STATEICP, AGE, SEX, MARST, RACE, HISPAN, BPL, CITIZEN, EDUC, LABFORCE, INCTOT. Have a look around and see what you are interested in, remembering that you will need to establish a correspondence to the survey. Download the relevant post-stratification data (it’s probably easiest to change the data format to .dta). Again, this can take some time. Please start this early. This will be a large file and is not yours to share. Do not push it to GitHub (use the .gitignore file - see here: https://carpentries-incubator.github.io/git-Rstudio-course/02-ignore/index.html). Given the expense of collecting this data, and the privilege of having access to it, if you don’t properly cite this dataset then you will get zero for this problem set. Clean and prepare the post-stratification dataset. Remember that you need cell counts for the sub-populations in your model. See examples in the readings. (It may be efficient to start with simulated data while waiting for the real data) Modelling. You will want to explain vote intention based on a variety of explanatory variables. Construct the vote intention variable so that it is binary (either ‘supports Trump’ or ‘supports Biden’). You are welcome to use lm() but you would need to explain the nuances of this decision in the model section (Hint: start here: https://statmodeling.stat.columbia.edu/2020/01/10/linear-or-logistic-regression-with-binary-outcomes/). That said, you should probably use logistic regression if it is at all possible for you. If you don’t know where to start then look at (in increasing levels of complexity) glm(), lme4::glmer(), or brms::brm(). There are examples of each in the readings. Think very deeply about model fit, diagnostics, and other similar things that you need in order to convince someone that your model is appropriate. You have flexibility of the model that you use, (and hence the cells that you’ll need to create next). In general, the more cells the better, but you may want fewer cells for simplicity in the writing process and to ensure a decent sample in each cell. Apply your trained model to the post-stratification dataset to make the best estimate of the election result that you can. The specifics will depend on your modelling approach but will likely involve predict(), add_predicted_draws(), or similar. See the examples in the readings. We are primarily interested in the distribution of your forecast of the overall Presidential popular vote, and how the explanatory variables affect this. But great submissions would go beyond that. Also, you’re taking a statistics course, so if you just gave a central estimate and nothing else, then that would not be great. Create beautiful graphs and tables of your model and results. Create wonderful paragraphs talking about and explaining everything. (Again, it’s probably efficient to start with simulated data/results while waiting) Write up. Using R Markdown, please write a very thorough paper about your analysis and compile it into a PDF. The paper must be well-written, draw on relevant literature, and show your statistical skills by explaining all statistical concepts that you draw on. The paper must have the following sections: title, name/s, and date, abstract and keywords, introduction, data, model, results, discussion, and references. The paper may use appendices for supporting, but not critical, material. The discussion needs to be substantial. For instance, if the paper were 10 pages long then a discussion should be at least 2.5 pages. In the discussion, the paper must include subsections on weaknesses and next steps - but these must be in proportion. The report must provide a link to a GitHub repo that contains everything (apart from the raw data that you git ignored because it is not yours to share). The code must be entirely reproducible, documented, and readable. The repo must be well-organised and appropriately use folders and README files. The graphs and tables must be of an incredibly high standard, well formatted, and report-ready. They should be clean and digestible. Furthermore, you should label and describe each table/figure. When you discuss the datasets (in the data section) (remember there will be at least two datasets to discuss) you should make sure to discuss (at least): Their key features, strengths, and weaknesses generally. The survey questionnaire - what is good and bad about it? A discussion of the methodology including how they find people to take the survey; what their population, frame, and sample were; what sampling approach they took and what some of the trade-offs may be; what they do about non-response; the cost. This is just some of the issues strong submissions will consider. Show off your knowledge. If this becomes too detailed then you should push some of this to footnotes or an appendix. The dataset section is probably an appropriate place to include an explanation of what post-stratification is (in non-statistical language) and the strengths and weaknesses of it, although this discussion may fit more naturally in another section. Regardless, be sure to justify the inclusion of each explanatory variable. When you discuss your model (in the model section), you must be extremely careful to spell out the statistical model that you are using, defining and explaining each aspect and why it is important. (For a Bayesian model, a discussion of priors and regularization is almost always important.) You should mention the software that you used to run the model. You should be clear about model convergence, model checks, and diagnostic issues, although you may push the details of this to an appendix depending on how detailed you get. How do the sampling and survey aspects that you discussed assert themselves in the modelling decisions that you make? How can you convince a reader that you’ve neither overfit nor underfit the data? Again, if it becomes too detailed then push some of the details to footnotes or an appendix. You should present model results, graphs, figures, etc, in the results section. This section should strictly relay results. It must include text explaining all of these and summary statistics and similar. However, interpretation of these results and conclusions drawn from the results should be left for the discussion section. Your discussion should focus on your model results, but this time interpreting them, and explaining what they mean. Put them in context. What do we learn about the world having understood your model and its results? What caveats could apply? To what extent does your model represent the small world and the large world (to use the language of McElreath, Ch 2)? What are some weaknesses and opportunities for future work? Who is going to win the election? How confident are you in that forecast? Do you have a small or large distribution? What could that mean? Are you more confident in certain states? Do certain explanatory variables carry more weight than others? Etc. Check that you have referenced everything. Strong submissions will draw on related literature in the discussion (and other sections) and would be sure to also reference those. The style of references does not matter, but it must be consistent. If you don’t cite R then you will get zero for this problem set. As a team, via Quercus, submit a PDF of your paper. Again, in your paper you must have a link to the associated GitHub repo. And you must include the R Markdown file that produced the PDF in that repo. The- R Markdown file must exactly produce the PDF. Don’t edit it manually ex post - that isn’t reproducible. A good way to work as a team would be to split up the work, so that one person is doing each section. The people doing the sections that rely on data (such as the analysis and the graphs) could just simulate it while they are waiting for the person putting together the data to finish. We have recommended a split above, but you do what works for you. 23.7.3 Check offs points It is expected that your submission be well written and able to be understood by the average reader of say 538. This means that you are allowed to use mathematical notation, but you must be able to explain it all in plain English. Similarly, you can (and hint: you should) use survey, sampling, observational, and statistical terminology, but again you need to explain it. The average person doesn’t know what a p-value is nor what a confidence interval is. You need to explain all of this in plain language the first time you use it. Your work should have flow and should be easy to follow and understand. To communicate well, anyone at the university level should be able to read your report once and relay back the methodology, overall results, findings, weaknesses and next steps without confusion. It is recommended that you (informally) proofread one another’s work - why not exchange papers with another group? Everyone in the team receives the same mark. There should be no evidence that this is a class assignment. 23.7.4 FAQ 23.8 ‘What’s next?’ 23.8.1 Task Please work individually. In this paper, you will conduct original research that applies methods from statistics to a question involving surveys, sampling or observational data. 23.8.2 Recommended steps You have various options for topics (pick one): Develop a research question that is of interest to you and obtain or create a relevant dataset. This option involves developing your own research question based on your own interests, background, and expertise. I encourage you to take this option, but please discuss your plans with me. How does one come up with ideas? One way is to be question-driven, where you keep an informal log of small ideas, questions, and puzzles, that you have as you’re reading and working. Often, after dwelling on it for a while you can manage to find some questions of interest. Another way is to be data-driven - try to find some interesting dataset and then work backward. Finally, yet another way, is to be methods-driven - let’s say that you happen to understand Gaussian processes, then just apply that expertise. (Thanks to Jack Bailey for this idea.) Build a MRP model based on the CES and a post-stratification dataset that you obtain, to identify how the 2019 Canadian Federal Election would have been different if ‘everyone’ had voted. What do we learn about the importance of turnout based on your model and results? (This option involves logistic regression in either frequentist or Bayesian settings.) Reproduce a paper. This means that you download the data and then write your own code (using their code and paper as a guide if it’s available) to try to get their results and then write up what you did and found. Options include: Angelucci, Charles, and Julia Cagé, 2019, ‘Newspapers in times of low advertising revenues’, American Economic Journal: Microeconomics, please see: https://www.openicpsr.org/openicpsr/project/116438/version/V1/view. (This option can be accomplished with just OLS. It is a ‘safe’ pick. I even already provided you with some code in class to get started - see the notes! ). Bailey, Michael A., Daniel J. Hopkins &amp; Todd Rogers, 2016, ‘Unresponsive and Unpersuaded: The Unintended Consequences of a Voter Persuasion Effort’, Political Behavior. Clark, Sam, 2019, ‘A General Age-Specific Mortality Model With an Example Indexed by Child Mortality or Both Child and Adult Mortality’, Demography, please see: https://github.com/sinafala/svd-comp. (This is an ambiguous pick!) Skinner, Ben, 2019, ‘Making the connection: Broadband access and online course enrollment at public open admissions institutions’, Research in Higher Education, please see: https://github.com/btskinner/oa_online_broadband_rep. Pons, Vincent, 2018, ‘Will a Five-Minute Discussion Change Your Mind? A Countrywide Experiment on Voter Choice in France’ American Economic Review. Valencia Caicedo, Felipe, 2019, ‘The Mission: Human Capital Transmission, Economic Persistence, and Culture in South America’, The Quarterly Journal of Economics, please see: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ML1155. If you have a favourite paper and want to reproduce it, then please let me know by the end of Week 12 so that I can check that it’s appropriate. Pretend that you work for Upworthy. Request the Upworthy dataset and then use it to evaluate the result of an A/B test. This request could take a week. Please plan ahead if you choose this option. Critique the following paper: AlShebli, Bedoor, Kinga Makovi &amp; Talal Rahwan, 2020, ‘The association between early career informal mentorship in academic collaborations and junior author performance’, Nature Communications. You should be able to download the data here: https://github.com/bedoor/Mentorship and the paper here: https://www.nature.com/articles/s41467-020-19723-8. For background and starting points for your critique, please see: https://statmodeling.stat.columbia.edu/2020/11/19/are-female-scientists-worse-mentors-this-study-pretends-to-know/ and https://danieleweeks.github.io/Mentorship/#summary. (This option involves extensive data exploration and thinking really hard about what they are trying to do and how they are doing it.) Use this post by Andrew Whitby - https://andrewwhitby.com/2020/11/24/contact-tracing-biased/ - as a starting point to explore biased sampling and its effects on what we know about COVID and how this affects public policy. (This option involves extensive simulation.) In ‘Bias Behind Bars’, data journalist Tom Cardoso finds that ‘(a)fter controlling for a number of variables, … Black and Indigenous inmates are more likely to get worse scores than white inmates, based solely on their race.’ The main story is here: https://www.theglobeandmail.com/canada/article-investigation-racial-bias-in-canadian-prison-risk-assessments/ The methodology discussion is here: https://www.theglobeandmail.com/canada/article-investigation-racial-bias-in-canadian-prisons-methodology/ The observational data is available here: https://www.theglobeandmail.com/files/editorial/News/nw-na-risk-1023/The_Globe_and_Mail_CSC_OMS_2012-2018_20201022235635.zip Your task is to follow the methodology that Tom published and attempt to replicate the results. Are you able to replicate them? Do the results change significantly under slightly different assumptions? (This option involves only frequentist logistic regression, although doing everything in a Bayesian setting would be lovely too). You should know the expectations by now. If you need a refresher then review the past problem sets. Everything is entirely reproducible. Your paper must be written in R Markdown. Your paper must have the following sections: Title, date, author, keywords, abstract, introduction, data, model, results, discussion, appendix (optional, for supporting, but not critical, material), and a reference list. Your paper must be well-written, draw on relevant literature, and show your statistical skills by explaining all statistical concepts that you draw on. The discussion needs to be substantial. For instance, if the paper were 10 pages long then a discussion should be at least 2.5 pages. In the discussion, the paper must include subsections on weaknesses and next steps - but these must be in proportion. The report must provide a link to a GitHub repo that contains everything (apart from any raw data that you git ignored if it is not yours to share). The code must be entirely reproducible, documented, and readable. The repo must be well-organised and appropriately use folders and README files. My expectations for this paper are very high. I’m very excited to read what you submit. To help you achieve this standard, there are two initial ‘submissions’ where you can get comments and feedback and then the final, actual, submission. Due dates: (Optional) December 9 11:59pm ET Submit initial materials for peer-review. As an individual, via Quercus, submit a PDF of your rough draft on Quercus by 11:59pm ET on Wednesday, December 9, 2020. At a minimum this must include: All top-matter (title, author (you can use a pseudonym if you want), date, keywords, abstract) completely filled out. A fully written Introduction section. All other sections must be present in your paper, but don’t have to be filled out (e.g. you must have a ‘Data’ heading, but you don’t need to have content in that section). To be clear - it is fine to later change any aspect of what you submit at this check-point. You will be awarded 1 percentage point just for submitting a draft that meets this minimum (that is 1 out of the 30 that are available for the final paper). If you don’t submit, then the percentage point will be pushed to part d). The point of this is to get feedback on your work (and to make sure you have at least started thinking about this project) so you are more than welcome to include other sections that you wish to get feedback on. There will be no extensions granted for this submission since the following submission is dependent on this date. (Optional) December 12 11:59pm ET Conduct peer-review. As an individual, on December 10, you will randomly be assigned a handful of rough drafts to provide feedback. You have until December 12, 2020 11:59pm ET to provide feedback to your peers. If you provide feedback to one peer you will receive 1 percentage point, if you provide feedback to two peers you will receive 2 percentage points, if you provide feedback to three (or more) peers you will receive the full 3 percentage points. You may complete this aspect whether or not you submitted something in part a). If you don’t complete it then the percentage points will be pushed to part d). Your feedback must include at least six comments (meaningful/useful bullet points). These must be well-written and thoughtful. There will be no extensions granted for this submission since the following submission is dependent on this date. Please remember that you are providing feedback here to help your colleagues. All comments should be professional and kind. It is challenging to receive criticism. Please remember that your goal here is to help your peers advance their writing/analysis. Any feedback that is inappropriate or not up to standard will receive a 0 and cannot be redeemed later. (Optional) December 16 11:59pm ET Submit materials for TA review. Submit a PDF to Quercus. The TA will provide high-level comments on December 17. At a minimum this must include: All top-matter. Fully written Introduction, Data, Model, and Results sections. All other sections must be present in your paper, but don’t have to be filled out (e.g. you must have a ‘Discussion’ heading, but you don’t need to have content in that section). To be clear - it is fine to later change any aspect of what you submit at this check-point. You receive 1 percentage point for submitting something that meets that minimum. If you don’t submit anything then this is pushed to the final paper. There are no extensions possible on this aspect. (Compulsory) December 20 11:59pm ET As an individual, via Quercus, submit a PDF of your paper. Again, in your paper, you must have a link to the associated GitHub repo. This submission will be graded based on a rubric that will be posted on Quercus and will be worth 25-30 percentage points depending on parts a) - c). 23.8.3 Check offs points 23.8.4 FAQ References "],["references-2.html", "References", " References "]]
