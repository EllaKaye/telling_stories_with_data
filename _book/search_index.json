[["index.html", "Telling Stories With Data Chapter 1 Introduction 1.1 Welcome 1.2 Structure 1.3 On telling stories 1.4 Telling stories with data 1.5 Acknowledgements 1.6 Contact", " Telling Stories With Data Rohan Alexander April 28, 2021 Chapter 1 Introduction Last updated: 25 April 2021. These notes are being actively developed. If you have comments or suggestions, or find mistakes, then please don’t hesitate to get in touch. Version: 0.0.0.9000. 1.1 Welcome Hi, I’m Rohan Alexander. You can find out more about me here. These are notes that I wrote to support my teaching at the University of Toronto across the Faculty of Information and the Department of Statistical Sciences. The focus is on using quantitative methods to tell stories with data. 1.2 Structure The parts of these notes are: Essentials Introduction Drinking from a fire hose R Essentials Workflow Communicate Static communication Interactive communication Hunt and gather Gather data Hunt data Farm data Clean Cleaning and preparing Storage and retrieval Dissemination and protection Model Exploratory data analysis It’s just a linear model Causality from observational data Multilevel regression with post-stratification Text as data Other Cloud Deploy 1.3 On telling stories Like many parents, when our child was born, one of the first things that my wife and I did regularly was read stories to him. In doing so we carried on a tradition that has occurred for millennia. Myths, fables, and fairy tales can be seen and heard all around us. Not only are they entertaining but they enable us to easily learn something about the world. While ‘The Very Hungry Caterpillar’ may seem quite far from the world of quantitative analysis, there are similarities. Both are trying to tell the reader a story. When conducting quantitative analysis we are trying to tell the reader a story that will convince them of something. It may be as exciting as predicting elections, as banal as increasing internet advertising click rates by 0.01 per cent, as serious as finding the cause of some disease, or as fun as forecasting the winner of a basketball game. In any case the key elements are the same. When writing fiction Wikipedia suggests there are five key elements: character, plot, setting, theme, and style. When we are conducting quantitative analysis we have analogous concerns: What is the data? Who generated it and how? What is the data trying to say? How can we let it say this? What is the broader context surrounding the data? Where and when was it generated? Could other data have been generated? What are we hoping others will see from this data? How can you convince them of this? In the past, certain elements of telling stories with quantitative data were easier. For instance, experimental design has a long and robust tradition within traditional applications such as agricultural and medical sciences, physics, and chemistry. Student’s t-distribution was identified by a chemist, William Sealy Gosset, who was working at Guinness and needed to assess the quality of the beer (Raju 2005)! It would have been possible for him to randomly sample the beer and change one aspect at a time. Indeed, many of the fundamental statistical methods that we use today were developed in an agricultural setting. In the settings for which they were developed it was typically possible to establish control groups, randomize, and easily deal with any ethical concerns. In such a setting any subsequent story that is told with the resulting data is likely to be fairly convincing. Unfortunately, such a set-up is rarely possible in modern applied statistics applications. On the other hand, there are many aspects that are easier today. For instance, we have well-developed statistical techniques, easier access to larger datasets, and open source statistical languages such as R. But the lack of ability to conduct traditional experiments means that we must turn to other aspects in order to tell a reader a convincing story about our data. These other aspects allow us to tell convincing stories even in the absence of a traditional experimental set-up. 1.4 Telling stories with data The aim of these notes is to equip you with everything you need to be able to write short(ish), technical, memos, that convince a reader of the story you are telling. These notes encourage research-based, independent learning. This means that you should develop your own questions and answer them to the extent that you can. We focus on methods that can provide convincing stories even when we cannot conduct traditional experiments. Importantly, these approaches do not rely on ‘big data’ – which is widely known by practitioners to not be a panacea (Meng and others 2018) – but instead on better using the data that are available. The purpose of the notes is to allow you to tell convincing stories using data and quantitative analysis. They blend theory and case studies to equip you to with practical skills, a sophisticated workflow, and an appreciation for how more-advanced methods build on what is covered here. Data science is multi-disciplinary. It takes the ‘best’ bits from fields such as statistics, data visualisation, programming, and experimental design (to name a few). As such, data science projects require a blend of these skills. These are hands-on notes in which you will learn these skills by conducting research projects using real-world data. This means that you will: obtain and clean relevant datasets; develop your own research questions; use statistical techniques to answer those questions; and communicate your results in a meaningful way. These notes were developed in collaboration with professional data scientists as well as academics from a variety of fields. They are designed around approaches that are used extensively in academia, government, and industry. Furthermore, they include many aspects, such as data cleaning and communication, that are critical, but rarely taught. However, these notes do not contain everything that you need. Your learning must be ‘active’ when using these notes because that is the way you will continue to learn through the rest of your life and career. You need to seek out additional information, critically evaluate it, and apply it to your situation. The workflow that we follow in these notes is: Research question development. Data collection. Data cleaning. Exploratory data analysis. Statistical modelling. Evaluation. Communication. Reproduce. All of these aspects are critical to being able to convince a reader of your story. Your ability to convince them of your story depends on the quality of all aspects of your workflow. If we were to expand on this workflow then we roughly get the chapters that are covered in these notes, although they are re-ordered as necessary. From the first chapter we will have a workflow (make a graph then write about it convincingly) that allows us to tell a convincing (albeit likely basic) story. In each subsequent chapter we add aspects and depth to our workflow that will allow us to speak with increasing sophistication and credibility. This workflow also aligns nicely with the skills that are sought in data scientists. For instance, Mango Solutions, a UK data science consultancy, describes ‘the six core capabilities of data scientists’ as: 1. communicator; 2. data-wrangler; 3. programmer; 4. technologist; 5. modeller; and 6. visualiser (“Data Science Radar: How to Identify World-Class Data Science Capabilities” 2020). These notes are also designed to enable you to build a portfolio of work that you could show to a potential employer. This is arguably the most important thing that you should be doing. (E. Robinson and Nolis 2020, 55) describe a portfolio as ‘a set of data science projects that you can show to people so they can see what kind of data science work you can do.’ They describe this as a ‘step [that] can really help you be successful.’ 1.4.1 Software The software that we use in these notes is R (R Core Team 2020). This language was chosen because it is open-source, widely used, general enough to cover the entire workflow, yet specific enough to have plenty of the tools that we need for statistical analysis built in. We do not assume that you have used R before, and so another reason for selecting R for these notes is the community of R users which is, in general, especially welcoming of new-comers and there are a lot of great beginner-friendly materials available. If you don’t have a programming language, then R is a great one to start with. If you have a preferred programming language already, then it wouldn’t hurt to pick up R as well. That said, if you have a good reason to prefer another open source programming language (for instance you use Python daily at work) then you may wish to stick with that. However, all examples in these notes are in R. Please download R and R Studio onto your own computer. You can download R for free here: http://cran.utstat.utoronto.ca/, and you can download R Studio Desktop for free here: https://rstudio.com/products/rstudio/download/#download. Please also create an account on R Studio Cloud: https://rstudio.cloud/. This will allow you to run R in the cloud, which will be helpful when we are getting started. 1.4.2 Assumed background These notes assume familiarity with first-year statistics. For instance, if you have a taken a course or two where you covered hypothesis testing and similar concepts then that should be enough. That said, enthusiasm and interest can take you pretty far, so if you’ve got those then don’t worry about too much else. 1.4.3 Structure These notes are structured around a fairly dense 12-week course. Each chapter contains a list of required reading, as well as a list of recommended reading for those who are interested in the topic and want a starting place for further exploration. All chapters contain a summary of the key concepts and skills that are developed in that chapter. Code and technical chapters additionally contain a list of the main packages and functions that are used in the chapter. Many of the chapters also have a pre-quiz. This is a short quiz that you should complete after doing the required readings, but before going through the chapter to test your knowledge. After completing the chapter, you should go back through the lists and the pre-quiz to make sure that you understand each aspect. There are problem sets throughout these notes. These are opportunities for you to conduct your own research on a topic that is of interest to you. Although the initial problem set requires you to use data from the Toronto Open Data Portal (https://open.toronto.ca/), after that you are able to use any appropriate dataset. Although open-ended research may be new to you, the extent to which you are able to develop your own questions, use quantitative methods to explore them, and communicate your story to a reader, is the true measure of the success of these notes. 1.5 Acknowledgements Many people gave generously of their time, code, and data to help develop these notes. Thank you to Monica Alexander, Michael Chong, and Sharla Gelfand for allowing their code to be used. Thank you to Kelly Lyons, Hareem Naveed, and Periklis Andritsos for helpful comments. Thank you to Greg Wilson for providing a structure to think about teaching. Thank you to Elle Côtè for enabling these notes to be written. These notes have greatly benefited from the notes and teaching materials of others that are freely available online, especially: Chris Bail - Text as Data; Scott Cunningham - Causal Inference: The Mixtape; Andrew Heiss - Program Evaluation for Public Service; Lisa Lendway - Advanced Data Science in R; Grant McDermott - Data Science for Economists; Nathan Matias - Designing Field Experiments at Scale; David Mimno - Text Mining for History and Literature Ed Rubin - PhD Econometrics (III) and Introduction to Econometrics (II); Thank you to the following students who identified specific improvements in these notes: A Mahfouz, Aaron Miller, Amy Farrow, Cesar Villarreal Guzman, Faria Khandaker, Flavia López, Hong Shi, Laura Cline, Lorena Almaraz De La Garza, Mounica Thanam, Reem Alasadi, Wijdan Tariq, and Yang Wu. Finally, thank you to the Winter 2020 and 2021 INF2178 and Fall 2020 Term STA304 students at the University of Toronto, whose feedback greatly improved all aspects. 1.6 Contact Any comments or suggestions on these notes would be welcomed. You can contact me: rohan.alexander@utoronto.ca. "],["drinking-from-a-fire-hose.html", "Chapter 2 Drinking from a fire hose 2.1 Hello world 2.2 Case study - Canadian elections 2.3 Case study - Toronto homelessness", " Chapter 2 Drinking from a fire hose Last updated: 30 January 2021. Required reading Barrett, Malcolm, 2021, ‘Data science as an atomic habit,’ 16 January, https://malco.io/2021/01/04/data-science-as-an-atomic-habit/. Gelfand, Sharla, 2019, ‘Cleaning up after the federal election,’ https://sharla.party/talk/2019-10-24-uoft-brown-bag/. Hao, Karen, 2019, ’This is how AI bias really happens - and why it’s so hard to fix, MIT Technology Review, 4 February, https://www.technologyreview.com/s/612876/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix/. Keyes, Os, 2019, ‘Counting the Countless,’ Real Life, 8 April, freely available at: https://reallifemag.com/counting-the-countless/. Required viewing Register, Yim, 2020, ‘Data Science Ethics in 6 Minutes,’ YouTube, 29 December, https://youtu.be/mA4gypAiRYU. Key libraries ggplot2 tidyverse Key concepts/skills/etc R is fun and allows you to accomplish really interesting projects. But like any language it is a slow path to mastery. The way to learn is to start with a really small project in mind, and break down the steps required to achieve it. Look at other people’s code to work out how you might deal with the steps. Copy, paste, and modify someone else’s code to achieve each step. Don’t worry about perfection, just worry about achieving each step. Complete that project and move onto the next project. Rinse and repeat. Each project you’ll get a little better. Key functions %&gt;% ‘pipe’ dplyr::arrange() dplyr::filter() dplyr::group_by() dplyr::mutate() dplyr::select() dplyr::summarise() dplyr::ungroup() ggplot::facet_wrap() ggplot::geom_histogram() Quiz According to Register, 2020, data decisions affect (pick one)? Real people. No one. Those in the training set. Those in the test set. In your own words, what is data science? According to Keyes, 2019, what is perhaps a more accurate accurate definition of data science (pick one)? ‘The inhumane reduction of humanity down to what can be counted.’; ‘The quantitative analysis of large amounts of data for the purpose of decision-making.’; ‘Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data.’ Imagine that you have a job in which including ‘race’ as an explanatory variable improves the performance of your model. What types of issues would you consider when deciding whether to include this variable in production? What if the variable was sexuality? Please be sure to refer to Mullainathan, 2019, in your answer. 2.1 Hello world To jump in we will get some data from the wild, make a graph with it, and then use this to tell a story. Some of the code may be a bit unfamiliar to you if it’s your first-time using R. It’ll all soon be familiar. But the only way to learn how to code is to code. Please try to get this working on your own computer, typing out (not copy/pasting) all the code that you need. It’s important and normal to realise that you’re going to be bad at this for a while. Whenever you’re learning a new tool, for a long time, you’re going to suck… But the good news is that is typical; that’s something that happens to everyone, and it’s only temporary. Hadley Wickham as quoted by Barrett (2021a). One of the great things about graphs is that sometimes this is all you need to have a convincing story, as Figure 2.1 from FT Visual &amp; Data Journalism team (2020) show. Figure 2.1: New confirmed cases of Covid-19 in United States, United Kingdom, Canada and Australia, as at 22 December 2020. In this section we are going to focus on making a table and a graph from our data. Although you will be guided thoroughly to achieve this, hopefully by seeing the power of quantitative analysis with R you will be motivated to stick with it when you run into difficulties later on. 2.2 Case study - Canadian elections Figure 2.2: The County Election (1851–52), by George Caleb Bingham (American, 1811 - 1879), as downloaded from https://artvee.com/dl/the-county-election. 2.2.1 Getting started To get started you should open a new R Markdown file (File -&gt; New File -&gt; R Markdown). As this is our first attempt at using R in the wild, we will just have everything in the one R Markdown document. (In later projects we will move to a more robust set-up.) Then you should create a new R code chunk (keyboard shortcut: Command + Option + I) and add some preamble documentation. I like to specify the purpose of the document, who the author is and their contact details, when the file was written or last updated, and pre-requisites that the file relies on. You may also like to include a license, and list outstanding issues or todos. Remember that in R, lines that start with ‘#’ are comments - they won’t run. #### Preamble #### # Purpose: Read in voting data from the 2019 Canadian Election and output a # dataset that can be used for analysis. # Author: Rohan Alexander # Email: rohan.alexander@utoronto.ca # Date: 9 January 2019 # Prerequisites: Need the text file from the Canadian elections website # Issues: # To do: After this I typically set-up my workspace. This usually involves installing and/or reading in any packages, and possibly updating them. Remember that you only need to install a package once for each computer. But you need to call it every time you want to use it. (Here I’ve added excessive comments so that you know what is going on and why - in general I wouldn’t explain what tidyverse is.) #### Workspace set-up #### install.packages(&quot;tidyverse&quot;) # Only need to do this once install.packages(&quot;janitor&quot;) # Only need to do this once install.packages(&quot;here&quot;) In this case we are going to use tidyverse Wickham (2017), janitor Firke (2020), and here Müller (2017a). #### Workspace set-up #### # tidyverse is a collection of packages # Try ?tidyverse to see more library(tidyverse) # Calls the tidyverse - need to do this each time. library(janitor) # janitor helps us clean datasets library(here) # here helps us to know where files are # update.packages() # You can uncomment this if you want to update your packages. 2.2.2 Get the data We read in the dataset from the Elections Canada website. We can actually pass a website to the read_tsv() function, which saves a lot of time. #### Read in the data #### # Read in the data using read_tsv from the readr package (part of the tidyverse) # The &#39;&lt;-&#39; is assigning the output of readr::read_tsv to a object called raw_data. raw_elections_data &lt;- readr::read_tsv(file = &quot;http://enr.elections.ca/DownloadResults.aspx&quot;, skip = 1) # There is some debris on the first line so we skip them. # We have read the data from the Elections Canada website. We may like to save # it just in case something happens and they move it. write_csv(raw_elections_data, here(&quot;inputs/data/canadian_voting.csv&quot;)) (Note that Elections Canada updates this link with the latest elections. When I run this on 31 December 2020, I get the results of a Toronto by-election. While I’ll certainly update these notes from time to time, it may be that there’s been an election between now and when you run these notes, so your specific results may be slightly different.) 2.2.3 Clean the data Now we’d like to clean the data so that we can use it. #### Basic cleaning #### raw_elections_data &lt;- read_csv(here(&quot;inputs/data/canadian_voting.csv&quot;)) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## `Electoral district number - Numéro de la circonscription` = col_character(), ## `Electoral district name` = col_character(), ## `Nom de la circonscription` = col_character(), ## `Type of results*` = col_character(), ## `Type de résultats**` = col_character(), ## `Surname - Nom de famille` = col_character(), ## `Middle name(s) - Autre(s) prénom(s)` = col_logical(), ## `Given name - Prénom` = col_character(), ## `Political affiliation` = col_character(), ## `Appartenance politique` = col_character(), ## `Votes obtained - Votes obtenus` = col_double(), ## `% Votes obtained - Votes obtenus %` = col_double(), ## `Rejected ballots - Bulletins rejetés***` = col_double(), ## `Total number of ballots cast - Nombre total de votes déposés` = col_double() ## ) # If you called the library (as we did) then you don&#39;t need to use this set-up # of janitor::clean_names, you could just use clean_names, but I&#39;m making it # explicit here, but won&#39;t in the future. cleaned_elections_data &lt;- janitor::clean_names(raw_elections_data) # One thing to notice for those who have a Stata background is that we just # overwrote the name - that&#39;s fine in R. # The pipe operator - %&gt;% - pushes the output from one line to be an input to the # next line. cleaned_elections_data &lt;- cleaned_elections_data %&gt;% # Filter to only have certain rows filter(type_of_results == &quot;validated&quot;) %&gt;% # Select only certain columns select(electoral_district_number_numero_de_la_circonscription, electoral_district_name, political_affiliation, surname_nom_de_famille, percent_votes_obtained_votes_obtenus_percent ) %&gt;% # Rename the columns to be a bit shorter rename(riding_number = electoral_district_number_numero_de_la_circonscription, riding = electoral_district_name, party = political_affiliation, surname = surname_nom_de_famille, votes = percent_votes_obtained_votes_obtenus_percent) head(cleaned_elections_data) ## # A tibble: 6 x 5 ## riding_number riding party surname votes ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 35108 Toronto Centre People&#39;s Party - PPC Bawa 1.1 ## 2 35108 Toronto Centre Free Party Canada Cappelletti 0.3 ## 3 35108 Toronto Centre NDP-New Democratic Party Chang 17 ## 4 35108 Toronto Centre Independent Clarke 0.5 ## 5 35108 Toronto Centre Liberal Ien 42 ## 6 35108 Toronto Centre Libertarian Komar 0.5 Finally we may like to save our cleaned dataset. #### Save #### readr::write_csv(cleaned_elections_data, &quot;outputs/data/cleaned_elections_data.csv&quot;) 2.2.4 Make a graph First we need to read in the dataset, we then filter the number of parties to a smaller number. #### Read in the data #### cleaned_elections_data &lt;- readr::read_csv(&quot;outputs/data/cleaned_elections_data.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## riding_number = col_double(), ## riding = col_character(), ## party = col_character(), ## surname = col_character(), ## votes = col_double() ## ) # Make a graph just considers Toronto riding cleaned_elections_data %&gt;% filter(party %in% c(&quot;Bloc Québécois&quot;, &quot;Conservative&quot;, &quot;Liberal&quot;, &quot;NDP-New Democratic Party&quot;) ) %&gt;% ggplot(aes(x = riding, y = votes, color = party)) + geom_point() + theme_minimal() + # Make the theme neater theme(axis.text.x = element_text(angle = 90, hjust = 1)) + # Change the angle labs(x = &quot;Riding&quot;, y = &quot;Votes (%)&quot;, color = &quot;Party&quot;) # Save the graph ggsave(&quot;outputs/figures/toronto_results.pdf&quot;, width = 40, height = 20, units = &quot;cm&quot;) 2.2.5 Make a table There are an awful lot of ways to make a table in R. First we’ll try the built-in function summary(). #### Read in the data #### cleaned_elections_data &lt;- readr::read_csv(&quot;outputs/data/cleaned_elections_data.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## riding_number = col_double(), ## riding = col_character(), ## party = col_character(), ## surname = col_character(), ## votes = col_double() ## ) #### Make some tables #### # Try some different default summary table summary(cleaned_elections_data) ## riding_number riding party surname ## Min. :35108 Length:15 Length:15 Length:15 ## 1st Qu.:35108 Class :character Class :character Class :character ## Median :35108 Mode :character Mode :character Mode :character ## Mean :35112 ## 3rd Qu.:35118 ## Max. :35118 ## votes ## Min. : 0.20 ## 1st Qu.: 0.55 ## Median : 3.60 ## Mean :13.34 ## 3rd Qu.:24.85 ## Max. :45.70 Now we can try a group_by() and summarise(). # Make our own cleaned_elections_data %&gt;% # Using group_by and summarise means that whatever summary statistics we # construct will be on a party basis. We could group_by multiple variables and # similarly, we could create a bunch of different other summary statistics. group_by(party) %&gt;% summarise(min = min(votes), mean = mean(votes), max = max(votes)) ## # A tibble: 9 x 4 ## party min mean max ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Conservative 5.7 23.8 41.8 ## 2 Free Party Canada 0.3 0.3 0.3 ## 3 Green Party 2.6 17.7 32.7 ## 4 Independent 0.5 0.55 0.6 ## 5 Liberal 42 43.8 45.7 ## 6 Libertarian 0.5 0.5 0.5 ## 7 NDP-New Democratic Party 5.8 11.4 17 ## 8 No Affiliation 0.2 0.2 0.2 ## 9 People&#39;s Party - PPC 1.1 2.35 3.6 2.3 Case study - Toronto homelessness Toronto has a large homeless population, and of course given the pandemic and winter it is critical that there are enough places in shelters. Unfortunately, as we will see in this case study, there are not enough places. However, the one good thing is that we have the data to see this is a problem. In this case study we are going to use data on the number of people in Toronto shelters to make a graph of usage. This will also introduce us to Tidy Tuesday! In my experience, people are most successful at ‘learning R’ when they are learning it to achieve something else. If you’re in a university course then that might be ‘pass the course,’ but often it’s nice to have some other projects. TidyTuesday is a weekly event in which the R community comes together around a dataset and shares code and approaches. You can learn more about it here: https://github.com/rfordatascience/tidytuesday. 2.3.1 Getting started Again, open a new R Markdown file: (File -&gt; New File -&gt; R Markdown). Update the details so that they reflect your own. Again, add some top matter with some comments and explanations of the code. #### Preamble #### # Purpose: Read in Toronto homelessness data and output a graph. # Author: Rohan Alexander # Email: rohan.alexander@utoronto.ca # Date: 22 December 2020 # Prerequisites: # Issues: # To do: I want to talk a little about the libraries this time. Libraries are bits of code that other people have written. There are a few common ones that you’ll see regularly, especially the tidyverse. To use a package we first have to install it and then we need to load it. Jenny Bryan has a wonderful analogy of installing a lightbulb - install.packages(\"tidyverse\"). You only need to do this once, but then if you want light then you need to turn on the switch - library(tidyverse). So because we installed everything earlier we won’t need to do it again, we can just call the library. #### Workspace set-up #### library(tidyverse) Given that a lot of people gave up their time to make R and the packages, it’s important to cite them. Luckily, it’s easy to get the information that you need to properly cite them. # To get the citation for R run: citation() ## ## To cite R in publications use: ## ## R Core Team (2020). R: A language and environment for statistical ## computing. R Foundation for Statistical Computing, Vienna, Austria. ## URL https://www.R-project.org/. ## ## A BibTeX entry for LaTeX users is ## ## @Manual{, ## title = {R: A Language and Environment for Statistical Computing}, ## author = {{R Core Team}}, ## organization = {R Foundation for Statistical Computing}, ## address = {Vienna, Austria}, ## year = {2020}, ## url = {https://www.R-project.org/}, ## } ## ## We have invested a lot of time and effort in creating R, please cite it ## when using it for data analysis. See also &#39;citation(&quot;pkgname&quot;)&#39; for ## citing R packages. # And to get the citation for a package run that function with the package name. For instance: citation(&#39;tidyverse&#39;) ## ## Wickham et al., (2019). Welcome to the tidyverse. Journal of Open ## Source Software, 4(43), 1686, https://doi.org/10.21105/joss.01686 ## ## A BibTeX entry for LaTeX users is ## ## @Article{, ## title = {Welcome to the {tidyverse}}, ## author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D&#39;Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani}, ## year = {2019}, ## journal = {Journal of Open Source Software}, ## volume = {4}, ## number = {43}, ## pages = {1686}, ## doi = {10.21105/joss.01686}, ## } # Again, don&#39;t worry too much about the details - we&#39;ll get into them later. 2.3.2 Get the data We are going to grab some data that has been made available about Toronto homeless shelters. Again, don’t worry too much about the details for now, but what we are saying here, is that there’s a CSV that has been made available to us on GitHub and this code downloads it to our own computer. After we download it we can quickly look at the data using head(). toronto_shelters &lt;- readr::read_csv( &#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-12-01/shelters.csv&#39; ) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## id = col_double(), ## occupancy_date = col_datetime(format = &quot;&quot;), ## organization_name = col_character(), ## shelter_name = col_character(), ## shelter_address = col_character(), ## shelter_city = col_character(), ## shelter_province = col_character(), ## shelter_postal_code = col_character(), ## facility_name = col_character(), ## program_name = col_character(), ## sector = col_character(), ## occupancy = col_double(), ## capacity = col_double() ## ) head(toronto_shelters) ## # A tibble: 6 x 13 ## id occupancy_date organization_name shelter_name shelter_address ## &lt;dbl&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 2017-01-01 00:00:00 COSTI Immigrant Ser… COSTI Receptio… 100 Lippincott… ## 2 2 2017-01-01 00:00:00 Christie Ossington … Christie Ossin… 973 Lansdowne … ## 3 3 2017-01-01 00:00:00 Christie Ossington … Christie Ossin… 973 Lansdowne … ## 4 4 2017-01-01 00:00:00 Christie Refugee We… Christie Refug… 43 Christie St… ## 5 5 2017-01-01 00:00:00 City of Toronto Birchmount Res… 1673 Kingston … ## 6 6 2017-01-01 00:00:00 City of Toronto Birkdale Resid… 1229 Ellesmere… ## # … with 8 more variables: shelter_city &lt;chr&gt;, shelter_province &lt;chr&gt;, ## # shelter_postal_code &lt;chr&gt;, facility_name &lt;chr&gt;, program_name &lt;chr&gt;, ## # sector &lt;chr&gt;, occupancy &lt;dbl&gt;, capacity &lt;dbl&gt; 2.3.3 Make a graph The dataset is on a daily basis for each shelter. What I’d like to do is to get an overall picture of the availability of shelter places in Toronto. This code is based on code by Florence Vallée-Dubois1 and Lisa Lendway2. Now, I’d like to first do some data manipulation before we graph it this time. I’m going to introduce a few new functions here that we’ll see a lot more soon. Again, don’t worry if this doesn’t all make sense right now. You’re learning a brand-new language! Just try to focus on picking up a word or two and staying motivated! Finally, because it’s Christmas, we can grab a seasonally-appropriate theme from my colleague Liza Bolton. # In contrast to the earlier packages, which were located in a central repository # of packages called CRAN, Liza&#39;s is available on her GitHub. Again, don&#39;t worry # about the details for now, it&#39;ll all be clarified later. devtools::install_github(&quot;elb0/decemberLB&quot;, ref = &quot;main&quot;) ## Skipping install of &#39;decemberLB&#39; from a github remote, the SHA1 (2cc38a25) has not changed since last install. ## Use `force = TRUE` to force installation # Once it&#39;s installed we call the library as usual. library(decemberLB) toronto_shelters %&gt;% tidyr::drop_na(occupancy, capacity) %&gt;% # We only want rows that have data group_by(occupancy_date, sector) %&gt;% # We want to know the occupancy by date and sector summarise(the_sum = sum(occupancy), the_capacity = sum(capacity), the_usage = the_sum / the_capacity, .groups = &#39;drop&#39;) %&gt;% ggplot(aes(x = occupancy_date, y = the_usage, color = sector)) + geom_smooth(aes(group = sector), se = FALSE) + scale_y_continuous(limits = c(0, NA)) + labs(color = &quot;Type&quot;, x = &quot;Date&quot;, y = &quot;Occupancy rate&quot;, title = &quot;Toronto shelters&quot;, subtitle = &quot;Occupancy per day&quot;) + theme_minimal() + scale_color_december(palette = &quot;xmas&quot;) ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; https://github.com/florencevdubois/MyTidyTuesdays/blob/master/2020.12.01.R↩︎ https://github.com/llendway/tidy_tuesday_in_thirty/blob/main/2020_12_01_tidy_tuesday.Rmd↩︎ "],["r-essentials.html", "Chapter 3 R Essentials 3.1 R essentials 3.2 Social impact 3.3 R, R Studio, and R Studio Cloud 3.4 Tidyverse I 3.5 Base 3.6 ggplot 3.7 Tidyverse II", " Chapter 3 R Essentials Last updated: 25 April 2021. Required reading Bryan, Jennifer and Jim Hester, 2020, What they Forgot to Teach You About R, Chapters 1 to 5, https://rstats.wtf/debugging-r-code.html. Wickham, Hadley, and Garrett Grolemund, 2017, R for Data Science, Chapters 3 - 6, 8, 10, 11, 13, 14, 15, and 18, https://r4ds.had.co.nz/. Required viewing Kuriwaki, Shiro, 2020, ‘Defining Custom Functions in R,’ Vimeo, 2 February, https://vimeo.com/388825332. Alternative reading There are a lot of great alternative ‘getting started with R’ type materials. Depending on your background and interests you may find some of the following useful: Arnold, Taylor, and Lauren Tilton, 2015, Humanities Data in R, Springer, Chapters 1 to 5. Hall, Megan, 2019, ‘An Introduction to R With Hockey Data,’ https://hockey-graphs.com/2019/12/11/an-introduction-to-r-with-hockey-data/. Hanretty, Chris, 2020, ‘ConveRt,’ slides http://chrishanretty.co.uk/conveRt/#1. Phillips, Nathaniel D., 2018, YaRrr! The Pirate’s Guide to R, Chapter 2, https://bookdown.org/ndphillips/YaRrr/started.html. Recommended reading Alexander, Monica, 2019, ‘The concentration and uniqueness of baby names in Australia and the US,’ https://www.monicaalexander.com/posts/2019-20-01-babynames/. Hvitfeldt, Emil, 2020, ‘Emoji in ggplot2,’ https://www.hvitfeldt.me/blog/real-emojis-in-ggplot2/. Pavlik, Kaylin, 2018, ‘Dairy Queen Deserts in Minnesota,’ https://www.kaylinpavlik.com/dairy-queen-deserts/. ‘R Studio Cloud Guide,’ https://rstudio.cloud/learn/guide. Scherer, Cédric, 2019, ‘Best TidyTuesday 2019,’ https://cedricscherer.netlify.com/2019/12/30/best-tidytuesday-2019/. Silge, Julia, 2019, ‘Reordering and facetting for ggplot2,’ https://juliasilge.com/blog/reorder-within/. Smale, David, 2019, ‘Happy Days,’ https://davidsmale.netlify.com/portfolio/happy-days/. Key libraries ggplot2 tidyverse Key concepts/skills/etc Tibbles Importing data Joining data Strings Factors Dates Pivot Key functions class() dplyr::case_when() ggplot::facet_wrap() ggplot::geom_density() ggplot::geom_histogram() ggplot::geom_point() janitor::clean_names() skimr::skim() tidyr::pivot_longer() tidyr::pivot_wider() Quiz If I had a dataset with the following columns: name, age and wanted to focus on name, then which verb should I use (pick one)? tidyverse::select(). tidyverse::mutate(). tidyverse::filter(). tidyverse::rename(). If I want to cite R then how do I find a recommended citation (pick one)? cite('R'). cite(). citation('R'). citation(). What are three advantages of R? What are three disadvantages? What is R Studio? An integrated development environment (IDE) A closed source paid program A programming language created by Guido van Rossum A statistical programming language What is R? A open-source statistical programming language A programming language created by Guido van Rossum A closed source statistical programming language An integrated development environment (IDE) Which of the following are not tidyverse verbs (pick one)? select(). filter(). arrange(). mutate(). visualize(). If I wanted to make a new column which verb should I use (pick one)? select(). filter(). arrange(). mutate(). visualize(). If I wanted to focus on particular rows which verb should I use (pick one)? select(). filter(). arrange(). mutate(). summarise() If I wanted a summary of the data that gave me the mean by sex, which two verbs should I use (pick one)? summarise(). filter(). arrange(). mutate(). group_by(). What are the three key aspects of the grammar of graphics (select all)? data. aesthetics. type. geom_histogram(). What is not one of the four challenges for mitigating bias mentioned in Hao 2019 (pick one)? Unknown unknowns. Imperfect processes. The definitions of fairness. Lack of social context. Disinterest given profit considerations. What would be the output of class('edward') (pick one)? ‘numeric.’ ‘character.’ ‘data.frame.’ ‘vector.’ How can I simulate 10,000 draws from a normal distribution with a mean of 27 and a standard deviation of 3 (pick one)? rnorm(10000, mean = 27, sd = 3). rnorm(27, mean = 10000, sd = 3). rnorm(3, mean = 10000, sd = 27). rnorm(27, mean = 3, sd = 1000). 3.1 R essentials This section is the basics of using R. Some of it may not make sense at first, but these are commands that we will come back to throughout these notes. You should initially just go through this chapter quickly, noting aspects that you don’t understand. Then start to play around with some of the initial case studies. Then maybe come back to this chapter. That way you will see how the various bits fit into context, and hopefully be more motivated to pick up various aspects. We will come back to everything in this chapter in more detail at some point in these notes. R is an open-source language that is useful for statistical programming You can download R for free here: http://cran.utstat.utoronto.ca/, and you can download R Studio Desktop for free here: https://rstudio.com/products/rstudio/download/#download. When you are using R you will run into trouble at some point. To work through that trouble: Look at the help file for the function by putting ‘?’ before the function e.g. ?pivot_wider(). Check the class of your data, by class(data_set$data_column). Check for typos. Google the error. Google what you are trying to do. Restart R (Session -&gt; Restart R and Clear Output). Try to make a small example and see if you have the same issues. Restart your computer. The past ten years or so of R have been characterised by the rise of the tidyverse. This is ‘… an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.’ Wickham (2020b). There are three distinctions here: the original R language, typically referred to as ‘base’; the ‘tidyverse’ which is a collection of packages that build on top of the R language; and other packages. Pretty much everything that you can do in the tidyverse, you can also do in base. However, as the tidyverse was built especially for modern data science it is usually easier to use the tidyverse, especially when you are setting out. Additionally, pretty much everything that you can do in the tidyverse, you can also do with other packages. However, as the tidyverse is a coherent collection of packages, it is often easier to use the tidyverse, especially when you are setting out. Eventually you will start to see cases where it makes sense to trade-off the convenience and coherence of the tidyverse for some features of base or other packages. Indeed, you’ll see that at various points in these notes. For instance, the tidyverse can be slow, and so if you need to import thousands of CSVs then it can make sense to switch away from read_csv(). That is great and the appropriate use of base and non-tidyverse packages, rather than dogmatic insistence on a solution, is a sign of your development as an applied statistician. Get started by loading the tidyverse package. library(tidyverse) The general workflow that we will use involves: Import Tidy Transforming, descriptive Plot Model Repeat 3/4 People like Keyes have tried to tell us this for a long time, but COVID-19 make it very clear to everyone - most of the data that we use will have humans at the heart of it. It’s vitally important that you keep that in mind and grapple with it in everything that you do with R. It can be really easy to forget that almost every point in our dataset is likely a person. 3.2 Social impact “We shouldn’t have to think about the societal impact of our work because it’s hard and other people can do it for us” is a really bad argument. I stopped doing CV research because I saw the impact my work was having. I loved the work but the military applications and privacy concerns eventually became impossible to ignore. But basically all facial recognition work would not get published if we took Broader Impacts sections seriously. There is almost no upside and enormous downside risk. To be fair though i should have a lot of humility here. For most of grad school I bought in to the myth that science is apolitical and research is objectively moral and good no matter what the subject is. Joe Redmon, 20 February 2020. Although the term ‘data science’ is ubiquitous in academia, industry, and even more generally, it is difficult to define. One deliberately antagonistic definition of data science is ‘[t]he inhumane reduction of humanity down to what can be counted’ (Keyes 2019). While purposefully controversial, this definition highlights one reason for the increased demand for data science and quantitative methods over the past decade—individuals and their behaviour are now at the heart of it. Many of the techniques have been around for many decades, but what makes them popular now is this human focus. Unfortunately, even though much of the work may be focused on individuals, issues of privacy and consent, and ethical concerns more broadly, rarely seem front of mind. While there are some exceptions, in general, even at the same time as claiming that AI, machine learning, and data science are going to revolutionise society, consideration of these types of issues appears to have been largely treated as something that would be nice to have, rather than something that we may like to think of before we embrace the revolution. For the most part, these are not new issues. In the sciences, there has been considerable recent ethical consideration around CRISPR technology and gene editing, but in an earlier time similar conversations were had, for instance, about Wernher von Braun being allowed to building rockets for the US. In medicine, of course, these concerns have been front-of-mind for some time. Data science seems determined to have its own Tuskegee syphilis experiment moment rather than think about and deal appropriately with these issues, based on the experiences of other fields, before they occur. That said, there is some evidence that data scientists are beginning to be more concerned about the ethics surrounding the practice. For instance, NeurIPS, the most prestigious machine learning conference, now requires a statement on ethics to accompany all submissions. In order to provide a balanced perspective, authors are required to include a statement of the potential broader impact of their work, including its ethical aspects and future societal consequences. Authors should take care to discuss both positive and negative outcomes. NeurIPS call for papers, as accessed 26 February 2020. Ethical considerations will be mentioned throughout these notes rather than clumped in one easily ignorable part that can be thrown away after ‘ethics week.’ The purpose is not to prescriptively rule things in or out, but to provide an opportunity to raise some issues that should be front of mind. The variety of data science applications, the relative youth of the field, and the speed of change, mean that ethical considerations can sometimes be set aside when it comes to data science. This is in contrast to fields such as science, medicine, engineering, and accounting where there is a long history. Nonetheless it can be helpful to think through some ethical considerations that you may encounter in the content of a usual data science project. Figure 3.1: Probability, from https://xkcd.com/881/. 3.3 R, R Studio, and R Studio Cloud My colleague Liza Bolton has a lovely analogy here on the relationship between R and R Studio which I really like. R is like a car engine and R Studio is like the car. Although some of us can use a car engine directly, most of us use a car to interact with the engine. 3.3.1 R R - https://www.r-project.org/ - is an open-source and free programming language that is focused on general statistics. (Free in this context doesn’t refer to a price of zero, but instead to ‘freedom,’ but it also does have a price of zero). This is in contrast with an open-source programming language that is designed for general purpose, such as Python, or an open-source programming language that is focused on probability, such as Stan. It was created by Ross Ihaka and Robert Gentleman at the University of Auckland in New Zealand. It is maintained by the R Core Team and changes to this ‘base’ of code occur methodically and with concern given to a variety of different priorities. If you are in Canada then you can download R here: http://cran.utstat.utoronto.ca/, if you are in Australia then you can download R here: https://cran.csiro.au/, otherwise you should go here - https://cran.r-project.org/mirrors.html - and find a location that suits you. (It doesn’t really matter where you get it from, it’s just that it may be slightly faster to use a closer option.) Many people build on this stable base, to extend the capabilities of R to better and more quickly suit their needs. They do this by creating packages. Typically, although not always, a package is a collection R code, and this allows you to more easily do things that you want to do. These packages are managed by the Comprehensive R Archive Network (CRAN) - https://cran.r-project.org/, and other repositories. CRAN is built into the download of R that you just got, so you can use it straight away. If you want to use a package then you need to firstly install it in your computer, and then you need to load it when you want to use it. Di Cook, who is a Professor of Business Analytics at Monash University in Australia, describes this as analogous to a lightbulb: if you want light in your house, first you need to screw in the lightbulb, and you need to turn the switch on. You only need to screw in the lightbulb once per house, but you need to turn the switch on every time you want to use the light. To install a package on your computer (again, you’ll need to do this only once per computer) you use the code: install.packages(&quot;tidyverse&quot;) Then when you want to use a package, you need to call it with this code: library(tidyverse) You can open R and use it on your computer. It is primarily designed to be interacted with through the command line. This is how I had to start with R, and it’s fine, but it can be useful to have a richer environment than the command line provides. In particular, it can be useful to install an Integrated Development Environment (IDE), which is an application that brings together various bits and pieces that you’ll use all the time. The one that we will use is R Studio. 3.3.2 R Studio R Studio is distinct to R and they are different entities. R Studio builds on top of R to make it easier for you to use R. This is in the same way that you can use the internet from the command line, but most of us use a browser such as Chrome, Firefox, or Safari. R Studio is free in the sense that you don’t pay anything for it. It is also free in the sense of being able to take the code, modify it, and distribute that code provided others are similarly allowed to take your code and modify it and distribute, etc. However, it is important to recognise that R Studio is an entity and so it is possible that in the future the current situation could change. You can download R Studio here: https://rstudio.com/products/rstudio/download/#download. When you open R Studio it will look like Figure 3.2. Figure 3.2: Opening R Studio for the first time The left pane is a console in which you can type and execute R code line by line. Try it with 2+2 by clicking next to the prompt ‘&gt;’ and typing that out then pressing enter. The code that you type should be: 2 + 2 ## [1] 4 And hopefully you get the same answer printed in the console. The pane on the top right has information about your environment. For instance, when we create variables a list of their names and some properties will appear there. Try to type the following code, replacing my name with your name, next to the prompt, and again press enter: my_name &lt;- &quot;Rohan&quot; You should notice a new value in the environment pane with the variable name and its value. The pane in the bottom right is a file manager. At the moment it should just have two files - an R History file and a R Project file. We’ll get to what these are later, but for now we will create and save a file. Type out the following code (don’t worry too much about the details for now): saveRDS(object = my_name, file = &quot;my_first_file.rds&quot;) And you should see a new ‘.rds’ file in your list of files. 3.3.3 R Studio Cloud While you can download R Studio to your own computer, initially we will us R Studio Cloud, which is an online version that is provided by R Studio. We will use this so that you can focus on getting comfortable with R and R Studio in an environment that is consistent. This way you don’t have to worry about what computer you have or installation permissions while you are still getting used to the basics. The R Studio Cloud - https://rstudio.cloud/ - is as easy as it gets in terms of moving to the cloud. The trade-off is that it is not very powerful, and it is sometimes slow, but for the purposes of the initial sections of these notes that will be fine. To get started, go to https://rstudio.cloud/ and create an account. If you are going to be a student for a while then it might be worthwhile using a university email account, because although they don’t yet charge for it, they will probably start charging soon, but with some luck they will offer education discounts. Once you have an account and log in, then it should look something like Figure 3.3. Figure 3.3: Opening R Studio Cloud for the first time (You’ll be in ‘Your Workspace,’ and you won’t have an ‘Example Workspace.’) From here you should start a ‘New Project.’ You can give the project a name by clicking on ‘Untitled Project’ and replacing it. We can now use R Studio in the cloud. While working line-by-line in the console is fine, it is easier to write out a whole script that can then be executed. We will do this by making an R Script. To do this go to: File -&gt; New File -&gt; R Script, or use the shortcut Command + Shift + N. The console pane will fall to the bottom left and an R Script will open in the top left. Let’s write some code that will grab all of the Australian politicians and then construct a small table about the genders of the prime ministers. (Some of this code won’t make sense at this stage, but just type it all out to get in the habit and then run it, by selecting all of the code and clicking ‘Run’ (or using the keyboard shortcut: Command + Return) # Install the packages that we&#39;ll need install.packages(&quot;devtools&quot;) install.packages(&quot;tidyverse&quot;) # Load the packages that we need to use this time library(devtools) library(tidyverse) # Grab the data on Australian politicians install_github(&quot;RohanAlexander/AustralianPoliticians&quot;) # Make a table of the counts of genders of the prime ministers AustralianPoliticians::all %&gt;% as_tibble() %&gt;% count(gender, wasPrimeMinister) ## # A tibble: 4 x 3 ## gender wasPrimeMinister n ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 female 1 1 ## 2 female NA 235 ## 3 male 1 29 ## 4 male NA 1511 You can save your R Script as ‘my_first_r_script.R’ using File -&gt; Save As (or the keyboard shortcut: Command + S). When you’re done your workspace should look something like Figure 3.4. Figure 3.4: After running an R Script One thing to be aware of is that each R Studio Cloud workspace is essentially a new computer. Because of this, you’ll need to install any package that you want to use for each workspace. For instance, before you can use the tidyverse, you need to install.packages(“tidyverse”). This is in contrast to when you use your own computer. A few final notes on R Studio Cloud for you to keep in the back of your mind: In the Australian politicians example, we got our data from the website GitHub, but you can get data into your workspace from your local computer in a variety of ways. One way is to use the ‘upload’ button in the Files panel. R Studio Cloud allows some degree of collaboration. For instance, you can give someone else access to a workspace that you create. This could be useful for collaborating on an assignment, although it is not quite full featured yet and you cannot both be in the workspace at the same time (in contrast to, say, Google Docs). There are a variety of weaknesses of R Studio Cloud, in particular at the moment there is a 1GB limit on RAM. Additionally, it is still under-developed, and things break from time to time. The R Studio Community page that is focused on R Studio Cloud can be helpful sometimes: https://community.rstudio.com/c/rstudio-cloud. 3.4 Tidyverse I Aspects of ‘Tidyverse I’ were written with Monica Alexander. One of the key packages that we use in these notes is the tidyverse Wickham et al. (2019a). The tidyverse is actually a package of packages (i.e. when you install tidyverse, you are actually installing a whole bunch of different packages). The key package in the tidyverse in terms of manipulating data is dplyr Wickham et al. (2020), and the key package in the tidyverse in terms of creating graphs is ggplot2 Wickham (2016). In this section we are going to cycle through some essentials from the Tidyverse. You’ll come back to the functions in this section regularly. I want to keep this section self-contained, so let’s start by installing the tidyverse (again, to use Di Cook’s analogy, this is the equivalent of screwing in the lightbulb). If you just did it, then you don’t need to do it again. install.packages(&quot;tidyverse&quot;) Now we can load the tidyverse (again, to use Di Cook’s analogy, the equivalent of turning on the light-switch). library(tidyverse) Here we are going to download the data about Australian politicians using the function read_csv(). australian_politicians &lt;- read_csv( file = &quot;https://raw.githubusercontent.com/RohanAlexander/telling_stories_with_data/master/inputs/data/australian_politicians.csv&quot; ) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_character(), ## birthDate = col_date(format = &quot;&quot;), ## birthYear = col_double(), ## deathDate = col_date(format = &quot;&quot;), ## member = col_double(), ## senator = col_double(), ## wasPrimeMinister = col_double() ## ) ## ℹ Use `spec()` for the full column specifications. We will now cover the pipe and six functions that are useful to know and that we will use all the time: select() filter() arrange() mutate() summarise()/summarize() group_by() 3.4.1 The pipe One key tidyverse helper is the ‘pipe’: %&gt;%. Read it as “and then” (keyboard shortcut: Command + Shift + M). This takes the output of a line of code and uses it as an input to the next line of code. You don’t have to use it, but it tends to make your code more readable. The idea of the pipe is that you take your dataset, and then, do something to it. In this case, we will look at the first few lines of our dataset by piping australian_politicians through to the head() function. australian_politicians %&gt;% head() ## # A tibble: 6 x 20 ## uniqueID surname allOtherNames firstName commonName displayName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Abbott1859 Abbott Richard Hartley Smith Richard &lt;NA&gt; Abbott, Richard ## 2 Abbott1869 Abbott Percy Phipps Percy &lt;NA&gt; Abbott, Percy ## 3 Abbott1877 Abbott Macartney Macartney Mac Abbott, Mac ## 4 Abbott1886 Abbott Charles Lydiard Aubrey Charles Aubrey Abbott, Aubrey ## 5 Abbott1891 Abbott Joseph Palmer Joseph &lt;NA&gt; Abbott, Joseph ## 6 Abbott1957 Abbott Anthony John Anthony Tony Abbott, Tony ## # … with 14 more variables: earlierOrLaterNames &lt;chr&gt;, title &lt;chr&gt;, ## # gender &lt;chr&gt;, birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, birthPlace &lt;chr&gt;, ## # deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;, ## # wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;, adb &lt;chr&gt;, comments &lt;chr&gt; 3.4.2 Selecting The select() function is used to get a particular column of a dataset. For instance, we might like to select the first names column. australian_politicians %&gt;% select(firstName) %&gt;% head() ## # A tibble: 6 x 1 ## firstName ## &lt;chr&gt; ## 1 Richard ## 2 Percy ## 3 Macartney ## 4 Charles ## 5 Joseph ## 6 Anthony In R, there are many ways to do things. Another way to get a particular column of a dataset is to use the dollar sign. This is from base R (as opposed to select() which is from the tidyverse package). australian_politicians$firstName %&gt;% head() ## [1] &quot;Richard&quot; &quot;Percy&quot; &quot;Macartney&quot; &quot;Charles&quot; &quot;Joseph&quot; &quot;Anthony&quot; The two are almost equivalent and differ only in the class of what they return (we’ll talk more about class later in the notes). For the sake of completeness, if you combine select() with pull() then you will get the same class of output as if you use the dollar sign. australian_politicians %&gt;% select(firstName) %&gt;% pull() %&gt;% head() ## [1] &quot;Richard&quot; &quot;Percy&quot; &quot;Macartney&quot; &quot;Charles&quot; &quot;Joseph&quot; &quot;Anthony&quot; You can also use select to get rid of columns, by selecting in a negative sense. australian_politicians %&gt;% select(-firstName) ## # A tibble: 1,776 x 19 ## uniqueID surname allOtherNames commonName displayName earlierOrLaterN… title ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Abbott18… Abbott Richard Hart… &lt;NA&gt; Abbott, Ri… &lt;NA&gt; &lt;NA&gt; ## 2 Abbott18… Abbott Percy Phipps &lt;NA&gt; Abbott, Pe… &lt;NA&gt; &lt;NA&gt; ## 3 Abbott18… Abbott Macartney Mac Abbott, Mac &lt;NA&gt; &lt;NA&gt; ## 4 Abbott18… Abbott Charles Lydi… Aubrey Abbott, Au… &lt;NA&gt; &lt;NA&gt; ## 5 Abbott18… Abbott Joseph Palmer &lt;NA&gt; Abbott, Jo… &lt;NA&gt; &lt;NA&gt; ## 6 Abbott19… Abbott Anthony John Tony Abbott, To… &lt;NA&gt; &lt;NA&gt; ## 7 Abel1939 Abel John Arthur &lt;NA&gt; Abel, John &lt;NA&gt; &lt;NA&gt; ## 8 Abetz1958 Abetz Eric &lt;NA&gt; Abetz, Eric &lt;NA&gt; &lt;NA&gt; ## 9 Adams1943 Adams Judith Anne &lt;NA&gt; Adams, Jud… nee Bird &lt;NA&gt; ## 10 Adams1951 Adams Dick Godfrey… &lt;NA&gt; Adams, Dick &lt;NA&gt; &lt;NA&gt; ## # … with 1,766 more rows, and 12 more variables: gender &lt;chr&gt;, ## # birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, birthPlace &lt;chr&gt;, deathDate &lt;date&gt;, ## # member &lt;dbl&gt;, senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;, wikidataID &lt;chr&gt;, ## # wikipedia &lt;chr&gt;, adb &lt;chr&gt;, comments &lt;chr&gt; Finally, you can select, based on conditions. For instance, selecting all of the columns that start with something, for instance, ‘birth.’ australian_politicians %&gt;% select(starts_with(&quot;birth&quot;)) ## # A tibble: 1,776 x 3 ## birthDate birthYear birthPlace ## &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 NA 1859 Bendigo ## 2 1869-05-14 1869 Hobart ## 3 1877-07-03 1877 Murrurundi ## 4 1886-01-04 1886 St Leonards ## 5 1891-10-18 1891 North Sydney ## 6 1957-11-04 1957 London ## 7 1939-06-25 1939 Sydney ## 8 1958-01-25 1958 Stuttgart ## 9 1943-04-11 1943 Picton ## 10 1951-04-29 1951 Launceston ## # … with 1,766 more rows 3.4.3 Filtering The filter() function is used to get particular rows from a dataset. For instance, we might like to filter to only politicians that became prime minister. australian_politicians %&gt;% filter(wasPrimeMinister == 1) ## # A tibble: 30 x 20 ## uniqueID surname allOtherNames firstName commonName displayName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Abbott1957 Abbott Anthony John Anthony Tony Abbott, Tony ## 2 Barton1849 Barton Edmund Edmund &lt;NA&gt; Barton, Edmund ## 3 Bruce1883 Bruce Stanley Melbourne Stanley &lt;NA&gt; Bruce, Stanley ## 4 Chifley1885 Chifley Joseph Benedict Joseph Ben Chifley, Ben ## 5 Cook1860 Cook Joseph Joseph &lt;NA&gt; Cook, Joseph ## 6 Curtin1885 Curtin John Joseph Ambrose John &lt;NA&gt; Curtin, John ## 7 Deakin1856 Deakin Alfred Alfred &lt;NA&gt; Deakin, Alfred ## 8 Fadden1894 Fadden Arthur William Arthur Arthur Fadden, Arthur ## 9 Fisher1862 Fisher Andrew Andrew &lt;NA&gt; Fisher, Andrew ## 10 Forde1890 Forde Francis Michael Francis Frank Forde, Frank ## # … with 20 more rows, and 14 more variables: earlierOrLaterNames &lt;chr&gt;, ## # title &lt;chr&gt;, gender &lt;chr&gt;, birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, ## # birthPlace &lt;chr&gt;, deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;, ## # wasPrimeMinister &lt;dbl&gt;, wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;, adb &lt;chr&gt;, ## # comments &lt;chr&gt; The filter() function also accepts two conditions. For instance, we can look at politicians who were prime minister and were named Joseph. australian_politicians %&gt;% filter(wasPrimeMinister == 1 &amp; firstName == &quot;Joseph&quot;) ## # A tibble: 3 x 20 ## uniqueID surname allOtherNames firstName commonName displayName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Chifley1885 Chifley Joseph Benedict Joseph Ben Chifley, Ben ## 2 Cook1860 Cook Joseph Joseph &lt;NA&gt; Cook, Joseph ## 3 Lyons1879 Lyons Joseph Aloysius Joseph &lt;NA&gt; Lyons, Joseph ## # … with 14 more variables: earlierOrLaterNames &lt;chr&gt;, title &lt;chr&gt;, ## # gender &lt;chr&gt;, birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, birthPlace &lt;chr&gt;, ## # deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;, ## # wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;, adb &lt;chr&gt;, comments &lt;chr&gt; We would get the same result if we use a comma instead of an ampersand. australian_politicians %&gt;% filter(wasPrimeMinister == 1, firstName == &quot;Joseph&quot;) ## # A tibble: 3 x 20 ## uniqueID surname allOtherNames firstName commonName displayName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Chifley1885 Chifley Joseph Benedict Joseph Ben Chifley, Ben ## 2 Cook1860 Cook Joseph Joseph &lt;NA&gt; Cook, Joseph ## 3 Lyons1879 Lyons Joseph Aloysius Joseph &lt;NA&gt; Lyons, Joseph ## # … with 14 more variables: earlierOrLaterNames &lt;chr&gt;, title &lt;chr&gt;, ## # gender &lt;chr&gt;, birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, birthPlace &lt;chr&gt;, ## # deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;, ## # wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;, adb &lt;chr&gt;, comments &lt;chr&gt; Similarly, we can look at politicians who were named Myles or Ruth. australian_politicians %&gt;% filter(firstName == &quot;Ruth&quot; | firstName == &quot;Myles&quot;) ## # A tibble: 3 x 20 ## uniqueID surname allOtherNames firstName commonName displayName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Coleman1931 Coleman Ruth Nancy Ruth &lt;NA&gt; Coleman, Ruth ## 2 Ferricks1875 Ferricks Myles Aloysius Myles &lt;NA&gt; Ferricks, Myles ## 3 Webber1965 Webber Ruth Stephanie Ruth &lt;NA&gt; Webber, Ruth ## # … with 14 more variables: earlierOrLaterNames &lt;chr&gt;, title &lt;chr&gt;, ## # gender &lt;chr&gt;, birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, birthPlace &lt;chr&gt;, ## # deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;, ## # wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;, adb &lt;chr&gt;, comments &lt;chr&gt; We can also pipe the results, for instance, pipe from the filter() to select() australian_politicians %&gt;% filter(firstName == &quot;Ruth&quot; | firstName == &quot;Myles&quot;) %&gt;% select(firstName, surname) ## # A tibble: 3 x 2 ## firstName surname ## &lt;chr&gt; &lt;chr&gt; ## 1 Ruth Coleman ## 2 Myles Ferricks ## 3 Ruth Webber Finally, we can filter() to a particular row number, for instance, in this case row 853. australian_politicians %&gt;% filter(row_number() == 853) ## # A tibble: 1 x 20 ## uniqueID surname allOtherNames firstName commonName displayName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Jarman1923 Jarman Alan William Alan &lt;NA&gt; Jarman, Alan ## # … with 14 more variables: earlierOrLaterNames &lt;chr&gt;, title &lt;chr&gt;, ## # gender &lt;chr&gt;, birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, birthPlace &lt;chr&gt;, ## # deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;, ## # wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;, adb &lt;chr&gt;, comments &lt;chr&gt; But there is also a dedicated function to do this, which is slice() australian_politicians %&gt;% slice(853) ## # A tibble: 1 x 20 ## uniqueID surname allOtherNames firstName commonName displayName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Jarman1923 Jarman Alan William Alan &lt;NA&gt; Jarman, Alan ## # … with 14 more variables: earlierOrLaterNames &lt;chr&gt;, title &lt;chr&gt;, ## # gender &lt;chr&gt;, birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, birthPlace &lt;chr&gt;, ## # deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;, ## # wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;, adb &lt;chr&gt;, comments &lt;chr&gt; 3.4.4 Arranging We can change the order of the dataset based on the values in a particular column using the arrange() function. For instance, we may like to arrange the data by year of birth. australian_politicians %&gt;% arrange(surname) ## # A tibble: 1,776 x 20 ## uniqueID surname allOtherNames firstName commonName displayName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Abbott1859 Abbott Richard Hartley Smith Richard &lt;NA&gt; Abbott, Richa… ## 2 Abbott1869 Abbott Percy Phipps Percy &lt;NA&gt; Abbott, Percy ## 3 Abbott1877 Abbott Macartney Macartney Mac Abbott, Mac ## 4 Abbott1886 Abbott Charles Lydiard Aubrey Charles Aubrey Abbott, Aubrey ## 5 Abbott1891 Abbott Joseph Palmer Joseph &lt;NA&gt; Abbott, Joseph ## 6 Abbott1957 Abbott Anthony John Anthony Tony Abbott, Tony ## 7 Abel1939 Abel John Arthur John &lt;NA&gt; Abel, John ## 8 Abetz1958 Abetz Eric Eric &lt;NA&gt; Abetz, Eric ## 9 Adams1943 Adams Judith Anne Judith &lt;NA&gt; Adams, Judith ## 10 Adams1951 Adams Dick Godfrey Harry Dick &lt;NA&gt; Adams, Dick ## # … with 1,766 more rows, and 14 more variables: earlierOrLaterNames &lt;chr&gt;, ## # title &lt;chr&gt;, gender &lt;chr&gt;, birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, ## # birthPlace &lt;chr&gt;, deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;, ## # wasPrimeMinister &lt;dbl&gt;, wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;, adb &lt;chr&gt;, ## # comments &lt;chr&gt; We can also use the desc() function to arrange in descending order. australian_politicians %&gt;% arrange(desc(surname)) ## # A tibble: 1,776 x 20 ## uniqueID surname allOtherNames firstName commonName displayName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Zimmerman1968 Zimmerman Trent Moir Trent &lt;NA&gt; Zimmerman, Trent ## 2 Zeal1830 Zeal William Austin William &lt;NA&gt; Zeal, William ## 3 Zappia1952 Zappia Antonio Antonio Tony Zappia, Tony ## 4 Zammit1941 Zammit Paul John Paul &lt;NA&gt; Zammit, Paul ## 5 Zakharov1929 Zakharov Alice Olive Alice Olive Zakharov, Olive ## 6 Zahra1973 Zahra Christian John Christian &lt;NA&gt; Zahra, Christian ## 7 Young1923 Young Harold William Harold &lt;NA&gt; Young, Harold ## 8 Young1936 Young Michael Jerome Michael Mick Young, Mick ## 9 Young1968 Young Terry James Terry &lt;NA&gt; Young, Terry ## 10 Yates1871 Yates George Edwin George Gunner Yates, Gunner ## # … with 1,766 more rows, and 14 more variables: earlierOrLaterNames &lt;chr&gt;, ## # title &lt;chr&gt;, gender &lt;chr&gt;, birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, ## # birthPlace &lt;chr&gt;, deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;, ## # wasPrimeMinister &lt;dbl&gt;, wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;, adb &lt;chr&gt;, ## # comments &lt;chr&gt; We can also arrange based on more than one column. australian_politicians %&gt;% arrange(firstName, surname) ## # A tibble: 1,776 x 20 ## uniqueID surname allOtherNames firstName commonName displayName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Blain1894 Blain Adair Macalister Adair &lt;NA&gt; Blain, Adair ## 2 Armstrong1909 Armstrong Adam Alexander Adam Bill Armstrong, Bill ## 3 Bandt1972 Bandt Adam Paul Adam &lt;NA&gt; Bandt, Adam ## 4 Dein1889 Dein Adam Kemball Adam Dick Dein, Dick ## 5 Ridgeway1962 Ridgeway Aden Derek Aden &lt;NA&gt; Ridgeway, Aden ## 6 Bennett1933 Bennett Adrian Frank Adrian &lt;NA&gt; Bennett, Adrian ## 7 Gibson1935 Gibson Adrian Adrian &lt;NA&gt; Gibson, Adrian ## 8 Wynne1850 Wynne Agar Agar &lt;NA&gt; Wynne, Agar ## 9 Robertson1882 Robertson Agnes Robertson Agnes &lt;NA&gt; Robertson, Agn… ## 10 Bird1906 Bird Alan Charles Alan &lt;NA&gt; Bird, Alan ## # … with 1,766 more rows, and 14 more variables: earlierOrLaterNames &lt;chr&gt;, ## # title &lt;chr&gt;, gender &lt;chr&gt;, birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, ## # birthPlace &lt;chr&gt;, deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;, ## # wasPrimeMinister &lt;dbl&gt;, wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;, adb &lt;chr&gt;, ## # comments &lt;chr&gt; We can pipe arrange() to another arrange(). australian_politicians %&gt;% arrange(firstName) %&gt;% arrange(surname) ## # A tibble: 1,776 x 20 ## uniqueID surname allOtherNames firstName commonName displayName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Abbott1957 Abbott Anthony John Anthony Tony Abbott, Tony ## 2 Abbott1886 Abbott Charles Lydiard Aubrey Charles Aubrey Abbott, Aubrey ## 3 Abbott1891 Abbott Joseph Palmer Joseph &lt;NA&gt; Abbott, Joseph ## 4 Abbott1877 Abbott Macartney Macartney Mac Abbott, Mac ## 5 Abbott1869 Abbott Percy Phipps Percy &lt;NA&gt; Abbott, Percy ## 6 Abbott1859 Abbott Richard Hartley Smith Richard &lt;NA&gt; Abbott, Richa… ## 7 Abel1939 Abel John Arthur John &lt;NA&gt; Abel, John ## 8 Abetz1958 Abetz Eric Eric &lt;NA&gt; Abetz, Eric ## 9 Adams1951 Adams Dick Godfrey Harry Dick &lt;NA&gt; Adams, Dick ## 10 Adams1943 Adams Judith Anne Judith &lt;NA&gt; Adams, Judith ## # … with 1,766 more rows, and 14 more variables: earlierOrLaterNames &lt;chr&gt;, ## # title &lt;chr&gt;, gender &lt;chr&gt;, birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, ## # birthPlace &lt;chr&gt;, deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;, ## # wasPrimeMinister &lt;dbl&gt;, wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;, adb &lt;chr&gt;, ## # comments &lt;chr&gt; It is just important to be clear about the precedence of each. australian_politicians %&gt;% arrange(surname, firstName) ## # A tibble: 1,776 x 20 ## uniqueID surname allOtherNames firstName commonName displayName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Abbott1957 Abbott Anthony John Anthony Tony Abbott, Tony ## 2 Abbott1886 Abbott Charles Lydiard Aubrey Charles Aubrey Abbott, Aubrey ## 3 Abbott1891 Abbott Joseph Palmer Joseph &lt;NA&gt; Abbott, Joseph ## 4 Abbott1877 Abbott Macartney Macartney Mac Abbott, Mac ## 5 Abbott1869 Abbott Percy Phipps Percy &lt;NA&gt; Abbott, Percy ## 6 Abbott1859 Abbott Richard Hartley Smith Richard &lt;NA&gt; Abbott, Richa… ## 7 Abel1939 Abel John Arthur John &lt;NA&gt; Abel, John ## 8 Abetz1958 Abetz Eric Eric &lt;NA&gt; Abetz, Eric ## 9 Adams1951 Adams Dick Godfrey Harry Dick &lt;NA&gt; Adams, Dick ## 10 Adams1943 Adams Judith Anne Judith &lt;NA&gt; Adams, Judith ## # … with 1,766 more rows, and 14 more variables: earlierOrLaterNames &lt;chr&gt;, ## # title &lt;chr&gt;, gender &lt;chr&gt;, birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, ## # birthPlace &lt;chr&gt;, deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;, ## # wasPrimeMinister &lt;dbl&gt;, wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;, adb &lt;chr&gt;, ## # comments &lt;chr&gt; 3.4.5 Grouping We can group variables using the function group_by() and then apply some other function within those groups. For instance, we could arrange by first name within gender, and then get the first three results. australian_politicians %&gt;% group_by(gender) %&gt;% arrange(firstName) %&gt;% slice(1:3) ## # A tibble: 6 x 20 ## # Groups: gender [2] ## uniqueID surname allOtherNames firstName commonName displayName ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Robertson1… Robertson Agnes Robertson Agnes &lt;NA&gt; Robertson, Ag… ## 2 MacTiernan… MacTiern… Alannah Joan Gerald… Alannah &lt;NA&gt; MacTiernan, A… ## 3 Zakharov19… Zakharov Alice Olive Alice Olive Zakharov, Oli… ## 4 Blain1894 Blain Adair Macalister Adair &lt;NA&gt; Blain, Adair ## 5 Armstrong1… Armstrong Adam Alexander Adam Bill Armstrong, Bi… ## 6 Bandt1972 Bandt Adam Paul Adam &lt;NA&gt; Bandt, Adam ## # … with 14 more variables: earlierOrLaterNames &lt;chr&gt;, title &lt;chr&gt;, ## # gender &lt;chr&gt;, birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, birthPlace &lt;chr&gt;, ## # deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;, ## # wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;, adb &lt;chr&gt;, comments &lt;chr&gt; 3.4.6 Mutating The mutate() function is used to make a new column. For instance, perhaps we want to make a new column that is 1 if a person was a member and a senator and 0 otherwise. australian_politicians &lt;- australian_politicians %&gt;% mutate(was_both = if_else(member == 1 &amp; senator == 1, 1, 0)) australian_politicians %&gt;% select(member, senator, was_both) %&gt;% head() ## # A tibble: 6 x 3 ## member senator was_both ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0 ## 2 1 1 1 ## 3 0 1 0 ## 4 1 0 0 ## 5 1 0 0 ## 6 1 0 0 3.4.7 Summarise The function summarise() is used to create new summary variables. For instance, looking at the maximum of birth year to find who the most recently born politicians are. australian_politicians %&gt;% summarise(youngest_politicians_birth_year = max(birthYear, na.rm = TRUE)) ## # A tibble: 1 x 1 ## youngest_politicians_birth_year ## &lt;dbl&gt; ## 1 1994 And we can check that using arrange(). australian_politicians %&gt;% arrange(-birthYear) %&gt;% select(uniqueID, surname, allOtherNames, birthYear) %&gt;% slice(1:3) ## # A tibble: 3 x 4 ## uniqueID surname allOtherNames birthYear ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 SteeleJohn1994 Steele-John Jordon Alexander 1994 ## 2 Chandler1990 Chandler Claire 1990 ## 3 Roy1990 Roy Wyatt Beau 1990 The summarise() function is particularly powerful in conjunction with group_by(). For instance, let’s look at the year of birth of the youngest by gender. australian_politicians %&gt;% group_by(gender) %&gt;% summarise(youngest_politician_birth_year = max(birthYear, na.rm = TRUE)) ## # A tibble: 2 x 2 ## gender youngest_politician_birth_year ## &lt;chr&gt; &lt;dbl&gt; ## 1 female 1990 ## 2 male 1994 Let’s look at mean of age at death by gender. australian_politicians %&gt;% mutate(days_lived = deathDate - birthDate) %&gt;% filter(!is.na(days_lived)) %&gt;% group_by(gender) %&gt;% summarise(mean_days_lived = round(mean(days_lived), 2)) %&gt;% arrange(-mean_days_lived) ## # A tibble: 2 x 2 ## gender mean_days_lived ## &lt;chr&gt; &lt;drtn&gt; ## 1 female 28857.30 days ## 2 male 27372.89 days We can use group_by() for more than one group for instance, looking again at average number of days lived by gender and by which house. australian_politicians %&gt;% mutate(days_lived = deathDate - birthDate) %&gt;% filter(!is.na(days_lived)) %&gt;% group_by(gender, wasPrimeMinister) %&gt;% summarise(mean_days_lived = round(mean(days_lived), 2)) %&gt;% arrange(-mean_days_lived) ## `summarise()` has grouped output by &#39;gender&#39;. You can override using the `.groups` argument. ## # A tibble: 3 x 3 ## # Groups: gender [2] ## gender wasPrimeMinister mean_days_lived ## &lt;chr&gt; &lt;dbl&gt; &lt;drtn&gt; ## 1 female NA 28857.30 days ## 2 male 1 28446.61 days ## 3 male NA 27345.20 days 3.4.8 Counting We can use the function count() to create counts by groups. For instance, the number of politicians by gender. australian_politicians %&gt;% group_by(gender) %&gt;% count() ## # A tibble: 2 x 2 ## # Groups: gender [2] ## gender n ## &lt;chr&gt; &lt;int&gt; ## 1 female 236 ## 2 male 1540 3.4.9 Proportions Finally, often calculating proportions is a combination of summarise() and mutate() (and group_by()). Let’s calculate the proportion of genders. Note here, that we needed to ungroup() the data before mutating. australian_politicians %&gt;% group_by(gender) %&gt;% count() %&gt;% ungroup() %&gt;% mutate(prop = n/(sum(n))) ## # A tibble: 2 x 3 ## gender n prop ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 female 236 0.133 ## 2 male 1540 0.867 3.5 Base 3.5.1 Class A class is the broader type of object that something is. For instance, your class is probably ‘human,’ which is itself an ‘animal.’ Similarly, if we create a number in R we can use class() to work out its class, which in this case will be numeric. my_number &lt;- 8 class(my_number) ## [1] &quot;numeric&quot; Or we could make it a character. my_name &lt;- &quot;rohan&quot; class(my_name) ## [1] &quot;character&quot; Finally, we can often coerce classes to be something else. my_number_as_character &lt;- as.character(my_number) class(my_number_as_character) ## [1] &quot;character&quot; There are many ways for your code to not run but having an issue with the classes is the almost always the first thing to check. 3.5.2 Simulating data Simulating data is a key skill for statistics. We will use the following functions all the time: rnorm(), sample(), and runif(). Arguably the most important function is set.seed(), which we need because while we want our data to be random, we want it to be repeatable. Let’s get 10 observations from the standard normal. set.seed(853) number_of_observations &lt;- 10 simulated_data &lt;- tibble(person = c(1:number_of_observations), observation = rnorm(number_of_observations, mean = 0, sd = 1) ) Then let’s add 10 draws from the uniform distribution between 0 and 10. simulated_data$another_observation &lt;- runif(number_of_observations, min = 0, max = 10) Finally, let’s use sample, which allows use to pick from a list of items, to add a favourite colour to each observation. simulated_data$fav_colour &lt;- sample(x = c(&quot;blue&quot;, &quot; white &quot;), size = number_of_observations, replace = TRUE) We set the option replace to TRUE because we are only choosing between two items, but we want ten outcomes. Depending on the simulation you should think about whether you need it TRUE or FALSE. Also, there is another useful option to adjust the probability with which each item is drawn. In particular, the default is that both options are equally likely, but perhaps we might like to have 10 per cent blue with 90 per cent white. The way to do this is to set the option prob. As always with functions, you can find more in the help with ?sample. 3.5.3 Functions There are a lot of functions in R, and almost any common task that you might need to do is likely already done. But you will need to write your own functions. The way to do this is to define a function and give it a name. Your function will probably have some inputs (note that these inputs can have default values). Your function will then do something with these inputs and then return something. my_function &lt;- function(some_names) { print(some_names) } my_function(c(&quot;rohan&quot;, &quot;monica&quot;)) ## [1] &quot;rohan&quot; &quot;monica&quot; 3.6 ggplot The ggplot package is the plotting package that is part of the tidyverse collection of packages. In a similar way to piping, it works in layers. But instead of using the pipe (%&gt;%) ggplot uses +. 3.6.1 Main features There are three key aspects: data; aesthetics / mapping; and type. For instances, let’s build up a histogram of age of death with increasing complexity. Starts with a grey box: australian_politicians %&gt;% mutate(days_lived = as.integer(deathDate - birthDate)) %&gt;% filter(!is.na(days_lived)) %&gt;% ggplot(mapping = aes(x = days_lived)) We need to tell it what we want to plot. This is where geom comes in australian_politicians %&gt;% mutate(days_lived = as.integer(deathDate - birthDate)) %&gt;% filter(!is.na(days_lived)) %&gt;% ggplot(mapping = aes(x = days_lived)) + geom_histogram(binwidth = 365) Now let’s color the bars by gender, which means adding an aesthetic. australian_politicians %&gt;% mutate(days_lived = as.integer(deathDate - birthDate)) %&gt;% filter(!is.na(days_lived)) %&gt;% ggplot(mapping = aes(x = days_lived, fill = gender)) + geom_histogram(binwidth = 365) We can add some labels, change the color, and background. australian_politicians %&gt;% mutate(days_lived = as.integer(deathDate - birthDate)) %&gt;% filter(!is.na(days_lived)) %&gt;% ggplot(mapping = aes(x = days_lived, fill = gender)) + geom_histogram(binwidth = 365) + labs(title = &quot;Length of life of Australian politicians&quot;, x = &quot;Age of deaths (days)&quot;, y = &quot;Number&quot;) + theme_classic() + scale_fill_brewer(palette = &quot;Set1&quot;) I forget who said this but, ‘ggplot makes it so easy to have nicely labelled axes, there’s no real excuse not to.’ 3.6.2 Facets Facets are subplots and are invaluable because they allow you to add another variable to your plot without having to make a 3D plot. australian_politicians %&gt;% mutate(days_lived = as.integer(deathDate - birthDate)) %&gt;% filter(!is.na(days_lived)) %&gt;% ggplot(mapping = aes(x = days_lived)) + geom_histogram(binwidth = 365) + labs(title = &quot;Length of life of Australian politicians&quot;, x = &quot;Age of deaths (days)&quot;, y = &quot;Number&quot;) + theme_classic() + scale_fill_brewer(palette = &quot;Set1&quot;) + facet_wrap(~gender) 3.7 Tidyverse II 3.7.1 Tibbles A tibble is a data frame, but it is a data frame with some particular changes that make it easier to work with. You should read Chapter 10 of Wickham and Grolemund (2017) for more detail. The main difference is that compared with a dataframe, a tibble doesn’t convert strings to factors, and it prints nicely, including letting you know the class of a column. You can make a tibble manually if you need, for instance this can be handy for simulating data, but usually we will just import data as a tibble. people &lt;- tibble(names = c(&quot;rohan&quot;, &quot;monica&quot;), website = c(&quot;rohanalexander.com&quot;, &quot;monicaalexander.com&quot;), fav_colour = c(&quot;blue&quot;, &quot; white &quot;), ) people ## # A tibble: 2 x 3 ## names website fav_colour ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 rohan rohanalexander.com &quot;blue&quot; ## 2 monica monicaalexander.com &quot; white &quot; 3.7.2 Importing data There are a variety of ways to import data. If you are dealing with CSV files then try read_csv() in the first instance. There were examples of that in earlier sections. 3.7.3 Joining data We can join two datasets together in a variety of ways. The most common join that I use is left_join(), where I have one main dataset and I want to join another to it based on some common column names. Here we’ll join two datasets based on favourite colour. both &lt;- simulated_data %&gt;% left_join(people, by = &quot;fav_colour&quot;) both ## # A tibble: 10 x 6 ## person observation another_observation fav_colour names website ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 -0.360 9.52 &quot;blue&quot; rohan rohanalexander.com ## 2 2 -0.0406 0.586 &quot; white &quot; monica monicaalexander.com ## 3 3 -1.78 2.48 &quot;blue&quot; rohan rohanalexander.com ## 4 4 -1.12 5.80 &quot; white &quot; monica monicaalexander.com ## 5 5 -1.00 5.26 &quot;blue&quot; rohan rohanalexander.com ## 6 6 1.78 4.09 &quot;blue&quot; rohan rohanalexander.com ## 7 7 -1.39 3.97 &quot;blue&quot; rohan rohanalexander.com ## 8 8 -0.497 2.52 &quot; white &quot; monica monicaalexander.com ## 9 9 -0.558 6.29 &quot;blue&quot; rohan rohanalexander.com ## 10 10 -0.824 8.57 &quot;blue&quot; rohan rohanalexander.com 3.7.4 Strings We’ve seen a string earlier, but it is an object that is created with single or double quotes. String manipulation is an entire book in itself, but you should start with the stringr package (Wickham 2019c). I’ll just cover a few essentials: stringr::str_detect(), stringr::str_replace(), stringr::str_squish(). head(people) ## # A tibble: 2 x 3 ## names website fav_colour ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 rohan rohanalexander.com &quot;blue&quot; ## 2 monica monicaalexander.com &quot; white &quot; people &lt;- people %&gt;% mutate(is_rohan = stringr::str_detect(names, &quot;rohan&quot;), make_howlett = stringr::str_replace(website, &quot;alexander&quot;, &quot;howlett&quot;), fav_colour_trim = stringr::str_squish(fav_colour) ) head(people) ## # A tibble: 2 x 6 ## names website fav_colour is_rohan make_howlett fav_colour_trim ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 rohan rohanalexander.com &quot;blue&quot; TRUE rohanhowlett.com blue ## 2 monica monicaalexander.c… &quot; white &quot; FALSE monicahowlett.c… white 3.7.5 Pivot Datasets tend to be either long or wide. Generally, in the tidyverse, and certainly for ggplot, we need long data. To go from one to the other you can use the pivot_longer() and pivot_wider() functions. Let’s see an example with some data on whether red team or blue team won a competition in some year. pivot_example_data &lt;- tibble(year = c(2019, 2020, 2021), blue_team = c(1, 2, 1), red_team = c(2, 1, 2)) head(pivot_example_data) ## # A tibble: 3 x 3 ## year blue_team red_team ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2019 1 2 ## 2 2020 2 1 ## 3 2021 1 2 This dataset is in wide format at the moment. To get it into long format, what we’d like is to have a column that specifies the team, and then another that specifies the result. We’ll use tidyr::pivot_longer. data_pivoted_longer &lt;- pivot_example_data %&gt;% tidyr::pivot_longer(cols = c(&quot;blue_team&quot;, &quot;red_team&quot;), names_to = &quot;team&quot;, values_to = &quot;position&quot;) head(data_pivoted_longer) ## # A tibble: 6 x 3 ## year team position ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2019 blue_team 1 ## 2 2019 red_team 2 ## 3 2020 blue_team 2 ## 4 2020 red_team 1 ## 5 2021 blue_team 1 ## 6 2021 red_team 2 Occasionally, you’ll need to go from long data to wide data. We accomplish this with tidyr::pivot_wider. data_pivoted_wider &lt;- data_pivoted_longer %&gt;% tidyr::pivot_wider(id_cols = c(&quot;year&quot;, &quot;team&quot;), names_from = &quot;team&quot;, values_from = &quot;position&quot;) head(data_pivoted_wider) ## # A tibble: 3 x 3 ## year blue_team red_team ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2019 1 2 ## 2 2020 2 1 ## 3 2021 1 2 3.7.6 Factors A factor is a string that has an inherent ordering. For instance, the days of the week have an order - Monday, Tuesday, Wednesday,… - which is not alphabetical. Factors feature prominently in base, but they often add more complication than they are worth and so the tidyverse gives them a less prominent role. Nonetheless taking advantage of factors is useful in certain circumstances, for instance when plotting the days of the week we probably want them in the usual ordering than in the alphabetical ordering that would result if we had them as characters. The package that we use to deal with factors is forcats (Wickham 2020a). Sometimes you will have a character vector and you will want it ordered in a particular way. The default is that a character vector is ordered alphabetically, but you may not want that, for instance, the days of the week would look strange on a graph if they were alphabetically ordered: Friday, Monday, Saturday, Sunday, Thursday, Tuesday, Wednesday! The way to change the ordering is to change the variable from a character to a factor. I would then use the forcats package to specify an ordering by hand. The help page is here: https://forcats.tidyverse.org/reference/fct_relevel.html. Let’s look at a concrete example. my_data &lt;- tibble(all_names = c(&quot;Rohan&quot;, &quot;Monica&quot;, &quot;Edward&quot;)) If we plotted this then Edward would be first, because it would be alphabetical. But if instead I want to be first as I am the oldest then we could use forcats in the following way. library(forcats) # (BTW you&#39;ll probably have to install that one) library(tidyverse) my_data &lt;- my_data %&gt;% mutate(all_names = factor(all_names), # Change to factor all_names_releveled = fct_relevel(all_names, &quot;Rohan&quot;, &quot;Monica&quot;)) # Change the levels # Then compare the two my_data$all_names ## [1] Rohan Monica Edward ## Levels: Edward Monica Rohan my_data$all_names_releveled ## [1] Rohan Monica Edward ## Levels: Rohan Monica Edward 3.7.7 Cases If you need to write a few conditional statements, then case_when is the way to go. Let’s start with a tibble of dates and pretend that we want to group them into ‘pre-1950,’ ‘1950-2000,’ ‘2000-onwards’ case_when_example &lt;- tibble(some_dates = c(&quot;1909-12-31&quot;, &quot;1919-12-31&quot;, &quot;1929-12-31&quot;, &quot;1939-12-31&quot;, &quot;1949-12-31&quot;, &quot;1959-12-31&quot;, &quot;1969-12-31&quot;, &quot;1979-12-31&quot;, &quot;1989-12-31&quot;, &quot;1999-12-31&quot;, &quot;2009-12-31&quot;) ) case_when_example &lt;- case_when_example %&gt;% mutate(some_dates = lubridate::ymd(some_dates) ) head(case_when_example) ## # A tibble: 6 x 1 ## some_dates ## &lt;date&gt; ## 1 1909-12-31 ## 2 1919-12-31 ## 3 1929-12-31 ## 4 1939-12-31 ## 5 1949-12-31 ## 6 1959-12-31 Now we’ll use dplyr::case_when() to group these. case_when_example &lt;- case_when_example %&gt;% mutate(year_group = case_when( some_dates &lt; lubridate::ymd(&quot;1950-01-01&quot;) ~ &quot;pre-1950&quot;, some_dates &lt; lubridate::ymd(&quot;2000-01-01&quot;) ~ &quot;1950-2000&quot;, some_dates &gt;= lubridate::ymd(&quot;2000-01-01&quot;) ~ &quot;2000-onwards&quot;, TRUE ~ &quot;CHECK ME&quot; ) ) head(case_when_example) ## # A tibble: 6 x 2 ## some_dates year_group ## &lt;date&gt; &lt;chr&gt; ## 1 1909-12-31 pre-1950 ## 2 1919-12-31 pre-1950 ## 3 1929-12-31 pre-1950 ## 4 1939-12-31 pre-1950 ## 5 1949-12-31 pre-1950 ## 6 1959-12-31 1950-2000 We could accomplish this with a series of if_else statements, but case_when is just a bit cleaner. The only thing to be aware of is that statements are evaluated in order. So as soon as something matches it doesn’t continue down the list of conditions. That’s why we have that catch-all at the end - if the date doesn’t fit any of the earlier conditions, then we’ve got a problem and want to know about it. "],["workflow.html", "Chapter 4 Workflow 4.1 Introduction 4.2 R Markdown 4.3 R projects 4.4 Git and GitHub 4.5 Using R in practice 4.6 Developing research questions", " Chapter 4 Workflow Last updated: 25 April 2021. Required reading Alexander, Monica, 2019, ‘Reproducibility in demographic research,’ https://www.monicaalexander.com/posts/2019-10-20-reproducibility/. Bailey, Jack, 2020, ‘UK Voting Intention Poll Tracker,’ https://github.com/jackobailey/poll_tracker. Bryan, Jennifer and Jim Hester, 2020, ‘What they Forgot to Teach You About R,’ Chapters 11, 12, and 13, https://rstats.wtf/debugging-r-code.html. Bryan, Jenny, 2020, Happy Git and GitHub for the useR, Chapters 4, 6, 7, 9, https://happygitwithr.com/. Fan, Jean, ‘R Style Guide,’ https://jef.works/R-style-guide/. Gelman, Andrew, 2016, ‘What has happened down here is the winds have changed,’ https://statmodeling.stat.columbia.edu/2016/09/21/what-has-happened-down-here-is-the-winds-have-changed/. Healy, Kieran, 2020, ‘The Kitchen Counter Observatory,’ 21 May, https://kieranhealy.org/blog/archives/2020/05/21/the-kitchen-counter-observatory/. Hill, Alison, 2020, ‘How I Teach R Markdown,’ 28 May, https://alison.rbind.io/post/2020-05-28-how-i-teach-r-markdown/. Mostipak, Jesse, 2018, ‘So you’ve been asked to make a reprex,’ 24 February, https://www.jessemaegan.com/post/so-you-ve-been-asked-to-make-a-reprex/. Phillips, Nathaniel D., 2018, YaRrr! The Pirate’s Guide to R, Chapter 4.3, https://bookdown.org/ndphillips/YaRrr/a-brief-style-guide-commenting-and-spacing.html. Taback, Nathan, 2019, ‘R Markdown for Class Reports,’ https://scidesign.github.io/Rmarkdownforclassreports.html. Tierney, Nicholas, 2020, RMarkdown for Scientists, 9 September, Chapters 7-10, 12-15, https://rmd4sci.njtierney.com. Wickham, Hadley, Advanced R, Chapter on Style, http://adv-r.had.co.nz/Style.html. Wickham, Hadley, and Garrett Grolemund, 2017, R for Data Science, Chapter 27, https://r4ds.had.co.nz/. Wilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, Tracy K. Teal, 2017, ‘Good enough practices in scientific computing,’ PLoS Computational Biology, 13(6). Recommended reading AirbnbEng, 2016, ‘Scaling Knowledge at Airbnb,’ 25 February, https://medium.com/airbnb-engineering/scaling-knowledge-at-airbnb-875d73eff091. Bik, Elisabeth, 2020, ‘The Tadpole Paper Mill,’ Science Integrity Digest, 21 February, https://scienceintegritydigest.com/2020/02/21/the-tadpole-paper-mill/. Chan, Martin, 2020, ‘RStudio Projects and Working Directories: A Beginner’s Guide,’ https://martinctc.github.io/blog/rstudio-projects-and-working-directories-a-beginner's-guide/. Daly, Darren, 2020, ‘A brief history of medical statistics and its impact on reproducibility,’ 2 February, https://medium.com/@darren_dahly/a-brief-history-of-medical-statistics-and-its-role-in-reproducibility-23e31082f024. Ebert, Philip A. “Bayesian reasoning in avalanche terrain: a theoretical investigation.” Journal of Adventure Education and Outdoor Learning 19, no. 1 (2019): 84-95. Gertler, Paul, Sebastian Martinez, Patrick Premand, Laura Rawlings, and Christel Vermeersch, ‘Impact Evaluation in Practice,’ Chapters 1, 2. Guzey, Alexey, 2020, ‘How Life Sciences Actually Work: Findings of a Year-Long Investigation,’ https://guzey.com/how-life-sciences-actually-work/. King, Gary, ‘How to Write a Publishable Paper as a Class Project,’ https://gking.harvard.edu/papers. King, Gary, 2006, ‘Publication, publication,’ PS: Political Science &amp; Politics, vol. 39, pp. 119-125, https://gking.harvard.edu/files/gking/files/paperspub.pdf. Lowndes, Julia S. Stewart, Benjamin D. Best, Courtney Scarborough, Jamie C. Afflerbach, Melanie R. Frazier, Casey C. O’Hara, Ning Jiang, and Benjamin S. Halpern, 2017, ‘Our path to better science in less time using open data science tools,’ Nature ecology &amp; evolution, 1(6). Miyakawa, Tsuyoshi, 2020, ‘No raw data, no science: another possible source of the reproducibility crisis,’ Molecular Brain, 13, 24. https://doi.org/10.1186/s13041-020-0552-2 Peek, Ryan, 2020, ‘10 tips to souping up rmarkdown,’ 20 February, https://ryanpeek.github.io/2020-02-20-10-tips-to-souping-up-rmarkdown/#S8. Riederer, Emily, 2019, ‘Resource Round-Up: Reproducible Research Edition,’ 29 August, https://emilyriederer.netlify.com/post/resource-round-up-reproducible-research-edition/. Riederer, Emily, 2019, ‘RMarkdown Driven Development (RmdDD),’ 4 May, https://emilyriederer.netlify.com/post/rmarkdown-driven-development/. Silver, Nate, 2020, ‘We Fixed An Issue With How Our Primary Forecast Was Calculating Candidates’ Demographic Strengths’, 19 February, https://fivethirtyeight.com/features/we-fixed-a-mistake-in-how-our-primary-forecast-was-calculating-candidates-demographic-strengths/. Soetewey, Antoine, 2020, ‘Getting started in R markdown,’ 18 February, https://www.statsandr.com/blog/getting-started-in-r-markdown/. Varian, Hal, ‘How to Build an Economic Model in Your Spare Time,’ http://people.ischool.berkeley.edu/~hal/Papers/how.pdf. (This paper was written a while ago for economists, please just ignore the economics specific parts.) Wayne, Hillel, 2017, ‘How do we trust our science code?’ 14 August, https://www.hillelwayne.com/post/how-do-we-trust-science-code/. Wayne, Hillel, 2020, ‘This is how science happens,’ 9 March, https://www.hillelwayne.com/post/this-is-how-science-happens/. Required viewing Bryan, Jenny, ‘Debugging in R,’ https://resources.rstudio.com/rstudio-conf-2020/object-of-type-closure-is-not-subsettable-jenny-bryan. Gelfand, Sharla, ‘Help! my code doesn’t work,’ Toronto Workshop on Reproducibility, 26 February, https://youtu.be/G5Nm-GpmrLw. Recommended viewing Gelfand, Sharla, 2020, ‘Don’t repeat yourself, talk to yourself! Repeated reporting in the R universe,’ 30 January, talk given at rstudio::conf, San Francisco, https://resources.rstudio.com/rstudio-conf-2020/dont-repeat-yourself-talk-to-yourself-repeated-reporting-in-the-r-universe-sharla-gelfand. Recommended activity ‘First Contributions’ https://github.com/forwards/first-contributions. Fun song The Magnetic Fields, 2016, ‘’86 How I Failed Ethics,’ 17 November, https://youtu.be/Hu5dEXZ7DOY (thanks to Paul Hodgetts). Key concepts/skills/etc Restart R often (Session -&gt; Restart R and Clear Output). Debugging is a skill, and you will get better at it with time and practice. Start with reading the error message. Check the class. You may get frustrated at times, this is normal. There are various tools that can help. Google is your friend. Make a small example and try to get the code running on that. Cultivating a tenacious mentality may help. Writing code that future-you can understand. Developing important questions. Reproducibility and replicability The importance of data and code access The importance of version control in a modern scientific workflow. The basics of Git and GitHub, as a solo data scientist. Key GitHub workflow with commands Get the latest changes: git pull. Add your updates: git add -A. Check on everything: git status. Commit your changes: git commit -m \"Short description of changes\". Push your changes to GitHub: git push. Quiz What are three features of a good research question (write a paragraph or two)? What is a counterfactual (pick one)? If-then statements in which the if didn’t happen. If-then statements in which the if happen. Statements that are either true or false. Statements that are neither true or false. How do you hide the warnings in a R Markdown R chunk (pick one)? echo = FALSE include = FALSE eval = FALSE warning = FALSE message = FALSE What is a reprex and why is it important to be able to make one (select all)? A reproducible example that enables your error to be reproduced. A reproducible example that helps others help you. A reproducible example during the construction of which you may solve your own problem. A reproducible example that demonstrates you’ve actually tried to help yourself. Why are R Projects important (select all)? They help with reproducibility. They make it easier to share code. They make your workspace more organized. They are all that needs to be done. Consider this sequence: ‘git pull, git status, ________, git status, git commit -m \"My message\", git push.’ What is the missing step (pick one)? git add -A. git status. git pull. git push. 4.1 Introduction Suppose you have cancer and you have to choose between a black box AI surgeon that cannot explain how it works but has a 90% cure rate and a human surgeon with an 80% cure rate. Do you want the AI surgeon to be illegal? Geoffrey Hinton, 20 February 2020. The number one thing to keep in mind about machine learning is that performance is evaluated on samples from one dataset, but the model is used in production on samples that may not necessarily follow the same characteristics… The finance industry has a saying for this: “past performance is no guarantee of future results.” Your model scoring X on your test dataset doesn’t mean it will perform at level X on the next N situations it encounters in the real world. The future may not be like the past. So when asking the question, “would you rather use a model that was evaluated as 90% accurate, or a human that was evaluated as 80% accurate,” the answer depends on whether your data is typical per the evaluation process. Humans are adaptable, models are not. If significant uncertainty is involved, go with the human. They may have inferior pattern recognition capabilities (versus models trained on enormous amounts of data), but they understand what they do, they can reason about it, and they can improvise when faced with novelty If every possible situation is known and you want to prioritize scalability and cost-reduction, go with the model. Models exist to encode and operationalize human cognition in well-understood situations. (“well understood” meaning either that it can be explicitly described by a programmer, or that you can amass a dataset that densely samples the distribution of possible situations – which must be static) François Chollet, 20 February 2020. If science is about systematically building and organising knowledge in terms of testable explanations and predictions, then data science takes this and focuses on data. Fundamentally data science is still science, and as such, building and organising knowledge is a critical aspect. Being able to do something yourself, once, does not achieve this. Hence, the focus on reproducibility and replicability. M. Alexander (2019) says ‘Research is reproducible if it can be reproduced exactly, given all the materials used in the study.’ ‘[Hence] materials need to be provided!’ ’‘[M]aterials’ usually means data, code and software.’ The minimum requirement is to be able to ‘reproduce the data, methods and results (including figures, tables).’ Similarly, from Gelman (2016) you should have noticed that this has been an issue in other sciences. e.g. psychology. The issue with not being reproducible is that we are not contributing to knowledge. We no longer have any idea was is fact in the field of psychology. (This is coming for other fields too, including the field that I was trained in - economics.) Do this matter? Yes. Some of the examples that Gelman (2016) cites (which turned out to be dodgy) don’t really matter e.g. ESP or the power pose. It doesn’t really matter. But increasingly the same methods are being applied in areas where they do matter e.g. ‘nudge’ units. Similarly, D. Simpson (2017) makes it clear that it’s a big problem in data science. The ‘gay face’ paper that D. Simpson (2017) writes about has not released their dataset. We have no way of knowing what is going on with it. They have found a certain set of results based on that dataset, their methods, and what they did, but we have no way of knowing how much that matters. As D. Simpson (2017) says ‘the paper itself does some things right. It has a detailed discussion of the limitations of the data and the method.’ You must do this in everything that you write, but it is not enough. Without the data, we don’t know what their results speak to as we don’t understand how representative the sample is. If the dataset is biased, then that undermines their claims. There’s a reason that while initial medical trials are done on mice, etc, eventually human trials are required. In order to do the study, they needed a trained dataset. They trained it using Mechanical Turk. Figure 4.1 is from Mattson (2017). Figure 4.1: Instructions for workers to do classification piecework on the Amazon Mechanical Turk platform, p. 46. From Mattson. Mattson (2017) comments: The problems here are legion: Barack Obama is biracial but simply “Black” by American cultural norms. “Clearly Latino” begs the question “to whom?” Latino is an ethnic category, not a racial one: many Latinos already are Caucasian, and increasingly so. By training their workers according to stereotypical American categories, WAK’s algorithm can only spit out the garbage they put in. ‘WAK’s algorithm can only spit out the garbage they put in.’ I would encourage you to print out that statement and paste it somewhere that you will see it every time you get a data science result. What steps can we take to make our work reproducible? Ensure your entire workflow is documented. How did you get the raw data? Can you save the raw data? Will the raw data always be available? Is the raw data available to others? What steps are you taking to transform the raw data in data that can be analysed? How are you analysing the data? How are you building the report? Try to improve each time. Can you run your entire workflow again? Can ‘another person’ run your entire workflow again? Can a future you run your entire workflow again? Can a future ‘another person’ run your entire workflow again? Each of these requirements is increasingly more onerous. We are going to start with worrying about the first. The way we are going to do this is by using R Markdown. 4.2 R Markdown 4.2.1 Getting started R Markdown is a mark-up language similar to html or LaTeX, in comparison to a WYSIWYG language, such as Word. This means that all of the aspects are consistent, for instance, all ‘main headings’ will look the same. However, it means that use symbols to designate how you would like certain aspects to appear, and it is only when you compile it that you get to see it. R Markdown is a variant of regular markdown that is specifically designed to allow R code chunks to be included. The advantage is that you can get a ‘live’ document in which code executes and is then printed to a document. The disadvantage is that it can take a while for the document to compile because all of the code needs to run. You can create a new R Markdown document within R Studio (File -&gt; New File -&gt; R Markdown Document). Another advantage of R Markdown is that very similar code can compile into a variety of documents, including html pages and PDFs. R Markdown also has default options set up for including a title, author, and date sections. 4.2.2 Basic commands If you ever need a reminder of the basics of R Markdown then this is built into R Studio (Help -&gt; Markdown Quick Reference). This provides the code for commonly needed commands: Emphasis: *italic*, **bold**, _italic_, __bold__ Headers (these need to go on their own line with a line before and after): # Header 1, ## Header 2, ### Header 3 Lists: Unordered List * Item 1 * Item 2 + Item 2a + Item 2b Ordered List 1. Item 1 2. Item 2 3. Item 3 + Item 3a + Item 3b URLs: Can just include an address: http://example.com, or can include a [linked phrase](http://example.com). Basic images can just be included either from the internet: ![alt text](http://example.com/logo.png) or from a local file: ![alt text](figures/img.png). In order to create an actual document, once you have these pieces set up, click ‘Knit.’ 4.2.3 R chunks You can include R (and a bunch of other languages) code in code chunks within your R Markdown document. Then when you knit your document, the R code will run and be included in your document. To create an R chunk start with three backticks and then within curly braces tell markdown that this is an R chunk. Anything inside this chunk will be considered R code and run as such. library(tidyverse) ggplot(data = diamonds) + geom_point(aes(x = price, y = carat)) There are various evaluation options that are available in chunks. You include these by putting a comma after r and then specifying any options before the closing curly brace. Helpful options include: echo = FALSE: run the code and include the output, but don’t print the code in the document. include = FALSE: run the code but don’t output anything and don’t print the code in the document. eval = FALSE: don’t run the code, and hence don’t include the outputs, but do print the code in the document. warning = FALSE: don’t display warnings. message = FALSE: don’t display messages. 4.2.4 Abstracts and PDF outputs In the default header, you can add a section for a header, so that it would look like this: --- title: My document author: Rohan Alexander date: 5 January 2020 output: html_document abstract: &quot;This is my abstract.&quot; --- Similarly, you can change the output from html_document to pdf_document in order to produce a PDF. This uses LaTeX in the background so you may need to install a bunch of related packages. 4.2.5 References You can reference a bibliography by including one in the preamble and then calling it in the text when you need. --- title: My document author: Rohan Alexander date: 5 January 2020 output: html_document abstract: &quot;This is my abstract.&quot; bibliography: bibliography.bib --- You need to make a separate file called bibliography.bib. In that you need an entry for the item that you want to reference. R and R packages usually provide this for you for instance, if you run citation() then it tells you the entry to put in your bibtex file: @Manual{, title = {R: A Language and Environment for Statistical Computing}, author = {{R Core Team}}, organization = {R Foundation for Statistical Computing}, address = {Vienna, Austria}, year = {2020}, url = {https://www.R-project.org/}, } You need to create a unique key that you’ll refer to it with in the text. This can be anything if it’s unique, but I try to use meaningful ones, so that bibtex entry could become: @Manual{citeR, title = {R: A Language and Environment for Statistical Computing}, author = {{R Core Team}}, organization = {R Foundation for Statistical Computing}, address = {Vienna, Austria}, year = {2020}, url = {https://www.R-project.org/}, } And to cite R you’d then include the following: @citeR, which would put the brackets around the year, like this: R Core Team (2020) or [@citeR], which would put the brackets around the whole thing, like this: (R Core Team 2020). 4.2.6 Cross-references Finally, it can be useful to cross-reference figures, tables and equations. This makes it easier to refer to them in the text. To do this for a figure you refer to the name of the R chunk that creates/contains the figure. For instance, (Figure \\@ref(fig:my_unique_name)) will produce: (Figure @ref(fig:my_unique_name)) as the name of the R chunk is my_unique_name (don’t forget to add the fig in front of the chunk name. Also, super annoyingly you need to have a ‘fig.cap’ in the R chunk, so it looks something like this: ```{r my_unique_name, fig.cap=&quot;More bills of penguins&quot;, echo = TRUE} library(palmerpenguins) ggplot(penguins, aes(x = island, fill = species)) + geom_bar(alpha = 0.8) + scale_fill_manual(values = c(&quot;darkorange&quot;,&quot;purple&quot;,&quot;cyan4&quot;), guide = FALSE) + theme_minimal() + facet_wrap(~species, ncol = 1) + coord_flip() (#fig:my_unique_name)More bills of penguins You can do similar for tables and equations e.g. (Table \\@ref(tab:penguinhead)) will produce: (Table 4.1) (again, don’t forget to ad tab in front). penguins %&gt;% select(species, bill_length_mm, bill_depth_mm) %&gt;% slice(1:5) %&gt;% knitr::kable(caption = &quot;A penguin table&quot;) Table 4.1: A penguin table species bill_length_mm bill_depth_mm Adelie 39.1 18.7 Adelie 39.5 17.4 Adelie 40.3 18.0 Adelie NA NA Adelie 36.7 19.3 And finally, you can, and should, cross-reference equations also, but this time you need to add a tag (\\#eq:slope) and then reference that e.g. use Equation \\@ref(eq:slope). to produce Equation (4.1). \\begin{equation} Y = a + b X (\\#eq:slope) \\end{equation} \\[\\begin{equation} Y = a + b X \\tag{4.1} \\end{equation}\\] When you are using cross-references, it’s important that your R chunks have simple labels. For instance, no underbars. In general, try to keep the names simple, and if possible, avoid punctuation and just stick to letters. 4.3 R projects RStudio has the option of creating a project, which allows you to keep all the files (data, analysis, report etc) associated with a particular project together. To create a project, select ‘Click File’ -&gt; ‘New Project,’ then select empty project, name your project and think about where you want to save it. For example, if you are creating a project for Problem Set 2, you might call it ps2 and save it in a sub-folder called PS2 in your INF2178 folder. Once you have created a project, a new file with the extension .RProj will appear in that file. As an example, download the R help folder. Whenever I work on class materials, I open the project file and work from that. The main advantage of projects is that you don’t have to set the working directory or type the whole file path to read in a file (for example, a data file). So instead of reading a csv from \"~/Documents/toronto/teaching/INF2178/data/\" you can just read it in from data/. To meet even the minimal expected level of reproducibility, you must use R projects. You must not use setwd() because that ties your work to your computer. 4.4 Git and GitHub 4.4.1 Introduction Here we introduce Git and GitHub. These are tools that: enhance the reproducibility of your work by making it easier to share your code and data; make it easier to show off your work; improve your workflow by encouraging you to think systematically about your approach; and (although we won’t take advantage of this) make it easier to work in teams. Git is a version control system. One way you might be used to doing version control is to have various versions of your files: first_go.R, first_go-fixed.R, first_go-fixed-with-mons-edits.R. But this soon becomes cumbersome. Some of you may use dates, for instance: 2020-03-12-analysis.R, 2020-03-13-analysis.R, 2020-03-14-analysis.R, etc. While this keeps a record it can be difficult to search if you need to go back - will you really remember the date of some change in a week? How about a month or a year? It gets unwieldy fairly quickly. Instead of this, Git allows you to only have one version of the file analysis.R and it keeps a record of the changes to that file, and a snapshot of that file at a given point in time. When it takes that snapshot is determined by you. When you want Git to take a snapshot you additionally include a message, saying what changed between this snapshot and the last. In that way, there is only ever one version of the file, but the history can be more easily searched. The issue is that Git was designed for software developers. As such, while it works, it can be a little ungainly for non-developers (Figure 4.2). Figure 4.2: An infamous response to the launch of Dropbox in 2007, trivialising the use-case for Dropbox, and while this actually would work, it wouldn’t for most of us. Hence, GitHub, GitLab, and various other companies offer easier-to-use services that build on Git. GitHub used to be the weapon of choice, but they were sold to Microsoft in 2018 and since then other variants such as GitLab have risen in popularity. We will introduce GitHub here because it remains the most popular, and it is built into RStudio, however you should feel free to explore other options. One of the hardest aspects of Git, and the rest, for me was the terminology. Folders are called ‘repos.’ Saving is called a ‘commit.’ You’ll get used to it eventually, but just so you know - it’s not you, it’s Git - feeling confused it entirely normal These are brief notes, and you should refer to Jenny Bryan’s book for further detail. Frankly, I can’t improve on Bryan’s book and I use them regularly myself. 4.4.2 Git Check if you have Git installed by opening R Studio and then going to the Terminal and typing the following and then return. git --version If you get a version number, then you are done (Figure 4.3). Figure 4.3: How to access the Terminal within R Studio If you have a Mac then Git should come pre-installed, if you have Windows then there’s a chance, and if you have Linux then you probably don’t need this guide. If you don’t get a version number, then you need to install it. Please go to Chapter 5 of Jenny Bryan (2020) for some instructions based on your operating system. After you have Git, then you need to tell it your username and email. You need this because Git adds this information whenever you take a ‘snapshot,’ or to use Git’s language whenever you make a commit. Again, within the Terminal, type the following, replacing the details with yours, and then return after each line. git config --global user.name &#39;Jane Doe&#39; git config --global user.email &#39;jane@example.com&#39; git config --global --list The details that you enter here will be public (there are various ways to hide your email address if you need to do this and GitHub provides instructions about this). Again, if you have issues or need more detailed instructions, please go to Chapter 7 of Jenny Bryan (2020). 4.4.3 GitHub The first step is to create an account on GitHub (https://github.com) (Figure 4.4). Figure 4.4: Sign up screen at GitHub GitHub doesn’t have the most intuitive user experience in the world, but we are now going to make a new folder (which is called a ‘repo’ in Git). You are looking for a plus sign in the top right, and then select ‘New Repository’ (Figure 4.5). Figure 4.5: Create a new repository At this point you can add a sensible name for your repo. Leave it as public (you can delete it later if you want). And check the box to initialize with a readme. In the ‘Add .gitignore’ option you can leave it for now, but if you start using GitHub more regularly then you may like to select the R option here. (That just tells Git to ignore various files.) After that, just click the button to create a new repository (Figure 4.6). Figure 4.6: Create a new repository, really You’ll now be taken to a screen that is fairly empty, but the details that you need are in the green ‘Clone or Download’ button, then click the clipboard (Figure 4.7). Figure 4.7: Get the details of your new repository Now you need to open Terminal, and use cd to get to where you want to save the folder, then: git clone https://github.com/RohanAlexander/test.git At this point, a new folder has been created. We can now interact with it. The first step is almost always to pull the latest changes with git pull (this is slightly pointless in this example because it’s just us but it’s a good habit to get into). We can then make a change to the folder, for instance, update the readme, and then save it as usual. Once this is done, we need to add, commit, and push. As before, use cd to navigate to your folder, then git status to see if there is anything going on (you should see some reference to the change you made). Then git add -A adds the changes to the staging area (this seems pointless, and it is in this context, but this allows you to specify specific files and similar if needed). Then git status to check what has happened. Then git commit -m \"Minor update to readme\", and then git status to check on everything, and finally git push. To summarise (assuming you are in the relevant folder): git pull git status git add -A git status git commit -m &quot;Short commit message&quot; git status git push 4.4.4 Using Git within RStudio I promised that GitHub was built into RStudio, but so far, we’ve not really taken advantage of that. The way to do it is to create a new repo in GitHub, and copy the information, as before. At this point, you open RStudio, select Files, New Project, Version Control, Git, and paste the information for the repo. Go through the rest of it, saving the folder somewhere sensible, and clicking ‘Open in new session.’ This will then create a new folder on your computer which will be a Git folder that is linked to the GitHub repo that you created. At this point, you’ll have a ‘Git’ tab (Figure 4.8). Figure 4.8: The Git pane in R Studio First pull (click the blue down arrow). Now you want to tick the ‘staged’ box against the files that you want to commit. Then click ‘Commit.’ Type a message in the ‘Commit message’ box and then click ‘Commit.’ Finally ‘Push.’ Again, the details are in Jenny Bryan (2020), especially Chapter 12. 4.4.5 Next steps We haven’t really taken advantage of GitHub’s features in terms of teams or branches. That is certainly something that you should get into once you have more confidence with these basics. As always, when it comes to Git for data scientists who use R, you should go to the relevant sections of Jenny Bryan (2020). 4.5 Using R in practice 4.5.1 Introduction This section is what do when your code doesn’t do what you want, discusses a mindset that may help when doing quantitative analysis with R, and finally, some recommendations around how to write your code. 4.5.2 Getting help Programming is hard and everyone struggles sometimes. At some point your code won’t run or will throw an error. This is normal, and it happens to everyone. It happens to me on a daily, sometimes hourly, basis. Everyone gets frustrated. There are a few steps that are worthwhile taking when this happens: Sometimes the error messages in R are useful. Read it carefully and see if there’s anything of use in it. At the very least, if you get the same message in the future, hopefully you might remember how you solved the problem this time! If you’re getting an error then try googling it, (I find it can help to include the term ‘R’ or ‘tidyverse’ or the relevant package name). If there’s a particular function that seems to be giving trouble, have a look at the help file for it. Sometimes you might be putting in the arguments in the wrong order. You can do this with ‘?function’ e.g. for help with select, you would type ‘?select’ and then run that line. Check the class of the object. Sometimes R is a little fussy and converting the class can help. If your code just isn’t running, then try searching for what you are trying to do, e.g. ‘save PDF of graph in R made using ggplot.’ Almost always there are relevant blog posts or Stack Overflow answers that will help. Try to restart R and R Studio and load everything again. Try to restart your computer. There are a few small mistakes that I often make and may be worth checking in case you make them too: check the class e.g. class(my_dataset$its_column) to make sure that is what it should be; when you’re using ggplot make sure you use ‘+’ not ‘%&gt;%’; and check whether you are using ‘.’ when you shouldn’t be, or vice versa. It’s almost always helpful to take a break and come back the next day. Asking for help is a skill that you will get better at. Try not to say ‘this doesn’t work,’ or ‘I tried everything’ or ‘here’s the error message, what do I do?’ In general, it’s not possible to help based on that because there are too many possibilities. Instead: Provide a small example of your data, and code, and detail what is going wrong. Document what you have tried so far - what Stack Overflow pages have you looked at and why are they not quite what you’re after? What RStudio Community pages have you tried? Be clear about the outcome that you would like. As the RStudio Community welcome page says ‘your job is to make it as easy as possible for others to help you.’ To enable that to happen you need to ‘create a minimal reproducible example, or reprex for short’ You can get more information about this here, but basically what is needed is that you code is reproducible (so include things like library(), etc) and that is it minimal - that means making a very simple smaller example and reproducing the error on that small example. Usually doing this actually allows you to solve your own problem. If it doesn’t then it’ll allow someone else a fighting chance as being able to help you. I especially recommend the reprex package (Jennifer Bryan et al. 2019). There’s almost no chance that you’ve got a problem that someone hasn’t addressed before, it’s just a matter of finding the answer! Try to be tenacious with this and learn how to solve your own problems. 4.5.3 Mentality (Y)ou are a real, valid, competent user and programmer no matter what IDE you develop in or what tools you use to make your work work for you (L)et’s break down the gates, there’s enough room for everyone Sharla Gelfand, 10 March 2020. I’m a little hesitant to make suggestions with regard to mentality. If you write code, then you’re a coder regardless of how you do it, what you’re using it for, or who you are. But I want to share a few traits that I have found have been useful to cultivate in myself. That said, entirely, whatever works for you is great, so take or leave this section. Focused: I’ve found that having an aim to ‘learn R’ or something similar tends to be problematic, because there’s no real end point to that. Instead, I would recommend smaller, more specific goals, such as ‘make a histogram about the 2019 Canadian Election with ggplot.’ That is something that you can focus on and achieve. With more nebulous goals it becomes easier to get lost on tangents, much more difficult to get help, and I’ve noticed that people who have nebulous goals seem to give up. Curious: I’ve found that it’s useful to just have a go. In general, the worst that happens is that you waste your time and have to give up. You can rarely break something irreparably with code. If you want to know what happens if you pass a ‘vector’ instead of a ‘dataframe’ to ggplot then just try it. Pragmatic: At the same time, I’ve found that it’s best to try to stick within the bounds of what I know and just make one small change each time. For instance if you’re wanting to do some regression, and curious about the tidymodels package (Kuhn and Wickham 2020) instead of lm(). Perhaps you could just use one aspect from the tidymodels package initially and then make another change next time. In my opinion ugly code that gets the job done, is better than beautiful code that is never finished. Tenacious: This is a balancing act. I always find there are unexpected problems and issues with every project. On the one hand persevering despite these is a good tendency. But on the other hand, I’ve learned that sometimes I need to be prepared to give up on something if it doesn’t seem like a break-through is possible. This is where I have found that mentors can be useful as they tend to have a better idea. This is also where planning comes in. Planned: I have found it is very useful to plan out what you are going to do. For instance, you may want to make a histogram of the 2019 Canadian Election. I find it useful to plan the steps that are needed and even to sketch out how I might implement each step. For instance, the first step is to get the data. What packages might be useful? Where might the data be? What is our back-up plan for if we can’t find the data in that initial spot? Done is better than perfect: We all have various perfectionist tendencies to a certain extent, but I recommend that you try to turn them off to a certain extent when it comes to R. In the first instance just try to write code that works, especially in the early days. You can always come back and improve aspects of it. But it is important to actually ship. 4.5.4 Code comments Comment your code. There is no one way to write code, especially in R. However, there are some general guidelines that will make it easier for you even if you’re just working on your own. Comment your code. Comments in R can be added by including the # symbol. The shortcut on Mac is ‘Command + Shift + m.’ You don’t have to put a comment at the start of the line, it can be midway through. In general, you don’t need to comment what every aspect of your code is doing but you should comment parts that are not obvious. For instance, if you read in some value then you may like to comment where it is coming from. Comment your code. You should comment why you are doing something. What are you trying to achieve? Comment your code. You must comment to explain weird things. Like if you’re removing some specific row, say row 27, then why are you removing that row? It may seem obvious in the moment, but future-you in six months won’t remember. Comment your code. I like to break my code into sections. For instance, setting up my workspace, reading in datasets, manipulating and cleaning the dataset, analysing the datasets, and finally producing tables and figures. While it can be difficult to speak generally, I usually separate each of those certainly with comments explaining what is going on, and sometimes into separate files, depending on the length. Comment your code. Additionally, at the top of each file I put basic information, such as the purpose of the file, and pre-requisites or dependencies, the date, the author and contact information, and finally and red-flags, bodies, or todos. Comment your code. At the very least I recommend something like the following for every R script: #### Preamble #### # Purpose: Brief sentence about what this script does # Author: Your name # Data: The date it was written # Contact: Add your email # License: Think about how your code may be used # Pre-requisites: # - Maybe you need some data or some other script to have been run? #### Workspace setup #### # Don&#39;t keep the install.packages line - just comment out if need be # Load libraries library(tidyverse) # Read in the raw data. raw_data &lt;- readr::read_csv(&quot;inputs/data/raw_data.csv&quot;) #### Next section #### ... 4.5.5 Learning more One of the great aspects of R is that there is a friendly community of people who use it. There are a variety of ways that I learn about new tricks, functions, and packages including: R Ladies ‘is a world-wide organization to promote gender diversity in the R community.’ Whether that is your cup of tea or not, they tend to share fantastic material on their twitter feeds: https://twitter.com/rladiesglobal and https://twitter.com/WeAreRLadies (where someone takes over the account for the week, so the enthusiasm never wanes). If you’re in Toronto, then the local chapter is: https://rladies.org/canada-rladies/locality/Toronto/ and https://www.meetup.com/rladies-toronto/. R Weekly is a weekly newsletter that highlights new things that have happened recently. A popular R podcast is Not So Standard Deviations. Another great way to learn is by exchanging your code with others. Initially, just have them read it and give you feedback about it. But after you get a bit more confident run each other’s code. The most efficiently I’ve ever improved in my R journey has been by having Monica try to run my code. 4.6 Developing research questions Both qualitative and quantitative approaches have their place, but here we focus on quantitative approaches. (Qualitative research is important as well, and often the most interesting work has a little of both - ‘mixed methods.’) This means that we are subject to issues surrounding data quality, scales, measures, sources, etc. We are especially interested in trying to tease out causality. Broadly there are two ways to go about research: data-first, question-first. If you get a job somewhere typically you will initially be data-first. This means that you will need to work out the questions that you can reasonably answer with the data available to you. After you show some promise, you may be given the latitude to explore specific questions, possibly even gathering data specifically for that purpose. Contrast this with the example of the Behavioural Insights Team, (Gertler et al. 2016, 23) who got to design and then carry out experiments given the remit of the entire British government (as they were spun out of the prime minister’s office). When deciding the questions that you can reasonably answer with the data that are available, you need to think about: Theory: Do you have a reasonable expectation that there is something causal that could be determined? Charting the stock market - maybe, but might be better with haruspex because at least that way you have something you could eat. You need a reasonable theory of how \\(x\\) may be affecting \\(y\\). Importance: There are plenty of trivial questions that you could ask, but it’s important to not waste your time. The importance of a question also helps with motivation when you are on your fourth straight week of cleaning data and de-bugging your code. It also (and this becomes important) makes it easier to get talented people to work with you, or similarly to convince people to fund you or allow you to work on this project. Availability: Can you reasonably expect get more data about this research question in the future or is this the extent of the data that could be gathered? Iteration: Is this something that you can come back to and run often or is this a once-off analysis? The ‘FINER framework’ as a mnemonic device used in medicine.3 This framework reminds us to ask questions that are (Hulley 2007): Feasible: Adequate number of subjects; adequate technical expertise; affordable in time and money; manageable in scope. Interesting: Getting the answer intrigues investigator, peers and community. Novel: Confirms, refutes or extends previous findings Ethical: Amenable to a study that institutional review board will approve. Relevant: To scientific knowledge; to clinical and health policy; to future research. Farrugia et al. (2010) build on this in terms of developing research questions and recommend ‘PICOT’: Population: What specific population are you interested in? Intervention: What is your investigational intervention? Comparison group: What is the main alternative to compare with the intervention? Outcome of interest: What do you intend to accomplish, measure, improve or affect? Time: What is the appropriate follow-up time to assess outcome Often time will be constrained, possibly in interesting ways and these can guide your research. If we are interested in the effect of Trump’s tweets on the stock market, then that can be done just by looking at the minutes (milliseconds?) after he tweets. But what if we are interested in the effect of a cancer drug on long term outcomes? If the effect takes 20 years, then we either have to wait a while, or we need to look at people who were treated in 2000, but then we have selection effects and different circumstances to if we give the drug today. Often the only reasonable thing to do is to build a statistical model, but then we need adequate sample sizes, etc. Usually, the creation of a counterfactual is crucial. We’ll discuss counterfactuals a lot more later, but briefly, a counterfactual is an if-then statement in which the ‘if’ is false. Consider the example of Humpty Dumpty from Lewis Carroll’s Through the Looking-Glass: Figure 4.9: Humpty Dumpty example Humpty is satisfied with what would happen if he were to fall off, even though he is similarly satisfied that this would never happen. (I won’t ruin the story for you.) The comparison group often determines your results e.g. the relationship between VO2 and athletic outcomes, compared with elite athletic outcomes. Finally, we can often dodge ethics boards in data science, especially once you leave university. Typically, ethics guides from medicine and other fields are focused on ethics boards. But we often don’t have those in data science applications. Even if your intentions are unimpeachable, I want to suggest one additional aspect to think about, and that is Bayes theorem: \\[P(A|B) = \\frac{P(B|A)\\times P(A)}{P(B)}\\] (The probability of A given B depends on the probability of B given A, the probability of A, and the probability of B.) To see why this may be relevant, let’s go to the canonical Bayes example: There is some test for a disease that is 99 per cent accurate both ways (that is, if a person actually has the disease there is a 99 per cent chance the test says positive, and is a person does not have the disease then there is a 99 per cent chance the test says negative). Let’s just say that only 0.005 of the population has the disease. Then if we randomly pick someone from the general population then the chance that they have the disease is outstandingly low. This is even if they test positive: \\[\\frac{0.99\\times0.005}{0.99\\times0.005 + 0.01\\times0.995} \\approx 33.2\\] To see why this may be relevant, consider the example of Google’s AI cancer testing (Shetty and Tse 2020). Basically, what they have done is to train a model that can identify breast cancer. They claim ‘greater accuracy, fewer false positives, and fewer false negatives than experts.’ I, and many others (Aschwanden 2020), would argue this is probably not where we would want these resources directed at this point. Even when perfectly healthy people go and get screened, they tend to find various things that are ‘wrong’ with them. The issue is that they’re perfectly healthy and that we’ve rarely got a good idea as to whether that aspect that was flagged by the test is a big deal or not. Given low prevalence in the community, we probably don’t want wide-spread use of a particular testing regime that only looks at one aspect (i.e. the mammogram in this case). Bayes rule guides us that the danger caused by the unnecessary ‘treatment’ would probably outweigh the benefits. The authors of that Google blog post likely have unimpeachable ethics, but they may not understand Bayes rule. Thanks to Aaron Miller for pointing me to this.↩︎ "],["static-communication.html", "Chapter 5 Static communication 5.1 Introduction 5.2 Graphs 5.3 Tables 5.4 Maps 5.5 Writing", " Chapter 5 Static communication Last updated: 25 April 2021. Required reading (The) Economist, 2013, ‘Johnson: Those six little rules,’ Prospero, 29 July 2013, available at: https://www.economist.com/prospero/2013/07/29/johnson-those-six-little-rules. Alexander, Monica, 2019, ‘The concentration and uniqueness of baby names in Australia and the US,’ https://www.monicaalexander.com/posts/2019-20-01-babynames/. (Look at how Monica explains concepts, especially the Gini coefficient, in a way that you can understand even if you’ve never heard of it before.) Bronner, Laura, 2020, ‘Quant Editing,’ http://www.laurabronner.com/quant-editing. (Read these points and evaluate your own writing against them. It’s fine to not comply with them if you have a good reason, but you need to know that you’re not complying with them). Girouard, Dave, 2020, ‘A Founder’s Guide to Writing Well,’ First Round Review, 4 August, https://firstround.com/review/a-founders-guide-to-writing-well/. Graham, Paul, 2020, ‘How to Write Usefully,’ http://paulgraham.com/useful.html. (Graham is good at writing for a programmer, but if you have a similar background then you may like this.) Healy, Kieran, 2019, Data Visualization: A Practical Introduction, Princeton University Press, Chapters 3 and 4, https://socviz.co/. Hodgetts, Paul, 2020, ‘The ggfortify Package,’ 31 December, https://www.hodgettsp.com/posts/r-ggfortify/. Wickham, Hadley, and Garrett Grolemund, 2017, R for Data Science, Chapter 28, https://r4ds.had.co.nz/. Zinsser, William, 1976 [2016], On Writing Well. (Any edition is fine. This book is included because if you’re serious about improving your writing then you should start with this book. It only takes a few hours to read. You’ll go onto other books, but start with this one.) Zinsser, William, 2009, ‘Writing English as a Second Language,’ Lecture, Columbia Graduate School of Journalism, 11 August, https://theamericanscholar.org/writing-english-as-a-second-language/. (I’m realistic enough to realise that requiring a book, even though I’ve said it’s great and it’s short, is a bit of a stretch. If you really don’t want to commit to reading the Zinsser, then please at least read this ‘crib notes’ version of it.) Required viewing Kuriwaki, Shiro, 2020, ‘Making maps in R with sf,’ 1 March, freely available at: https://vimeo.com/394800836. Recommended reading (The) Economist, 1991 [2014], ‘The Economist Style Guide,’ Twelfth edition. (Any edition is fine. Pick a point or two each day and think about how it related to your own writing.) Cochrane, John H., 2005, ‘Writing Tips for Ph. D. Students,’ https://faculty.chicagobooth.edu/john.cochrane/research/papers/phd_paper_writing.pdf. (This is aimed at academic research papers, but parts are still broadly relevant. And if you’re going into academia then this is very relevant.) Codrey, Laura, 2013, ‘Churchill’s call for brevity,’ 17 October, https://blog.nationalarchives.gov.uk/churchills-call-for-brevity/. Engel, Claudia A, 2019, Using Spatial Data with R, 11 February, Chapter 3 Making Maps in R, freely available at: https://cengel.github.io/R-spatial/mapping.html. Five Thirty Eight, 2020, Pick almost any article in their sports (https://fivethirtyeight.com/sports/) or politics (https://fivethirtyeight.com/politics/) sections. (The people at 538 write beautifully. Look at how their titles tell you exactly what is going on, or what they found. Look at how nicely their first paragraphs motivates you to read the rest of the article. Why am I reading about BYU basketball when I’m indifferent to both BYU and college basketball? Because that title and first paragraph hooked me.) Graham, Paul, 2005, ‘Writing, Briefly,’ http://paulgraham.com/writing44.html. Lovelace, Robin, Jakub Nowosad, Jannes Muenchow, 2020, Geocomputation with R, 29 March, Chapter 8, Making maps with R, freely available at: https://geocompr.robinlovelace.net/adv-map.html. Patrick, Cameron, 2019, ‘Plotting multiple variables at once using ggplot2 and tidyr,’ 26 November, https://cameronpatrick.com/post/2019/11/plotting-multiple-variables-ggplot2-tidyr/. Patrick, Cameron, 2020, ‘Making beautiful bar charts with ggplot,’ 15 March, https://cameronpatrick.com/post/2020/03/beautiful-bar-charts-ggplot/. Shapiro, Jesse M., ‘Four Steps to an Applied Micro Paper,’ https://www.brown.edu/Research/Shapiro/pdfs/foursteps.pdf. (This is mostly recommended for the part about ‘the robot’ with regard to your data section.) Shapiro, Julian, ‘Writing Well,’ https://www.julian.com/guide/write/intro. Strunk, William Jr., 1959 [2009] ‘The Elements of Style.’ (Any edition is fine. Eventually you’ll move beyond this, but it’s important to know the rules before you break them). Vanderplas, Susan, Dianne Cook, and Heike Hofmann, 2020, ‘Testing Statistical Charts: What Makes a Good Graph?’ Annual Review of Statistics and Its Application, https://www.annualreviews.org/doi/abs/10.1146/annurev-statistics-031219-041252 Examples of well-written papers Barron, Alexander TJ, Jenny Huang, Rebecca L. Spang, and Simon DeDeo. “Individuals, institutions, and innovation in the debates of the French Revolution.” Proceedings of the National Academy of Sciences 115, no. 18 (2018): 4607-4612. r Chambliss, Daniel F. “The Mundanity of Excellence: An Ethnographic Report on Stratification and Olympic Swimmers.” Sociological Theory 7, no. 1 (1989): 70-86. doi:10.2307/202063. Joyner, Michael J. “Modeling: optimal marathon performance on the basis of physiological factors.” Journal of Applied Physiology, 70, no. 2 (1991): 683-687. Kharecha, Pushker A., and James E. Hansen, 2013, ‘Prevented mortality and greenhouse gas emissions from historical and projected nuclear power,’ Environmental science &amp; technology, 47, no. 9, pp. 4889-4895. Samuel, Arthur L., 1959, ‘Some studies in machine learning using the game of checkers,’ IBM Journal of research and development, 3, no. 3, pp. 210-229. Wardrop, Robert L., 1995, ‘Simpson’s paradox and the hot hand in basketball,’ The American Statistician, 49, no. 1, 24-28. Key concepts/skills/etc Show the reader your raw data, or as close as you can come to it. Use either geom_point or geom_bar initially. Writing efficiently and effectively is a requirement if you want your work to be convincing. Don’t waste your reader’s time. A good title says what the paper is about, a great title says what the paper found. For a six-page paper, a good abstract is a three to five sentence paragraph. For a longer paper your abstract can be slightly longer. Thinking of maps as a (often fiddly, but strangely enjoyable) variant of a usual ggplot. Broadening the data that we make available via interactive maps, while still telling a clear story. Becoming comfortable with (and excited about) creating static maps. Key libraries ggplot patchwork ggmap maps Key functions/etc ggplot::geom_point() ggplot::geom_bar() canada.cities geom_polygon() ggmap() map() map_data() Quiz I have a dataset that contains measurements of height (in cm) for a sample of 300 penguins, who are either the Adeline or Emperor species. I am interested in visualizing the distribution of heights by species in a graphical way. Please discuss whether a pie chart is an appropriate type of graph to use. What about a box and whisker plot? Finally, what are some considerations if you made a histogram? [Please write a paragraph or two for each aspect.] Assume the dataset and columns exist. Would this code work? data %&gt;% ggplot(aes(x = col_one)) %&gt;% geom_point() (pick one)? Yes No If I have categorical data, which geom should I use to plot it (pick one)? geom_bar() geom_point() geom_abline() geom_boxplot() Why are box plots often inappropriate (pick one)? They hide the full distribution of the data. They are hard to make. They are ugly. The mode is clearly displayed. Which of the following is the best title (pick one)? “Problem Set 1” “Unemployment” “Examining Canada’s Unemployment (2010-2020)” “Canada’s Unemployment Increased between 2010 and 2020” 5.1 Introduction In order to convince someone of your story, your paper must be well-written, well-organized, and easy to follow. It should flow easily from one point to the next. It should have proper sentence structure, spelling, vocabulary, and grammar. Each point should be articulated clearly and completely without being overly verbose. Papers should demonstrate your understanding of the topics you are writing about and your confidence in discussing the terms, techniques and issues that are relevant. References must be included and properly cited because this enhances your credibility. People who need to write: founders, VCs, lawyers, software engineers, designers, painters, data scientists, musicians, filmmakers, creative directors, physical trainers, teachers, writers. Learn to write. Sahil Lavingia. This is great advice. Writing well has done just as much for me as knowing how to code. I’d add that if you’re intimidated by writing, start a blog and write often about something you’re interested in. You’ll get better. At least that’s what I’ve done for the past 10 years. :) Vicki Boykis. This chapter is about writing. By the end of it you will have a better idea of how to write short, detailed, quantitative papers that communicate exactly what you want them to and don’t waste the time of your reader. One critical part of telling stories with data, is that it’s ultimately the data that has to convince them. You’re the medium, but the data are the message. To that end, the easiest way to try to convince someone of your story is to show them the data that allowed you to come to that story. Plot your raw data, or as close to it as possible. While ggplot is a fantastic tool for doing this, there is a lot to that package and so it can be difficult to know where to start. My recommendation is that you start with either a scatter plot or a bar chart. What is critical is that you show the reader your raw data. These notes run through how to do that. It then discusses some more advanced options, but the important thing is that you show the reader your raw data (or as close to it as you can). Students seem to get confused what ‘raw’ means; I’m using it to refer to as close to the original dataset as possible, so no sums, or averages, etc, if possible. Sometimes your data are too disperse for that or you’ve got other constraints, so there needs to be an element of manipulation. The main point is that you, at the very least, need to plot the data that you’re going to be modelling. If you are dealing with larger datasets then just take a 10/1/0.1/etc per cent sample. Figure 5.1: Show me the data! Source: YouTube screenshot. 5.2 Graphs Graphs are critical to tell a compelling story. And the most important thing with your graphs is to plot your raw data. Again: Plot. Your. Raw. Data. Figure 5.2 provides invaluable advice (thank you to Thomas William Rosenthal). Figure 5.2: How do we get started with our data? Let’s look at a somewhat fun example from the datasauRus package (Locke and D’Agostino McGowan 2018). library(datasauRus) # Code from: https://juliasilge.com/blog/datasaurus-multiclass/ datasaurus_dozen %&gt;% filter(dataset %in% c(&quot;dino&quot;, &quot;star&quot;, &quot;away&quot;, &quot;bullseye&quot;)) %&gt;% group_by(dataset) %&gt;% summarise(across(c(x, y), list(mean = mean, sd = sd)), x_y_cor = cor(x, y) ) %&gt;% ungroup() ## # A tibble: 4 x 6 ## dataset x_mean x_sd y_mean y_sd x_y_cor ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 away 54.3 16.8 47.8 26.9 -0.0641 ## 2 bullseye 54.3 16.8 47.8 26.9 -0.0686 ## 3 dino 54.3 16.8 47.8 26.9 -0.0645 ## 4 star 54.3 16.8 47.8 26.9 -0.0630 And despite these similarities at a summary statistic level, they’re actually very different, well, beasts, when you plot the raw data. datasaurus_dozen %&gt;% filter(dataset %in% c(&quot;dino&quot;, &quot;star&quot;, &quot;away&quot;, &quot;bullseye&quot;)) %&gt;% ggplot(aes(x=x, y=y, colour=dataset)) + geom_point() + theme_minimal() + facet_wrap(vars(dataset), nrow = 2, ncol = 2) + labs(colour = &quot;Dataset&quot;) 5.2.1 Bar chart Bar charts are useful when you have one variable that you want to focus on. Hint: you almost always have one variable that you want to focus on. Hence, you should almost always include at least one (and likely many) bar charts. Bar charts go by a variety of names, depending on their specifics. I recommend the R Studio Data Viz Cheat Sheet. To get started, let’s simulate some data. set.seed(853) number_of_observation &lt;- 10000 example_data &lt;- tibble(person = c(1:number_of_observation), smoker = sample(x = c(&quot;Smoker&quot;, &quot;Non-smoker&quot;), size = number_of_observation, replace = TRUE), age_died = runif(number_of_observation, min = 0, max = 100) %&gt;% round(digits = 0), height = sample(x = c(50:220), size = number_of_observation, replace = TRUE), num_children = sample(x = c(0:5), size = number_of_observation, replace = TRUE, prob = c(0.1, 0.2, 0.40, 0.15, 0.1, 0.05)) ) First, let’s have a look at the data. head(example_data) ## # A tibble: 6 x 5 ## person smoker age_died height num_children ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 1 Smoker 55 80 3 ## 2 2 Non-smoker 54 78 2 ## 3 3 Non-smoker 84 109 1 ## 4 4 Smoker 75 114 4 ## 5 5 Smoker 32 135 1 ## 6 6 Smoker 37 220 0 Now let’s plot the age distribution. Based on our simulated data, we’re expecting a fairly uniform plot. example_data %&gt;% ggplot(mapping = aes(x = age_died)) + geom_bar() Now let’s make it look a little better. There are themes that are built into ggplot, or you can install other themes from other packages, or you can edit aspects yourself. I’d recommend starting with the ggthemes package for some fun ones, but I tend to just use classic or minimal. Remember that you must always refer to your graphs in your text (Figure 5.3). example_data %&gt;% ggplot(mapping = aes(x = age_died)) + geom_bar() + theme_minimal() + labs(x = &quot;Age died&quot;, y = &quot;Number&quot;, title = &quot;Number of people who died at each age&quot;, caption = &quot;Source: Simulated data.&quot;) Figure 5.3: Number of people who died at each age We may want to facet by some variable, in this case whether the person is a smoker (Figure 5.4). example_data %&gt;% ggplot(mapping = aes(x = age_died)) + geom_bar() + theme_minimal() + facet_wrap(vars(smoker)) + labs(x = &quot;Age died&quot;, y = &quot;Number&quot;, title = &quot;Number of people who died at each age, by whether they smoke&quot;, caption = &quot;Source: Simulated data.&quot;) Figure 5.4: Number of people who died at each age, by whether they smoke Alternatively, we may wish to colour by that instead (Figure 5.5). I’ll filter to just a handful of age-groups to keep it tractable. example_data %&gt;% filter(age_died &lt; 25) %&gt;% ggplot(mapping = aes(x = age_died, fill = smoker)) + geom_bar(position = &quot;dodge&quot;) + theme_minimal() + labs(x = &quot;Age died&quot;, y = &quot;Number&quot;, fill = &quot;Smoker&quot;, title = &quot;Number of people who died at each age, by whether they smoke&quot;, caption = &quot;Source: Simulated data.&quot;) Figure 5.5: Number of people who died at each age, by whether they smoke It’s important to recognise that a boxplot hides the full distribution of a variable. Unless you need to communicate the general distribution of many variables at once then you should not use them. The same box plot can apply to very different distributions. 5.2.2 Scatter plot Often, we are also interested in the relationship between two series. We’ll do that with a scatter plot. In this case, let’s simulate some data, say years of education and income. set.seed(853) number_of_observation &lt;- 500 scatter_data &lt;- tibble(years_of_education = runif(n = number_of_observation, min = 10, max = 25), error = rnorm(n= number_of_observation, mean = 0, sd = 10000), ) %&gt;% mutate(income = years_of_education * 5000 + error, income = if_else(income &lt; 0, 0, income)) head(scatter_data) ## # A tibble: 6 x 3 ## years_of_education error income ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 15.4 -13782. 63180. ## 2 11.8 7977. 66985. ## 3 17.3 -9787. 76498. ## 4 14.7 12999. 86689. ## 5 10.6 -1500. 51302. ## 6 16.1 1911. 82202. Now let’s look at income as a function of years of education (Figure 5.6). scatter_data %&gt;% ggplot(mapping = aes(x = years_of_education, y = income)) + geom_point() + theme_minimal() + labs(x = &quot;Years of education&quot;, y = &quot;Income&quot;, title = &quot;Relationship between income and years of education&quot;, caption = &quot;Source: Simulated data.&quot;) Figure 5.6: Relationship between income and years of education 5.2.3 Never use box plots Box plots are almost never appropriate because they hide the distribution of data. To see this, consider some data from a beta distribution. left &lt;- rbeta(10000,5,2) right &lt;- rbeta(10000,2,5) middle &lt;- rbeta(10000,5,5) tricky_data &lt;- tibble(left_and_right = c( rbeta(10000,5,2), rbeta(10000,2,5) ), middle = rbeta(20000,1,1)) Then compare the box plots. boxplot(tricky_data$left_and_right) boxplot(tricky_data$middle) hist(tricky_data$left_and_right) hist(tricky_data$middle) 5.2.4 Other 5.2.4.1 Best fit If we’re interested in quickly adding a line of best fit then, continuing with the earlier income example, we can do that with geom_smooth() (Figure 5.7). scatter_data %&gt;% ggplot(mapping = aes(x = years_of_education, y = income)) + geom_point() + geom_smooth(method = lm, color = &quot;black&quot;) + theme_minimal() + labs(x = &quot;Years of education&quot;, y = &quot;Income&quot;, title = &quot;Relationship between income and years of education&quot;, caption = &quot;Source: Simulated data.&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 5.7: Relationship between income and years of education 5.2.4.2 Histogram If we want to get counts by groups, then we may want to use a histogram. Figure 5.8 shows the counts for our simulated incomes. scatter_data %&gt;% ggplot(mapping = aes(x = income)) + geom_histogram() + theme_minimal() + labs(x = &quot;Income&quot;, y = &quot;Number&quot;, title = &quot;Distribution of income&quot;, caption = &quot;Source: Simulated data.&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 5.8: Distribution of income 5.2.4.3 Multiple plots Finally, let’s try putting them together. We’re going to use the patchwork package (Pedersen 2020) and the penguins package for data. Don’t forget install.packages(\"palmerpenguins\") as this is probably the first time you’ve used the package. library(patchwork) library(palmerpenguins) p1 &lt;- ggplot(palmerpenguins::penguins) + geom_point(aes(bill_length_mm, bill_depth_mm)) + labs(x = &quot;Bill length (mm)&quot;, y = &quot;Bill depth (mm)&quot;) p2 &lt;- ggplot(palmerpenguins::penguins) + geom_bar(aes(species)) + labs(x = &quot;Species&quot;, y = &quot;Number&quot;) p1 + p2 And we can make things fairly involved fairly quickly. (p1 | p2) / p2 5.3 Tables Tables are also critical to tell a compelling story. We may prefer a table to a graph when there are only a few features that we want to focus on. We’ll use knitr::kable() alongside the ‘kableExtra’ package and also the gt package. Let’s start with the kable package and the summary dinosaur data from earlier. example_data &lt;- datasaurus_dozen %&gt;% filter(dataset %in% c(&quot;dino&quot;, &quot;star&quot;, &quot;away&quot;)) %&gt;% group_by(dataset) %&gt;% summarize( Mean = mean(x), Std_dev = sd(x), ) example_data %&gt;% knitr::kable() dataset Mean Std_dev away 54.26610 16.76983 dino 54.26327 16.76514 star 54.26734 16.76896 Even the defaults are pretty good, but we can add a few tweaks to make the table better. The first is that this many significant digits is inappropriate, we may also like to add a caption, make the column names consistent, and change the alignment. example_data %&gt;% knitr::kable(digits = 2, caption = &quot;My first table.&quot;, col.names = c(&quot;Dataset&quot;, &quot;Mean&quot;, &quot;Standard deviation&quot;), align = c(&#39;l&#39;, &#39;l&#39;, &#39;l&#39;) ) Table 5.1: My first table. Dataset Mean Standard deviation away 54.27 16.77 dino 54.26 16.77 star 54.27 16.77 The ‘’kableExtra’ package builds extra functionality (Zhu 2020). The gt package (Iannone, Cheng, and Schloerke 2020a) is a newer package that brings a lot of exciting features. However, being newer it sometimes has issues with PDF output. library(gt) example_data %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #ghkvxdrhjj .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #ghkvxdrhjj .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ghkvxdrhjj .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #ghkvxdrhjj .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #ghkvxdrhjj .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ghkvxdrhjj .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ghkvxdrhjj .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #ghkvxdrhjj .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #ghkvxdrhjj .gt_column_spanner_outer:first-child { padding-left: 0; } #ghkvxdrhjj .gt_column_spanner_outer:last-child { padding-right: 0; } #ghkvxdrhjj .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #ghkvxdrhjj .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #ghkvxdrhjj .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #ghkvxdrhjj .gt_from_md > :first-child { margin-top: 0; } #ghkvxdrhjj .gt_from_md > :last-child { margin-bottom: 0; } #ghkvxdrhjj .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #ghkvxdrhjj .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #ghkvxdrhjj .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ghkvxdrhjj .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #ghkvxdrhjj .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ghkvxdrhjj .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #ghkvxdrhjj .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #ghkvxdrhjj .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ghkvxdrhjj .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ghkvxdrhjj .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #ghkvxdrhjj .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ghkvxdrhjj .gt_sourcenote { font-size: 90%; padding: 4px; } #ghkvxdrhjj .gt_left { text-align: left; } #ghkvxdrhjj .gt_center { text-align: center; } #ghkvxdrhjj .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #ghkvxdrhjj .gt_font_normal { font-weight: normal; } #ghkvxdrhjj .gt_font_bold { font-weight: bold; } #ghkvxdrhjj .gt_font_italic { font-style: italic; } #ghkvxdrhjj .gt_super { font-size: 65%; } #ghkvxdrhjj .gt_footnote_marks { font-style: italic; font-size: 65%; } dataset Mean Std_dev away 54.26610 16.76982 dino 54.26327 16.76514 star 54.26734 16.76896 We could add sub-titles easily. example_data %&gt;% gt() %&gt;% tab_header( title = &quot;Summary stats can be misleading&quot;, subtitle = &quot;With an example from a dinosaur!&quot; ) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #ymmvvnknyk .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #ymmvvnknyk .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ymmvvnknyk .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #ymmvvnknyk .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #ymmvvnknyk .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ymmvvnknyk .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ymmvvnknyk .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #ymmvvnknyk .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #ymmvvnknyk .gt_column_spanner_outer:first-child { padding-left: 0; } #ymmvvnknyk .gt_column_spanner_outer:last-child { padding-right: 0; } #ymmvvnknyk .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #ymmvvnknyk .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #ymmvvnknyk .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #ymmvvnknyk .gt_from_md > :first-child { margin-top: 0; } #ymmvvnknyk .gt_from_md > :last-child { margin-bottom: 0; } #ymmvvnknyk .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #ymmvvnknyk .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #ymmvvnknyk .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ymmvvnknyk .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #ymmvvnknyk .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ymmvvnknyk .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #ymmvvnknyk .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #ymmvvnknyk .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ymmvvnknyk .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ymmvvnknyk .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #ymmvvnknyk .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ymmvvnknyk .gt_sourcenote { font-size: 90%; padding: 4px; } #ymmvvnknyk .gt_left { text-align: left; } #ymmvvnknyk .gt_center { text-align: center; } #ymmvvnknyk .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #ymmvvnknyk .gt_font_normal { font-weight: normal; } #ymmvvnknyk .gt_font_bold { font-weight: bold; } #ymmvvnknyk .gt_font_italic { font-style: italic; } #ymmvvnknyk .gt_super { font-size: 65%; } #ymmvvnknyk .gt_footnote_marks { font-style: italic; font-size: 65%; } Summary stats can be misleading With an example from a dinosaur! dataset Mean Std_dev away 54.26610 16.76982 dino 54.26327 16.76514 star 54.26734 16.76896 One common reason for needing a table is to report regression results. You should consider gtsummary, stargazer, and modelsummary. But at the moment, my favourite is modelsummary (Arel-Bundock 2020). library(modelsummary) mod &lt;- lm(y ~ x, datasaurus_dozen) modelsummary(mod) Model 1 (Intercept) 53.590 (2.119) x -0.106 (0.037) Num.Obs. 1846 R2 0.004 R2 Adj. 0.004 AIC 17383.0 BIC 17399.6 Log.Lik. -8688.506 F 8.072 5.4 Maps In many ways maps can be thought of as a fancy graph, where the x-axis is latitude, the y-axis is longitude, and there is some outline or a background image. We are used to this type of set-up, for instance, in a ggplot setting that is quite familiar. Static maps will be useful for printed output, such as a PDF or Word report, or where there is something in particular that you want to illustrate. ggplot() + geom_polygon( # First draw an outline data = some_data, aes(x = latitude, y = longitude, group = group )) + geom_point( # Then add points of interest data = some_other_data, aes(x = latitude, y = longitude) ) And while there are some small complications, for the most part it is as straight-forward as that. The first step is to get some data. And helpfully, there is some geographic data built into ggplot, and there is some other information built into a package called maps. library(maps) library(tidyverse) canada &lt;- map_data(database = &quot;world&quot;, regions = &quot;canada&quot;) canadian_cities &lt;- maps::canada.cities head(canada) ## long lat group order region subregion ## 1 -59.78760 43.93960 1 1 Canada Sable Island ## 2 -59.92227 43.90391 1 2 Canada Sable Island ## 3 -60.03775 43.90664 1 3 Canada Sable Island ## 4 -60.11426 43.93911 1 4 Canada Sable Island ## 5 -60.11748 43.95337 1 5 Canada Sable Island ## 6 -59.93604 43.93960 1 6 Canada Sable Island head(canadian_cities) ## name country.etc pop lat long capital ## 1 Abbotsford BC BC 157795 49.06 -122.30 0 ## 2 Acton ON ON 8308 43.63 -80.03 0 ## 3 Acton Vale QC QC 5153 45.63 -72.57 0 ## 4 Airdrie AB AB 25863 51.30 -114.02 0 ## 5 Aklavik NT NT 643 68.22 -135.00 0 ## 6 Albanel QC QC 1090 48.87 -72.42 0 With that information in hand we can then create a map of Canada that shows the cities with a population over 1,000. (The geom_polygon() function within ggplot draws shapes, by connecting points within groups. And the coord_map() function adjusts for the fact that we are making something that is 2D map to represent something that is 3D.) ggplot() + geom_polygon(data = canada, aes(x = long, y = lat, group = group), fill = &quot;white&quot;, colour = &quot;grey&quot;) + coord_map(ylim = c(40, 70)) + geom_point(aes(x = canadian_cities$long, y = canadian_cities$lat), alpha = 0.3, color = &quot;black&quot;) + theme_classic() + labs(x = &quot;Longitude&quot;, y = &quot;Latitude&quot;) # If I&#39;m being honest, this &#39;simple example&#39; took me six hours to work out. Firstly # to find Canada and then to find Canadian cities. As is often the case with R, there are many different ways to get started creating static maps. We’ve already seen how they can be built using simply ggplot, but here we’ll explore one package that has a bunch of functionalities built in that will make things easier: ggmap. There are two essential components to a map: 1) some border or background image (also known as a tile); and 2) something of interest within that border or on top of that tile. In ggmap, we will use an open-source option for our tile, Stamen Maps (maps.stamen.com), and we will use plot points based on latitude and longitude. 5.4.1 Australian polling places Like Canada, in Australia people go to specific locations, called booths, to vote. These booths have latitudes and longitudes and so we can plot these. One reason we may like to do this is to notice patterns over geographies. To get started we need to get a tile. We are going to use ggmap to get a tile from Stamen Maps, which builds on OpenStreetMap (openstreetmap.org). The main argument to this function is to specify a bounding box. This requires two latitudes - one for the top of the box and one for the bottom of the box - and two longitudes - one for the left of the box and one for the right of the box. (It can be useful to use Google Maps, or an alternative, to find the values of these that you need.) The bounding box provides the coordinates of the edges that you are interested in. In this case I have provided it with coordinates such that it will be centered around Canberra, Australia (our equivalent of Ottawa - a small city that was created for the purposes of being the capital). library(ggmap) bbox &lt;- c(left = 148.95, bottom = -35.5, right = 149.3, top = -35.1) Once you have defined the bounding box, then the function get_stamenmap() will get the tiles in that area. The number of tiles that it needs to get depends on the zoom, and the type of tiles that it gets depends on the maptype. I’ve chosen the maptype that I like here - the black and white option - but the helpfile specifies a few others that you may like. At this point you can pass your maps to ggmap and it will plot the tile! It will be actively downloading these tiles, so you need an internet connection. canberra_stamen_map &lt;- get_stamenmap(bbox, zoom = 11, maptype = &quot;toner-lite&quot;) ggmap(canberra_stamen_map) Once we have a map then we can use ggmap() to plot it. (That circle in the middle of the map is where the Australian Parliament House is… yes, our parliament is surrounded by circular roads (we call them ‘roundabouts’), actually it’s surrounded by two of them.) Now we want to get some data that we will plot on top of our tiles. We will just plot the location of the polling places, based on which ‘division’ (the Australian equivalent to ‘ridings’ in Canada) it is. This is available here: https://results.aec.gov.au/20499/Website/Downloads/HouseTppByPollingPlaceDownload-20499.csv. (The Australian Electoral Commission (AEC) is the official government agency that is responsible for elections in Australia.) # Read in the booths data for each year booths &lt;- readr::read_csv(&quot;https://results.aec.gov.au/24310/Website/Downloads/GeneralPollingPlacesDownload-24310.csv&quot;, skip = 1, guess_max = 10000) head(booths) ## # A tibble: 6 x 15 ## State DivisionID DivisionNm PollingPlaceID PollingPlaceTypeID PollingPlaceNm ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 ACT 318 Bean 93925 5 Belconnen BEAN … ## 2 ACT 318 Bean 93927 5 BLV Bean PPVC ## 3 ACT 318 Bean 11877 1 Bonython ## 4 ACT 318 Bean 11452 1 Calwell ## 5 ACT 318 Bean 8761 1 Chapman ## 6 ACT 318 Bean 8763 1 Chisholm ## # … with 9 more variables: PremisesNm &lt;chr&gt;, PremisesAddress1 &lt;chr&gt;, ## # PremisesAddress2 &lt;chr&gt;, PremisesAddress3 &lt;chr&gt;, PremisesSuburb &lt;chr&gt;, ## # PremisesStateAb &lt;chr&gt;, PremisesPostCode &lt;chr&gt;, Latitude &lt;dbl&gt;, ## # Longitude &lt;dbl&gt; This dataset is for the whole of Australia, but as we are just going to plot the area around Canberra we will filter to that and only to booths that are geographic (the AEC has various options for people who are in hospital, or not able to get to a booth, etc, and these are still ‘booths’ in this dataset). # Reduce the booths data to only rows with that have latitude and longitude booths_reduced &lt;- booths %&gt;% filter(State == &quot;ACT&quot;) %&gt;% select(PollingPlaceID, DivisionNm, Latitude, Longitude) %&gt;% filter(!is.na(Longitude)) %&gt;% # Remove rows that don&#39;t have a geography filter(Longitude &lt; 165) # Remove Norfolk Island Now we can use ggmap in the same way as before to plot our underlying tiles, and then build on that using geom_point() to add our points of interest. ggmap(canberra_stamen_map, extent = &quot;normal&quot;, maprange = FALSE) + geom_point(data = booths_reduced, aes(x = Longitude, y = Latitude, colour = DivisionNm), ) + scale_color_brewer(name = &quot;2019 Division&quot;, palette = &quot;Set1&quot;) + coord_map(projection=&quot;mercator&quot;, xlim=c(attr(map, &quot;bb&quot;)$ll.lon, attr(map, &quot;bb&quot;)$ur.lon), ylim=c(attr(map, &quot;bb&quot;)$ll.lat, attr(map, &quot;bb&quot;)$ur.lat)) + labs(x = &quot;Longitude&quot;, y = &quot;Latitude&quot;) + theme_minimal() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) We may like to save the map so that we don’t have to draw it every time, and we can do that in the same way as any other graph, using ggsave(). ggsave(&quot;outputs/figures/map.pdf&quot;, width = 20, height = 10, units = &quot;cm&quot;) Finally, the reason that I used Stamen Maps and OpenStreetMap is because it is open source, however you can also use Google Maps if you want. This requires you to first register a credit card with Google, and specify a key, but with low usage should be free. The get_googlemap() function with ggmap, brings some nice features that get_stamenmap() does not have. For instance, you can enter a placename and it’ll do it’s best to find it rather than needing to specify a bounding box. 5.4.2 Toronto bike parking Let’s see another example of a static map, this time using Toronto data accessed via the opendatatoronto package. The dataset that we are going to plot is available here: https://open.toronto.ca/dataset/street-furniture-bicycle-parking/. # This code is based on code from: https://open.toronto.ca/dataset/street-furniture-bicycle-parking/. library(opendatatoronto) # (The string identifies the package.) resources &lt;- list_package_resources(&quot;71e6c206-96e1-48f1-8f6f-0e804687e3be&quot;) # In this case there is only one dataset within this resource so just need the first one raw_data &lt;- filter(resources, row_number()==1) %&gt;% get_resource() write_csv(raw_data, &quot;inputs/data/bike_racks.csv&quot;) head(raw_data) Now that we’ve saved a copy of the data, we can use that one. First, we need to clean it up a bit. There are some clear errors in the ADDRESSNUMBERTEXT field, but not too many, so we’ll just ignore it. raw_data &lt;- read_csv(&quot;inputs/data/bike_racks.csv&quot;) # We&#39;ll just focus on the data that we want bike_data &lt;- tibble(ward = raw_data$WARD, id = raw_data$ID, status = raw_data$STATUS, street_address = paste(raw_data$ADDRESSNUMBERTEXT, raw_data$ADDRESSSTREET), latitude = raw_data$LATITUDE, longitude = raw_data$LONGITUDE) rm(raw_data) Some of the bike racks were temporary so remove them and also let’s just look at the area around the university, which is Ward 11 # Only keep ones that still exist bike_data &lt;- bike_data %&gt;% filter(status == &quot;Existing&quot;) %&gt;% select(-status) bike_data &lt;- bike_data %&gt;% filter(ward == 11) %&gt;% select(-ward) If you look at the dataset at this point, then you’ll notice that there is a row for every bike parking spot. But we don’t really need to know that, because sometimes there are lots right next to each other. Instead, we’d just like the one point (we’ll take advantage of this in an interactive graph in a moment). So, we want to create a count by address, and then just get one instance per address. bike_data &lt;- bike_data %&gt;% group_by(street_address) %&gt;% mutate(number_of_spots = n(), running_total = row_number() ) %&gt;% ungroup() %&gt;% filter(running_total == 1) %&gt;% select(-id, -running_total) head(bike_data) ## # A tibble: 6 x 4 ## street_address latitude longitude number_of_spots ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 8 Kensington Ave 43.7 -79.4 1 ## 2 87 Avenue Rd 43.7 -79.4 4 ## 3 162 Mc Caul St 43.7 -79.4 1 ## 4 147 Baldwin St 43.7 -79.4 2 ## 5 888 Yonge St 43.7 -79.4 1 ## 6 180 Elizabeth St 43.7 -79.4 10 write_csv(bike_data, &quot;outputs/data/bikes.csv&quot;) Now we can grab our tile and add our bike rack data onto it. bbox &lt;- c(left = -79.420390, bottom = 43.642658, right = -79.383354, top = 43.672557) toronto_stamen_map &lt;- get_stamenmap(bbox, zoom = 14, maptype = &quot;toner-lite&quot;) ggmap(toronto_stamen_map, maprange = FALSE) + geom_point(data = bike_data, aes(x = longitude, y = latitude), alpha = 0.3 ) + labs(x = &quot;Longitude&quot;, y = &quot;Latitude&quot;) + theme_minimal() 5.4.3 Geocoding To this point we just assumed that we already had geocoded data. The places ‘Canberra, Australia,’ or ‘Ottawa, Canada,’ are just names, they don’t actually inherently have a location. In order to plot them we need to get a latitude and longitude for them. The process of going from names to coordinates is called geocoding. There are a range of options to geocode data in R, but one good package is tidygeocoder (Cambon and Belanger 2021). To get started using the package we need a dataframe of locations. So we’ll just quickly make one here. some_locations &lt;- tibble(city = c(&#39;Canberra&#39;, &#39;Ottawa&#39;), country = c(&#39;Australia&#39;, &#39;Canada&#39;)) tidygeocoder::geo(city = some_locations$city, country = some_locations$country, method = &#39;osm&#39;) ## # A tibble: 2 x 4 ## city country lat long ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Canberra Australia -35.3 149. ## 2 Ottawa Canada 45.4 -75.7 5.5 Writing I had not indeed published anything before I commenced “The Professor,” but in many a crude effort, destroyed almost as soon as composed, I had got over any such taste as I might once have had for ornamented and redundant composition, and come to prefer what was plain and homely. Currer Bell (aka Charlotte Brontë), The Professor. 5.5.1 Title, abstract, and introduction A title is the first opportunity that you have to tell the reader your story. Ideally you will tell the reader exactly what you found. An effective title is critical in order to get your work read when there are other competing priorities. A title doesn’t have to be ‘cute’ to be great. Good: ‘On the 2019 Canadian Federal Election.’ (At least the reader knows what the paper is about.) Better: ‘The Liberal Party performance in the 2019 Canadian Federal Election.’ (At least the reader knows what the paper is about more specifically.) Even better: ‘The Liberal Party did poorly in rural areas in the 2019 Canadian Federal Election.’ (The reader knows what the paper is about.) You should put your name and the date on the paper because this provides an important context to the paper. For a six-page paper, a good abstract is a three to five sentence paragraph. For a longer paper your abstract can be slightly longer. The abstract should say: What you did, what you found, and why the reader should care. Each of these should just be a sentence or two, so keep it very high level. You should then have an introduction that tells the reader everything they need to know. You are not writing a mystery story - tell the reader the most important points in the introduction. For a six-page paper, your introduction may be two or three paragraphs. Four would likely be too much, but it depends on the context. Your introduction should set the scene and give the reader some background. For instance, you may like to start of a little broader, to provide some context to your paper. You should then describe how your paper fits into that context. Then give some high-level results - provide more detail than you provided in the abstract, but don’t get into the weeds - and finally broadly discuss next steps or glaring weaknesses. With regard to that high-level result: you need to pick one. If you have a bunch of interesting findings, then good for you, but pick one and write your introduction around that. If it’s compelling enough then the reader will end up reading all your other interesting findings in the discussion/results sections. Finally, you should highlight the remainder of the paper. As an example: The Canadian Liberal Party has always struggled in rural ridings. In the past 100 years they have never won more than 25 per cent of them. But even by those standards the 2019 Federal Election was a disappointment with the Liberal Party winning only 2 of the 40 rural ridings. In this paper we look at why the performance of the Liberal Party in this most recent election was so poor. We construct a model in which whether the Liberal Party won the riding is explained by the number of farms in the riding, the average internet connectivity, and the median age. We find that as the median age of a riding increases, the likelihood that a riding was won by the Liberal Party decreases by 14 percentage points. Future work could expand the time horizon that is considered which would allow a more nuanced understanding of these effects. The remainder of this paper is structured as follows: Section 2 discusses the data, Section 3 discusses the model, Section 4 presents the results, and finally Section 5 discusses our findings and some weaknesses. The recommended readings provide some lovely examples of titles, abstracts, and introductions. Please take the time to briefly read these papers. 5.5.2 Figures, tables, equations, and technical terms Figure and tables are a critical aspect of convincing people of your story. In a graph you can show your data and then let people decide for themselves. And in a table, you can more easily summarise your data. Figures, tables, equations, etc, should be numbered and then referenced in the text e.g. “Figure 1 shows…” and then have Figure 1. You should make sure that all aspects of your graph are legible. Always label all of the axes. Your graphs should have titles, and the point that you want to communicate should be clear. If you use a technical term, then it should be briefly explained in plain language for readers who might not be familiar with it. A great example of this is this post by Monica Alexander where she explains the Gini coefficient: To look at the concentration of baby names, let’s calculate the Gini coefficient for each country, sex and year. The Gini coefficient measures dispersion or inequality among values of a frequency distribution. It can take any value between 0 and 1. In the case of income distributions, a Gini coefficient of 1 would mean one person has all the income. In this case, a Gini coefficient of 1 would mean that all babies have the same name. In contrast, a Gini coefficient of 0 would mean names are evenly distributed across all babies. 5.5.3 On brevity Figure 5.9: ‘No more than four pages, or he’s never going to read it. Two pages is preferable.’ Source: Shipman, Tim, 2020, \"The prime minister’s vanishing briefs’, The Sunday Times, 23 February, available at: https://www.thetimes.co.uk/article/the-prime-ministers-vanishing-briefs-67mt0bg95 via Sarah Nickson. Insisting on two page briefs is sensible - not ‘government by ADHD.’ PM has to be across lots of issues - cannot and should not be across (most of) them in the same depth as secretaries of state. Danger lies in PM trying to take on too much and getting bogged down in detail. This might irk officials who lack a sense of where their issue sits within the PM’s list of priorities - or the writing skills to draft a succinct brief. But there’d be very few occasions when a brief to the PM warrants more than two pages. This is not something peculiar to the current PM - other ministers have raised the same in interviews with @instituteforgov Oliver Letwin complained of ‘huge amount of terrible guff, at huge, colossal, humungous length coming from some departments’ https://www.instituteforgovernment.org.uk/ministers-reflect/person/oliver-letwin/ Letwin sent briefs back and asked they be re-drafted to one quarter of the length. ‘Somewhere along the line the Civil Service had got used to splurge of the meaningless kind’ Similarly, Theresa Villiers talked about the civil service’s ‘frustrating tendency to produce six pages of obscure and rather impenetrable text’ and wishes she’d be firmer in sending documents back for re-drafting: https://www.instituteforgovernment.org.uk/ministers-reflect/person/theresa-villiers/ Sarah Nickson, 23 Feb 2020. Brevity is important. Partly this because you are writing for the reader, not yourself, and your reader has other priorities. But it is also because as the writer it focuses you to consider what your most important points are, how you can best support them, and where your arguments are weakest. If you don’t think that examples from government are persuasive, then please consider Amazon’s 2017 Letter to Shareholders, or other statements about Bezos and memo writing, for instance: Well structured, narrative text is what we’re after rather than just text… The reason writing a 4 page memo is harder than “writing” a 20 page powerpoint is because the narrative structure of a good memo forces better thought and better understanding of what’s more important than what, and how things are related. Jeff Bezos, 9 June 2004. 5.5.4 Other Typos and other grammatical mistakes affect the credibility of your claims. If the reader can’t trust you to use a spell-checker, then why should they trust you to use logistic regression? Microsoft Word has a fantastic spell-checker that is much better than what is available for R Markdown: copy/paste your work into there, look for the red lines and fix them in your R Markdown. Then look for the green lines and think about if you need to fix them in your R Markdown. If you don’t have Word, then Google Docs is pretty good and so is Apple’s Pages. A few other general tips that I have stolen from various people including the Reserve Bank of Australia’s style guide: Think about what you are writing. Aim to write everything as though it were on the front page of the newspaper, because one day it could be. Be concise. Remove as many words as possible. Be direct. Think about the structure of your story and identify the key pieces of information and arrange them so that your paper flows logically from one to the next. You should use sub-headings if you need. Be precise. For instance, the stock-market didn’t improve or worsen, it rose or fell. Distinguish levels from rates of change. Be clear. Write simply. Use short sentence where possible. Avoid jargon. You should break these rules when you need to. But the only way to know whether you need to break a rule is to know the rules in the first instance. "],["interactive-communication.html", "Chapter 6 Interactive communication 6.1 Making a website 6.2 Interactive maps 6.3 Shiny", " Chapter 6 Interactive communication Required reading Hill, Alison, 2017, ‘Up &amp; Running with blogdown,’ 12 June, freely available at: https://alison.rbind.io/post/2017-06-12-up-and-running-with-blogdown/. Salmon, Maëlle, 2020, ‘What to know before you adopt Hugo/blogdown,’ 29 February, freely available at: https://masalmon.eu/2020/02/29/hugo-maintenance/. Xie, Yihui, Amber Thomas, and Alison Presmanes Hill, 2020, blogdown: Creating Websites with R Markdown’, as at 6 February, freely available at: https://bookdown.org/yihui/blogdown/. Cooley, David, 2020, ‘mapdeck,’ freely available at: https://symbolixau.github.io/mapdeck/index.html. Kolb, Jan-Philipp, 2019, ‘Using Web Services to Work with Geodata in R,’ The R Journal, 11:2, pages 6-23, freely available at: https://journal.r-project.org/archive/2019/RJ-2019-041/index.html. Gabrielle, 2019, ‘Visualising spatial data using sf and mapdeck - part one,’ 4 December, freely available at: https://resources.symbolix.com.au/2019/12/04/mapdeck-1/. ‘Leaflet for R,’ freely available at: https://rstudio.github.io/leaflet/. Required viewing Kuriwaki, Shiro, 2020, ‘Making maps in R with sf,’ 1 March, freely available at: https://vimeo.com/394800836. Recommended reading Hill, Alison, 2019, ‘A Spoonful of Hugo: Troubleshooting Your Build,’ 4 March, freely available at: https://alison.rbind.io/post/2019-03-04-hugo-troubleshooting/. De Leon, Desirée, 2019, ‘Trying out Blogdown,’ 4 September, freely available at: http://desiree.rbind.io/post/2019/trying-out-blogdown/. Navarro, Danielle, 2018, ‘Day 1: Getting started with blogdown,’ 27 April, freely available at: https://djnavarro.net/post/starting-blogdown/. Key concepts/skills/etc Building a website using blogdown and hugo. Thinking of maps as a (often fiddly, but strangely enjoyable) variant of a usual ggplot. Broadening the data that we make available via interactive maps, while still telling a clear story. Becoming comfortable with (and excited about) creating both static and interactive maps. Key libraries blogdown tidyverse ggmap leaflet maps mapdeck Key functions/etc add_arc() addCircleMarkers() addMarkers() addTiles() canada.cities geom_polygon() get_stamenmap() ggmap() leaflet() map() map_data() mapdeck() mapdeck_style() 6.1 Making a website 6.1.1 Introduction A website is a critical part of communication. If you are searching for a job then it acts as one place to bring everything that you can do together. If you are using R, then you might like a website that makes it easy to share your work. This is where blogdown helps. blogdown is a package that allows you to make websites (not just blogs, notwithstanding its name) largely within R Studio. It builds on Hugo, which is a popular tool for making websites. blogdown lets you freely and quickly get a website up-and-running. It is easy to add content from time-to-time. It integrates with R Markdown which lets you easily share your work. And the separation of content and styling allows you to relatively quickly change your website’s design. That said, using blogdown is more work than Google sites or Squarespace. It requires a little more knowledge than using a basic Wordpress site. And if you want to customise many aspects of your website, or need everything to be ‘just so’ then blogdown may not be for you. blogdown is still under active development and various aspects may break in future releases. That said, the investment of time required to set up a blogdown website is unlikely to be wasted. Even if blogdown were shuttered tomorrow most of the content could be repurposed for a regular Hugo website. This post is a simplified version of the blogdown user-guide and the blog post by Alison Presmanes Hill. It sticks to the basics and doesn’t require much decision-making. The purpose is to allow someone without much experience to use blogdown to get a website up-and-running. Head to those two resources once you’ve got a website working and want to dive a bit deeper. 6.1.2 Foundations We’ll install the blogdown package then use GitHub to create a new folder where we’ll create our website. First install blogdown: install.packages(&quot;blogdown&quot;) Now we want to create a folder in GitHub (because it will be easier to put your website onto the internet if you have a GitHub account). We have seen GitHub earlier in this notes, but if you didn’t do this at that point, then please create a free GitHub account at https://github.com/. Once you have an account, create a new repository by clicking on the plus and call it ‘my_website’ (or whatever you want). Don’t worry about including a readme or gitignore. Once you get to the ‘Quick setup’ page, copy the website address. At this point, we want to get that folder onto our own computer. So open RStudio, select Files, New Project, Version Control, Git, and paste the information for the repo. Go through the rest of it, saving the folder somewhere sensible, and clicking ‘Open in new session.’ This will then create a new folder on your computer which will be a Git folder that is linked to the GitHub repo that you created. We can now construct a frame for our website in that folder. 6.1.3 Build the frame Open R Studio and install Hugo via the blogdown package with the following code: blogdown::install_hugo() In R Studio create a new project in the folder that you just created ‘my_website.’ To do this click on: File -&gt; New Project -&gt; Existing Directory. Then navigate to the folder ‘my_website.’ This will open a new R Studio session. Creating a project just adds a .proj file in the folder that makes it easier to come back to your website later. Using that new R Studio session create your website with the following code: blogdown::new_site(theme = &quot;gcushen/hugo-academic&quot;, theme_example = TRUE) This will: download files into your ‘my_website’ folder; open a R Markdown file that you can close for now; and begin serving the site in your R Studio viewer. The console and viewer of your R Studio session should look like this: Now that we have a frame, we can add our own content. 6.1.4 Add content At this point, the default website is being ‘served’ locally. This means that changes you make will be reflected in the website that you see in your R Studio Viewer. To see the website in a web browser click the ‘show in new window’ button on the top left of the Viewer. This is circled in the above image. That will open the website using the address that the R Studio also tells you. 6.1.4.1 Headshot The first change to make is to update the headshot. In your folder, go to my_website -&gt; static -&gt; img. Replace ‘portrait.jpg’ with your own square headshot jpg. If you do this correctly then when you go back to your website the image will have updated. 6.1.4.2 Personal details, contacts, and main menu To update the biography and other details in that first pane, go to File -&gt; Open File in the R Studio menu and open config.toml which is in my_website -&gt; config.toml. This file will either open in a text editor or in R Studio – it doesn’t matter which. When you save the file the changes will be reflected in the website. Search for ‘title’ or go to line 2. It should say: &#39;title = &quot;Academic&quot;&#39; Change that to: &#39;title = &quot;Your Name&quot;&#39; Search for ‘[params]’ or go to line 21. There you can update parameters such as name, role, and contact details. If you don’t want a particular parameter to show up on your website then set it equal to \"\". (An example of this is on line 33.) Once you’ve updated these parameters, search for ‘[[params.social]]’ or go to line 126. There you can update your contact details, such as email, twitter, etc. Just delete or comment out the full four lines if you don’t want a particular contact type displayed on your website. You can always add more later. Finally, search for ‘[[menu.main]]’ or go to line 152. There you can change the menu items that are displayed on the top right of your website. For instance if you don’t want a blog then delete or comment out the four lines: [[menu.main]] name = &quot;Posts&quot; url = &quot;#posts&quot; weight = 3 If you want to change the order of the items then change the ‘weight.’ Ascending values from left to right. 6.1.4.3 Biography In your folder, go to my_website -&gt; content -&gt; home -&gt; about.md. That should open in R Studio or your text editor. Any changes that you save should immediately show up in your website. Search for ‘# List your academic interests.’ or go to line 12. There you can change your academic interests. If you don’t want this to show up on your website then you can just delete or comment out lines 12-18. Search for ‘# List your qualifications (such as academic degrees).’ or go to line 20. There you can change your academic qualification. If you don’t want this to show up on your website then you can just delete or comment out these lines. The ‘year’ is a numeric field. If you’d prefer to include duration (e.g. 2013 – 2017), then replace the ‘2012’ with ‘“2013 – 2017”’ (the \"\" are important). Or similarly, if you are expecting a degree then you could replace the ‘year’ with ‘“Expected month year”.’ Search for ‘# Biography’ or go to line 43. There you can add a brief biography. 6.1.4.4 Teaching Most of the other files in my_website -&gt; content -&gt; home just display content from elsewhere. This is because of the setup of the website. The exception is teaching.md. Open that and edit everything after line 15. 6.1.4.5 Publications In your folder, go to my_website -&gt; content -&gt; publication. There are two default publications added there. You can edit those and then copy them to add extra publications. 6.1.4.6 Posts If you want a blog in your website then the content is saved in: my_website -&gt; content -&gt; post. If you don’t want a blog then just delete this folder and comment out the posts menu item from my_website -&gt; config.toml file so it doesn’t show up in the menu. Once your website is working, if you want a new blog post, then you can simply use the R Studio menu bar: Tools -&gt; Addins -&gt; New Post. 6.1.4.7 Etc Go through the different parts and change it as you need. 6.1.4.8 Subsequent editing To come back to editing your website once you’ve closed R Studio, go to the ‘my_website’ folder and then double-click on the Rproj file, ‘blogdown_test.Rproj.’ That will open a new instance of R Studio. From there you can type ‘blogdown:::serve_site()’ into the console to serve your site and then continue editing, or you could use the R Studio menu bar: Tools -&gt; Addins -&gt; Serve Site. 6.1.5 Making your website public 6.1.5.1 Commit So far everything has happened on your own computer. The first step to making your website public is to commit these changes to GitHub. To do this open Terminal again and as before use cd and ls to navigate to ‘my_website.’ Once there, type each of the following lines (adding your own description) and follow each by ‘return’ git add -A git commit -m &quot;DESCRIBE THE CHANGE YOU ARE ADDING&quot; git push (You may be asked for your GitHub password. Terminal is a bit tricky to type passwords into because you don’t know how many characters you’ve typed, but have a go and follow it by ‘return.’) 6.1.5.2 Netlify There are many ways to make your website public, but one way is to use Netlify. What we are going to do is to link GitHub and Netlify, so that when you make a change in GitHub then Netlify automatically updates. Once you have an account then click “New site from Git” and at that point you can link your GitHub account. Once it has been authorised, then you should select the repo that you want to make public. The publish director is ‘public.’ Once this is all specified then you can “Deploy site.” Netlify gives you a default address, but you can change this. However it will still have .netlify.com. To get rid of this you need to purchase a domain, and then go through the custom domains setting in Netlify. 6.2 Interactive maps The nice thing about interactive maps is that you can let your users decide what they are interested in. Additionally, if there is a lot of information then you may like to leave it to your users as to selectively focus on what they are interested in. For instance, in the case of Canadian politics, some people will be interested in Toronto ridings, while others will be interested in Manitoba, etc. But it would be difficult to present a map that focuses on both of those, so an interactive map is a great option for allowing users to zoom in on what they want. 6.2.0.1 Leaflet The leaflet package is originally a JavaScript library of the same name that has been brought over to R. It makes it easy to make interactive maps. The basics are fairly similar to the ggmap set-up, but of course after that, there are many, many, options. Let’s redo the bike map from earlier, and possibly the interaction will allow us to see what the issue is with the data. In the same way as a graph in ggplot begins with the ggplot() function, a map in the leaflet package begins with a call to the leaflet() function. This allows you to specify data, and a bunch of other options such as width and height. After this, we add ‘layers,’ in the same way that we added them in ggplot. The first layer that we’ll add is a tile with the function addTiles(). In this case, the default is from OpenStreeMap. After that we’ll add markers that show the location of each bike parking spot with addMarkers(). library(leaflet) library(tidyverse) bike_data &lt;- read_csv(&quot;outputs/data/bikes.csv&quot;) leaflet(data = bike_data) %&gt;% addTiles() %&gt;% # Add default OpenStreetMap map tiles addMarkers(lng = bike_data$longitude, lat = bike_data$latitude, popup = bike_data$street_address, label = ~as.character(bike_data$number_of_spots)) There are two options here that may not be familiar. The first is popup, and this is what happens when you click on the marker. In this example this is giving the address. The second is label, which is what happens when you hover over the marker. In this example it is given the number of spots. 6.2.0.2 COVID-19 Let’s have another go, this time with Ontario data on COVID-19. We can download the latest data from the Ontario Data Catalogue. This is a fast moving situation in which they are likely to make breaking changes to this dataset. To ensure these notes work, I will save and then use the dataset as at 4 April 2020, but you are able to get the up-to-date dataset using the link and the code. ontario_covid &lt;- read_csv(&quot;https://data.ontario.ca/datastore/dump/455fd63b-603d-4608-8216-7d8647f43350?bom=True&quot;) write_csv(ontario_covid, &quot;inputs/data/ontario_covid_2020-04-04.csv&quot;) ontario_covid &lt;- read_csv(&quot;inputs/data/ontario_covid_2020-04-04.csv&quot;) head(ontario_covid) ## # A tibble: 6 x 14 ## `_id` ROW_ID ACCURATE_EPISODE_DATE Age_Group CLIENT_GENDER CASE_ACQUISITIONIN… ## &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 1 2020-03-07 00:00:00 40s MALE Neither ## 2 2 2 2020-03-08 00:00:00 20s MALE Neither ## 3 3 3 2020-03-10 00:00:00 40s FEMALE Neither ## 4 4 4 2020-03-11 00:00:00 50s FEMALE Neither ## 5 5 5 2020-03-12 00:00:00 30s FEMALE Neither ## 6 6 6 2020-03-15 00:00:00 50s MALE Neither ## # … with 8 more variables: OUTCOME1 &lt;chr&gt;, Reporting_PHU &lt;chr&gt;, ## # Reporting_PHU_Address &lt;chr&gt;, Reporting_PHU_City &lt;chr&gt;, ## # Reporting_PHU_Postal_Code &lt;chr&gt;, Reporting_PHU_Website &lt;chr&gt;, ## # Reporting_PHU_Latitude &lt;dbl&gt;, Reporting_PHU_Longitude &lt;dbl&gt; There is a lot of information here, but we’ll just plot the number of cases, by the reporting area (health areas). So this isn’t the location of the person, but the location of the responsible health unit. Because of this, we’ll add a little bit of noise so that the marker for each person can be seen. We do this with jitter(). ontario_covid &lt;- ontario_covid %&gt;% mutate(Reporting_PHU_Latitude = jitter(Reporting_PHU_Latitude, amount = 0.1), Reporting_PHU_Longitude = jitter(Reporting_PHU_Longitude, amount = 0.1)) We will introduce a different type of marker here, which is circles. This will allow us to use different colours for the outcomes of each case. There are three possible outcomes: the case is resolved, it is not resolved, or it was fatal. library(leaflet) pal &lt;- colorFactor(&quot;Dark2&quot;, domain = ontario_covid$OUTCOME1 %&gt;% unique()) leaflet() %&gt;% addTiles() %&gt;% # Add default OpenStreetMap map tiles addCircleMarkers( data = ontario_covid, lng = ontario_covid$Reporting_PHU_Longitude, lat = ontario_covid$Reporting_PHU_Latitude, color = pal(ontario_covid$OUTCOME1), popup = paste(&quot;&lt;b&gt;Age-group:&lt;/b&gt;&quot;, as.character(ontario_covid$Age_Group), &quot;&lt;br&gt;&quot;, &quot;&lt;b&gt;Gender:&lt;/b&gt;&quot;, as.character(ontario_covid$CLIENT_GENDER), &quot;&lt;br&gt;&quot;, &quot;&lt;b&gt;Acquisition:&lt;/b&gt;&quot;, as.character(ontario_covid$CASE_ACQUISITIONINFO), &quot;&lt;br&gt;&quot;, &quot;&lt;b&gt;Episode date:&lt;/b&gt;&quot;, as.character(ontario_covid$ACCURATE_EPISODE_DATE), &quot;&lt;br&gt;&quot;) ) %&gt;% addLegend(&quot;bottomright&quot;, pal = pal, values = ontario_covid$OUTCOME1 %&gt;% unique(), title = &quot;Case outcome&quot;, opacity = 1 ) 6.2.0.3 mapdeck Thank you to Shaun Ratcliff for introducing me to mapdeck. The package mapdeck is an R package that is built on top of Mapbox (https://www.mapbox.com). It is based on WebGL, which means that your web browser does a lot of work for you. The nice thing is that because of this, it can do a bunch of things that leaflet struggles with, especially dealing with larger datasets. Mapbox is a full-featured application that many businesses that you may have heard of use: https://www.mapbox.com/showcase. To close out these notes on mapping, I want to briefly touch on mapdeck, as it is a newer, but very exciting, package. To this point we have used stamen maps as our tile, but mapdeck uses mapbox - https://www.mapbox.com/ - and so you need to register and get a token for this. (It’s free.) Once you have that token you add it to R using: library(mapdeck) set_token(&quot;asdf&quot;) # replace asdf with your token. mapdeck_tokens() set_token(test$key) (Don’t add it into your script otherwise everyone will be able to take it and use it, especially once you add it to GitHub.) Then we need some data. Here we’re going to just use the example dataset, which is about flights. # Code taken from the example: https://github.com/SymbolixAU/mapdeck library(mapdeck) url &lt;- &#39;https://raw.githubusercontent.com/plotly/datasets/master/2011_february_aa_flight_paths.csv&#39; flights &lt;- read.csv(url) flights$info &lt;- paste0(&quot;&lt;b&gt;&quot;,flights$airport1, &quot; - &quot;, flights$airport2, &quot;&lt;/b&gt;&quot;) head(flights) ## start_lat start_lon end_lat end_lon airline airport1 airport2 cnt ## 1 32.89595 -97.03720 35.04022 -106.60919 AA DFW ABQ 444 ## 2 41.97960 -87.90446 30.19453 -97.66987 AA ORD AUS 166 ## 3 32.89595 -97.03720 41.93887 -72.68323 AA DFW BDL 162 ## 4 18.43942 -66.00183 41.93887 -72.68323 AA SJU BDL 56 ## 5 32.89595 -97.03720 33.56294 -86.75355 AA DFW BHM 168 ## 6 25.79325 -80.29056 36.12448 -86.67818 AA MIA BNA 56 ## info ## 1 &lt;b&gt;DFW - ABQ&lt;/b&gt; ## 2 &lt;b&gt;ORD - AUS&lt;/b&gt; ## 3 &lt;b&gt;DFW - BDL&lt;/b&gt; ## 4 &lt;b&gt;SJU - BDL&lt;/b&gt; ## 5 &lt;b&gt;DFW - BHM&lt;/b&gt; ## 6 &lt;b&gt;MIA - BNA&lt;/b&gt; Finally, we can call the map. Again, this is just the example in the package’s website. mapdeck(style = mapdeck_style(&#39;dark&#39;) ) %&gt;% add_arc( data = flights , origin = c(&quot;start_lon&quot;, &quot;start_lat&quot;) , destination = c(&quot;end_lon&quot;, &quot;end_lat&quot;) , stroke_from = &quot;airport1&quot; , stroke_to = &quot;airport2&quot; , tooltip = &quot;info&quot; , layer_id = &#39;arclayer&#39; ) And this is pretty nice! 6.3 Shiny Shiny is a way of making interactive web applications (not just maps) using R. It’s fun, but fiddly. Here we’re going to step through one way to take advantage of Shiny, and that’s to quickly build a survey. "],["gather-data.html", "Chapter 7 Gather data 7.1 APIs 7.2 Case study - arXiv 7.3 Case study - rtweet 7.4 Case study - spotifyr 7.5 Scraping 7.6 Case study - Rohan’s books 7.7 Case study - Canadian Prime Ministers 7.8 PDFs 7.9 Case-study: US Total Fertility Rate, by state and year (2000-2018) 7.10 Case-study: Kenyan census data 7.11 Optical Character Recognition 7.12 Text", " Chapter 7 Gather data Last updated: 2 February 2021. Recommended reading Benoit, Kenneth, 2019, ‘Text as data: An overview,’ 17 July, https://kenbenoit.net/pdfs/28%20Benoit%20Text%20as%20Data%20draft%202.pdf. Bolton, Liza, 2019, ‘A quick look at museums per capita,’ 26 March, http://blog.dataembassy.co.nz/museums-per-capita/. Bryan, Jennifer, and Jim Hester, 2020, What They Forgot to Teach You About R, Chapter 7, https://rstats.wtf/index.html. Clavelle, Tyler, 2017, ‘Using R to extract data from web APIs,’ 5 June, https://www.tylerclavelle.com/code/2017/randapis/. Cooksey, Brian, 2014, ‘An Introduction to APIs,’ Zapier, 22 April, https://zapier.com/learn/apis/. Dogucu, Mine, and Mine Çetinkaya-Runde, 2020,‘Web Scraping in the Statistics and Data Science Curriculum: Challenges and Opportunities,’ 6 May. Gelfand, Sharla, 2019, ‘Crying @ Sephora,’ 8 November, https://sharla.party/post/crying-sephora/. Goldman, Shayna, 2019, ‘How Much Do NHL Players Really Make? Part 2: Taxes,’ https://hockey-graphs.com/2019/01/08/how-much-do-nhl-players-really-make-part-2-taxes/. Graham, Shawn, 2019, ‘Scraping with rvest,’ 7 November, https://electricarchaeology.ca/2019/11/07/scraping-with-rvest/. Henze, Martin, 2020, ‘Web Scraping with rvest + Astro Throwback,’ 23 January, https://heads0rtai1s.github.io/2020/01/23/rvest-intro-astro/. Hudon, Caitlin, 2017, ‘’Blue Christmas: A data-driven search for the most depressing Christmas song,’ 22 December, https://caitlinhudon.com/2017/12/22/blue-christmas/. Luscombe, Alex, 2020, ‘A Gentle Introduction to Tesseract OCR,’ 3 June, https://alexluscombe.ca/post/ocr-tutorial/. Luscombe, Alex, 2020, ‘Getting your .pdfs into R,’ 5 August, https://alexluscombe.ca/post/r-pdftools/. Luscombe, Alex, 2020, ‘Parsing your .pdfs in R,’ 10 August, https://alexluscombe.ca/post/parsing-pdfs/. Marshall, James, ‘HTML Made Really Easy,’ https://www.jmarshall.com/easy/html/. Marshall, James, ‘HTTP Made Really Easy,’ https://www.jmarshall.com/easy/http/. Nakagawara, Ryo, 2020, ‘Intro to {polite} Web Scraping of Soccer Data with R!’ 14 May, https://ryo-n7.github.io/2020-05-14-webscrape-soccer-data-with-R/. Pavlik, Kaylin, 2020, ‘How do fiber types appear together in yarn blends?’ 17 February, https://www.kaylinpavlik.com/ravelry-yarn-fibers/. Silge, Julia and David Robinson, 2020, Text Mining with R, Chapters 1, 3, and 6, https://www.tidytextmining.com/. Silge, Julia, 2017, ‘Scraping CRAN with rvest,’ 5 March, https://juliasilge.com/blog/scraping-cran/. Smale, David, 2020, ‘Daniel Johnston,’ https://davidsmale.netlify.com/portfolio/daniel-johnston/. Taddy, Matt, 2019, Business Data Science, Chapter 8, pp. 231-259. Wickham, Hadley, ‘Managing Secrets,’ https://cran.r-project.org/web/packages/httr/vignettes/secrets.html. Wickham, Hadley, 2014, ‘rvest: easy web scraping with R,’ 24 November, https://blog.rstudio.com/2014/11/24/rvest-easy-web-scraping-with-r/. Wickham, Hadley, nd, ‘Getting started with httr,’ https://cran.r-project.org/web/packages/httr/vignettes/quickstart.html. Recommended viewing D’Agostino McGowan, Lucy, 2020 ‘Harnessing the Power of the Web via R Clients for Web APIs,’ talk at ASA Joint Statistical Meeting 2018, https://www.lucymcgowan.com/talk/asa_joint_statistical_meeting_2018/. Tatman, Rachel, 2018, ‘Character Encoding and You,’ 21 February, https://youtu.be/2U9EHYqc59Y. Key concepts/skills/etc Use APIs where possible because the data provider has specified the data they would like to make available to you, and the conditions under which they are making it available. Often R packages have been written to make it easier to use APIs. Use R environments to manage your keys. Using the verb GET (‘a GET request’) means providing a URL and the server will return something, using the verb POST (a POST request’) means providing some data and the server will deal with that data. Cleaning data Graphing data to tell a story Respectfully scraping data Approaching extracting text from PDFs as a workflow. Planning what is needed at the start. Starting small and then iterating. Putting in place checks. Gathering text data. Preparing text datasets. Key libraries babynames broom dplyr ggplot2 gutenbergr janitor jsonlite pdftools purrr rtweet rvest spotifyr stringi tidymodels tidytext tidyverse usethis Key functions/etc as_factor() as_tibble() bind_tf_idf() c() case_when() cat() edit_r_environ() file() fromJSON() function() GET() get_artist_audio_features() get_favorites() get_my_top_artists_or_tracks() html_node() html_nodes() html_text() pdf_data() pdf_text() pmap_dfr() read_html() readRDS() safely() search_tweets() sleep() tesseract() unnest_tokens() walk2() write_html() write_lines() Quiz In your own words, what is an API (write a paragraph or two)? Find two APIs and discuss how you could use them to tell interesting stories (write a paragraph or two for each). Find two APIs that have an R packages written around them. How could you use these to tell interesting stories? (Write a paragraph or two for each.) What is the main argument to httr::GET() (pick one)? ‘url’ ‘website’ ‘domain’ ‘location’ Name three reasons why we should be respectful when getting scraping data from websites (write a paragraph or two). What features of a website do we typically take advantage of when we parse the code (select all)? HTML/CSS mark-up. Cookies. Facebook beacons. Code comments. What are three advantages and three disadvantages of scraping compared with using an API (write a paragraph or two)? What are three delimiters that could be useful when trying to bring order to the PDF that you read in as a character vector (write a paragraph or two)? What do I need to put inside “SOMETHING_HERE” if I want to match regular expressions for a full stop i.e. “.” (hint: see the ‘strings’ cheat sheet) (pick one)? . \\. \\\\. \\\\\\. Name three reasons for sketching out what you want before starting to try to extract data from a PDF (write a paragraph or two for each). If you are interested in demographic data then what are three checks that you might like to do? What are three if you are interested in economic data such as GDP, interest rates, and exchange rates? (Write an explanation for each.) What does the purrr package do (select all)? Enhances R’s functional programming toolkit. Makes loops easier to code and read. Checks the consistency of datasets. Identifies issues in data structures and proposes replacements. Which of these are functions from the purrr package (select all)? map() walk() run() safely() Why should we use safely() when scraping data (pick one)? To protect us from hackers. To avoid side effects of pages with issues. To slow down our scraping to an appropriate speed. What are some principles to follow when scraping (select all)? Avoid it if possible Follow the site’s guidance Slow down Use a scalpel not an axe. What is a robots.txt file (pick one)? The instructions that Frankenstein followed. Notes that web scrapers should follow when scraping. What is the html tag for an item in list (pick one)? li body b em If I have the following text data ‘rohan_alexander’ in a column called ‘names’ and want to split it into first name and surname based on the underbar what function should I use (pick one)? separate() slice() spacing() text_to_columns() Gather some data yourself using a method that is introduced here - APIs directly or via a wrapper package, web scraping, PDF parsing, OCR, or text. Write a few paragraphs about the data source, what you gathered, and how you went about it. What took longer than you expected? When did it become fun? What would you do differently next time you do this? Please include a link to your GitHub repo so I can see the code, but it won’t be strictly marked - this is more about encouraging you to have a go. (Start with something tiny and very specific, get that working, and then increase the scope - almost everything will be more difficult and time-consuming than you think - and don’t forget to plan it out before you start.) 7.1 APIs In everyday language, and for our purposes, an Application Programming Interface (API) is simply a situation in which someone has set up specific files on their computer such that you can follow their instructions to get them. For instance, when you use a gif on Slack, Slack asks Giphy’s server for the appropriate gif, Giphy’s server gives that gif to Slack and then Slack inserts it into your chat. The way in which Slack and Giphy interact is determined by Giphy’s API. More strictly, an API is just an application that runs on a server that we access using the HTTP protocol. In our case, we are going to focus on using APIs for gathering data. I’ll tailor the language that I use toward that: [a]n API is the tool that makes a website’s data digestible for a computer. Through it, a computer can view and edit data, just like a person can by loading pages and submitting forms. Cooksey (2014), Chapter 1. For instance, you could go to Google Maps and then scroll and click and drag to center the map on Canberra, Australia, or you could just paste this into your browser: https://www.google.ca/maps/@-35.2812958,149.1248113,16z. You just used the Google Maps API.4 The result should be a map that looks something like Figure 7.1 . Figure 7.1: Example of Google Maps, as at 25 January 2021. The advantage of using an API is that the data provider specifies exactly the data that they are willing to provide, and the terms under which they will provide it. These terms may include things like rate limits (i.e. how often you can ask for data), and what you can do with the data (e.g. maybe you’re not allowed to use it for commercial purposes, or to republish it, or whatever). Additionally, because the API is being provided specifically for you to use it, it is less likely to be subject to unexpected changes. Because of this it is ethically and legally clear that when an API is available you should try to use it. We’re going to run through some case studies interacting with APIs in R. In the first we will deal directly with an API. That works and is a handy skill to have, but there are a lot of R packages that wrap around APIs making it easier for you to use an API within ‘familiar surroundings.’ I’ll also run through two fun APIs that have R packages built around them. 7.2 Case study - arXiv In this section we introduce GET requests in which we use an API directly. We will use the httr package (Wickham 2019a). A GET request tries to obtain some specific data and the main argument is url. Exactly as before with the Google Maps example! In that case, the specific information was a map and some information about it. For this example we’ll look at arXiv, which is a repository for academic articles before they go through peer-review. I’ll ask arXiv to return some information about a paper that I recently uploaded with a former student. The content that is returned will be a series of information about that paper. # install.packages(&#39;httr&#39;) library(httr) arxiv &lt;- httr::GET(&#39;http://export.arxiv.org/api/query?id_list=2101.05225&#39;) class(arxiv) ## [1] &quot;response&quot; content(arxiv, &quot;text&quot;) %&gt;% cat(&quot;\\n&quot;) ## &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; ## &lt;feed xmlns=&quot;http://www.w3.org/2005/Atom&quot;&gt; ## &lt;link href=&quot;http://arxiv.org/api/query?search_query%3D%26id_list%3D2101.05225%26start%3D0%26max_results%3D10&quot; rel=&quot;self&quot; type=&quot;application/atom+xml&quot;/&gt; ## &lt;title type=&quot;html&quot;&gt;ArXiv Query: search_query=&amp;amp;id_list=2101.05225&amp;amp;start=0&amp;amp;max_results=10&lt;/title&gt; ## &lt;id&gt;http://arxiv.org/api/p9UZyl2Vt0cHwPSKinDSThE23qI&lt;/id&gt; ## &lt;updated&gt;2021-04-28T00:00:00-04:00&lt;/updated&gt; ## &lt;opensearch:totalResults xmlns:opensearch=&quot;http://a9.com/-/spec/opensearch/1.1/&quot;&gt;1&lt;/opensearch:totalResults&gt; ## &lt;opensearch:startIndex xmlns:opensearch=&quot;http://a9.com/-/spec/opensearch/1.1/&quot;&gt;0&lt;/opensearch:startIndex&gt; ## &lt;opensearch:itemsPerPage xmlns:opensearch=&quot;http://a9.com/-/spec/opensearch/1.1/&quot;&gt;10&lt;/opensearch:itemsPerPage&gt; ## &lt;entry&gt; ## &lt;id&gt;http://arxiv.org/abs/2101.05225v1&lt;/id&gt; ## &lt;updated&gt;2021-01-13T17:37:07Z&lt;/updated&gt; ## &lt;published&gt;2021-01-13T17:37:07Z&lt;/published&gt; ## &lt;title&gt;On consistency scores in text data with an implementation in R&lt;/title&gt; ## &lt;summary&gt; In this paper, we introduce a reproducible cleaning process for the text ## extracted from PDFs using n-gram models. Our approach compares the originally ## extracted text with the text generated from, or expected by, these models using ## earlier text as stimulus. To guide this process, we introduce the notion of a ## consistency score, which refers to the proportion of text that is expected by ## the model. This is used to monitor changes during the cleaning process, and ## across different corpuses. We illustrate our process on text from the book Jane ## Eyre and introduce both a Shiny application and an R package to make our ## process easier for others to adopt. ## &lt;/summary&gt; ## &lt;author&gt; ## &lt;name&gt;Ke-Li Chiu&lt;/name&gt; ## &lt;/author&gt; ## &lt;author&gt; ## &lt;name&gt;Rohan Alexander&lt;/name&gt; ## &lt;/author&gt; ## &lt;arxiv:comment xmlns:arxiv=&quot;http://arxiv.org/schemas/atom&quot;&gt;13 pages, 0 figures&lt;/arxiv:comment&gt; ## &lt;link href=&quot;http://arxiv.org/abs/2101.05225v1&quot; rel=&quot;alternate&quot; type=&quot;text/html&quot;/&gt; ## &lt;link title=&quot;pdf&quot; href=&quot;http://arxiv.org/pdf/2101.05225v1&quot; rel=&quot;related&quot; type=&quot;application/pdf&quot;/&gt; ## &lt;arxiv:primary_category xmlns:arxiv=&quot;http://arxiv.org/schemas/atom&quot; term=&quot;cs.CL&quot; scheme=&quot;http://arxiv.org/schemas/atom&quot;/&gt; ## &lt;category term=&quot;cs.CL&quot; scheme=&quot;http://arxiv.org/schemas/atom&quot;/&gt; ## &lt;/entry&gt; ## &lt;/feed&gt; ## We get a variety of information about this paper including the title, abstract, and authors. 7.3 Case study - rtweet Twitter is a rich source of text and other data. The Twitter API is the way in which Twitter ask that you interact with Twitter in order to gather these data. The rtweet package (Kearney 2019) is built around this API and allows us to interact with it in ways that are similar to using any other R package. Initially all you need a regular Twitter account. Get started by install the library if you need and then calling it. # install.packages(&#39;rtweet&#39;) library(rtweet) library(tidyverse) To get started we need to authorise rtweet. We start that process by calling a function from the package. get_favorites(user = &quot;RohanAlexander&quot;) This will open a browser on your computer, and you will then have to log into your regular Twitter account as shown in Figure 7.2. Figure 7.2: rtweet authorisation page Once that is done we can actually get my favourites and then save them. rohans_favs &lt;- get_favorites(&quot;RohanAlexander&quot;) saveRDS(rohans_favs, &quot;dont_push/rohans_favs.rds&quot;) And then looking at the most recent favourite, we can see it was when Professor Bolton tweeted about one of the stellar students in ISSC. rohans_favs %&gt;% arrange(desc(created_at)) %&gt;% slice(1) %&gt;% select(screen_name, text) ## # A tibble: 1 x 2 ## screen_name text ## &lt;chr&gt; &lt;chr&gt; ## 1 les_ja I&#39;ve signed an offer letter, so I think I can formally announce: … Let’s look at who is tweeting about R, using one of the common R hashtags: #rstats. I’ve removed retweets so that we hopefully get some actual interesting projects. rstats_tweets &lt;- search_tweets( q = &quot;#rstats&quot;, include_rts = FALSE ) saveRDS(rstats_tweets, &quot;dont_push/rstats_tweets.rds&quot;) And then have a look at them. names(rstats_tweets) ## [1] &quot;user_id&quot; &quot;status_id&quot; ## [3] &quot;created_at&quot; &quot;screen_name&quot; ## [5] &quot;text&quot; &quot;source&quot; ## [7] &quot;display_text_width&quot; &quot;reply_to_status_id&quot; ## [9] &quot;reply_to_user_id&quot; &quot;reply_to_screen_name&quot; ## [11] &quot;is_quote&quot; &quot;is_retweet&quot; ## [13] &quot;favorite_count&quot; &quot;retweet_count&quot; ## [15] &quot;quote_count&quot; &quot;reply_count&quot; ## [17] &quot;hashtags&quot; &quot;symbols&quot; ## [19] &quot;urls_url&quot; &quot;urls_t.co&quot; ## [21] &quot;urls_expanded_url&quot; &quot;media_url&quot; ## [23] &quot;media_t.co&quot; &quot;media_expanded_url&quot; ## [25] &quot;media_type&quot; &quot;ext_media_url&quot; ## [27] &quot;ext_media_t.co&quot; &quot;ext_media_expanded_url&quot; ## [29] &quot;ext_media_type&quot; &quot;mentions_user_id&quot; ## [31] &quot;mentions_screen_name&quot; &quot;lang&quot; ## [33] &quot;quoted_status_id&quot; &quot;quoted_text&quot; ## [35] &quot;quoted_created_at&quot; &quot;quoted_source&quot; ## [37] &quot;quoted_favorite_count&quot; &quot;quoted_retweet_count&quot; ## [39] &quot;quoted_user_id&quot; &quot;quoted_screen_name&quot; ## [41] &quot;quoted_name&quot; &quot;quoted_followers_count&quot; ## [43] &quot;quoted_friends_count&quot; &quot;quoted_statuses_count&quot; ## [45] &quot;quoted_location&quot; &quot;quoted_description&quot; ## [47] &quot;quoted_verified&quot; &quot;retweet_status_id&quot; ## [49] &quot;retweet_text&quot; &quot;retweet_created_at&quot; ## [51] &quot;retweet_source&quot; &quot;retweet_favorite_count&quot; ## [53] &quot;retweet_retweet_count&quot; &quot;retweet_user_id&quot; ## [55] &quot;retweet_screen_name&quot; &quot;retweet_name&quot; ## [57] &quot;retweet_followers_count&quot; &quot;retweet_friends_count&quot; ## [59] &quot;retweet_statuses_count&quot; &quot;retweet_location&quot; ## [61] &quot;retweet_description&quot; &quot;retweet_verified&quot; ## [63] &quot;place_url&quot; &quot;place_name&quot; ## [65] &quot;place_full_name&quot; &quot;place_type&quot; ## [67] &quot;country&quot; &quot;country_code&quot; ## [69] &quot;geo_coords&quot; &quot;coords_coords&quot; ## [71] &quot;bbox_coords&quot; &quot;status_url&quot; ## [73] &quot;name&quot; &quot;location&quot; ## [75] &quot;description&quot; &quot;url&quot; ## [77] &quot;protected&quot; &quot;followers_count&quot; ## [79] &quot;friends_count&quot; &quot;listed_count&quot; ## [81] &quot;statuses_count&quot; &quot;favourites_count&quot; ## [83] &quot;account_created_at&quot; &quot;verified&quot; ## [85] &quot;profile_url&quot; &quot;profile_expanded_url&quot; ## [87] &quot;account_lang&quot; &quot;profile_banner_url&quot; ## [89] &quot;profile_background_url&quot; &quot;profile_image_url&quot; rstats_tweets %&gt;% select(screen_name, text) %&gt;% head() ## # A tibble: 6 x 2 ## screen_name text ## &lt;chr&gt; &lt;chr&gt; ## 1 RahaPhD &quot;#WFH multitasking woes: I was just sitting here, working on … ## 2 AmandaKMontoya &quot;Teaching with the #PublishingPaidMe data this week in my intr… ## 3 digitalke1 &quot;130 #MachineLearning ProjectsSolved and Explained\\n@ruben_arc… ## 4 digitalke1 &quot;#Infographic: 6 simple steps to effectively analyse data.\\nVi… ## 5 dataclaudius &quot;When Did the US Senate Best Reflect the US Population? via #r… ## 6 alexpghayes &quot;has anyone written an #rstats package to interface with SNAP … There is a bunch of other things that you can do just using a regular user account, and if you’re interested then you should try the examples in the rtweet package documentation: https://rtweet.info/index.html. But more is available once you register as a developer (https://developer.twitter.com/en/apply-for-access). The Twitter API document is surprisingly readable and you may enjoy some of it: https://developer.twitter.com/en/docs. When I introduced APIs I said that the ‘data provider specifies exactly the data that they are willing to provide…’ and we have certainly been able to take advantage of what they provide But I continued ‘…and the terms under which they will provide it’ and here we haven’t done our part. In particular, I took some tweets and saved them. If I had pushed these to GitHub then it’s possible I may have accidently stored sensitive information if there happened to be some in the tweets. Or if I had taken enough tweets to start to do some reasonable statistical analysis then even if there wasn’t sensitive information I may have violated the terms if I had pushed those saved tweets to GitHub. Finally, I linked a Twitter user name, in this case @Liza_Bolton with Professor Bolton. I happened to ask her if this was okay, but if I hadn’t done that then I would have been violating the Twitter terms of service. If you use Twitter data, please take a moment to look at the terms: https://developer.twitter.com/en/developer-terms/more-on-restricted-use-cases. 7.4 Case study - spotifyr For the next example I will introduce the spotifyr package (Thompson et al. 2020). Again, this is a wrapper that has been developed around an API, in this case the Spotify API. You should install the package from the developer’s GitHub repo using devtools (Wickham, Hester, and Chang 2020). # devtools::install_github(&#39;charlie86/spotifyr&#39;) library(spotifyr) In order to use this account you need a Spotify Developer Account, which you can set-up here: https://developer.spotify.com/dashboard/. That’ll have you log in with your Spotify details and then accept their terms (it’s worth looking at some of these and I’ll follow up on a few below) as in Figure 7.3. Figure 7.3: rtweet authorisation page What we need from here is a ‘Client ID’ and you can just fill out some basic details. In our case we probably ‘don’t know’ what we’re building, which means that Spotify requires us to use a non-commercial agreement, which is fine. In order to use the Spotify API we need a Client ID and a Client Secret. These are things that you want to keep to yourself. There are a variety of ways of keeping this secret, (and my understanding is that a helpful package is on its way) but we’ll keep them in our System Environment. In this way, when we push to GitHub they won’t be included. To do this we need to be careful about the naming, because spotifyr will look in our environment for specifically named keys. To do this we are going to use the usethis package Wickham and Bryan (2020). If you don’t have that then please install it. There is a file called ‘.Renviron’ which we will open and add our secrets to. This file also controls things like your default library location and more information is available at Lopp (2017) and Jennifer Bryan and Hester (2020). usethis::edit_r_environ() When you run that function it will open a file. There you can add your Spotify secrets. SPOTIFY_CLIENT_ID = &#39;PUT_YOUR_CLIENT_ID_HERE&#39; SPOTIFY_CLIENT_SECRET = &#39;PUT_YOUR_SECRET_HERE&#39; Save your ‘.Renviron’ file, and then restart R (Session -&gt; Restart R). You can now draw on that variable when you need. Some functions that require your secrets as arguments will now just work. For instance, we will get information about Radiohead using get_artist_audio_features(). One of the arguments is authorization, but as that is set to default to look at the R Environment, we don’t need to do anything further. radiohead &lt;- get_artist_audio_features(&#39;radiohead&#39;) saveRDS(radiohead, &quot;inputs/radiohead.rds&quot;) radiohead &lt;- readRDS(&quot;inputs/radiohead.rds&quot;) names(radiohead) ## [1] &quot;artist_name&quot; &quot;artist_id&quot; ## [3] &quot;album_id&quot; &quot;album_type&quot; ## [5] &quot;album_images&quot; &quot;album_release_date&quot; ## [7] &quot;album_release_year&quot; &quot;album_release_date_precision&quot; ## [9] &quot;danceability&quot; &quot;energy&quot; ## [11] &quot;key&quot; &quot;loudness&quot; ## [13] &quot;mode&quot; &quot;speechiness&quot; ## [15] &quot;acousticness&quot; &quot;instrumentalness&quot; ## [17] &quot;liveness&quot; &quot;valence&quot; ## [19] &quot;tempo&quot; &quot;track_id&quot; ## [21] &quot;analysis_url&quot; &quot;time_signature&quot; ## [23] &quot;artists&quot; &quot;available_markets&quot; ## [25] &quot;disc_number&quot; &quot;duration_ms&quot; ## [27] &quot;explicit&quot; &quot;track_href&quot; ## [29] &quot;is_local&quot; &quot;track_name&quot; ## [31] &quot;track_preview_url&quot; &quot;track_number&quot; ## [33] &quot;type&quot; &quot;track_uri&quot; ## [35] &quot;external_urls.spotify&quot; &quot;album_name&quot; ## [37] &quot;key_name&quot; &quot;mode_name&quot; ## [39] &quot;key_mode&quot; radiohead %&gt;% select(artist_name, track_name, album_name) %&gt;% head() ## artist_name track_name ## 1 Radiohead Airbag - Remastered ## 2 Radiohead Paranoid Android - Remastered ## 3 Radiohead Subterranean Homesick Alien - Remastered ## 4 Radiohead Exit Music (For a Film) - Remastered ## 5 Radiohead Let Down - Remastered ## 6 Radiohead Karma Police - Remastered ## album_name ## 1 OK Computer OKNOTOK 1997 2017 ## 2 OK Computer OKNOTOK 1997 2017 ## 3 OK Computer OKNOTOK 1997 2017 ## 4 OK Computer OKNOTOK 1997 2017 ## 5 OK Computer OKNOTOK 1997 2017 ## 6 OK Computer OKNOTOK 1997 2017 Let’s just make a quick graph looking at track length over time. radiohead %&gt;% ggplot(aes(x = album_release_year, y = duration_ms)) + geom_point() Just because we can, let’s settle an argument. I’ve always said that Radiohead of quite depressing, but they’re my wife’s favourite band. Let’s see how depressing they are. Spotify provides various information about each track, including ‘valence,’ which Spotify define as ‘(a) measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).’ Higher values are happier. Let’s compare someone who we know it likely to be happy - Taylor Swift - with Radiohead. swifty &lt;- get_artist_audio_features(&#39;taylor swift&#39;) saveRDS(swifty, &quot;inputs/swifty.rds&quot;) swifty &lt;- readRDS(&quot;inputs/swifty.rds&quot;) tibble(name = c(swifty$artist_name, radiohead$artist_name), year = c(swifty$album_release_year, radiohead$album_release_year), valence = c(swifty$valence, radiohead$valence) ) %&gt;% ggplot(aes(x = year, y = valence, color = name)) + geom_point() + theme_minimal() + labs(x = &quot;Year&quot;, y = &quot;Valence&quot;, color = &quot;Name&quot;) + scale_color_brewer(palette = &quot;Set1&quot;) Finally, for the sake of embarrassment, let’s look at our most played artists. top_artists &lt;- get_my_top_artists_or_tracks(type = &#39;artists&#39;, time_range = &#39;long_term&#39;, limit = 20) saveRDS(top_artists, &quot;inputs/top_artists.rds&quot;) top_artists &lt;- readRDS(&quot;inputs/top_artists.rds&quot;) top_artists %&gt;% select(name, popularity) ## name popularity ## 1 Radiohead 81 ## 2 Bombay Bicycle Club 66 ## 3 Drake 100 ## 4 Glass Animals 74 ## 5 JAY-Z 85 ## 6 Laura Marling 65 ## 7 Sufjan Stevens 75 ## 8 Vampire Weekend 73 ## 9 Sturgill Simpson 65 ## 10 Nick Drake 66 ## 11 Dire Straits 78 ## 12 Lorde 80 ## 13 Marian Hill 65 ## 14 José González 68 ## 15 Stevie Wonder 79 ## 16 Disclosure 82 ## 17 Ben Folds Five 52 ## 18 Ainslie Wills 40 ## 19 Coldplay 89 ## 20 alt-J 75 So pretty much my wife and I like what everyone else likes, with the exception of Ainslie Wills, who is an Australian and I suspect we used to listen to her when we were homesick. How amazing that we live in a world that all that information is available with very little effort or cost. Again, there is a lot more at the package’s website: https://www.rcharlie.com/spotifyr/. A very nice little application of the Spotify API using some statistical analysis is Pavlik (2019). 7.5 Scraping 7.5.1 Introduction Web-scraping is a way to get data from websites into R. Rather than going to a website ourselves through a browser, we write code that does it for us. This opens up a lot of data to us, but on the other hand, it is not typically data that is being made available for these purposes and so it is important to be respectful of it. While generally not illegal, the specifics with regard to the legality of web-scraping depends on jurisdictions and the specifics of what you’re doing, and so it is also important to be mindful of this. And finally, web-scraping imposes a cost on the website host, and so it is important to reduce this to the extent that it’s possible. That all said, web-scraping is an invaluable source of data. But they are typically datasets that can be created as a by-product of someone trying to achieve another aim. For instance, a retailer may have a website with their products and their prices. That has not been created deliberately as a source of data, but we can scrape it to create a dataset. As such, the following principles guide my web-scraping. Avoid it. Try to use an API wherever possible. Abide by their desires. Some websites have a file ‘robots.txt’ that contains information about what they are comfortable with scrapers doing, for instance ‘https://www.google.com/robots.txt.’ If they have one of these then you should read it and abide by it. Reduce the impact. Firstly, slow down your scraper, for instance, rather than having it visit the website every second, slow it down (using sys.sleep()). If you’re only after a few hundred files then why not just have it visit once a minute, running in the background overnight? Secondly, consider the timing of when you run the scraper. For instance, if it’s a retailer then why not set your script to run from 10pm through to the morning, when fewer customers are likely to need the site? If it’s a government website and they have a big monthly release then why not avoid that day? Take only what you need. For instance, don’t scrape the entire of Wikipedia if all you need is to know the names of the 10 largest cities in Canada. This reduces the impact on their website and allows you to more easily justify what you are doing. Only scrape once. Save everything as you go so that you don’t have to re-collect data. Similarly, once you have the data, you should keep that separate and not modify it. Of course, if you need data over time then you will need to go back, but this is different to needlessly re-scraping a page. Don’t republish the pages that you scraped. (This is in contrast to datasets that you create from it.) Take ownership and ask permission if possible. At a minimum level your scripts should have your contact details in them. Depending on the circumstances, it may be worthwhile asking for permission before you scrape. 7.5.2 Getting started Web-scraping is possible by taking advantage of the underlying structure of a webpage. We use patterns in the HTML/CSS to get the data that we want. To look at the underlying HTML/CSS you can either: 1) open a browser, right-click, and choose something like ‘Inspect’; or 2) save the website and then open it with a text editor rather than a browser. HTML/CSS is a markup language comprised of matching tags. If you want text to be bold then you would use something like: &lt;b&gt;My bold text&lt;/b&gt; Similarly, if you want a list then you start and end the list as well as each item. &lt;ul&gt; &lt;li&gt;Learn webscraping&lt;/li&gt; &lt;li&gt;Do data science&lt;/li&gt; &lt;li&gt;Proft&lt;/li&gt; &lt;/ul&gt; When scraping we will search for these tags. To get started, this is some HTML/CSS from my website. Let’s say that we want to grab my name from it. We can see that the name is in bold, so we want to probably focus on that feature and extract it. website_extract &lt;- &quot;&lt;p&gt;Hi, I’m &lt;b&gt;Rohan&lt;/b&gt; Alexander.&lt;/p&gt;&quot; We will use the rvest package Wickham (2019b). # install.packages(&quot;rvest&quot;) library(rvest) rohans_data &lt;- read_html(website_extract) rohans_data ## {html_document} ## &lt;html&gt; ## [1] &lt;body&gt;&lt;p&gt;Hi, I’m &lt;b&gt;Rohan&lt;/b&gt; Alexander.&lt;/p&gt;&lt;/body&gt; The language used by rvest to look for tags is ‘node,’ so we will focus on bold nodes. By default html_nodes() returns the tags as well. We can focus on the text that they contain, using html_text(). rohans_data %&gt;% html_nodes(&quot;b&quot;) ## {xml_nodeset (1)} ## [1] &lt;b&gt;Rohan&lt;/b&gt; first_name &lt;- rohans_data %&gt;% html_nodes(&quot;b&quot;) %&gt;% html_text() first_name ## [1] &quot;Rohan&quot; The result is that we learn my first name. 7.6 Case study - Rohan’s books 7.6.1 Introduction In this case study we are going to scrape a list of books that I own, clean it, and look at the distribution of the first letters of author surnames. It is slightly more complicated than the example above, but the underlying approach is the same - download the website, look for the nodes of interest, extract the information, clean it. 7.6.2 Gather Again, the key library that we are using is the rvest library. This makes it easier to download a website, and to then navigate the html to find the aspects that we are interested in. You should create a new project in a new folder (File -&gt; New Project). Within that new folder you should make three new folders: inputs, outputs, and scripts. In the scripts folder you should write and save a script along these lines. This script loads the libraries that we need, then visits my website, and saves a local copy. #### Contact details #### # Title: Get data from rohanalexander.com # Purpose: This script gets data from Rohan&#39;s website about the books that he # owns. It calls his website and then saves the dataset to inputs. # Author: Rohan Alexander # Contact: rohan.alexander@utoronto.ca # Last updated: 20 May 2020 #### Set up workspace #### library(rvest) library(tidyverse) #### Get html #### rohans_data &lt;- read_html(&quot;https://rohanalexander.com/bookshelf.html&quot;) # This takes a website as an input and will read it into R, in the same way that we # can read a, say, CSV into R. write_html(rohans_data, &quot;inputs/my_website/raw_data.html&quot;) # Always save your raw dataset as soon as you get it so that you have a record # of it. This is the equivalent of, say, write_csv() that we have used earlier. 7.6.3 Clean Now we need to navigate the HTML to get the aspects that we want, and to then put them into some sensible structure. I always try to get the data into a tibble as early as possible. While it’s possible to work with the nested data, I move to a tibble so that the usual verbs that I’m used to can be used. In the scripts folder you should write and save a new R script along these lines. First, we need to add the top matter, read in the libraries and the data that we scraped. #### Contact details #### # Title: Clean data from rohanaledander.com # Purpose: This script cleans data that was downloaded in 01-get_data.R. # Author: Rohan Alexander # Contact: rohan.alexander@utoronto.ca # Pre-requisites: Need to have run 01_get_data.R and have saved the data. # Last updated: 20 May 2020 #### Set up workspace #### library(tidyverse) library(rvest) rohans_data &lt;- read_html(&quot;inputs/my_website/raw_data.html&quot;) rohans_data ## {html_document} ## &lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot; lang=&quot;&quot; xml:lang=&quot;&quot;&gt; ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... ## [2] &lt;body&gt;\\n\\n&lt;!--radix_placeholder_front_matter--&gt;\\n\\n&lt;script id=&quot;distill-fr ... Now we need to identify the data that we are interested in using html tags and convert it to a tibble. If you look at the website, then you should notice that we are likely trying to focus on list items (Figure 7.4). Figure 7.4: Some of Rohan’s books Let’s look at the source (Figure 7.5). Figure 7.5: Source code for top of the page There’s a lot of debris, but scrolling down we eventually get to a list (Figure 7.6). Figure 7.6: Source code for list The tag for a list item is ‘li,’ so we modify the earlier code to focus on that and to get the text. #### Clean data #### # Identify the lines that have books on them based on the list html tag text_data &lt;- rohans_data %&gt;% html_nodes(&quot;li&quot;) %&gt;% html_text() all_books &lt;- tibble(books = text_data) head(all_books) ## # A tibble: 6 x 1 ## books ## &lt;chr&gt; ## 1 &quot;-“A Little Life”, Hanya Yanighara. Recommended by Lauren.&quot; ## 2 &quot;“The Andromeda Strain”, Michael Crichton.&quot; ## 3 &quot;“Is There Life After Housework”, Don Aslett.\\nGot given this at the Museum o… ## 4 &quot;“The Chosen”, Chaim Potok.&quot; ## 5 &quot;“The Forsyth Saga”, John Galsworthy.&quot; ## 6 &quot;“Freakonomics”, Steven Levitt and Stephen Dubner.&quot; We now need to clean the data. First we want to separate the title and the author # All content is just one string, so need to separate title and author all_books &lt;- all_books %&gt;% separate(books, into = c(&quot;title&quot;, &quot;author&quot;), sep = &quot;”&quot;) # Remove leading comma and clean up the titles a little all_books &lt;- all_books %&gt;% mutate(author = str_remove_all(author, &quot;^, &quot;), author = str_squish(author), title = str_remove(title, &quot;“&quot;), title = str_remove(title, &quot;^-&quot;) ) head(all_books) ## # A tibble: 6 x 2 ## title author ## &lt;chr&gt; &lt;chr&gt; ## 1 A Little Life Hanya Yanighara. Recommended by Lauren. ## 2 The Andromeda Strain Michael Crichton. ## 3 Is There Life After H… Don Aslett. Got given this at the Museum of Clean in P… ## 4 The Chosen Chaim Potok. ## 5 The Forsyth Saga John Galsworthy. ## 6 Freakonomics Steven Levitt and Stephen Dubner. Finally, some specific cleaning is needed. # Some authors have comments after their name, so need to get rid of them, although there are some exceptions that will not work # J. K. Rowling. # M. Mitchell Waldrop. # David A. Price all_books &lt;- all_books %&gt;% mutate(author = str_replace_all(author, c(&quot;J. K. Rowling.&quot; = &quot;J K Rowling.&quot;, &quot;M. Mitchell Waldrop.&quot; = &quot;M Mitchell Waldrop.&quot;, &quot;David A. Price&quot; = &quot;David A Price&quot;) ) ) %&gt;% separate(author, into = c(&quot;author_correct&quot;, &quot;throw_away&quot;), sep = &quot;\\\\.&quot;, extra = &quot;drop&quot;) %&gt;% select(-throw_away) %&gt;% rename(author = author_correct) # Some books have multiple authors, so need to separate them # One has multiple authors: # &quot;Daniela Witten, Gareth James, Robert Tibshirani, and Trevor Hastie&quot; all_books &lt;- all_books %&gt;% mutate(author = str_replace(author, &quot;Daniela Witten, Gareth James, Robert Tibshirani, and Trevor Hastie&quot;, &quot;Daniela Witten and Gareth James and Robert Tibshirani and Trevor Hastie&quot;)) %&gt;% separate(author, into = c(&quot;author_first&quot;, &quot;author_second&quot;, &quot;author_third&quot;, &quot;author_fourth&quot;), sep = &quot; and &quot;, fill = &quot;right&quot;) %&gt;% pivot_longer(cols = starts_with(&quot;author_&quot;), names_to = &quot;author_position&quot;, values_to = &quot;author&quot;) %&gt;% select(-author_position) %&gt;% filter(!is.na(author)) head(all_books) ## # A tibble: 6 x 2 ## title author ## &lt;chr&gt; &lt;chr&gt; ## 1 A Little Life Hanya Yanighara ## 2 The Andromeda Strain Michael Crichton ## 3 Is There Life After Housework Don Aslett ## 4 The Chosen Chaim Potok ## 5 The Forsyth Saga John Galsworthy ## 6 Freakonomics Steven Levitt It looks there is some at the end because I have a best of. I’ll just get rid of those manually because it’s not the focus. all_books &lt;- all_books %&gt;% slice(1:118) 7.6.4 Explore Finally, just because we have the data now, so we may as well try to do something with it, let’s look at the distribution of the first letter of the author names. all_books %&gt;% mutate( first_letter = str_sub(author, 1, 1) ) %&gt;% group_by(first_letter) %&gt;% count() ## # A tibble: 21 x 2 ## # Groups: first_letter [21] ## first_letter n ## &lt;chr&gt; &lt;int&gt; ## 1 &quot;&quot; 1 ## 2 &quot;A&quot; 8 ## 3 &quot;B&quot; 5 ## 4 &quot;C&quot; 4 ## 5 &quot;D&quot; 10 ## 6 &quot;E&quot; 3 ## 7 &quot;F&quot; 1 ## 8 &quot;G&quot; 10 ## 9 &quot;H&quot; 6 ## 10 &quot;I&quot; 1 ## # … with 11 more rows 7.7 Case study - Canadian Prime Ministers 7.7.1 Introduction In this case study we are interested in how long Canadian prime ministers lived, based on the year that they were born. We will scrape data from Wikipedia, clean it, and then make a graph. The key library that we will use for scraping is rvest. This adds a lot of functions that will make life easier. That said, every time you scrape a website things will change. Each scrape will largely be bespoke, even if you can borrow some code from earlier projects that you have completed. It is completely normal to feel frustrated at times. It helps to begin with an end in mind. To that end, let’s generate some simulated data. Ideally, we want a table that has a row for each prime minister, a column for their name, and a column each for the birth and death years. If they are still alive, then that death year can be empty. We know that birth and death years should be somewhere between 1700 and 1990, and that death year should be larger than birth year. Finally, we also know that the years should be integers, and the names should be characters. So, we want something that looks roughly like this: library(babynames) library(tidyverse) simulated_dataset &lt;- tibble(prime_minister = sample(x = babynames %&gt;% filter(prop &gt; 0.01) %&gt;% select(name) %&gt;% unique() %&gt;% unlist(), size = 10, replace = FALSE), birth_year = sample(x = c(1700:1990), size = 10, replace = TRUE), years_lived = sample(x = c(50:100), size = 10, replace = TRUE), death_year = birth_year + years_lived) %&gt;% select(prime_minister, birth_year, death_year, years_lived) %&gt;% arrange(birth_year) head(simulated_dataset) ## # A tibble: 6 x 4 ## prime_minister birth_year death_year years_lived ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Barbara 1714 1789 75 ## 2 Scott 1721 1802 81 ## 3 Marilyn 1726 1799 73 ## 4 Gregory 1737 1800 63 ## 5 Judy 1748 1816 68 ## 6 Jessica 1759 1828 69 One of the advantages of generating a simulated dataset is that if you are working in groups then one person can start making the graph, using the simulated dataset, while the other person gathers the data. In terms of a graph, we want something like Figure 7.7. Figure 7.7: Sketch of planned graph. 7.7.2 Gather We are starting with a question that is of interest, which how long each Canadian prime minister lived. As such, we need to identify a source of data While there are likely to be plenty of data sources that have the births and deaths of each prime minister, we want one that we can trust, and as we are going to be scraping, we want one that has some structure to it. The Wikipedia page (https://en.wikipedia.org/wiki/List_of_prime_ministers_of_Canada) fits both these criteria. As it is a popular page the information is more likely to be correct, and the data are available in a table. We load the library and then we read in the data from the relevant page. The key function here is read_html(), which you can use in the same way as, say, read_csv(), except that it takes a html page as an input. Once you call read_html() then the page is downloaded to your own computer, and it is usually a good idea to save this, using write_html() as it is your raw data. Saving it also means that we don’t have to keep visiting the website when we want to start again with our cleaning, and so it is part of being polite. However, it is likely not our property (in the case of Wikipedia, we might be okay), and so you should probably not share it. library(rvest) raw_data &lt;- read_html(&quot;https://en.wikipedia.org/wiki/List_of_prime_ministers_of_Canada&quot;) write_html(raw_data, &quot;inputs/wiki/pms.html&quot;) # Note that we save the file as a html file. 7.7.3 Clean Websites are made up of html, which is a markup language. We are looking for patterns in the mark-up that we can use to help us get closer to the data that we want. This is an iterative process and requires a lot of trial and error. Even simple examples will take time. You can look at the html by using a browser, right clicking, and then selecting view page source. Similarly, you could open the html file using a text editor. 7.7.3.1 By inspection We are looking for patterns that we can use to select the information that is of interest - names, birth year, and death year. When we look at the html it looks like there is something going on with &lt;tr&gt;, and then &lt;td&gt; (thanks to Thomas Rosenthal for identifying this). We select those nodes using html_nodes(), which takes the tags as an input. If you only want the first one then there is a singular version, html_node(). # Read in our saved data raw_data &lt;- read_html(&quot;inputs/wiki/pms.html&quot;) # We can parse tags in order parse_data_inspection &lt;- raw_data %&gt;% html_nodes(&quot;tr&quot;) %&gt;% html_nodes(&quot;td&quot;) %&gt;% html_text() # html_text removes any remaining html tags # But this code does exactly the same thing - the nodes are just pushed into # the one function call parse_data_inspection &lt;- raw_data %&gt;% html_nodes(&quot;tr td&quot;) %&gt;% html_text() head(parse_data_inspection) ## [1] &quot;Abbreviation key:&quot; ## [2] &quot;No.: Incumbent number, Min.: Ministry, Refs: References\\n&quot; ## [3] &quot;Colour key:&quot; ## [4] &quot;\\n\\n Liberal Party of Canada\\n \\n Historical Conservative parties (including Liberal-Conservative, Conservative (Historical), Unionist, National Liberal and Conservative, Progressive Conservative) \\n Conservative Party of Canada\\n\\n&quot; ## [5] &quot;Provinces key:&quot; ## [6] &quot;AB: Alberta, BC: British Columbia, MB: Manitoba, NS: Nova Scotia,ON: Ontario, QC: Quebec, SK: Saskatchewan\\n&quot; At this point our data is in a character vector, we want to convert it to a table, and reduce the data down to just the information that we want. The key that is going to allow us to do this is the fact that there seems to be a blank line (which in html is denoted by \\n) before the key information that we need. So, once we identify that line then we can filter to just the line below it! parsed_data &lt;- tibble(raw_text = parse_data_inspection) %&gt;% # Convert the character vector to a table mutate(is_PM = if_else(raw_text == &quot;\\n\\n&quot;, 1, 0), # Look for the blank line that is # above the row that we want is_PM = lag(is_PM, n = 1)) %&gt;% # Identify the actual row that we want filter(is_PM == 1) # Just get the rows that we want head(parsed_data) ## # A tibble: 6 x 2 ## raw_text is_PM ## &lt;chr&gt; &lt;dbl&gt; ## 1 &quot;\\nSir John A. MacDonald(1815–1891)MP for Kingston, ON\\n&quot; 1 ## 2 &quot;\\nAlexander Mackenzie(1822–1892)MP for Lambton, ON\\n&quot; 1 ## 3 &quot;\\nSir John A. MacDonald(1815–1891)MP for Victoria, BC until 1882MP for… 1 ## 4 &quot;\\nSir John Abbott(1821–1893)Senator for Quebec\\n&quot; 1 ## 5 &quot;\\nSir John Thompson(1845–1894)MP for Antigonish, NS\\n&quot; 1 ## 6 &quot;\\nSir Mackenzie Bowell(1823–1917)Senator for Ontario\\n&quot; 1 7.7.3.2 Using the selector gadget If you are comfortable with html then you might be able to see patterns, but one tool that may help is the SelectorGadget: https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html. This allows you to pick and choose the elements that you want, and then gives you the input to give to html_nodes() (Figure 7.8) Figure 7.8: Using the Selector Gadget to identify the tag, as at 13 March 2020. # Read in our saved data raw_data &lt;- read_html(&quot;inputs/wiki/pms.html&quot;) # We can parse tags in order parse_data_selector_gadget &lt;- raw_data %&gt;% html_nodes(&quot;td:nth-child(3)&quot;) %&gt;% html_text() # html_text removes any remaining html tags head(parse_data_selector_gadget) ## [1] &quot;\\nSir John A. MacDonald(1815–1891)MP for Kingston, ON\\n&quot; ## [2] &quot;\\nAlexander Mackenzie(1822–1892)MP for Lambton, ON\\n&quot; ## [3] &quot;\\nSir John A. MacDonald(1815–1891)MP for Victoria, BC until 1882MP for Carleton, ON until 1887MP for Kingston, ON\\n&quot; ## [4] &quot;\\nSir John Abbott(1821–1893)Senator for Quebec\\n&quot; ## [5] &quot;\\nSir John Thompson(1845–1894)MP for Antigonish, NS\\n&quot; ## [6] &quot;\\nSir Mackenzie Bowell(1823–1917)Senator for Ontario\\n&quot; In this case there is one prime minister - Robert Borden - who changed party and we would need to filter away that row: \\nUnionist Party\\n\". 7.7.3.3 Clean data Now that we have the parsed data, we need to clean it to match what we wanted. In particular we want a names column, as well as columns for birth year and death year. We will use separate() to take advantage of the fact that it looks like the dates are distinguished by brackets. initial_clean &lt;- parsed_data %&gt;% separate(raw_text, into = c(&quot;Name&quot;, &quot;not_name&quot;), sep = &quot;\\\\(&quot;, remove = FALSE) %&gt;% # The remove = FALSE option here means that we # keep the original column that we are separating. separate(not_name, into = c(&quot;Date&quot;, &quot;all_the_rest&quot;), sep = &quot;\\\\)&quot;, remove = FALSE) head(initial_clean) ## # A tibble: 6 x 6 ## raw_text Name not_name Date all_the_rest is_PM ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 &quot;\\nSir John A. Ma… &quot;\\nSir … &quot;1815–1891)MP fo… 1815… &quot;MP for Kingston, O… 1 ## 2 &quot;\\nAlexander Mack… &quot;\\nAlex… &quot;1822–1892)MP fo… 1822… &quot;MP for Lambton, ON… 1 ## 3 &quot;\\nSir John A. Ma… &quot;\\nSir … &quot;1815–1891)MP fo… 1815… &quot;MP for Victoria, B… 1 ## 4 &quot;\\nSir John Abbot… &quot;\\nSir … &quot;1821–1893)Senat… 1821… &quot;Senator for Quebec… 1 ## 5 &quot;\\nSir John Thomp… &quot;\\nSir … &quot;1845–1894)MP fo… 1845… &quot;MP for Antigonish,… 1 ## 6 &quot;\\nSir Mackenzie … &quot;\\nSir … &quot;1823–1917)Senat… 1823… &quot;Senator for Ontari… 1 Finally, we need to clean up the columns. cleaned_data &lt;- initial_clean %&gt;% select(Name, Date) %&gt;% separate(Date, into = c(&quot;Birth&quot;, &quot;Died&quot;), sep = &quot;–&quot;, remove = FALSE) %&gt;% # The # PMs who have died have their birth and death years separated by a hyphen, but # you need to be careful with the hyphen as it seems to be a slightly odd type of # hyphen and you need to copy/paste it. mutate(Birth = str_remove(Birth, &quot;b. &quot;)) %&gt;% # Alive PMs have slightly different format select(-Date) %&gt;% mutate(Name = str_remove(Name, &quot;\\n&quot;)) %&gt;% # Remove some html tags that remain mutate_at(vars(Birth, Died), ~as.integer(.)) %&gt;% # Change birth and death to integers mutate(Age_at_Death = Died - Birth) %&gt;% # Add column of the number of years they lived distinct() # Some of the PMs had two goes at it. head(cleaned_data) ## # A tibble: 6 x 4 ## Name Birth Died Age_at_Death ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Sir John A. MacDonald 1815 1891 76 ## 2 Alexander Mackenzie 1822 1892 70 ## 3 Sir John Abbott 1821 1893 72 ## 4 Sir John Thompson 1845 1894 49 ## 5 Sir Mackenzie Bowell 1823 1917 94 ## 6 Sir Charles Tupper 1821 1915 94 7.7.4 Explore At this point we’d like to make a graph that illustrates how long each prime minister lived. If they are still alive then we would like them to run to the end, but we would like to colour them differently. cleaned_data %&gt;% mutate(still_alive = if_else(is.na(Died), &quot;Yes&quot;, &quot;No&quot;), Died = if_else(is.na(Died), as.integer(2020), Died)) %&gt;% mutate(Name = as_factor(Name)) %&gt;% ggplot(aes(x = Birth, xend = Died, y = Name, yend = Name, color = still_alive)) + geom_segment() + labs(x = &quot;Year of birth&quot;, y = &quot;Prime minister&quot;, color = &quot;PM is alive&quot;, title = &quot;Canadian Prime Minister, by year of birth&quot;) + theme_minimal() + scale_color_brewer(palette = &quot;Set1&quot;) 7.8 PDFs 7.8.1 Introduction In contrast to an API, a PDF is usually only produced for human (not computer) consumption. The nice thing about PDFs is that they are static and constant. And it is nice that they make data available at all. But the trade-off is that: It is not overly useful to do larger-scale statistical analysis. We don’t know how the PDF was put together so we don’t know whether we can trust it. We can’t manipulate the data to get results that we are interested in. Indeed, sometimes governments publish data as PDFs because they don’t actually want you to be able to analyse it! Being able to get data from PDFs opens up a large number of datasets for you, some of which we’ll see in this chapter. There are two important aspects to keep in mind when approaching a PDF with a mind to extracting data from it: Begin with an end in mind. Planning and then literally sketching out what you want from a final dataset/graph/paper stops you wasting time and keeps you focused. Start simple, then iterate. The quickest way to make a complicated model is often to first build a simple model and then complicate it. Start with just trying to get one page of the PDF working or even just one line. Then iterate from there. In this chapter we start by walking through several examples and then go through three case studies of varying difficulty. 7.8.2 Getting started Figure 7.9 is a PDF that consists of just the first sentence from Jane Eyre taken from Project Gutenberg Bronte (1847). Figure 7.9: First sentence of Jane Eyre We will use the package pdftools Ooms (2019a) to get the text in this one page PDF into R. # install.packages(&quot;pdftools&quot;) library(pdftools) library(tidyverse) first_example &lt;- pdftools::pdf_text(&quot;inputs/pdfs/first_example.pdf&quot;) first_example ## [1] &quot;There was no possibility of taking a walk that day.&quot; class(first_example) ## [1] &quot;character&quot; We can see that the PDF has been correctly read in, as a character vector. We will now try a slightly more complicated example that consists of the first few paragraphs of Jane Eyre (Figure 7.10). Also notice that now we have the chapter heading as well. Figure 7.10: First few paragraphs of Jane Eyre We use the same function as before. second_example &lt;- pdftools::pdf_text(&quot;inputs/pdfs/second_example.pdf&quot;) second_example ## [1] &quot;CHAPTER I\\nThere was no possibility of taking a walk that day. We had been wandering, indeed, in the\\nleafless shrubbery an hour in the morning; but since dinner (Mrs. Reed, when there was no\\ncompany, dined early) the cold winter wind had brought with it clouds so sombre, and a rain so\\npenetrating, that further out-door exercise was now out of the question.\\nI was glad of it: I never liked long walks, especially on chilly afternoons: dreadful to me was the\\ncoming home in the raw twilight, with nipped fingers and toes, and a heart saddened by the\\nchidings of Bessie, the nurse, and humbled by the consciousness of my physical inferiority to\\nEliza, John, and Georgiana Reed.\\nThe said Eliza, John, and Georgiana were now clustered round their mama in the drawing-room:\\nshe lay reclined on a sofa by the fireside, and with her darlings about her (for the time neither\\nquarrelling nor crying) looked perfectly happy. Me, she had dispensed from joining the group;\\nsaying, “She regretted to be under the necessity of keeping me at a distance; but that until she\\nheard from Bessie, and could discover by her own observation, that I was endeavouring in good\\nearnest to acquire a more sociable and childlike disposition, a more attractive and sprightly\\nmanner—something lighter, franker, more natural, as it were—she really must exclude me from\\nprivileges intended only for contented, happy, little children.”\\n“What does Bessie say I have done?” I asked.\\n“Jane, I don’t like cavillers or questioners; besides, there is something truly forbidding in a child\\ntaking up her elders in that manner. Be seated somewhere; and until you can speak pleasantly,\\nremain silent.”\\nA breakfast-room adjoined the drawing-room, I slipped in there. It contained a bookcase: I soon\\npossessed myself of a volume, taking care that it should be one stored with pictures. I mounted\\ninto the window-seat: gathering up my feet, I sat cross-legged, like a Turk; and, having drawn the\\nred moreen curtain nearly close, I was shrined in double retirement.\\nFolds of scarlet drapery shut in my view to the right hand; to the left were the clear panes of\\nglass, protecting, but not separating me from the drear November day. At intervals, while\\nturning over the leaves of my book, I studied the aspect of that winter afternoon. Afar, it offered\\na pale blank of mist and cloud; near a scene of wet lawn and storm-beat shrub, with ceaseless\\nrain sweeping away wildly before a long and lamentable blast.\\n&quot; class(second_example) ## [1] &quot;character&quot; Again, we have a character vector. The end of each line is signalled by ‘\\n,’ but other than that it looks pretty good. Finally, we consider the first two pages. We use the same function as before. third_example &lt;- pdftools::pdf_text(&quot;inputs/pdfs/third_example.pdf&quot;) third_example ## [1] &quot;CHAPTER I\\nThere was no possibility of taking a walk that day. We had been wandering, indeed, in the\\nleafless shrubbery an hour in the morning; but since dinner (Mrs. Reed, when there was no\\ncompany, dined early) the cold winter wind had brought with it clouds so sombre, and a rain so\\npenetrating, that further out-door exercise was now out of the question.\\nI was glad of it: I never liked long walks, especially on chilly afternoons: dreadful to me was the\\ncoming home in the raw twilight, with nipped fingers and toes, and a heart saddened by the\\nchidings of Bessie, the nurse, and humbled by the consciousness of my physical inferiority to\\nEliza, John, and Georgiana Reed.\\nThe said Eliza, John, and Georgiana were now clustered round their mama in the drawing-room:\\nshe lay reclined on a sofa by the fireside, and with her darlings about her (for the time neither\\nquarrelling nor crying) looked perfectly happy. Me, she had dispensed from joining the group;\\nsaying, “She regretted to be under the necessity of keeping me at a distance; but that until she\\nheard from Bessie, and could discover by her own observation, that I was endeavouring in good\\nearnest to acquire a more sociable and childlike disposition, a more attractive and sprightly\\nmanner—something lighter, franker, more natural, as it were—she really must exclude me from\\nprivileges intended only for contented, happy, little children.”\\n“What does Bessie say I have done?” I asked.\\n“Jane, I don’t like cavillers or questioners; besides, there is something truly forbidding in a child\\ntaking up her elders in that manner. Be seated somewhere; and until you can speak pleasantly,\\nremain silent.”\\nA breakfast-room adjoined the drawing-room, I slipped in there. It contained a bookcase: I soon\\npossessed myself of a volume, taking care that it should be one stored with pictures. I mounted\\ninto the window-seat: gathering up my feet, I sat cross-legged, like a Turk; and, having drawn the\\nred moreen curtain nearly close, I was shrined in double retirement.\\nFolds of scarlet drapery shut in my view to the right hand; to the left were the clear panes of\\nglass, protecting, but not separating me from the drear November day. At intervals, while\\nturning over the leaves of my book, I studied the aspect of that winter afternoon. Afar, it offered\\na pale blank of mist and cloud; near a scene of wet lawn and storm-beat shrub, with ceaseless\\nrain sweeping away wildly before a long and lamentable blast.\\nI returned to my book—Bewick’s History of British Birds: the letterpress thereof I cared little\\nfor, generally speaking; and yet there were certain introductory pages that, child as I was, I could\\nnot pass quite as a blank. They were those which treat of the haunts of sea-fowl; of “the solitary\\nrocks and promontories” by them only inhabited; of the coast of Norway, studded with isles from\\nits southern extremity, the Lindeness, or Naze, to the North Cape—\\n“Where the Northern Ocean, in vast whirls,\\nBoils round the naked, melancholy isles\\n&quot; ## [2] &quot;Of farthest Thule; and the Atlantic surge\\nPours in among the stormy Hebrides.”\\nNor could I pass unnoticed the suggestion of the bleak shores of Lapland, Siberia, Spitzbergen,\\nNova Zembla, Iceland, Greenland, with “the vast sweep of the Arctic Zone, and those forlorn\\nregions of dreary space,—that reservoir of frost and snow, where firm fields of ice, the\\naccumulation of centuries of winters, glazed in Alpine heights above heights, surround the pole,\\nand concentre the multiplied rigours of extreme cold.” Of these death-white realms I formed an\\nidea of my own: shadowy, like all the half-comprehended notions that float dim through\\nchildren’s brains, but strangely impressive. The words in these introductory pages connected\\nthemselves with the succeeding vignettes, and gave significance to the rock standing up alone in\\na sea of billow and spray; to the broken boat stranded on a desolate coast; to the cold and ghastly\\nmoon glancing through bars of cloud at a wreck just sinking.\\nI cannot tell what sentiment haunted the quite solitary churchyard, with its inscribed headstone;\\nits gate, its two trees, its low horizon, girdled by a broken wall, and its newly-risen crescent,\\nattesting the hour of eventide.\\nThe two ships becalmed on a torpid sea, I believed to be marine phantoms.\\nThe fiend pinning down the thief’s pack behind him, I passed over quickly: it was an object of\\nterror.\\nSo was the black horned thing seated aloof on a rock, surveying a distant crowd surrounding a\\ngallows.\\nEach picture told a story; mysterious often to my undeveloped understanding and imperfect\\nfeelings, yet ever profoundly interesting: as interesting as the tales Bessie sometimes narrated on\\nwinter evenings, when she chanced to be in good humour; and when, having brought her ironing-\\ntable to the nursery hearth, she allowed us to sit about it, and while she got up Mrs. Reed’s lace\\nfrills, and crimped her nightcap borders, fed our eager attention with passages of love and\\nadventure taken from old fairy tales and other ballads; or (as at a later period I discovered) from\\nthe pages of Pamela, and Henry, Earl of Moreland.\\nWith Bewick on my knee, I was then happy: happy at least in my way. I feared nothing but\\ninterruption, and that came too soon. The breakfast-room door opened.\\n“Boh! Madam Mope!” cried the voice of John Reed; then he paused: he found the room\\napparently empty.\\n“Where the dickens is she!” he continued. “Lizzy! Georgy! (calling to his sisters) Joan is not\\nhere: tell mama she is run out into the rain—bad animal!”\\n“It is well I drew the curtain,” thought I; and I wished fervently he might not discover my hiding-\\nplace: nor would John Reed have found it out himself; he was not quick either of vision or\\nconception; but Eliza just put her head in at the door, and said at once—\\n&quot; class(third_example) ## [1] &quot;character&quot; Now, notice that the first page is the first element of the character vector and the second page is the second element. As we’re most familiar with rectangular data we’ll try to get it into that format as quickly as possible. And then we can use our regular tools to deal with it. First we want to convert the character vector into a tibble. At this point we may like to add page numbers as well. jane_eyre &lt;- tibble(raw_text = third_example, page_number = c(1:2)) We probably now want to separate the lines so that each line is an observation. We can do that by looking for the ‘\\n’ remembering that we need to escape the backslash as it’s a special character. jane_eyre &lt;- separate_rows(jane_eyre, raw_text, sep = &quot;\\\\n&quot;, convert = FALSE) head(jane_eyre) ## # A tibble: 6 x 2 ## raw_text page_number ## &lt;chr&gt; &lt;int&gt; ## 1 CHAPTER I 1 ## 2 There was no possibility of taking a walk that day. We had been w… 1 ## 3 leafless shrubbery an hour in the morning; but since dinner (Mrs.… 1 ## 4 company, dined early) the cold winter wind had brought with it cl… 1 ## 5 penetrating, that further out-door exercise was now out of the qu… 1 ## 6 I was glad of it: I never liked long walks, especially on chilly … 1 7.9 Case-study: US Total Fertility Rate, by state and year (2000-2018) 7.9.1 Introduction If you’re married to a demographer it is not too long until you are asked to look at a US Department of Health and Human Services Vital Statistics Report. In this case we are interested in trying to get the total fertility rate (the average number of births per woman assuming that woman experience the current age-specific fertility rates throughout their reproductive years)5 for each state for nineteen years. Annoyingly, the US persists in only making this data available in PDFs, but it makes a nice case study. In the case of the year 2000 the table that we are interested in is on page 40 of a PDF that is available https://www.cdc.gov/nchs/data/nvsr/nvsr50/nvsr50_05.pdf and it is the column labelled: “Total fertility rate” (Figure 7.11). Figure 7.11: Example Vital Statistics Report, from 2000 7.9.2 Begin with an end in mind The first step when getting data out of a PDF is to sketch out what you eventually want. A PDF typically contains a lot of information, and so it is handy to be very clear about what you need. This helps keep you focused, and prevents scope creep, but it is also helpful when thinking about data checks. Literally write down on paper what you have in mind. In this case, what is needed is a table with a column for state, year and TFR (Figure 7.12). Figure 7.12: Desired output from the PDF 7.9.3 Start simple, then iterate. There are 19 different PDFs and we are interested in a particular column in a particular table in each of them. Unfortunately there is nothing magical about what is coming. This first step requires working out the link for each, and the page and column name that is of interest. In the end, this looks like this. monicas_data &lt;- read_csv(&quot;inputs/tfr_tables_info.csv&quot;) monicas_data %&gt;% select(year, page, table, column_name, url) %&gt;% gt() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #skmcfbdfyb .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #skmcfbdfyb .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #skmcfbdfyb .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #skmcfbdfyb .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #skmcfbdfyb .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #skmcfbdfyb .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #skmcfbdfyb .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #skmcfbdfyb .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #skmcfbdfyb .gt_column_spanner_outer:first-child { padding-left: 0; } #skmcfbdfyb .gt_column_spanner_outer:last-child { padding-right: 0; } #skmcfbdfyb .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #skmcfbdfyb .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #skmcfbdfyb .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #skmcfbdfyb .gt_from_md > :first-child { margin-top: 0; } #skmcfbdfyb .gt_from_md > :last-child { margin-bottom: 0; } #skmcfbdfyb .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #skmcfbdfyb .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #skmcfbdfyb .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #skmcfbdfyb .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #skmcfbdfyb .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #skmcfbdfyb .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #skmcfbdfyb .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #skmcfbdfyb .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #skmcfbdfyb .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #skmcfbdfyb .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #skmcfbdfyb .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #skmcfbdfyb .gt_sourcenote { font-size: 90%; padding: 4px; } #skmcfbdfyb .gt_left { text-align: left; } #skmcfbdfyb .gt_center { text-align: center; } #skmcfbdfyb .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #skmcfbdfyb .gt_font_normal { font-weight: normal; } #skmcfbdfyb .gt_font_bold { font-weight: bold; } #skmcfbdfyb .gt_font_italic { font-style: italic; } #skmcfbdfyb .gt_super { font-size: 65%; } #skmcfbdfyb .gt_footnote_marks { font-style: italic; font-size: 65%; } year page table column_name url 2000 40 10 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr50/nvsr50_05.pdf 2001 41 10 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr51/nvsr51_02.pdf 2002 46 10 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr52/nvsr52_10.pdf 2003 45 10 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr54/nvsr54_02.pdf 2004 52 11 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr55/nvsr55_01.pdf 2005 52 11 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr56/nvsr56_06.pdf 2006 49 11 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr57/nvsr57_07.pdf 2007 41 11 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr58/nvsr58_24.pdf 2008 43 12 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr59/nvsr59_01.pdf 2009 43 12 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr60/nvsr60_01.pdf 2010 42 12 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr61/nvsr61_01.pdf 2011 40 12 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr62/nvsr62_01.pdf 2012 38 12 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr62/nvsr62_09.pdf 2013 37 12 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr64/nvsr64_01.pdf 2014 38 12 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr64/nvsr64_12.pdf 2015 42 12 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr66/nvsr66_01.pdf 2016 29 8 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr67/nvsr67_01.pdf 2016 30 8 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr67/nvsr67_01.pdf 2017 23 12 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr67/nvsr67_08-508.pdf 2017 24 12 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr67/nvsr67_08-508.pdf 2018 23 12 Total fertility rate https://www.cdc.gov/nchs/data/nvsr/nvsr68/nvsr68_13-508.pdf The first step is to get some code that works for one of them. I’ll step through the code in a lot more detail than normal because we’re going to use these pieces a lot. We will choose the year 2000. We first download the data and save it. download.file(url = monicas_data$url[1], destfile = &quot;inputs/pdfs/dhs/year_2000.pdf&quot;) We now want to read the PDF in as a character vector. dhs_2000 &lt;- pdftools::pdf_text(&quot;inputs/pdfs/dhs/year_2000.pdf&quot;) Convert it to a tibble, so that we can use familiar verbs on it. dhs_2000 &lt;- tibble(raw_data = dhs_2000) head(dhs_2000) ## # A tibble: 6 x 1 ## raw_data ## &lt;chr&gt; ## 1 &quot;Volume 50, Number 5 … ## 2 &quot;2 National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\\nH… ## 3 &quot; … ## 4 &quot;4 National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\\nD… ## 5 &quot; … ## 6 &quot;6 National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\\n … Grab the page that is of interest (remembering that each page is a element of the character vector, hence a row in the tibble). dhs_2000 &lt;- dhs_2000 %&gt;% slice(monicas_data$page[1]) head(dhs_2000) ## # A tibble: 1 x 1 ## raw_data ## &lt;chr&gt; ## 1 &quot;40 National Vital Statistics Report, Vol. 50, No. 5, Revised May 15, 20022\\n… Now we want to separate the rows. dhs_2000 &lt;- dhs_2000 %&gt;% separate_rows(raw_data, sep = &quot;\\\\n&quot;, convert = FALSE) head(dhs_2000) ## # A tibble: 6 x 1 ## raw_data ## &lt;chr&gt; ## 1 40 National Vital Statistics Report, Vol. 50, No. 5, Revised May 15, 20022 ## 2 Table 10. Number of births, birth rates, fertility rates, total fertility rat… ## 3 United States, each State and territory, 2000 ## 4 [By place of residence. Birth rates are live births per 1,000 estimated popul… ## 5 estimated in each area; total fertility rates are sums of birth rates for 5-y… ## 6 age group estimated in each area] Now we are searching for patterns that we can use. (If you have a lot of tables that you are interested in grabbing from PDFs then it may also be worthwhile considering the tabulizer package which is specifically designed for that. The issue is that it depends on Java and I always seem to run into trouble when I need to use Java so I avoid it when I can.) Let’s look at the first ten lines of content. dhs_2000[13:22,] ## # A tibble: 10 x 1 ## raw_data ## &lt;chr&gt; ## 1 United States 1 ...................................................... … ## 2 Alabama ............................................................... … ## 3 Alaska ................................................................... … ## 4 Arizona ................................................................. … ## 5 Arkansas ............................................................... … ## 6 California .............................................................. … ## 7 Colorado ............................................................... … ## 8 Connecticut ........................................................... … ## 9 Delaware .............................................................. … ## 10 District of Columbia .............................................. … It doesn’t get much better than this: We have dots separating the states from the data. We have a space between each of the columns. So we can now separate this in to separate columns. First we want to match on when there is at least two dots (remembering that the dot is a special character and so needs to be escaped). dhs_2000 &lt;- dhs_2000 %&gt;% separate(col = raw_data, into = c(&quot;state&quot;, &quot;data&quot;), sep = &quot;\\\\.{2,}&quot;, remove = FALSE, fill = &quot;right&quot; ) head(dhs_2000) ## # A tibble: 6 x 3 ## raw_data state data ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 40 National Vital Statistics Report,… 40 National Vital Statistics Repo… &lt;NA&gt; ## 2 Table 10. Number of births, birth ra… Table 10. Number of births, birth… &lt;NA&gt; ## 3 United States, each State and territ… United States, each State and ter… &lt;NA&gt; ## 4 [By place of residence. Birth rates … [By place of residence. Birth rat… &lt;NA&gt; ## 5 estimated in each area; total fertil… estimated in each area; total fer… &lt;NA&gt; ## 6 age group estimated in each area] age group estimated in each area] &lt;NA&gt; We get the expected warnings about the top and the bottom as they don’t have multiple dots. (Another option here is to use the pdf_data() function which would allow us to use location rather than delimiters.) We can now separate the data based on spaces. There is an inconsistent number of spaces, so we first squish any example of more than one space into just one. dhs_2000 &lt;- dhs_2000 %&gt;% mutate(data = str_squish(data)) %&gt;% tidyr::separate(col = data, into = c(&quot;number_of_births&quot;, &quot;birth_rate&quot;, &quot;fertility_rate&quot;, &quot;TFR&quot;, &quot;teen_births_all&quot;, &quot;teen_births_15_17&quot;, &quot;teen_births_18_19&quot;), sep = &quot;\\\\s&quot;, remove = FALSE ) head(dhs_2000) ## # A tibble: 6 x 10 ## raw_data state data number_of_births birth_rate fertility_rate TFR ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 40 National … 40 Natio… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 Table 10. Nu… Table 10… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 United State… United S… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 [By place of… [By plac… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 estimated in… estimate… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 age group es… age grou… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## # … with 3 more variables: teen_births_all &lt;chr&gt;, teen_births_15_17 &lt;chr&gt;, ## # teen_births_18_19 &lt;chr&gt; This is all looking fairly great. The only thing left is to clean up. dhs_2000 &lt;- dhs_2000 %&gt;% select(state, TFR) %&gt;% slice(13:69) %&gt;% mutate(year = 2000) dhs_2000 ## # A tibble: 57 x 3 ## state TFR year ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 &quot;United States 1 &quot; 2,130.0 2000 ## 2 &quot;Alabama &quot; 2,021.0 2000 ## 3 &quot;Alaska &quot; 2,437.0 2000 ## 4 &quot;Arizona &quot; 2,652.5 2000 ## 5 &quot;Arkansas &quot; 2,140.0 2000 ## 6 &quot;California &quot; 2,186.0 2000 ## 7 &quot;Colorado &quot; 2,356.5 2000 ## 8 &quot;Connecticut &quot; 1,931.5 2000 ## 9 &quot;Delaware &quot; 2,014.0 2000 ## 10 &quot;District of Columbia &quot; 1,975.5 2000 ## # … with 47 more rows And we’re done for that year. Now we want to take these pieces, put them into a function and then run that function over all 19 years. 7.9.4 Iterating 7.9.4.1 Get the PDFs The first part is downloading each of the 19 PDFs that we need. We’re going to build on the code that we used before. That code was: download.file(url = monicas_data$url[1], destfile = &quot;inputs/pdfs/dhs/year_2000.pdf&quot;) To modify this we need: To have it iterate through each of the lines in the dataset that contains our CSVs (i.e. where it says 1, we want 1, then 2, then 3, etc.). Where it has a filename, we need it to iterate through our desired filenames (i.e. year_2000, then year_2001, then year_2002, etc). We’d like for it to do all of this in a way that is a little robust to errors. For instance, if one of the URLs is wrong or the internet drops out then we’d like it to just move onto the next PDF, and then warn us at the end that it missed one, not to stop. (This doesn’t really matter because it’s only 19 files, but it’s pretty easy to find yourself doing this for thousands of files). We will draw on the purrr package for this Henry and Wickham (2020). library(purrr) monicas_data &lt;- monicas_data %&gt;% mutate(pdf_name = paste0(&quot;inputs/pdfs/dhs/year_&quot;, year, &quot;.pdf&quot;)) purrr::walk2(monicas_data$url, monicas_data$pdf_name, purrr::safely(~download.file(.x , .y))) What this code does it take the function download.file() and give it two arguments: .x and .y. The function walk2() then applies that function to the inputs that we give it, in this case the URLs columns is the .x and the pdf_names column is the .y. Finally, the safely() function means that if there are any failures then it just moves onto the next file instead of throwing an error. We now have each of the PDFs saved and we can move onto getting the data from them. 7.9.4.2 Get data from the PDFs Now we need to get the data from the PDFs. As before, we’re going to build on the code that we used before. That code (overly condensed) was: dhs_2000 &lt;- pdftools::pdf_text(&quot;inputs/pdfs/dhs/year_2000.pdf&quot;) dhs_2000 &lt;- tibble(raw_data = dhs_2000) %&gt;% slice(monicas_data$page[1]) %&gt;% separate_rows(raw_data, sep = &quot;\\\\n&quot;, convert = FALSE) %&gt;% separate(col = raw_data, into = c(&quot;state&quot;, &quot;data&quot;), sep = &quot;\\\\.{2,}&quot;, remove = FALSE) %&gt;% mutate(data = str_squish(data)) %&gt;% separate(col = data, into = c(&quot;number_of_births&quot;, &quot;birth_rate&quot;, &quot;fertility_rate&quot;, &quot;TFR&quot;, &quot;teen_births_all&quot;, &quot;teen_births_15_17&quot;, &quot;teen_births_18_19&quot;), sep = &quot;\\\\s&quot;, remove = FALSE) %&gt;% select(state, TFR) %&gt;% slice(13:69) %&gt;% mutate(year = 2000) dhs_2000 There are a bunch of aspects here that have been hardcoded, but the first thing that we want to iterate is the argument to pdf_text(), then the number in in slice() will also need to change (that is doing the work to get only the page that we are interested in). Two aspects are hardcoded and these may need to be updated. In particular: 1) The separate only works if each of the tables has the same columns in the same order; and 2) the slice (which restricts the data to just the states) only works in this particular case. Finally, we add the year only at the end, whereas we’d need to bring that up earlier in the process. We’ll start by writing a function that will go through all the files, grab the data, get the page of interest, and then expand the rows. We’ll then use a function from purrr to apply that function to all of the PDFs and to output a tibble. get_pdf_convert_to_tibble &lt;- function(pdf_name, page, year){ dhs_table_of_interest &lt;- tibble(raw_data = pdftools::pdf_text(pdf_name)) %&gt;% slice(page) %&gt;% separate_rows(raw_data, sep = &quot;\\\\n&quot;, convert = FALSE) %&gt;% separate(col = raw_data, into = c(&quot;state&quot;, &quot;data&quot;), sep = &quot;[�|\\\\.]\\\\s+(?=[[:digit:]])&quot;, remove = FALSE) %&gt;% mutate( data = str_squish(data), year_of_data = year) print(paste(&quot;Done with&quot;, year)) return(dhs_table_of_interest) } raw_dhs_data &lt;- purrr::pmap_dfr(monicas_data %&gt;% select(pdf_name, page, year), get_pdf_convert_to_tibble) ## [1] &quot;Done with 2000&quot; ## [1] &quot;Done with 2001&quot; ## [1] &quot;Done with 2002&quot; ## [1] &quot;Done with 2003&quot; ## [1] &quot;Done with 2004&quot; ## [1] &quot;Done with 2005&quot; ## [1] &quot;Done with 2006&quot; ## [1] &quot;Done with 2007&quot; ## [1] &quot;Done with 2008&quot; ## [1] &quot;Done with 2009&quot; ## [1] &quot;Done with 2010&quot; ## [1] &quot;Done with 2011&quot; ## [1] &quot;Done with 2012&quot; ## [1] &quot;Done with 2013&quot; ## [1] &quot;Done with 2014&quot; ## [1] &quot;Done with 2015&quot; ## [1] &quot;Done with 2016&quot; ## [1] &quot;Done with 2016&quot; ## [1] &quot;Done with 2017&quot; ## [1] &quot;Done with 2017&quot; ## [1] &quot;Done with 2018&quot; head(raw_dhs_data) ## # A tibble: 6 x 4 ## raw_data state data year_of_data ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 40 National Vital Statistics … 40 National Vital Statistic… 50, … 2000 ## 2 Table 10. Number of births, b… Table 10. Number of births,… &lt;NA&gt; 2000 ## 3 United States, each State and… United States, each State a… &lt;NA&gt; 2000 ## 4 [By place of residence. Birth… [By place of residence. Bir… &lt;NA&gt; 2000 ## 5 estimated in each area; total… estimated in each area; tot… &lt;NA&gt; 2000 ## 6 age group estimated in each a… age group estimated in each… &lt;NA&gt; 2000 Now we need to clean up the state names and then filter on them. states &lt;- c(&quot;Alabama&quot;, &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;California&quot;, &quot;Colorado&quot;, &quot;Connecticut&quot;, &quot;Delaware&quot;, &quot;Florida&quot;, &quot;Georgia&quot;, &quot;Hawaii&quot;, &quot;Idaho&quot;, &quot;Illinois&quot;, &quot;Indiana&quot;, &quot;Iowa&quot;, &quot;Kansas&quot;, &quot;Kentucky&quot;, &quot;Louisiana&quot;, &quot;Maine&quot;, &quot;Maryland&quot;, &quot;Massachusetts&quot;, &quot;Michigan&quot;, &quot;Minnesota&quot;, &quot;Mississippi&quot;, &quot;Missouri&quot;, &quot;Montana&quot;, &quot;Nebraska&quot;, &quot;Nevada&quot;, &quot;New Hampshire&quot;, &quot;New Jersey&quot;, &quot;New Mexico&quot;, &quot;New York&quot;, &quot;North Carolina&quot;, &quot;North Dakota&quot;, &quot;Ohio&quot;, &quot;Oklahoma&quot;, &quot;Oregon&quot;, &quot;Pennsylvania&quot;, &quot;Rhode Island&quot;, &quot;South Carolina&quot;, &quot;South Dakota&quot;, &quot;Tennessee&quot;, &quot;Texas&quot;, &quot;Utah&quot;, &quot;Vermont&quot;, &quot;Virginia&quot;, &quot;Washington&quot;, &quot;West Virginia&quot;, &quot;Wisconsin&quot;, &quot;Wyoming&quot;, &quot;District of Columbia&quot;) raw_dhs_data &lt;- raw_dhs_data %&gt;% mutate(state = str_remove_all(state, &quot;\\\\.&quot;), state = str_remove_all(state, &quot;�&quot;), state = str_remove_all(state, &quot;\b&quot;), state = str_replace_all(state, &quot;United States 1&quot;, &quot;United States&quot;), state = str_replace_all(state, &quot;United States1&quot;, &quot;United States&quot;), state = str_replace_all(state, &quot;United States 2&quot;, &quot;United States&quot;), state = str_replace_all(state, &quot;United States2&quot;, &quot;United States&quot;), state = str_replace_all(state, &quot;United States²&quot;, &quot;United States&quot;), ) %&gt;% mutate(state = str_squish(state)) %&gt;% filter(state %in% states) head(raw_dhs_data) ## # A tibble: 6 x 4 ## raw_data state data year_of_data ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Alabama ............................… Alabama 63,299 14.4 65.0 2… 2000 ## 2 Alaska .............................… Alaska 9,974 16.0 74.6 2,… 2000 ## 3 Arizona ............................… Arizona 85,273 17.5 84.4 2… 2000 ## 4 Arkansas ...........................… Arkans… 37,783 14.7 69.1 2… 2000 ## 5 California .........................… Califo… 531,959 15.8 70.7 … 2000 ## 6 Colorado ...........................… Colora… 65,438 15.8 73.1 2… 2000 The next step is to separate the data and get the correct column from it. We’re going to separate based on spaces once it is cleaned up. raw_dhs_data &lt;- raw_dhs_data %&gt;% mutate(data = str_remove_all(data, &quot;\\\\*&quot;)) %&gt;% separate(data, into = c(&quot;col_1&quot;, &quot;col_2&quot;, &quot;col_3&quot;, &quot;col_4&quot;, &quot;col_5&quot;, &quot;col_6&quot;, &quot;col_7&quot;, &quot;col_8&quot;, &quot;col_9&quot;, &quot;col_10&quot;), sep = &quot; &quot;, remove = FALSE) head(raw_dhs_data) ## # A tibble: 6 x 14 ## raw_data state data col_1 col_2 col_3 col_4 col_5 col_6 col_7 col_8 col_9 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Alabama ..… Alab… 63,29… 63,2… 14.4 65.0 2,02… 62.9 37.9 97.3 &lt;NA&gt; &lt;NA&gt; ## 2 Alaska ...… Alas… 9,974… 9,974 16.0 74.6 2,43… 42.4 23.6 69.4 &lt;NA&gt; &lt;NA&gt; ## 3 Arizona ..… Ariz… 85,27… 85,2… 17.5 84.4 2,65… 69.1 41.1 111.3 &lt;NA&gt; &lt;NA&gt; ## 4 Arkansas .… Arka… 37,78… 37,7… 14.7 69.1 2,14… 68.5 36.7 114.1 &lt;NA&gt; &lt;NA&gt; ## 5 California… Cali… 531,9… 531,… 15.8 70.7 2,18… 48.5 28.6 75.6 &lt;NA&gt; &lt;NA&gt; ## 6 Colorado .… Colo… 65,43… 65,4… 15.8 73.1 2,35… 49.2 28.6 79.8 &lt;NA&gt; &lt;NA&gt; ## # … with 2 more variables: col_10 &lt;chr&gt;, year_of_data &lt;dbl&gt; We can now grab the correct column. tfr_data &lt;- raw_dhs_data %&gt;% mutate(TFR = if_else(year_of_data &lt; 2008, col_4, col_3)) %&gt;% select(state, year_of_data, TFR) %&gt;% rename(year = year_of_data) head(tfr_data) ## # A tibble: 6 x 3 ## state year TFR ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Alabama 2000 2,021.0 ## 2 Alaska 2000 2,437.0 ## 3 Arizona 2000 2,652.5 ## 4 Arkansas 2000 2,140.0 ## 5 California 2000 2,186.0 ## 6 Colorado 2000 2,356.5 Finally, we need to convert the case. head(tfr_data) ## # A tibble: 6 x 3 ## state year TFR ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Alabama 2000 2,021.0 ## 2 Alaska 2000 2,437.0 ## 3 Arizona 2000 2,652.5 ## 4 Arkansas 2000 2,140.0 ## 5 California 2000 2,186.0 ## 6 Colorado 2000 2,356.5 tfr_data &lt;- tfr_data %&gt;% mutate(TFR = str_remove_all(TFR, &quot;,&quot;), TFR = as.numeric(TFR)) head(tfr_data) ## # A tibble: 6 x 3 ## state year TFR ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alabama 2000 2021 ## 2 Alaska 2000 2437 ## 3 Arizona 2000 2652. ## 4 Arkansas 2000 2140 ## 5 California 2000 2186 ## 6 Colorado 2000 2356. And run some checks. # tfr_data %&gt;% # skimr::skim() In particular we want for there to be 51 states and for there to be 19 years. And we’re done. head(tfr_data) ## # A tibble: 6 x 3 ## state year TFR ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alabama 2000 2021 ## 2 Alaska 2000 2437 ## 3 Arizona 2000 2652. ## 4 Arkansas 2000 2140 ## 5 California 2000 2186 ## 6 Colorado 2000 2356. write_csv(tfr_data, &quot;outputs/monicas_tfr.csv&quot;) 7.10 Case-study: Kenyan census data The distribution of population by age, sex, and administrative unit from the 2019 Kenyan census can be downloaded here: https://www.knbs.or.ke/?wpdmpro=2019-kenya-population-and-housing-census-volume-iii-distribution-of-population-by-age-sex-and-administrative-units. And while it is great that they make it easily available, and it is easy to look-up a particular result, it is not overly useful to do larger-scale data analysis, such as building a Bayesian hierarchical model. In this section we will convert a PDF of Kenyan census results of counts, by age and sex, by county and sub-county, into a tidy dataset that can be analysed. I will draw on and introduce a bunch of handy packages including: janitor by Firke (2020), pdftools by Ooms (2019b), tidyverse by Wickham et al. (2019a), and stringi by Gagolewski (2020). 7.10.1 Set-up To get started I need to load the necessary packages. library(janitor) library(pdftools) library(tidyverse) library(stringi) And then I need to read in the PDF that I want to convert. # Read in the PDF all_content &lt;- pdftools::pdf_text(&quot;inputs/pdfs/2019_Kenya_census.pdf&quot;) The pdf_text function from pdftools is useful when you have a PDF and you want to read the content into R. For many recently produced PDFs it’ll work pretty well, but there are alternatives. If the PDF is an image, then it won’t work and you’ll need to turn to OCR. You can see a page of the PDF here: knitr::include_graphics(&quot;figures/2020-04-10-screenshot-of-census.png&quot;) 7.10.2 Extract The first challenge is to get the dataset into a format that we can more easily manipulate. The way that I am going to do this is to consider each page of the PDF and extract the relevant parts. To do this, I first write a function that I want to apply to each page. # The function is going to take an input of a page get_data &lt;- function(i){ # Just look at the page of interest # Based on https://stackoverflow.com/questions/47793326/tabulize-function-in-r just_page_i &lt;- stringi::stri_split_lines(all_content[[i]])[[1]] # Grab the name of the location area &lt;- just_page_i[3] %&gt;% str_squish() area &lt;- str_to_title(area) # Grab the type of table type_of_table &lt;- just_page_i[2] %&gt;% str_squish() # Get rid of the top matter just_page_i_no_header &lt;- just_page_i[5:length(just_page_i)] # Just manually for now, but could create some rules if needed # Get rid of the bottom matter just_page_i_no_header_no_footer &lt;- just_page_i_no_header[1:62] # Just manually for now, but could create some rules if needed # Convert into a tibble demography_data &lt;- tibble(all = just_page_i_no_header_no_footer) # # Split columns demography_data &lt;- demography_data %&gt;% mutate(all = str_squish(all)) %&gt;% # Any space more than two spaces is squished down to one mutate(all = str_replace(all, &quot;10 -14&quot;, &quot;10-14&quot;)) %&gt;% mutate(all = str_replace(all, &quot;Not Stated&quot;, &quot;NotStated&quot;)) %&gt;% # Any space more than two spaces is squished down to one separate(col = all, into = c(&quot;age&quot;, &quot;male&quot;, &quot;female&quot;, &quot;total&quot;, &quot;age_2&quot;, &quot;male_2&quot;, &quot;female_2&quot;, &quot;total_2&quot;), sep = &quot; &quot;, # Just looking for a space. Seems to work fine because the tables are pretty nicely laid out remove = TRUE, fill = &quot;right&quot; ) # They are side by side at the moment, need to append to bottom demography_data_long &lt;- rbind(demography_data %&gt;% select(age, male, female, total), demography_data %&gt;% select(age_2, male_2, female_2, total_2) %&gt;% rename(age = age_2, male = male_2, female = female_2, total = total_2) ) # There is one row of NAs, so remove it demography_data_long &lt;- demography_data_long %&gt;% janitor::remove_empty(which = c(&quot;rows&quot;)) # Add the area and the page demography_data_long$area &lt;- area demography_data_long$table &lt;- type_of_table demography_data_long$page &lt;- i rm(just_page_i, i, area, type_of_table, just_page_i_no_header, just_page_i_no_header_no_footer, demography_data) return(demography_data_long) } At this point, I have a function that does what I need to each page of the PDF. I’m going to use the function map_dfr from the purrr package to apply that function to each page, and then combine all the outputs into one tibble. # Run through each relevant page and get the data pages &lt;- c(30:513) all_tables &lt;- map_dfr(pages, get_data) rm(pages, get_data, all_content) 7.10.3 Clean I now need to clean the dataset to make it useful. 7.10.3.1 Values The first step is to make the numbers into actual numbers, rather than characters. Before I can convert the type I need to remove anything that is not a number otherwise it’ll be converted into an NA. I first identify any values that are not numbers so that I can remove them. # Need to convert male, female, and total to integers # First find the characters that should not be in there all_tables %&gt;% select(male, female, total) %&gt;% mutate_all(~str_remove_all(., &quot;[:digit:]&quot;)) %&gt;% mutate_all(~str_remove_all(., &quot;,&quot;)) %&gt;% mutate_all(~str_remove_all(., &quot;_&quot;)) %&gt;% mutate_all(~str_remove_all(., &quot;-&quot;)) %&gt;% distinct() ## # A tibble: 3 x 3 ## male female total ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &quot;&quot; &quot;&quot; &quot;&quot; ## 2 &quot;Aug&quot; &quot;&quot; &quot;&quot; ## 3 &quot;Jun&quot; &quot;&quot; &quot;&quot; # We clearly need to remove &quot;,&quot;, &quot;_&quot;, and &quot;-&quot;. # This also highlights a few issues on p. 185 that need to be manually adjusted # https://twitter.com/RohanAlexander/status/1244337583016022018 all_tables$male[all_tables$male == &quot;23-Jun&quot;] &lt;- 4923 all_tables$male[all_tables$male == &quot;15-Aug&quot;] &lt;- 4611 While you could use the janitor package here, it is worthwhile at least first looking at what is going on because sometimes there is odd stuff that janitor (and other packages) will deal with, but not in a way that you want. In this case, they’ve used Excel or similar and this has converted a couple of their entries into dates. If we just took the numbers from the column then we’d have 23 and 15 here, but by inspecting the column we can use Excel to reverse the process and enter the correct values of 4,923 and 4,611, respectively. Having identified everything that needs to be removed, we can do the actual removal and convert our character column of numbers to integers. all_tables &lt;- all_tables %&gt;% mutate_at(vars(male, female, total), ~str_remove_all(., &quot;,&quot;)) %&gt;% # First get rid of commas mutate_at(vars(male, female, total), ~str_replace(., &quot;_&quot;, &quot;0&quot;)) %&gt;% mutate_at(vars(male, female, total), ~str_replace(., &quot;-&quot;, &quot;0&quot;)) %&gt;% mutate_at(vars(male, female, total), ~as.integer(.)) 7.10.3.2 Areas The next thing to clean is the areas. We know that there are 47 counties in Kenya, and a whole bunch of sub-counties. They give us a list on pages 19 to 22 of the PDF (document pages 7 to 10). However, this list is not complete, and there are a few minor issues that we’ll deal with later. In any case, I first need to fix a few inconsistencies. # Fix some area names all_tables$area[all_tables$area == &quot;Taita/ Taveta&quot;] &lt;- &quot;Taita/Taveta&quot; all_tables$area[all_tables$area == &quot;Elgeyo/ Marakwet&quot;] &lt;- &quot;Elgeyo/Marakwet&quot; all_tables$area[all_tables$area == &quot;Nairobi City&quot;] &lt;- &quot;Nairobi&quot; Kenya has 47 counties, each of which has sub-counties. The PDF has them arranged as the county data then the sub-counties, without designating which is which. We can use the names, to a certain extent, but in a handful of cases, there is a sub-county that has the same name as a county so we need to first fix that. The PDF is made-up of three tables. all_tables$table %&gt;% table() ## . ## Table 2.3: Distribution of Population by Age, Sex*, County and Sub- County ## 48216 ## Table 2.4a: Distribution of Rural Population by Age, Sex* and County ## 5535 ## Table 2.4b: Distribution of Urban Population by Age, Sex* and County ## 5781 So I can first get the names of the counties based on those first two tables and then reconcile them to get a list of the counties. # Get a list of the counties list_counties &lt;- all_tables %&gt;% filter(table %in% c(&quot;Table 2.4a: Distribution of Rural Population by Age, Sex* and County&quot;, &quot;Table 2.4b: Distribution of Urban Population by Age, Sex* and County&quot;) ) %&gt;% select(area) %&gt;% distinct() As I hoped, there are 47 of them. But before I can add a flag based on those names, I need to deal with the sub-counties that share their name. We will do this based on the page, then looking it up and deciding which is the county page and which is the sub-county page. # The following have the issue of the name being used for both a county and a sub-county: all_tables %&gt;% filter(table == &quot;Table 2.3: Distribution of Population by Age, Sex*, County and Sub- County&quot;) %&gt;% filter(area %in% c(&quot;Busia&quot;, &quot;Garissa&quot;, &quot;Homa Bay&quot;, &quot;Isiolo&quot;, &quot;Kiambu&quot;, &quot;Machakos&quot;, &quot;Makueni&quot;, &quot;Samburu&quot;, &quot;Siaya&quot;, &quot;Tana River&quot;, &quot;Vihiga&quot;, &quot;West Pokot&quot;) ) %&gt;% select(area, page) %&gt;% distinct() ## # A tibble: 24 x 2 ## area page ## &lt;chr&gt; &lt;int&gt; ## 1 Samburu 42 ## 2 Tana River 53 ## 3 Tana River 56 ## 4 Garissa 65 ## 5 Garissa 69 ## 6 Isiolo 98 ## 7 Isiolo 100 ## 8 Machakos 149 ## 9 Machakos 154 ## 10 Makueni 159 ## # … with 14 more rows Now we can add the flag for whether the area is a county and adjust for the ones that are troublesome, # Add flag for whether it is a county or a sub-county all_tables &lt;- all_tables %&gt;% mutate(area_type = if_else(area %in% list_counties$area, &quot;county&quot;, &quot;sub-county&quot;)) # Fix the flag for the ones that have their names used twice all_tables &lt;- all_tables %&gt;% mutate(area_type = case_when( area == &quot;Samburu&quot; &amp; page == 42 ~ &quot;sub-county&quot;, area == &quot;Tana River&quot; &amp; page == 56 ~ &quot;sub-county&quot;, area == &quot;Garissa&quot; &amp; page == 69 ~ &quot;sub-county&quot;, area == &quot;Isiolo&quot; &amp; page == 100 ~ &quot;sub-county&quot;, area == &quot;Machakos&quot; &amp; page == 154 ~ &quot;sub-county&quot;, area == &quot;Makueni&quot; &amp; page == 164 ~ &quot;sub-county&quot;, area == &quot;Kiambu&quot; &amp; page == 213 ~ &quot;sub-county&quot;, area == &quot;West Pokot&quot; &amp; page == 233 ~ &quot;sub-county&quot;, area == &quot;Vihiga&quot; &amp; page == 333 ~ &quot;sub-county&quot;, area == &quot;Busia&quot; &amp; page == 353 ~ &quot;sub-county&quot;, area == &quot;Siaya&quot; &amp; page == 360 ~ &quot;sub-county&quot;, area == &quot;Homa Bay&quot; &amp; page == 375 ~ &quot;sub-county&quot;, TRUE ~ area_type ) ) rm(list_counties) 7.10.3.3 Ages Now we can deal with the ages. First we need to fix some errors. # Clean up ages table(all_tables$age) %&gt;% head() ## ## 0 0-4 1 10 10-14 10-19 ## 484 484 484 484 482 1 unique(all_tables$age) %&gt;% head() ## [1] &quot;Total&quot; &quot;0&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; # Looks like there should be 484, so need to follow up on some: all_tables$age[all_tables$age == &quot;NotStated&quot;] &lt;- &quot;Not Stated&quot; all_tables$age[all_tables$age == &quot;43594&quot;] &lt;- &quot;5-9&quot; all_tables$age[all_tables$age == &quot;43752&quot;] &lt;- &quot;10-14&quot; all_tables$age[all_tables$age == &quot;9-14&quot;] &lt;- &quot;5-9&quot; all_tables$age[all_tables$age == &quot;10-19&quot;] &lt;- &quot;10-14&quot; The census has done some of the work of putting together age-groups for us, but we want to make it easy to just focus on the counts by single-year-age. As such I’ll add a flag as to the type of age it is: an age group, such as ages 0 to 5, or a single age, such as 1. # Add a flag as to whether it&#39;s a summary or not all_tables$age_type &lt;- if_else(str_detect(all_tables$age, c(&quot;-&quot;)), &quot;age-group&quot;, &quot;single-year&quot;) all_tables$age_type &lt;- if_else(str_detect(all_tables$age, c(&quot;Total&quot;)), &quot;age-group&quot;, all_tables$age_type) At the moment, age is a character variable. We have a decision to make here, because we don’t want it to be a character variable (because it won’t graph properly), but we don’t want it to be a numeric, because there is total and also 100+ in there. For now, we’ll just make it into a factor, and at least that will be able to be nicely graphed. all_tables$age &lt;- as_factor(all_tables$age) 7.10.4 Check 7.10.4.1 Gender sum Given the format of the data, at this point it is easy to check that total is the sum of male and female. # Check the parts and the sums follow_up &lt;- all_tables %&gt;% mutate(check_sum = male + female, totals_match = if_else(total == check_sum, 1, 0) ) %&gt;% filter(totals_match == 0) There is just one that seems wrong. # There is just one that looks wrong all_tables$male[all_tables$age == &quot;10&quot; &amp; all_tables$page == 187] &lt;- as.integer(1) rm(follow_up) 7.10.4.2 Rural-urban split The census provides different tables for the total of each county and sub-county; and then within each county, for the number in an urban area in that county, and the number in a urban area in that county. Some counties only have an urban count, but we’d like to make sure that the sum of rural and urban counts equals the total count. This requires reshaping the data from a long to wide format. First, construct different tables for each of the three. I just do it manually, but I could probably do this a nicer way. # Table 2.3 table_2_3 &lt;- all_tables %&gt;% filter(table == &quot;Table 2.3: Distribution of Population by Age, Sex*, County and Sub- County&quot;) table_2_4a &lt;- all_tables %&gt;% filter(table == &quot;Table 2.4a: Distribution of Rural Population by Age, Sex* and County&quot;) table_2_4b &lt;- all_tables %&gt;% filter(table == &quot;Table 2.4b: Distribution of Urban Population by Age, Sex* and County&quot;) Having constructed the constituent parts, I now join then based on age, area, and whether it is a county. both_2_4s &lt;- full_join(table_2_4a, table_2_4b, by = c(&quot;age&quot;, &quot;area&quot;, &quot;area_type&quot;), suffix = c(&quot;_rural&quot;, &quot;_urban&quot;)) all &lt;- full_join(table_2_3, both_2_4s, by = c(&quot;age&quot;, &quot;area&quot;, &quot;area_type&quot;), suffix = c(&quot;_all&quot;, &quot;_&quot;)) all &lt;- all %&gt;% mutate(page = glue::glue(&#39;Total from p. {page}, rural from p. {page_rural}, urban from p. {page_urban}&#39;)) %&gt;% select(-page, -page_rural, -page_urban, -table, -table_rural, -table_urban, -age_type_rural, -age_type_urban ) rm(both_2_4s, table_2_3, table_2_4a, table_2_4b) We can now check that the sum of rural and urban is the same as the total. # Check that the urban + rural = total follow_up &lt;- all %&gt;% mutate(total_from_bits = total_rural + total_urban, check_total_is_rural_plus_urban = if_else(total == total_from_bits, 1, 0), total_from_bits - total) %&gt;% filter(check_total_is_rural_plus_urban == 0) head(follow_up) ## # A tibble: 3 x 16 ## age male female total area area_type age_type male_rural female_rural ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Not St… 31 10 41 Naku… county single-y… 8 6 ## 2 Total 434287 441379 875666 Bomet county age-group 420119 427576 ## 3 Not St… 3 2 5 Bomet county single-y… 2 1 ## # … with 7 more variables: total_rural &lt;int&gt;, male_urban &lt;int&gt;, ## # female_urban &lt;int&gt;, total_urban &lt;int&gt;, total_from_bits &lt;int&gt;, ## # check_total_is_rural_plus_urban &lt;dbl&gt;, total_from_bits - total &lt;int&gt; rm(follow_up) There are just a few, but they only have a a difference of 1, so I’ll just move on. 7.10.4.3 Ages sum to age-groups Finally, I want to check that the single age counts sum to the age-groups. # One last thing to check is that the ages sum to their age-groups. follow_up &lt;- all %&gt;% mutate(groups = case_when(age %in% c(&quot;0&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;0-4&quot;) ~ &quot;0-4&quot;, age %in% c(&quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;, &quot;5-9&quot;) ~ &quot;5-9&quot;, age %in% c(&quot;10&quot;, &quot;11&quot;, &quot;12&quot;, &quot;13&quot;, &quot;14&quot;, &quot;10-14&quot;) ~ &quot;10-14&quot;, age %in% c(&quot;15&quot;, &quot;16&quot;, &quot;17&quot;, &quot;18&quot;, &quot;19&quot;, &quot;15-19&quot;) ~ &quot;15-19&quot;, age %in% c(&quot;20&quot;, &quot;21&quot;, &quot;22&quot;, &quot;23&quot;, &quot;24&quot;, &quot;20-24&quot;) ~ &quot;20-24&quot;, age %in% c(&quot;25&quot;, &quot;26&quot;, &quot;27&quot;, &quot;28&quot;, &quot;29&quot;, &quot;25-29&quot;) ~ &quot;25-29&quot;, age %in% c(&quot;30&quot;, &quot;31&quot;, &quot;32&quot;, &quot;33&quot;, &quot;34&quot;, &quot;30-34&quot;) ~ &quot;30-34&quot;, age %in% c(&quot;35&quot;, &quot;36&quot;, &quot;37&quot;, &quot;38&quot;, &quot;39&quot;, &quot;35-39&quot;) ~ &quot;35-39&quot;, age %in% c(&quot;40&quot;, &quot;41&quot;, &quot;42&quot;, &quot;43&quot;, &quot;44&quot;, &quot;40-44&quot;) ~ &quot;40-44&quot;, age %in% c(&quot;45&quot;, &quot;46&quot;, &quot;47&quot;, &quot;48&quot;, &quot;49&quot;, &quot;45-49&quot;) ~ &quot;45-49&quot;, age %in% c(&quot;50&quot;, &quot;51&quot;, &quot;52&quot;, &quot;53&quot;, &quot;54&quot;, &quot;50-54&quot;) ~ &quot;50-54&quot;, age %in% c(&quot;55&quot;, &quot;56&quot;, &quot;57&quot;, &quot;58&quot;, &quot;59&quot;, &quot;55-59&quot;) ~ &quot;55-59&quot;, age %in% c(&quot;60&quot;, &quot;61&quot;, &quot;62&quot;, &quot;63&quot;, &quot;64&quot;, &quot;60-64&quot;) ~ &quot;60-64&quot;, age %in% c(&quot;65&quot;, &quot;66&quot;, &quot;67&quot;, &quot;68&quot;, &quot;69&quot;, &quot;65-69&quot;) ~ &quot;65-69&quot;, age %in% c(&quot;70&quot;, &quot;71&quot;, &quot;72&quot;, &quot;73&quot;, &quot;74&quot;, &quot;70-74&quot;) ~ &quot;70-74&quot;, age %in% c(&quot;75&quot;, &quot;76&quot;, &quot;77&quot;, &quot;78&quot;, &quot;79&quot;, &quot;75-79&quot;) ~ &quot;75-79&quot;, age %in% c(&quot;80&quot;, &quot;81&quot;, &quot;82&quot;, &quot;83&quot;, &quot;84&quot;, &quot;80-84&quot;) ~ &quot;80-84&quot;, age %in% c(&quot;85&quot;, &quot;86&quot;, &quot;87&quot;, &quot;88&quot;, &quot;89&quot;, &quot;85-89&quot;) ~ &quot;85-89&quot;, age %in% c(&quot;90&quot;, &quot;91&quot;, &quot;92&quot;, &quot;93&quot;, &quot;94&quot;, &quot;90-94&quot;) ~ &quot;90-94&quot;, age %in% c(&quot;95&quot;, &quot;96&quot;, &quot;97&quot;, &quot;98&quot;, &quot;99&quot;, &quot;95-99&quot;) ~ &quot;95-99&quot;, TRUE ~ &quot;Other&quot;) ) %&gt;% group_by(area_type, area, groups) %&gt;% mutate(group_sum = sum(total, na.rm = FALSE), group_sum = group_sum / 2, difference = total - group_sum) %&gt;% ungroup() %&gt;% filter(age == groups) %&gt;% filter(total != group_sum) head(follow_up) ## # A tibble: 6 x 16 ## age male female total area area_type age_type male_rural female_rural ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 0-4 1 5 6 Mt. Kenya… sub-coun… age-gro… NA NA ## 2 5-9 1 2 3 Mt. Kenya… sub-coun… age-gro… NA NA ## 3 10-14 6 0 6 Mt. Kenya… sub-coun… age-gro… NA NA ## 4 15-19 9 1 10 Mt. Kenya… sub-coun… age-gro… NA NA ## 5 20-24 21 4 25 Mt. Kenya… sub-coun… age-gro… NA NA ## 6 25-29 59 9 68 Mt. Kenya… sub-coun… age-gro… NA NA ## # … with 7 more variables: total_rural &lt;int&gt;, male_urban &lt;int&gt;, ## # female_urban &lt;int&gt;, total_urban &lt;int&gt;, groups &lt;chr&gt;, group_sum &lt;dbl&gt;, ## # difference &lt;dbl&gt; rm(follow_up) Mt. Kenya Forest, Aberdare Forest, Kakamega Forest are all slightly dodgy. I can’t see it in the documentation, but it looks like they have apportioned these between various countries. It’s understandable why they’d do this and it’s probably not a big deal, so I’ll just move on. 7.10.5 Tidy-up Now that we are confident that everything is looking good, we can just convert it to long-format so that it is easy to work with. all &lt;- all %&gt;% rename(male_total = male, female_total = female, total_total = total) %&gt;% pivot_longer(cols = c(male_total, female_total, total_total, male_rural, female_rural, total_rural, male_urban, female_urban, total_urban), names_to = &quot;type&quot;, values_to = &quot;number&quot; ) %&gt;% separate(col = type, into = c(&quot;gender&quot;, &quot;part_of_area&quot;), sep = &quot;_&quot;) %&gt;% select(area, area_type, part_of_area, age, age_type, gender, number) write_csv(all, path = &quot;outputs/data/cleaned_kenya_2019_census.csv&quot;) head(all) ## # A tibble: 6 x 7 ## area area_type part_of_area age age_type gender number ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Mombasa county total Total age-group male 610257 ## 2 Mombasa county total Total age-group female 598046 ## 3 Mombasa county total Total age-group total 1208303 ## 4 Mombasa county rural Total age-group male NA ## 5 Mombasa county rural Total age-group female NA ## 6 Mombasa county rural Total age-group total NA 7.10.6 Make Monica’s dataset The original purpose of all of this was to make a table for Monica. She needed single-year counts, by gender, for the counties. monicas_dataset &lt;- all %&gt;% filter(area_type == &quot;county&quot;) %&gt;% filter(part_of_area == &quot;total&quot;) %&gt;% filter(age_type == &quot;single-year&quot;) %&gt;% select(area, age, gender, number) head(monicas_dataset) ## # A tibble: 6 x 4 ## area age gender number ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 Mombasa 0 male 15111 ## 2 Mombasa 0 female 15009 ## 3 Mombasa 0 total 30120 ## 4 Mombasa 1 male 15805 ## 5 Mombasa 1 female 15308 ## 6 Mombasa 1 total 31113 write_csv(monicas_dataset, &quot;outputs/data/monicas_dataset.csv&quot;) I’ll leave the fancy stats to Monica, but I’ll just make a quick graph of Nairobi. monicas_dataset %&gt;% filter(area == &quot;Nairobi&quot;) %&gt;% ggplot() + geom_col(aes(x = age, y = number, fill = gender), position = &quot;dodge&quot;) + scale_y_continuous(labels = scales::comma) + scale_x_discrete(breaks = c(seq(from = 0, to = 99, by = 5), &quot;100+&quot;)) + theme_classic()+ scale_fill_brewer(palette = &quot;Set1&quot;) + labs(y = &quot;Number&quot;, x = &quot;Age&quot;, fill = &quot;Gender&quot;, title = &quot;Distribution of age and gender in Nairobi in 2019&quot;, caption = &quot;Data source: 2019 Kenya Census&quot;) 7.11 Optical Character Recognition All of the above is predicated on having a PDF that is already ‘digitized.’ But what if it is images? In that case you need to first use Optical Character Recognition (OCR). The go-to package is Tesseract (Ooms 2019c). This is a R wrapper around the Tesseract open-source OCR engine. Let’s see an example with a scan from the first page of Jane Eyre (Figure 7.13). Figure 7.13: Scan of first page of Jane Eyre. # install.packages(&#39;tesseract&#39;) library(tesseract) text &lt;- tesseract::ocr(here::here(&quot;figures/jane_scan.png&quot;), engine = tesseract(&quot;eng&quot;)) cat(text) ## 1 THERE was no possibility of taking a walk that day. We had ## been wandering, indeed, in the leafless shrubbery an hour in ## the morning; but since dinner (Mrs Reed, when there was no com- ## pany, dined early) the cold winter wind had brought with it clouds ## so sombre, and a rain so penetrating, that further out-door exercise ## ## was now out of the question. ## ## I was glad of it: I never liked long walks, especially on chilly ## afternoons: dreadful to me was the coming home in the raw twi- ## light, with nipped fingers and toes, and a heart saddened by the ## chidings of Bessie, the nurse, and humbled by the consciousness of ## my physical inferiority to Eliza, John, and Georgiana Reed. ## ## The said Eliza, John, and Georgiana were now clustered round ## their mama in the drawing-room: she lay reclined on a sofa by the ## fireside, and with her darlings about her (for the time neither quar- ## relling nor crying) looked perfectly happy. Me, she had dispensed ## from joining the group; saying, ‘She regretted to be under the ## necessity of keeping me at a distance; but that until she heard from ## Bessie, and could discover by her own observation that I was ## endeavouring in good earnest to acquire a more sociable and ## child-like disposition, a more attractive and sprightly manner— ## something lighter, franker, more natural as it were—she really ## must exclude me from privileges intended only for contented, ## happy, littie children.’ ## ## ‘What does Bessie say I have done?’ I asked. ## ## ‘Jane, I don’t like cavillers or questioners: besides, there is ## something truly forbidding in a child taking up her elders in that ## manner. Be seated somewhere; and until you can speak pleasantly, ## remain silent.’ ## ## . Bs aT sae] eae ## ## i; AN TCM TAN | Beal | Sees ## a) } ; | i) ## i i 4 | | A ae | i | eee eek? ## ## a an eames yi | bee ## 1 nea elem | | oe pee ## i i ae BC i i Hale ## oul | ec hi ## pan || i re a al! | ## ## ase } Oty 2 RIES ORT Sata ariel ## SEEN BE — =——_ ## 15 7.12 Text Aspects of this section have been previously published. 7.12.1 Introduction Text data is all around us, and in many cases is some of the earliest types of data that we are exposed to. Recent increases in computational power, the development of new methods, and the enormous availability of text, means that there has been a great deal of interest in using text as data. Initial methods tend to focus, essentially, on converting text into numbers and then analysing them using traditional methods. More recent methods have begun to take advantage of the structure that is inherent in text, to draw additional meaning. The difference is perhaps akin to a child who can group similar colors, compared with a child who knows what objects are; although both crocodiles and trees are green, and you can do something with that knowledge, you can do more by knowing that a crocodile could eat you, and a tree probably won’t. In this section we cover a variety of techniques designed to equip you with the basics of using text as data. One of the great things about text data is that it is typically not generated for the purposes of our analysis. That’s great because it removes one of the unobservable variables that we typically have to worry about. The trade-off is that we typically have to do a bunch more work to get it into a form that we can work with. 7.12.2 Getting text data Text as data is an exciting tool to apply. But many guides assume that you already have a nice dataset. Because we’ve focused on workflow in these notes, we know that’s not likely to be true! In this section we will scrape some text from a website. We’ve already seen examples of scraping, but in general those were focused on exploiting tables in the website. Here we’re going to instead focus on paragraphs of text, hence we’ll focus on different html/css tags. We’re going to us the rvest package to make it easier to scrape data. We’re also going to use the purrr package to apply a function to a bunch of different URLs. For those of you with a little bit of programming, this is an alternative to using a for loop. For those of you with a bit of CS, this is a package that adds functional programming to R. library(rvest) library(tidyverse) # Some websites address_to_visit &lt;- c(&quot;https://www.rba.gov.au/monetary-policy/rba-board-minutes/2020/2020-03-03.html&quot;, &quot;https://www.rba.gov.au/monetary-policy/rba-board-minutes/2020/2020-02-04.html&quot;, &quot;https://www.rba.gov.au/monetary-policy/rba-board-minutes/2019/2019-12-03.html&quot;, &quot;https://www.rba.gov.au/monetary-policy/rba-board-minutes/2019/2019-11-05.html&quot;, &quot;https://www.rba.gov.au/monetary-policy/rba-board-minutes/2019/2019-10-01.html&quot;, &quot;https://www.rba.gov.au/monetary-policy/rba-board-minutes/2019/2019-09-03.html&quot; ) # Save names save_name &lt;- address_to_visit %&gt;% str_remove(&quot;https://www.rba.gov.au/monetary-policy/rba-board-minutes/&quot;) %&gt;% str_remove(&quot;.html&quot;) %&gt;% str_remove(&quot;20[:digit:]{2}/&quot;) %&gt;% str_c(&quot;inputs/rba/&quot;, ., &quot;.csv&quot;) Create the function that will visit address_to_visit and save to save_name files. visit_address_and_save_content &lt;- function(name_of_address_to_visit, name_of_file_to_save_as) { # The function takes two inputs name_of_address_to_visit &lt;- address_to_visit[1] name_of_file_to_save_as &lt;- save_name[1] read_html(name_of_address_to_visit) %&gt;% # Go to the website and read the html html_node(&quot;#content&quot;) %&gt;% # Find the content part html_text() %&gt;% # Extract the text of the content part write_lines(name_of_file_to_save_as) # Save as a text file print(paste(&quot;Done with&quot;, name_of_address_to_visit, &quot;at&quot;, Sys.time())) # Helpful so that you know progress when running it on all the records Sys.sleep(sample(30:60, 1)) # Space out each request by somewhere between # 30 and 60 seconds each so that we don&#39;t overwhelm their server } # If there is an error then ignore it and move to the next one visit_address_and_save_content &lt;- safely(visit_address_and_save_content) We now apply that function to our list of URLs. # Walk through the addresses and apply the function to each walk2(address_to_visit, save_name, ~ visit_address_and_save_content(.x, .y)) The result is a bunch of files with saved text data. In this case we used scraping, but there are, of course, many ways. We may be able to use APIs, for instance, In the case of the Airbnb dataset that we examined earlier in the notes. If you are lucky then it may simply be that there is a column that contains text data in your dataset. 7.12.3 Preparing text datasets This section draws on Sharla Gelfand’s blog post, linked in the required readings. As much as I would like to stick with Australian economics and politics examples, I realise that this is probably only of limited interest to most of you. As such, in this section we will consider a dataset of Sephora reviews. Please read Sharla’s blog post (https://sharla.party/post/crying-sephora/) for another take on this dataset. In this section we assume that there is some text data that you have gathered. At this point we need to change it into a form that we can work with. For some applications this will be counts of words. For others it may be some variant of this. The dataset that we are going to use is from Sephora, was scraped by Connie and I originally became aware of it because of Sharla. First let’s read in the data. # This code is taken from https://sharla.party/post/crying-sephora/ library(dplyr) library(jsonlite) library(tidytext) crying &lt;- jsonlite::fromJSON(&quot;https://raw.githubusercontent.com/everestpipkin/datagardens/master/students/khanniie/5_newDataSet/crying_dataset.json&quot;, simplifyDataFrame = TRUE ) crying &lt;- as_tibble(crying[[&quot;reviews&quot;]]) head(crying) ## # A tibble: 6 x 6 ## date product_info$bra… $name $type $url review_body review_title stars ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 29 Ma… Too Faced Bett… Masc… https:… &quot;Now I can c… AWESOME 5 st… ## 2 29 Se… Too Faced Bett… Masc… https:… &quot;This holds … if you&#39;re sa… 5 st… ## 3 23 Ma… Too Faced Bett… Masc… https:… &quot;I just boug… Hate it 1 st… ## 4 15 Au… Too Faced Bett… Masc… https:… &quot;To start of… Nearly perfe… 5 st… ## 5 21 Se… Too Faced Bett… Masc… https:… &quot;This mascar… Amazing!! 5 st… ## 6 30 Ma… Too Faced Bett… Masc… https:… &quot;Let&#39;s talk … Tricky but w… 5 st… ## # … with 1 more variable: userid &lt;dbl&gt; names(crying) ## [1] &quot;date&quot; &quot;product_info&quot; &quot;review_body&quot; &quot;review_title&quot; &quot;stars&quot; ## [6] &quot;userid&quot; We’ll focus on the review_body variable and the number of stars stars that the reviewer gave. Most of them are 5 stars, so we’ll just focus on whether or not the review is five stars. crying &lt;- crying %&gt;% select(review_body, stars) %&gt;% mutate(stars = str_remove(stars, &quot; stars?&quot;), # The question mark at the end means it&#39;l get rid of &#39;star&#39; and &#39;stars&#39;. stars = as.integer(stars) ) %&gt;% mutate(five_stars = if_else(stars == 5, 1, 0)) table(crying$stars) ## ## 1 2 3 4 5 ## 6 2 4 14 79 In this example we are going to split everything into separate words. When we do this it is just searching for a space, and so what other types of elements are going to be considered ‘words?’ crying_by_words &lt;- crying %&gt;% tidytext::unnest_tokens(word, review_body, token = &quot;words&quot;) head(crying_by_words) ## # A tibble: 6 x 3 ## stars five_stars word ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 5 1 now ## 2 5 1 i ## 3 5 1 can ## 4 5 1 cry ## 5 5 1 all ## 6 5 1 i We now want to count the number of times each word is used by each of the star classifications. crying_by_words &lt;- crying_by_words %&gt;% count(stars, word, sort = TRUE) head(crying_by_words) ## # A tibble: 6 x 3 ## stars word n ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 5 i 348 ## 2 5 and 249 ## 3 5 the 239 ## 4 5 it 211 ## 5 5 a 193 ## 6 5 this 178 crying_by_words %&gt;% filter(stars == 1) %&gt;% head() ## # A tibble: 6 x 3 ## stars word n ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 the 39 ## 2 1 i 24 ## 3 1 and 21 ## 4 1 it 21 ## 5 1 to 19 ## 6 1 my 16 So you can see that the most popular word for five star reviews is ‘i,’ and that the most popular word for one star reviews is ‘the.’ At this point, we can use the data to do a whole bunch of different things, but one nice measure to look at is term frequency e.g. in this case how many times is a word used in reviews with a particular star rating. The issue is that there are a lot of words that are commonly used regardless of context. As such, we may also like to look at the inverse document frequency in which we ‘penalise’ words that occur in many particular star ratings. For instance, ‘the’ probably occurs in both one star and five star reviews and so its idf is lower than ‘hate’ which probably only occurs in one star reviews. The term frequency–inverse document frequency (tf-idf) is then the product of these. We can create this value using the bind_tf_idf() function from the tidytext package, and this will create a bunch of new columns, one for each word and star combination. # This code, and the one in the next block, is from Julia Silge: https://juliasilge.com/blog/sherlock-holmes-stm/ crying_by_words_tf_idf &lt;- crying_by_words %&gt;% bind_tf_idf(word, stars, n) %&gt;% arrange(-tf_idf) head(crying_by_words_tf_idf) ## # A tibble: 6 x 6 ## stars word n tf idf tf_idf ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 below 1 0.00826 1.61 0.0133 ## 2 2 boy 1 0.00826 1.61 0.0133 ## 3 2 choice 1 0.00826 1.61 0.0133 ## 4 2 contrary 1 0.00826 1.61 0.0133 ## 5 2 exceptionally 1 0.00826 1.61 0.0133 ## 6 2 migrates 1 0.00826 1.61 0.0133 crying_by_words_tf_idf %&gt;% group_by(stars) %&gt;% top_n(10) %&gt;% ungroup %&gt;% mutate(word = reorder_within(word, tf_idf, stars)) %&gt;% mutate(stars = as_factor(stars)) %&gt;% filter(stars %in% c(1, 5)) %&gt;% ggplot(aes(word, tf_idf, fill = stars)) + geom_col(show.legend = FALSE) + facet_wrap(vars(stars), scales = &quot;free&quot;) + scale_x_reordered() + coord_flip() + labs(x = &quot;Word&quot;, y = &quot;tf-idf&quot;) + theme_minimal() + scale_fill_brewer(palette = &quot;Set1&quot;) There are at least six great coffee shops shown just in this section of map including: Mocan &amp; Green Grout; The Cupping Room; Barrio Collective Coffee; Lonsdale Street Cafe; Two Before Ten; and Red Brick. There are also two coffee shops that I love but that most wouldn’t classify as ‘great’ including: The Street Theatre Cafe; and the CBE Cafe.↩︎ And if you’d like to know more about this then I’d recommend starting a PhD with Monica Alexander.↩︎ "],["hunt-data.html", "Chapter 8 Hunt data 8.1 Experiments and randomised controlled trials 8.2 Case study - Fisher’s tea party 8.3 Case study - Tuskegee Syphilis Study 8.4 Case study - The Oregon Health Insurance Experiment 8.5 Case study - Student Coaching: How Far Can Technology Go? 8.6 Case study - Civic Honesty Around The Globe 8.7 A/B testing 8.8 Case study - Upworthy 8.9 Sampling and survey essentials 8.10 Implementing surveys 8.11 Next steps", " Chapter 8 Hunt data Last updated: 9 March 2021. Required reading Banerjee, Abhijit Vinayak, 2020, ‘Field Experiments and the Practice of Economics,’ American Economic Review, Vol. 110, No. 7, pp. 1937-1951. Berry, Donald, 1989, ‘Comment: Ethics and ECMO,’ Statistical Science, Vol 4, No 4, pp. 306-310. Duflo, Esther, 2020, ‘Field Experiments and the Practice of Policy,’ American Economic Review, Vol. 110, No. 7, pp. 1952-1973 (or watch the speech detailed below). Fisher, Ronald, 1935, The Design of Experiments, pp. 20-29, https://archive.org/details/in.ernet.dli.2015.502684/page/n33/mode/2up. Fry, Hanna, 2020, ‘Experiments on Trial,’ The New Yorker, 2 March, pp. 61-65, https://www.newyorker.com/magazine/2020/03/02/big-tech-is-testing-you. Gertler, Paul, Sebastian Martinez, Patrick Premand, Laura Rawlings, and Christel Vermeersch, Impact Evaluation in Practice, Chapters 3 and 4, https://www.worldbank.org/en/programs/sief-trust-fund/publication/impact-evaluation-in-practice. Hill, Austin Bradford, 1965, ‘The Environment and Disease: Association or Causation?’ Proceedings of the Royal Society of Medicine, 58, 5, 295-300. Kohavi, Ron and Stefan Thomke, 2017, ‘The Surprising Power of Online Experiments,’ Harvard Business Review, September-October, https://hbr.org/2017/09/the-surprising-power-of-online-experiments. Kohavi, Ron, Diane Tang, and Ya Xu, 2020, Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing, Cambridge University Press. (This sounds like a lot, but it’s a light book - it’s more about providing examples of issues to think about.) (Freely available through the U of T library.) Taback, Nathan, 2020, Design of Experiments and Observational Studies, Chapter 8 - Completely Randomized Designs: Comparing More Than Two Treatments, https://scidesign.github.io/designbook/completely-randomized-designs-comparing-more-than-two-treatments.html. Taylor, Sean, Dean Eckles, 2017, ‘Randomized experiments to detect and estimate social influence in networks,’ arXiv, https://arxiv.org/abs/1709.09636v1. Ware, James H., 1989, ‘Investigating Therapies of Potentially Great Benefit: ECMO,’ Statistical Science, Vol 4, No 4, pp. 298-306. Wu, Changbao and Mary E. Thompson, 2020, Sampling Theory and Practice, Springer, Chapters 1-3, and 5 (freely available through the U of T library). Required viewing Ge, Kathy, 2021, ‘Experimentation and product design at Uber,’ Toronto Data Workshop, 4 February, https://youtu.be/UYzXElJTovg. Register, Yim, 2020, ‘Introduction to Sampling and Randomization,’ Online Causal Inference Seminar, 14 November, https://youtu.be/U272FFxG8LE. Xu, Ya, 2020, ‘Causal inference challenges in industry, a perspective from experiences at LinkedIn,’ Online Causal Inference Seminar, 16 July, https://youtu.be/OoKsLAvyIYA. Recommended reading Angrist, Joshua D., and Jörn-Steffen Pischke, 2008, Mostly harmless econometrics: An empiricist’s companion, Princeton University Press, Chapter 2. Banerjee, Abhijit Vinayak, Esther Duflo, Rachel Glennerster, and Dhruva Kothari, 2010, ‘Improving immunisation coverage in rural India: clustered randomised controlled evaluation of immunisation campaigns with and without incentives,’ BMJ, 340, c2220. Beaumont, Jean-François, 2020, ‘Are probability surveys bound to disappear for the production of official statistics?’ Survey Methodology, 46 (1), Statistics Canada, Catalogue No. 12-001-X. Christian, Brian, 2012, ‘The A/B Test: Inside the Technology That’s Changing the Rules of Business,’ Wired, 25 April, https://www.wired.com/2012/04/ff-abtesting/. Dablander, Fabian, 2020, “An Introduction to Causal Inference,” PsyArXiv, 13 February, doi:10.31234/osf.io/b3fkw, https://psyarxiv.com/b3fkw. Deaton, Angus, 2010, ‘Instruments, Randomization, and Learning about Development,’ Journal of Economic Literature, vol. 48, no. 2, pp. 424-455. Duflo, Esther, Rachel Glennerster, and Michael Kremer, 2007, ‘Using Randomization In Development Economics Research: A Toolkit,’ https://economics.mit.edu/files/806. Gordon, Brett R., Florian Zettelmeyer, Neha Bhargava, and Dan Chapsky, 2019, ‘A Comparison of Approaches to Advertising Measurement: Evidence from Big Field Experiments at Facebook,’ Marketing Science, Vol. 38, No. 2, March–April, pp. 193–225. Groves, Robert M., 2011, ‘Three Eras of Survey Research,’ Public Opinion Quarterly, 75 (5), pp. 861–871, https://doi.org/10.1093/poq/nfr057. Hillygus, D. Sunshine, 2011, ‘The evolution of election polling in the United States,’ Public Opinion Quarterly, 75 (5), pp. 962-981. Imai, Kosuke, 2017, Quantitative Social Science: An Introduction, Princeton University Press, Ch 2.3, 2.4, 4.3. Jeffries, Adrianne, Leon Yin, and Surya Mattu, 2020, ‘Swinging the Vote?’ The Markup, 26 February, https://themarkup.org/google-the-giant/2020/02/26/wheres-my-email. Kohavi, Ron, Alex Deng, Brian Frasca, Roger Longbotham, Toby Walker, and Ya Xu. 2012. Trustworthy online controlled experiments: five puzzling outcomes explained. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD ’12). Association for Computing Machinery, New York, NY, USA, 786–794. DOI:https://doi.org/10.1145/2339530.2339653 Landesberg, Eddie, Molly Davies, and Stephanie Yee, 2019, ‘Want to make good business decisions? Learn causality,’ MultiThreaded, Stitchfix blog, 19 December, https://multithreaded.stitchfix.com/blog/2019/12/19/good-marketing-decisions/. Levay, Kevin E., Jeremy Freese, and James N. Druckman, 2016, ‘The demographic and political composition of Mechanical Turk samples,’ Sage Open, 6 (1), 2158244016636433. Lewis, Randall A., and David H. Reiley, 2014 ‘Online ads and offline sales: Measuring the effects of retail advertising via a controlled experiment on Yahoo!’ Quantitative Marketing and Economics, Vol 12, pp. 235–266. Mullinix, Kevin J., Leeper, Thomas J., Druckman, James N. and Freese, Jeremy, 2015, ‘The generalizability of survey experiments,’ Journal of Experimental Political Science, 2 (2), pp. 109-138. Novak, Greg, Sven Schmit, and Dave Spiegel, 2020, Experimentation with resource constraints, 18 November, StitchFix Blog, https://multithreaded.stitchfix.com/blog/2020/11/18/virtual-warehouse/. Prepared for the AAPOR Executive Council by a Task Force operating under the auspices of the AAPOR Standards Committee, with members including:, Reg Baker, Stephen J. Blumberg, J. Michael Brick, Mick P. Couper, Melanie Courtright, J. Michael Dennis, Don Dillman, Martin R. Frankel, Philip Garland, Robert M. Groves, Courtney Kennedy, Jon Krosnick, Paul J. Lavrakas, Sunghee Lee, Michael Link, Linda Piekarski, Kumar Rao, Randall K. Thomas, Dan Zahs, 2010, ‘Research Synthesis: AAPOR Report on Online Panels,’ Public Opinion Quarterly, 74 (4), pp. 711–781, https://doi.org/10.1093/poq/nfq048. Ryan, A. C., A. R. MacKenzie, S. Watkins, and R. Timmis, 2012, ‘World War II contrails: a case study of aviation‐induced cloudiness,’ International journal of climatology, 32, no. 11, pp. 1745-1753. Said, Chris, 2020, ‘Optimizing sample sizes in A/B testing, Part I: General summary,’ 10 January, https://chris-said.io/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-I/. (See also parts 2 and 3). Stolberg, Michael, 2006, ‘Inventing the randomized double-blind trial: the Nuremberg salt test of 1835,’ Journal of the Royal Society of Medicine, 99, no. 12, pp. 642-643. Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel, 2019, popular science background, https://www.nobelprize.org/uploads/2019/10/popular-economicsciencesprize2019-2.pdf. Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel, 2019, scientific background, https://www.nobelprize.org/uploads/2019/10/advanced-economicsciencesprize2019.pdf. Taddy, Matt, 2019, Business Data Science, Chapter 5. Urban, Steve, Rangarajan Sreenivasan, and Vineet Kannan, 2016, ‘It’s All A/Bout Testing: The Netflix Experimentation Platform,’ Netflix Technology Blog, 29 April, https://netflixtechblog.com/its-all-a-bout-testing-the-netflix-experimentation-platform-4e1ca458c15. VWO, ‘A/B Testing Guide,’ https://vwo.com/ab-testing/. Yeager, David S., Jon A. Krosnick, LinChiat Chang, Harold S. Javitz, Matthew S. Levendusky, Alberto Simpser, Rui Wang, 2011, ‘Comparing the Accuracy of RDD Telephone Surveys and Internet Surveys Conducted with Probability and Non-Probability Samples,’ Public Opinion Quarterly, 75 (4), pp. 709–747, https://doi.org/10.1093/poq/nfr020. Yin, Xuan and Ercan Yildiz, 2020, ‘The Causal Analysis of Cannibalization in Online Products,’ Code as Craft, Etsy blog, 24 February, https://codeascraft.com/2020/02/24/the-causal-analysis-of-cannibalization-in-online-products/. Recommended listening Galef, Julia, 2020, ‘Episode 246: Deaths of despair / Effective altruism (Angus Deaton),’ Rationally Speaking, from 35:30 through to the end, available at: http://rationallyspeakingpodcast.org/show/episode-246-deaths-of-despair-effective-altruism-angus-deato.html. Recommended viewing Duflo, Esther, 2020, ‘Inteview with Esther Duflo,’ 12 October, Online Causal Inference Seminar, https://youtu.be/WWW9q3oMYxU. Duflo, Esther, 2019, ‘Nobel Prize Lecture,’ 8 December 2019, Stockholm: https://www.nobelprize.org/prizes/economic-sciences/2019/duflo/lecture/. Tipton, Elizabeth, 2020, ‘Will this Intervention Work in this Population? Designing Randomized Trials for Generalization,’ Online Causal Inference Seminar, 14 April, https://youtu.be/HYP32wzEZMA. Key concepts/skills/etc Treatment and control groups. Internal and external validity. Average treatment effect. Generating simulated datasets. Defining populations, frames and samples. Distinguishing probability and non-probability sampling Distinguishing strata and clusters. Key libraries broom ggplot2 tidyverse Key functions/etc aov() rnorm() sample() t.test() Quiz In your own words, what is the role of randomisation in constructing a counterfactual (write two or three paragraphs)? What is external validity (pick one)? Findings from an experiment hold in that setting. Findings from an experiment hold outside that setting. Findings from an experiment that has been repeated many times. Findings from an experiment for which code and data are available. What is internal validity (pick one)? Findings from an experiment hold in that setting. Findings from an experiment hold outside that setting. Findings from an experiment that has been repeated many times. Findings from an experiment for which code and data are available. If we have a dataset named ‘netflix_data,’ with the columns ‘person’ and ‘tv_show’ and ‘hours,’ (person is a character class uniqueID for every person, tv_show is a character class name of a tv show, and hours is double expressing the number of hours that person watched that tv show). Could you please write some code that would randomly assign people into one of two groups? The data looks like this: library(tidyverse) netflix_data &lt;- tibble(person = c(&quot;Rohan&quot;, &quot;Rohan&quot;, &quot;Monica&quot;, &quot;Monica&quot;, &quot;Monica&quot;, &quot;Patricia&quot;, &quot;Patricia&quot;, &quot;Helen&quot;), tv_show = c(&quot;Broadchurch&quot;, &quot;Duty-Shame&quot;, &quot;Broadchurch&quot;, &quot;Duty-Shame&quot;, &quot;Shetland&quot;, &quot;Broadchurch&quot;, &quot;Shetland&quot;, &quot;Duty-Shame&quot;), hours = c(6.8, 8.0, 0.8, 9.2, 3.2, 4.0, 0.2, 10.2) ) In the context of randomisation, what does stratification mean to you (write a paragraph or two)? How could you check that your randomisation had been done appropriately (write two or three paragraphs)? Identify three companies that conduct A/B testing commercially and write a short paper about how they work and the trade-offs of each. Are there any notable Toronto-based or Canadian companies? Why do you think this might be the case? Pretend that you work as a junior analyst for a large consulting firm. Further, pretend that your consulting firm has taken a contract to put together a facial recognition model for the Canada Border Services Agency’s Inland Enforcement branch. Taking a page or two, please discuss your thoughts on this matter. What would you do and why? What are some types of probability sampling, and in what circumstances might you want to implement them (write two or three pages)? There have been some substantial political polling ‘misses’ in recent years (Trump and Brexit come to mind). To what extent do you think non-response bias was the cause of this (write a page or two, being sure to ground your writing with citations)? What is an estimate (pick one)? A rule for calculating an estimate of a given quantity based on observed data. The quantity of interest. The result. Unknown numbers that determine a statistical model. What is an estimator (pick one)? A rule for calculating an estimate of a given quantity based on observed data. The quantity of interest. The result. Unknown numbers that determine a statistical model. What is an estimand (pick one)? A rule for calculating an estimate of a given quantity based on observed data. The quantity of interest. The result. Unknown numbers that determine a statistical model. What is a parameter (pick one)? A rule for calculating an estimate of a given quantity based on observed data. The quantity of interest. The result. Unknown numbers that determine a statistical model. It seems like a lot of businesses have closed in downtown Toronto since the pandemic. To investigate this, I decide to walk along some blocks downtown and count the number of businesses that are closed and open. To decide which blocks to walk, I open a map of Toronto, start at the lake, and then pick every 10th street. This type of sampling is (select all)? Cluster sampling. Systematic sampling. Stratified sampling. Simple random sampling. Convenience sampling. Please name some reasons why you may wish to use cluster sampling (select all)? Balance in responses. Administrative convenience. Efficiency in terms of money. Underlying systematic concerns. Estimation of sub-populations. Please consider Beaumont, 2020, ‘Are probability surveys bound to disappear for the production of official statistics?’ With reference to that paper, do you think that probability surveys will disappear, and why or why not (please write a paragraph or two)? Ware (1989, 298) mentions ‘a randomized play the winner design.’ What is it? Ware (1989, 299) mentions ‘adaptive randomization.’ What is it, in your own words? Ware (1989, 299) mentions ‘randomized-consent.’ He continues that it was ‘attractive in this setting because a standard approach to informed consent would require that parents of infants near death be approached to give informed consent for an invasive surgical procedure that would then, in some instances, not be administered. Those familiar with the agonizing experience of having a child in a neonatal intensive care unit can appreciate that the process of obtaining informed consent would be both frightening and stressful to parents.’ To what extent do you agree with this position, especially given, as Ware (1989), p. 305, mentions ‘the need to withhold information about the study from parents of infants receiving CMT?’ Ware (1989, 300) mentions ‘equipoise.’ In your own words, please define and discuss it, using an example from your own experience. What is power (in a statistical context)? 8.1 Experiments and randomised controlled trials 8.1.1 Introduction First a note on Ronald Fisher and Francis Galton. Fisher and Galton are the intellectual grandfathers of much of the work that we cover. In some cases it is directly their work, in other cases it is work that built on their contributions. Both of these men believed in eugenics, amongst other things that are generally reprehensible. This chapter is about experiments. This is a situation in which we can explicitly control and vary some aspects. The advantage of this is that identification should be clear. There is a treatment group that is treated and a control group that is not. These are randomly split. And so if they end up different then it must be because of the treatment. Unfortunately, life is rarely so smooth. Arguing about how similar the treatment and control groups were tends to carry on indefinitely, because our ability to speak to internal validity affects our ability to speak to external validity. It’s also important to note that the statistics of this were designed in agricultural settings ‘does fertilizer work?’ etc. In those settings you can more easily divide a field into ‘treated’ and ‘non-treated,’ and the magnitude of the effect is large. In general, these same statistical approaches are still used today (especially in the social sciences) but often inappropriately. If you hear someone talking about ‘having enough power’ and similar phrases, then it’s not necessarily that they’re not right, but it usually pays to take a step back and really think about what is being done and whether they really know what they’re doing. 8.1.2 Motivation and notation Never forget: if your sampling is in any way non-representative, your observe[d] data is not sufficient for population estimates. You must deal with design, sampling issues, data quality, and misclassification. Otherwise you’ll just be wrong. Dan Simpson, 30 January 2020. When Monica and I moved to San Francisco, the Giants immediately won the baseball, and the Warriors began a historic streak. We moved to Chicago and the Cubs won the baseball for the first time in a hundred years. We then moved to Massachusetts, and the Patriots won the Super Bowl again and again and again. Finally, we moved to Toronto, and the Raptors won the basketball. Should a city pay us to live there or could their funds be better spent elsewhere? One way to get at the answer would be to run an experiment. Make a list of the North American cities with major sports teams, and then roll a dice and send us to live there for a year. If we had enough lifetimes, then we could work it out. The fundamental issue is that we cannot both live in a city and not live in a city. Experiments and randomised controlled trials are circumstances in which we try to randomly allocate some treatment, so as to have a belief that everything else was constant (or at least ignorable). In the words of Hernan and Robins (2020, 3) an action, \\(A\\), is also known ‘as an intervention, an exposure, or a treatment.’ I’ll typically use ‘treated/control’ language, reflecting whether an action was imposed or not. That treatment random variable will typically be binary, that is 0 or 1, ‘treated’ or ‘not treated/control/comparison.’ We’ll then typically have some outcome random variable, \\(Y\\), which will typically be binary, able to be made binary, or continuous, although we’ll touch on other options. An example of a binary outcome could be vote choice - ‘Conservative’ vs ‘Not Conservative’ - noticing there that I grouped all the other parties into simply ‘Not Conservative’ to force the binary outcome. Further following Hernan and Robins (2020, 4), but in the notation of Gertler et al. (2016, 48) we describe a treatment as ‘causal’ when \\((Y|a=0)\\neq (Y|a=1)\\). As discussed above, the fundamental problem of causal inference is that we cannot both treat and control the one individual. So when we want to know the effect of the treatment, we need to compare it with the counterfactual, which is what would have happened if the individual were not treated. So causal inference turns out to be fundamentally a missing data problem.6 To quote from Gertler et al. (2016, 48), in the context of evaluating income in response to an intervention program: To put it another way, we would like to measure income at the same point in time for the same unit of observation (a person, in this case), but in two different states of the world. If it were possible to do this, we would be observing how much income the same individual would have had at the same point in time both with and without the program, so that the only possible explanation for any difference in that person’s income would be the program. By comparing the same individual with herself at the same moment, we would have managed to eliminate any outside factors that might also have explained the difference in outcomes. We could then be confident that the relationship between the vocational training program and the change in income is causal… [A] unit either participated in the program or did not participate. The unit cannot be observed simultaneously in two different states (in other words, with and without the program). As we cannot compared treatment and control in one particular individual, we instead compare the average of two groups - all those treated and all those not. We are looking to estimate the counterfactual. We usually consider a default that there’s no effect and we require evidence for us to change our mind. As we’re interested in what is happening in groups, we turn to expectations, and notions of probability to express ourselves. Hence, we’ll make claims that talk, on average. Maybe wearing fun socks really does make you have a lucky day, but on average across the population, it’s probably not the case.7 It’s worth pointing out that we don’t just have to be interested in the average effect. We may consider the median, or variance, or whatever. Nonetheless, if we were interested in the average effect, then one way to proceed would be to divide the dataset into two - treated and not treated - have an effect column of 0s and 1s, sum the column and divide it by the length of the column, and then look at the ratio. This would be an estimator, which is a way of putting together a guess of something of interest. The estimand is the thing of interest, in this case the average effect, and the estimate is whatever our guess turns out to be. To give another example, following Gelman, Hill, and Vehtari (2020): An estimand, or quantity of interest, is some summary of parameters or data that somebody is interested in estimating. For example, in the regression model, \\(y = a + bx + \\epsilon\\), the parameters \\(a\\) and \\(b\\) might be of interest…. We use the data to construct estimates of parameters and other quantities of interest. More broadly, Cunningham (2021) defines causal inference as ‘…the leveraging of theory and deep knowledge of institutional details to estimate the impact of events and choices on a given outcome of interest.’ In the previous chapter we discussed gathering data which we observed about the world. In this chapter we are going to be more active. Cunningham (2021) says that experimental data ‘is collected in something akin to a laboratory environment. In a traditional experiment, the researcher participates actively in the process being recorded.’ That is, if we want to use this data then as researchers we have to go out and hunt it, if you like. 8.1.3 Randomised sampling Correlation can be enough in some settings, but in order to be able to make forecasts when things change and the circumstances are slightly different we need to understand causation. The key is the counterfactual - what would have happened in the absence of the treatment. Ideally we could keep everything else constant, randomly divide the world into two groups, and then treat one and not the other. Then we can be pretty confident that any difference between the two groups is due to that treatment. The reason for this is that if we have some population and we randomly select two groups from it, then our two groups (so long as they are both big enough) should have the same characteristics as the population. Randomised controlled trials (RCTs) and A/B testing attempts to get us as close to this ‘gold standard’ as we can hope. RCTs are often described as the ‘gold standard,’ for instance by Athey and Imbens (2017). In doing so, we’re not saying that RCTs are perfect, just that they’re generally better than most of the other options. There is plenty that is wrong with RCTs. Remember that our challenge is (Gertler et al. 2016, 51–52): …to identify a treatment group and a comparison group that are statistically identical, on average, in the absence of the program. If the two groups are identical, with the sole exception that one group participates in the program and the other does not, then we can be sure that any difference in outcomes must be due to the program. Finding such comparison groups is the crux of any impact evaluation, regardless of what type of program is being evaluated. Simply put, without a comparison group that yields an accurate estimate of the counterfactual, the true impact of a program cannot be established. We might be worried about underlying trends (the issues with before/after comparison), or selection bias (the issue with self-selection), either of which would result in biased estimators. Our solution is randomisation. To get started, let’s generate a simulated dataset and then sample from it. In general, this is a good way to approach problems: generate a simulated dataset; do your analysis on the simulated dataset; and take your analysis to the real dataset. The reason this is a good approach is that you know roughly what the outcomes should be in step 2, whereas if you go directly to the real dataset then you don’t know if unexpected outcomes are likely due to your own analysis errors, or actual results. The first time you generate a simulated dataset it will take a while, but after a bit of practice you’ll get good at it. There are also packages that can help, including DeclareDesign (Blair et al. 2019) and survey (Lumley 2020). Another good reason it’s useful to take this approach of simulation is that when you’re working in teams the analysis can get started before the data collection and cleaning is completed. That simulation will also help the collection and cleaning team think about tests they should run on their data. library(tidyverse) set.seed(853) # Construct a population so that 25 per cent of people like blue and 75 per cent # like white. population &lt;- tibble(person = c(1:10000), favourite_color = sample(x = c(&quot;Blue&quot;, &quot;White&quot;), size = 10000, replace = TRUE, prob = c(0.25, 0.75)), supports_the_leafs = sample(x = c(&quot;Yes&quot;, &quot;No&quot;), size = 10000, replace = TRUE, prob = c(0.80, 0.20)), ) %&gt;% mutate(in_frame = sample(x = c(0:1), size = 10000, replace = TRUE)) %&gt;% mutate(group = sample(x = c(1:10), size = 10000, replace = TRUE)) %&gt;% mutate(group = ifelse(in_frame == 1, group, NA)) We’ll get more into this terminology later, but the sampling frame is subset of the population that can actually be sampled, for instance they are listed somewhere. For instance, Lauren Kennedy likes to use the analogy of a city’s population, and the phonebook - almost everyone is in there (or at least they used to be), so the population and the sampling frame are almost the same, but they are not. Now look at the mean for two groups drawn out of the sampling frame. population %&gt;% filter(in_frame == 1) %&gt;% filter(group %in% c(1, 2)) %&gt;% group_by(group, favourite_color) %&gt;% count() ## # A tibble: 4 x 3 ## # Groups: group, favourite_color [4] ## group favourite_color n ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 Blue 114 ## 2 1 White 420 ## 3 2 Blue 105 ## 4 2 White 369 We are probably convinced by looking at it, but to formally test if there is a difference in the two samples, we can use a t-test. library(broom) population &lt;- population %&gt;% mutate(color_as_integer = case_when( favourite_color == &quot;White&quot; ~ 0, favourite_color == &quot;Blue&quot; ~ 1, TRUE ~ 999 )) group_1 &lt;- population %&gt;% filter(group == 1) %&gt;% select(color_as_integer) %&gt;% as.vector() %&gt;% unlist() group_2 &lt;- population %&gt;% filter(group == 2) %&gt;% select(color_as_integer) %&gt;% unlist() t.test(group_1, group_2) ## ## Welch Two Sample t-test ## ## data: group_1 and group_2 ## t = -0.30825, df = 988.57, p-value = 0.758 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.05919338 0.04312170 ## sample estimates: ## mean of x mean of y ## 0.2134831 0.2215190 # We could also use the tidy function in the broom package. tidy(t.test(group_1, group_2)) ## # A tibble: 1 x 10 ## estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.00804 0.213 0.222 -0.308 0.758 989. -0.0592 0.0431 ## # … with 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt; If properly done then not only will we get a ‘representative’ share of people with the favourite color blue, but we should also get a representative share of people who support the Maple Leafs. Why should that happen when we haven’t randomised on these variables? Let’s start by looking at our dataset. population %&gt;% filter(in_frame == 1) %&gt;% filter(group %in% c(1, 2)) %&gt;% group_by(group, supports_the_leafs) %&gt;% count() ## # A tibble: 4 x 3 ## # Groups: group, supports_the_leafs [4] ## group supports_the_leafs n ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 No 102 ## 2 1 Yes 432 ## 3 2 No 81 ## 4 2 Yes 393 This is very exciting. We have a representative share on ‘unobservables’ (in this case we do ‘observe’ them - to illustrate the point - but we didn’t select on them). We get this because they were correlated. But it will breakdown in a number of ways that we will discuss. It also assumes large enough groups - if we sampled in Toronto are we likely to get a ‘representative’ share of people who support the Canadiens? What about F.C. Hansa Rostock? If we want to check that the two groups are the same then what can we do? Exactly what we did above - just check if we can identify a difference between the two groups based on observables (we looked at the mean, but we could look at other aspects as well). 8.1.4 ANOVA ‘I refuse to teach anova.’ Statistics professor who prefers to remain anonymous. Analysis of Variation (ANOVA) was introduced by Fisher while he was working on statistical problems in agriculture. To steal Darren L Dahly’s ‘favorite joke of all time’ (Dahly 2020): Q: “What’s the difference between agricultural and medical research?” A: “The former isn’t conducted by farmers.” We need to cover ANOVA because of its importance historically, but in general you probably shouldn’t actually use ANOVA day-to-day. There’s nothing wrong with it, in the right circumstances, it’s more just that it is a hundred years old and the number of modern use-case where it’s still your best-bet is pretty small. In any case, typically, the null is that all of the groups are from the same distribution. We can run ANOVA with the function built into R - aov(). just_two_groups &lt;- population %&gt;% filter(in_frame == 1) %&gt;% filter(group %in% c(1, 2)) aov(group ~ favourite_color, data = just_two_groups) %&gt;% tidy() ## # A tibble: 2 x 6 ## term df sumsq meansq statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 favourite_color 1 0.0238 0.0238 0.0952 0.758 ## 2 Residuals 1006 251. 0.250 NA NA In this case, we fail to reject the null that the samples are the same. This all said, it’s just linear regression. So I’m not sure why it got a fancy name. lm(group ~ favourite_color, data = just_two_groups) %&gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 1.48 0.0338 43.8 1.67e-235 ## 2 favourite_colorWhite -0.0118 0.0382 -0.308 7.58e- 1 My favourite discussion of ANOVA is Taback (2020Chapter 8). 8.1.5 Treatment and control If the treated and control groups are the same in all ways and remain that way, then we have internal validity, which is to say that our control will work as a counterfactual and our results can speak to a difference between these groups in that study. In the words of Gertler et al. (2016, 71): Internal validity means that the estimated impact of the program is net of all other potential confounding factors—or, in other words, that the comparison group provides an accurate estimate of the counterfactual, so that we are estimating the true impact of the program. If the group to which we applied our randomisation were representative of the broader population, and the experimental set-up were fairly similar to outside conditions, then we further have external validity. That means that the difference that we find does not just apply in our own experiment, but also in the broader population. Again, in the words of Gertler et al. (2016, 73): External validity means that the evaluation sample accurately represents the population of eligible units. The results of the evaluation can then be generalized to the population of eligible units. We use random sampling to ensure that the evaluation sample accurately reflects the population of eligible units so that impacts identified in the evaluation sample can be extrapolated to the population. But this means we need randomisation twice. How does this trade-off happen and to what extent does it matter? As such, we are interested in the effect of being ‘treated.’ This may be that we charge different prices (continuous treatment variable), or that we compare different colours on a website (discrete treatment variable, and a staple of A/B testing). If we consider just discrete treatments (so that we can use dummy variables) then need to make sure that all of the groups are otherwise the same. How can we do this? One way is to ignore the treatment variable and to examine all other variables - can you detect a difference between the groups based on any other variables? In the website example, are there a similar number of: PC/Mac users? Safari/Chrome/Firefox/other users? Mobile/desktop users? Users from certain locations? These are all threats to the validity of our claims. But if done properly, that is if the treatment is truly independent, then we can estimate an ‘average treatment effect,’ which in a binary treatment variable setting is: \\[\\mbox{ATE} = \\mbox{E}[y|d=1] - \\mbox{E}[y|d=0].\\] That is, the difference between the treated group, \\(d = 1\\), and the control group, \\(d = 0\\), when measured by the expected value of some outcome variable, \\(y\\). So the mean causal effect is simply the difference between the two expectations! Let’s again get stuck into some code. First we need to generate some data. set.seed(853) example_data &lt;- tibble(person = c(1:1000), treatment = sample(x = 0:1, size = 1000, replace = TRUE) ) # We want to make the outcome slightly more likely if they were treated than if not. example_data &lt;- example_data %&gt;% rowwise() %&gt;% mutate(outcome = if_else(treatment == 0, rnorm(n = 1, mean = 5, sd = 1), rnorm(n = 1, mean = 6, sd = 1) ) ) example_data$treatment &lt;- as.factor(example_data$treatment) example_data %&gt;% ggplot(aes(x = outcome, fill = treatment)) + geom_histogram(position = &quot;dodge&quot;, binwidth = 0.2) + theme_minimal() + labs(x = &quot;Outcome&quot;, y = &quot;Number of people&quot;, fill = &quot;Person was treated&quot;) + scale_fill_brewer(palette = &quot;Set1&quot;) example_regression &lt;- lm(outcome ~ treatment, data = example_data) tidy(example_regression) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 5.00 0.0430 116. 0 ## 2 treatment1 1.01 0.0625 16.1 5.14e-52 But then reality happens. Your experiment cannot run for too long otherwise people may be treated many times, or become inured to the treatment, but it cannot be too short otherwise you can’t measure longer term outcomes. You cannot have a ‘representative’ sample on every cross-tab, but if not then the treatment and control will be different. Practical difficulties may make it difficult to follow up with certain groups. Questions to ask (if they haven’t been answered already) include: How are the participants being selected into the frame for consideration? How are they being selected for treatment? We would hope this is a lottery, but this term is applied to a variety of situations. Additionally, early ‘success’ can lead to pressure to treat everyone. How is treatment being assessed? To what extent is random allocation ethical and fair? Some argue that shortages mean it is reasonable to randomly allocate, but that may depend on how linear the benefits are. It may also be difficult to establish boundaries. If we only want to include people in Ontario then that may be clear, but what about ‘students’ in Ontario - who is a student, and who is making the decision? Bias and other issues are not the end of the world. But you need to think about it carefully. In the famous example, Abraham Wald was given data on the planes that came back to Britain after being shot at in WW2. The question is where to place the armour. One option is to place it over the bullet holes. Wald recognised that there is a selection effect here - these are the planes that made it back - they didn’t need the armour, but instead we should put the armour where there were no bullet holes. To consider an example that may be closer to home - how would the results of a survey differ if I only asked students who completed this course what was difficult about it and not those who dropped out? While, as Dan suggests, we should work to try to make the dataset as good as possible, it may be possible to use the model to control for some of the bias. If there is a variable that is correlated with say, attrition, then we can add it to the model. Either by itself, or as an interaction. What if there is a correlation between the individuals? For instance, what if there were some ‘hidden variable’ that we didn’t know about, such as province, and it turned out that people from the same province were similar? In that case we could use ‘wider’ standard errors. But a better way to deal with this may be to change the experiment. For instance, we discussed stratified sampling - perhaps we should stratify by province? How would we implement this? And of course, these days we’d not really use a 100-year-old method but would instead use Bayes-based approaches. 8.2 Case study - Fisher’s tea party Figure 8.1: Afternoon Tea Party (1890–1891), by Mary Cassatt (American, 1844-1926), as downloaded from https://artvee.com/dl/afternoon-tea-party. Fisher (see note above) introduced a, now, famous example of an experiment designed to see if a person can distinguish between a cup of tea when the milk was added first, or last.8 From Fisher (1935, 13): A lady declares that by tasting a cup of tea made with milk she can discriminate whether the milk or the tea infusion was first added to the cup. We will consider the problem of designing an experiment by means of which this assertion can be tested. Fisher continues: Our experiment consists in mixing eight cups of tea, four in one way and four in the other, and presenting them to the subject for judgment in a random order. The subject has been told in advance of what the test will consist, namely that she will be asked to taste eight cups, that these shall be four of each kind, and that they shall be presented to her in a random order, that is in an order not determined arbitrarily by human choice, but by the actual manipulation of the physical apparatus used in games of chance, cards, dice, roulettes, etc., or, more expeditiously, from a published collection of random sampling-numbers purporting to give the actual results of such manipulation. Her task is to divide the 8 cups into two sets of 4, agreeing, if possible, with the treatments received. To summarize, the set-up is: Eight randomly ordered cups of tea. Four had tea put in first. Four had milk put in first. The person has to choose the four that are the same. The person knows it’s an experiment. We’ll now try this experiment. So brew some tea, grab eight cups, and pour eight cups of tea for a friend that you’re isolating with9 - four where you put the milk in first and four where you put the milk in last. Make sure you use the same amount of tea and milk in each! Don’t forget to randomise the order, possibly even using the following code: sample(c(1:8), size = 8, replace = FALSE) ## [1] 3 7 6 4 1 8 2 5 Then have your friend guess which four you put milk in first and which four you put milk in last! To decide if the person’s choices were likely to have occurred at random or not, we need to think about the probability of this happening by chance. First count the number of successes out of the four that were chosen. Fisher (1935, 14) claims there are: \\({8 \\choose 4} = \\frac{8!}{4!(8-4)!}=70\\) possible outcomes. By chance, there are two ways for the person to be perfectly correct (because we are only asking them to be grouped): correctly identify all the ones that were milk-first (one outcome out of 70) or correctly identify all the ones that were tea-first (one outcome out of 70), so the chance of that is \\(2/70 \\approx 0.028\\). Now, as Fisher (1935, 15) says, ‘[i]t is open to the experimenter to be more or less exacting in respect of the smallness of the probability he would require before he would be willing to admit that his observations have demonstrated a positive result.’ You need to decide what evidence it takes for you to be convinced. If there’s no possible evidence that will dissuade you from your view (that there is no difference between milk-first and tea-first) then what is the point of doing an experiment? In any case, if the null is that they can’t distinguish, but they correctly separate them all, then at the five-per-cent level, we reject the null. What if they miss one? Similarly, by chance there are 16 ways for a person to be ‘off-by-one.’ Either they think there was one that was milk-first when it was tea-first - there are, \\({4 \\choose 1}\\), four ways this could happen - or they think there was one that was tea-first when it was milk-first - again, there are, \\({4 \\choose 1}\\), four ways this could happen. But these outcomes are independent, so the probability is \\(\\frac{4\\times 4}{70} \\approx 0.228\\). And so on. So, we fail to reject the null. Finally, an aside on this magical ‘5 per cent.’ Fisher himself describes this as merely ‘usual and convenient’ (Fisher 1935, 15). Fisher (1935, 16) continues: In order to assert that a natural phenomenon is experimentally demonstrable we need, not an isolated record, but a reliable method of procedure. In relation to the test of significance, we may say that a phenomenon is experimentally demonstrable when we know how to conduct an experiment which will rarely fail to give us a statistically significant result. At the start of these notes, I said that Fisher held views that we would consider reprehensible today. My guess is, were he around today, he would think our use of p-values as discrediting. Do not just go searching for meaning in constellations of stars. Thoroughly interrogate your data and think precisely about the statistical methods you are applying. For conclusions that you want to hold up in the long-run, aim to use as simple, and as understandable, statistical methods as you can. Ensure that you can explain and justify your statistical decisions without recourse to astrology. Source: https://xkcd.com/882/ Figure 8.2: ‘The triumph of wisdom over fortune’ by Otto van Veen (Flemish, 1556 - 1629), as downloaded from https://artvee.com/dl/the-triumph-of-wisdom-over-fortune. 8.3 Case study - Tuskegee Syphilis Study The Tuskegee Syphilis Study is an infamous medical trial in which Black Americans with syphilis (and a ‘control group’ without) were not given appropriate treatment, nor even told they had syphilis, well after standard syphilis treatments were established in the mid-1940s (Alsan and Wanamaker 2018). The study began in 1932 when poor Black Americans in the South were identified and offered compensation including ‘hot meals, the guise of treatment, and burial payments’ (Alsan and Wanamaker 2018). The men were not treated for syphilis. Further, and this is almost unbelievable, some of the men were drafted, told they had syphilis, and ordered to get treatment. This treatment was blocked. By the time the study was stopped, ‘the majority of the study’s victims were deceased, many from syphilis-related causes.’ (Alsan and Wanamaker 2018). The study continued through to 1972, only stopping when it was leaked and published in newspapers. In response the US established requirements for Institutional Review Boards and President Clinton made a formal apology in 1997. Brandt (1978) as quoted by Alsan and Wanamaker (2018) says ‘“In retrospect the Tuskegee Study revealed more about the pathology of racism than the pathology of syphilis; more about the nature of scientific inquiry than the nature of the disease process…. The degree of deception and the damages have been severely underestimated.”’ On the Tuskegee Syphilis Study Professor Monica Alexander says: While it may be illegal to do this exact research these days, it doesn’t mean that unethical research doesn’t still happen, and we see it all the time in ML and health. Just because you can’t explicitly discriminate when you design experiments, doesn’t mean you can’t implicitly discriminate. For an example of this, start with Obermeyer et al. (2019): Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts. 8.4 Case study - The Oregon Health Insurance Experiment The Oregon Health Insurance Experiment involved 74,922 adults in Oregon from 2008 to 2010. The opportunity to apply for health insurance was randomly allocated and then health and earnings evaluated. It was found that (Finkelstein et al. 2012): In the year after random assignment, the treatment group selected by the lottery was about 25 percentage points more likely to have insurance than the control group that was not selected. We find that in this first year, the treatment group had substantively and statistically significantly higher health care utilization (including primary and preventive care as well as hospitalizations), lower out-of-pocket medical expenditures and medical debt (including fewer bills sent to collection), and better self-reported physical and mental health than the control group. A lottery was used to determine which of the 89,824 individuals who signed up would be allowed to apply for Medicaid. This random allocation of insurance allowed the researchers to understand the effect of health insurance. It’s not usually possible to compare those with and without insurance because the type of people that sign up to get health insurance differ to those who don’t - that decision is ‘confounded’ with other variables. They use administrative data, such as hospital discharge data, credit reports that were matched to 68.5 per cent of lottery participants, and mortality records, which will be uncommon. Interestingly this collection of data is actually fairly restrained and so they included a survey conducted via mail. Turning to external validity, the authors restrain themselves and say (Finkelstein et al. 2012): Our estimates of the impact of public health insurance apply to able-bodied uninsured adults below 100 percent of poverty who express interest in insurance coverage. This is a population of considerable policy interest. A lottery was used to allocate 10,000 places in the state-run Medicaid. A lottery was judged fair because ‘the state (correctly) anticipated that the demand for the program among eligible individuals would far exceed the 10,000 available new enrollment slots’ (Finkelstein et al. 2012). People had a month to sign up to enter the draw. The draws were conducted over a six-month period and those who were selected had the opportunity to sign up. 35,169 individuals were selected (the household of those who actually won the draw was given the opportunity) but only 30 per cent of them completed the paperwork and were eligible (typically they earned too much). The insurance lasted indefinitely. The model they consider is (Finkelstein et al. 2012): \\[\\begin{equation} y_{ihj} = \\beta_0 + \\beta_1\\mbox{Lottery} + X_{ih}\\beta+2 + V_{ih}\\beta_3 + \\epsilon_{ihj} \\tag{8.1} \\end{equation}\\] Equation (8.1) explains various \\(j\\) outcomes (such as health) for an individual \\(i\\) in household \\(h\\) as a function of an indicator variable as to whether household \\(h\\) was selected by the lottery. Hence, ‘(t)he coefficient on Lottery, \\(\\beta_1\\), is the main coefficient of interest, and gives the average difference in (adjusted) means between the treatment group (the lottery winners) and the control group (those not selected by the lottery).’ To complete the specification of Equation (8.1), \\(X_{ih}\\) is a set of variables that are correlated with the probability of being treated. These adjust for that impact to a certain extent. An example of that is the number of individuals in a household. And finally, \\(V_{ih}\\) is a set of variables that are not correlated with the lottery. These variables include demographics, hospital discharge and lottery draw. There is a wide range of literature related to this intervention. More papers are available here. 8.5 Case study - Student Coaching: How Far Can Technology Go? There is a general concern about students dropping out of university before they finish their degree. If you work one-on-one with a student then this addresses the issue. But that doesn’t scale. The point of this experiment was to see if technology-based options could be more efficient. The focus was the University of Toronto, and in particular first-year economics courses in Fall 2015. The intervention was administered to students as part of an economics class. Students received 2 per cent of their grade for completing the exercise. The specific exercise depended on the group of the student. The intervention involved three treatments as well as a control group that was just given a Big Five personality traits test. Additional information that was obtained included ‘the highest level of education obtained by students’ parents, the amount of education they expect to obtain, whether they are first-year or international students, and their work and study time plans for the upcoming year.’ (Oreopoulos and Petronijevic 2018, 6). The treatments were (Oreopoulos and Petronijevic 2018, 4): ‘[A] one-time, online exercise completed during the first two weeks of class in the fall.’ This exercise was ‘designed to get them thinking about the future they envision and the steps they could take in the upcoming year at U of T to help make that future a reality. They were told that the exercise was designed for their benefit and to take their time while completing it. The online module lasted approximately 60 to 90 minutes and led students through a series of writing exercises in which they wrote about their ideal futures, both at work and at home, what they would like to accomplish in the current year at U of T, how they intend on following certain study strategies to meet their goals, and whether they want to get involved with extracurricular activities at the university’ (Oreopoulos and Petronijevic 2018, 6). ‘[T]he online intervention plus text and email messaging throughout the full academic year.’ This involved the students being given ‘the opportunity to provide their phone numbers and participate in a text and email messaging campaign lasting throughout both the fall semester in 2015 and the winter semester in 2016’ (Oreopoulos and Petronijevic 2018, 8). All students in this group got the emails, but only those that provided phone numbers got the messages. They were able to opt out, but ‘few chose to do so’ (Oreopoulos and Petronijevic 2018, 8). This was a two-way interaction in which students could ask questions. Some asked for the ‘locations of certain facilities on campus or how to stay on residence during the holiday break, while others said they need help with English skills or specific courses. Some students expressed relatively deep emotions, such as feeling anxious about family pressure to succeed in school or from doing poorly on an evaluation’ (Oreopoulos and Petronijevic 2018, 9). A response was usually given within an hour. ‘[T]he online intervention plus one-on-one coaching in which students are assigned to upper-year undergraduate coaches.’ ‘Coaches were available to meet with students to answer any questions via Skype, phone, or in person, and would send their students regular text and email messages of advice, encouragement, and motivation, much like the You@UofT program described above. In contrast to the messaging program, however, coaches were instructed to be proactive and regularly monitor their students’ progress. Whereas the You@UofT program attempts to “nudge” students in the right direction with academic advice, coaches play a greater “support” role, sensitively guiding students through problems.’ (Oreopoulos and Petronijevic 2018, 11). This coaching program was only available at UTM. ‘Our coaching treatment group was established by randomly drawing twenty-four students from the group of students that were randomly assigned into the text message campaign treatment. At the conclusion of the online exercise, instead of being invited to provide a phone number for the purpose of receiving text messages, these twenty-four students were given the opportunity to participate in a pilot coaching program. A total of seventeen students agreed to participate in the coaching program, while seven students declined.’ Our coaching treatment group was established by randomly drawing twenty-four students from the group of students that were randomly assigned into the text message campaign treatment. At the conclusion of the online exercise, instead of being invited to provide a phone number for the purpose of receiving text messages, these twenty-four students were given the opportunity to participate in a pilot coaching program. A total of seventeen students agreed to participate in the coaching program, while seven students declined. (Oreopoulos and Petronijevic 2018, 14) The model they consider is (Oreopoulos and Petronijevic 2018, 15): \\[\\begin{equation} y_{ij} = \\alpha + \\beta_1\\mbox{Online}_i + \\beta_2\\mbox{Text}_i + + \\beta_3\\mbox{Coach}_i + \\delta_j + \\mu \\mbox{First year}_i + \\epsilon_{ij} \\tag{8.2} \\end{equation}\\] Equation (8.2) explains the outcome of student \\(i\\) at campus \\(j\\) based on ‘indicators for each of the three treatment exercises students were given, campus fixed effects, and a first-year student indicator.’ The main parameters of interest are \\(\\beta_1\\), \\(\\beta_2\\) and \\(\\beta_3\\). The main outcomes were course grades, GPA, credits earned and failed. It was found, that the one-on-one coaching ‘increased grades by approximately 5 percentage points,’ while the other treatments had ‘had no detectable impact.’ One set of results are summarised in Figure @ref(fig:toronto_intervention). (#fig:toronto_intervention)Example of the results of the intervention. The results are important not only in a teaching context, but also for businesses hoping to retain customers. More papers are available here. 8.6 Case study - Civic Honesty Around The Globe Trust isn’t something that we think regularly about, but it’s actually fairly fundamental to most interactions, both economic and personal. For instance, many of us get paid after we do some work - we’re trusting our employer will make good; and vice versa - if you get paid in advance then they are trusting you. In a strictly naive, one-shot, transaction-cost-less world, this doesn’t make sense. If you get paid in advance, the incentive is for you to take the money and run in the last pay period before you quit, and through backward induction everything falls apart. Of course, we don’t live in such a world. For one thing there are transaction costs, for another generally we have repeated interactions, and finally, in my experience, the world usually ends up being fairly small. Understanding the extent of honestly in different countries may help us to explain economic development and other aspects of interest such as tax compliance, but it’s fairly hard to measure. We can’t really ask people how honest they are - wouldn’t the liars lie, resulting in a lemons problem (Akerlof 1978)? To get around, this A. Cohn et al. (2019a) conduct an experiment in 355 cities across 40 countries where they ‘turn in’ either: a wallet with the local equivalent of US$13.45 in it, or no money. They are interested in whether the ‘recipient’ attempts to return the wallet. They find that ‘[in virtually all countries, citizens were more likely to return wallets that contained more money’ (A. Cohn et al. 2019a, 1). The set-up of the experiment is fascinating. They ‘turn in’ 17,303 wallets to various institutions including: ‘(i) banks; (ii) theaters, museums, or other cultural establishments; (iii) post offices; (iv) hotels; and (v) police stations, courts of law, or other public offices’ (A. Cohn et al. 2019a, 1). These institutions were roughly equally sampled, although banks were slightly over sampled and post offices were slightly under-sampled. The importance of such institutions in the economy is generally well-accepted (Acemoglu, Johnson, and Robinson 2001) and they are common across most countries. Importantly for the experiment, they ‘typically have a public reception area where we could perform the drop-offs’ (A. Cohn et al. 2019a, 1). The way the experiment worked is that a research assistant turned the wallet in to an employee at a counter in the public reception area, saying ‘Hi, I found this [showing the wallet] on the street just around the corner. [Place wallet on counter.] Somebody must have lost it. I’m in a hurry and have to go. Can you please take care of it?’ (A. Cohn et al. 2019a, 2). The outcome of interest is whether an email is sent to the unique address on a business card in the wallet within 100 days. The research assistant had to note various features of the setting, including features such as the gender, age-group, and busyness of the ‘recipient.’ The wallets were transparent, and the business card had a name and email contact details. It also had a key and a grocery list (Figure 8.3). Figure 8.3: Example of the wallet. The grocery list was an attempt to convince the ‘recipient’ that the ‘owner’ was a local. Language and currency were adapted to local conditions. The key is only useful to the ‘owner,’ not to the ‘recipient’ of the wallet and was included to test for altruistic concerns. The primary treatment in the experiment is whether the wallet contained money or not. The key outcome was whether the wallet was attempted to be returned or not. It was found that ‘[t]he median response time was roughly 26 minutes across all countries, and about 88% of emails arrived within 24 hours’ (A. Cohn et al. 2019b, 10). If an email was received, then 3 hours later a response was sent, saying that the owner had left town, the contents were unimportant to them and that they could keep it or donate it to charity (A. Cohn et al. 2019b, 9). Considerable differences were found between countries (Figure 8.4). Figure 8.4: Key finding as to wallet return rates. Figure 8.4 shows that in almost all countries wallets with money were more likely to be returned than wallets without. The authors further conducted the experiment with the equivalent of US$94.15 in three countries - Poland, the UK, and the US - and found that reporting rates further increased. In those same three countries further tests were done comparing the situation when the wallet always contained money, but the presence of the key was varied. The wallet was slightly more likely to be reported when there was a key. The full set of 40 countries were chosen based on having enough cities with populations of at least 100,000, as well as the ability for the research assistants to safely visit and withdraw cash. The cities were chosen starting with the largest ones and there were usually 400 observations in each country (A. Cohn et al. 2019b, 5). Real-world concerns affected the specifics of the experiment. For instance, in ‘…India, we made a last minute change by replacing Chennai with Coimbatore due to severe flooding that took place in February 2015. In Kenya we did not carry out data collection in the last city visited (Malindi) because the research assistant was arrested and interrogated by the military police for suspicious activity’ (A. Cohn et al. 2019b, 5). In addition to the experiments, A. Cohn et al. (2019a) conducted surveys that allowed them to understand some reasons for their findings. It also allowed them to be specific about the respondents. The survey involved 2,525 respondents (829 in the UK, 809 in Poland, and 887 in the US) (A. Cohn et al. 2019b, 36). ‘To qualify for participation, individuals had to pass a simple attention check and meet the demographic quotas (based on age, gender, and residence) set by Qualtrics to construct the representative samples. Participants received a flat payment of US $4.00 for their participation’ (A. Cohn et al. 2019b, 36). The participants were given one of the scenarios and then asked to answer questions. Annoyingly the authors don’t explicitly specify the estimating equation. However they do say that important covariates about the ‘recipient’ include: gender, age-group, busyness, whether they were local, spoke English, understood the situation, friendliness, presence of: a computer, co-workers, bystanders, security cameras, security guards. Important covariates at a country-level include: country GDP, soil fertility, latitude, distance to water, temperature and its volatility, precipitation and its volatility, elevation, terrain roughness, pathogens, language features such as: pronouns, politeness, future time; share of protestants; family ties; state history; years of democracy; executive constraints; judicial independence; constitutional review; electoral rule; and primary school enrollment in 1920. The code or data for the paper are available: A. Cohn (2019). 8.7 A/B testing 8.7.1 Introduction Large companies, particularly tech companies, have developed incredibly sophisticated infrastructure for running complex experiments. In the tech industry, these experiments are often called A/B tests because they compare the effectiveness of two treatments: A and B. Such experiments are frequently run for things like increasing click-through rates on ads, but the same experimental infrastructure can also be used for research that advances scientific understanding. Salganik (2018, 185). The past decade has probably seen the most experiments ever run by several orders of magnitude with the extensive use of A/B testing on websites. Every time you are online you are probably subject to tens, hundreds, or potentially thousands, of different A/B tests. If you use apps like TikTok then this could run to the tens of thousands. While, at their heart, they are still just surveys that result in data that need to be analysed, they have several interesting features, which we will discuss. The opening example of Kohavi, Tang, and Xu (2020, 3) is a particularly nice illustration. In 2012, an employee working on Bing, Microsoft’s search engine, suggested changing how ad headlines display (Kohavi and Thomke 2017). The idea was to lengthen the title line of ads by combining it with the text from the first line below the title, as shown in Figure 1.1. Nobody thought this simple change, among the hundreds suggested, would be the best revenue-generating idea in Bing’s history! The feature was prioritized low and languished in the backlog for more than six months until a software developer decided to try the change, given how easy it was to code. He implemented the idea and began evaluating the idea on real users, randomly showing some of them the new title layout and others the old one. User interactions with the website were recorded, including ad clicks and the revenue generated from them. This is an example of an A/B test, the simplest type of controlled experiment that compares two variants: A and B, or a Control and a Treatment. A few hours after starting the test, a revenue-too-high alert triggered, indicating that something was wrong with the experiment. The Treatment, that is, the new title layout, was generating too much money from ads. Such “too good to be true” alerts are very useful, as they usually indicate a serious bug, such as cases where revenue was logged twice (double billing) or where only ads displayed, and the rest of the web page was broken. For this experiment, however, the revenue increase was valid. Bing’s revenue increased by a whopping 12%, which at the time translated to over $100M annually in the US alone, without significantly hurting key user-experience metrics. The experiment was replicated multiple times over a long period. The example typifies several key themes in online controlled experiments: It is hard to assess the value of an idea. In this case, a simple change worth over $100M/year was delayed for months. Small changes can have a big impact. A $100M/year return-on-investment (ROI) on a few days’ work for one engineer is about as extreme as it gets. Experiments with big impact are rare. Bing runs over 10,000 experiments a year, but simple features resulting in such a big improvement happen only once every few years. The overhead of running an experiment must be small. Bing’s engineers had access to ExP, Microsoft’s experimentation system, which made it easy to scientifically evaluate the idea. The overall evaluation criterion (OEC, described more later in this chapter) must be clear. In this case, revenue was a key component of the OEC, but revenue alone is insufficient as an OEC. It could lead to plastering the web site with ads, which is known to hurt the user experience. Bing uses an OEC that weighs revenue against user-experience metrics, including Sessions per user (are users abandoning or increasing engagement) and several other components. The key point is that user-experience metrics did not significantly degrade even though revenue increased dramatically. In these notes, I’m going to use A/B testing to strictly refer to the situation in which we’re dealing with a tech firm, and some type of change in code. If we are dealing with the physical world then we’ll stick with RCTs. I’m usually fairly dismissive of the CS folks who adopt different language for concepts that have been around for a long time. However, in the case of A/B testing I think that it’s possibly justified. There is something different about doing tens of thousands of small experiments all the time, compared with our normal RCT set-up of one experiment conducted over months. And finally, if you don’t work in a tech firm, then don’t discount the difficulty of shifting to an experimental set-up. You may think that it’s easy to go to a workplace and say ‘hey, let’s test stuff before we spend thousands/millions of dollars.’ You’d be wrong. In my opinion, the hardest part of A/B testing isn’t the science, it’s the politics. 8.7.2 Delivery Drawing on Kohavi, Tang, and Xu (2020, 153–61), we first consider how we will be delivering the A/B test. In the case of a RCT it’s fairly obvious how we deliver it - for instance, make a person come to a doctor’s clinic and inject them with a drug or a placebo. In the case of A/B testing, it’s less obvious - do you run it ‘server-side’ or ‘client-side?’ What this means is, do you just change the website - ‘server side,’ or do you change an app - ‘client side.’ This may seem like a silly issue, but it affects: 1) release; and 2) data transmission. In the case of the effect on release, it’s easy and normal to update a website all the time, so small changes can be easily implemented in the case of server-side. However, in the case of client-side, let’s say an app, it’s likely a much bigger deal. It needs to get through an app store (a bigger or lesser deal depending on which one). It need to go through a release cycle (a bigger or lesser deal depending on the specifics of the company and how it ships). Users have the opportunity to not upgrade. Are they likely different to those that do upgrade? (Yes.) Now, in the case of the effect on data transmission, again server-side is less of a big deal - you kind of get the data as part of the user interacting. But in the case of client-side - it’s not necessarily the case that the user will have the internet at the time they’re using your application, and if they do, they may have limitations on the data uploads. The phone may limit data transmission depending on its effect on battery, CPU, general performance, etc. Then maybe you decide to cache, but then the user may find it weird that some minor app takes up as much size as their photos. The effect of all this is that you need to plan and build this into your expectations - don’t promise results the day after a release if you’re evaluating a client-side change. Adjust for the fact that your results are conditional and gather data on those conditions e.g. battery level or whatever. Adjust in your analysis for different devices and platforms, etc. This is a lovely opportunity for multilevel regression. 8.7.3 Instrumentation Drawing on Kohavi, Tang, and Xu (2020, 162 - 165), I’ll now discuss instrumentation. Kohavi, Tang, and Xu (2020) use the name ‘instrumentation.’ I’d prefer something like ‘measurement methods’ so that we don’t confuse this with the entirely different concept of instrumental variables later in the course, but instrumentation is what is used in industry, so we’ll use that here too. Regardless of what it’s called, the point of this is that you need to consider how you are getting your data in the first place. For instance, if we put a cookie on your device then different types of users will remove that at different rates. Using things like beacons can be great (this is when you force the user to ‘download’ some tiny thing they don’t notice so that you know they’ve gone somewhere - see ‘email’ etc). But again, there are practical issues - do we force the beacon before the main content loads - which makes for a worse customer experience; or do we allow the beacon to load after the main content, in which case we may get a biased sample? There are likely different servers and databases for different faces of the product. For instance, Twitter in Australia, compared with Twitter in Canada, compared with Twitter on my phone’s app, compared with Twitter accessed via the browser. Joining these different datasets can be difficult and requires either a unique id or some probabilistic approach. Kohavi, Tang, and Xu (2020, 165) recommend changing the culture of your workplace to ensure instrumentation is normalised, which I mean, yeah, good luck. 8.7.4 Randomisation unit Again, drawing on Kohavi, Tang, and Xu (2020, 162 - 165), we need to be very aware of what are we actually randomising over? Again, this is something that’s kind of obvious in normal RCTs, but gets like really interesting in the case of A/B testing. Let’s consider the malaria netting experiments - either a person/village/state gets a net or it doesn’t. Easy (relatively). But in the case of server-side A/B testing - are we randomising the page, the session, or the user? To think about this, let’s think about colour. Let’s say that we change our logo from red to blue on the ‘home’ page. If we’re randomising at the page level, then when the user goes to the ‘about’ page the logo could be back to red. If we’re randomising at the session level, then it’ll be blue while they’re using the website that time, but if they close it and come back then it’ll be red. Finally, if we’re randomising at a user level then it’ll always be red for me, but always blue for my friend. That last bit assumes perfect identity tracking, which might be generally okay if you’re Google or Facebook, but for anyone else is going to be a challenge - what if you visit cbc.ca on your phone and then on your laptop? You’re likely considered a different ‘user.’ Does this matter? It’s a trade-off between consistency and importance. We are always interested in whether the treatment and control groups have been created randomly. One way to test it is an A/A test. Taddy (2019, 129) describes how ‘AB platforms typically run “AA” tests that show the same website in groups A and B. If you see a significant difference between groups in an AA trial, then something is likely wrong in your randomization.’ Kohavi, Tang, and Xu (2020, 201) says similarly, that ‘(w)e highly recommend running continuous A/A tests in parallel with other experiments to uncover problems, including distribution mismatches and plat- form anomalies.’ 8.7.5 Partnerships Unless we work at a Facebook/Twitter type firm, it may not be possible to run A/B tests ourselves at scale. While we can randomise our own personal website fairly easily, for most of us there won’t be many visitors. Hence it can be important to partner with such firms. Salganik (2018, 187) draws our attention to the fact that there may be tension between ‘the researchers and the partners.’ As an example, Salganik (2018) discusses a situation where one treatment (out of the three that were possible) accounted for 98 per cent of the sample because Facebook wanted to treat everyone. The researchers were only able to convince ‘them to hold back 1 per cent for a related treatment and 1 per cent for a control group.’ He continues: without the control group, it would have been basically impossible to measure the effect of the Info + Social treatment because it would have been a “perturb and observe” experiment, rather than a randomized controlled experiment. This example provides a valuable practical lesson for working with partners: sometimes you create an experiment by convincing someone to deliver a treatment and sometimes you create an experiment by convincing someone not to deliver a treatment (i.e. to create a control group). Salganik (2018, 188). In order to identify such opportunities, Salganik (2018, 188) advises us ‘to notice a real problem that you can solve while you are doing interesting science.’ Salganik (2018) closes with four other pieces of advice: ‘(Y)ou should think as much as possible before any data have been collected’ Salganik (2018, 189). ‘(Y)ou should consider designing a series of experiments that reinforce each other’ Salganik (2018, 190). You should ‘(c)reate zero variable cost data,’ by: 1) trying to replace human work with computer work; and 2) creating fun experiments that participants want to participate in (Salganik 2018, 191). You should ‘(b)uild ethics into your design: replace, refine, and reduce,’ that is ‘(m)ake your experiment more humane by replacing experiments with non-experimental studies, refining the treatments [to be as harmless as possible], and reducing the number of participants’ (Salganik 2018, 196). 8.7.6 Speed vs quality Don’t peek at your results early and then call off the rest of the experiment if you’ve got significance. You essentially ruin everything that underpins statistics if you do that. 8.7.7 Conflicting priorities One of the interesting aspects of A/B testing is that we’re usually running them not because we desperately care about the specific outcome, but because that feeds into some other measure that we care about. For instance, do we care whether the website is quite-dark-blue or slightly-darker-blue or white? Probably not, but we probably care a lot about the company share price. But then what if picking the best blue comes at a cost to the share price? Obviously, this is a bit contrived, so let’s pretend that we work at a food delivery app and that we’re the junior data scientist in charge of driver satisfaction. We do some A/B tests and we find that drivers are always happier when they are able to deliver food to the customer faster. Faster is better, always. But one way to achieve faster deliveries, is for them to not put the food into a hot box that will maintain the temperature. Something like that might save 30 seconds, which is significant on a 10-15 minute deliver. Unfortunately, although making a decision like that on the basis of A/B tests designed to optimize driver-satisfaction, would ultimately likely make the customer experience worse. If customers receive cold food, (when it’s meant to be hot) then they may stop using the service and so this is likely bad for the app in the longer term. This trade-off may be obvious if you’re running the driver-experiment and you’re looking at the customer complaints. Maybe on a small team or in a start-up you would be. But if you work for a larger team, you’d likely not and so ensuring that A/B tests aren’t resulting in false optimization is something that is especially interesting, and not a typical trade-off in a normal RCT. 8.8 Case study - Upworthy The trouble with much of A/B testing is that because it’s done by firms we typically don’t have datasets that we can use. However, J. Nathan Matias (Cornell), Kevin Munger (Penn State), and Marianne Aubin Le Quere (Cornell) obtained a dataset of A/B tests from Upworthy that they provide access to (Matias et al. 2019). You are able to request access to the dataset here: https://upworthy.natematias.com (this request may take a couple of weeks to be processed). Upworthy was a click-bait news company that used A/B testing to optimize their content. More details are provided by Fitts (2014). Let’s have a quick look at the data. upworthy &lt;- read_csv(here::here(&quot;dont_push/upworthy-archive-exploratory-packages-03.12.2020.csv&quot;)) upworthy %&gt;% head() ## # A tibble: 6 x 17 ## X1 created_at updated_at clickability_test… excerpt ## &lt;dbl&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0 2014-11-20 06:43:16 2016-04-02 16:33:38 546d88fb84ad38b2c… Things that … ## 2 1 2014-11-20 06:43:44 2016-04-02 16:25:54 546d88fb84ad38b2c… Things that … ## 3 2 2014-11-20 06:44:59 2016-04-02 16:25:54 546d88fb84ad38b2c… Things that … ## 4 3 2014-11-20 06:54:36 2016-04-02 16:25:54 546d902c26714c6c4… Things that … ## 5 4 2014-11-20 06:54:57 2016-04-02 16:31:45 546d902c26714c6c4… Things that … ## 6 5 2014-11-20 06:55:07 2016-04-02 16:25:54 546d902c26714c6c4… Things that … ## # … with 12 more variables: headline &lt;chr&gt;, lede &lt;chr&gt;, slug &lt;chr&gt;, ## # eyecatcher_id &lt;chr&gt;, impressions &lt;dbl&gt;, clicks &lt;dbl&gt;, significance &lt;dbl&gt;, ## # first_place &lt;lgl&gt;, winner &lt;lgl&gt;, share_text &lt;chr&gt;, square &lt;chr&gt;, ## # test_week &lt;dbl&gt; upworthy %&gt;% names() ## [1] &quot;X1&quot; &quot;created_at&quot; &quot;updated_at&quot; ## [4] &quot;clickability_test_id&quot; &quot;excerpt&quot; &quot;headline&quot; ## [7] &quot;lede&quot; &quot;slug&quot; &quot;eyecatcher_id&quot; ## [10] &quot;impressions&quot; &quot;clicks&quot; &quot;significance&quot; ## [13] &quot;first_place&quot; &quot;winner&quot; &quot;share_text&quot; ## [16] &quot;square&quot; &quot;test_week&quot; From the documentation: ‘The Upworthy Research Archive contains packages within tests. On Upworthy, packages are bundles of headlines and images that were randomly assigned to people on the website as part of a test. Tests can include many packages.’ So each row is a package and it should be part of a test ‘clickability_test_id.’ We have a variety of variables. We’ll focus on ‘created_at,’ ‘clickability_test_id’ so that we can create comparison groups, ‘headline,’ ‘impressions’ which is the number of people that saw the package, and ‘clicks’ which is the number that clicked on that package. So within each batch of tests, we’re interested in the effect of varied headlines on impressions and clicks. upworthy_restricted &lt;- upworthy %&gt;% select(created_at, clickability_test_id, headline, impressions, clicks) head(upworthy_restricted) ## # A tibble: 6 x 5 ## created_at clickability_test… headline impressions clicks ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2014-11-20 06:43:16 546d88fb84ad38b2c… They&#39;re Being Calle… 3052 150 ## 2 2014-11-20 06:43:44 546d88fb84ad38b2c… They&#39;re Being Calle… 3033 122 ## 3 2014-11-20 06:44:59 546d88fb84ad38b2c… They&#39;re Being Calle… 3092 110 ## 4 2014-11-20 06:54:36 546d902c26714c6c4… This Is What Sexism… 3526 90 ## 5 2014-11-20 06:54:57 546d902c26714c6c4… This Is What Sexism… 3506 120 ## 6 2014-11-20 06:55:07 546d902c26714c6c4… This Is What Sexism… 3380 98 We are going to focus on the text contained in headlines. We also want to remove the effect of different pictures, by comparing on the same image. I’m interested in whether headlines that asked a question got more clicks than those that didn’t. To identify whether a headline asks a question, I’m going to just search for a question mark. Although there are more complicated constructions that we could use, this will be enough to get started. upworthy_restricted &lt;- upworthy_restricted %&gt;% mutate(asks_question = stringr::str_detect(string = headline, pattern = &quot;\\\\?&quot;)) upworthy_restricted %&gt;% count(asks_question) ## # A tibble: 2 x 2 ## asks_question n ## &lt;lgl&gt; &lt;int&gt; ## 1 FALSE 19130 ## 2 TRUE 3536 Now for every test, for every picture, we want to know whether asking a question affected the number of clicks. to_question_or_not_to_question &lt;- upworthy_restricted %&gt;% group_by(clickability_test_id, asks_question) %&gt;% summarise(ave_clicks = mean(clicks)) %&gt;% ungroup() look_at_differences &lt;- to_question_or_not_to_question %&gt;% pivot_wider(id_cols = clickability_test_id, names_from = asks_question, values_from = ave_clicks) %&gt;% rename(ave_clicks_not_question = `FALSE`, ave_clicks_is_question = `TRUE`) %&gt;% filter(!is.na(ave_clicks_not_question)) %&gt;% filter(!is.na(ave_clicks_is_question)) %&gt;% mutate(difference_in_clicks = ave_clicks_is_question - ave_clicks_not_question) look_at_differences$difference_in_clicks %&gt;% mean() ## [1] -4.890435 So we find that in general, having a question in the headline may slightly decrease the number of clicks on a headline, although if there is an effect it does not appear to be very large (Figure 8.5). Figure 8.5: Comparison of the average number of clicks when a headline contains a question mark or not. 8.9 Sampling and survey essentials 8.9.1 Introduction Let’s say that we have some data. For instance, a particular toddler goes to sleep at 6:00pm every night. We might be interested to know whether that bed-time is common more generally among all toddlers, or if we have an unusual toddler. We only have one toddler so our ability to use his bed time to speak about all toddlers is limited. But what about if we talk to our friends who also have toddlers? How many friends, and friends of friends, do we have to ask because we can begin to feel comfortable speaking about some underlying truth of toddler bedtime? In the wonderful phrase of Wu and Thompson (2020, 3) ‘[s]tatistics is the science of how to collect and analyze data, and draw statements and conclusions about unknown populations. The term population usually refers to a real or hypothetical set of units with characteristics and attributes which can be modelled by random variables and their respective probability distributions.’ In my own much less wonderful phrasing, ‘statistics involves having some data and trying to say something sensible about it.’ I mean, it’s really up to you which one you want to go with. In the case of surveys, our population is a finite set of \\(N\\) labels: ‘person 1,’ ‘person 2,’ ‘person 3,’ …, ‘person \\(N\\).’ It is important here to recognise that there is a difference between the population of interest to a survey and a population in the sense that it is used when we talk of limits and similar infinity concepts in statistics. For instance, from time to time, you hear people who work with census data say that they don’t need to worry about confidence intervals because they have the whole population of the country. Nothing could be further from the truth. Wu and Thompson (2020, 4) have a lovely example of the ambiguity that surrounds the definition of a population. Let’s consider the population of voters. In Canada that means anyone who is 18 or older. Fine. But what if we are interested in consumers - what is the definition of hipsters? I regularly eat avocado toast, (+1), but I’ve never had bullet coffee (-1). Am I in the population or not? More things are formally defined than you may realise. For instance, the idea of a rural area is precisely defined. A property is either in a rural area or not. But then we come to the lovely example of Wu and Thompson (2020, 4) when it comes to whether someone is a smoker. If a 15 year old has had 100 cigarettes then it’s pretty clear that we need to treat them differently than if they have had none. But if a 100 year old has had 100 cigarettes then we consider them to have none. That’s fine, but what is the age at which this changes? Further, think about how this changes over time. At one point, parents used to be worried if children had more than two hours of screen time, now those same children (and possibly even the parents) regularly likely spend more than eight hours in front of a screen if they work in an office job. So we come to some critical terminology: Population: ‘The set of all units covered by the main objective of the study.’ Wu and Thompson (2020, 5). Frame: ‘Lists of sampling units’ Wu and Thompson (2020, 9) where sampling units are either the observational units themselves or the clusters. Sample: Those who complete and return the survey. To be a little more concrete about this, consider that we are trying to conduct a survey about the attitudes Australians who live in Toronto. So the target population is all Australians who live in Toronto, the frame might be all those Australians who live in Toronto who use Facebook, because we are going to use Facebook to choose who to sample. And then finally, if we take that Facebook list of all Australians living in Canada and we gave each one a chance at being surveyed then that would be our sampled population, but if we just picked the ones that I know then it would just be Dan, Monica, and Liza (from New Zealand but we’ll claim her because that’s a thing that Australians do). In that example the target population and the frame will be different because not all Australians who live in Toronto are on Facebook. Similarly, if not everyone that we gave the survey to actually completed the survey then the sample and the frame would be different. Having identified a population of interest and a frame (i.e. a list that gets the closest to that population) At this point we distinguish between probability and non-probability sampling. With probability sampling, every member of the frame has some chance of being sampled. Consider the example of the Australian Election Study - they get a list of all the addresses in Australia, and then randomly choose some to send letters to. The ‘randomista’ and RCT revolution that we discuss later, is needed because of a lack of probability sampling, but when it exists it plays a role here. Importantly it ensure that we are clear about the role of uncertainty (Wu and Thompson 2020, 11). The trade-off is that it is expensive and difficult. Note that each unit in the frame doesn’t have to have the same probability necessarily, it just needs to be determined by a probability measure. In contrast, with non-probability sampling we focus on populations that are ‘readily available’ or convenient, satisfy certain quotas, based on judgement, or those that volunteer. The difference between probability and non-probability sampling is that of degree - we typically cannot force someone to take our survey, and hence, there is almost also as aspect of volunteering. While acknowledging that it is a spectrum, most of statistics was developed based on probability sampling. But much of modern sampling is done using non-probability sampling. In particular, a common approach is to have a bunch of Facebook ads trying to recruit a panel of people in exchange for compensation. This panel is then the group that is sent various surveys as necessary. But think for a moment about the implications of this - what type of people are likely to respond to such an ad? I don’t know who Canada’s richest person is, but are they likely to be in this panel? Is your grandmother likely to respond to that ad? What about you - do you even use Facebook? In some cases it is possible to do a census. Nation-states typically do one every five to ten years. But there is a reason that it is only nation states that do them - they are expensive, time-consuming, and surprisingly, they are sometimes not as accurate as we may hope because of how general they need to be. Hence, the role of surveys. Note, however that censuses will typically have many of the same concerns. When we consider our population, it will typically have some ordering. This may be as simple as a country having states/provinces. We consider a stratified structure to be one in which we can divide the population into mutually exclusive and collectively exhaustive sub-populations, or strata. Examples of strata in Wu and Thompson (2020, 8) include provinces, federal electoral districts, or health regions. But strata need not be geographic, and it may be possible to use different majors. We use stratification to help with the efficiency of sampling or with the balance of the survey. For instance, if we surveyed provinces in proportion to their population, then even a survey of 10,000 responses would only expect to have 10 responses from the Yukon. The other word that is used that takes advantage of the ordering of some population is clusters. Again, these are collectively exhaustive and mutually exclusive. Again, they may be geographically based, but need not be. The difference between stratified sampling and cluster sampling, is that ‘under stratified sampling, sample data are collected from every stratum, (whereas) under cluster sampling, only a portion of the clusters has members in the final sample’ Wu and Thompson (2020, 8). That all said, this difference can become less clear in practice, especially ex post - what if you stratify then randomly sample within that strata, but no one is selected - but in terms of intention the difference is clear. We now turn to the first of our claims, which is that if we have a perfect frame and no non-response, then our sample results will match that of the population. We’d of course be very worried if that weren’t the case, but it’s nice to have it stated. We establish some type of population mean for the study variable, \\(\\mu_y\\), and population means for the auxiliary variables \\(\\mu_x\\), which could be things like age, gender, etc. Remembering that when we do this in the real world, we may have many study variables, and indeed, some overlap. If a variable is an indicator then in this set-up all we have to do is to work out the proportion in order to estimate it, which is \\(P\\). And finally, we get a rule of thumb for large samples whereby the variance in this binary and perfect setting becomes \\(\\sigma_y^2 = P/(1-P)\\) (Wu and Thompson 2020, 11). Finally, we conclude with the steps that you should consider. These are all critical. Strong reports would grapple with all of these. 8.9.2 Simple random sampling TBD 8.9.3 Stratified and cluster sampling TBD 8.10 Implementing surveys 8.10.1 Google 8.10.2 Facebook 8.10.3 Survey Monkey 8.10.4 Mechanical Turk 8.10.5 Prolific 8.10.6 Qualtrics 8.10.7 Other 8.11 Next steps Large scale experiments are happening all around us. These days I feel we all know a lot more about healthcare experiments than perhaps we’d like to know and the AstraZeneca/Oxford situation is especially interesting, for instance, Oxford-AstraZeneca (2020), but see Bastian (2020) for how this is actually possibly more complicated. There are also well-known experiments that tried to see if big government programs are effective, such as: The RAND Health Insurance Experiment randomly gave health insurance to people in the US between 1974 and 1982 (Brook et al. 1984). The Oregon Health Study randomly gave health insurance in Oregon in 2008 (Finkelstein et al. 2012). There’s a joke in statistics, okay, well, TBH, I have a joke about statistics, and it’s that at some point every professor is like ‘… and so X really just boils down to a missing data problem’ and it’s funny because, that’s kind of the fundamental issue of statistics, we’d not really need the science if we had all the data. In hindsight, this is not really a joke, but I’m a father now so I’ll just lean into it.↩︎ As someone who oddly is somewhat superstitious, believes fully in the irony gods, and does have a pair of lucky, fun, socks, this example was not randomly chosen.↩︎ I’m personally very attached to this example as this issue also matters a lot to my father↩︎ For posteriority, 2020 was quite a year.↩︎ "],["farm-data.html", "Chapter 9 Farm data 9.1 Open Government Data 9.2 Electoral Studies", " Chapter 9 Farm data Last updated: 30 January 2021. Required reading Recommended reading Key concepts/skills/etc Key libraries Key functions/etc Pre-quiz Key sources of data University of Toronto Dataverse: https://dataverse.scholarsportal.info/dataverse/toronto. Data is Plural structured archive: https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/edit#gid=0. Kaggle Datasets: https://www.kaggle.com/datasets. Figure Eight: https://www.figure-eight.com/data-for-everyone/. Google dataset search: https://datasetsearch.research.google.com/. Awesome Data: https://github.com/awesomedata/awesome-public-datasets. Quiz Please identify three other sources of data that you are interested in and describe where are they available (please include a link if possible)? Please focus on one of those sources. What steps do you have to go through in order to get a dataset that can be analysed in R? Let’s say you take a job at RBC (a Canadian bank) and they already have some quantitative data for you to use. What are some questions that you should explore when deciding whether that data will be useful to you? Please identify three sources of government data in your country and describe where are they available (please include a link if possible)? Please focus on one of those sources. What steps do you have to go through in order to get a dataset such that it can be analysed in R? Please identify three other sources of data that you are interested in and describe where are they available (please include a link if possible)? Please focus on one of those sources. What steps do you have to go through in order to get a dataset that can be analysed in R? Let’s say you take a job at RBC (a Canadian bank) and they already have some quantitative data for you to use. What are some questions that you should explore when deciding whether that data will be useful to you? 9.1 Open Government Data Canadian Government Open Data: https://open.canada.ca/en/open-data. 9.1.1 Canadian Census 9.1.2 City of Toronto Open Data Portal 9.2 Electoral Studies 9.2.1 Canadian Electoral Study 9.2.2 Australian Electoral Study "],["cleaning-and-preparing-data.html", "Chapter 10 Cleaning and preparing data", " Chapter 10 Cleaning and preparing data Last updated: 30 January 2021. Required reading Recommended reading Key concepts/skills/etc Key libraries Key functions/etc Quiz "],["storing-and-retrieving-data.html", "Chapter 11 Storing and retrieving data", " Chapter 11 Storing and retrieving data Last updated: 30 January 2021. Required reading Recommended reading Key concepts/skills/etc Key libraries Key functions/etc Quiz "],["disseminating-and-protecting-data.html", "Chapter 12 Disseminating and protecting data", " Chapter 12 Disseminating and protecting data Last updated: 30 January 2021. Hawes, M. B. (2020). Implementing Differential Privacy: Seven Lessons From the 2020 United States Census. Harvard Data Science Review. https://doi.org/10.1162/99608f92.353c6f99 https://hdsr.mitpress.mit.edu/pub/g9o4z8au/release/2 https://www.census.gov/newsroom/blogs/research-matters/2020/02/census_bureau_works.html Required reading Recommended reading Key concepts/skills/etc Key libraries Key functions/etc Quiz "],["exploratory-data-analysis.html", "Chapter 13 Exploratory data analysis 13.1 Introduction 13.2 Case study - TTC subway delays 13.3 Case study - Opinions about a casino in Toronto 13.4 Case study - Airbnb listing in Toronto", " Chapter 13 Exploratory data analysis Last updated: 9 March 2021. Required reading Barocas, Solon, and Danah Boyd, 2017, ‘Engaging the ethics of data science in practice,’ Communications of the ACM, 60.11 (2017): 23-25. DiCiccio, Thomas J., and Mary E. Thompson, 2004, ‘A Conversation with Donald A. S. Fraser,’ Statistical Science, 19 (2) pp. 370-386, https://utstat.toronto.edu/craiu/DonFraser_SSInterview.pdf. Jordan, Michael I, 2019, ‘AI - The revolution hasn’t started yet,’ Harvard Data Science Review, 1 July, https://hdsr.mitpress.mit.edu/pub/wot7mkc1. Tukey, John W., 1961, ‘The Future of Data Analysis,’ The annals of mathematical statistics, Part 1 ‘General Considerations,’ https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-33/issue-1/The-Future-of-Data-Analysis/10.1214/aoms/1177704711.full. Wickham, Hadley, and Garrett Grolemund, 2017, R for Data Science, Chapters 3 and 7, https://r4ds.had.co.nz/. Recommended reading Hall, Megan, 2019, ‘Exploratory Data Analysis Using Tidyverse,’ https://hockey-graphs.com/2019/10/08/exploratory-data-analysis-using-tidyverse/. Kommenda, Niko, Helen Pidd and Libby Brooks, 2020, ‘Revealed: the areas in the UK with one Airbnb for every four homes,’ The Guardian, 20 February, https://www.theguardian.com/technology/2020/feb/20/revealed-the-areas-in-the-uk-with-one-airbnb-for-every-four-homes. Silge, Julia, 2018, ‘Understanding PCA using Stack Overflow data,’ https://juliasilge.com/blog/stack-overflow-pca/. Soetewey, Antoine, 2020, ‘Descriptive statistics in R,’ https://www.statsandr.com/blog/descriptive-statistics-in-r/. Stodulka, Jiri, 2019, ‘Toronto Crime and Folium,’ https://www.jiristodulka.com/post/toronto-crime/. Wong, Julia Carrie, 2020, ‘One year inside Trump’s monumental Facebook campaign,’ The Guardian, 29 January, https://www.theguardian.com/us-news/2020/jan/28/donald-trump-facebook-ad-campaign-2020-election. Key concepts/skills/etc Quickly coming to terms with a new dataset by constructing graphs and tables. Understanding the issues and features of the dataset and how this may affect your modelling decisions. Thinking about missing values and outliers. Key libraries broom ggrepel here janitor lubridate opendatatoronto tidymodels tidyverse visdat Key functions/etc augment() clean_names() coord_flip() count() distinct() facet_grid() facet_wrap() geom_bar() geom_col() geom_density() geom_histogram() geom_line() geom_point() geom_smooth() geom_text_repel() get_dupes() glance() if_else() ifelse() initial_split() left_join() mutate() mutate_all() names() ncol() nrow() pivot_wider() scale_color_brewer() scale_fill_brewer() scale_x_log10() scale_y_log10() str_detect() str_extract() str_remove() str_split() str_starts() summarise() summarise_all() theme_classic() theme_minimal() vis_dat() vis_miss() Quiz In your own words what is exploratory data analysis (this will be difficult, but please write only one nuanced paragraph)? In Tukey’s words, what is exploratory data analysis (please write one paragraph)? Who was Tukey (please write a paragraph or two)? What is Tukey’s link to DoSS (hint: he was an advisor on someone’s PhD - who was that person)? Can you identify a female equivalent to Tukey who we (as historians of statistics) may have overlooked? If you have a dataset called ‘my_data,’ which has two columns: ‘first_col’ and ‘second_col,’ then could you please write some rough R code that would generate a graph (the type of graph doesn’t matter). Consider a dataset that has 500 rows and 3 columns, so there are 1,500 cells. If 100 of the cells are missing data for at least one of the columns, then would you remove the whole row your dataset or try to run your analysis on the data as is, or some other procedure? What if your dataset had 10,000 rows instead, but the same number of missing cells? Please note three ways of identifying unusual values. What is the difference between a categorical and continuous variable? What is the difference between a factor and an integer variable? How can we think about who is systematically excluded from a dataset? Using the opendatatoronto package, download the data on mayoral campaign contributions for 2014. (note: the 2014 file you will get from get_resource, so just keep the sheet that relates to the Mayor election). Clean up the data format (fixing the parsing issue and standardizing the column names using janitor) Summarize the variables in the dataset. Are there missing values, and if so, should we be worried about them? Is every variable in the format it should be? If not, create new variable(s) that are in the right format. Visually explore the distribution of values of the contributions. What contributions are notable outliers? Do they share a similar characteristic(s)? It may be useful to plot the distribution of contributions without these outliers to get a better sense of the majority of the data. List the top five candidates in each of these categories: 1) total contributions; 2) mean contribution; and 3) number of contributions. Repeat that process, but without contributions from the candidates themselves. How many contributors gave money to more than one candidate? Name three geoms that produce graphs that have bars on them ggplot(). Consider a dataset 10,000 observations and 27 variables. For each observation, there is at least one missing variable. Please discuss, in a paragraph or two, the steps that you would take to understand what is going on. Known missing data, are those that leave holes in your dataset. But what about data that were never collected? Please look at McClelland, Alexander, 2019, ‘“Lock This Whore Up”: Legal Violence and Flows of Information Precipitating Personal Violence against People Criminalised for HIV-Related Crimes in Canada,’ European Journal of Risk Regulation, 10 (1), pp. 132-147. Then look at Policing the Pandemic - https://www.policingthepandemic.ca/. Look into how they gathered their dataset and what it took to put this together. What is in the dataset and why? What is missing and why? How could this affect the results? How might similar biases enter into other datasets that you have used or read about? 13.1 Introduction The future of data analysis can involve great progress, the overcoming of real difficulties, and the provision of a great service to all fields of science and technology. Will it? That remains to us, to our willingness to take up the rocky road of real problems in preference to the smooth road of unreal assumptions, arbitrary criteria, and abstract results without real attachments. Who is for the challenge? Tukey (1962, 64). Exploratory data analysis is never finished, you just die. It is the active process of exploring and becoming familiar with your data. Like a farmer with their hands in the earth, you need to know every contour and aspect of your data. You need to know how it changes, what it shows, hides, and what are its limits. Exploratory data analysis is the unstructured process of doing this. That said, exploratory data analysis (EDA) is not something that ends up in your final paper. It is a means to an end and while it will inform your entire paper, especially the data section, it’s not typically something that belongs in a final draft. The best way to proceed is to make a separate .Rmd and add code and brief notes as you go. Don’t delete previous code, just add to it. When you run out of time, you’ll have a useful notebook that captures your exploration. This is a document for you and your collaborators and will guide all the subsequent modelling that you do. EDA draws on everything that you know as an analyst. Every tool is fair game and should be considered. Look at the raw data, make some tables, some plots, some summary statistics, make some models. The key here is to iterate, move quickly not perfectly, and come to understand your data. In this chapter we will working with real data that has many issues so that you can understand the main characteristics and potential issues. We will use the opendatatoronto package (Gelfand 2020), among other sources. 13.2 Case study - TTC subway delays This section was written with Monica Alexander. 13.2.1 Introduction The opendatatoronto package (Gelfand 2020) provides an interface to all data available on the Open Data Portal provided by the City of Toronto. We are going to use that to take a quick look at the subway delays. We’re additionally going to especially draw on the tidyverse (Wickham et al. 2019b), as well as the ggrepel (Slowikowski 2021), janitor (Firke 2020), lubridate (Grolemund and Wickham 2011), and visdat (Tierney 2017) packages. library(opendatatoronto) library(tidyverse) library(ggrepel) library(janitor) library(lubridate) library(visdat) 13.2.2 Gather the data To begin with, use opendatatoronto::list_packages() to look at some of the datasets that available. all_data &lt;- opendatatoronto::list_packages(limit = 500) all_data ## # A tibble: 412 x 11 ## title id topics civic_issues publisher excerpt dataset_category ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Outbrea… 80ce0b… Health,… &lt;NA&gt; Toronto … &quot;This dat… Document ## 2 Municip… 5da2e2… City go… Affordable h… Municipa… &quot;This dat… Document ## 3 Buildin… 8219e9… Develop… &lt;NA&gt; Toronto … &quot;Provides… Document ## 4 Buildin… 108c2b… Develop… Affordable h… Toronto … &quot;Provides… Document ## 5 License… 059d37… Communi… &lt;NA&gt; Children… &quot;Licensed… Map ## 6 EarlyON… 261962… Communi… Poverty redu… Children… &quot;EarlyON … Map ## 7 TTC Sub… 996cfe… Transpo… Mobility Toronto … &quot;TTC Subw… Document ## 8 Short T… fc4189… Permits… Affordable h… Municipa… &quot;This dat… Table ## 9 COVID-1… 64b545… Health &lt;NA&gt; Toronto … &quot;Line-lis… Table ## 10 City Su… 2db048… City go… &lt;NA&gt; City Cle… &quot;This sub… Document ## # … with 402 more rows, and 4 more variables: num_resources &lt;int&gt;, ## # formats &lt;chr&gt;, refresh_rate &lt;chr&gt;, last_refreshed &lt;date&gt; We’ll download the data on TTC subway delays in 2019. There are multiple files for 2019 so we need to get them all and then make them into one big dataframe. # We know this number based on the &#39;id&#39; of the interest. ttc_resources &lt;- list_package_resources(&quot;996cfe8d-fb35-40ce-b569-698d51fc683b&quot;) ttc_resources &lt;- ttc_resources %&gt;% mutate(year = str_extract(name, &quot;201.?&quot;)) delay_2019_ids &lt;- ttc_resources %&gt;% filter(year==2019) %&gt;% select(id) %&gt;% pull() delay_2019 &lt;- c() for(i in 1:length(delay_2019_ids)) { delay_2019 &lt;- bind_rows(delay_2019, get_resource(delay_2019_ids[i])) } # make the column names nicer to work with delay_2019 &lt;- clean_names(delay_2019) Let’s also download the delay code and readme, as reference. You’d probably want to save all this into an ‘inputs’ folder, as this is the raw data that would underpin any analysis. delay_codes &lt;- get_resource(&quot;fece136b-224a-412a-b191-8d31eb00491e&quot;) delay_data_codebook &lt;- get_resource(&quot;54247e39-5a7d-40db-a137-82b2a9ab0708&quot;) This dataset has a bunch of interesting variables. You can refer to the readme for descriptions. Our outcome of interest is min_delay, which give the delay in minutes. head(delay_2019) ## # A tibble: 6 x 10 ## date time day station code min_delay min_gap bound line ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2019-01-01 00:00:00 01:08 Tuesd… YORK MIL… PUSI 0 0 S YU ## 2 2019-01-01 00:00:00 02:14 Tuesd… ST ANDRE… PUMST 0 0 &lt;NA&gt; YU ## 3 2019-01-01 00:00:00 02:16 Tuesd… JANE STA… TUSC 0 0 W BD ## 4 2019-01-01 00:00:00 02:27 Tuesd… BLOOR ST… SUO 0 0 N YU ## 5 2019-01-01 00:00:00 03:03 Tuesd… DUPONT S… MUATC 11 16 N YU ## 6 2019-01-01 00:00:00 03:08 Tuesd… EGLINTON… EUATC 11 16 S YU ## # … with 1 more variable: vehicle &lt;dbl&gt; Next we’re going to highlight some tools that might be useful when you are getting used to a new dataset. There’s no one way to explore, but it’s important to keep in mind: what should your variables look like (type, values, distribution, etc); what would be surprising (outliers etc); and what is your end goal (here, it might be understanding factors associated with delays, e.g. stations, time of year, time of day, etc). In any data analysis project, if it turns out you have data issues, surprising values, missing data etc, it’s important you document anything you found and the subsequent steps or assumptions you made before moving onto your data analysis and modeling. As always: Start with an end in mind. Be as lazy as possible. 13.2.3 Sanity Checks We need to check that the variables are what they say they are. If they aren’t, the natural next question is to what to do with any issues. Should we recode them, or even remove them? It’s important to distinguish between factors and characters, as well as between factors and numerics. For instance, have a look at the column that claims to be the days of week using unique(). unique(delay_2019$day) ## [1] &quot;Tuesday&quot; &quot;Wednesday&quot; &quot;Thursday&quot; &quot;Friday&quot; &quot;Saturday&quot; &quot;Sunday&quot; ## [7] &quot;Monday&quot; Another function that that’s useful here is table(). table(delay_2019$day) ## ## Friday Monday Saturday Sunday Thursday Tuesday Wednesday ## 2979 2970 2238 1978 3116 2939 3002 Let’s now check the lines. unique(delay_2019$line) ## [1] &quot;YU&quot; &quot;BD&quot; &quot;YU/BD&quot; ## [4] &quot;SHP&quot; &quot;SRT&quot; NA ## [7] &quot;YUS&quot; &quot;B/D&quot; &quot;BD LINE&quot; ## [10] &quot;999&quot; &quot;YU/ BD&quot; &quot;YU &amp; BD&quot; ## [13] &quot;BD/YU&quot; &quot;YU\\\\BD&quot; &quot;46 MARTIN GROVE&quot; ## [16] &quot;RT&quot; &quot;BLOOR-DANFORTH&quot; &quot;YU / BD&quot; ## [19] &quot;134 PROGRESS&quot; &quot;YU - BD&quot; &quot;985 SHEPPARD EAST EXPR&quot; ## [22] &quot;22 COXWELL&quot; &quot;100 FLEMINGDON PARK&quot; &quot;YU LINE&quot; It looks like we have many issues here, and some of them have an obvious re-code, but some do not. Should we drop them? There’s no absolute right answer here, it depends on what you’re using the data for, but you need to be aware of these issues in the data. 13.2.4 Missing values Exploring missing data is a course in itself, but the main point is that their presence (or lack thereof) will haunt your analysis. Insert joke about ghostbusters here. To get started look at known-unknowns, which are the NAs for each variable. delay_2019 %&gt;% summarise_all(list(~sum(is.na(.)))) ## # A tibble: 1 x 10 ## date time day station code min_delay min_gap bound line vehicle ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 0 0 0 0 0 0 0 4380 50 0 The visdat package (Tierney 2017) is also useful here, particularly to see how missing values are distributed. vis_dat(delay_2019) vis_miss(delay_2019) For these known-unknowns, what we are interested in is whether the they are missing at random. We want to, ideally, show that data happened to just drop out. Of course, this is unlikely the case, and so we are looking to see what is systematic about how our data are missing. 13.2.5 Duplicate rows Sometime data happen to be duplicated. If we didn’t notice this then our analysis would be wrong in ways that we’d not be able to consistently expect. There are a variety of ways to look for duplicated rows, but the janitor::get_dupes() function from the janitor package (Firke 2020) is especially useful. janitor::get_dupes(delay_2019) ## # A tibble: 158 x 11 ## date time day station code min_delay min_gap bound line ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2019-01-01 00:00:00 08:18 Tuesd… DONLAND… MUESA 5 10 W BD ## 2 2019-01-01 00:00:00 08:18 Tuesd… DONLAND… MUESA 5 10 W BD ## 3 2019-02-01 00:00:00 05:51 Friday SCARB C… MRTO 10 15 S SRT ## 4 2019-02-01 00:00:00 05:51 Friday SCARB C… MRTO 10 15 S SRT ## 5 2019-02-01 00:00:00 06:45 Friday MIDLAND… MRWEA 3 8 S SRT ## 6 2019-02-01 00:00:00 06:45 Friday MIDLAND… MRWEA 3 8 S SRT ## 7 2019-02-01 00:00:00 06:55 Friday LAWRENC… ERDO 0 0 S SRT ## 8 2019-02-01 00:00:00 06:55 Friday LAWRENC… ERDO 0 0 S SRT ## 9 2019-02-01 00:00:00 07:16 Friday MCCOWAN… MRWEA 5 10 N SRT ## 10 2019-02-01 00:00:00 07:16 Friday MCCOWAN… MRWEA 5 10 N SRT ## # … with 148 more rows, and 2 more variables: vehicle &lt;dbl&gt;, dupe_count &lt;int&gt; Our delays dataset has quite a few duplicates. Again, we’re interested in whether there is something systematic going on. Remembering that we’re trying to quickly come to terms with dataset, one way forward is to flag this as an issue to come back to and explore later, and to just remove them for now. delay_2019 &lt;- delay_2019 %&gt;% distinct() 13.2.6 Visualizing distributions You need to see your data in its most raw form to understand it, and histograms, barplots, and density plots are your friends here. We’re not looking for beauty here, we’re looking to get a look at the data as quickly as possible. Let’s look at one outcome of interest: ‘min_delay.’ First of all let’s just look at a histogram of all the data. ## Removing the observations that have non-standardized lines delay_2019 &lt;- delay_2019 %&gt;% filter(line %in% c(&quot;BD&quot;, &quot;YU&quot;, &quot;SHP&quot;, &quot;SRT&quot;)) ggplot(data = delay_2019) + geom_histogram(aes(x = min_delay)) Somewhat concerningly we have some evidence of outliers (given the large x-axis). There are a variety of ways to focus on what is going on, but a quick way is to plot it on a logged scale (remembering that we’d expect any values of 0 to drop away). ggplot(data = delay_2019) + geom_histogram(aes(x = min_delay)) + scale_x_log10() This initial exploration is further hinting at an outlying delay time, so let’s take a look at the largest delays. We need to join this dataset with the ‘delay_codes’ dataset to see what the delay is, and this requires some wrangling because of slightly different codes. delay_2019 &lt;- delay_2019 %&gt;% left_join(delay_codes %&gt;% rename(code = `SUB RMENU CODE`, code_desc = `CODE DESCRIPTION...3`) %&gt;% select(code, code_desc) ) delay_2019 &lt;- delay_2019 %&gt;% mutate(code_srt = ifelse(line==&quot;SRT&quot;, code, &quot;NA&quot;)) %&gt;% left_join(delay_codes %&gt;% rename(code_srt = `SRT RMENU CODE`, code_desc_srt = `CODE DESCRIPTION...7`) %&gt;% select(code_srt, code_desc_srt)) %&gt;% mutate(code = ifelse(code_srt==&quot;NA&quot;, code, code_srt), code_desc = ifelse(is.na(code_desc_srt), code_desc, code_desc_srt)) %&gt;% select(-code_srt, -code_desc_srt) And so we can see that the 455 minute delay was due to ‘Rail Related Problem’ and seems to very much be an outlier. delay_2019 %&gt;% left_join(delay_codes %&gt;% rename(code = `SUB RMENU CODE`, code_desc = `CODE DESCRIPTION...3`) %&gt;% select(code, code_desc)) %&gt;% arrange(-min_delay) %&gt;% select(date, time, station, line, min_delay, code, code_desc) ## # A tibble: 18,697 x 7 ## date time station line min_delay code code_desc ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2019-06-25 00:00:00 18:48 WILSON TO … YU 455 PUTR Rail Related Pro… ## 2 2019-02-12 00:00:00 20:28 LAWRENCE E… SRT 284 MRWEA Weather Reports … ## 3 2019-06-05 00:00:00 12:42 UNION TO S… YU 250 MUPLA Fire/Smoke Plan A ## 4 2019-10-22 00:00:00 14:22 LAWRENCE S… YU 228 PUTS Structure Relate… ## 5 2019-09-26 00:00:00 11:38 YORK MILLS… YU 193 MUPR1 Priority One - T… ## 6 2019-06-08 00:00:00 08:51 SPADINA BD… BD 180 MUPLB Fire/Smoke Plan … ## 7 2019-12-02 00:00:00 06:59 DUNDAS WES… BD 176 MUPLB Fire/Smoke Plan … ## 8 2019-01-29 00:00:00 05:46 VICTORIA P… BD 174 MUWEA Weather Reports … ## 9 2019-02-22 00:00:00 17:32 ELLESMERE … SRT 168 PRW Rail Defect/Fast… ## 10 2019-02-10 00:00:00 07:53 BAYVIEW ST… SHP 165 PUSI Signals or Relat… ## # … with 18,687 more rows 13.2.7 Groups and small counts Another thing that we’re looking for is various groupings of the data, especially where sub-groups may end up with small numbers of observations in them (because any analysis could be easily influenced by them). A quick way to do this is to group the data by a variable that is of interest, for instance ‘line,’ using colour. ggplot(data = delay_2019) + geom_histogram(aes(x = min_delay, y = ..density.., fill = line), position = &#39;dodge&#39;, bins = 10) + scale_x_log10() That plot uses density so that we can look at the the distributions more comparably, but we should also be aware of differences in frequency. In this case, we’ll see that SHP and SRT have much smaller counts. ggplot(data = delay_2019) + geom_histogram(aes(x = min_delay, fill = line), position = &#39;dodge&#39;, bins = 10) + scale_x_log10() To group by a second variable it can be useful to use facets. They’re a little fiddly initially, but once you get used to them, they’re both quick and powerful. ggplot(data = delay_2019) + geom_density(aes(x = min_delay, color = day), bw = .08) + scale_x_log10() + facet_grid(~line) As an aside, the station names are a mess. We could try to quickly bring a little order to the chaos by just taking just the first word (or, the first two if it starts with ‘ST’). delay_2019 &lt;- delay_2019 %&gt;% mutate(station_clean = ifelse(str_starts(station, &quot;ST&quot;), word(station, 1,2), word(station, 1))) We can now plot the top five stations by mean delay. delay_2019 %&gt;% group_by(line, station_clean) %&gt;% summarise(mean_delay = mean(min_delay), n_obs = n()) %&gt;% filter(n_obs&gt;1) %&gt;% arrange(line, -mean_delay) %&gt;% slice(1:5) %&gt;% ggplot(aes(station_clean, mean_delay)) + geom_col() + coord_flip() + facet_wrap(~line, scales = &quot;free_y&quot;) 13.2.8 Visualizing time series Dates are a pain to work with. They always go wrong and give issues, and so they are a critical aspect to explore during EDA. The daily plot of this data is, well, very messy (you can check for yourself). Instead, let’s look by week to see if there’s any seasonality. The lubridate package (Grolemund and Wickham 2011) has a lot of helpful functions that deal with date variables. It’s essentially indispensable. To get started, let’s look at mean delay (of those that were delayed more than zero minutes). delay_2019 %&gt;% filter(min_delay&gt;0) %&gt;% mutate(week = week(date)) %&gt;% group_by(week, line) %&gt;% summarise(mean_delay = mean(min_delay)) %&gt;% ggplot(aes(week, mean_delay, color = line)) + geom_point() + geom_smooth() + facet_grid(~line) Now let’s look at the proportion of delays that were greater than 10 minutes. delay_2019 %&gt;% mutate(week = week(date)) %&gt;% group_by(week, line) %&gt;% summarise(prop_delay = sum(min_delay&gt;10)/n()) %&gt;% ggplot(aes(week, prop_delay, color = line)) + geom_point() + geom_smooth() + facet_grid(~line) Again, and it’s important to be clear here. These charts and tables and analyse have no place in a final report, but what they are doing is allowing you to become comfortable with the data. If you were doing this yourself, you should additionally be making notes about each plot and table as you go, noting the warnings and any implications or aspects to return to. 13.2.9 Visualizing relationships We are also interested in looking at the relationship between two variables. Scatter plots are especially useful here for continuous variables, and are a good precursor to modeling. We saw a little of that with ‘mean_delay’ and ‘week.’ delay_2019 %&gt;% ggplot(aes(x = min_delay, y = min_gap)) + geom_point() + scale_x_log10() + scale_y_log10() The relationship between categorical variables takes more work, but we could also, for instance, look at the top five reasons for delay by station. In particular, we may be interested in whether they differ, and how any difference could be modelled. delay_2019 %&gt;% group_by(line, code_desc) %&gt;% summarise(mean_delay = mean(min_delay)) %&gt;% arrange(-mean_delay) %&gt;% slice(1:5) %&gt;% ggplot(aes(x = code_desc, y = mean_delay)) + geom_col() + facet_wrap(vars(line), scales = &quot;free_y&quot;, nrow = 4) + coord_flip() 13.2.10 Principal components analysis Principal components analysis (PCA) is another powerful exploratory tool. It allows you to pick up potential clusters and/or outliers that can help to inform model building. To see this let’s do a quick (and imperfect) example looking at types of delays by station. The delay categories are a bit of a mess, and there’s hundreds of them. As a simple start, remembering that our task is to come to terms with the dataset as quickly as possible, let’s just take the first word. delay_2019 &lt;- delay_2019 %&gt;% mutate(code_red = case_when( str_starts(code_desc, &quot;No&quot;) ~ word(code_desc, 1, 2), str_starts(code_desc, &quot;Operator&quot;) ~ word(code_desc, 1,2), TRUE ~ word(code_desc,1)) ) Let’s also just restrict the analysis to causes that happen at least 50 times over 2019. To do the PCA, the dataframe also needs to be switched to wide format. dwide &lt;- delay_2019 %&gt;% group_by(line, station_clean) %&gt;% mutate(n_obs = n()) %&gt;% filter(n_obs&gt;1) %&gt;% group_by(code_red) %&gt;% mutate(tot_delay = n()) %&gt;% arrange(tot_delay) %&gt;% filter(tot_delay&gt;50) %&gt;% group_by(line, station_clean, code_red) %&gt;% summarise(n_delay = n()) %&gt;% pivot_wider(names_from = code_red, values_from = n_delay) %&gt;% mutate_all(.funs = funs(ifelse(is.na(.), 0, .))) Now we can quickly do some PCA. delay_pca &lt;- prcomp(dwide[,3:ncol(dwide)]) df_out &lt;- as_tibble(delay_pca$x) df_out &lt;- bind_cols(dwide %&gt;% select(line, station_clean), df_out) head(df_out) ## # A tibble: 6 x 40 ## # Groups: line, station_clean [6] ## line station_clean PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BD BATHURST 6.50 26.9 -2.71 -10.8 -8.40 -11.7 -3.33 -4.11 ## 2 BD BAY 24.8 7.63 -2.19 -7.05 0.714 3.90 -2.29 -4.14 ## 3 BD BLOOR -62.4 -112. 57.3 -23.4 -5.09 -14.1 13.7 5.06 ## 4 BD BROADVIEW -6.60 28.1 -1.06 -14.0 -6.49 -8.29 -6.29 -1.40 ## 5 BD CASTLE 23.8 11.8 -1.31 -7.93 -3.62 -3.37 -2.08 -3.48 ## 6 BD CHESTER 24.6 -1.87 -18.6 2.75 1.85 0.0736 3.79 -1.27 ## # … with 30 more variables: PC9 &lt;dbl&gt;, PC10 &lt;dbl&gt;, PC11 &lt;dbl&gt;, PC12 &lt;dbl&gt;, ## # PC13 &lt;dbl&gt;, PC14 &lt;dbl&gt;, PC15 &lt;dbl&gt;, PC16 &lt;dbl&gt;, PC17 &lt;dbl&gt;, PC18 &lt;dbl&gt;, ## # PC19 &lt;dbl&gt;, PC20 &lt;dbl&gt;, PC21 &lt;dbl&gt;, PC22 &lt;dbl&gt;, PC23 &lt;dbl&gt;, PC24 &lt;dbl&gt;, ## # PC25 &lt;dbl&gt;, PC26 &lt;dbl&gt;, PC27 &lt;dbl&gt;, PC28 &lt;dbl&gt;, PC29 &lt;dbl&gt;, PC30 &lt;dbl&gt;, ## # PC31 &lt;dbl&gt;, PC32 &lt;dbl&gt;, PC33 &lt;dbl&gt;, PC34 &lt;dbl&gt;, PC35 &lt;dbl&gt;, PC36 &lt;dbl&gt;, ## # PC37 &lt;dbl&gt;, PC38 &lt;dbl&gt; We can plot the first two principal components, and add labels to some of the outlying stations. ggplot(df_out,aes(x=PC1,y=PC2,color=line )) + geom_point() + geom_text_repel(data = df_out %&gt;% filter(PC2&gt;100|PC1&lt;100*-1), aes(label = station_clean) ) We could also plot the factor loadings. We se some evidence that perhaps one is to do with the public, compared with another to do with the operator. df_out_r &lt;- as_tibble(delay_pca$rotation) df_out_r$feature &lt;- colnames(dwide[,3:ncol(dwide)]) df_out_r ## # A tibble: 38 x 39 ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.0412 0.0638 0.0133 -0.0467 0.0246 0.0184 -0.00363 0.0198 ## 2 -0.0332 -0.00469 -0.0414 -0.00751 0.0201 -0.0122 -0.0914 -0.0903 ## 3 -0.135 0.207 0.0237 -0.144 0.135 -0.0381 -0.00931 -0.320 ## 4 -0.0652 0.0475 -0.0443 -0.0251 -0.00139 -0.0748 -0.144 -0.428 ## 5 -0.00443 0.00878 -0.0000499 -0.000830 0.00967 0.00954 -0.0160 -0.0144 ## 6 -0.0268 -0.00722 -0.00439 0.000534 -0.0151 -0.0125 -0.00381 -0.0423 ## 7 -0.0813 0.0960 -0.0462 0.0479 -0.0978 -0.0365 -0.0766 0.278 ## 8 -0.0117 0.0135 0.00548 -0.0294 0.0125 0.0377 -0.0790 -0.0321 ## 9 -0.516 0.655 -0.0177 -0.162 -0.221 -0.287 -0.184 0.0465 ## 10 -0.151 0.0826 0.0548 0.352 -0.397 0.281 0.110 0.477 ## # … with 28 more rows, and 31 more variables: PC9 &lt;dbl&gt;, PC10 &lt;dbl&gt;, ## # PC11 &lt;dbl&gt;, PC12 &lt;dbl&gt;, PC13 &lt;dbl&gt;, PC14 &lt;dbl&gt;, PC15 &lt;dbl&gt;, PC16 &lt;dbl&gt;, ## # PC17 &lt;dbl&gt;, PC18 &lt;dbl&gt;, PC19 &lt;dbl&gt;, PC20 &lt;dbl&gt;, PC21 &lt;dbl&gt;, PC22 &lt;dbl&gt;, ## # PC23 &lt;dbl&gt;, PC24 &lt;dbl&gt;, PC25 &lt;dbl&gt;, PC26 &lt;dbl&gt;, PC27 &lt;dbl&gt;, PC28 &lt;dbl&gt;, ## # PC29 &lt;dbl&gt;, PC30 &lt;dbl&gt;, PC31 &lt;dbl&gt;, PC32 &lt;dbl&gt;, PC33 &lt;dbl&gt;, PC34 &lt;dbl&gt;, ## # PC35 &lt;dbl&gt;, PC36 &lt;dbl&gt;, PC37 &lt;dbl&gt;, PC38 &lt;dbl&gt;, feature &lt;chr&gt; ggplot(df_out_r,aes(x=PC1,y=PC2,label=feature )) + geom_text_repel() 13.3 Case study - Opinions about a casino in Toronto This was written by Michael Chong. 13.3.1 Data preparation Here we use the opendatatoronto package again. See the previous case study for a deeper explanation of how the code below works. The dataset I’m extracting below are the results from a survey in 2012 regarding the establishment of a casino in Toronto. More info available by following this link. In this analysis, we’ll be hoping to address the question: which demographic (age/gender) groups are more likely to be supportive of a new casino in Toronto? # Get the data casino_resource &lt;- search_packages(&quot;casino survey&quot;)%&gt;% list_package_resources() %&gt;% filter(name == &quot;toronto-casino-survey-results&quot;) %&gt;% get_resource() head(casino_resource) ## $tblSurvey ## # A tibble: 17,766 x 94 ## SurveyID Q1_A Q1_B1 Q1_B2 Q1_B3 Q2_A Q2_B Q3_A Q3_B Q3_C Q3_D Q3_E ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Stron… Do no… Do no… Do n… Does… &quot;As … Not … Very… Not … Not … Not … ## 2 2 Stron… Econo… Jobs Arts… Fits… &quot;Cos… Very… Very… Very… Very… Very… ## 3 3 Stron… There… If to… &lt;NA&gt; Fits… &quot;Big… Very… Very… Very… Very… Very… ## 4 4 Somew… belie… money… evid… Does… &quot;My … Very… Very… Some… Some… Very… ## 5 5 Neutr… Like … Conce… &lt;NA&gt; Neut… &quot;Aga… Very… Very… Very… Not … Very… ## 6 6 Stron… have … &lt;NA&gt; &lt;NA&gt; Does… &quot;Tor… Not … Not … Not … Not … Not … ## 7 7 Stron… The o… Peopl… We s… Does… &quot;#3 … Not … Not … Not … Not … Not … ## 8 8 Stron… It wi… Moral… &lt;NA&gt; Does… &quot;Cas… Very… Very… Very… Very… Very… ## 9 9 Stron… It&#39;s … traff… heal… Does… &quot;No … Not … Very… Not … Not … Some… ## 10 10 Stron… Toron… Avoid… Prov… Fits… &quot;Tor… Very… Very… Very… Not … Very… ## # … with 17,756 more rows, and 82 more variables: Q3_F &lt;chr&gt;, Q3_G &lt;chr&gt;, ## # Q3_H &lt;chr&gt;, Q3_I &lt;chr&gt;, Q3_J &lt;chr&gt;, Q3_K &lt;chr&gt;, Q3_L &lt;chr&gt;, Q3_M &lt;chr&gt;, ## # Q3_N &lt;chr&gt;, Q3_O &lt;chr&gt;, Q3_P &lt;chr&gt;, Q3_Q &lt;chr&gt;, Q3_Q_Other &lt;chr&gt;, ## # Q3_Comments &lt;chr&gt;, Q4_A &lt;chr&gt;, Q5 &lt;chr&gt;, Q6 &lt;chr&gt;, Q6_Comments &lt;chr&gt;, ## # Q7_A_StandAlone &lt;chr&gt;, Q7_A_Integrated &lt;chr&gt;, Q7_A1 &lt;chr&gt;, Q7_A2 &lt;chr&gt;, ## # Q7_A3 &lt;chr&gt;, Q7_A_A &lt;chr&gt;, Q7_A_B &lt;chr&gt;, Q7_A_C &lt;chr&gt;, Q7_A_D &lt;chr&gt;, ## # Q7_A_E &lt;chr&gt;, Q7_A_F &lt;chr&gt;, Q7_A_G &lt;chr&gt;, Q7_A_H &lt;chr&gt;, Q7_A_I &lt;chr&gt;, ## # Q7_A_J &lt;chr&gt;, Q7_A_J_Other &lt;chr&gt;, Q7_B_StandAlone &lt;chr&gt;, ## # Q7_B_Integrated &lt;chr&gt;, Q7_B1 &lt;chr&gt;, Q7_B2 &lt;chr&gt;, Q7_B3 &lt;chr&gt;, Q7_B_A &lt;chr&gt;, ## # Q7_B_B &lt;chr&gt;, Q7_B_C &lt;chr&gt;, Q7_B_D &lt;chr&gt;, Q7_B_E &lt;chr&gt;, Q7_B_F &lt;chr&gt;, ## # Q7_B_G &lt;chr&gt;, Q7_B_H &lt;chr&gt;, Q7_B_I &lt;chr&gt;, Q7_B_J &lt;chr&gt;, Q7_B_J_Other &lt;chr&gt;, ## # Q7_C_StandAlone &lt;chr&gt;, Q7_C_Integrated &lt;chr&gt;, Q7_C1 &lt;chr&gt;, Q7_C2 &lt;chr&gt;, ## # Q7_C3 &lt;chr&gt;, Q7_C_A &lt;chr&gt;, Q7_C_B &lt;chr&gt;, Q7_C_C &lt;chr&gt;, Q7_C_D &lt;chr&gt;, ## # Q7_C_E &lt;chr&gt;, Q7_C_F &lt;chr&gt;, Q7_C_G &lt;chr&gt;, Q7_C_H &lt;chr&gt;, Q7_C_I &lt;chr&gt;, ## # Q7_C_J &lt;chr&gt;, Q7_C_J_Other &lt;chr&gt;, Q8_A1 &lt;chr&gt;, Q8_A2 &lt;chr&gt;, Q8_B1 &lt;chr&gt;, ## # Q8_B2 &lt;chr&gt;, Q8_B3 &lt;chr&gt;, Q9 &lt;chr&gt;, Q9_Considerations &lt;chr&gt;, Q10 &lt;chr&gt;, ## # Q11 &lt;chr&gt;, Age &lt;chr&gt;, Gender &lt;chr&gt;, PostalCode &lt;chr&gt;, GroupName &lt;chr&gt;, ## # DateCreated &lt;dttm&gt;, ...93 &lt;lgl&gt;, ...94 &lt;lgl&gt; ## ## $Sheet1 ## # A tibble: 0 x 0 The object casino_resource isn’t quite useable yet, because it’s (inconveniently) stored as a list of 2 data frames: # Check what kind of object the casino_resource object is class(casino_resource) ## [1] &quot;list&quot; If we just return the object, we can see that the second list item is empty, and we just want to keep the first one: casino_resource ## $tblSurvey ## # A tibble: 17,766 x 94 ## SurveyID Q1_A Q1_B1 Q1_B2 Q1_B3 Q2_A Q2_B Q3_A Q3_B Q3_C Q3_D Q3_E ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Stron… Do no… Do no… Do n… Does… &quot;As … Not … Very… Not … Not … Not … ## 2 2 Stron… Econo… Jobs Arts… Fits… &quot;Cos… Very… Very… Very… Very… Very… ## 3 3 Stron… There… If to… &lt;NA&gt; Fits… &quot;Big… Very… Very… Very… Very… Very… ## 4 4 Somew… belie… money… evid… Does… &quot;My … Very… Very… Some… Some… Very… ## 5 5 Neutr… Like … Conce… &lt;NA&gt; Neut… &quot;Aga… Very… Very… Very… Not … Very… ## 6 6 Stron… have … &lt;NA&gt; &lt;NA&gt; Does… &quot;Tor… Not … Not … Not … Not … Not … ## 7 7 Stron… The o… Peopl… We s… Does… &quot;#3 … Not … Not … Not … Not … Not … ## 8 8 Stron… It wi… Moral… &lt;NA&gt; Does… &quot;Cas… Very… Very… Very… Very… Very… ## 9 9 Stron… It&#39;s … traff… heal… Does… &quot;No … Not … Very… Not … Not … Some… ## 10 10 Stron… Toron… Avoid… Prov… Fits… &quot;Tor… Very… Very… Very… Not … Very… ## # … with 17,756 more rows, and 82 more variables: Q3_F &lt;chr&gt;, Q3_G &lt;chr&gt;, ## # Q3_H &lt;chr&gt;, Q3_I &lt;chr&gt;, Q3_J &lt;chr&gt;, Q3_K &lt;chr&gt;, Q3_L &lt;chr&gt;, Q3_M &lt;chr&gt;, ## # Q3_N &lt;chr&gt;, Q3_O &lt;chr&gt;, Q3_P &lt;chr&gt;, Q3_Q &lt;chr&gt;, Q3_Q_Other &lt;chr&gt;, ## # Q3_Comments &lt;chr&gt;, Q4_A &lt;chr&gt;, Q5 &lt;chr&gt;, Q6 &lt;chr&gt;, Q6_Comments &lt;chr&gt;, ## # Q7_A_StandAlone &lt;chr&gt;, Q7_A_Integrated &lt;chr&gt;, Q7_A1 &lt;chr&gt;, Q7_A2 &lt;chr&gt;, ## # Q7_A3 &lt;chr&gt;, Q7_A_A &lt;chr&gt;, Q7_A_B &lt;chr&gt;, Q7_A_C &lt;chr&gt;, Q7_A_D &lt;chr&gt;, ## # Q7_A_E &lt;chr&gt;, Q7_A_F &lt;chr&gt;, Q7_A_G &lt;chr&gt;, Q7_A_H &lt;chr&gt;, Q7_A_I &lt;chr&gt;, ## # Q7_A_J &lt;chr&gt;, Q7_A_J_Other &lt;chr&gt;, Q7_B_StandAlone &lt;chr&gt;, ## # Q7_B_Integrated &lt;chr&gt;, Q7_B1 &lt;chr&gt;, Q7_B2 &lt;chr&gt;, Q7_B3 &lt;chr&gt;, Q7_B_A &lt;chr&gt;, ## # Q7_B_B &lt;chr&gt;, Q7_B_C &lt;chr&gt;, Q7_B_D &lt;chr&gt;, Q7_B_E &lt;chr&gt;, Q7_B_F &lt;chr&gt;, ## # Q7_B_G &lt;chr&gt;, Q7_B_H &lt;chr&gt;, Q7_B_I &lt;chr&gt;, Q7_B_J &lt;chr&gt;, Q7_B_J_Other &lt;chr&gt;, ## # Q7_C_StandAlone &lt;chr&gt;, Q7_C_Integrated &lt;chr&gt;, Q7_C1 &lt;chr&gt;, Q7_C2 &lt;chr&gt;, ## # Q7_C3 &lt;chr&gt;, Q7_C_A &lt;chr&gt;, Q7_C_B &lt;chr&gt;, Q7_C_C &lt;chr&gt;, Q7_C_D &lt;chr&gt;, ## # Q7_C_E &lt;chr&gt;, Q7_C_F &lt;chr&gt;, Q7_C_G &lt;chr&gt;, Q7_C_H &lt;chr&gt;, Q7_C_I &lt;chr&gt;, ## # Q7_C_J &lt;chr&gt;, Q7_C_J_Other &lt;chr&gt;, Q8_A1 &lt;chr&gt;, Q8_A2 &lt;chr&gt;, Q8_B1 &lt;chr&gt;, ## # Q8_B2 &lt;chr&gt;, Q8_B3 &lt;chr&gt;, Q9 &lt;chr&gt;, Q9_Considerations &lt;chr&gt;, Q10 &lt;chr&gt;, ## # Q11 &lt;chr&gt;, Age &lt;chr&gt;, Gender &lt;chr&gt;, PostalCode &lt;chr&gt;, GroupName &lt;chr&gt;, ## # DateCreated &lt;dttm&gt;, ...93 &lt;lgl&gt;, ...94 &lt;lgl&gt; ## ## $Sheet1 ## # A tibble: 0 x 0 So, let’s only keep the first item by indexing the list with double square brackets: casino_data &lt;- casino_resource[[1]] Let’s check out what the first couple rows of the dataframe looks like. By default, head() returns the first 6 rows: head(casino_data) ## # A tibble: 6 x 94 ## SurveyID Q1_A Q1_B1 Q1_B2 Q1_B3 Q2_A Q2_B Q3_A Q3_B Q3_C Q3_D Q3_E ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Stron… Do no… Do no… Do no… Does… &quot;As … Not … Very… Not … Not … Not … ## 2 2 Stron… Econo… Jobs Arts … Fits… &quot;Cos… Very… Very… Very… Very… Very… ## 3 3 Stron… There… If to… &lt;NA&gt; Fits… &quot;Big… Very… Very… Very… Very… Very… ## 4 4 Somew… belie… money… evide… Does… &quot;My … Very… Very… Some… Some… Very… ## 5 5 Neutr… Like … Conce… &lt;NA&gt; Neut… &quot;Aga… Very… Very… Very… Not … Very… ## 6 6 Stron… have … &lt;NA&gt; &lt;NA&gt; Does… &quot;Tor… Not … Not … Not … Not … Not … ## # … with 82 more variables: Q3_F &lt;chr&gt;, Q3_G &lt;chr&gt;, Q3_H &lt;chr&gt;, Q3_I &lt;chr&gt;, ## # Q3_J &lt;chr&gt;, Q3_K &lt;chr&gt;, Q3_L &lt;chr&gt;, Q3_M &lt;chr&gt;, Q3_N &lt;chr&gt;, Q3_O &lt;chr&gt;, ## # Q3_P &lt;chr&gt;, Q3_Q &lt;chr&gt;, Q3_Q_Other &lt;chr&gt;, Q3_Comments &lt;chr&gt;, Q4_A &lt;chr&gt;, ## # Q5 &lt;chr&gt;, Q6 &lt;chr&gt;, Q6_Comments &lt;chr&gt;, Q7_A_StandAlone &lt;chr&gt;, ## # Q7_A_Integrated &lt;chr&gt;, Q7_A1 &lt;chr&gt;, Q7_A2 &lt;chr&gt;, Q7_A3 &lt;chr&gt;, Q7_A_A &lt;chr&gt;, ## # Q7_A_B &lt;chr&gt;, Q7_A_C &lt;chr&gt;, Q7_A_D &lt;chr&gt;, Q7_A_E &lt;chr&gt;, Q7_A_F &lt;chr&gt;, ## # Q7_A_G &lt;chr&gt;, Q7_A_H &lt;chr&gt;, Q7_A_I &lt;chr&gt;, Q7_A_J &lt;chr&gt;, Q7_A_J_Other &lt;chr&gt;, ## # Q7_B_StandAlone &lt;chr&gt;, Q7_B_Integrated &lt;chr&gt;, Q7_B1 &lt;chr&gt;, Q7_B2 &lt;chr&gt;, ## # Q7_B3 &lt;chr&gt;, Q7_B_A &lt;chr&gt;, Q7_B_B &lt;chr&gt;, Q7_B_C &lt;chr&gt;, Q7_B_D &lt;chr&gt;, ## # Q7_B_E &lt;chr&gt;, Q7_B_F &lt;chr&gt;, Q7_B_G &lt;chr&gt;, Q7_B_H &lt;chr&gt;, Q7_B_I &lt;chr&gt;, ## # Q7_B_J &lt;chr&gt;, Q7_B_J_Other &lt;chr&gt;, Q7_C_StandAlone &lt;chr&gt;, ## # Q7_C_Integrated &lt;chr&gt;, Q7_C1 &lt;chr&gt;, Q7_C2 &lt;chr&gt;, Q7_C3 &lt;chr&gt;, Q7_C_A &lt;chr&gt;, ## # Q7_C_B &lt;chr&gt;, Q7_C_C &lt;chr&gt;, Q7_C_D &lt;chr&gt;, Q7_C_E &lt;chr&gt;, Q7_C_F &lt;chr&gt;, ## # Q7_C_G &lt;chr&gt;, Q7_C_H &lt;chr&gt;, Q7_C_I &lt;chr&gt;, Q7_C_J &lt;chr&gt;, Q7_C_J_Other &lt;chr&gt;, ## # Q8_A1 &lt;chr&gt;, Q8_A2 &lt;chr&gt;, Q8_B1 &lt;chr&gt;, Q8_B2 &lt;chr&gt;, Q8_B3 &lt;chr&gt;, Q9 &lt;chr&gt;, ## # Q9_Considerations &lt;chr&gt;, Q10 &lt;chr&gt;, Q11 &lt;chr&gt;, Age &lt;chr&gt;, Gender &lt;chr&gt;, ## # PostalCode &lt;chr&gt;, GroupName &lt;chr&gt;, DateCreated &lt;dttm&gt;, ...93 &lt;lgl&gt;, ## # ...94 &lt;lgl&gt; Unfortunately the column names aren’t very informative. For simplicity, we’ll use the ‘.pdf’ questionnaire that accompanies this dataset from the Toronto Open Data website. Alternatively, we could get and parse the ‘readme’ through the R package. Here’s a link to the questionnaire. Question 1 indicates the level of support for a casino in Toronto. We’ll use this as the response variable. Concerning potential predictor variables, most of the questions ask respondents about their opinions on different aspects of a potential casino development, which aren’t particularly useful towards our cause. The only demographic variables are Age and Gender, so let’s choose these. Here I’m also going to rename the columns so that my resulting data frame has columns ‘opinion,’ ‘age,’ and ‘gender.’ # Narrow down the dataframe to our variables of interest casino_data &lt;- casino_data %&gt;% select(Q1_A, Age, Gender) %&gt;% rename(opinion = Q1_A, age = Age, gender = Gender) # Look at first couple rows: head(casino_data) ## # A tibble: 6 x 3 ## opinion age gender ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Strongly Opposed 25-34 Male ## 2 Strongly in Favour 35-44 Female ## 3 Strongly in Favour 55-64 Male ## 4 Somewhat Opposed 25-34 Male ## 5 Neutral or Mixed Feelings 25-34 Female ## 6 Strongly Opposed 45-54 Female 13.3.2 Some visual exploration (and more cleanup, of course) Let’s first do some quick exploration to get a feel for what’s going on in the data. We’ll first calculate proportions of casino support for each age-gender combination: # Calculate proportions casino_summary &lt;- casino_data %&gt;% group_by(age, gender, opinion) %&gt;% summarise(n = n()) %&gt;% # Count the number in each group and response group_by(age, gender) %&gt;% mutate(prop = n/sum(n)) # Calculate proportions within each group Some notes: we use geom_col() to make a bar chart, facet_grid() modifies the plot so that the plot has panels that correspond only to certain values of discrete variables (in this case, we will “facet” by age and gender). This is helpful in this case because we are interested in how the distribution of opinions changes by age and gender. ggplot(casino_summary) + geom_col(aes(x = opinion, y = prop)) + # Specify a histogram of opinion responses facet_grid(age~gender) + #Facet by age and gender theme(axis.text.x = element_text(angle = 90)) # Rotate the x-axis labels to be readable Some things to note: the x-axis labels are out of order in the sense that they are not in a monotone order of increasing/decreasing support there are NA values in opinion, age, and gender, as well as “Prefer not to disclose” responses 13.3.3 Getting the data into a more model-suitable format First we need to get rid of responses that aren’t suitable. For simplicity we’ll assume that NA values and “Prefer not to disclose” responses occur randomly, and remove them from our dataset (note in reality this assumption might not hold up and we might want to be more careful). Let’s check how many rows are in the original dataset: # nrow() returns the number of rows in a dataframe: nrow(casino_data) ## [1] 17766 Now let’s dplyr::filter() accordingly to omit the responses we don’t want. In case you’re unfamiliar, I’m going to make use of: is.na(), which returns TRUE if the argument is NA, the ! operator, which flips TRUE and FALSE. So for instance, !is.na(x) will return TRUE if x is NOT NA, which is what we want to keep. casino_data &lt;- casino_data %&gt;% # Only keep rows with non-NA: filter(!is.na(opinion), !is.na(age), !is.na(gender)) %&gt;% # Only keep rows where age and gender are disclosed: filter(age != &quot;Prefer not to disclose&quot;, gender != &quot;Prefer not to disclose&quot;) Let’s check how many rows of data we’re left with: nrow(casino_data) ## [1] 13658 Now we need to convert the response variable into binary. To clean up the first problem (response variables out of order), we might as well take this opportunity to convert these into a format suitable for our model. In a logistic regression, we would like our response variable to be binary, but in this case we have 5 possible categories ranging from “Strongly Opposed” to “Strongly in Favour.” We’ll recategorize them into a new ‘supportive_or_not’ variable as follows. ‘supportive = 1’ if “Strongly in Favour” or “Somewhat in Favour” ‘supportive = 0’ if “Neutral or Mixed Feelings,” “Somewhat Opposed,” or “Strongly Opposed” We do this with the dplyr::mutate() function, which creates new columns (possibly as functions of existing columns), and dplyr::case_when(), which provides a way to assign values conditional on if-statements. The syntax here is a little strange. On the LHS of the ~ is the “if” condition, and the RHS of the tilde is the value to return. For example, ‘x == 0 ~ 3’ would return 3 when ‘x’ is 0. Another commonly used operator here is the %in% operator, which checks whether something is an element of a vector, for instance, e.g.: 1 %in% c(1, 3, 4) returns TRUE 2 %in% c(1, 3, 4) returns FALSE # Store possible opinions in vectors yes_opinions &lt;- c(&quot;Strongly in Favour&quot;, &quot;Somewhat in Favour&quot;) no_opinions &lt;- c(&quot;Neutral or Mixed Feelings&quot;, &quot;Somewhat Opposed&quot;, &quot;Strongly Opposed&quot;) # Create `supportive` column: casino_data &lt;- casino_data %&gt;% mutate(supportive = case_when( opinion %in% yes_opinions ~ TRUE, # Assign TRUE opinion %in% no_opinions ~ FALSE # Assign FALSE )) Now we need to convert age to a numeric variable. Age in this survey is given in age groups. Let’s instead map it to a numeric variable so that we can more easily talk about trends with age. We’ll map the youngest age to 1, and so on: casino_data &lt;- casino_data %&gt;% mutate(age_group = case_when( age == &quot;Under 15&quot; ~ 0, age == &quot;15-24&quot; ~ 1, age == &quot;25-34&quot; ~ 2, age == &quot;35-44&quot; ~ 3, age == &quot;45-54&quot; ~ 4, age == &quot;55-64&quot; ~ 5, age == &quot;65 or older&quot; ~ 6 )) Now let’s make the same plot again, with our new processed data: casino_summary2 &lt;- casino_data %&gt;% group_by(age_group, gender, supportive) %&gt;% summarise(n = n()) %&gt;% # Count the number in each group and response group_by(age_group, gender) %&gt;% mutate(prop = n/sum(n)) # Calculate proportions within each group ggplot(casino_summary2) + facet_grid(age_group ~ gender) + geom_col(aes(x = supportive, y = prop)) We can sort of see some difference in the distribution between different panels. To formalize this, we can run a logistic regression. 13.3.4 Logistic Regression Now, we’re set up to feed it to the regression. We can do this with glm(), which allows us to fit generalized linear models. We use family = \"binomial\" to specify a logistic regression, and our formula is supportive ~ age_group + gender, which indicates that supportive is the (binary) response variable since it’s on the LHS, and age_group and gender are our predictor variables. casino_glm &lt;- glm(supportive ~ age_group + gender, data = casino_data, family = &quot;binomial&quot;) We can take a look at the results of running the GLM using summary(): summary(casino_glm) ## ## Call: ## glm(formula = supportive ~ age_group + gender, family = &quot;binomial&quot;, ## data = casino_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.0107 -0.8888 -0.6804 1.4249 1.8822 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.10594 0.05863 -18.862 &lt; 2e-16 *** ## age_group -0.07983 0.01376 -5.801 6.59e-09 *** ## genderMale 0.70036 0.04027 17.390 &lt; 2e-16 *** ## genderTransgendered 0.69023 0.39276 1.757 0.0789 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 16010 on 13657 degrees of freedom ## Residual deviance: 15653 on 13654 degrees of freedom ## AIC: 15661 ## ## Number of Fisher Scoring iterations: 4 Interpretation can be a little tricky. Here are some important things to note about our results: Firstly, we have a numeric age group variable. Remember that we coded age_group as numbers 1 to 5. Because we’ve used age groups instead of age, we have to be careful with how we phrase our conclusion. The coefficient estimate corresponds to the effect of moving up a unit on the age group scale (e.g. from the 25-34 age group to the 35-44 age group), rather than 1 year in age (e.g. from age 28 to 29). Secondly, the results are in log-odds ratios. The effect estimates are on the log-odds scale. This means the effect of -0.07983 for age_group is interpreted as: ‘for each unit increase in age_group, we estimate a 0.07983 decrease in the log-odds of being supportive of a casino.’ We could exponentiate the coefficient estimate to make this at least a little easier to interpret. The number we get is interpreted as a factor for the odds. exp(-0.07983) ## [1] 0.9232733 So our (cleaner) interpretation is: ‘the odds of an individuals of the same gender being pro-casino are predicted to change by a factor of 0.9232733 for each unit increase in age_group’ Finally, we have a baseline category. First, note that because we have categorical variables, the gender coefficients are relative to a “baseline” category. The value of gender that doesn’t appear in the table, Female, is implicitly used as our baseline gender category. Technical note: if the variable is stored as a character class, then glm() will choose the alphabetically first value to use as the baseline. exp(0.70036) ## [1] 2.014478 So, the interpretation of the genderMale coefficient is: ‘the odds of a male individual supporting a casino is 2.0144778 times higher than a female individual of the same age_group.’ We can make estimates in a variety of ways. First we’ll look at it manually. Using the formula found in James et al. (2017, 4.3.3), we can make estimates for an individual of certain characteristics. Suppose we wanted to predict the the probability of supporting a Toronto casino for an individual who was 36 and identified as transgender. Then: age_group takes a value of 3, since they are in the age group of 35-44 coded as 3, genderTransgendered takes a value of 1 First, let’s extract the coefficient estimates as a vector using coefficients(): coefs &lt;- coefficients(casino_glm) coefs ## (Intercept) age_group genderMale genderTransgendered ## -1.10593925 -0.07983372 0.70036199 0.69022910 Since this vector is labeled, we can index it using square brackets and names. For instance: coefs[&quot;age_group&quot;] ## age_group ## -0.07983372 So first let’s evaluate the exponent term \\(e^{\\beta_0 + \\cdots + \\beta_p X_p}\\): exp_term &lt;- exp(coefs[&quot;(Intercept)&quot;] + coefs[&quot;age_group&quot;]*3 + coefs[&quot;genderTransgendered&quot;]*1) Now evaluate the expression that gives the probability of casino support: # The unname() command just takes off the label that it &quot;inherited&quot; from the coefs vector. # (don&#39;t worry about it, doesn&#39;t affect any functionality) unname(exp_term / (1 + exp_term)) ## [1] 0.3418161 That works, but there are more stream-lined ways. Thankfully R comes with a convenient function to make prediction estimates from a glm(). We do this using the predict() function. First, we need to make a dataframe that has the relevant variables and values that we’re interested in predicting. We’ll use the same values as before: prediction_df &lt;- data.frame(age_group = 3, gender = &quot;Transgendered&quot;) The dataframe looks like this: prediction_df ## age_group gender ## 1 3 Transgendered Then we feed it into the predict() function, along with our glm object. To get the probability, we need to specify type = \"response\". predict(casino_glm, newdata = prediction_df, type = &quot;response&quot;) ## 1 ## 0.3418161 This matches the probability we got from doing this manually, yay! 13.4 Case study - Airbnb listing in Toronto 13.4.1 Essentials In this case study we look at Airbnb listings in Toronto. 13.4.2 Set up library(broom) # Helps with model outputs etc library(here) # Helps with specifying path names library(janitor) # Helps with initial data cleaning and pretty tables library(tidymodels) # Help with modelling library(tidyverse) library(visdat) # Helps check missing values 13.4.3 Get data The dataset is from Inside Airbnb (Cox 2021). The here package will help (Müller 2017b). We can give read_csv() a link to where the dataset is and it will download it. This helps with reproducibility because the source is clear. But, as that link could change at any time, longer-term reproducibility, as well as wanting to minimise the effect on the Inside Airbnb servers, suggest that we should also save a local copy of the data and then use that. (As the original data is not ours, we should not make that public without first getting written permission.) # For reproducibility # airbnb_data_reduced &lt;- read_csv(&quot;http://data.insideairbnb.com/canada/on/toronto/2021-01-02/data/listings.csv.gz&quot;, guess_max = 20000) # write_csv(airbnb_data_reduced, &quot;week_6/data/airbnb_toronto_2019-12-07.csv&quot;) # For actual work airbnb_data &lt;- read_csv(here::here(&quot;dont_push/airbnb_toronto_2021_january-listings.csv&quot;), guess_max = 20000) # The guess_max option in read_csv helps us avoid having to specify the column types. Usually read_csv takes a best guess at the column types based on the first few rows. But sometimes those first ones are misleading and so guess_max forces it to look at a larger number of rows to try to work out what is going on. 13.4.4 Clean data There are an enormous number of columns, so we’ll just select a few. names(airbnb_data) %&gt;% length() ## [1] 74 airbnb_data_selected &lt;- airbnb_data %&gt;% select(host_id, host_since, host_response_time, host_is_superhost, host_listings_count, host_total_listings_count, host_neighbourhood, host_listings_count, neighbourhood_cleansed, room_type, bathrooms, bedrooms, price, number_of_reviews, has_availability, review_scores_rating, review_scores_accuracy, review_scores_cleanliness, review_scores_checkin, review_scores_communication, review_scores_location, review_scores_value ) We might like to have a brief look at the dataset to see if anything weird is going on. There are a bunch of ways of doing this. A few things jump out: There are a character variables that should probably be numerics or dates/times: host_response_time, price, weekly_price, monthly_price, cleaning_fee. Weekly and monthly price is missing in an overwhelming number of observations. Roughly a fifth of observations are missing a review score and there seems like there is some correlation between those review-type variables. There are two variants of the neighbourhood name. There are NAs in host_is_superhost. The reviews seem really skewed. There is someone who has 328 properties on Airbnb. 13.4.4.1 Price First we need to convert to a numeric. This is a common problem, and you need to be a little careful that it doesn’t all just convert to NAs. In our case if we just force the price data to be a numeric then it will go to NA because there are a lot of characters that R doesn’t know how to convert, e.g. what is the numeric for ‘$?’ So we need to remove those characters first. airbnb_data_selected$price %&gt;% head() ## [1] &quot;$469.00&quot; &quot;$96.00&quot; &quot;$64.00&quot; &quot;$70.00&quot; &quot;$45.00&quot; &quot;$127.00&quot; # First work out what is going on airbnb_data_selected$price %&gt;% str_split(&quot;&quot;) %&gt;% unlist() %&gt;% unique() ## [1] &quot;$&quot; &quot;4&quot; &quot;6&quot; &quot;9&quot; &quot;.&quot; &quot;0&quot; &quot;7&quot; &quot;5&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;8&quot; &quot;,&quot; # It&#39;s clear that &#39;$&#39; needs to go. The only odd thing is &#39;,&#39; so take a look at those: airbnb_data_selected %&gt;% select(price) %&gt;% filter(str_detect(price, &quot;,&quot;)) ## # A tibble: 145 x 1 ## price ## &lt;chr&gt; ## 1 $1,724.00 ## 2 $1,000.00 ## 3 $1,100.00 ## 4 $1,450.00 ## 5 $1,019.00 ## 6 $1,000.00 ## 7 $1,300.00 ## 8 $2,142.00 ## 9 $2,000.00 ## 10 $1,200.00 ## # … with 135 more rows # It&#39;s clear that the data is just nicely formatted, but we need to remove the comma: airbnb_data_selected &lt;- airbnb_data_selected %&gt;% mutate(price = str_remove(price, &quot;\\\\$&quot;), price = str_remove(price, &quot;,&quot;), price = as.integer(price) ) Now we can have a look at prices. # Look at distribution of price airbnb_data_selected %&gt;% ggplot(aes(x = price)) + geom_histogram(binwidth = 10) + theme_classic() + labs(x = &quot;Price per night&quot;, y = &quot;Number of properties&quot;) # We use bins with a width of 10, so that&#39;s going to aggregate prices into 10s so that we don&#39;t get overwhelmed with bars. So we have some outliers. Let’s zoom in on prices that are more than $1,000. # Look at distribution of high prices airbnb_data_selected %&gt;% filter(price &gt; 1000) %&gt;% ggplot(aes(x = price)) + geom_histogram(binwidth = 10) + theme_classic() + labs(x = &quot;Price per night&quot;, y = &quot;Number of properties&quot;) Let’s look in more detail at those with a price more than $4,999. airbnb_data_selected %&gt;% filter(price &gt; 4999) ## # A tibble: 11 x 21 ## host_id host_since host_response_time host_is_superhost host_listings_count ## &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 9310264 2013-10-08 N/A FALSE 1 ## 2 99076885 2016-10-10 N/A FALSE 1 ## 3 119693302 2017-03-07 N/A FALSE 1 ## 4 147240941 2017-08-22 N/A FALSE 2 ## 5 174625477 2018-02-21 N/A FALSE 0 ## 6 70349386 2016-05-04 N/A FALSE 1 ## 7 215966560 2018-09-17 N/A FALSE 1 ## 8 12931053 2014-03-08 within a few hours TRUE 2 ## 9 116137780 2017-02-12 N/A FALSE 6 ## 10 113826425 2017-01-29 within a few hours FALSE 3 ## 11 184607983 2018-04-16 a few days or more FALSE 1 ## # … with 16 more variables: host_total_listings_count &lt;dbl&gt;, ## # host_neighbourhood &lt;chr&gt;, neighbourhood_cleansed &lt;chr&gt;, room_type &lt;chr&gt;, ## # bathrooms &lt;lgl&gt;, bedrooms &lt;dbl&gt;, price &lt;int&gt;, number_of_reviews &lt;dbl&gt;, ## # has_availability &lt;lgl&gt;, review_scores_rating &lt;dbl&gt;, ## # review_scores_accuracy &lt;dbl&gt;, review_scores_cleanliness &lt;dbl&gt;, ## # review_scores_checkin &lt;dbl&gt;, review_scores_communication &lt;dbl&gt;, ## # review_scores_location &lt;dbl&gt;, review_scores_value &lt;dbl&gt; # It&#39;s pretty clear that there is something odd going on with some of these, but some of them seem legit. Let’s look at the distribution of prices that are in a ‘reasonable’ range, which until Monica is a full professor, will be defined as a nightly price of less than $1,000. airbnb_data_selected %&gt;% filter(price &lt; 1000) %&gt;% ggplot(aes(x = price)) + geom_histogram(binwidth = 10) + theme_classic() + labs(x = &quot;Price per night&quot;, y = &quot;Number of properties&quot;) Interestingly it looks like there is some bunching of prices, possible around numbers ending in zero or nine? Let’s just zoom in on prices between $90 and $210, out of interest, but change the bins to be smaller. # Look at distribution of price again airbnb_data_selected %&gt;% filter(price &gt; 90) %&gt;% filter(price &lt; 210) %&gt;% ggplot(aes(x = price)) + geom_histogram(binwidth = 1) + theme_classic() + labs(x = &quot;Price per night&quot;, y = &quot;Number of properties&quot;) 13.4.4.2 Superhosts Airbnb says that: Superhosts are experienced hosts who provide a shining example for other hosts, and extraordinary experiences for their guests. Once a host reaches Superhost status, a badge superhost badge will automatically appear on their listing and profile to help you identify them. We check Superhosts’ activity four times a year, to ensure that the program highlights the people who are most dedicated to providing outstanding hospitality. First we’ll look at the NAs in host_is_superhost. airbnb_data_selected %&gt;% filter(is.na(host_is_superhost)) ## # A tibble: 11 x 21 ## host_id host_since host_response_time host_is_superhost host_listings_count ## &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 23472830 NA &lt;NA&gt; NA NA ## 2 31675651 NA &lt;NA&gt; NA NA ## 3 75779190 NA &lt;NA&gt; NA NA ## 4 47614482 NA &lt;NA&gt; NA NA ## 5 201103629 NA &lt;NA&gt; NA NA ## 6 111820893 NA &lt;NA&gt; NA NA ## 7 23472830 NA &lt;NA&gt; NA NA ## 8 196269219 NA &lt;NA&gt; NA NA ## 9 23472830 NA &lt;NA&gt; NA NA ## 10 266594170 NA &lt;NA&gt; NA NA ## 11 118516038 NA &lt;NA&gt; NA NA ## # … with 16 more variables: host_total_listings_count &lt;dbl&gt;, ## # host_neighbourhood &lt;chr&gt;, neighbourhood_cleansed &lt;chr&gt;, room_type &lt;chr&gt;, ## # bathrooms &lt;lgl&gt;, bedrooms &lt;dbl&gt;, price &lt;int&gt;, number_of_reviews &lt;dbl&gt;, ## # has_availability &lt;lgl&gt;, review_scores_rating &lt;dbl&gt;, ## # review_scores_accuracy &lt;dbl&gt;, review_scores_cleanliness &lt;dbl&gt;, ## # review_scores_checkin &lt;dbl&gt;, review_scores_communication &lt;dbl&gt;, ## # review_scores_location &lt;dbl&gt;, review_scores_value &lt;dbl&gt; There are 285 of these and it’s clear that there is something odd going on - maybe the host removed the listing or similar? We’ll also want to create a binary variable out of this. It’s true/false at the moment, which is fine for the modelling, but there are a handful of situations where it’ll be easier if we have a 0/1. airbnb_data_selected &lt;- airbnb_data_selected %&gt;% mutate(host_is_superhost_binary = case_when( host_is_superhost == TRUE ~ 1, host_is_superhost == FALSE ~ 0, TRUE ~ 999 ) ) airbnb_data_selected$host_is_superhost_binary[airbnb_data_selected$host_is_superhost_binary == 999] &lt;- NA 13.4.4.3 Reviews Airbnb says that: In addition to written reviews, guests can submit an overall star rating and a set of category star ratings for their stay. Hosts can view their star ratings on their Progress page, under Reviews. Hosts using professional hosting tools can find reviews and quality details on their Performance page, under Quality. Guests can give ratings on: Overall experience. Overall, how was the stay? Cleanliness. Did guests feel that the space was clean and tidy? Accuracy. How accurately did the listing page represent the space? For example, guests should be able to find up-to-date info and photos in the listing description. Value. Did the guest feel that the listing provided good value for the price? Communication. How well did you communicate before and during the stay? Guests often care that their host responds quickly, reliably, and frequently to their messages and questions. Check-in. How smoothly did check-in go? Location. How did guests feel about the neighbourhood? This may mean that there’s an accurate description for proximity and access to transportation, shopping centres, city centre, etc., and a description that includes special considerations, like noise, and family safety. Amenities. How did guests feel about the amenities that were available during their stay? Guests often care that all the amenities listed are available, working, and in good condition. In each category, hosts are able to see how often they get 5 stars, how guests rated nearby hosts, and, in some cases, tips to help improve the listing. The number of stars displayed at the top of a listing page is an aggregate of the primary scores guests have given for that listing. At the bottom of a listing page, there’s an aggregate for each category rating. A host needs to receive star ratings from at least 3 guests before their aggregate score appears. TODO: I don’t understand how these review scores are being constructed. Airbnb says it’s a star rating, but how are they converting this into the 10 point scale, similarly, how are their constructing the overall one, which seems to be out of 100? There’s a lot of clumping around 20, 40, 60, 80, 100 - could they be averaging a five-star scale and then rebasing it to 100? Now we’ll deal with the NAs in review_scores_rating. This one is more complicated as there are a lot of them. airbnb_data_selected %&gt;% filter(is.na(review_scores_rating)) ## # A tibble: 4,368 x 22 ## host_id host_since host_response_time host_is_superhost host_listings_count ## &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 48239 2009-10-25 N/A FALSE 1 ## 2 187320 2010-08-01 within a few hours TRUE 13 ## 3 188183 2010-08-01 a few days or more FALSE 1 ## 4 187320 2010-08-01 within a few hours TRUE 13 ## 5 304551 2010-11-29 within an hour TRUE 6 ## 6 545074 2011-04-29 N/A FALSE 2 ## 7 1210571 2011-09-26 N/A FALSE 1 ## 8 1411076 2011-11-15 N/A FALSE 1 ## 9 1409872 2011-11-15 N/A FALSE 1 ## 10 1664812 2012-01-28 N/A FALSE 1 ## # … with 4,358 more rows, and 17 more variables: ## # host_total_listings_count &lt;dbl&gt;, host_neighbourhood &lt;chr&gt;, ## # neighbourhood_cleansed &lt;chr&gt;, room_type &lt;chr&gt;, bathrooms &lt;lgl&gt;, ## # bedrooms &lt;dbl&gt;, price &lt;int&gt;, number_of_reviews &lt;dbl&gt;, ## # has_availability &lt;lgl&gt;, review_scores_rating &lt;dbl&gt;, ## # review_scores_accuracy &lt;dbl&gt;, review_scores_cleanliness &lt;dbl&gt;, ## # review_scores_checkin &lt;dbl&gt;, review_scores_communication &lt;dbl&gt;, ## # review_scores_location &lt;dbl&gt;, review_scores_value &lt;dbl&gt;, ## # host_is_superhost_binary &lt;dbl&gt; Now see if it’s just because they don’t have any reviews. airbnb_data_selected %&gt;% filter(is.na(review_scores_rating)) %&gt;% select(number_of_reviews) %&gt;% table() ## . ## 0 1 2 3 4 ## 4105 227 23 10 3 So it’s clear that in almost all these cases they don’t have a review yet because they don’t have enough reviews. However, it’s a large proportion of the total - almost a fifth of properties don’t have any reviews (hence an NA in review_scores_rating). We can use vis_miss from the visdat package (Tierney 2017) to make sure that all components of the review are missing. If the NAs are being driven by the Airbnb requirement of at least three reviews then we would expect they would all be missing. # We&#39;ll just check whether this is the same for all of the different variants of reviews airbnb_data_selected %&gt;% select(review_scores_rating, review_scores_accuracy, review_scores_cleanliness, review_scores_checkin, review_scores_communication, review_scores_location, review_scores_value) %&gt;% vis_miss() It looks pretty convincing that in almost all cases, all the different variants of reviews are missing. So let’s just focus on the main review. airbnb_data_selected %&gt;% filter(!is.na(review_scores_rating)) %&gt;% ggplot(aes(x = review_scores_rating)) + geom_histogram(binwidth = 1) + theme_classic() + labs(x = &quot;Average review score&quot;, y = &quot;Number of properties&quot;) It’s pretty clear that almost all the reviews are more than 80. Let’s just zoom in on that 60 to 80 range to check what the distribution looks like in that range. airbnb_data_selected %&gt;% filter(!is.na(review_scores_rating)) %&gt;% filter(review_scores_rating &gt; 60) %&gt;% filter(review_scores_rating &lt; 80) %&gt;% ggplot(aes(x = review_scores_rating)) + geom_histogram(binwidth = 1) + theme_classic() + labs(x = &quot;Average review score&quot;, y = &quot;Number of properties&quot;) 13.4.4.4 Response time Airbnb says that: Hosts have 24 hours to officially accept or decline reservation requests. You’ll be updated by email about the status of your request. More than half of all reservation requests are accepted within one hour of being received. The vast majority of hosts reply within 12 hours. If a host confirms your request, your payment is processed and collected by Airbnb in full. If a host declines your request or the request expires, we don’t process your payment. TODO: I don’t understand how you can get a response time of NA? It must be related to some other variable. Looking now at response time: table(airbnb_data_selected$host_response_time) ## ## a few days or more N/A within a day within a few hours ## 816 8469 1235 2062 ## within an hour ## 5672 Interestingly it seems like what looks like ‘NAs’ in the host_response_time variable are not being coded as proper NAs, but are instead being treated as another category. We’ll recode them to be actual NAs. airbnb_data_selected$host_response_time[airbnb_data_selected$host_response_time == &quot;N/A&quot;] &lt;- NA So here we clearly have issues with NAs. We probably want to filter them away for this example because it’s just a quick example, but there are an awful lot of them (more than 20 per cent) so we’ll have a quick look at them in relation to the review score. airbnb_data_selected %&gt;% filter(is.na(host_response_time)) %&gt;% ggplot(aes(x = review_scores_rating)) + geom_histogram(binwidth = 1) There seem to be an awful lot that have an overall review of 100. There are also an awful lot that have a review score of NA. airbnb_data_selected %&gt;% filter(is.na(host_response_time)) %&gt;% filter(is.na(review_scores_rating)) ## # A tibble: 2,590 x 22 ## host_id host_since host_response_time host_is_superhost host_listings_count ## &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 48239 2009-10-25 &lt;NA&gt; FALSE 1 ## 2 545074 2011-04-29 &lt;NA&gt; FALSE 2 ## 3 1210571 2011-09-26 &lt;NA&gt; FALSE 1 ## 4 1411076 2011-11-15 &lt;NA&gt; FALSE 1 ## 5 1409872 2011-11-15 &lt;NA&gt; FALSE 1 ## 6 1664812 2012-01-28 &lt;NA&gt; FALSE 1 ## 7 1828773 2012-02-28 &lt;NA&gt; FALSE 1 ## 8 1923052 2012-03-14 &lt;NA&gt; FALSE 1 ## 9 2432916 2012-05-22 &lt;NA&gt; FALSE 1 ## 10 2577688 2012-06-07 &lt;NA&gt; FALSE 1 ## # … with 2,580 more rows, and 17 more variables: ## # host_total_listings_count &lt;dbl&gt;, host_neighbourhood &lt;chr&gt;, ## # neighbourhood_cleansed &lt;chr&gt;, room_type &lt;chr&gt;, bathrooms &lt;lgl&gt;, ## # bedrooms &lt;dbl&gt;, price &lt;int&gt;, number_of_reviews &lt;dbl&gt;, ## # has_availability &lt;lgl&gt;, review_scores_rating &lt;dbl&gt;, ## # review_scores_accuracy &lt;dbl&gt;, review_scores_cleanliness &lt;dbl&gt;, ## # review_scores_checkin &lt;dbl&gt;, review_scores_communication &lt;dbl&gt;, ## # review_scores_location &lt;dbl&gt;, review_scores_value &lt;dbl&gt;, ## # host_is_superhost_binary &lt;dbl&gt; 13.4.4.5 Host number of listings There are two versions of a variable telling you how many properties a host has on Airbnb, so to start just check whether there’s a difference. airbnb_data_selected %&gt;% mutate(listings_count_is_same = if_else(host_listings_count == host_total_listings_count, 1, 0)) %&gt;% filter(listings_count_is_same == 0) ## # A tibble: 0 x 23 ## # … with 23 variables: host_id &lt;dbl&gt;, host_since &lt;date&gt;, ## # host_response_time &lt;chr&gt;, host_is_superhost &lt;lgl&gt;, ## # host_listings_count &lt;dbl&gt;, host_total_listings_count &lt;dbl&gt;, ## # host_neighbourhood &lt;chr&gt;, neighbourhood_cleansed &lt;chr&gt;, room_type &lt;chr&gt;, ## # bathrooms &lt;lgl&gt;, bedrooms &lt;dbl&gt;, price &lt;int&gt;, number_of_reviews &lt;dbl&gt;, ## # has_availability &lt;lgl&gt;, review_scores_rating &lt;dbl&gt;, ## # review_scores_accuracy &lt;dbl&gt;, review_scores_cleanliness &lt;dbl&gt;, ## # review_scores_checkin &lt;dbl&gt;, review_scores_communication &lt;dbl&gt;, ## # review_scores_location &lt;dbl&gt;, review_scores_value &lt;dbl&gt;, ## # host_is_superhost_binary &lt;dbl&gt;, listings_count_is_same &lt;dbl&gt; There are none in this dataset so we can just remove one column for now and have a quick look at the other one. airbnb_data_selected &lt;- airbnb_data_selected %&gt;% select(-host_listings_count) airbnb_data_selected %&gt;% count(host_total_listings_count) ## # A tibble: 49 x 2 ## host_total_listings_count n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 2128 ## 2 1 7662 ## 3 2 2476 ## 4 3 1384 ## 5 4 802 ## 6 5 478 ## 7 6 385 ## 8 7 270 ## 9 8 291 ## 10 9 170 ## # … with 39 more rows So there are a large number who have somewhere in the 2-10 properties range, but the usual long tail. The number with 0 listings is unexpected and worth following up on. And there are a bunch with NA that we’ll need to deal with. airbnb_data_selected %&gt;% filter(host_total_listings_count == 0) %&gt;% head() ## # A tibble: 6 x 21 ## host_id host_since host_response_time host_is_superhost host_total_listings_c… ## &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 140602 2010-06-08 &lt;NA&gt; FALSE 0 ## 2 1024550 2011-08-26 &lt;NA&gt; FALSE 0 ## 3 2647656 2012-06-15 &lt;NA&gt; FALSE 0 ## 4 3783106 2012-10-06 within an hour FALSE 0 ## 5 3814089 2012-10-09 within an hour FALSE 0 ## 6 3827668 2012-10-10 within a day FALSE 0 ## # … with 16 more variables: host_neighbourhood &lt;chr&gt;, ## # neighbourhood_cleansed &lt;chr&gt;, room_type &lt;chr&gt;, bathrooms &lt;lgl&gt;, ## # bedrooms &lt;dbl&gt;, price &lt;int&gt;, number_of_reviews &lt;dbl&gt;, ## # has_availability &lt;lgl&gt;, review_scores_rating &lt;dbl&gt;, ## # review_scores_accuracy &lt;dbl&gt;, review_scores_cleanliness &lt;dbl&gt;, ## # review_scores_checkin &lt;dbl&gt;, review_scores_communication &lt;dbl&gt;, ## # review_scores_location &lt;dbl&gt;, review_scores_value &lt;dbl&gt;, ## # host_is_superhost_binary &lt;dbl&gt; There’s nothing that immediately jumps out as odd about the people with zero listings, but there must be something going on. Based on this dataset, there’s a third way of looking at the number of properties someone has and that’s to look at the number of times the unique ID occurs. airbnb_data_selected %&gt;% count(host_id) %&gt;% arrange(-n) %&gt;% head() ## # A tibble: 6 x 2 ## host_id n ## &lt;dbl&gt; &lt;int&gt; ## 1 10202618 74 ## 2 1919294 63 ## 3 152088065 59 ## 4 293274089 56 ## 5 785826 54 ## 6 846505 46 Again this makes it clear that there are many with multiple properties listed. 13.4.4.6 Decisions The purpose of this document is to just give a quick introduction to using real-world data, so we’ll just remove anything that is annoying, but if you’re using this for research then you’d need to justify these decisions and/or possibly make different ones. Get rid of prices more than $999. airbnb_data_filtered &lt;- airbnb_data_selected %&gt;% filter(price &lt; 1000) dim(airbnb_data_filtered) ## [1] 18120 21 Get rid of anyone with an NA for whether they are a super host. # Just remove host_is_superhost NAs for now. airbnb_data_filtered &lt;- airbnb_data_filtered %&gt;% filter(!is.na(host_is_superhost)) dim(airbnb_data_filtered) ## [1] 18109 21 Get rid of anyone with an NA in their main review score - this removes roughly 20 per cent of observations. # We&#39;ll just get rid of them for now, but this is probably something that deserves more attention - possibly in an appendix or similar. airbnb_data_filtered &lt;- airbnb_data_filtered %&gt;% filter(!is.na(review_scores_rating)) # There are still some where the rest of the reviews are missing even though there is a main review score # There seem to be an awful lot that have an overall review of 100. Does that make sense? dim(airbnb_data_filtered) ## [1] 13801 21 Get rid of anyone with a main review score less than 70. # We&#39;ll just get rid of them for now, but this is probably something that deserves more attention - possibly in an appendix or similar. airbnb_data_filtered &lt;- airbnb_data_filtered %&gt;% filter(review_scores_rating &gt; 69) # There are still some where the rest of the reviews are missing even though there is a main review score # There seem to be an awful lot that have an overall review of 100. Does that make sense? dim(airbnb_data_filtered) ## [1] 13471 21 Get rid of anyone with a NA in their response time - this removes roughly another 20 per cent of the observations. airbnb_data_filtered &lt;- airbnb_data_filtered %&gt;% filter(!is.na(host_response_time)) dim(airbnb_data_filtered) ## [1] 7763 21 TODO: We don’t have to do this next step as we’ve already got rid of them at some other point. So there’s something systematic going on and we should come back and look into it. Get rid of anyone with a NA in their number of properties. airbnb_data_filtered &lt;- airbnb_data_filtered %&gt;% filter(!is.na(host_total_listings_count)) dim(airbnb_data_filtered) ## [1] 7763 21 Get rid of anyone with a 100 for their review_scores_rating. airbnb_data_filtered &lt;- airbnb_data_filtered %&gt;% filter(review_scores_rating != 100) dim(airbnb_data_filtered) ## [1] 5648 21 Only keep people with one property: airbnb_data_filtered &lt;- airbnb_data_filtered %&gt;% add_count(host_id) %&gt;% filter(n == 1) %&gt;% select(-n) dim(airbnb_data_filtered) ## [1] 2304 21 13.4.5 Explore data We might like to make some graphs to see if we can any relationships jump out. Some aspects that come to mind is looking at prices and reviews and super hosts, and number of properties and neighbourhood. 13.4.5.1 Price and reviews Look at the relationship between price and reviews, and whether they are a super-host. # Look at both price and reviews airbnb_data_filtered %&gt;% ggplot(aes(x = price, y = review_scores_rating, color = host_is_superhost)) + geom_point(size = 1, alpha = 0.1) + # Make the points smaller and more transparent as they overlap considerably. theme_classic() + labs(x = &quot;Price per night&quot;, y = &quot;Average review score&quot;, color = &quot;Super host&quot;) + # Probably should recode this to more meaningful than TRUE/FALSE. scale_color_brewer(palette = &quot;Set1&quot;) 13.4.5.2 Superhost and response-time One of the aspects that may make someone a super host is how quickly they respond to enquiries. One could imagine that being a superhost involves quickly saying yes or no to enquiries. Let’s look at the data. First, we want to look at the possible values of superhost by their response times. airbnb_data_filtered %&gt;% tabyl(host_is_superhost) %&gt;% adorn_totals(&quot;row&quot;) %&gt;% adorn_pct_formatting() ## host_is_superhost n percent ## FALSE 1224 53.1% ## TRUE 1080 46.9% ## Total 2304 100.0% Fortunately, it looks like when we removed the reviews rows we removed any NAs from whether they were a super host, but if we go back and look into that we may need to check again. The tabyl function within the janitor package (Firke, 2020) would list the NAs if there were any, but in case you don’t trust it, another way of check this is to try to filter to just the NAs. airbnb_data_filtered %&gt;% filter(is.na(host_is_superhost)) ## # A tibble: 0 x 21 ## # … with 21 variables: host_id &lt;dbl&gt;, host_since &lt;date&gt;, ## # host_response_time &lt;chr&gt;, host_is_superhost &lt;lgl&gt;, ## # host_total_listings_count &lt;dbl&gt;, host_neighbourhood &lt;chr&gt;, ## # neighbourhood_cleansed &lt;chr&gt;, room_type &lt;chr&gt;, bathrooms &lt;lgl&gt;, ## # bedrooms &lt;dbl&gt;, price &lt;int&gt;, number_of_reviews &lt;dbl&gt;, ## # has_availability &lt;lgl&gt;, review_scores_rating &lt;dbl&gt;, ## # review_scores_accuracy &lt;dbl&gt;, review_scores_cleanliness &lt;dbl&gt;, ## # review_scores_checkin &lt;dbl&gt;, review_scores_communication &lt;dbl&gt;, ## # review_scores_location &lt;dbl&gt;, review_scores_value &lt;dbl&gt;, ## # host_is_superhost_binary &lt;dbl&gt; Now let’s look at the response time. airbnb_data_filtered %&gt;% tabyl(host_response_time) %&gt;% adorn_totals(&quot;row&quot;) %&gt;% adorn_pct_formatting() ## host_response_time n percent ## a few days or more 167 7.2% ## within a day 378 16.4% ## within a few hours 519 22.5% ## within an hour 1240 53.8% ## Total 2304 100.0% So a vast majority respond within an hour. Finally, we can look at the cross-tab. airbnb_data_filtered %&gt;% tabyl(host_response_time, host_is_superhost) %&gt;% adorn_percentages(&quot;row&quot;) %&gt;% adorn_pct_formatting(digits = 0) %&gt;% adorn_ns() %&gt;% adorn_title() ## host_is_superhost ## host_response_time FALSE TRUE ## a few days or more 88% (147) 12% (20) ## within a day 67% (254) 33% (124) ## within a few hours 51% (266) 49% (253) ## within an hour 45% (557) 55% (683) So if someone doesn’t respond within an hour then it’s unlikely that they are a super host. 13.4.5.3 Neighbourhood Finally, let’s look at neighbourhood. The data provider has attempted to clean the neighbourhood variable for us, so we’ll just use this for now. If we were doing this analysis properly, we’d need to check whether they’d made any mistakes. # We expect something in the order of 100 to 150 neighbourhoods, with the top ten accounting for a large majority of listings. airbnb_data_filtered %&gt;% tabyl(neighbourhood_cleansed) %&gt;% adorn_totals(&quot;row&quot;) %&gt;% adorn_pct_formatting() %&gt;% nrow() ## [1] 140 airbnb_data_filtered %&gt;% tabyl(neighbourhood_cleansed) %&gt;% adorn_pct_formatting() %&gt;% arrange(-n) %&gt;% filter(n &gt; 100) %&gt;% adorn_totals(&quot;row&quot;) %&gt;% head() ## neighbourhood_cleansed n percent ## Waterfront Communities-The Island 409 17.8% ## Niagara 101 4.4% ## Total 510 - 13.4.6 Model data We will now run some models on our dataset. We will first split the data into test/training groups, we do this using functions from the tidymodels package (Kuhn and Wickham 2020) (which like the tidyverse package (Wickham et al. 2019b) is a package of packages). set.seed(853) airbnb_data_filtered_split &lt;- airbnb_data_filtered %&gt;% initial_split(prop = 3/4) airbnb_train &lt;- training(airbnb_data_filtered_split) airbnb_test &lt;- testing(airbnb_data_filtered_split) rm(airbnb_data_filtered_split) 13.4.6.1 Logistic regression We may like to look at whether we can forecast whether someone is a super host, and the factors that go into explaining that. As the dependent variable is binary, this is a good opportunity to look at logistic regression. We expect that better reviews will be associated with faster responses and higher reviews. Specifically, the model that we estimate is: \\[\\mbox{Prob(Is super host} = 1) = \\beta_0 + \\beta_1 \\mbox{Response time} + \\beta_2 \\mbox{Reviews} + \\epsilon\\] We estimate the model using glm in the R language (R Core Team 2020). logistic_reg_superhost_response_review &lt;- glm(host_is_superhost ~ host_response_time + review_scores_rating, data = airbnb_train, family = binomial ) We can have a quick look at the results, for instance, the summary function. We could also use tidy or glance from the broom package (D. Robinson, Hayes, and Couch 2020). The details should be the same, but the broom functions are tibbles, which means that we can more easily deal with them within a tidy framework. summary(logistic_reg_superhost_response_review) ## ## Call: ## glm(formula = host_is_superhost ~ host_response_time + review_scores_rating, ## family = binomial, data = airbnb_train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8952 -0.8615 -0.0573 0.8621 4.0356 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -40.77616 2.49677 -16.332 &lt; 2e-16 *** ## host_response_timewithin a day 1.03353 0.32344 3.195 0.0014 ** ## host_response_timewithin a few hours 1.73271 0.31243 5.546 2.92e-08 *** ## host_response_timewithin an hour 2.00654 0.30004 6.688 2.27e-11 *** ## review_scores_rating 0.40792 0.02564 15.910 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2385.4 on 1727 degrees of freedom ## Residual deviance: 1752.0 on 1723 degrees of freedom ## AIC: 1762 ## ## Number of Fisher Scoring iterations: 6 tidy(logistic_reg_superhost_response_review) ## # A tibble: 5 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -40.8 2.50 -16.3 5.89e-60 ## 2 host_response_timewithin a day 1.03 0.323 3.20 1.40e- 3 ## 3 host_response_timewithin a few hours 1.73 0.312 5.55 2.92e- 8 ## 4 host_response_timewithin an hour 2.01 0.300 6.69 2.27e-11 ## 5 review_scores_rating 0.408 0.0256 15.9 5.39e-57 glance(logistic_reg_superhost_response_review) ## # A tibble: 1 x 8 ## null.deviance df.null logLik AIC BIC deviance df.residual nobs ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2385. 1727 -876. 1762. 1789. 1752. 1723 1728 We might like to look at what our model predicts, compared with whether the person was actually a super host. We can do that in a variety of ways, but one way is to use augment from the broom package (D. Robinson, Hayes, and Couch 2020). This will add the prediction and associated uncertainty to the data. For every row we will then have the probability that our model is estimating that they are a superhost. But ultimately, we need a binary forecast. There are a bunch of different options, but one is to just say that if the model estimates a probability of more than 0.5 then we bin it into a superhost, and other not. airbnb_data_filtered_logistic_fit_train &lt;- augment(logistic_reg_superhost_response_review, data = airbnb_train %&gt;% select(host_is_superhost, host_is_superhost_binary, host_response_time, review_scores_rating ), type.predict = &quot;response&quot;) %&gt;% # We use the &quot;response&quot; option here so that the function does the work of worrying about the exponential and log odds for us. Our result will be a probability. select(-.hat, -.sigma, -.cooksd, -.std.resid) %&gt;% mutate(predict_host_is_superhost = if_else(.fitted &gt; 0.5, 1, 0), # How do things change if we change the 0.5 cutoff? host_is_superhost_binary = as.factor(host_is_superhost_binary), predict_host_is_superhost_binary = as.factor(predict_host_is_superhost) ) We can look at how far off the model is. There are a bunch of ways of doing this, but one is to look at what probability the model has given each person. airbnb_data_filtered_logistic_fit_train %&gt;% ggplot(aes(x = .fitted, fill = host_is_superhost_binary)) + geom_histogram(binwidth = 0.05, position = &quot;dodge&quot;) + theme_classic() + labs(x = &quot;Estimated probability that host is super host&quot;, y = &quot;Number&quot;, fill = &quot;Host is super host&quot;) + scale_fill_brewer(palette = &quot;Set1&quot;) We can look at how the model probabilities change based on average review score, and their average time to respond. ggplot(airbnb_data_filtered_logistic_fit_train, aes(x = review_scores_rating, y = .fitted, color = host_response_time)) + geom_line() + geom_point() + labs(x = &quot;Average review score&quot;, y = &quot;Predicted probability of being a superhost&quot;, color = &quot;Host response time&quot;) + theme_classic() + scale_color_brewer(palette = &quot;Set1&quot;) This nice thing about this graph is that it illustrates nicely the effect of a host having an average response time of, say, ‘within an hour’ compared with ‘within a few hours.’ We can focus on how the model does in terms of raw classification using confusionMatrix from the caret package (Kuhn, 2020). This also gives a bunch of diagnostics (the help file explains what they are). In general, they suggest this isn’t the best model that’s ever existed. caret::confusionMatrix(data = airbnb_data_filtered_logistic_fit_train$predict_host_is_superhost_binary, reference = airbnb_data_filtered_logistic_fit_train$host_is_superhost_binary) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 687 178 ## 1 243 620 ## ## Accuracy : 0.7564 ## 95% CI : (0.7354, 0.7764) ## No Information Rate : 0.5382 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.5127 ## ## Mcnemar&#39;s Test P-Value : 0.001814 ## ## Sensitivity : 0.7387 ## Specificity : 0.7769 ## Pos Pred Value : 0.7942 ## Neg Pred Value : 0.7184 ## Prevalence : 0.5382 ## Detection Rate : 0.3976 ## Detection Prevalence : 0.5006 ## Balanced Accuracy : 0.7578 ## ## &#39;Positive&#39; Class : 0 ## In any case, to this point we’ve been looking at how the model has done on the training set. It’s also relevant how it does on the test set. Again, there are a bunch of ways to do this, but one is to again use the augment function, but to include a newdata argument. airbnb_data_filtered_logistic_fit_test &lt;- augment(logistic_reg_superhost_response_review, data = airbnb_train %&gt;% select(host_is_superhost, host_is_superhost_binary, host_response_time, review_scores_rating ), newdata = airbnb_test %&gt;% select(host_is_superhost, host_is_superhost_binary, host_response_time, review_scores_rating ), # I&#39;m selecting just because the # dataset is quite wide, and so this makes it easier to look at. type.predict = &quot;response&quot;) %&gt;% mutate(predict_host_is_superhost = if_else(.fitted &gt; 0.5, 1, 0), host_is_superhost_binary = as.factor(host_is_superhost_binary), predict_host_is_superhost_binary = as.factor(predict_host_is_superhost) ) We would expect the performance to be slightly worse on the test set. But it’s actually fairly similar. caret::confusionMatrix(data = airbnb_data_filtered_logistic_fit_test$predict_host_is_superhost_binary, reference = airbnb_data_filtered_logistic_fit_test$host_is_superhost_binary) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 227 60 ## 1 67 222 ## ## Accuracy : 0.7795 ## 95% CI : (0.7434, 0.8127) ## No Information Rate : 0.5104 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.5591 ## ## Mcnemar&#39;s Test P-Value : 0.5944 ## ## Sensitivity : 0.7721 ## Specificity : 0.7872 ## Pos Pred Value : 0.7909 ## Neg Pred Value : 0.7682 ## Prevalence : 0.5104 ## Detection Rate : 0.3941 ## Detection Prevalence : 0.4983 ## Balanced Accuracy : 0.7797 ## ## &#39;Positive&#39; Class : 0 ## We could compare the test with the training sets in terms of forecasts. training &lt;- airbnb_data_filtered_logistic_fit_train %&gt;% select(host_is_superhost_binary, .fitted) %&gt;% mutate(type = &quot;Training set&quot;) test &lt;- airbnb_data_filtered_logistic_fit_test %&gt;% select(host_is_superhost_binary, .fitted) %&gt;% mutate(type = &quot;Test set&quot;) both &lt;- rbind(training, test) rm(training, test) both %&gt;% ggplot(aes(x = .fitted, fill = host_is_superhost_binary)) + geom_histogram(binwidth = 0.05, position = &quot;dodge&quot;) + theme_minimal() + labs(x = &quot;Estimated probability that host is super host&quot;, y = &quot;Number&quot;, fill = &quot;Host is super host&quot;) + scale_fill_brewer(palette = &quot;Set1&quot;) + facet_wrap(vars(type), nrow = 2, scales = &quot;free_y&quot;) "],["its-just-a-linear-model.html", "Chapter 14 It’s Just A Linear Model 14.1 Overview 14.2 Simple linear regression 14.3 Multiple linear regression 14.4 Logistic regression 14.5 Poisson regression", " Chapter 14 It’s Just A Linear Model Last updated: 16 March 2021. Required reading Greenland, Sander, Stephen J. Senn, Kenneth J. Rothman, John B. Carlin, Charles Poole, Steven N. Goodman, and Douglas G. Altman, 2016, ‘Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations,’ European journal of epidemiology, 31, no. 4, pp. 337-350. James, Gareth, Daniela Witten, Trevor Hastie and Robert Tibshirani, 2017, An Introduction to Statistical Learning with Applications in R, 1st Edition, Chapters 3 and 4.1-4.3., https://www.statlearning.com. Obermeyer, Z., Powers, B., Vogeli, C., &amp; Sendhill, M., 2019, ‘Dissecting racial bias in an algorithm used to manage the health of populations,’ Science, (366): 447-453. Wickham, Hadley, and Garrett Grolemund, 2017, R for Data Science, Chapter 23, https://r4ds.had.co.nz/. Zook M, Barocas S, boyd d, Crawford K, Keller E, Gangadharan SP, et al. (2017) ‘Ten simple rules for responsible big data research,’ PLoS Comput Biol 13(3): e1005399. https://doi.org/10.1371/journal.pcbi.1005399 Recommended reading Angrist, Joshua D., and Jörn-Steffen Pischke, 2008, Mostly harmless econometrics: An empiricist’s companion, Princeton University Press, Chapter 3.4.3. Cunningham, Scott, Causal Inference: The Mixtape, Chapter 2, Yale University Press, https://mixtape.scunning.com. ElHabr, Tony, 2019, ‘A Bayesian Approach to Ranking English Premier League Teams (using R),’ https://tonyelhabr.rbind.io/post/bayesian-statistics-english-premier-league/. Ioannidis, John PA, 2005, ‘Why most published research findings are false,’ PLoS medicine, 2, no. 8, e124. Pavlik, Kaylin, 2018, ‘Exploring the Relationship Between Dog Names and Breeds,’ https://www.kaylinpavlik.com/dog-names-tfidf/. Pavlik, Kaylin, 2019, ‘Understanding + classifying genres using Spotify audio features,’ https://www.kaylinpavlik.com/classifying-songs-genres/. Silge, Julia, 2019, ‘Modeling salary and gender in the tech industry,’ https://juliasilge.com/blog/salary-gender/. Silge, Julia, 2019, ‘Opioid prescribing habits in Texas,’ https://juliasilge.com/blog/texas-opioids/. Silge, Julia, 2019, ‘Tidymodels,’ https://juliasilge.com/blog/intro-tidymodels/. Silge, Julia, 2020, ‘#TidyTuesday hotel bookings and recipes,’ https://juliasilge.com/blog/hotels-recipes/. Silge, Julia, 2020, ‘Hyperparameter tuning and #TidyTuesday food consumption,’ https://juliasilge.com/blog/food-hyperparameter-tune/. Taddy, Matt, 2019, Business Data Science, Chapters 2 and 4. Wasserstein, Ronald L. and Nicole A. Lazar, 2016, ‘The ASA Statement on p-Values: Context, Process, and Purpose,’ The American Statistician, 70:2, 129-133, DOI: 10.1080/00031305.2016.1154108. Fun reading Chellel, Kit, 2018, ‘The Gambler Who Cracked the Horse-Racing Code,’ Bloomberg Businessweek, 3 May, https://www.bloomberg.com/news/features/2018-05-03/the-gambler-who-cracked-the-horse-racing-code. Key concepts/skills/etc Simple and multiple linear regression. Logistic and Poisson regression. The key role of uncertainty. Threats to validity of inferences Overfitting. Key libraries broom huxtable rstanarm tidymodels tidyverse Key functions broom::augment() broom::glance() broom::tidy() glm() huxtable::huxreg() lm() parsnip::fit() parsnip::linear_reg() parsnip::logistic_reg() parsnip::set_engine() poissonreg::poisson_reg() rnorm() rpois() rsample::initial_split() rsample::testing() rsample::training() sample() set.seed() summary() Quiz Please write a linear relationship between some response variable, Y, and some predictor, X. What is the intercept term? What is the slope term? What would adding a hat to these indicate? What is the least squares criterion? Similarly, what is RSS and what are we trying to do when we run least squares regression? What is statistical bias? If there were three variables: Snow, Temperature, and Wind, please write R code that would fit a simple linear regression to explain Snow as a function of Temperature and Wind. What do you think about another explanatory variable - daily stock market returns - to your model? According to Greenland et al. (2016), p-values test (pick one)? All the assumptions about how the data were generated (the entire model), not just the targeted hypothesis it is supposed to test (such as a null hypothesis). Whether the hypothesis targeted for testing is true or not. A dichotomy whereby results can be declared ‘statistically significant.’ According to Greenland et al. (2016), a p-value may be small because (select all)? The targeted hypothesis is false. The study protocols were violated. It was selected for presentation based on its small size. According to Obermeyer et al. (2019), why does racial bias occur in an algorithm used to guide health decisions in the US (pick one)? The algorithm uses health costs as a proxy for health needs. The algorithm was trained on Reddit data. When should we use logistic regression (pick one)? Continuous dependent variable. Binary dependent variable. Count dependent variable. I am interested in studying how voting intentions in the recent US presidential election vary by an individual’s income. I set up a logistic regression model to study this relationship. In my study, one possible dependent variable would be (pick one)? Whether the respondent is a US citizen (yes/no) The respondent’s personal income (high/low) Whether the respondent is going to vote for Trump (yes/no) Who the respondent voted for in 2016 (Trump/Clinton) I am interested in studying how voting intentions in the recent US presidential election vary by an individual’s income. I set up a logistic regression model to study this relationship. In my study, one possible dependent variable would be (pick one)? The race of the respondent (white/not white) The respondent’s marital status (married/not) Whether the respondent is registered to vote (yes/no) Whether the respondent is going to vote for Biden (yes/no) Please explain what a p-value is, using only the term itself (i.e. ‘p-value’) and words that are amongst the 1,000 most common in the English language according to the XKCD Simple Writer - https://xkcd.com/simplewriter/. (Please write one or two paragraphs.) The mean of a Poisson distribution is equal to its? Median. Standard deviation. Variance. 14.1 Overview Words! Mere words! How terrible they were! How clear, and vivid, and cruel! One could not escape from them. And yet what a subtle magic there was in them! They seemed to be able to give a plastic form to formless things, and to have a music of their own as sweet as that of viol or of lute. Mere words! Was there anything so real as words? Oscar Wilde, The Picture of Dorian Gray. Regression will not sort it out. Regression is indeed an oracle, but a cruel one. It speaks in riddles and delights in punishing us for asking bad questions. McElreath (2020, 162). Linear models have been around for a long time, at least since Galton and many others (some of whom were eugenicists) used linear regression in earnest. The generalized linear model framework came into being, in a formal sense, in the 70s with the seminal folks being Nelder and Wedderburn (Nelder and Wedderburn 1972). The idea of generalized linear models is that we broaden the types of outcomes that are allowed. You’re still modelling things as a linear function, but you’re not constrained to an outcome that is normally distributed. The outcome can be anything in the exponential family. A further, well, generalization of generalized linear models is generalized additive models where you’re not generalizing anything to do with the outcome, but instead the structure of the explanatory side, as it were. We’re still explaining the dependent variable as an additive function of bits, but those bits can be functions. This framework, in this way, came about in the 90s, with Hastie and Tibshirani (Hastie and Tibshirani 1990) (fun fact, Tibshirani did a stats masters at Toronto, and was a professor here from 1985 through to 1998!). It’s important to recognise that when we build models we are not discovering ‘the truth.’ We are using the model to help us explore and understand the data that we have. There is no one best model, there are just useful models that help us learn something about the data that we have and hence, hopefully, something about the world from which the data were generated. Ben Rhodes, who was an Obama staffer, titled his White House memoirs ‘The World as It Is: A Memoir of the Obama White House.’ When we use models, we are similarly trying to understand the world, but as the second part of the title makes clear, there are enormous constraints on the perspective. In the same way that we’d not expect Rhodes to advocate an Australian, Canadian, or even US Republican, perspective about the world, it’s silly to expect one model to be universal. We use models to understand the world. We poke, push, and test them. We build them and rejoice in their beauty, and then seek to understand their limits and ultimately destroy them. It is this process that is important, it is this process that allows us to better understand the world. McElreath (2020, 19) talks about small and large worlds, saying ‘(a)ll statistical modeling has these two frames: the small world of the model itself and the large world we hope to deploy the model in.’ To what extent does a model trained on the experiences of straight, cis, men, speak to the world as it is? It’s not worthless, but it’s also not unimpeachable. To what extent does the model teach us about the data that we have? To what extent do the data that we have reflect the world for which we would like to draw conclusions? Keep these questions front of mind. 14.2 Simple linear regression Figure 14.1: Oh my. Source: Mijke Rhemtulla, 3 March 2020. 14.2.1 Overview When we have two continuous variables we use simple linear regression. This is based on the Normal (also ‘Gaussian’) distribution. From Pitman (1993, 94) ‘The normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) is the distribution over the x-axis defined by areas under the normal curve with these parameters. The equation of the normal curve with parameters \\(\\mu\\) and \\(\\sigma\\), can be written as: \\[y = \\frac{1}{\\sqrt{2\\pi\\sigma}}e^{-\\frac{1}{2}z^2},\\] where \\(z = (x - \\mu)/\\sigma\\) measures the number of standard deviations from the mean \\(\\mu\\) to the number \\(x\\).’ In R we can simulate \\(n\\) data points from the Normal distribution with rnorm(). rnorm(n = 20, mean = 0, sd = 1) ## [1] -0.7195816 1.5129669 1.0892396 -0.7643677 1.9500980 2.5212818 ## [7] 1.8184490 -0.6686949 1.7879044 1.4109558 1.4115767 -1.4008841 ## [13] -0.3295842 -0.2270651 0.7046669 2.1948797 1.0188842 0.5635043 ## [19] 0.4671846 -1.0193147 It will take a few draws before we get the expected shape. library(tidyverse) set.seed(853) tibble( number_of_draws = c( rep.int(x = &quot;2 draws&quot;, times = 2), rep.int(x = &quot;5 draws&quot;, times = 5), rep.int(x = &quot;10 draws&quot;, times = 10), rep.int(x = &quot;50 draws&quot;, times = 50), rep.int(x = &quot;100 draws&quot;, times = 100), rep.int(x = &quot;500 draws&quot;, times = 500), rep.int(x = &quot;1,000 draws&quot;, times = 1000), rep.int(x = &quot;10,000 draws&quot;, times = 10000), rep.int(x = &quot;100,000 draws&quot;, times = 100000)), draws = c( rnorm(n = 2, mean = 0, sd = 1), rnorm(n = 5, mean = 0, sd = 1), rnorm(n = 10, mean = 0, sd = 1), rnorm(n = 50, mean = 0, sd = 1), rnorm(n = 100, mean = 0, sd = 1), rnorm(n = 500, mean = 0, sd = 1), rnorm(n = 1000, mean = 0, sd = 1), rnorm(n = 10000, mean = 0, sd = 1), rnorm(n = 100000, mean = 0, sd = 1)) ) %&gt;% mutate(number_of_draws = as_factor(number_of_draws)) %&gt;% ggplot(aes(x = draws)) + geom_density() + theme_classic() + facet_wrap(vars(number_of_draws), scales = &quot;free_y&quot;) + labs(x = &#39;Draw&#39;, y = &#39;Density&#39;) When we use simple linear regression, we assume that our relationship is characterised by the variables and the parameters, with any difference, often denoted by \\(\\epsilon\\), between the expectation and the reality being normally distributed. If we have two variables, \\(Y\\) and \\(X\\), then we could characterise the relationship between these as: \\[Y \\sim \\beta_0 + \\beta_1 X.\\] There are two coefficients/parameters: the ‘intercept’ is \\(\\beta_0\\), and the ‘slope’ is \\(\\beta_1\\). We are saying that \\(Y\\) will have some value, \\(\\beta_0\\), even when \\(X\\) is 0, and that \\(Y\\) will change by \\(\\beta_1\\) units for every one unit change in \\(X\\). The language that we use is that ‘X is being regressed on Y.’ We may then take this relationship to the data that we have about the relationship in order to estimate these coefficients for those particular values that we have: \\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x.\\] The hats are used to indicate that these are estimated values. We are saying this is a linear regression because we assume that if \\(x\\) doubles then \\(y\\) would also double. Linear regressions considers how the average of a dependent variable changes based on the independent variables. I want to focus on data, so we’ll make this example concrete, by generating some data and then discussing everything in the context of that. The example will be looking at someone’s time for running five kilometers, compared with their time for running a marathon. set.seed(853) number_of_observations &lt;- 100 running_data &lt;- tibble(five_km_time = rnorm(number_of_observations, 20, 3), noise = rnorm(number_of_observations, 0, 10), marathon_time = five_km_time * 8.4 + noise, was_raining = sample(c(&quot;Yes&quot;, &quot;No&quot;), size = number_of_observations, replace = TRUE, prob = c(0.2, 0.8)) ) running_data %&gt;% ggplot(aes(x = five_km_time, y = marathon_time)) + geom_point() + labs(x = &quot;Five-kilometer time (minutes)&quot;, y = &quot;Marathon time (minutes)&quot;) + theme_classic() In this set-up we may like to use \\(x\\), which is the five-kilometer time, to produce estimates of \\(y\\), which is the marathon time. This would involve also estimating values of \\(\\beta_0\\) and \\(\\beta_1\\), which is why they have a hat on them. But how should we estimate the coefficients? Even if we impose a linear relationship there are a lot of options (how many straight lines can you fit on a piece of paper?). But clearly some of the fits are not all that great. One way we may define being great would be to impose that they be as close as possible to each of the \\(x\\) and \\(y\\) combinations that we know. There are a lot of candidates for how we define ‘as close as possible,’ but one is to minimise the sum of least squares. To do this we produce our estimates of \\(\\hat{y}\\) based on some estimates of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), given the \\(x\\), and then work out how ‘wrong,’ for every point \\(i\\), we were: \\[e_i = y_i - \\hat{y}_i.\\] The residual sum of squares (RSS) then requires summing across all the points: \\[\\mbox{RSS} = e^2_1+ e^2_2 +\\dots + e^2_n.\\] This results in one ‘linear best-fit’ line, but it is worth thinking about all of the assumptions and decisions that it took to get us to this point. running_data %&gt;% ggplot(aes(x = five_km_time, y = marathon_time)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;black&quot;, linetype = &quot;dashed&quot;, formula = &#39;y ~ x&#39;) + labs(x = &quot;Five-kilometer time (minutes)&quot;, y = &quot;Marathon time (minutes)&quot;) + theme_classic() With the least squares criterion we want the values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) that result in the smallest RSS. 14.2.2 Implementation in base R Within R, the main function for doing linear regression is lm. This is included in base R, so you don’t need to call any packages, but in a moment, we will call a bunch of packages that will surround lm within an environment that we are more familiar with. You specify the relationship with the dependent variable first, then ~, then the independent variables. Finally, you should specify the dataset (or you could pipe to it as usual). lm(y ~ x, data = dataset) In general, you should assign this to an object: running_data_first_model &lt;- lm(marathon_time ~ five_km_time, data = running_data) To see the result of your regression you can then call summary(). summary(running_data_first_model) ## ## Call: ## lm(formula = marathon_time ~ five_km_time, data = running_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -24.763 -5.686 0.722 6.650 16.707 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.4114 6.0610 0.068 0.946 ## five_km_time 8.3617 0.3058 27.343 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.474 on 98 degrees of freedom ## Multiple R-squared: 0.8841, Adjusted R-squared: 0.8829 ## F-statistic: 747.6 on 1 and 98 DF, p-value: &lt; 2.2e-16 The first part of the result tells us the regression that we called, then information about the residuals, and the estimated coefficients. And then finally some useful diagnostics. We are considering that there is some relationship between \\(X\\) and \\(Y\\), that is: \\(Y = f(X) + \\epsilon\\). We are going to say that function, \\(f()\\), is linear and so our relationship is: \\[\\hat{Y} = \\beta_0 + \\beta_1 X + \\epsilon.\\] There is some ‘true’ relationship between \\(X\\) and \\(Y\\), but we don’t know what it is. All we can do is use our sample of data to try to estimate it. But because our understanding depends on that sample, for every possible sample, we would get a slightly different relationship (as measured by the coefficients). That \\(\\epsilon\\) is a measure of our error - what does the model not know? There’s going to be plenty that the model doesn’t know, but we hope is that the error does not depend on \\(X\\), and that the error is normally distributed. The intercept is marathon time that we would expect with a five-kilometer time of 0 minutes. Hopefully this example illustrates the need to carefully interpret the intercept coefficient! The coefficient on five-kilometer run time shows how we expect the marathon time to change if five-kilometer run time changed by one unit. In this case it’s about 8.4, which makes sense seeing as a marathon is roughly that many times longer than a five-kilometer run. 14.2.3 Tidy up with broom While there is nothing wrong with the base approach, I want to introduce the broom package because that will provide us with outputs in a tidy framework (D. Robinson, Hayes, and Couch 2020). There are three key functions: broom::tidy(): Gives the coefficient estimates in a tidy output. broom::glance(): Gives the diagnostics. broom::augment(): Adds the forecast values, and hence, residuals, to your dataset. library(broom) tidy(running_data_first_model) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.411 6.06 0.0679 9.46e- 1 ## 2 five_km_time 8.36 0.306 27.3 1.17e-47 glance(running_data_first_model) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.884 0.883 8.47 748. 1.17e-47 1 -355. 715. 723. ## # … with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; Notice how the results are fairly similar to the base summary function. running_data &lt;- augment(running_data_first_model, data = running_data) head(running_data) ## # A tibble: 6 x 10 ## five_km_time noise marathon_time was_raining .fitted .resid .hat .sigma ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 18.9 -3.73 155. No 159. -3.42 0.0106 8.51 ## 2 19.9 8.42 175. No 167. 8.77 0.0101 8.47 ## 3 14.7 4.32 127. No 123. 4.47 0.0422 8.50 ## 4 16.6 -2.74 137. No 139. -2.51 0.0217 8.51 ## 5 17.0 -4.89 138. No 142. -4.65 0.0190 8.50 ## 6 25.3 0.648 213. No 212. 1.21 0.0524 8.52 ## # … with 2 more variables: .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt; We could now make plots of the residuals. ggplot(running_data, aes(x = .resid)) + geom_histogram(binwidth = 1) + theme_classic() + labs(y = &quot;Number of occurrences&quot;, x = &quot;Residuals&quot;) ggplot(running_data, aes(five_km_time, .resid)) + geom_point() + geom_hline(yintercept = 0, linetype = &quot;dotted&quot;, color = &quot;grey&quot;) + theme_classic() + labs(y = &quot;Residuals&quot;, x = &quot;Five-kilometer time (minutes)&quot;) When we say our estimate is unbiased, we are trying to say that even though with some sample our estimate might be too high, and with another sample our estimate might be too low, eventually if we have a lot of data then our estimate would be the same as the population. (A pro hockey player may sometimes shoot right of the net, and sometimes left of the net, but we’d hope that on average they’d be right in the middle of the net). In the words of James et al. (2017), ‘an unbiased estimator does not systematically over- or under-estimate the true parameter.’ But we want to try to speak to the ‘true’ relationship, so we need to try to capture how much we think our understanding depends on the particular sample that we have to analyse. And this is where standard error comes in. It tells us how off our estimate is compared with the actual. From standard errors, we can compute a confidence interval. A 95 per cent confidence interval means that there is a 0.95 probability that the interval happens to contain the population parameter (which is typically unknown). running_data %&gt;% ggplot(aes(x = five_km_time, y = marathon_time)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = TRUE, color = &quot;black&quot;, linetype = &quot;dashed&quot;, formula = &#39;y ~ x&#39;) + labs(x = &quot;Five-kilometer time (minutes)&quot;, y = &quot;Marathon time (minutes)&quot;) + theme_classic() There are a bunch of different tests that you can use to understand how your model is performing given this data. One quick way to look at a whole bunch of different aspects is to use the performance package (Lüdecke et al. 2020). library(performance) performance::check_model(running_data_first_model) 14.2.4 Testing hypothesis Now that we have an interval for which we can say there is a 95 per cent probability it contains the true population parameter we can test claims. For instance, a null hypothesis that there is no relationship between \\(X\\) and \\(Y\\) (i.e. \\(\\beta_1 = 0\\)), compared with an alternative hypothesis that there is some relationship between \\(X\\) and \\(Y\\) (i.e. \\(\\beta_1 \\neq 0\\)). We need to know whether our estimate of \\(\\beta_1\\), which is \\(\\hat{\\beta}_1\\), is ‘far enough’ away from zero for us to be comfortable claiming that \\(\\beta_1 \\neq 0\\). How far is ‘far enough?’ If we were very confident in our estimate of \\(\\beta_1\\) then it wouldn’t have to be far, but if we were not then it would have to be substantial. So it depends on a bunch of things, but essentially the standard error of \\(\\hat{\\beta}_1\\). We compare this standard error with \\(\\hat{\\beta}_1\\) to get the t-statistic: \\[t = \\frac{\\hat{\\beta}_1 - 0}{\\mbox{SE}(\\hat{\\beta}_1)}.\\] And we then compare our t-statistic to the t-distribution to compute the probability of getting this absolute t-statistic or a larger one, if \\(\\beta_1 = 0\\). This is the p-value. A small p-value means it is unlikely that we would observe our association due to chance if there wasn’t a relationship. 14.2.5 On p-values The p-value is a specific and subtle concept. It is easy to abuse. The main issue is that it embodies, and assumes correct, every assumption of the model. From Greenland et al. (2016, 339): ‘The p-value is then the probability that the chosen test statistic would have been at least as large as its observed value if every model assumption were correct, including the test hypothesis.’ To provide background on the language used here in case you’re unfamiliar, a test hypothesis is typically a ‘null hypothesis,’ and a ‘test statistic’ is ‘the distance between the data and the model prediction’ (Greenland et al. 2016). The following quote (minor edits for consistency with above) summarises the situation: It is true that the smaller the p-value, the more unusual the data would be if every single assumption were correct; but a very small p-value does not tell us which assumption is incorrect. For example, the p-value may be very small because the targeted hypothesis is false; but it may instead (or in addition) be very small because the study protocols were violated, or because it was selected for presentation based on its small size. Conversely, a large p-value indicates only that the data are not unusual under the model, but does not imply that the model or any aspect of it (such as the targeted hypothesis) is correct; it may instead (or in addition) be large because (again) the study protocols were violated, or because it was selected for presentation based on its large size. The general definition of a p-value may help one to understand why statistical tests tell us much less than what many think they do: Not only does a p-value not tell us whether the hypothesis targeted for testing is true or not; it says nothing specifically related to that hypothesis unless we can be completely assured that every other assumption used for its computation is correct—an assurance that is lacking in far too many studies. Greenland et al. (2016, 339). There is nothing inherently wrong about using p-values, but it is important to use them in sophisticated and thoughtful ways. Typically one application where it’s easy to see abuse of p-values is in power analysis. As Gelman and Hill (2007, 438) say, ‘[s]ample size is never large enough…. this is not a problem… [w]e are just emphasizing that, just as you never have enough money, because perceived needs increase with resources, your inferential needs with increase with your sample size.’ Power refers to the probability of incorrectly failing to reject the null hypothesis. As Imai (2017, 303) says: We use power analysis in order to formalize the degree of informativeness of data in hypothesis tests. The power of a statistical hypothesis test is defined as one minus the probability of type II error: power = 1-P(type II error) In a vacuum, we’d like to have high power and we can achieve that either by having really big effect sizes, or by having a larger number of observations. 14.3 Multiple linear regression To this point we’ve just considered one explanatory variable. But we’ll usually have more than one. One approach would be to run separate regressions for each explanatory variable. But compared with separate linear regressions for each, adding more explanatory variables allows us to have a better understanding of the intercept and accounts for interaction. Often the results will be quite different. This slightly counterintuitive result is very common in many real life situations. Consider an absurd example to illustrate the point. Running a regression of shark attacks versus ice cream sales for data collected at a given beach community over a period of time would show a positive relationship, similar to that seen between sales and newspapers. Of course no one (yet) has suggested that ice creams should be banned at beaches to reduce shark attacks. In reality, higher temperatures cause more people to visit the beach, which in turn results in more ice cream sales and more shark attacks. A multiple regression of attacks versus ice cream sales and temperature reveals that, as intuition implies, the former predictor is no longer significant after adjusting for temperature. (James et al. 2017, 74). We may also like to consider variables that do not have an inherent ordering. For instance, pregnant or not. When there are only two options then we can use a binary variable which is 0 or 1. If there are more than two levels then use a combination of binary variables, where the ‘missing’ outcome (baseline) gets pushed onto the intercept. In other languages you may need to explicitly construct dummy variables, but as R was designed as a language to do statistical programming, it does a lot of the work here for you and is fairly forgiving. For instance, if you have a column of character values that only had two values: c(\"Monica\", \"Rohan\", \"Rohan\", \"Monica\", \"Monica\", \"Rohan\"), and you used this as a independent variable in your usual regression set up then R would treat it as a dummy variable. running_data_rain_model &lt;- lm(marathon_time ~ five_km_time + was_raining, data = running_data) summary(running_data_rain_model) ## ## Call: ## lm(formula = marathon_time ~ five_km_time + was_raining, data = running_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -24.6239 -5.5806 0.8377 6.7636 16.8671 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.1430 6.1476 0.023 0.981 ## five_km_time 8.3689 0.3081 27.166 &lt;2e-16 *** ## was_rainingYes 0.7043 2.2220 0.317 0.752 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.513 on 97 degrees of freedom ## Multiple R-squared: 0.8842, Adjusted R-squared: 0.8818 ## F-statistic: 370.4 on 2 and 97 DF, p-value: &lt; 2.2e-16 The result probably isn’t too surprising if we look at a plot of the data. running_data %&gt;% ggplot(aes(x = five_km_time, y = marathon_time, color = was_raining)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;black&quot;, linetype = &quot;dashed&quot;) + labs(x = &quot;Five-kilometer time (minutes)&quot;, y = &quot;Marathon time (minutes)&quot;, color = &quot;Was raining&quot;) + theme_classic() + scale_color_brewer(palette = &quot;Set1&quot;) In addition to wanting to include additional explanatory variables we may think that they are related with one another. For instance, if we were wanting to explain the amount of snowfall in Toronto, then we may be interested in the humidity and the temperature, but those two variables may also interact. We can do this by using * instead of + when we specify the model in R. If you do interact variables, then you should almost always also include the individual variables as well (Figure 14.2). Figure 14.2: Don’t leave out the main effects in an interactive model Source: By Kai Arzheimer, 16 February 2020. 14.3.1 Threats to validity and aspects to think about There are a variety of weaknesses and aspects that you should discuss when you use linear regression. A quick list includes (James et al. 2017, 92): Non-linearity of the response-predictor relationships. Correlation of error terms. Non-constant variance of error terms. Outliers. High-leverage points. Collinearity These are also aspects that you should discuss if you use linear regression. Including plots tends to be handy here to illustrate your points. Other aspects that you may consider discussing include (James et al. 2017, 75): Is at least one of the predictors \\(X_1, X_2, \\dots, X_p\\) useful in predicting the response? Do all the predictors help to explain \\(Y\\), or is only a subset of the predictors useful? How well does the model fit the data? Given a set of predictor values, what response value should we predict, and how accurate is our prediction? 14.3.2 More credible outputs Finally, after creating beautiful graphs and tables you may want your regression output to look just as nice. There are a variety of packages in R that will automatically format your regression outputs. One that is particularly nice is huxtable (Hugh-Jones 2020). library(huxtable) huxreg(running_data_first_model, running_data_rain_model) Table 14.1: (1)(2) (Intercept)0.411&nbsp;&nbsp;&nbsp;&nbsp;0.143&nbsp;&nbsp;&nbsp;&nbsp; (6.061)&nbsp;&nbsp;&nbsp;(6.148)&nbsp;&nbsp;&nbsp; five_km_time8.362 ***8.369 *** (0.306)&nbsp;&nbsp;&nbsp;(0.308)&nbsp;&nbsp;&nbsp; was_rainingYes&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.704&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2.222)&nbsp;&nbsp;&nbsp; N100&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;100&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.884&nbsp;&nbsp;&nbsp;&nbsp;0.884&nbsp;&nbsp;&nbsp;&nbsp; logLik-354.584&nbsp;&nbsp;&nbsp;&nbsp;-354.532&nbsp;&nbsp;&nbsp;&nbsp; AIC715.168&nbsp;&nbsp;&nbsp;&nbsp;717.064&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. 14.3.3 Implementation in tidymodels The reason that we went to all that trouble to do simple regression is that we often want to fit a bunch of models. One way is to copy/paste code a bunch of times. There’s nothing wrong with that. And that’s the way that most people get started, but you may want to take an approach that scales more easily. We also need to think more carefully about over-fitting, and being able to evaluate our models. The tidymodels package (Kuhn and Wickham 2020) is what all the cool kids are using these days. It’s an attempt to bring some order to the chaos that has been different modelling packages in R. (There have been other attempts in the past and they’ve crashed and burned, but hopefully this time is different.) The issue is that let’s say you want to run a simple linear regression and then run a random forest. The language that you’d use to code these models is fairly different. The tidymodels package is the latest attempt to bring a coherent grammar to this. It’s also a package of packages. We’ll create test and training datasets. set.seed(853) library(tidymodels) running_data_split &lt;- rsample::initial_split(running_data, prop = 0.80) running_data_split ## &lt;Analysis/Assess/Total&gt; ## &lt;81/19/100&gt; So we have 81 points in our training set, 19 in our test set and 100 in total. We can then make datasets for the test and training samples. running_data_train &lt;- rsample::training(running_data_split) running_data_test &lt;- rsample::testing(running_data_split) If we have a look at the dataset that we made we can see that it’s got fewer rows. We could have reached the same outcome with something like: running_data &lt;- running_data %&gt;% mutate(magic_number = sample(x = c(1:nrow(running_data)), size = nrow(running_data), replace = FALSE)) running_data_test &lt;- running_data %&gt;% filter(magic_number &lt;= 20) running_data_train &lt;- running_data %&gt;% filter(magic_number &gt; 20) first_go &lt;- parsnip::linear_reg() %&gt;% parsnip::set_engine(engine = &quot;lm&quot;) %&gt;% parsnip::fit(marathon_time ~ five_km_time + was_raining, data = running_data_train ) 14.3.4 Implementation in rstanarm The tidymodels package will be fine for specific types of tasks. For instance if you are doing machine learning then chances are you are interested in forecasting. That’s the kind of thing that tidymodels is really built for. If you want equivalent firepower for explanatory modelling then one option is to use Bayesian approaches more directly. Yes, you can use Bayesian models within the tidymodels ecosystem, but as you start to move away from out-of-the-box solutions, it becomes important to start to understand what is going on under the hood. There are a variety of ways of getting started, but essentially what you need is a probabilistic programming language. That is one that is specifically designed for this sort of thing, in comparison to R, which is designed for more general statistical computing. We will use Stan in these notes within the context of our familiar R environment. We will interface with Stan using the rstanarm package (Goodrich et al. 2020). library(rstanarm) first_go_in_rstanarm &lt;- stan_lm( marathon_time ~ five_km_time + was_raining, data = running_data, prior = NULL, seed = 853 ) first_go_in_rstanarm ## stan_lm ## family: gaussian [identity] ## formula: marathon_time ~ five_km_time + was_raining ## observations: 100 ## predictors: 3 ## ------ ## Median MAD_SD ## (Intercept) 0.4 6.0 ## five_km_time 8.4 0.3 ## was_rainingYes 0.7 2.2 ## ## Auxiliary parameter(s): ## Median MAD_SD ## R2 0.9 0.0 ## log-fit_ratio 0.0 0.0 ## sigma 8.6 0.6 ## ## ------ ## * For help interpreting the printed output see ?print.stanreg ## * For info on the priors used see ?prior_summary.stanreg 14.4 Logistic regression 14.4.1 Overview To steal a joke from someone, ‘it’s AI when you’re fundraising, machine learning when you’re hiring, and logistic regression when you’re implementing.’ When the dependent variable is a binary outcome, that is 0 or 1, then instead of linear regression we may like to use logistic regression. Although a binary outcome may sound limiting, there are a lot of circumstances in which your outcome either naturally falls into this situation, or can be adjusted into it (e.g. a voter supports the liberals or not the liberals). The reason that we use logistic regression is that we’ll be modelling a probability and so it will be bounded between 0 and 1. Whereas with linear regression we may end up with values outside this. In practice it is usually fine to start with linear regression and then move to logistic regression as you build confidence. This all said, logistic regression, as Daniella Witten teaches us, is just a linear model! 14.4.2 Implementation in base I’d like to consider a slightly more interesting example, which is a dataset of pearl jewellery, from the Australian retailer Paspaley. paspaley_dataset &lt;- read_csv(&quot;https://raw.githubusercontent.com/RohanAlexander/paspaley/master/outputs/data/cleaned_dataset.csv&quot;) paspaley_dataset$metal %&gt;% table() ## . ## Other Platinum Rose gold White gold Yellow gold ## 98 20 63 366 411 In this case we’ll model whether some jewellery is made of white or yellow gold, based on their price and the year (Figure 14.3). paspaley_logistic_dataset &lt;- paspaley_dataset %&gt;% filter(metal %in% c(&#39;White gold&#39;, &#39;Yellow gold&#39;)) %&gt;% select(metal, price, year) Figure 14.3: Examining the type of gold some jewellery is made from. The graph suggests that we should filter any price higher than $100,000. paspaley_logistic_dataset &lt;- paspaley_logistic_dataset %&gt;% filter(price &lt; 100000) As with linear regression, logistic regression is built into R, with the glm function. In this case, we’ll try to work out if the jewellery was white gold. Although not strictly necessary for this particular function, we’ll change it to a binary, that will be 1 if white gold and 0 if not. paspaley_logistic_dataset &lt;- paspaley_logistic_dataset %&gt;% mutate(is_white_gold = if_else(metal == &quot;White gold&quot;, 1, 0)) white_gold_model &lt;- glm(is_white_gold ~ price + year, data = paspaley_logistic_dataset, family = &#39;binomial&#39;) summary(white_gold_model) ## ## Call: ## glm(formula = is_white_gold ~ price + year, family = &quot;binomial&quot;, ## data = paspaley_logistic_dataset) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.243 -1.140 -1.060 1.212 1.302 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.992e+02 1.264e+02 1.577 0.115 ## price 3.503e-06 6.248e-06 0.561 0.575 ## year -9.877e-02 6.261e-02 -1.578 0.115 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1056.8 on 763 degrees of freedom ## Residual deviance: 1054.2 on 761 degrees of freedom ## AIC: 1060.2 ## ## Number of Fisher Scoring iterations: 3 One reason that logistic regression can be a bit of a pain initially is because the coefficients take a bit of work to interpret. In particular, our estimate on price is -3.170e-06. This is the odds. So the odds that it was white gold decrease by -3.170e-06 as the price increases. We can have our model make forecasts in terms of a probability, by asking for that. paspaley_logistic_dataset &lt;- broom::augment(white_gold_model, data = paspaley_logistic_dataset, type.predict = &quot;response&quot;) head(paspaley_logistic_dataset) Table 14.2: metalpriceyearis_white_gold.fitted.resid.std.resid.hat.sigma.cooksd Yellow gold2.58e+032.02e+0300.504-1.18-1.190.003921.180.00134 Yellow gold2.08e+032.02e+0300.503-1.18-1.190.003971.180.00135 Yellow gold3.08e+032.02e+0300.504-1.18-1.190.003861.180.00132 White gold7.38e+032.02e+0310.5081.161.170.003611.180.00117 White gold3.08e+032.02e+0310.5041.171.170.003861.180.00127 White gold3.95e+032.02e+0310.5051.171.170.003781.180.00124 14.4.3 Implementation in tidymodels We can use tidymodels to run this if we wanted. In this case, we need it as a factor. set.seed(853) paspaley_logistic_dataset &lt;- paspaley_logistic_dataset %&gt;% mutate(is_white_gold = as_factor(is_white_gold)) paspaley_logistic_dataset_split &lt;- rsample::initial_split(paspaley_logistic_dataset, prop = 0.80) paspaley_logistic_dataset_train &lt;- rsample::training(paspaley_logistic_dataset_split) paspaley_logistic_dataset_test &lt;- rsample::testing(paspaley_logistic_dataset_split) white_gold_model_tidymodels &lt;- parsnip::logistic_reg(mode = &quot;classification&quot;) %&gt;% parsnip::set_engine(&quot;glm&quot;) %&gt;% fit(is_white_gold ~ price + year, data = paspaley_logistic_dataset_train) white_gold_model_tidymodels ## parsnip model object ## ## Fit time: 3ms ## ## Call: stats::glm(formula = is_white_gold ~ price + year, family = stats::binomial, ## data = data) ## ## Coefficients: ## (Intercept) price year ## 1.990e+02 6.188e-06 -9.864e-02 ## ## Degrees of Freedom: 611 Total (i.e. Null); 609 Residual ## Null Deviance: 846.9 ## Residual Deviance: 844.3 AIC: 850.3 14.4.4 Implementation in rstanarm paspaley_in_rstanarm &lt;- rstanarm::stan_glm( is_white_gold ~ price + year, data = paspaley_logistic_dataset, family = binomial(link = &quot;logit&quot;), prior = NULL, seed = 853 ) 14.5 Poisson regression 14.5.1 Overview When we have count data, we use Poisson distribution. From Pitman (1993, 121) ’The Poisson distribution with parameter \\(\\mu\\) or Poisson (\\(\\mu\\)) distribution is the distribution of probabilities \\(P_{\\mu}(k)\\) over \\({0, 1, 2, ...}\\) defined by: \\[P_{\\mu}(k) = e^{-\\mu}\\mu^k/k!\\mbox{, for }k=0,1,2,...\\] We can simulate \\(n\\) data points from the Poisson distribution with rpois() where \\(\\lambda\\) is the mean and the variance. rpois(n = 20, lambda = 3) ## [1] 2 1 3 2 3 1 1 0 2 4 6 1 1 4 4 2 4 4 4 6 That \\(\\lambda\\) parameter governs the shape of the distribution. set.seed(853) number_of_each &lt;- 1000 tibble(lambda = c(rep(0, number_of_each), rep(1, number_of_each), rep(2, number_of_each), rep(5, number_of_each), rep(10, number_of_each)), draw = c(rpois(n = number_of_each, lambda = 0), rpois(n = number_of_each, lambda = 1), rpois(n = number_of_each, lambda = 2), rpois(n = number_of_each, lambda = 5), rpois(n = number_of_each, lambda = 10))) %&gt;% ggplot(aes(x = draw)) + geom_density() + facet_wrap(vars(lambda)) + theme_classic() For instance, if we look at the number of A+ grades that are awarded in each university course in a given term then for each course we would have a count. set.seed(853) count_of_A_plus &lt;- tibble( # https://stackoverflow.com/questions/1439513/creating-a-sequential-list-of-letters-with-r department = c(rep.int(&quot;1&quot;, 26), rep.int(&quot;2&quot;, 26)), course = c(paste0(&quot;DEP_1_&quot;, letters), paste0(&quot;DEP_2_&quot;, letters)), number_of_A_plus = c(sample(c(1:10), size = 26, replace = TRUE), sample(c(1:50), size = 26, replace = TRUE) ) ) 14.5.2 Implementation in base grades_model &lt;- glm(number_of_A_plus ~ department, data = count_of_A_plus, family = &#39;poisson&#39;) summary(grades_model) ## ## Call: ## glm(formula = number_of_A_plus ~ department, family = &quot;poisson&quot;, ## data = count_of_A_plus) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -6.7386 -1.2102 -0.2515 1.3292 3.9520 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.44238 0.09535 15.13 &lt;2e-16 *** ## department2 1.85345 0.10254 18.07 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 816.08 on 51 degrees of freedom ## Residual deviance: 334.57 on 50 degrees of freedom ## AIC: 545.38 ## ## Number of Fisher Scoring iterations: 5 14.5.3 Implementation in tidymodels We can use tidymodels to run this if we wanted although we first need to install a helper package poissonreg. # install.packages(&quot;poissonreg&quot;) set.seed(853) count_of_A_plus_split &lt;- rsample::initial_split(count_of_A_plus, prop = 0.80) count_of_A_plus_train &lt;- rsample::training(count_of_A_plus_split) count_of_A_plus_test &lt;- rsample::testing(count_of_A_plus_split) a_plus_model_tidymodels &lt;- poissonreg::poisson_reg(mode = &quot;regression&quot;) %&gt;% parsnip::set_engine(&quot;glm&quot;) %&gt;% parsnip::fit(number_of_A_plus ~ department, data = count_of_A_plus_train) a_plus_model_tidymodels ## parsnip model object ## ## Fit time: 4ms ## ## Call: stats::glm(formula = number_of_A_plus ~ department, family = stats::poisson, ## data = data) ## ## Coefficients: ## (Intercept) department2 ## 1.493 1.760 ## ## Degrees of Freedom: 41 Total (i.e. Null); 40 Residual ## Null Deviance: 644.8 ## Residual Deviance: 298.4 AIC: 470.8 "],["causality-from-observational-data.html", "Chapter 15 Causality from observational data 15.1 Introduction 15.2 DAGs and trying not to be tricked by the data 15.3 Difference in differences 15.4 Case study - Lower advertising revenue reduced French newspaper prices between 1960 and 1974 15.5 Case study - Funding of Clinical Trials and Reported Drug Efficacy 15.6 Regression discontinuity design 15.7 Case study - Stiers, Hooghe, and Dassonneville, 2020 15.8 Case study - Caughey, and Sekhon., 2011 15.9 Instrumental variables 15.10 Case study - Effect of Police on Crime", " Chapter 15 Causality from observational data Last updated: 17 March 2021. TODO: Replace the arm matching with https://kosukeimai.github.io/MatchIt/index.html Required reading Angelucci, Charles, and Julia Cagé, 2019, ‘Newspapers in times of low advertising revenues,’ American Economic Journal: Microeconomics, vol. 11, no. 3, pp. 319-364, DOI: 10.1257/mic.20170306, available at: https://www.aeaweb.org/articles?id=10.1257/mic.20170306. Better Evaluation, ‘Regression Discontinuity,’ https://www.betterevaluation.org/en/evaluation-options/regressiondiscontinuity Dagan, Noa, Noam Barda, Eldad Kepten, Oren Miron, Shay Perchik, Mark A. Katz, Miguel A. Hernán, Marc Lipsitch, Ben Reis, and Ran D. Balicer, 2021, ‘BNT162b2 mRNA Covid-19 vaccine in a nationwide mass vaccination setting,’ New England Journal of Medicine, 24 February, https://www.nejm.org/doi/full/10.1056/NEJMoa2101765. Eggers, Andrew C., Anthony Fowler, Jens Hainmueller, Andrew B. Hall, and James M. Snyder Jr, 2015, ‘On the validity of the regression discontinuity design for estimating electoral effects: New evidence from over 40,000 close races,’ American Journal of Political Science, 59 (1), pp. 259-274 Gelman, Andrew, 2019, ‘Another Regression Discontinuity Disaster and what can we learn from it,’ 25 June, https://statmodeling.stat.columbia.edu/2019/06/25/another-regression-discontinuity-disaster-and-what-can-we-learn-from-it/. Gelman, Andrew, Jennifer Hill and Aki Vehtari, 2020, Regression and Other Stories, Cambridge University Press, Chs 18 - 21. Gertler, Paul, Sebastian Martinez, Patrick Premand, Laura Rawlings, and Christel Vermeersch, ‘Impact Evaluation in Practice,’ Chapter 5 - 8. McElreath, Richard, 2020, Statistical Rethinking, 2nd Edition, CRC Press, Ch 14. Meng, Xiao-Li, 2021, ‘What Are the Values of Data, Data Science, or Data Scientists?’ Harvard Data Science Review, https://doi.org/10.1162/99608f92.ee717cf7, https://hdsr.mitpress.mit.edu/pub/bj2dfcwg/release/2. Riederer, Emily, 2021, ‘Causal design patterns for data analysts,’ 30 January, https://emilyriederer.netlify.app/post/causal-design-patterns/ Sekhon, Jasjeet and Rocio Titiunik, 2016, ‘Understanding Regression Discontinuity Designs As Observational Studies,’ Observational Studies 2 (2016) 174-182, http://sekhon.berkeley.edu/papers/SekhonTitiunik2016-OS.pdf. Wong, Jeffrey, and Colin McFarland, 2020, ‘Computational Causal Inference at Netflix,’ Netflix Technology Blog, 11 Aug, https://netflixtechblog.com/computational-causal-inference-at-netflix-293591691c62. Required viewing Gelman, Andrew, 2020 ‘100 Stories of Causal Inference,’ 4 August, https://www.youtube.com/watch?v=jnI5KI843Lk. King, Gary, 2020, ‘Research Designs,’ Lectures on Quantitative Social Science Methods 1, https://youtu.be/SBwPLwVOb7s. Kuriwaki, Shiro, 2020, ‘Difference-in-Differences Estimation in R (parts 1 and 2),’ 18 April, https://vimeo.com/409267138 and https://vimeo.com/409267190. Kuriwaki, Shiro, 2020, ‘Instrumental variables in R,’ 11 April, https://vimeo.com/406629459. Kuriwaki, Shiro, 2020, ‘Regression Discontinuity in R (parts 1 and 2),’ 25 March, https://vimeo.com/400826628 and https://vimeo.com/400826660. Oostrom, Tamar, 2021, ‘Funding of Clinical Trials and Reported Drug Efficacy,’ 2 March, https://youtu.be/DdnpWS9Km5U. Riederer, Emily, 2021, ‘Observational Causal Inference,’ Toronto Data Workshop, 15 February, https://youtu.be/VP3BBZ7poc0. Recommended reading Alexander, Monica, Polimis, Kivan, and Zagheni, Emilio, 2019,’ The impact of Hurricane Maria on out-migration from Puerto Rico: Evidence from Facebook data’, Population and Development Review. (Example of using diff-in-diff to measure the effect of Hurricane Maria.) Alexander, Rohan, and Zachary Ward, 2018, ‘Age at arrival and assimilation during the age of mass migration,’ The Journal of Economic History, 78, no. 3, 904-937. (Example where I used differences between brothers to estimate the effect of education.) Angrist, Joshua D., and Jörn-Steffen Pischke, 2008, Mostly harmless econometrics: An empiricist’s companion, Princeton University Press, Chapter 4. Angrist, Joshua D., and Jörn-Steffen Pischke, 2008, Mostly harmless econometrics: An empiricist’s companion, Princeton University Press, Chapter 6. Angrist, Joshua D., and Jörn-Steffen Pischke, 2008, Mostly harmless econometrics: An empiricist’s companion, Princeton University Press, Chapters 3.3.2 and 5. Austin, Peter C., 2011, ‘An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies,’ Multivariate Behavioral Research, vol. 46, no. 3, pp.399-424. (Broad overview of propensity score matching, with a nice discussion of the comparison to randomised controlled trials.) Baker, Andrew, 2019, ‘Difference-in-Differences Methodology,’ 25 September, https://andrewcbaker.netlify.app/2019/09/25/difference-in-differences-methodology/. Coppock, Alenxader, and Donald P. Green, 2016, ‘Is Voting Habit Forming? New Evidence from Experiments and Regression Discontinuities,’ American Journal of Political Science, Volume 60, Issue 4, pp. 1044-1062, available at: https://onlinelibrary.wiley.com/doi/abs/10.1111/ajps.12210. (Has code and data.) Cunningham, Scott, ‘Causal Inference: The Mixtape,’ Chapter ‘Instrumental variables,’ http://www.scunning.com/causalinference_norap.pdf. Cunningham, Scott, ‘Causal Inference: The Mixtape,’ chapter ‘Regression discontinuity,’ http://www.scunning.com/causalinference_norap.pdf. Cunningham, Scott, Causal Inference: The Mixtape, chapters ‘Matching and subclassifications’ and ‘Differences-in-differences,’ http://www.scunning.com/causalinference_norap.pdf. (Very well-written notes on diff-in-diff.) Dell, Melissa, Pablo Querubin, 2018, ‘Nation Building Through Foreign Intervention: Evidence from Discontinuities in Military Strategies,’ The Quarterly Journal of Economics, Volume 133, Issue 2, pp. 701–764, https://doi.org/10.1093/qje/qjx037. Evans, David, 2013, ‘Regression Discontinuity Porn,’ World Bank Blogs, 16 November, https://blogs.worldbank.org/impactevaluations/regression-discontinuity-porn. Gelman, Andrew, 2019, ‘Another Regression Discontinuity Disaster and what can we learn from it,’ Statistical Modeling, Causal Inference, and Social Science, 25 June, https://statmodeling.stat.columbia.edu/2019/06/25/another-regression-discontinuity-disaster-and-what-can-we-learn-from-it/. Gelman, Andrew, and Guido Imbens, 2019, “Why high-order polynomials should not be used in regression discontinuity designs,” Journal of Business &amp; Economic Statistics, 37, pp. 447-456. Gelman, Andrew, and Jennifer Hill, 2007, Data Analysis Using Regression and Muiltilevel/Hierarchical Models, Chapter 10, pp. 207-215. Grogger, Jeffrey, Andreas Steinmayr, Joachim Winter, 2020, ‘The Wage Penalty of Regional Accents,’ NBER Working Paper No. 26719. Harris, Rich, Mlacki Migliozzi and Niraj Chokshi, ‘13,000 Missing Flights: The Global Consequences of the Coronavirus,’ New York Times, 21 February 2020. freely available here (if you make an account): https://www.nytimes.com/interactive/2020/02/21/business/coronavirus-airline-travel.html. Imai, Kosuke, 2017, Quantitative Social Science: An Introduction, Princeton University Press, Ch 2.5. Imbens, Guido W., and Thomas Lemieux, 2008, ‘Regression discontinuity designs: A guide to practice,’ Journal of Econometrics, vol. 142, no. 2, pp. 615-635. King, Gary, and Richard Nielsen, 2019, ‘Why Propensity Scores Should Not Be Used for Matching,’ Political Analysis. (Academic paper on the limits of propensity score matching. Propensity score matching was a big thing in the 90s but everyone knew about these weaknesses and so it died off. Lately, there has been a resurgence because of the CS/ML folks using it without thinking so King and Nielsen wrote a nice paper about the flaws. I mean, you can’t say you weren’t warned.) Myllyvirta, Lauri, 2020, ‘Analysis: Coronavirus has temporarily reduced China’s CO2 emissions by a quarter,’ Carbon Brief, 19 February, https://www.carbonbrief.org/analysis-coronavirus-has-temporarily-reduced-chinas-co2-emissions-by-a-quarter. Saeed, Sahar, Erica E. M. Moodie, Erin C. Strumpf, Marina B. Klein, 2019, ‘Evaluating the impact of health policies: using a difference-in-differences approach,’ International Journal of Public Health, 64, pp. 637–642, https://doi.org/10.1007/s00038-018-1195-2. Taddy, Matt, 2019, Business Data Science, Chapter 5, pp. 146-162. Tang, John, 2015, ‘Pollution havens and the trade in toxic chemicals: evidence from U.S. trade flows,’ Ecological Economics, vol. 112, pp. 150-160. (Example of using diff-in-diff to estimate pollution.) Travis, D.J., Carleton, A.M. and Lauritsen, R.G., 2004. ‘Regional variations in US diurnal temperature range for the 11–14 September 2001 aircraft groundings: Evidence of jet contrail influence on climate,’ Journal of climate, 17(5), pp.1123-1134. Travis, David J., Andrew M. Carleton, and Ryan G. Lauritsen. “Contrails reduce daily temperature range.” Nature, 418, no. 6898 (2002): 601-601. Valencia Caicedo, Felipe. ‘The mission: Human capital transmission, economic persistence, and culture in South America.’ The Quarterly Journal of Economics 134.1 (2019): 507-556. (Data available at: Valencia Caicedo, Felipe, 2018, “Replication Data for: ‘The Mission: Human Capital Transmission, Economic Persistence, and Culture in South America’,” https://doi.org/10.7910/DVN/ML1155, Harvard Dataverse, V1.). Zinovyeva, Natalia and Maryna Tverdostup, 2019, ‘Why are women who earn slightly more than their husbands hard to find?’ 10 June, https://blogs.lse.ac.uk/businessreview/2019/06/10/why-are-women-who-earn-slightly-more-than-their-husbands-hard-to-find/. Key concepts/skills/etc Essential matching methods. Weaknesses of matching. Difference-in-differences. Identifying opportunities for instrumental variables. Implementing instrumental variables. Challenges to the validity of instrumental variables. Reading in foreign data. Difference in differences. Replicating work. Displaying multiple regression results. Discussing results. Generating simulated data. Understanding regression discontinuity and implementing it both manually and using packages. Appreciating the threats to the validity of regression discontinuity. Key libraries broom tidyverse estimatr tidyverse haven huxtable scales tidyverse broom rdrobust tidyverse Key functions/etc tidy() lm() iv_robust() dollar_format() hux_reg() lm() mutate_at() read_dta() lm() tidy() rdrobust()() Quiz Sharla Gelfand has been ‘(s)haring two #rstats functions most days - one I know and love, and one that’s new to me!’ Please go to Sharla’s GitHub page: https://github.com/sharlagelfand/twofunctionsmostdays. Please find a package that she mentions that you have never used. Please find the relevant website for the package. Please describe what the package does and a context in which it could be useful to you. Sharla Gelfand has been ‘(s)haring two #rstats functions most days - one I know and love, and one that’s new to me!’ Please go to Sharla’s GitHub page: https://github.com/sharlagelfand/twofunctionsmostdays. Please find a function that she mentions that you have never used. Please look at the help file for that function. Please detail the arguments of the function, and a context in which it could be useful to you. What is propensity score matching? If you were matching people, then what are some of the features that you would like to match on? What sort of ethical questions does collecting and storing such information raise for you? Putting to one side, the ethical issues, what are some statistical weaknesses with propensity score matching? What is the key assumption when using diff-in-diff? Please read the fascinating article in The Markup about car insurance algorithms: https://themarkup.org/allstates-algorithm/2020/02/25/car-insurance-suckers-list. Please read the article and tell me what you think. You may wish to focus on ethical, legal, social, statistical, or other, aspects. Please go to the GitHub page related to the fascinating article in The Markup about car insurance algorithms: https://github.com/the-markup/investigation-allstates-algorithm. What is great about their work? What could be improved? What are the fundamental features of regression discontinuity design? What are the conditions that are needed in order for RDD to be able to be used? Can you think of a situation in your own life where RDD may be useful? What are some threats to the validity of RDD estimates? Please look at the performance package: https://easystats.github.io/performance/index.html. What are some features of this package that may be useful in your own work? What do you think about using COVID-19 in an RDD setting? Statistically? Ethically? Please read and reproduce the main findings from Eggers, Fowler, Hainmueller, Hall, Snyder, 2015. What is an instrumental variable? What are some circumstances in which instrumental variables might be useful? What conditions must instrumental variables satisfy? Who were some of the early instrumental variable authors? Can you please think of and explain an application of instrumental variables in your own life? What is the key assumption in difference-in-differences Parallel trends. Heteroscedasticity. If you’re using regression discontinuity, whare are some aspects to be aware of and think really hard about (select all that apply)? Is the cut-off free of manipulation? Is the forcing function continuous? To what extent is the functional form driving the estimate? Would different fitted lines affect the results? What is the main reason that Oostrom (2021) finds that the outcome of an RCT can depend on who is funding it (pick one)? Publication bias Explicit manipulation Specialisation Larger number of arms What is the key coefficient of interest in Angelucci and Cagé, 2019 (pick one)? \\(\\beta_0\\) \\(\\beta_1\\) \\(\\lambda\\) \\(\\gamma\\) The instrumental variable is (please pick all that apply): Correlated with the treatment variable. Not correlated with the outcome. Heteroskedastic. Who are the two candidates to have invented instrumental variables? Sewall Wright Philip G. Wright Sewall Cunningham Philip G. Cunningham What are the two main assumptions of instrumental variables? Exclusion Restriction. Relevance. Ignorability. Randomization. According to Meng, 2021, ‘Data science can persuade via…’ (pick all that apply): the careful establishment of evidence from fair-minded and high-quality data collection processing and analysis the honest interpretation and communication of findings large sample sizes According to Reiderer, 2021, if I have ‘disjoint treated and untreated groups partitioned by a sharp cut-off’ then which method should I use to measure the local treatment effect at the juncture between groups (pick one)? regression discontinuity matching difference-in-differences event study methods According to Reiderer, 2021, ‘Causal inference requires investment in’ (pick all that apply): data management domain knowledge probabilistic reasoning data science I am an Australian 30-39 year old male living in Toronto with one child and a PhD. Which of the following do you think I would match most closely with and why (please explain in a paragraph or two)? An Australian 30-39 year old male living in Toronto with one child and a bachelors degree A Canadian 30-39 year old male living in Toronto with one child and a PhD An Australian 30-39 year old male living in Ottawa with one child and a PhD A Canadian 18-29 year old male living in Toronto with one child and a PhD In your most disdainful tone (jokes, I love DAGs), what is a DAG (in your own words please)? What is a confounder (please select one answer)? A variable, z, that causes both x and y, where x also causes y. A variable, z, that is caused by both x and y, where x also causes y. A variable, z, that causes y and is caused by x, where x also causes y. What is a mediator (please select one answer)? A variable, z, that causes y and is caused by x, where x also causes y. A variable, z, that causes both x and y, where x also causes y. A variable, z, that is caused by both x and y, where x also causes y. What is a collider (please select one answer)? A variable, z, that causes both x and y, where x also causes y. A variable, z, that causes y and is caused by x, where x also causes y. A variable, z, that is caused by both x and y, where x also causes y. Please talk through a brief example of when you may want to be very careful about checking for Simpson’s paradox. Please talk through a brief example of when you may want to be very careful about checking for Berkson’s paradox. According to McElreath (2020, 162) ‘Regression will not sort it out. Regression is indeed an oracle, but a cruel one. It speaks in riddles and delights in punishing us for…’ (please select one answer)? overcomplicating models. asking bad questions. using bad data. Is a model that fits the small or large world more important to you, and why? According to Gebru et al, 2020, p.2, a datasheet should document a dataset’s (please select all that apply): composition. recommended uses. motivation. collection process. 15.1 Introduction Life it grand when you can conduct experiments to be able to speak to causality. But what if you can only run the survey - you can’t run an experiment? Here we begin our discussion of the circumstances and methods that would allow you to nonetheless speak to causality. We use (relatively) simple methods, in sophisticated, well-developed, ways (cf, much of what is done these days) and our applied statistics draw from a variety of social sciences including economics, and political science. Following the publication of Dagan et al. (2021), one of the authors tweeted (slight edits for formatting): We’ve just confirmed the effectiveness of the Pfizer-BioNTech vaccine outside of randomized trials. Yes, great news, but let’s talk about methodological issues that arise when using observational data to estimate vaccine effectiveness. A critical concern in observational studies of vaccine effectiveness is confounding: Suppose that people who get vaccinated have, on average, a lower risk of infection/disease than those who don’t get vaccinated. Then, even if the vaccine were useless, it’d look beneficial. To adjust for confounding: We start by identifying potential confounders. For example: Age (vaccination campaigns prioritize older people and older people are more likely to develop severe disease). Then we choose a valid adjustment method. In our paper, we matched on age. After age adjustment, how do we know if there is residual confounding? Here is one way to go about that: We know from the previous randomized trial that the vaccine has no effect in the first few days. So we check whether matching on age suffices to replicate that finding. No, it doesn’t. After matching on age (and sex), the curves of infection start to diverge from day 0, which indicates that the vaccinated had a lower risk of infection than the unvaccinated. Conclusion: adjustment for age and sex is insufficient. We learned that we had to match on other COVID-19 risk factors, e.g., location, comorbidities, healthcare use… And we could do so with high-quality data from the Clalit Research Institute, part of a health services organization that covers &gt;50% of the Israeli population. As an example, a vaccinated 76 year-old Arab male from a specific neighborhood who received 4 influenza vaccines in the last 5 years and had 2 comorbidities was matched with an unvaccinated Arab male from the same neighborhood, aged 76-77, with 3-4 influenza vaccines and 2 comorbidities. After matching on all those risk factors, the curves of infection start to diverge after day ~12, as expected if the vaccinated and the unvaccinated had a comparable risk of infection. Using this “negative control,” we provide evidence against large residual confounding. This is a good illustration of how randomized trials and observational studies complement each other for better and more efficient #causalinference. First, a randomized trial is conducted to estimate the effectiveness of the vaccine to prevent symptomatic infection, but… the trial’s estimates for severe disease and specific age groups are imprecise. Second, an observational analysis emulates a #targettrial (an order of magnitude greater) and confirms the vaccine’s effectiveness on severe disease and in different age groups. However… the observational study needs the trial’s findings as a benchmark to guide the data analysis and strengthen the quality of the causal inference. Randomized trials &amp; Observational studies working together. The best of both worlds. Let’s keep doing it after the pandemic. What a luxury having been able to think about these issues with my colleagues Noa Dagan, Noam Barda, Marc Lipsitch, Ben Reis, and Ran D. Balicer. We hope that our experience is helpful for researchers around the world who use observational data to estimate vaccine effectiveness. Miguel Hernán, 24 Februrary 2021. This is what this chapter is about. How we can nonetheless be comfortable making causal statements, even when we can’t run A/B tests or RCTs. Indeed, in what circumstances may we actually prefer to not run those or to run observational-based approaches in addition to them. We cover three of the major methods that are in popular use these days: difference-in-differences; regression discontinuity; and instrumental variables. 15.2 DAGs and trying not to be tricked by the data 15.2.1 DAGs and confounding When we are discussing causality it can help to be very specific about what we mean. It’s easy to get caught up in the data that it tricks you. It’s important to think really hard. One framework to help with this that has become popular recently is the use of directed acyclic graph (DAG), which is essentially just a fancy name for a flow diagram. A DAG involves drawing arrows between your variables indicating the relationship between them. We will use the DiagrammeR package to draw them (Iannone 2020), because that provides quite a lot of control (Figure 15.1). However it can be a little finicky and if you’re just looking to do something really quickly then the ggdag package can be useful (Barrett 2021b). The code to draw these DAGs draws heavily on Igelström (2020). library(DiagrammeR) DiagrammeR::grViz(&quot; digraph { graph [ranksep = 0.2] node [shape = plaintext] x y edge [minlen = 2, arrowhead = vee] x-&gt;y { rank = same; x; y } } &quot;) Figure 15.1: Using a DAG to illustrate perceived relationships In this example, we claim that \\(x\\) causes \\(y\\). We could build another where the situation is less clear. I find all the \\(x\\) and \\(y\\) very confusing, so will change to fruits (Figure 15.2). DiagrammeR::grViz(&quot; digraph { graph [ranksep = 0.2] node [shape = plaintext] Apple Banana Carrot edge [minlen = 2, arrowhead = vee] Apple-&gt;Banana Carrot-&gt;Apple Carrot-&gt;Banana { rank = same; Apple; Banana } } &quot;) Figure 15.2: Carrot as a confounder In this case we again think \\(apple\\) causes \\(banana\\). But it’s also clear that \\(carrot\\) causes \\(banana\\), and \\(carrot\\) also causes \\(apple\\). That relationship is a ‘backdoor path,’ and would create spurious correlation in our analysis. We may think that changes in \\(apple\\) are causing changes in \\(banana\\), but it’s actually that \\(carrot\\) is changing them both and hence that variable is called a ‘confounder.’ There is an excellent discussion by Hernan and Robins (2020, 83): Suppose an investigator conducted an observational study to answer the causal question “does one’s looking up to the sky make other pedestrians look up too?” She found an association between a first pedestrian’s looking up and a second one’s looking up. However, she also found that pedestrians tend to look up when they hear a thunderous noise above. Thus it was unclear what was making the second pedestrian look up, the first pedestrian’s looking up or the thunderous noise? She concluded the effect of one’s looking up was confounded by the presence of a thunderous noise. In randomized experiments treatment is assigned by the flip of a coin, but in observational studies treatment (e.g., a person’s looking up) may be determined by many factors (e.g., a thunderous noise). If those factors affect the risk of developing the outcome (e.g., another person’s looking up), then the effects of those factors become entangled with the effect of treatment. We then say that there is confounding, which is just a form of lack of exchangeability between the treated and the untreated. Confounding is often viewed as the main shortcoming of observational studies. In the presence of confounding, the old adage “association is not causation” holds even if the study population is arbitrarily large. If we were interested in causal effects we would need to adjust for \\(carrot\\), or \\(thunder\\) and one way is to include it in the regression. However, the validity of this requires a number of assumptions. In particular, Gelman and Hill (2007, 169) warns us our estimate will only correspond to the average causal effect in the sample if: 1) we include ‘all confounding covariates’; and 2) ‘the model is correct.’ Putting to one side the second requirement, and focusing only on the first, if we don’t observe a confounder, then we can’t adjust for it. This is where the role of domain experts, experience, and theory, really add a lot to an analysis. We might have a similar situation, again we think that \\(apple\\) causes \\(banana\\), but this time \\(apple\\) also causes \\(carrot\\), which itself causes \\(banana\\) (Figure 15.3). DiagrammeR::grViz(&quot; digraph { graph [ranksep = 0.2] node [shape = plaintext] Apple Banana Carrot edge [minlen = 2, arrowhead = vee] Apple-&gt;Banana Apple-&gt;Carrot Carrot-&gt;Banana { rank = same; Apple; Banana } } &quot;) Figure 15.3: Carrot as a mediator In this case, \\(carrot\\) is called a ‘mediator’ and we’d not like to adjust for it, because that would affect our estimate of the effect of \\(apple\\) on \\(banana\\). Finally, we might have yet another similar situation, where we again think that \\(apple\\) causes \\(banana\\), but this time both \\(apple\\) and \\(banana\\) cause \\(carrot\\) (Figure 15.4). DiagrammeR::grViz(&quot; digraph { graph [ranksep = 0.2] node [shape = plaintext] Apple Banana Carrot edge [minlen = 2, arrowhead = vee] Apple-&gt;Banana Apple-&gt;Carrot Banana-&gt;Carrot { rank = same; Apple; Banana } } &quot;) Figure 15.4: Carrot as a collider In this case, \\(carrot\\) is called a ‘collider’ and if we were to condition on it we would create a misleading relationship. I’ve been circling around this point for a while, but it’s time to address it. You will create your DAG - there is nothing that will create it for you. That means that you need to think really carefully about the situation. Because it’s one thing to see something in the DAG and then do something about it. But it’s another to not know that it’s there. McElreath (2020, 180) describes these as haunted DAGs. DAGs have become fashionable. They are of course helpful, but they are just a tool to help you think really deeply about your situation. As McElreath (2020, 162) says ‘Regression will not sort it out. Regression is indeed an oracle, but a cruel one. It speaks in riddles and delights in punishing us for asking bad questions.’ The same is true of DAGs, and all the methods that we cover in these notes. 15.2.2 Selection and measurement bias Selection bias occurs when the outcomes are dependent on ‘the process by which individuals are selected into the analysis’ (Hernan and Robins 2020, 99). We are not going to be able to see this from a DAG (I mean one could draw a DAG that shows it, but you need to know about it in order to draw that DAG is what I mean), or many default diagnostics. One way to go about things is A/A testing in an experimental settings, or comparing the sample with some more general characteristics, for instance age-group, gender, and education. But the fundamental point, as Dr Jill Sheppard says, is that ‘people who respond to surveys are weird,’ and this generalises to whatever method you’re using to gather your data. Using Facebook ads? People who click on Facebook ads are weird. Going door knocking? People who answer their door are weird. Call people on the phone? Literally who answers their phone anymore. There is a pernicious aspect of selection bias, which is that it pervades every aspect of your analysis. Even a sample that starts off as perfectly representative, may become selected over time. For instance, the survey panels used for political polling need to be updated from time to time because the folks who don’t get anything out of it stop responding - but those people may still vote and so need to be polled. Another bias to be very aware of is measurement bias, which when ‘the association between treatment and outcome is weakened or strengthened as a result of the process by which the study data are measured’ (Hernan and Robins 2020, 113). It is implicit in the definition by Hernan and Robins (2020), but it is important here that this be systematic. For instance, if I ask people in person what their income is then that is likely to get different answers than if I ask over the phone or via an online form. 15.2.3 Two common paradoxes There are two situations where data can trick you that are so common that I’d like to explicitly go through them. These are Simpson’s paradox, and Berkson’s paradox. Keep these situations in the back of your mind at all times when dealing with data. Simpson’s paradox occurs when we estimate some relationship for subsets of our data, but a different relationship when we consider the entire dataset (E. H. Simpson 1951). For instance, it may be that there is a positive relationship between undergraduate grades and performance in graduate school in both statistics and economics when considering each department individually. But if undergraduate grades tended to be higher in statistics than economics while graduate school performance tended to be opposite, we may actually find a negative relationship between undergraduate grades and performance in graduate school. To see this let’s simulate some data. set.seed(853) number_in_each &lt;- 1000 statistics &lt;- tibble(undergrad = runif(n = number_in_each, min = 0.7, max = 0.9), noise = rnorm(n = number_in_each, 0, sd = 0.1), grad = undergrad + noise, type = &quot;Statistics&quot;) economics &lt;- tibble(undergrad = runif(n = number_in_each, min = 0.6, max = 0.8), noise = rnorm(n = number_in_each, 0, sd = 0.1), grad = undergrad + noise + 0.3, type = &quot;Economics&quot;) both = rbind(statistics, economics) both %&gt;% ggplot(aes(x = undergrad, y = grad)) + geom_point(aes(color = type), alpha = 0.1) + geom_smooth(aes(color = type), method = &#39;lm&#39;, formula = &#39;y ~ x&#39;) + geom_smooth(method = &#39;lm&#39;, formula = &#39;y ~ x&#39;, color = &#39;black&#39;) + labs(x = &quot;Undergraduate results&quot;, y = &quot;Graduate results&quot;, color = &quot;Type&quot;) + theme_minimal() + scale_color_brewer(palette = &quot;Set1&quot;) Berkson’s paradox occurs when we estimate some relationship based on the dataset that we have, but because the dataset is selected the relationship is different in a more general dataset (Berkson 1946). For instance, if we have a dataset of professional cyclists then we would find there is no relationship between their VO2 max and their chance of winning a bike race. But if we had a dataset of the general population then we would find an enormous relationship between their VO2 max and their chance of winning a bike race. The professional dataset has just been so selected that the relationship disappears - you can’t become a professional unless you have a high VO2 max. To see this let’s simulate some data. set.seed(853) number_of_pros &lt;- 100 number_of_public &lt;- 1000 professionals &lt;- tibble(VO2 = runif(n = number_of_pros, min = 0.7, max = 0.9), chance_of_winning = runif(n = number_of_pros, min = 0.7, max = 0.9), type = &quot;Professionals&quot;) general_public &lt;- tibble(VO2 = runif(n = number_of_public, min = 0.6, max = 0.8), noise = rnorm(n = number_of_public, 0, sd = 0.03), chance_of_winning = VO2 + noise + 0.1, type = &quot;Public&quot;) %&gt;% select(-noise) both = rbind(professionals, general_public) both %&gt;% ggplot(aes(x = VO2, y = chance_of_winning)) + geom_point(aes(color = type), alpha = 0.1) + geom_smooth(aes(color = type), method = &#39;lm&#39;, formula = &#39;y ~ x&#39;) + geom_smooth(method = &#39;lm&#39;, formula = &#39;y ~ x&#39;, color = &#39;black&#39;) + labs(x = &quot;VO2 max&quot;, y = &quot;Chance of winning a bike race&quot;, color = &quot;Type&quot;) + theme_minimal() + scale_color_brewer(palette = &quot;Set1&quot;) 15.3 Difference in differences 15.3.1 Matching and difference-in-differences 15.3.1.1 Introduction The ideal situation of being able to conduct an experiment is rarely possible in a data science setting. Can we really reasonably expect that Netflix would allow us to change prices. And even if they did once, would they let us do it again, and again, and again? Further, rarely can we explicitly create treatment and control groups. Finally, experiments are really expensive and potentially unethical. Instead, we need to make do with what we have. Rather than our counterfactual coming to us through randomisation, and hence us knowing that the two are the same but for the treatment, we try to identify groups that were similar before the treatment, and hence any differences can be attributed to the treatment. In practice, we tend to even have differences between our two groups before we treat. Provided those pre-treatment differences satisfy some assumptions (basically that they were consistent, and we expect that consistency to continue in the absence of the treatment) – the ‘parallel trends’ assumption – then we can look to any difference in the differences as the effect of the treatment. One of the lovely aspects of difference in differences analysis is that we can do it using fairly straight-forward quantitative methods - linear regression with a dummy variable is all that is needed to do a convincing job. 15.3.1.2 Motivation Consider us wanting to know the effect of a new tennis racket on serve speed. One way to test this would be to measure the difference between Roger Federer’s serve speed without the tennis racket and mine with the tennis racket. Sure, we’d find a difference but how do we know how much to attribute to the tennis racket? Another way would be to consider the difference between my serve speed without the tennis racket and my serve speed with the tennis racket. But what if serves were just getting faster naturally over time? Instead, let’s combine the two to look at the difference in the differences! In this world we measure Federer’s serve and compare it to my serve without the new racket. We then measure Federer’s serve again and measure my serve with the new racket. That difference in the differences would then be our estimate of the effect of the new racket. What sorts of assumptions jump out at you that we are going to have to make in order for this analysis to be appropriate? Is there something else that may have affected only me, and not Roger that could affect my serve speed? Probably. Is it likely that Roger Federer and I have the same trajectory of serve speed improvement? Probably not. This is the ‘parallel trends’ assumption, and it dominates any discussion of difference in differences analysis. Finally, is it likely that the variance of our serve speeds is the same? Probably not. Why might this be powerful? We don’t need the treatment and control group to be the same before the treatment. We just need to have a good idea of how they differ. 15.3.1.3 Simulated example Let’s generate some data. library(broom) library(tidyverse) set.seed(853) diff_in_diff_example_data &lt;- tibble(person = rep(c(1:1000), times = 2), time = c(rep(0, times = 1000), rep(1, times = 1000)), treatment_group = rep(sample(x = 0:1, size = 1000, replace = TRUE), times = 2) ) ## We want to make the outcome slightly more likely if they were treated than if not. diff_in_diff_example_data &lt;- diff_in_diff_example_data %&gt;% rowwise() %&gt;% mutate(serve_speed = case_when( time == 0 &amp; treatment_group == 0 ~ rnorm(n = 1, mean = 5, sd = 1), time == 1 &amp; treatment_group == 0 ~ rnorm(n = 1, mean = 6, sd = 1), time == 0 &amp; treatment_group == 1 ~ rnorm(n = 1, mean = 8, sd = 1), time == 1 &amp; treatment_group == 1 ~ rnorm(n = 1, mean = 14, sd = 1), ) ) head(diff_in_diff_example_data) ## # A tibble: 6 x 4 ## # Rowwise: ## person time treatment_group serve_speed ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 0 0 4.43 ## 2 2 0 1 6.96 ## 3 3 0 1 7.77 ## 4 4 0 0 5.31 ## 5 5 0 0 4.09 ## 6 6 0 0 4.85 Let’s make a graph. diff_in_diff_example_data$treatment_group &lt;- as.factor(diff_in_diff_example_data$treatment_group) diff_in_diff_example_data$time &lt;- as.factor(diff_in_diff_example_data$time) diff_in_diff_example_data %&gt;% ggplot(aes(x = time, y = serve_speed, color = treatment_group)) + geom_point() + geom_line(aes(group = person), alpha = 0.2) + theme_minimal() + labs(x = &quot;Time period&quot;, y = &quot;Serve speed&quot;, color = &quot;Person got a new racket&quot;) + scale_color_brewer(palette = &quot;Set1&quot;) As it is a simple example, we could do this manually, by getting the average difference of the differences. average_differences &lt;- diff_in_diff_example_data %&gt;% pivot_wider(names_from = time, values_from = serve_speed, names_prefix = &quot;time_&quot;) %&gt;% mutate(difference = time_1 - time_0) %&gt;% group_by(treatment_group) %&gt;% summarise(average_difference = mean(difference)) average_differences$average_difference[2] - average_differences$average_difference[1] ## [1] 5.058414 Let’s use OLS to do the same analysis. The general regression equation is: \\[Y_{i,t} = \\beta_0 + \\beta_1\\mbox{Treatment group dummy}_i + \\beta_2\\mbox{Time dummy}_t + \\beta_3(\\mbox{Treatment group dummy} \\times\\mbox{Time dummy})_{i,t} + \\epsilon_{i,t}\\] If we use * in the regression then it automatically includes the separate aspects as well as their interaction. It’s the estimate of \\(\\beta_3\\) which is of interest. diff_in_diff_example_regression &lt;- lm(serve_speed ~ treatment_group*time, data = diff_in_diff_example_data) tidy(diff_in_diff_example_regression) Table 15.1: termestimatestd.errorstatisticp.value (Intercept)4.970.0428116&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; treatment_group13.030.062248.70&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; time11.010.060516.62.97e-58 treatment_group1:time15.060.088&nbsp;57.50&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fortunately, our estimates are the same! 15.3.1.4 Assumptions If we want to use difference in differences, then we need to satisfy the assumptions. There were three that were touched on earlier, but here I want to focus on one: the ‘parallel trends’ assumption. The parallel trends assumption haunts everything to do with diff-in-diff analysis because we can never prove it, we can just be convinced of it. To see why we can never prove it, consider an example in which we want to know the effect of a new stadium on a professional sports team’s wins/loses. To do this we consider two teams: the Warriors and the Raptors. The Warriors changed stadiums at the start of the 2019-20 season (the Raptors did not), so we will consider four time periods: the 2016-17 season, 2017-18 season, 2018-19 season, and finally we will compare the performance with the one after they moved, so in the 2019-20 season. The Raptors here act as our counterfactual. This means that we assume the relationship between the Warriors and the Raptors, in the absence of a new stadium, would have continued to change in a consistent way. But we can never know that for certain. We have to present sufficient evidence to assuage any concerns that a reader may have. For a variety of reasons, it is worth having tougher than normal requirements around the evidence it would take to convince you of an effect. There are four main ‘threats to validity’ when you are using difference in differences and you should address all of these (Cunningham, 2020, pp. 272–277): Non-parallel trends. The treatment and control groups may be based on differences. As such it can be difficult to convincingly argue for parallel trends. In this case, maybe try to find another factor to consider in your model that may adjust for some of that. This may require difference in difference in differences (in the earlier example, perhaps could add in the San Francisco 49ers as they are in the same broad geographic area as the Warriors). Or maybe re-think your analysis to see if you can make a different control group. Adding additional earlier time periods may help but may introduce more issues (see third point). Compositional differences. This is a concern when working with repeated cross-sections. What if the composition of those cross-sections change? For instance, if we work at Tik Tok or some other app that is rapidly growing and want to look at the effect of some change. In our initial cross-section, we may have mostly young people, but in a subsequent cross-section, we may have more older people as the demographics of the app usage change. Hence our results may just be an age-effect, not an effect of the change that we are interested in. Long-term effects vs. reliability. As we discussed in the last chapter, there is a trade-off between the length of the analysis that we run. As we run the analysis for longer there is more opportunity for other factors to affect the results. There is also increased chance for someone who was not treated to be treated. But, on the other hand, it can be difficult to convincingly argue that short-term results will continue in the long-term. Functional form dependence. This is less of an issue when the outcomes are similar, but if they are different then functional form may be responsible for some aspects of the results. 15.3.1.5 Matching This section draws on material from Gelman and Hill, 2007, pp. 207-212. Difference in differences is a powerful analysis framework. After I learnt about it I began to see opportunities to implement it everywhere. But it can be tough to identify appropriate treatment and control groups. In Alexander and Ward, 2018, we compare migrant brothers - one of whom had most of their education in a different country, and the other who had most of their education in the US. Is this really the best match? We may be able to match based on observable variables. For instance, age-group or education. At two different times we compare smoking rates in 18-year-olds in one city with smoking rates in 18-year-olds in another city. That is fine, but it is fairly coarse. We know that there are differences between 18-year-olds, even in terms of the variables that we commonly observe, say gender and education. One way to deal with this may be to create sub-groups: 18-year-old males with a high school education, etc. But the sample sizes are likely to quickly become small. How do we deal with continuous variables? And also, is the difference between an 18-year-old and a 19-year-old really so different? Shouldn’t we also compare with them? One way to proceed is to consider a nearest neighbour approach. But there is limited concern for uncertainty in this approach. There is also an issue if you have a large number of variables because you end up with a high-dimension graph. This leads us to propensity score matching. Propensity score matching involves assigning some probability to each observation. We construct that probability based on the observation’s values for the independent variables, at their values before the treatment. That probability is our best guess at the probability of the observation being treated, regardless of whether it was treated or not. For instance, if 18-year-old males were treated but 19-year-old males were not, then as there is not much difference between 18-year-old males and 19-year-old males our assigned probability would be fairly similar. We can then compare the outcomes of observations with similar propensity scores. One advantage of propensity score matching is that is allows us to easily consider many independent variables at once, and it can be constructed using logistic regression. Let’s generate some data to illustrate propensity score matching. Let’s pretend that we work for Amazon. We are going to treat some individuals with free-shipping to see what happens to their average purchase. library(tidyverse) sample_size &lt;- 10000 set.seed(853) amazon_purchase_data &lt;- tibble( unique_person_id = c(1:sample_size), age = runif(n = sample_size, min = 18, max = 100), city = sample( x = c(&quot;Toronto&quot;, &quot;Montreal&quot;, &quot;Calgary&quot;), size = sample_size, replace = TRUE ), gender = sample( x = c(&quot;Female&quot;, &quot;Male&quot;, &quot;Other/decline&quot;), size = sample_size, replace = TRUE, prob = c(0.49, 0.47, 0.02) ), income = rlnorm(n = sample_size, meanlog = 0.5, sdlog = 1) ) Now we need to add some probability of being treated with free shipping, which depends on our variables. Younger, higher-income, male and in Toronto all make it slightly more likely. amazon_purchase_data &lt;- amazon_purchase_data %&gt;% mutate(age_num = case_when( age &lt; 30 ~ 3, age &lt; 50 ~ 2, age &lt; 70 ~ 1, TRUE ~ 0), city_num = case_when( city == &quot;Toronto&quot; ~ 3, city == &quot;Montreal&quot; ~ 2, city == &quot;Calgary&quot; ~ 1, TRUE ~ 0), gender_num = case_when( gender == &quot;Male&quot; ~ 3, gender == &quot;Female&quot; ~ 2, gender == &quot;Other/decline&quot; ~ 1, TRUE ~ 0), income_num = case_when( income &gt; 3 ~ 3, income &gt; 2 ~ 2, income &gt; 1 ~ 1, TRUE ~ 0) ) %&gt;% rowwise() %&gt;% mutate(sum_num = sum(age_num, city_num, gender_num, income_num), softmax_prob = exp(sum_num)/exp(12), free_shipping = sample( x = c(0:1), size = 1, replace = TRUE, prob = c(1-softmax_prob, softmax_prob) ) ) %&gt;% ungroup() amazon_purchase_data &lt;- amazon_purchase_data %&gt;% dplyr::select(-age_num, -city_num, -gender_num, -income_num, -sum_num, -softmax_prob) Finally, we need to have some measure of a person’s average spend. We want those with free shipping to be slightly higher than those without. amazon_purchase_data &lt;- amazon_purchase_data %&gt;% mutate(mean_spend = if_else(free_shipping == 1, 60, 50)) %&gt;% rowwise() %&gt;% mutate(average_spend = rnorm(1, mean_spend, sd = 5) ) %&gt;% ungroup() %&gt;% dplyr::select(-mean_spend) ## Fix the class on some amazon_purchase_data &lt;- amazon_purchase_data %&gt;% mutate_at(vars(city, gender, free_shipping), ~as.factor(.)) ## Change some to factors table(amazon_purchase_data$free_shipping) ## ## 0 1 ## 9629 371 head(amazon_purchase_data) Table 15.2: unique_person_idagecitygenderincomefree_shippingaverage_spend 147.5CalgaryFemale1.72&nbsp;041.1 227.8MontrealMale1.54&nbsp;055.7 357.7TorontoFemale3.16&nbsp;056.5 443.9TorontoMale0.636050.5 521.1TorontoFemale1.43&nbsp;044.7 651.1CalgaryMale1.18&nbsp;048.8 Now we construct a logistic regression model that ‘explains’ whether a person was treated as a function of the variables that we think explain it. propensity_score &lt;- glm(free_shipping ~ age + city + gender + income, family = binomial, data = amazon_purchase_data) We will now add our forecast to our dataset. amazon_purchase_data &lt;- augment(propensity_score, data = amazon_purchase_data, type.predict = &quot;response&quot;) %&gt;% dplyr::select(-.resid, -.std.resid, -.hat, -.sigma, -.cooksd) Now we use our forecast to create matches. There are a variety of ways to do this. In a moment I’ll step through some code that does it all at once, but as this is a worked example and we only have a small number of possibilities, we can just do it manually. For every person who was actually treated (given free shipping) we want the untreated person who was considered as similar to them (based on propensity score) as possible. amazon_purchase_data &lt;- amazon_purchase_data %&gt;% arrange(.fitted, free_shipping) Here we’re going to use a matching function from the arm package. This finds which is the closest of the ones that were not treated, to each one that was treated. amazon_purchase_data$treated &lt;- if_else(amazon_purchase_data$free_shipping == 0, 0, 1) amazon_purchase_data$treated &lt;- as.integer(amazon_purchase_data$treated) matches &lt;- arm::matching(z = amazon_purchase_data$treated, score = amazon_purchase_data$.fitted) amazon_purchase_data &lt;- cbind(amazon_purchase_data, matches) Now we reduce the dataset to just those that are matched. We had 371 treated, so we expect a dataset of 742 observations. amazon_purchase_data_matched &lt;- amazon_purchase_data %&gt;% filter(match.ind != 0) %&gt;% dplyr::select(-match.ind, -pairs, -treated) head(amazon_purchase_data_matched) Table 15.3: unique_person_idagecitygenderincomefree_shippingaverage_spend.fittedcnts 571081.2MontrealFemale0.675&nbsp;047.40.001381 945897&nbsp;&nbsp;MontrealFemale9.5&nbsp;&nbsp;&nbsp;161.20.001381 642883.2CalgaryMale0.0585049.90.001561 202299&nbsp;&nbsp;MontrealMale1.67&nbsp;&nbsp;157.80.001561 982464.6CalgaryFemale3.35&nbsp;&nbsp;164.70.002211 127297.1TorontoFemale0.718&nbsp;056.60.002211 Finally, we can examine the ‘effect’ of being treated on average spend in the ‘usual’ way. propensity_score_regression &lt;- lm(average_spend ~ age + city + gender + income + free_shipping, data = amazon_purchase_data_matched) huxtable::huxreg(propensity_score_regression) Table 15.4: (1) (Intercept)49.694 *** (0.809)&nbsp;&nbsp;&nbsp; age0.005&nbsp;&nbsp;&nbsp;&nbsp; (0.011)&nbsp;&nbsp;&nbsp; cityMontreal0.169&nbsp;&nbsp;&nbsp;&nbsp; (0.734)&nbsp;&nbsp;&nbsp; cityToronto0.652&nbsp;&nbsp;&nbsp;&nbsp; (0.623)&nbsp;&nbsp;&nbsp; genderMale-0.968 *&nbsp;&nbsp; (0.422)&nbsp;&nbsp;&nbsp; genderOther/decline-1.973&nbsp;&nbsp;&nbsp;&nbsp; (2.621)&nbsp;&nbsp;&nbsp; income0.009&nbsp;&nbsp;&nbsp;&nbsp; (0.021)&nbsp;&nbsp;&nbsp; free_shipping110.488 *** (0.380)&nbsp;&nbsp;&nbsp; N742&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.513&nbsp;&nbsp;&nbsp;&nbsp; logLik-2267.486&nbsp;&nbsp;&nbsp;&nbsp; AIC4552.971&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. I cover propensity score matching here because it is widely used. Hence, you need to know how to use it. People would think it’s weird if you didn’t, in the same way that we have to cover ANOVA people would think it’s weird if we had an entire experimental design course and didn’t cover it even though there are more modern ways of looking at differences between two means. But at the same time you need to know that there are flaws with propensity score matching. I will now discuss some of them. Matching. Propensity score matching cannot match on unobserved variables. This may be fine in a class-room setting, but in more realistic settings it will likely cause issues. Modelling. The results tend to be specific to the model that is used. King and Nielsen, 2019, discuss this thoroughly. Statistically. We are using the data twice. 15.4 Case study - Lower advertising revenue reduced French newspaper prices between 1960 and 1974 15.4.1 Introduction In this case study we introduce Angelucci and Cagé, 2019, and replicate its main findings. Angelucci and Cagé, 2019, is a paper in which difference in differences is used to examine the effect of the reduction in advertising revenues on newspapers’ content and prices. They create a dataset of ‘French newspapers between 1960 and 1974.’ They ‘perform a difference-in-differences analysis’ and exploit ‘the introduction of advertising on television’ as this change ‘affected national newspapers more severely than local ones.’ They ‘find robust evidence of a decrease in the amount of journalistic-intensive content produced and the subscription price.’ In order to conduct this analysis we will use the dataset that they provide alongside their paper. This dataset is available at: https://www.openicpsr.org/openicpsr/project/116438/version/V1/view. It is available for you to download after registration. As their dataset is in Stata data format, we will use the haven package to read it in (Wickham and Miller, 2019). library(here) library(haven) library(huxtable) library(scales) library(tidyverse) 15.4.2 Background Newspapers are in trouble. We can probably all think of a local newspaper that has closed recently because of pressure brought on by the internet. But this issue isn’t new. When television started, there were similar concerns. In this paper, Angelucci and Cagé use the introduction of television advertising in France, announced in 1967, to examine the effect of decreased advertising revenue on newspapers. The reason this is important is because it allows us to disentangle a few competing effects. For instance, are newspapers becoming redundant because they can no longer charge high prices for their ads or because consumers prefer to get their news in other ways? Are fewer journalists needed because smartphones and other technology mean they can be more productive? Angelucci and Cagé look at advertising revenue and a few other features, when a new advertising platform arrives, in this case television advertising. 15.4.3 Data (The) dataset contains annual data on local and national newspapers between 1960 and 1974, as well as detailed information on television content. In 1967, the French government announced it would relax long-standing regulations that prohibited television advertising. We provide evidence that this reform can be plausibly interpreted as an exogenous and negative shock to the advertising side of the newspaper industry… [I]t is likely that the introduction of television advertising constituted a direct shock to the advertising side of the newspaper industry and only an indirect shock to the reader side… (O)ur empirical setting constitutes a unique opportunity to isolate the consequences of a decrease in newspapers’ advertising revenues on their choices regarding the size of their newsroom, the amount of information to produce, and the prices they charge to both sides of the market. The authors’ argue that national newspapers were affected by the television advertising change, but local newspapers were not. So the national newspapers are the treatment group and the local newspapers are the control group. The dataset can be read in using read_dta(), which is a function within the haven package for reading in Stata dta files. This is equivalent to read_csv(). newspapers &lt;- read_dta(here::here(&quot;inputs/data/116438-V1/data/dta/Angelucci_Cage_AEJMicro_dataset.dta&quot;)) dim(newspapers) ## [1] 1196 52 There are 1,196 observations in the dataset and 52 variables. The authors are interested in the 1960-1974 time period which has around 100 newspapers. There are 14 national newspapers at the beginning of the period and 12 at the end. We just want to replicate their main results, so we don’t need all their variables. As such we will just select() the ones that we are interested in and change the class() where needed. newspapers &lt;- newspapers %&gt;% dplyr::select(year, id_news, after_national, local, national, ## Diff in diff variables ra_cst, qtotal, ads_p4_cst, ads_s, ## Advertising side dependents ps_cst, po_cst, qtotal, qs_s, rs_cst) %&gt;% #Reader side dependents mutate(ra_cst_div_qtotal = ra_cst / qtotal) %&gt;% ## An advertising side dependents needs to be built mutate_at(vars(id_news, after_national, local, national), ~as.factor(.)) %&gt;% ## Change some to factors mutate(year = as.integer(year)) We can now have a look at the main variables of interest for both national (Figure 15.5) and local daily newspapers (Figure 15.6). Figure 15.5: Angelucci and Cagé, 2019, summary statistics: national daily newspapers Source: Angelucci and Cagé, 2019, p. 333. Figure 15.6: Angelucci and Cagé, 2019, summary statistics: local daily newspapers Source: Angelucci and Cagé, 2019, p. 334. Please read this section of their paper to see how they describe their dataset. We are interested in the change from 1967 onward. newspapers %&gt;% mutate(type = if_else(local == 1, &quot;Local&quot;, &quot;National&quot;)) %&gt;% ggplot(aes(x = year, y = ra_cst)) + geom_point(alpha = 0.5) + scale_y_continuous(labels = dollar_format(prefix=&quot;$&quot;, suffix = &quot;M&quot;, scale = 0.000001)) + labs(x = &quot;Year&quot;, y = &quot;Advertising revenue&quot;) + facet_wrap(vars(type), nrow = 2) + theme_classic() + geom_vline(xintercept = 1966.5, linetype = &quot;dashed&quot;) 15.4.4 Model The model that we are interested in estimating is: \\[\\mbox{ln}(y_{n,t}) = \\beta_0 + \\beta_1(\\mbox{National dummy}\\times\\mbox{1967 onward dummy}) + \\lambda_n + \\gamma_y + \\epsilon.\\] The \\(\\lambda_n\\) is a fixed effect for each newspaper, and the \\(\\gamma_y\\) is a fixed effect for each year. We just use regular linear regression, with a few different dependent variables. It is the \\(\\beta_1\\) coefficient that we are interested in. 15.4.5 Results We can run the models using lm(). ## Advertising side ad_revenue &lt;- lm(log(ra_cst) ~ after_national + id_news + year, data = newspapers) ad_revenue_div_circulation &lt;- lm(log(ra_cst_div_qtotal) ~ after_national + id_news + year, data = newspapers) ad_price &lt;- lm(log(ads_p4_cst) ~ after_national + id_news + year, data = newspapers) ad_space &lt;- lm(log(ads_s) ~ after_national + id_news + year, data = newspapers) ## Consumer side subscription_price &lt;- lm(log(ps_cst) ~ after_national + id_news + year, data = newspapers) unit_price &lt;- lm(log(po_cst) ~ after_national + id_news + year, data = newspapers) circulation &lt;- lm(log(qtotal) ~ after_national + id_news + year, data = newspapers) share_of_sub &lt;- lm(log(qs_s) ~ after_national + id_news + year, data = newspapers) revenue_from_sales &lt;- lm(log(rs_cst) ~ after_national + id_news + year, data = newspapers) Looking at the advertising-side variables. omit_me &lt;- c(&quot;(Intercept)&quot;, &quot;id_news3&quot;, &quot;id_news6&quot;, &quot;id_news7&quot;, &quot;id_news13&quot;, &quot;id_news16&quot;, &quot;id_news25&quot;, &quot;id_news28&quot;, &quot;id_news34&quot;, &quot;id_news38&quot;, &quot;id_news44&quot;, &quot;id_news48&quot;, &quot;id_news51&quot;, &quot;id_news53&quot;, &quot;id_news54&quot;, &quot;id_news57&quot;, &quot;id_news60&quot;, &quot;id_news62&quot;, &quot;id_news66&quot;, &quot;id_news67&quot;, &quot;id_news70&quot;, &quot;id_news71&quot;, &quot;id_news72&quot;, &quot;id_news80&quot;, &quot;id_news82&quot;, &quot;id_news88&quot;, &quot;id_news95&quot;, &quot;id_news97&quot;, &quot;id_news98&quot;, &quot;id_news103&quot;, &quot;id_news105&quot;, &quot;id_news106&quot;, &quot;id_news118&quot;, &quot;id_news119&quot;, &quot;id_news127&quot;, &quot;id_news136&quot;, &quot;id_news138&quot;, &quot;id_news148&quot;, &quot;id_news151&quot;, &quot;id_news153&quot;, &quot;id_news154&quot;, &quot;id_news157&quot;, &quot;id_news158&quot;, &quot;id_news161&quot;, &quot;id_news163&quot;, &quot;id_news167&quot;, &quot;id_news169&quot;, &quot;id_news179&quot;, &quot;id_news184&quot;, &quot;id_news185&quot;, &quot;id_news187&quot;, &quot;id_news196&quot;, &quot;id_news206&quot;, &quot;id_news210&quot;, &quot;id_news212&quot;, &quot;id_news213&quot;, &quot;id_news224&quot;, &quot;id_news225&quot;, &quot;id_news234&quot;, &quot;id_news236&quot;, &quot;id_news245&quot;, &quot;id_news247&quot;, &quot;id_news310&quot;, &quot;id_news452&quot;, &quot;id_news467&quot;, &quot;id_news469&quot;, &quot;id_news480&quot;, &quot;id_news20040&quot;, &quot;id_news20345&quot;, &quot;id_news20346&quot;, &quot;id_news20347&quot;, &quot;id_news20352&quot;, &quot;id_news20354&quot;, &quot;id_news21006&quot;, &quot;id_news21025&quot;, &quot;id_news21173&quot;, &quot;id_news21176&quot;, &quot;id_news33718&quot;, &quot;id_news34689&quot;, &quot;id_news73&quot;) huxreg(&quot;Ad. rev.&quot; = ad_revenue, &quot;Ad rev. div. circ.&quot; = ad_revenue_div_circulation, &quot;Ad price&quot; = ad_price, &quot;Ad space&quot; = ad_space, omit_coefs = omit_me, number_format = 2 ) Table 15.5: Ad. rev.Ad rev. div. circ.Ad priceAd space after_national1-0.23 ***-0.15 ***-0.31 ***0.01&nbsp;&nbsp;&nbsp;&nbsp; (0.03)&nbsp;&nbsp;&nbsp;(0.03)&nbsp;&nbsp;&nbsp;(0.07)&nbsp;&nbsp;&nbsp;(0.05)&nbsp;&nbsp;&nbsp; year0.05 ***0.04 ***0.04 ***0.02 *** (0.00)&nbsp;&nbsp;&nbsp;(0.00)&nbsp;&nbsp;&nbsp;(0.00)&nbsp;&nbsp;&nbsp;(0.00)&nbsp;&nbsp;&nbsp; N1052&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1048&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;809&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1046&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.99&nbsp;&nbsp;&nbsp;&nbsp;0.90&nbsp;&nbsp;&nbsp;&nbsp;0.89&nbsp;&nbsp;&nbsp;&nbsp;0.72&nbsp;&nbsp;&nbsp;&nbsp; logLik345.34&nbsp;&nbsp;&nbsp;&nbsp;449.52&nbsp;&nbsp;&nbsp;&nbsp;-277.71&nbsp;&nbsp;&nbsp;&nbsp;-164.01&nbsp;&nbsp;&nbsp;&nbsp; AIC-526.68&nbsp;&nbsp;&nbsp;&nbsp;-735.05&nbsp;&nbsp;&nbsp;&nbsp;705.43&nbsp;&nbsp;&nbsp;&nbsp;478.02&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. Similarly, we can look at the reader-side variables. omit_me &lt;- c(&quot;(Intercept)&quot;, &quot;id_news3&quot;, &quot;id_news6&quot;, &quot;id_news7&quot;, &quot;id_news13&quot;, &quot;id_news16&quot;, &quot;id_news25&quot;, &quot;id_news28&quot;, &quot;id_news34&quot;, &quot;id_news38&quot;, &quot;id_news44&quot;, &quot;id_news48&quot;, &quot;id_news51&quot;, &quot;id_news53&quot;, &quot;id_news54&quot;, &quot;id_news57&quot;, &quot;id_news60&quot;, &quot;id_news62&quot;, &quot;id_news66&quot;, &quot;id_news67&quot;, &quot;id_news70&quot;, &quot;id_news71&quot;, &quot;id_news72&quot;, &quot;id_news80&quot;, &quot;id_news82&quot;, &quot;id_news88&quot;, &quot;id_news95&quot;, &quot;id_news97&quot;, &quot;id_news98&quot;, &quot;id_news103&quot;, &quot;id_news105&quot;, &quot;id_news106&quot;, &quot;id_news118&quot;, &quot;id_news119&quot;, &quot;id_news127&quot;, &quot;id_news136&quot;, &quot;id_news138&quot;, &quot;id_news148&quot;, &quot;id_news151&quot;, &quot;id_news153&quot;, &quot;id_news154&quot;, &quot;id_news157&quot;, &quot;id_news158&quot;, &quot;id_news161&quot;, &quot;id_news163&quot;, &quot;id_news167&quot;, &quot;id_news169&quot;, &quot;id_news179&quot;, &quot;id_news184&quot;, &quot;id_news185&quot;, &quot;id_news187&quot;, &quot;id_news196&quot;, &quot;id_news206&quot;, &quot;id_news210&quot;, &quot;id_news212&quot;, &quot;id_news213&quot;, &quot;id_news224&quot;, &quot;id_news225&quot;, &quot;id_news234&quot;, &quot;id_news236&quot;, &quot;id_news245&quot;, &quot;id_news247&quot;, &quot;id_news310&quot;, &quot;id_news452&quot;, &quot;id_news467&quot;, &quot;id_news469&quot;, &quot;id_news480&quot;, &quot;id_news20040&quot;, &quot;id_news20345&quot;, &quot;id_news20346&quot;, &quot;id_news20347&quot;, &quot;id_news20352&quot;, &quot;id_news20354&quot;, &quot;id_news21006&quot;, &quot;id_news21025&quot;, &quot;id_news21173&quot;, &quot;id_news21176&quot;, &quot;id_news33718&quot;, &quot;id_news34689&quot;, &quot;id_news73&quot;) huxreg(&quot;Subscription price&quot; = subscription_price, &quot;Unit price&quot; = unit_price, &quot;Circulation&quot; = circulation, &quot;Share of sub&quot; = share_of_sub, &quot;Revenue from sales&quot; = revenue_from_sales, omit_coefs = omit_me, number_format = 2 ) Table 15.6: Subscription priceUnit priceCirculationShare of subRevenue from sales after_national1-0.04 *&nbsp;&nbsp;0.06 **&nbsp;-0.06 **&nbsp;0.19 ***-0.06 *&nbsp;&nbsp; (0.02)&nbsp;&nbsp;&nbsp;(0.02)&nbsp;&nbsp;&nbsp;(0.02)&nbsp;&nbsp;&nbsp;(0.03)&nbsp;&nbsp;&nbsp;(0.03)&nbsp;&nbsp;&nbsp; year0.05 ***0.05 ***0.01 ***-0.01 ***0.05 *** (0.00)&nbsp;&nbsp;&nbsp;(0.00)&nbsp;&nbsp;&nbsp;(0.00)&nbsp;&nbsp;&nbsp;(0.00)&nbsp;&nbsp;&nbsp;(0.00)&nbsp;&nbsp;&nbsp; N1044&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1063&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1070&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1072&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1046&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.88&nbsp;&nbsp;&nbsp;&nbsp;0.87&nbsp;&nbsp;&nbsp;&nbsp;0.99&nbsp;&nbsp;&nbsp;&nbsp;0.97&nbsp;&nbsp;&nbsp;&nbsp;0.99&nbsp;&nbsp;&nbsp;&nbsp; logLik882.14&nbsp;&nbsp;&nbsp;&nbsp;907.28&nbsp;&nbsp;&nbsp;&nbsp;759.57&nbsp;&nbsp;&nbsp;&nbsp;321.91&nbsp;&nbsp;&nbsp;&nbsp;451.11&nbsp;&nbsp;&nbsp;&nbsp; AIC-1600.28&nbsp;&nbsp;&nbsp;&nbsp;-1650.57&nbsp;&nbsp;&nbsp;&nbsp;-1355.15&nbsp;&nbsp;&nbsp;&nbsp;-477.81&nbsp;&nbsp;&nbsp;&nbsp;-738.22&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. 15.4.6 Other points We certainly find that in many cases there appears to be a difference from 1967 onward. In general, we are able to obtain results that are similar to Angelucci and Cagé, 2019. If we spent more time, we could probably replicate their findings perfectly. Isn’t this great! What else could do? Parallel trends: Notice the wonderful way in which they test the ‘parallel trends’ assumption on pp. 350-351. Discussion: Look at their wonderful discussion (pp. 353-358) of interpretation, external validity, and robustness. –&gt; 15.5 Case study - Funding of Clinical Trials and Reported Drug Efficacy Oostrom (2021) looks at clinical trials of drugs. These days, of course, we all know a lot more than we may have ever wished to, about clinical trials. But the one thing that we (think) we know is that they are, well, clinical. By that I mean, that it doesn’t matter who does the actual trial, the outcome would be the same. Oostrom (2021) says this isn’t true. By way of background, clinical trials are needed before a drug can be approved. Oostrom (2021) finds that when pharmaceutical firms sponsor a clinical trial, ‘a drug appears 0.15 standard deviations more effective when the trial is sponsored by that drug’s manufacturer, compared with the same drug in the same trial without the drug manufacturer’s involvement.’ She does this by exploiting the fact that often ‘the exact same sets of drugs are often compared in different randomized control trials conducted by parties with different financial interests.’ The main finding is (Oostrom 2021, 2): Utilizing dozens of drug combinations across hundreds of clinical trials, I estimate that a drug appears 36 percent more effective (0.15 standard deviations off of a base of 0.42) when the trial is sponsored by that drug’s manufacturing or marketing firm, compared with the same drug, evaluated against the same comparators, but without the drug manufacturer’s involvement. As in the medical literature, I measure efficacy, in the case of antidepressants, as the share of patients that respond to medication or, in the case of schizophrenia, as the average decline in symptoms. Why might this happen? Oostrom (2021) looks at a variety of different options, grouped into those that happen before the trial and those that happen after the trial ‘publication bias.’ She finds that ‘publication bias can explain as much as half of this sponsorship effect. Incorporating data on unpublished clinical trials, I find sponsored trials are less likely to publish non-positive results for their drugs.’ Oostrom (2021) focuses on antidepressant and antipsychotic drugs and this allows her to obtain a dataset of trials. An ‘arm’ of a trial refers to ‘the unit at which randomization occurs. Arms are often unique drugs but occasionally refer to unique drug and dosage combinations.’ (Oostrom 2021, 9). Summary statistics are provided in a summary table (Figure 15.7) (this approach is common in economics, but not a great idea because it hides the distribution of the data - better to plot the raw data.) Figure 15.7: Summary statistics from Ooostrom The model is: \\[y_{ij} = \\alpha + \\beta \\mbox{ Sponsor}_{ij} + X_{ij}\\gamma + G_{d(i),s(j)} +\\epsilon_{ij}\\] where \\(y_{ij}\\) is the efficacy for arm \\(i\\) in trial \\(j\\). The main coefficient of interest is \\(\\beta\\) which is based on whether \\(\\mbox{Sponsor}_{ij}\\). The outcome is relative to the placebo arm in that trial, or the least effective arm. ‘Table 3.3’ from the paper is actually the reason that I included this as a case student. If this sounds odd to you then you’ve not had to read millions of papers that are unclear about their results. ‘Table 3.3’ (republished here as Figure 15.8) is beautiful and I’ll allow it to speak for itself. Figure 15.8: Results The paper is available here: https://www.tamaroostrom.com/research and I’d recommend a brief read. 15.6 Regression discontinuity design 15.6.1 Introduction Regression discontinuity design (RDD) is a popular way to get causality when there is some continuous variable with cut-offs that determine treatment. Is there a difference between a student who gets 79 per cent and a student who gets 80 per cent? Probably not much, but one gets an A-, while the other gets a B+, and seeing that on a transcript could affect who gets a job which could affect income. In this case the percentage is a ‘forcing variable’ and the cut-off for an A- is a ‘threshold.’ As the treatment is determined by the forcing variable all you need to do is to control for that variable. And, these seemingly arbitrary cut-offs can be seen all the time. Hence, there has been an ‘explosion’ in the use of regression discontinuity design (Figure 15.9). Please note that I’ve followed the terminology of Taddy, 2019. Gelman and Hill, 2007, and others use slightly different terminology. For instance, Cunningham refers to the forcing function as the running variable. It doesn’t matter what you use so long as you are consistent. If you have a terminology that you are familiar with then please feel free to use it, and to share it with me! Figure 15.9: The explosion of regression discontinuity designs in recent years. Source: John Holbein, 13 February 2020. The key assumptions are: The cut-off is ‘known, precise and free of manipulation’ (Cunningham, 2020, p. 163). The forcing function should be continuous because this means we can say that people on either side of the threshold are the same, other than happening to just fall on either side of the threshold. 15.6.2 Simulated example Let’s generate some data. library(broom) library(tidyverse) set.seed(853) number_of_observation &lt;- 1000 rdd_example_data &lt;- tibble(person = c(1:number_of_observation), grade = runif(number_of_observation, min = 78, max = 82), income = rnorm(number_of_observation, 10, 1) ) ## We want to make income more likely to be higher if they are have a grade over 80 rdd_example_data &lt;- rdd_example_data %&gt;% mutate(income = if_else(grade &gt; 80, income + 2, income)) head(rdd_example_data) Table 15.7: persongradeincome 179.49.43 278.59.69 379.910.8&nbsp; 479.39.34 578.110.7&nbsp; 679.69.83 Let’s make a graph. rdd_example_data %&gt;% ggplot(aes(x = grade, y = income)) + geom_point(alpha = 0.2) + geom_smooth(data = rdd_example_data %&gt;% filter(grade &lt; 80), method=&#39;lm&#39;, color = &quot;black&quot;) + geom_smooth(data = rdd_example_data %&gt;% filter(grade &gt;= 80), method=&#39;lm&#39;, color = &quot;black&quot;) + theme_minimal() + labs(x = &quot;Grade&quot;, y = &quot;Income ($)&quot;) We can use a dummy variable with linear regression to estimate the effect (we’re hoping that it’s 2 because that is what we imposed.) rdd_example_data &lt;- rdd_example_data %&gt;% mutate(grade_80_and_over = if_else(grade &lt; 80, 0, 1)) lm(income ~ grade + grade_80_and_over, data = rdd_example_data) %&gt;% tidy() Table 15.8: termestimatestd.errorstatisticp.value (Intercept)11.7&nbsp;&nbsp;4.24&nbsp;&nbsp;2.76&nbsp;0.00585&nbsp; grade-0.0210.0537-0.3910.696&nbsp;&nbsp;&nbsp; grade_80_and_over1.99&nbsp;0.123&nbsp;16.2&nbsp;&nbsp;1.34e-52 There are various caveats to this estimate that we’ll get into later, but the essentials are here. The other great thing about regression discontinuity is that is can almost be as good as an RCT. For instance, (and I thank John Holbein for the pointer) Bloom, Bell, and Reiman (2020) compare randomized trials with RDDs and find that the RCTs compare favourably. 15.6.2.1 Different slopes Figure 15.10 shows an example with different slopes. Figure 15.10: Effect of minimum unit pricing for alcohol in Scotland. Source: John Burn-Murdoch, 7 February 2020. 15.6.3 Overlap In the randomised control trial and A/B testing section, because of randomised assignment of the treatment, we imposed that the control and treatment groups were the same but for the treatment. We moved to difference-in-differences, and we assumed that there was a common trend between the treated and control groups. We allowed that the groups could be different, but that we could ‘difference out’ their differences. Finally, we considered matching, and we said that even if we the control and treatment groups seemed quite different we were able to match those who were treated with a group that were similar to them in all ways, apart from the fact that they were not treated. In regression discontinuity we consider a slightly different setting - the two groups are completely different in terms of the forcing variable - they are on either side of the threshold. So there is no overlap at all. But we know the threshold and believe that those on either side are essentially matched. Let’s consider the 2019 NBA Eastern Conference Semifinals - Toronto and the Philadelphia. Game 1: Raptors win 108-95; Game 2: 76ers win 94-89; Game 3: 76ers win 116-95; Game 4: Raptors win 101-96; Game 5: Raptors win 125-89; Game 6: 76ers win 112-101; and finally, Game 7: Raptors win 92-90, because of a ball that win in after bouncing on the rim four times. Was there really that much difference between the teams (Figure 15.11)? Figure 15.11: It took four bounces to go in, so how different were the teams…? Source: Stan Behal / Postmedia Network. 15.6.4 Examples As with difference-in-differences, after I learnt about it, I began to see opportunities to implement it everywhere. Frankly, I find it a lot easier to think of legitimate examples of using regression discontinuity than difference-in-differences. But, at the risk of mentioning yet another movie from the 1990s that none of you have seen, when I think of RDD, my first thought is often of Sliding Doors (Figure 15.12). Figure 15.12: Nobody expects the Spanish Inquisition. Source: Mlotek, Haley, 2018, ‘The Almosts and What-ifs of ’Sliding Doors’’, The Ringer, 24 April, freely available at: https://www.theringer.com/movies/2018/4/24/17261506/sliding-doors-20th-anniversary. Not only did the movie have a great soundtrack and help propel Gwyneth Paltrow to super-stardom, but it features an iconic moment in which Paltrow’s character, Helen, arrives at a tube station at which point the movie splits into two. In one version she just makes the train, and arrives home to find her boyfriend cheating on her; and in another she just misses the train and doesn’t find out about the boyfriend. I’d say, spoiler alert, but the movie was released in 1998, so… Of course, that ‘threshold’ turns out to be important. In the world in which she gets the train she leaves the boyfriend, cuts her hair, and changes everything about her life. In the world in which she misses the train she doesn’t. At least initially. But, and I can’t say this any better than Ashley Fetters: At the end of Sliding Doors, the “bad” version of Helen’s life elides right into the “good” version; even in the “bad” version, the philandering !@#$%^&amp; boyfriend eventually gets found out and dumped, the true love eventually gets met-cute, and the MVP friend comes through. According to the Sliding Doors philosophy, in other words, even when our lives take fluky, chaotic detours, ultimately good-hearted people find each other, and the bad boyfriends and home-wreckers of the world get their comeuppance. There’s no freak turn of events that allows the cheating boyfriend to just keep cheating, or the well-meaning, morally upright soulmates to just keep floating around in the universe unacquainted. Fetters, Ashley, 2018, ‘I Think About This a Lot: The Sliding Doors in Sliding Doors,’ The Cut, 9 April, freely available at: https://www.thecut.com/2018/04/i-think-about-this-a-lot-the-sliding-doors-in-sliding-doors.html. I’m getting off-track here, but the point is, not only does it seem as though we have a ‘threshold,’ but it seems as though there’s continuity! Let’s see some more legitimate implementations of regression discontinuity. (And thank you to Ryan Edwards for pointing me to these.) 15.6.4.1 Elections Elections are a common area of application for regression discontinuity because if the election is close then arguably there’s not much difference between the candidates. There are plenty of examples of regression discontinuity in an elections setting, but one recent one is George, Siddharth Eapen, 2019, ‘Like Father, Like Son? The Effect of Political Dynasties on Economic Development,’ freely available at: https://www.dropbox.com/s/orhvh3n03wd9ybl/sid_JMP_dynasties_latestdraft.pdf?dl=0. In this paper George is interested in political dynasties. But is the child of a politician more likely to be elected because they are the child of a politician, or because they happen to also be similarly skilled at politics? Regression discontinuity can help because in a close election, we can look at differences between places where someone narrowly won with where a similar someone narrowly lost. In the George, 2019, case he examines: descendant effects using a close elections regression discontinuity (RD) design. We focus on close races between dynastic descendants (i.e. direct relatives of former officeholders) and non-dynasts, and we compare places where a descendant narrowly won to those where a descendant narrowly lost. In these elections, descendants and non-dynasts have similar demographic and political characteristics, and win in similar places and at similar rates. Nevertheless, we find negative economic effects when a descendant narrowly wins. Villages represented by a descendant have lower asset ownership and public good provision after an electoral term: households are less likely to live in a brick house and to own basic amenities like a refrigerator, mobile phone, or vehicle. Moreover, voters assess descendants to perform worse in office. An additional standard deviation of exposure to descendants lowers a village’s wealth rank by 12pp. The model that George, 2019, estimates is (p. 19: \\[y_i = \\alpha_{\\mbox{district}} + \\beta \\times \\mbox{Years descendant rule}_i + f(\\mbox{Descendant margin}) + \\gamma X_i + \\epsilon_{i,t}.\\] In this model, \\(y_i\\) is various development outcomes in village \\(i\\); \\(\\mbox{Years descendant rule}_i\\) is the number of years a dynastic descendant has represented village \\(i\\) in the national or state parliament; \\(\\mbox{Descendant margin}\\) is the vote share difference between the dynastic descendant and non-dynast; and \\(\\gamma X_i\\) is a vector of village-level adjustments. George, 2019, then conducts a whole bunch of tests of the validity of the regression discontinuity design (p. 19). These are critical in order for the results to be believed. There are a lot of different results but one is shown in Figure 15.13. Figure 15.13: George, 2019, descendant effects identified using close elections RD design (p. 41). 15.6.4.2 Economic development One of the issues with considering economic development is that a place typically is either subject to some treatment or not. However, sometimes regression discontinuity allows us to compare areas that were just barely treated with those that were just barely not. One recent paper that does this Esteban Mendez-Chacon and Diana Van Patten, 2020, ‘Multinationals, monopsony and local development: Evidence from the United Fruit Company’ available here: https://www.dianavanpatten.com/. They are interested in the effect of the United Fruit Company (UFCo), which was given land in Costa Rica between 1889 and 1984. They were given roughly 4 per cent of the national territory or around 4500 acres. They key is that this land assignment was redrawn in 1904 based on a river and hence the re-assignment was essentially random with regard to determinants of growth to that point. They compare areas that were assigned to UFCo with those that were not. They find: We find that the firm had a positive and persistent effect on living standards. Regions within the UFCo were 26 per cent less likely to be poor in 1973 than nearby counterfactual locations, with only 63 per cent of the gap closing over the following three decades. Company documents explain that a key concern at the time was to attract and maintain a sizable workforce, which induced the firm to invest heavily in local amenities that likely account for our result. The model is: \\[y_{i,g,t} = \\gamma\\mbox{UFCo}_g + f(\\mbox{geographic location}_g) + X_{i,g,t}\\beta + X_g\\Gamma + \\alpha_t + \\epsilon_{i,g,t}.\\] In this model, \\(y_{i,g,t}\\) is the development outcome for a household \\(i\\) in census-block \\(g\\) and year \\(t\\); \\(\\gamma\\mbox{UFCo}_g\\) is an indicator variable as to whether the census-block was in a UFCo area or not; \\(f(\\mbox{geographic location}_g)\\) is a function of the latitude and longitude to adjust for geographic area; \\(X_{i,g,t}\\) is covariates for household \\(i\\); \\(X_g\\) is geographic characteristics for that census-block; and \\(\\alpha_t\\) is a year fixed effect. Again, there are a lot of different results but one is shown in Figure 15.14. Figure 15.14: George, 2020, UFCo effect on the probability of being poor (p. 17). 15.6.5 Implementation Although they are fairly conceptually similar to work that we have done in the past, if you are wanting to use regression discontinuity in your work then you might like to consider a specialised package. The package rdrobust is a one recommendation, although there are others available and you should try those if you are interested. (The rdd package had been the go-to for a while, but seems to have been taken off CRAN recently. If you use RDD, then maybe just follow up to see if it comes back on as that one is pretty nice.) Let’s look at our example using rdrobust. library(rdrobust) rdrobust(y = rdd_example_data$income, x = rdd_example_data$grade, c = 80, h = 2, all = TRUE) %&gt;% summary() ## Call: rdrobust ## ## Number of Obs. 1000 ## BW type Manual ## Kernel Triangular ## VCE method NN ## ## Number of Obs. 497 503 ## Eff. Number of Obs. 497 503 ## Order est. (p) 1 1 ## Order bias (q) 2 2 ## BW est. (h) 2.000 2.000 ## BW bias (b) 2.000 2.000 ## rho (h/b) 1.000 1.000 ## Unique Obs. 497 503 ## ## ============================================================================= ## Method Coef. Std. Err. z P&gt;|z| [ 95% C.I. ] ## ============================================================================= ## Conventional 1.974 0.143 13.783 0.000 [1.693 , 2.255] ## Bias-Corrected 1.977 0.143 13.805 0.000 [1.696 , 2.258] ## Robust 1.977 0.211 9.374 0.000 [1.564 , 2.390] ## ============================================================================= 15.6.6 Fuzzy RDD The examples to this point have been ‘sharp’ RDD. That is, the threshold is strict. However, in reality, often the boundary is a little less strict. For instance, consider the drinking age. Although there is a legal drinking age, say 19. If we looked at the number of people who had drank, then it’s likely to increase in the few years leading up to that age. Perhaps you went to Australia where the drinking age is 18 and drank. Or perhaps you snuck into a bar when you were 17, etc. In a sharp RDD setting, if you know the value of the forcing function then you know the outcome. For instance, if you get a grade of 80 then we know that you got an A-, but if you got a grade of 79 then we know that you got a B+. But with fuzzy RDD it is only known with some probability. We can say that a Canadian 19-year-old is more likely to have drunk alcohol than a Canadian 18 year old, but the number of Canadian 18-year-olds who have drunk alcohol is not zero. It may be possible to deal with fuzzy RDD settings with appropriate choice of model or data. It may also be possible to deal with them using instrumental variables, which we cover in the next section. 15.6.7 Threats to validity The continuity assumption is fairly important, but we cannot test this as it is based on a counterfactual. Instead we need to convince people of it. Ways to do this include: Using a test/train set-up. Trying different specifications (and be very careful if your results don’t broadly persist when just consider linear or quadratic functions). Considering different subsets of the data. Consider different windows. Be up-front about uncertainty intervals, especially in graphs. Discuss and assuage concerns about the possibility of omitted variables. The threshold is also important. For instance, is there an actual shift or is there a non-linear relationship? We want as ‘sharp’ an effect as possible, but if the thresholds are known, then they will be gamed. For instance, there is a lot of evidence that people run for certain marathon times, and we know that people aim for certain grades. Similarly, from the other side, it is a lot easier for an instructor to just give out As than it is to have to justify Bs. One way to look at this is to consider how ‘balanced’ the sample is on either side of the threshold. Do this by using histograms with appropriate bins, for instance Figure 15.15, which is from E. J. Allen et al. (2017). Figure 15.15: Bunching around marathon times. You need to really think about the possible effect of the decision around the choice of model. To see this consider the difference between a linear and polynomial. some_data &lt;- tibble(outcome = rnorm(n = 100, mean = 1, sd = 1), running_variable = c(1:100), location = &quot;before&quot;) some_more_data &lt;- tibble(outcome = rnorm(n = 100, mean = 2, sd = 1), running_variable = c(101:200), location = &quot;after&quot;) both &lt;- rbind(some_data, some_more_data) both %&gt;% ggplot(aes(x = running_variable, y = outcome, color = location)) + geom_point(alpha = 0.5) + geom_smooth(formula = y~x, method = &#39;lm&#39;) both %&gt;% ggplot(aes(x = running_variable, y = outcome, color = location)) + geom_point(alpha = 0.5) + geom_smooth(formula = y ~ poly(x, 3), method = &#39;lm&#39;) 15.6.8 Weaknesses External validity may be hard - think about the A-/B+ example - do you think the findings generalise to B-/C+? The important responses are those that are close to the cut-off. So even if we have a whole bunch of B- and A+ students, they don’t really help much. Hence we need a lot of data. There is a lot of freedom for the researcher, so open science best practice becomes vital. 15.7 Case study - Stiers, Hooghe, and Dassonneville, 2020 Paper: Stiers, D., Hooghe, M. and Dassonneville, R., 2020. Voting at 16: Does lowering the voting age lead to more political engagement? Evidence from a quasi-experiment in the city of Ghent (Belgium). Political Science Research and Methods, pp.1-8. Available at: https://www.cambridge.org/core/journals/political-science-research-and-methods/article/voting-at-16-does-lowering-the-voting-age-lead-to-more-political-engagement-evidence-from-a-quasiexperiment-in-the-city-of-ghent-belgium/172A2D9B75ECB66E98C9680787F302AD#fndtn-information Data: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/J1FQW9 15.8 Case study - Caughey, and Sekhon., 2011 Paper: Caughey, Devin, and Jasjeet S. Sekhon. “Elections and the regression discontinuity design: Lessons from close US house races, 1942–2008.” Political Analysis 19.4 (2011): 385-408. Available at: https://www.cambridge.org/core/journals/political-analysis/article/elections-and-the-regression-discontinuity-design-lessons-from-close-us-house-races-19422008/E5A69927D29BE682E012CAE9BFD8AEB7 Data: https://dataverse.harvard.edu/dataset.xhtml?persistentId=hdl:1902.1/16357&amp;version=1.0 15.9 Instrumental variables 15.9.1 Introduction Instrumental variables (IV) is an approach that can be handy when we have some type of treatment and control going on, but we have a lot of correlation with other variables and we possibly don’t have a variable that actually measures what we are interested in. So adjusting for observables will not be enough to create a good estimate. Instead we find some variable - the eponymous instrumental variable - that is: correlated with the treatment variable, but not correlated with the outcome. This solves our problem because the only way the instrumental variable can have an effect is through the treatment variable, and so we are able to adjust our understanding of the effect of the treatment variable appropriately. The trade-off is that instrumental variables must satisfy a bunch of different assumptions, and that, frankly, they are difficult to identify ex ante. Nonetheless, when you are able to use them they are a powerful tool for speaking about causality. The canonical instrumental variables example is smoking. These days we know that smoking causes cancer. But because smoking is correlated with a lot of other variables, for instance, education, it could be that it was actually education that causes cancer. RCTs may be possible, but they are likely to be troublesome in terms of speed and ethics, and so instead we look for some other variable that is correlated with smoking, but not, in and of itself, with lung cancer. In this case, we look to tax rates, and other policy responses, on cigarettes. As the tax rates on cigarettes are correlated with the number of cigarettes that are smoked, but not correlated with lung cancer, other than through their impact on cigarette smoking, through them we can assess the effect of cigarettes smoked on lung cancer. To implement instrumental variables we first regress tax rates on cigarette smoking to get some coefficient on the instrumental variable, and then (in a separate regression) regress tax rates on lung cancer to again get some coefficient on the instrumental variable. Our estimate is then the ratio of these coefficients. (Gelman and Hill 2007, 219) describe this ratio as the ‘Wald estimate.’ Following the language of (Gelman and Hill 2007, 216) when we use instrumental variables we make a variety of assumptions including: Ignorability of the instrument. Correlation between the instrumental variable and the treatment variable. Monotonicity. Exclusion restriction. To summarise exactly what instrumental variables is about, I cannot do better than recommend the first few pages of the ‘Instrumental Variables’ chapter in Cunningham (2020), and this key paragraph in particular (by way of background, Cunningham has explained why it would have been impossible to randomly allocate ‘clean’ and ‘dirty’ water through a randomised controlled trial and then continues…): Snow would need a way to trick the data such that the allocation of clean and dirty water to people was not associated with the other determinants of cholera mortality, such as hygiene and poverty. He just would need for someone or something to be making this treatment assignment for him. Fortunately for Snow, and the rest of London, that someone or something existed. In the London of the 1800s, there were many different water companies serving different areas of the city. Some were served by more than one company. Several took their water from the Thames, which was heavily polluted by sewage. The service areas of such companies had much higher rates of cholera. The Chelsea water company was an exception, but it had an exceptionally good filtration system. That’s when Snow had a major insight. In 1849, Lambeth water company moved the intake point upstream along the Thames, above the main sewage discharge point, giving its customers purer water. Southwark and Vauxhall water company, on the other hand, left their intake point downstream from where the sewage discharged. Insofar as the kinds of people that each company serviced were approximately the same, then comparing the cholera rates between the two houses could be the experiment that Snow so desperately needed to test his hypothesis. 15.9.2 History The history of instrumental variables is a rare statistical mystery, and Stock and Trebbi (2003) provide a brief overview. The method was first published in Wright (1928). This is a book about the effect of tariffs on animal and vegetable oil. So why might instrumental variables be important in a book about tariffs on animal and vegetable oil? The fundamental problem is that the effect of tariffs depends on both supply and demand. But we only know prices and quantities, so we don’t know what is driving the effect. We can use instrumental variables to pin down causality. Where is gets interesting, and becomes something of a mystery, is that the instrumental variables discussion is only in Appendix B. If you made a major statistical break-through would you hide it in an appendix? Further, Philip G. Wright, the book’s author, had a son Sewall Wright, who had considerable expertise in statistics and the specific method used in Appendix B. Hence the mystery of Appendix B - did Philip or Sewall write it? Both Cunningham (2020) and Stock and Trebbi (2003) go into more detail, but on balance feel that it is likely that Philip did actually author the work. 15.9.3 Simulated example Let’s generate some data. We will explore a simulation related to the canonical example of health status, smoking, and tax rates. So we are looking to explain how healthy someone is based on the amount they smoke, via the tax rate on smoking. We are going to generate different tax rates by provinces. My understanding is that the tax rate on cigarettes is now pretty much the same in each of the provinces, but that this is fairly recent. So we’ll pretend that Alberta had a low tax, and Nova Scotia had a high tax. As a reminder, we are simulating data for illustrative purposes, so we need to impose the answer that we want. When you actually use instrumental variables you will be reversing the process. library(broom) library(tidyverse) set.seed(853) number_of_observation &lt;- 10000 iv_example_data &lt;- tibble(person = c(1:number_of_observation), smoker = sample(x = c(0:1), size = number_of_observation, replace = TRUE) ) Now we need to relate the number of cigarettes that someone smoked to their health. We’ll model health status as a draw from the normal distribution, with either a high or low mean depending on whether the person smokes. iv_example_data &lt;- iv_example_data %&gt;% mutate(health = if_else(smoker == 0, rnorm(n = n(), mean = 1, sd = 1), rnorm(n = n(), mean = 0, sd = 1) ) ) ## So health will be one standard deviation higher for people who don&#39;t or barely smoke. Now we need a relationship between cigarettes and the province (because in this illustration, the provinces have different tax rates). iv_example_data &lt;- iv_example_data %&gt;% rowwise() %&gt;% mutate(province = case_when(smoker == 0 ~ sample(x = c(&quot;Nova Scotia&quot;, &quot;Alberta&quot;), size = 1, replace = FALSE, prob = c(1/2, 1/2)), smoker == 1 ~ sample(x = c(&quot;Nova Scotia&quot;, &quot;Alberta&quot;), size = 1, replace = FALSE, prob = c(1/4, 3/4)))) %&gt;% ungroup() iv_example_data &lt;- iv_example_data %&gt;% mutate(tax = case_when(province == &quot;Alberta&quot; ~ 0.3, province == &quot;Nova Scotia&quot; ~ 0.5, TRUE ~ 9999999 ) ) iv_example_data$tax %&gt;% table() ## . ## 0.3 0.5 ## 6206 3794 head(iv_example_data) Table 15.9: personsmokerhealthprovincetax 101.11&nbsp;&nbsp;Alberta0.3 21-0.0831Alberta0.3 31-0.0363Alberta0.3 402.48&nbsp;&nbsp;Alberta0.3 500.617&nbsp;Alberta0.3 600.748&nbsp;Nova Scotia0.5 Now we can look at our data. iv_example_data %&gt;% mutate(smoker = as_factor(smoker)) %&gt;% ggplot(aes(x = health, fill = smoker)) + geom_histogram(position = &quot;dodge&quot;, binwidth = 0.2) + theme_minimal() + labs(x = &quot;Health rating&quot;, y = &quot;Number of people&quot;, fill = &quot;Smoker&quot;) + scale_fill_brewer(palette = &quot;Set1&quot;) + facet_wrap(vars(province)) Finally, we can use the tax rate as an instrumental variable to estimate the effect of smoking on health. health_on_tax &lt;- lm(health ~ tax, data = iv_example_data) smoker_on_tax &lt;- lm(smoker ~ tax, data = iv_example_data) coef(health_on_tax)[&quot;tax&quot;] / coef(smoker_on_tax)[&quot;tax&quot;] ## tax ## -0.8554502 So we find, luckily, that if you smoke then your health is likely to be worse than if you don’t smoke. Equivalently, we can think of instrumental variables in a two-stage regression context. first_stage &lt;- lm(smoker ~ tax, data = iv_example_data) health_hat &lt;- first_stage$fitted.values second_stage &lt;- lm(health ~ health_hat, data = iv_example_data) summary(second_stage) ## ## Call: ## lm(formula = health ~ health_hat, data = iv_example_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.9867 -0.7600 0.0068 0.7709 4.3293 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.91632 0.04479 20.46 &lt;2e-16 *** ## health_hat -0.85545 0.08911 -9.60 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.112 on 9998 degrees of freedom ## Multiple R-squared: 0.009134, Adjusted R-squared: 0.009034 ## F-statistic: 92.16 on 1 and 9998 DF, p-value: &lt; 2.2e-16 15.9.4 Implementation As with regression discontinuity, although it is possible to use existing functions, it might be worth looking at specialised packages. Instrumental variables has a few moving pieces, so a specialised package can help keep everything organised, and additionally, standard errors need to be adjusted and specialised packages make this easier. The package estimatr is a recommendation, although there are others available and you should try those if you are interested. The estimatr package is from the same team as DeclareDesign. Let’s look at our example using iv_robust(). library(estimatr) iv_robust(health ~ smoker | tax, data = iv_example_data) %&gt;% summary() ## ## Call: ## iv_robust(formula = health ~ smoker | tax, data = iv_example_data) ## ## Standard error type: HC2 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper DF ## (Intercept) 0.9163 0.04057 22.59 3.163e-110 0.8368 0.9958 9998 ## smoker -0.8555 0.08047 -10.63 2.981e-26 -1.0132 -0.6977 9998 ## ## Multiple R-squared: 0.1971 , Adjusted R-squared: 0.197 ## F-statistic: 113 on 1 and 9998 DF, p-value: &lt; 2.2e-16 15.9.5 Assumptions As discussed earlier, there are a variety of assumptions that are made when using instrumental variables. The two most important are: Exclusion Restriction. This assumption is that the instrumental variable only affects the dependent variable through the independent variable of interest. Relevance. There must actually be a relationship between the instrumental variable and the independent variable. There is typically a trade-off between these two. There are plenty of variables that When thinking about potential instrumental variables Cunningham (2020), p. 211, puts it brilliantly: But, let’s say you think you do have a good instrument. How might you defend it as such to someone else? A necessary but not a sufficient condition for having an instrument that can satisfy the exclusion restriction is if people are confused when you tell them about the instrument’s relationship to the outcome. Let me explain. No one is going to be confused when you tell them that you think family size will reduce female labor supply. They don’t need a Becker model to convince them that women who have more children probably work less than those with fewer children. It’s common sense. But, what would they think if you told them that mothers whose first two children were the same gender worked less than those whose children had a balanced sex ratio? They would probably give you a confused look. What does the gender composition of your children have to do with whether a woman works? It doesn’t – it only matters, in fact, if people whose first two children are the same gender decide to have a third child. Which brings us back to the original point – people buy that family size can cause women to work less, but they’re confused when you say that women work less when their first two kids are the same gender. But if when you point out to them that the two children’s gender induces people to have larger families than they would have otherwise, the person “gets it,” then you might have an excellent instrument. Relevance can be tested using regression and other tests for correlation. The exclusion restriction cannot be tested. You need to present evidence and convincing arguments. As Cunningham (2020) p. 225 says ‘Instruments have a certain ridiculousness to them[.] That is, you know you have a good instrument if the instrument itself doesn’t seem relevant for explaining the outcome of interest because that’s what the exclusion restriction implies.’ 15.9.6 Conclusion Instrumental variables is a useful approach because one can obtain causal estimates even without explicit randomisation. Finding instrumental variables used to be a bit of a white whale, especially in academia. However, I will leave the final (and hopefully motivating) word to Taddy (2019), p. 162: As a final point on the importance of IV models and analysis, note that when you are on the inside of a firm—especially on the inside of a modern technology firm—explicitly randomised instruments are everywhere…. But it is often the case that decision-makers want to understand the effects of policies that are not themselves randomised but are rather downstream of the things being AB tested. For example, suppose an algorithm is used to predict the creditworthiness of potential borrowers and assign loans. Even if the process of loan assignment is never itself randomised, if the parameters in the machine learning algorithms used to score credit are AB tested, then those experiments can be used as instruments for the loan assignment treatment. Such ‘upstream randomisation’ is extremely common and IV analysis is your key tool for doing causal inference in that setting.’ 15.10 Case study - Effect of Police on Crime 15.10.1 Overview Here we’ll use an example of Levitt (2002) that looks at the effect of police on crime. This is interesting because you might think, that more police is associated with lower crime. But, it could actually be the opposite, if more crime causes more police to be hired - how many police would a hypothetical country with no crime need? Hence there is a need to find some sort of instrumental variable that affects crime only through its relationship with the number of police (that is, not in and of itself, related to crime), and yet is also correlated with police numbers. Levitt (2002) suggests the number of firefighters in a city. Levitt (2002) argues that firefighters are appropriate as an instrument, because ‘(f)actors such as the power of public sector unions, citizen tastes for government services, affirmative action initiatives, or a mayor’s desire to provide spoils might all be expected to jointly influence the number of firefighters and police.’ Levitt (2002) also argues that the relevance assumption is met by showing that ‘changes in the number of police officers and firefighters within a city are highly correlated over time.’ In terms of satisfying the exclusion restriction, Levitt (2002) argues that the number of firefighters should not have a ‘direct impact on crime.’ However, it may be that there are common factors, and so Levitt (2002) adjusts for this in the regression. 15.10.2 Data The dataset is based on 122 US cities between 1975 and 1995. Summary statistics are provided in Figure 15.16. Figure 15.16: Summary statistics for Levitt 2002. Source: Levitt (2002) p. 1,246. 15.10.3 Model In the first stage Levitt (2002) looks at police as a function of firefighters, and a bunch of adjustment variables: \\[\\ln(\\mbox{Police}_{ct}) = \\gamma \\ln(\\mbox{Fire}_{ct}) + X&#39;_{ct}\\Gamma + \\lambda_t + \\phi_c + \\epsilon_{ct}.\\] The important part of this is the police and firefighters numbers which are on a per capita basis. There are a bunch of adjustment variables in \\(X\\) which includes things like state prisoners per capita, the unemployment rate, etc, as well as year dummy variables and fixed-effects for each city. Having established the relationship between police and firefights, Levitt (2002) can then use the estimates of the number of police, based on the number of firefighters, to explain crime rates: \\[\\Delta\\ln(\\mbox{Crime}_{ct}) = \\beta_1 \\ln(\\mbox{Police}_{ct-1}) + X&#39;_{ct}\\Gamma + \\Theta_c + \\mu_{ct}.\\] The typical way to present instrumental variable results is to show both stages. Figure 15.17 shows the relationship between police and firefighters. Figure 15.17: The relationship between firefighters, police and crime. Source: Levitt (2002) p. 1,247. And then Figure 15.18 shows the relationship between police and crime, where is it the IV results that are the ones of interest. Figure 15.18: The impact of police on crime. Source: Levitt (2002) p. 1,248. 15.10.4 Discussion The key finding of Levitt (2002) is that there is a negative effect of the number of police on the amount of crime. There are a variety of points that I want to raise in regard to this paper. They will come across as a little negative, but this is mostly just because this a paper from 2002, that I am reading today, and so the standards have changed. It’s fairly remarkable how reliant on various model specifications the results are. The results bounce around a fair bit and that’s just the ones that are reported. Chances are there are a bunch of other results that were not reported, but it would be of interest to see their impact. On that note, there is fairly limited model validation. This is probably something that I am more aware of these days, but it seems likely that there is a fair degree of over-fitting here. Levitt (2002) is actually a response, after another researcher, McCrary (2002), found some issues with the original paper: Levitt (1997). While Levitt appears quite decent about it, it is jarring to see that Levitt was thanked by McCrary (2002) for providing ‘both the data and computer code.’ What if Levitt had not been decent about providing the data and code? Or what if the code was unintelligible? In some ways it is nice to see how far that we have come - the author of a similar paper these days would be forced to make their code and data available as part of the paper, we wouldn’t have to ask them for it. But it reinforces the importance of open data and reproducible science. "],["multilevel-regression-with-post-stratification.html", "Chapter 16 Multilevel regression with post-stratification 16.1 Introduction 16.2 Simulation - Toddler bedtimes 16.3 Case study - Xbox paper 16.4 Simulation - Australian voting 16.5 Forecasting the 2020 US election 16.6 Concluding remarks and next steps", " Chapter 16 Multilevel regression with post-stratification Last updated: 28 April 2021. Required reading Alexander, Monica, 2019, ‘Analyzing name changes after marriage using a non-representative survey,’ 7 August, https://www.monicaalexander.com/posts/2019-08-07-mrp/. Gelman, Andrew, Jennifer Hill and Aki Vehtari, 2020, Regression and Other Stories, Cambridge University Press, Chapter 17. Hanretty, Chris, 2019, ‘An introduction to multilevel regression and post-stratification for estimating constituency opinion,’ Political Studies Review, https://doi.org/10.1177/1478929919864773. Kastellec, Jonathan, Jeffrey Lax, and Justin Phillips, 2016, ‘Estimating State Public Opinion With Multi-Level Regression and Poststratification using R,’ https://scholar.princeton.edu/sites/default/files/jkastellec/files/mrp_primer.pdf. Kennedy, Lauren, Katharine Khanna, Daniel Simpson, and Andrew Gelman, 2020, ‘Using sex and gender in survey adjustment,’ https://arxiv.org/abs/2009.14401. Kennedy, Lauren, and Jonah Gabry, 2019, ‘MRP with rstanarm,’ rstanarm vignettes, 8 October, https://mc-stan.org/rstanarm/articles/mrp.html. Kennedy, Lauren, and Andrew Gelman, 2019, ‘Know your population and know your model: Using model-based regression and poststratification to generalize findings beyond the observed sample,’ https://arxiv.org/abs/1906.11323. Wang, Wei, David Rothschild, Sharad Goel, and Andrew Gelman, 2015, ‘Forecasting elections with non-representative polls,’ International Journal of Forecasting, 31, no. 3, pages 980-991. Wu, Changbao and Mary E. Thompson, 2020, Sampling Theory and Practice, Springer, Chapter 17. Required viewing Gelman, Andrew, 2020, ‘Statistical Models of Election Outcomes,’ CPSR Summer Program in Quantitative Methods of Social Research, https://youtu.be/7gjDnrbLQ4k. Required listening Galef, Julia, 2020, ‘Episode 248: Are Democrats being irrational? (David Shor),’ Rationally Speaking, available at: http://rationallyspeakingpodcast.org/show/episode-248-are-democrats-being-irrational-david-shor.html. Recommended reading Arnold, Jeffrey B., 2018, ‘Simon Jackman’s Bayesian Model Examples in Stan,’ Ch 13, 7 May, https://jrnold.github.io/bugs-examples-in-stan/campaign.html. Cohn, Nate, 2016, ‘We Gave Four Good Pollsters the Same Raw Data. They Had Four Different Results,’ The New York Times, The Upshot, 20 September, https://www.nytimes.com/interactive/2016/09/20/upshot/the-error-the-polling-world-rarely-talks-about.html. Edelman, M., Vittert, L., &amp; Meng, X.-L., 2021, ‘An Interview with Murray Edelman on the History of the Exit Poll,’ Harvard Data Science Review, https://doi.org/10.1162/99608f92.3a25cd24 https://hdsr.mitpress.mit.edu/pub/fekmqbv4/release/2. Gelman, Andrew, and Julia Azari, 2017, ‘19 things we learned from the 2016 election,’ Statistics and Public Policy, 4 (1), pp. 1-10. Gelman, Andrew, Jessica Hullman, and Christopher Wlezien, 2020, ‘Information, incentives, and goals in election forecasts,’ 8 September, available at: http://www.stat.columbia.edu/~gelman/research/unpublished/forecast_incentives3.pdf Gelman, Andrew, Merlin Heidemanns, and Elliott Morris, 2020, ‘2020 US POTUS model,’ The Economist, freely available: https://github.com/TheEconomist/us-potus-model. Ghitza, Yair, and Andrew Gelman, 2013, ‘Deep Interactions with MRP: Election Turnout and Voting Patterns Among Small Electoral Subgroups,’ American Journal of Political Science, 57 (3), pp. 762-776. Ghitza, Yair, and Andrew Gelman, 2020, ‘Voter Registration Databases and MRP: Toward the Use of Large-Scale Databases in Public Opinion Research,’ Political Analysis, pp. 1-25. Imai, Kosuke, 2017, Quantitative Social Science: An Introduction, Princeton University Press, Ch 4.1, and 5.3. Jackman, Simon, 2005, ‘Pooling the polls over an election campaign,’ Australian Journal of Political Science, 40 (4), pp. 499-517. Jackman, Simon, Shaun Ratcliff and Luke Mansillo, 2019, ‘Small area estimates of public opinion: Model-assisted post-stratification of data from voter advice applications,’ 4 January, https://www.cambridge.org/core/membership/services/aop-file-manager/file/5c2f6ebb7cf9ee1118d11c0a/APMM-2019-Simon-Jackman.pdf. Lauderdale, Ben, Delia Bailey, Jack Blumenau, and Doug Rivers, 2020, ‘Model-based pre-election polling for national and sub-national outcomes in the US and UK,’ International Journal of Forecasting, 36 (2), pp. 399-413. Leigh, Andrew, and Justin Wolfers, 2006, ‘Competing approaches to forecasting elections: Economic models, opinion polling and prediction markets,’ Economic Record, 82 (258), pp.325-340. Nickerson, David W., and Todd Rogers, 2014, ‘Political campaigns and big data,’ Journal of Economic Perspectives, 28 (2), pp. 51-74. Shirani-Mehr, Houshmand, David Rothschild, Sharad Goel, and Andrew Gelman, 2018, ‘Disentangling bias and variance in election polls,’ Journal of the American Statistical Association, 113 (522), pp. 607-614. Recommended viewing Jackman, Simon, 2020, ‘The triumph of the quants?: Model-based poll aggregation for election forecasting,’ Ihaka Lecture Series, https://youtu.be/MvGYsKIsLFs. Key libraries brms broom gtsummary haven labelled lme4 modelsummary rstanarm tidybayes tidyverse Quiz Your Mum asked you what you’ve been learning this term. You decide to tell her about multilevel regression with post-stratification (MRP). Please explain what MRP is. Your Mum has a university-education, but has not necessarily taken any statistics, so you will need to explain any technical terms that you use. [Please write two or three paragraphs; strong answers would be clear about both strengths and weaknesses.] With respect to Wang et al. (2015): Why is this paper interesting? What do you like about this paper? What do you wish it did better? To what extent can you reproduce this paper? [Please write one or two paragraphs about each aspect.] With respect to Wang et al. (2015), what is not a feature they mention election forecasts need? Explainable. Accurate. Cost-effective. Relevant. Timely. With respect to Wang et al. (2015), what is a weakness of MRP? Detailed data requirement. Allows use of biased data. Expensive to conduct. With respect to Wang et al. (2015), what is concerning about the Xbox sample? Non-representative. Small sample size. Multiple responses from the same respondent. I am interested in studying how voting intentions in the 2020 US presidential election vary by an individual’s income. I set up a logistic regression model to study this relationship. In my study, some possible independent variables would be: [Please check all that apply.] Whether the respondent is registered to vote (yes/no). Whether the respondent is going to vote for Biden (yes/no). The race of the respondent (white/not white). The respondent’s marital status (married/not). Please think about N. Cohn (2016) Why is this type of exercise not carried out more? Why do you think that different groups, even with the same background and level of quantitative sophistication, could have such different estimates even when they use the same data? [Please write a paragraph or two about each aspect.] When we think about multilevel regression with post-stratification, what are the key assumptions that we are making? [Please write one or two paragraphs about each aspect.] I train a model based on a survey, and then post-stratify it using the 2020 ACS dataset. What are some of the practical considerations that I may have to contend when I am doing this? [Please write a paragraph each about at least three considerations.] In a similar manner to Ghitza and Gelman (2020) pretend you’ve got access to a US voter file record from a private company. You train a model on the 2020 US CCES, and post-stratify it, on an individual-basis, based on that voter file. Could you please put-together a datasheet for the voter file dataset following Gebru et al. (2020)? As a reminder, datasheets accompany datasets and document ‘motivation, composition, collection process, recommended uses,’ among other aspects. Could you also please put together a model card for your model, following Mitchell et al. (2019)? As a reminder, model cards are deliberately straight-forward one- or two-page documents that report aspects such as: model details; intended use; metrics; training data; ethical considerations; as well as caveats and recommendations (Mitchell et al. 2019). Could you please discuss three ethical aspects around the features that you are using in your model? [Please write a paragraph or two for each point.] Could you please detail the protections that you would put in place in terms of the dataset, the model, and the predictions? If I have a model output from lm() called ‘my_model_output’ how can I use modelsummary to display the output (assume the package has been loaded) [please select all that apply]? modelsummary::modelsummary(my_model_output) modelsummary(my_model_output) my_model_output %&gt;% modelsummary() my_model_output %&gt;% modelsummary(statistic = NULL) Which of the following are examples of linear models [please select all that apply]? lm(y ~ x_1 + x_2 + x_3, data = my_data) lm(y ~ x_1 + x_2^2 + x_3, data = my_data) lm(y ~ x_1 * x_2 + x_3, data = my_data) lm(y ~ x_1 + x_1^2 + x_2 + x_3, data = my_data) Consider a situation in which you have a survey dataset with these age-groups: 18-29; 30-44; 45- 60; and 60+. And a post-stratification dataset with these age-groups: 18-24; 25-29; 30-34; 35-39; 40-44; 45-49; 50-54; 55-59; and 60+. What approach would you take to bringing these together? [Please write a paragraph.] Consider a situation in which you again have a survey dataset with these age-groups: 18-29; 30-44; 45- 60; and 60+. But this time the post-stratification dataset has these age-groups: 18-34; 35-49; 50-64; and 65+. What approach would you take to bringing these together? [Please write a paragraph.] Please consider Kennedy et al. (2020). What are some statistical facets when considering a survey focused on gender, with a post-stratification survey that is not? [Please check all that apply.] Impute all non-male as female Estimate gender using auxiliary information Impute population Impute sample values Model population distribution using auxiliary data Remove all non-binary respondents Remove respondents Assume population distribution Please consider Kennedy et al. (2020). What are some ethical facets when considering a survey focused on gender, with a post-stratification survey that is not? [Please check all that apply.] Impute all non-male as female Estimate gender using auxiliary information Impute population Impute sample values Model population distribution using auxiliary data Remove all non-binary respondents Remove respondents Assume population distribution Please consider Kennedy et al. (2020). How do they define ethics? Respecting the perspectives and dignity of individual survey respondents. Generating estimates of the general population and for subpopulations of interest. Using more complicated procedures only when they serve some useful function. 16.1 Introduction [The Presidential election of] 2016 was the largest analytics failure in US political history. David Shor, 13 August 2020 Multilevel regression with post-stratification (MRP) is a popular way to adjust non-representative samples to better analyse opinion and other survey responses.10 It uses a regression model to relate individual-level survey responses to various characteristics and then rebuilds the sample to better match the population. In this way MRP can not only allow a better understanding of responses, but also allow us to analyse data that may otherwise be unusable. However, it can be a challenge to get started with MRP as the terminology may be unfamiliar, and the data requirements can be onerous. Let’s say that we have a biased survey. Maybe we conducted a survey about computers at an academic seminar, so folks with post-graduate degrees are likely over-represented. We are nonetheless interested in making claims about the population. Let’s say that we found 37.5 per cent of our respondents prefer Macs. One way forward is to just ignore the bias and say that ‘37.5 per cent of people prefer Macs.’ Another way is to say, well 50 per cent of our respondents with a post-graduate degree prefer Macs, and of those without a post-graduate degree, 25 per cent prefer Macs. If we knew what proportion of the broader population has post-graduate degree, let’s assume 10 per cent, then we could conduct re-weighting, or post-stratification, as follows: \\(0.5 * 0.1 + 0.25 * 0.9 = 0.275\\), and so our estimate is that 27.5 per cent of people prefer Macs. MRP is a third approach, and uses a model to help do that re-weighting. So we use logistic regression to estimate the relationship between preferring Macs and highest educational attainment in our survey. We then apply that relationship to population dataset. MRP is a handy approach when dealing with survey data. Hanretty (2020) puts it well when he says ‘MRP is used because the alternatives are either very poor or very expensive.’ Essentially, it trains a model based on the survey, and then applies that trained model to another dataset. There are two main, related, advantages: It can allow us to ‘re-weight’ in a way that includes uncertainty front-of-mind and isn’t as hamstrung by small samples. The alternative way to deal with having a small sample is to either go and gather more data or throw it away. It can allow us to use broad surveys to speak to subsets. As Hanretty (2020) says ‘A poor alternative [to using MRP] is simply splitting a large sample into (much) smaller geographic subsamples. This is a poor alternative because there is no guarantee that a sample which is representative at the national level will be representative when it is broken down into smaller groups.’ From a practical perspective, it tends to be less expensive to collect non-probability samples and so there are benefits of being able to use these types of data. That said, it is not a magic-bullet and the laws of statistics still apply. We will have larger uncertainty around our estimates and they will still be subject to all the usual biases. As Lauren Kennedy points out, ‘MRP has traditionally been used in probability surveys and had potential for non-probability surveys, but we’re not sure of the limitations at the moment.’ It’s an exciting area of research in both academia and industry. The workflow that you need for MRP is straight-forward, but the details and tiny decisions that have to be made at each step can become overwhelming. The point that you need to keep in mind is that you are trying to create a relationship between two datasets using a statistical model, and so you need to establish similarity between the two datasets in terms of their variables and levels. The steps are: gather and prepare the survey dataset, thinking about what is needed for coherence with the post-stratification dataset; gather and prepare the post-stratification dataset thinking about what is needed for coherence with the survey dataset; model the variable of interest from the survey using independent variables and levels that are available in both the survey and the post-stratification datasets; apply the model to the post-stratification data. In these notes, we begin with simulating a situation in which we pretend that we know the features of the population. We then move to a famous example of MRP that used survey data from the Xbox platform and exit poll data to forecast the 2012 US election. We will then move to examples from the Australian political situation. We will then discuss some features to be aware of when conducting MRP. 16.2 Simulation - Toddler bedtimes 16.2.1 Construct a population To get started we will simulate some data from a population that has various properties, take a biased sample, and then conduct MRP to demonstrate how we can get those properties back. We are going to have two ‘explanatory variables’ - age-group and toilet-trained - and one dependent variable - bedtime. Bed-time will increase as age-group increases, and will be later for children that are toilet-trained, compared with those that are not. To be clear, in this example we will ‘know’ the ‘true’ features of the population, but this isn’t something that occurs when we use real data - it is just to help you understand what MRP is doing. We’re going to rely heavily on the tidyverse package (Wickham et al. 2019b). # Uncomment this (by deleting the #) if you need to install the packages # install.packages(&#39;tidyverse&#39;) library(tidyverse) # This helps reproducibility # It makes it more likely that you&#39;re able to get the same random numbers as in this example. set.seed(853) # One million people in our population. size_of_population &lt;- 1000000 population_for_mrp_example &lt;- tibble(age_group = sample(x = c(1:3), # Draw from any of 1, 2, 3. size = size_of_population, replace = TRUE # After you draw a number, allow that number to be drawn again. ), toilet_trained = sample(x = c(0, 1), size = size_of_population, replace = TRUE ), noise = rnorm(size_of_population, mean = 0, sd = 1), bed_time = 5 + 0.5 * age_group + 1 * toilet_trained + noise, # Make bedtime a linear function of the variables that we just generated and a intercept (no special reason for that intercept to be five; it could be anything). ) %&gt;% select(-noise) %&gt;% mutate(age_group = as_factor(age_group), toilet_trained = as_factor(toilet_trained) ) population_for_mrp_example %&gt;% head() Table 16.1: age_grouptoilet_trainedbed_time 105.74 216.48 106.53 115.39 118.4&nbsp; 306.54 At this point, Figure 16.1 provides invaluable advice (thank you to A Mahfouz). Figure 16.1: What does Bernie ask us to do? That is, as always, when we have a dataset, we first try to plot it to better understand what is going on (as there are a million points, I’ll just grab the first 1,000 so that it plots nicely). population_for_mrp_example %&gt;% slice(1:1000) %&gt;% ggplot(aes(x = age_group, y = bed_time)) + geom_jitter(aes(color = toilet_trained), alpha = 0.4, width = 0.1, height = 0) + labs(x = &quot;Age-group&quot;, y = &quot;Bed time&quot;, color = &quot;Toilet trained&quot;) + theme_classic() + scale_color_brewer(palette = &quot;Set1&quot;) And we can also work out what the ‘truth’ is for the information that we are interested in (remembering that we’d never actually know this when we move away from simulated examples). population_for_mrp_example_summarised &lt;- population_for_mrp_example %&gt;% group_by(age_group, toilet_trained) %&gt;% summarise(median_bed_time = median(bed_time)) population_for_mrp_example_summarised %&gt;% knitr::kable(digits = 2, col.names = c(&quot;Age-group&quot;, &quot;Is toilet trained&quot;, &quot;Average bed time&quot;)) Age-group Is toilet trained Average bed time 1 0 5.50 1 1 6.50 2 0 6.00 2 1 7.00 3 0 6.50 3 1 7.51 16.2.2 Get a biased sample from it Now we want to pretend that we have some survey that has a biased sample. We’ll allow that it over-samples children that are younger and those that are not toilet-trained. For instance, perhaps we gathered our sample based on the records of a paediatrician, so it’s more likely that they will see this biased sample of children. We are interested in knowing what proportion of children are toilet-trained at various age-groups. # Thanks to Monica Alexander set.seed(853) # Add a weight for each &#39;type&#39; (has to sum to one) population_for_mrp_example &lt;- population_for_mrp_example %&gt;% mutate(weight = case_when(toilet_trained == 0 &amp; age_group == 1 ~ 0.7, toilet_trained == 0 ~ 0.1, age_group %in% c(1, 2, 3) ~ 0.2 ), id = 1:n() ) get_these &lt;- sample( x = population_for_mrp_example$id, size = 1000, prob = population_for_mrp_example$weight ) sample_for_mrp_example &lt;- population_for_mrp_example %&gt;% filter(id %in% get_these) %&gt;% select(-weight, -id) # Clean up poststratification_dataset &lt;- population_for_mrp_example %&gt;% select(-weight, -id) And we can plot those also. sample_for_mrp_example %&gt;% mutate(toilet_trained = as_factor(toilet_trained)) %&gt;% ggplot(aes(x = age_group, y = bed_time)) + geom_jitter(aes(color = toilet_trained), alpha = 0.4, width = 0.1, height = 0) + labs(x = &quot;Age-group&quot;, y = &quot;Bed time&quot;, color = &quot;Toilet trained&quot;) + theme_classic() + scale_color_brewer(palette = &quot;Set1&quot;) It’s pretty clear that our sample has a different bedtime than the overall population, but let’s just do the same exercise as before to look at the median, by age and toilet-trained status. sample_for_mrp_example_summarized &lt;- sample_for_mrp_example %&gt;% group_by(age_group, toilet_trained) %&gt;% summarise(median_bed_time = median(bed_time)) sample_for_mrp_example_summarized %&gt;% knitr::kable(digits = 2, col.names = c(&quot;Age-group&quot;, &quot;Is toilet trained&quot;, &quot;Average bed time&quot;)) Age-group Is toilet trained Average bed time 1 0 5.41 1 1 6.35 2 0 5.89 2 1 6.85 3 0 6.49 3 1 7.62 16.2.3 Model the sample We will quickly train a model based on the (biased) survey. We’ll use modelsummary (Arel-Bundock 2020) to format our estimates. library(modelsummary) mrp_example_model &lt;- sample_for_mrp_example %&gt;% lm(bed_time ~ age_group + toilet_trained, data = .) mrp_example_model %&gt;% modelsummary::modelsummary(fmt = 2) Model 1 (Intercept) 5.47 (0.04) age_group2 0.54 (0.09) age_group3 1.15 (0.09) toilet_trained1 0.91 (0.07) Num.Obs. 1000 R2 0.373 R2 Adj. 0.371 AIC 2839.3 BIC 2863.8 Log.Lik. -1414.630 F 197.594 This is the ‘multilevel regression’ part of the MRP (although this isn’t really a multilevel model just to keep things simple for now). 16.2.4 Get a post-stratification dataset Now we will use a post-stratification dataset to get some estimates of the number in each cell. We typically use a larger dataset that may more closely reflection the population. In the US a popular choice is the ACS, while in other countries we typically have to use the census. In this simulation example, I’ll just take a 10 per cent sample from the population and use that as our post-stratification dataset. set.seed(853) poststratification_dataset &lt;- population_for_mrp_example %&gt;% slice(1:100000) %&gt;% select(-bed_time) poststratification_dataset %&gt;% head() Table 16.2: age_grouptoilet_trainedweightid 100.71 210.22 100.73 110.24 110.25 300.16 In an ideal world we have individual-level data in our post-stratification dataset (that’s the case above). In that world we can apply our model to each individual. The more likely situation, in reality, is that we just have counts by groups, so we’re going to try to construct an estimate for each group. poststratification_dataset_grouped &lt;- poststratification_dataset %&gt;% group_by(age_group, toilet_trained) %&gt;% count() poststratification_dataset_grouped %&gt;% head() ## # A tibble: 6 x 3 ## # Groups: age_group, toilet_trained [6] ## age_group toilet_trained n ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 1 0 16766 ## 2 1 1 16649 ## 3 2 0 16801 ## 4 2 1 16617 ## 5 3 0 16625 ## 6 3 1 16542 16.2.5 Post-stratify our model estimates Now we create an estimate for each group, and add some confidence intervals. poststratification_dataset_grouped &lt;- mrp_example_model %&gt;% predict(newdata = poststratification_dataset_grouped, interval = &quot;confidence&quot;) %&gt;% as_tibble() %&gt;% cbind(poststratification_dataset_grouped, .) # The dot modifies the behaviour of the pipe; it pipes to the dot instead of the first argument as normal. At this point we can have a look at our MRP estimates (circles) along with their confidence intervals, and compare them the raw estimates from the data (squares). In this case, because we know the truth, we can also compare them to the known truth (triangles) (but that’s not something we can do normally). poststratification_dataset_grouped %&gt;% ggplot(aes(x = age_group, y = fit)) + geom_point(data = population_for_mrp_example_summarised, aes(x = age_group, y = median_bed_time, color = toilet_trained), shape = 17) + geom_point(data = sample_for_mrp_example_summarized, aes(x = age_group, y = median_bed_time, color = toilet_trained), shape = 15) + geom_pointrange(aes(ymin=lwr, ymax=upr, color = toilet_trained)) + labs(x = &quot;Age-group&quot;, y = &quot;Bed time&quot;, color = &quot;Toilet trained&quot;) + theme_classic() + scale_color_brewer(palette = &quot;Set1&quot;) 16.3 Case study - Xbox paper 16.3.1 Overview One famous MRP example is Wang et al. (2015). They used data from the Xbox gaming platform to forecast the 2012 US Presidential Election. Key facts about the set-up: Data are from an opt-in poll which was available on the Xbox gaming platform during the 45 days leading up to the 2012 US presidential election (Obama and Romney). Each day there were three to five questions, including voter intention: ‘If the election were held today, who would you vote for?’ Respondents were allowed to answer at most once per day. First-time respondents were asked to provide information about themselves, including their sex, race, age, education, state, party ID, political ideology, and who they voted for in the 2008 presidential election. In total, 750,148 interviews were conducted, with 345,858 unique respondents - over 30,000 of whom completed five or more polls. Young men dominate the Xbox population: 18-to-29-year-olds comprise 65 per cent of the Xbox dataset, compared to 19 per cent in the exit poll; and men make up 93 per cent of the Xbox sample but only 47 per cent of the electorate. 16.3.2 Model Given the structure of the US electorate, they use a two-stage modelling approach. The details don’t really matter too much, but essentially they model how likely a respondent is to vote for Obama, given various information such as state, education, sex, etc: \\[ \\begin{align} Pr\\left(Y_i = \\mbox{Obama} | Y_i\\in\\{\\mbox{Obama, Romney}\\}\\right) = \\\\ \\mbox{logit}^{-1}(\\alpha_0 + \\alpha_1(\\mbox{state last vote share}) + \\alpha_{j[i]}^{\\mbox{state}} + \\\\ \\alpha_{j[i]}^{\\mbox{edu}} + \\alpha_{j[i]}^{\\mbox{sex}} + ...) \\\\ \\end{align} \\] They run this in R using glmer() from ‘lme4’ (Bates et al. 2015). 16.3.3 Post-stratify Having a trained model that considers the effect of these various independent variables on support for the candidates, they now post-stratify, where each of these ‘cell-level estimates are weighted by the proportion of the electorate in each cell and aggregated to the appropriate level (i.e., state or national).’ This means that they need cross-tabulated population data. In general, the census would have worked, or one of the other large surveys available in the US, but the difficulty is that the variables need to be available on a cross-tab basis. As such, they use exit polls (not a viable option for most other countries). They make state-specific estimates by post-stratifying to the features of each state (Figure 16.2). Figure 16.2: Post-stratified estimates for each state based on the Xbox survey and MRP Similarly, they can examine demographic-differences (Figure 16.3). Figure 16.3: Post-stratified estimates on a demographic basis based on the Xbox survey and MRP Finally, they convert their estimates into electoral college estimates (Figure 16.4). Figure 16.4: Post-stratified estimates of electoral college outcomes based on the Xbox survey and MRP 16.4 Simulation - Australian voting 16.4.1 Overview As a reminder, the workflow that we use is: read in the poll; model the poll; read in the post-stratification data; and apply the model to the post-stratification data. In the earlier example, we didn’t really do too much in the modelling step, and despite the name ‘multilevel modelling with post-stratification,’ we didn’t actually use a multilevel model. There’s nothing that says you have to use a multilevel model, but a lot of situations will have circumstances such that it’s not likely to do any worse. To be clear, this means that although we have individual-level data, there is some grouping of the individuals that we’ll take advantage of. For instance, in the case of trying to model elections, usually districts/divisions/electorates/ridings/etc exist within provinces/states so it would likely make sense to, at least, include a coefficient that adjusts the intercept for each province. In this section we’re simulate another dataset and then fit a few different models to it. We’re going to draw on the Australian elections set-up. In Australia we have a parliamentary system, with 151 seats in the parliament, one for each electorate. These electorates are grouped within six states and two territories. There are two major parties - the Australian Labor Party (ALP) and the Liberal Party (LP). Somewhat confusingly, the Liberal party are actually the conservative, right-wing party, while the Labor party are the progressive, left-wing, party. 16.4.2 Construct a survey To move us slightly closer to reality, we are going to simulate a survey (rather than sample from a population as we did earlier) and then post-stratify it using real data. The dependent variable is ‘supports_ALP,’ which is a binary variable - either 0 or 1. We’ll just start with three independent variables here: ‘gender,’ which is either ‘female’ or ‘male’ (as that is what is available from the Australian Bureau of Statistics); ‘age_group,’ which is one of four groups: ‘ages 18 to 29,’ ‘ages 30 to 44,’ ‘ages 45 to 59,’ ‘ages 60 plus’; ‘state,’ which is one of eight integers: 1 - 8 (inclusive). At this point, it’s worth briefly discussing the role of sex and gender in survey research, following Kennedy et al. (2020). Sex is based on biological attributes, while gender is socially constructed. We are likely interested in the effect of gender on our dependent variable. Moving away from a non-binary concept of gender, in terms of official statistics, is only something that has happened recently. As a researcher one of the problems of insisting on a binary is that, as Kennedy et al. (2020, 2) say ‘…when measuring gender with simply two categories, there is a failure to capture the unique experiences of those who do not identify as either male or female, or for those whose gender does not align with their sex classification.’ A researcher has a variety of ways of proceeding, and Kennedy et al. (2020) discuss these based on: ethics, accuracy, practicality, and flexibility. However, ‘there is no single good solution that can be applied to all situations. Instead, it is important to recognize that there is a compromise between ethical concerns, statistical concerns, and the most appropriate decision will be reflective of this’ [p. 16]. The most important consideration is to ensure appropriate ‘respect and consideration for the survey respondent.’ library(tidyverse) set.seed(853) size_of_sample_for_australian_polling &lt;- 2000 sample_for_australian_polling &lt;- tibble(age_group = sample(x = c(0:3), size = size_of_sample_for_australian_polling, replace = TRUE ), gender = sample(x = c(0:1), size = size_of_sample_for_australian_polling, replace = TRUE ), state = sample(x = c(1:8), size = size_of_sample_for_australian_polling, replace = TRUE ), noise = rnorm(size_of_sample_for_australian_polling, mean = 0, sd = 1), support_alp = 1 + 0.5 * age_group + 0.5 * gender + 0.01 * state + noise ) # Normalize the outcome variable sample_for_australian_polling &lt;- sample_for_australian_polling %&gt;% mutate(support_alp = if_else(support_alp &gt; median(support_alp, na.rm = TRUE), &#39;Supports ALP&#39;, &#39;Does not&#39;) ) # Clean up the simulated data sample_for_australian_polling &lt;- sample_for_australian_polling %&gt;% mutate( age_group = case_when( age_group == 0 ~ &#39;Ages 18 to 29&#39;, age_group == 1 ~ &#39;Ages 30 to 44&#39;, age_group == 2 ~ &#39;Ages 45 to 59&#39;, age_group == 3 ~ &#39;Ages 60 plus&#39;, TRUE ~ &#39;Problem&#39; ), gender = case_when( gender == 0 ~ &#39;Male&#39;, gender == 1 ~ &#39;Female&#39;, TRUE ~ &#39;Problem&#39; ), state = case_when( state == 1 ~ &#39;Queensland&#39;, state == 2 ~ &#39;New South Wales&#39;, state == 3 ~ &#39;Australian Capital Territory&#39;, state == 4 ~ &#39;Victoria&#39;, state == 5 ~ &#39;Tasmania&#39;, state == 6 ~ &#39;Northern Territory&#39;, state == 7 ~ &#39;South Australia&#39;, state == 8 ~ &#39;Western Australia&#39;, TRUE ~ &#39;Problem&#39; ), ) %&gt;% select(-noise) # Tidy the class sample_for_australian_polling &lt;- sample_for_australian_polling %&gt;% mutate(across(c(age_group, gender, state, support_alp), as_factor)) sample_for_australian_polling %&gt;% head() Table 16.3: age_groupgenderstatesupport_alp Ages 18 to 29FemaleSouth AustraliaSupports ALP Ages 60 plusMaleSouth AustraliaSupports ALP Ages 30 to 44MaleVictoriaDoes not Ages 18 to 29MaleTasmaniaDoes not Ages 18 to 29FemaleVictoriaDoes not Ages 18 to 29MaleQueenslandSupports ALP Finally, we want our survey to over-sample females, so we’ll just get rid of 300 males. sample_for_australian_polling &lt;- sample_for_australian_polling %&gt;% arrange(gender) %&gt;% slice(1:1700) 16.4.3 Model the survey This polling data was generated to make both males and older people less likely to vote for the ALP; and females and younger people more likely to vote for the Labor Party. Females are over-sampled. As such, we should have an ALP skew on the dataset. We’re going to use the gtsummary package to quickly make a summary table (Sjoberg et al. 2021). library(gtsummary) sample_for_australian_polling %&gt;% gtsummary::tbl_summary() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #oaligqhunl .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #oaligqhunl .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #oaligqhunl .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #oaligqhunl .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #oaligqhunl .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #oaligqhunl .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #oaligqhunl .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #oaligqhunl .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #oaligqhunl .gt_column_spanner_outer:first-child { padding-left: 0; } #oaligqhunl .gt_column_spanner_outer:last-child { padding-right: 0; } #oaligqhunl .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #oaligqhunl .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #oaligqhunl .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #oaligqhunl .gt_from_md > :first-child { margin-top: 0; } #oaligqhunl .gt_from_md > :last-child { margin-bottom: 0; } #oaligqhunl .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #oaligqhunl .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #oaligqhunl .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #oaligqhunl .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #oaligqhunl .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #oaligqhunl .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #oaligqhunl .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #oaligqhunl .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #oaligqhunl .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #oaligqhunl .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #oaligqhunl .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #oaligqhunl .gt_sourcenote { font-size: 90%; padding: 4px; } #oaligqhunl .gt_left { text-align: left; } #oaligqhunl .gt_center { text-align: center; } #oaligqhunl .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #oaligqhunl .gt_font_normal { font-weight: normal; } #oaligqhunl .gt_font_bold { font-weight: bold; } #oaligqhunl .gt_font_italic { font-style: italic; } #oaligqhunl .gt_super { font-size: 65%; } #oaligqhunl .gt_footnote_marks { font-style: italic; font-size: 65%; } Characteristic N = 1,7001 age_group Ages 18 to 29 458 (27%) Ages 60 plus 421 (25%) Ages 30 to 44 401 (24%) Ages 45 to 59 420 (25%) gender Female 1,023 (60%) Male 677 (40%) state South Australia 233 (14%) Victoria 189 (11%) Tasmania 229 (13%) Queensland 214 (13%) Western Australia 198 (12%) New South Wales 219 (13%) Australian Capital Territory 237 (14%) Northern Territory 181 (11%) support_alp Supports ALP 896 (53%) Does not 804 (47%) 1 n (%) Now we’d like to see if we can get our results back (we should find females less likely than males to vote for Australian Labor Party and that people are less likely to vote Australian Labor Party as they get older). Our model is: \\[ \\mbox{ALP support}_j = \\mbox{gender}_j + \\mbox{age_group}_j + \\epsilon_j \\] This model says that the probability that some person, \\(j\\), will vote for the Australian Labor Party depends on their gender and their age-group. Based on our simulated data, we would like older age-groups to be less likely to vote for the Australian Labor Party and for males to be less likely to vote for the Australian Labor Party. alp_support &lt;- glm(support_alp ~ gender + age_group + state, data = sample_for_australian_polling, family = &quot;binomial&quot; ) alp_support %&gt;% modelsummary::modelsummary(fmt = 2, exponentiate = TRUE) Model 1 (Intercept) 1.44 (0.18) genderMale 3.22 (0.12) age_groupAges 60 plus 0.07 (0.17) age_groupAges 30 to 44 0.42 (0.15) age_groupAges 45 to 59 0.17 (0.15) stateVictoria 1.65 (0.22) stateTasmania 1.24 (0.21) stateQueensland 1.46 (0.22) stateWestern Australia 1.18 (0.22) stateNew South Wales 1.42 (0.21) stateAustralian Capital Territory 1.73 (0.21) stateNorthern Territory 1.49 (0.23) Num.Obs. 1700 AIC 1959.7 BIC 2024.9 Log.Lik. -967.836 Essentially we’ve got our inputs back. Our dependent variable is a binary, and so we used logistic regression so the results are a little more difficult to interpret. 16.4.4 Post-stratify Now we’d like to see if we can use what we found in the poll to get an estimate for each state based on their demographic features. First read in some real demographic data, on a state basis, from the ABS. post_strat_census_data &lt;- read_csv(&quot;outputs/data/census_data.csv&quot;) head(post_strat_census_data) Table 16.4: stategenderage_groupnumbercell_prop_of_division_total ACTFemaleages18to293.47e+040.125 ACTFemaleages30to444.3e+04&nbsp;0.155 ACTFemaleages45to593.38e+040.122 ACTFemaleages60plus3.03e+040.109 ACTMaleages18to293.42e+040.123 ACTMaleages30to444.13e+040.149 At this point, we’ve got a decision to make because we need the variables to be the same in the survey and the post-stratification dataset, but here the state abbreviations have been used, while in the survey, the full names were used. We’ll change the post-stratification dataset because the survey data has already modelled. post_strat_census_data &lt;- post_strat_census_data %&gt;% mutate( state = case_when( state == &#39;ACT&#39; ~ &#39;Australian Capital Territory&#39;, state == &#39;NSW&#39; ~ &#39;New South Wales&#39;, state == &#39;NT&#39; ~ &#39;Northern Territory&#39;, state == &#39;QLD&#39; ~ &#39;Queensland&#39;, state == &#39;SA&#39; ~ &#39;South Australia&#39;, state == &#39;TAS&#39; ~ &#39;Tasmania&#39;, state == &#39;VIC&#39; ~ &#39;Victoria&#39;, state == &#39;WA&#39; ~ &#39;Western Australia&#39;, TRUE ~ &quot;Problem&quot; ), age_group = case_when( age_group == &#39;ages18to29&#39; ~ &#39;Ages 18 to 29&#39;, age_group == &#39;ages30to44&#39; ~ &#39;Ages 30 to 44&#39;, age_group == &#39;ages45to59&#39; ~ &#39;Ages 45 to 59&#39;, age_group == &#39;ages60plus&#39; ~ &#39;Ages 60 plus&#39;, TRUE ~ &quot;Problem&quot; ) ) We’re just going to do some rough forecasts. For each gender and age-group we want the relevant coefficient in the example data and we can construct the estimates. post_strat_census_data &lt;- alp_support %&gt;% predict(newdata = post_strat_census_data, type = &#39;response&#39;, se.fit = TRUE) %&gt;% as_tibble() %&gt;% cbind(post_strat_census_data, .) post_strat_census_data %&gt;% mutate(alp_predict_prop = fit*cell_prop_of_division_total) %&gt;% group_by(state) %&gt;% summarise(alp_predict = sum(alp_predict_prop)) Table 16.5: statealp_predict Australian Capital Territory0.551 New South Wales0.487 Northern Territory0.546 Queensland0.491 South Australia0.403 Tasmania0.429 Victoria0.521 Western Australia0.46&nbsp; We now have post-stratified estimates for each state Our model has a fair few weaknesses. For instance, small cell counts are going to be problematic. And our approach ignores uncertainty, but now that we have something working we can complicate it. 16.4.5 Improving the model We’d like to address some of the major issues with our approach, specifically being able to deal with small cell counts, and also taking better account of uncertainty. As we are dealing with survey data, prediction intervals or something similar are critical, and it’s not appropriate to only report central estimates. To do this we’ll use the same broad approach as before, but just improve the model. We’re going to change to a Bayesian model and use the rstanarm package (Goodrich et al. 2020). Now, using the same basic model as before, but in a Bayesian setting. library(rstanarm) improved_alp_support &lt;- rstanarm::stan_glm(support_alp ~ gender + age_group + state, data = sample_for_australian_polling, family = binomial(link = &quot;logit&quot;), prior = normal(0, 1), prior_intercept = normal(0, 1), cores = 2, seed = 12345) As before, we’d like an estimate for each state based on their demographic features. We’re just going to do some rough forecasts. For each gender and age-group we want the relevant coefficient in the example data and we can construct the estimates (this code is from Monica Alexander). We’re going to use tidybayes for this (Kay 2020). library(tidybayes) post_stratified_estimates &lt;- improved_alp_support %&gt;% tidybayes::add_fitted_draws(newdata = post_strat_census_data) %&gt;% rename(alp_predict = .value) %&gt;% mutate(alp_predict_prop = alp_predict*cell_prop_of_division_total) %&gt;% group_by(state, .draw) %&gt;% summarise(alp_predict = sum(alp_predict_prop)) %&gt;% group_by(state) %&gt;% summarise(mean = mean(alp_predict), lower = quantile(alp_predict, 0.025), upper = quantile(alp_predict, 0.975)) post_stratified_estimates Table 16.6: statemeanlowerupper Australian Capital Territory0.55&nbsp;0.4940.604 New South Wales0.4860.4290.544 Northern Territory0.5440.4830.607 Queensland0.4910.4320.548 South Australia0.4120.3610.464 Tasmania0.4290.3720.487 Victoria0.5190.4530.583 Western Australia0.46&nbsp;0.4010.52&nbsp; We now have post-stratified estimates for each division. Our new Bayesian approach will enable us to think more deeply about uncertainty. We could complicate this in a variety of ways including adding more coefficients (but remember that we’d need to get new cell counts), or adding some layers. One interesting aspect is that our multilevel approach will allow us to deal with small cell counts by borrowing information from other cells. Even if we were to remove most of the, say, 18-to-29-year-old, male respondents from Tasmania our model would still provide estimates. It does this by pooling, in which the effect of these young, male, Tasmanians is partially determined by other cells that do have respondents. There are many interesting aspects that we may like to communicate to others. For instance, we may like to show how the model is affecting the results. We can make a graph that compares the raw estimate with the model estimate. post_stratified_estimates %&gt;% ggplot(aes(y = mean, x = forcats::fct_inorder(state), color = &quot;MRP estimate&quot;)) + geom_point() + geom_errorbar(aes(ymin = lower, ymax = upper), width = 0) + labs(y = &quot;Proportion ALP support&quot;, x = &quot;State&quot;) + theme_minimal() + scale_color_brewer(palette = &quot;Set1&quot;) + theme(legend.position = &quot;bottom&quot;) + theme(legend.title = element_blank()) + coord_flip() Similarly, we may like to plot the distribution of the coefficients.11 # tidybayes::get_variables(improved_alp_support) # improved_alp_support %&gt;% # gather_draws(genderMale) %&gt;% # ungroup() %&gt;% # # mutate(coefficient = stringr::str_replace_all(.variable, c(&quot;b_&quot; = &quot;&quot;))) %&gt;% # mutate(coefficient = forcats::fct_recode(coefficient, # Intercept = &quot;Intercept&quot;, # `Is male` = &quot;genderMale&quot;, # `Age 30-44` = &quot;age_groupages30to44&quot;, # `Age 45-59` = &quot;age_groupages45to59&quot;, # `Age 60+` = &quot;age_groupages60plus&quot; # )) %&gt;% # # # both %&gt;% # ggplot(aes(y=fct_rev(coefficient), x = .value)) + # ggridges::geom_density_ridges2(aes(height = ..density..), # rel_min_height = 0.01, # stat = &quot;density&quot;, # scale=1.5) + # xlab(&quot;Distribution of estimate&quot;) + # ylab(&quot;Coefficient&quot;) + # scale_fill_brewer(name = &quot;Dataset: &quot;, palette = &quot;Set1&quot;) + # theme_minimal() + # theme(panel.grid.major = element_blank(), # panel.grid.minor = element_blank()) + # theme(legend.position = &quot;bottom&quot;) 16.5 Forecasting the 2020 US election The US election has a lot of features that are unique to the US, but the model that we are going to build here is going to be fairly generic and, largely a generalization of the earlier model for the Australian election. One good thing about forecasting the US election is that there is a lot of data around. In this case we can use survey data from the Democracy Fund Voter Study Group.12 They conducted polling in the lead-up to the US election and make this publicly available after registration. We will use the Integrated Public Use Microdata Series (IPUMS), to access the 2018 American Community Survey (ACS) as a post-stratification dataset. We will use state, age-group, gender, and education as explanatory variables. 16.5.1 Survey data The first step is that we need to actually get the survey data. Go to their website:https://www.voterstudygroup.org and then you’re looking for ‘Nationscape’ and then a button along the lines of ‘Get the latest Nationscape data.’ To get the dataset, you need to fill out a form, which they will process and then email you. There is a real person on the other side of this form, and so your request could take a few days. Once you get access you’ll want to download the .dta files. Nationscape conducted many surveys and so there are many files. The filename is the reference date, and so ‘ns20200625’ refers to 25 June 2020, which is the one that I’ll use here. (This data is not mine to share, which is why I’ll refer) We can read in ‘.dta’ files using the haven package (Wickham and Miller 2020). I’ve based this code on that written by Alen Mitrovski, Xiaoyan Yang, Matthew Wankiewicz, which is available here: https://github.com/matthewwankiewicz/US_election_forecast. library(haven) library(tidyverse) raw_nationscape_data &lt;- read_dta(here::here(&quot;dont_push/ns20200625.dta&quot;)) # The Stata format separates labels so reunite those raw_nationscape_data &lt;- labelled::to_factor(raw_nationscape_data) # Just keep relevant variables nationscape_data &lt;- raw_nationscape_data %&gt;% select(vote_2020, gender, education, state, age) # For simplicity, remove anyone undecided or planning to vote for someone other than Biden/Trump and make vote a binary variable: 1 for Biden, 0 for Trump. nationscape_data &lt;- nationscape_data %&gt;% filter(vote_2020 == &quot;Joe Biden&quot; | vote_2020 == &quot;Donald Trump&quot;) %&gt;% mutate(vote_biden = if_else(vote_2020 == &quot;Joe Biden&quot;, 1, 0)) %&gt;% select(-vote_2020) # Create the dependent variables by grouping the existing variables nationscape_data &lt;- nationscape_data %&gt;% mutate( age_group = case_when( # case_when works in order and exits when there&#39;s a match age &lt;= 29 ~ &#39;age_18-29&#39;, age &lt;= 44 ~ &#39;age_30-44&#39;, age &lt;= 59 ~ &#39;age_45-59&#39;, age &gt;= 60 ~ &#39;age_60_or_more&#39;, TRUE ~ &#39;Trouble&#39; ), gender = case_when( gender == &quot;Female&quot; ~ &#39;female&#39;, gender == &quot;Male&quot; ~ &#39;male&#39;, TRUE ~ &#39;Trouble&#39; ), education_level = case_when( education == &quot;3rd Grade or less&quot; ~ &quot;High school or less&quot;, education == &quot;Middle School - Grades 4 - 8&quot; ~ &quot;High school or less&quot;, education == &quot;Completed some high school&quot; ~ &quot;High school or less&quot;, education == &quot;High school graduate&quot; ~ &quot;High school or less&quot;, education == &quot;Other post high school vocational training&quot; ~ &quot;Some post secondary&quot;, education == &quot;Completed some college, but no degree&quot; ~ &quot;Some post secondary&quot;, education == &quot;Associate Degree&quot; ~ &quot;Post secondary or higher&quot;, education == &quot;College Degree (such as B.A., B.S.)&quot; ~ &quot;Post secondary or higher&quot;, education == &quot;Completed some graduate, but no degree&quot; ~ &quot;Post secondary or higher&quot;, education == &quot;Masters degree&quot; ~ &quot;Graduate degree&quot;, education == &quot;Doctorate degree&quot; ~ &quot;Graduate degree&quot;, TRUE ~ &#39;Trouble&#39; ) ) %&gt;% select(-education, -age) tests &lt;- nationscape_data %&gt;% mutate(test = stringr::str_detect(age_group, &#39;Trouble&#39;), test = if_else(test == TRUE, TRUE, stringr::str_detect(education_level, &#39;Trouble&#39;)), test = if_else(test == TRUE, TRUE, stringr::str_detect(gender, &#39;Trouble&#39;)) ) %&gt;% filter(test == TRUE) if(nrow(tests) != 0) { print(&quot;Check nationscape_data&quot;) } else { rm(tests) } nationscape_data %&gt;% head() Table 16.7: genderstatevote_bidenage_groupeducation_level femaleWI0age_45-59Post secondary or higher femaleVA0age_45-59Post secondary or higher femaleTX0age_60_or_moreHigh school or less femaleWA0age_45-59High school or less femaleMA1age_18-29Some post secondary femaleTX1age_30-44Some post secondary As we’ve seen, one of the most difficult aspects with MRP is ensuring consistency between the datasets. In this case, we need to do some work to make the variables consistent. # This code is very directly from Alen Mitrovski, Xiaoyan Yang, and Matthew Wankiewicz. # Format state names so the whole state name is written out, to match IPUMS data states_names_and_abbrevs &lt;- tibble(stateicp = state.name, state = state.abb) nationscape_data &lt;- nationscape_data %&gt;% left_join(states_names_and_abbrevs) rm(states_names_and_abbrevs) # Make lowercase to match IPUMS data nationscape_data &lt;- nationscape_data %&gt;% mutate(stateicp = tolower(stateicp)) # Replace NAs with DC nationscape_data$stateicp &lt;- replace_na(nationscape_data$stateicp, &quot;district of columbia&quot;) # Tidy the class nationscape_data &lt;- nationscape_data %&gt;% mutate(across(c(gender, stateicp, education_level, age_group), as_factor)) # Save data write_csv(nationscape_data, &quot;outputs/data/polling_data.csv&quot;) nationscape_data %&gt;% head() Table 16.8: genderstatevote_bidenage_groupeducation_levelstateicp femaleWI0age_45-59Post secondary or higherwisconsin femaleVA0age_45-59Post secondary or highervirginia femaleTX0age_60_or_moreHigh school or lesstexas femaleWA0age_45-59High school or lesswashington femaleMA1age_18-29Some post secondarymassachusetts femaleTX1age_30-44Some post secondarytexas 16.5.2 Post-stratification data We have a lot of options for a dataset to post-stratify by and there are various considerations. We are after a dataset that is better quality (however that is to be defined), and likely larger. From a strictly data perspective, the best choice would probably be something like the Cooperative Congressional Election Study (CCES), however for whatever reason that is only released after the election and so it’s not a reasonable choice. Wang et al. (2015) use exit poll data, but again that’s only available after the election. In most countries we’d be stuck using the census, which is of course quite large, but likely out-of-date. Luckily in the US we have the opportunity to use the American Community Survey (ACS) which asks analogous questions to a census, is conducted every month, and over the course of a year, we end up with a few million responses. In this case we’re going to access the ACS through IPUMS. To do this go to the IPUMS website - https://ipums.org - and we’re looking for something like IPUMS USA and then ‘get data.’ Create an account, if you need to. That’ll take a while to process. But once you have an account, go to ‘Select Samples’ and de-select everything apart from the 2019 ACS. Then we need to get the variables that we’re interested in. From the household we want ‘STATEICP,’ then in person we want ‘SEX,’ ‘AGE,’ ‘EDUC.’ Once everything is selected, ‘view cart,’ and we want to be careful to change the ‘data format’ to ‘.dta’ (there’s nothing wrong with the other formats, but we’ve just already got code earlier to deal with that type). Briefly just check how many rows and columns you’re requesting. It should be around a million rows, and around ten to twenty columns. If it’s much more than 300MB then maybe just see if you’ve accidently selected something that you don’t need. Submit the request and within a day, you should get an email saying that your data can be downloaded. It should only take 30 minutes or so, but if you don’t get an email within a day then check again the size of the dataset, and customize the sample size to reduce the size initially. In any case let’s tidy up the data. # Again, closely following code from Alen Mitrovski, Xiaoyan Yang, and Matthew Wankiewicz. library(haven) library(tidyverse) raw_poststrat_data &lt;- read_dta(here::here(&quot;dont_push/usa_00004.dta&quot;)) # The Stata format separates labels so reunite those raw_poststrat_data &lt;- labelled::to_factor(raw_poststrat_data) head(raw_poststrat_data) Table 16.9: yearsampleserialcbserialhhwtclusterregionstateicpstratagqpernumperwtsexagemarstraceracedhispanhispandbplbpldcitizeneduceducdempstatempstatdlabforceinctot 20182018 acs22.02e+12392&nbsp;&nbsp;2.02e+12east south central divalabama1.9e+05other group quarters1392&nbsp;&nbsp;female18never married/singleblack/african american/negroblack/african american/negronot hispanicnot hispanicgeorgiageorgian/agrade 12some college, but less than 1 yearnot in labor forcenot in labor forceno, not in the labor force1.6e+03&nbsp; 20182018 acs72.02e+1294.12.02e+12east south central divalabama4e+04&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;group quarters--institutions194.1female66widowedwhitewhitenot hispanicnot hispanicalabamaalabaman/a1 year of college1 or more years of college credit, no degreenot in labor forcenot in labor forceno, not in the labor force1.5e+04&nbsp; 20182018 acs132.02e+1283.72.02e+12east south central divalabama1.3e+05other group quarters183.7male57separatedother race, necother race, n.e.cotherdominicanwest indieswest indies, nsnaturalized citizen1 year of college1 or more years of college credit, no degreenot in labor forcenot in labor forceno, not in the labor force1.4e+03&nbsp; 20182018 acs182.02e+1257.52.02e+12east south central divalabama1e+05&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;group quarters--institutions157.5male37separatedblack/african american/negroblack/african american/negronot hispanicnot hispanicalabamaalabaman/agrade 12regular high school diplomanot in labor forcenot in labor forceno, not in the labor force2.41e+04 20182018 acs232.02e+12157&nbsp;&nbsp;2.02e+12east south central divalabama1.9e+05group quarters--institutions1157&nbsp;&nbsp;female71divorcedwhitewhitenot hispanicnot hispanicalabamaalabaman/agrade 12regular high school diplomanot in labor forcenot in labor forceno, not in the labor force1.28e+04 20182018 acs282.02e+12157&nbsp;&nbsp;2.02e+12east south central divalabama2.2e+05other group quarters1157&nbsp;&nbsp;male39never married/singlewhitewhitenot hispanicnot hispanicalabamaalabaman/agrade 12regular high school diplomanot in labor forcenot in labor forceno, not in the labor force1.12e+04 raw_poststrat_data$age &lt;- as.numeric(raw_poststrat_data$age) poststrat_data &lt;- raw_poststrat_data %&gt;% filter(inctot &lt; 9999999) %&gt;% filter(age &gt;= 18) %&gt;% mutate(gender = sex) %&gt;% mutate( age_group = case_when( # case_when works in order and exits when there&#39;s a match age &lt;= 29 ~ &#39;age_18-29&#39;, age &lt;= 44 ~ &#39;age_30-44&#39;, age &lt;= 59 ~ &#39;age_45-59&#39;, age &gt;= 60 ~ &#39;age_60_or_more&#39;, TRUE ~ &#39;Trouble&#39; ), education_level = case_when( educd == &quot;nursery school, preschool&quot; ~ &quot;High school or less&quot;, educd == &quot;kindergarten&quot; ~ &quot;High school or less&quot;, educd == &quot;grade 1&quot; ~ &quot;High school or less&quot;, educd == &quot;grade 2&quot; ~ &quot;High school or less&quot;, educd == &quot;grade 3&quot; ~ &quot;High school or less&quot;, educd == &quot;grade 4&quot; ~ &quot;High school or less&quot;, educd == &quot;grade 5&quot; ~ &quot;High school or less&quot;, educd == &quot;grade 6&quot; ~ &quot;High school or less&quot;, educd == &quot;grade 7&quot; ~ &quot;High school or less&quot;, educd == &quot;grade 8&quot; ~ &quot;High school or less&quot;, educd == &quot;grade 9&quot; ~ &quot;High school or less&quot;, educd == &quot;grade 10&quot; ~ &quot;High school or less&quot;, educd == &quot;grade 11&quot; ~ &quot;High school or less&quot;, educd == &quot;12th grade, no diploma&quot; ~ &quot;High school or less&quot;, educd == &quot;regular high school diploma&quot; ~ &quot;High school or less&quot;, educd == &quot;ged or alternative credential&quot; ~ &quot;High school or less&quot;, educd == &quot;some college, but less than 1 year&quot; ~ &quot;Some post secondary&quot;, educd == &quot;1 or more years of college credit, no degree&quot; ~ &quot;Some post secondary&quot;, educd == &quot;associate&#39;s degree, type not specified&quot; ~ &quot;Post secondary or higher&quot;, educd == &quot;bachelor&#39;s degree&quot; ~ &quot;Post secondary or higher&quot;, educd == &quot;master&#39;s degree&quot; ~ &quot;Graduate degree&quot;, educd == &quot;professional degree beyond a bachelor&#39;s degree&quot; ~ &quot;Graduate degree&quot;, educd == &quot;doctoral degree&quot; ~ &quot;Graduate degree&quot;, educd == &quot;no schooling completed&quot; ~ &quot;High school or less&quot;, TRUE ~ &#39;Trouble&#39; ) ) # Just keep relevant variables poststrat_data &lt;- poststrat_data %&gt;% select(gender, age_group, education_level, stateicp) # Tidy the class poststrat_data &lt;- poststrat_data %&gt;% mutate(across(c(gender, stateicp, education_level, age_group), as_factor)) # Save data write_csv(poststrat_data, &quot;outputs/data/us_poststrat.csv&quot;) poststrat_data %&gt;% head() Table 16.9: genderage_groupeducation_levelstateicp femaleage_18-29Some post secondaryalabama femaleage_60_or_moreSome post secondaryalabama maleage_45-59Some post secondaryalabama maleage_30-44High school or lessalabama femaleage_60_or_moreHigh school or lessalabama maleage_30-44High school or lessalabama This dataset is on an individual level. So we’ll create counts of each sub-cell, and then proportions by state. poststrat_data_cells &lt;- poststrat_data %&gt;% group_by(stateicp, gender, age_group, education_level) %&gt;% count() Now we’d like to add proportions by state. poststrat_data_cells &lt;- poststrat_data_cells %&gt;% group_by(stateicp) %&gt;% mutate(prop = n/sum(n)) %&gt;% ungroup() poststrat_data_cells %&gt;% head() Table 16.10: stateicpgenderage_groupeducation_levelnprop connecticutmaleage_18-29Some post secondary1490.026&nbsp;&nbsp; connecticutmaleage_18-29High school or less2320.0404&nbsp; connecticutmaleage_18-29Post secondary or higher960.0167&nbsp; connecticutmaleage_18-29Graduate degree250.00436 connecticutmaleage_60_or_moreSome post secondary1420.0248&nbsp; connecticutmaleage_60_or_moreHigh school or less3710.0647&nbsp; 16.5.3 Model We’re going to use logistic regression to estimate a model where the binary of support for Biden is explained by gender, age-group, education-level, and state. We’re going to do this in a Bayesian framework using rstanarm (Goodrich et al. 2020). There are a variety of reasons for using rstanarm here, but the main one is that Stan is pre-compiled which eases some of the computer set-up issues that we may otherwise have. A great further resource about implementing MRP with rstanarm is Kennedy and Gabry (2020). library(rstanarm) us_election_model &lt;- rstanarm::stan_glmer(vote_biden ~ gender + age_group + (1 | stateicp) + education_level, data = nationscape_data, family = binomial(link = &quot;logit&quot;), prior = normal(0, 1), prior_intercept = normal(0, 1), cores = 2, seed = 853) There are a variety of options here that we’ve largely unthinkingly set, and exploring the effect of these would be a good idea, but for now we can just have a quick look at the model. modelsummary::get_estimates(us_election_model) Table 16.11: termestimatestd.errorconf.lowconf.high (Intercept)0.276&nbsp;0.08680.09880.443&nbsp;&nbsp; gendermale-0.541&nbsp;0.0586-0.651&nbsp;-0.428&nbsp;&nbsp; age_groupage_60_or_more0.04890.0778-0.107&nbsp;0.206&nbsp;&nbsp; age_groupage_18-290.878&nbsp;0.09220.699&nbsp;1.06&nbsp;&nbsp;&nbsp; age_groupage_30-440.125&nbsp;0.0792-0.03130.282&nbsp;&nbsp; education_levelHigh school or less-0.351&nbsp;0.0779-0.506&nbsp;-0.197&nbsp;&nbsp; education_levelSome post secondary-0.15&nbsp;&nbsp;0.0753-0.296&nbsp;0.00993 education_levelGraduate degree-0.22&nbsp;&nbsp;0.0887-0.39&nbsp;&nbsp;-0.0418&nbsp; # The default usage of modelsummary requires statistics that we don&#39;t have. # Uncomment the following line if you want to look at what is available and specify your own: # modelsummary::get_estimates(us_election_model) modelsummary::modelsummary(us_election_model, statistic = c(&#39;conf.low&#39;, &#39;conf.high&#39;) ) Model 1 (Intercept) 0.276 (0.099) (0.443) gendermale -0.541 (-0.651) (-0.428) age_groupage_60_or_more 0.049 (-0.107) (0.206) age_groupage_18-29 0.878 (0.699) (1.063) age_groupage_30-44 0.125 (-0.031) (0.282) education_levelHigh school or less -0.351 (-0.506) (-0.197) education_levelSome post secondary -0.150 (-0.296) (0.010) education_levelGraduate degree -0.220 (-0.390) (-0.042) Num.Obs. 5200 algorithm sampling pss 4000.000 16.5.4 Post-stratify biden_support_by_state &lt;- us_election_model %&gt;% tidybayes::add_fitted_draws(newdata=poststrat_data_cells) %&gt;% rename(support_biden_predict = .value) %&gt;% mutate(support_biden_predict_prop = support_biden_predict*prop) %&gt;% group_by(stateicp, .draw) %&gt;% summarise(support_biden_predict = sum(support_biden_predict_prop)) %&gt;% group_by(stateicp) %&gt;% summarise(mean = mean(support_biden_predict), lower = quantile(support_biden_predict, 0.025), upper = quantile(support_biden_predict, 0.975)) And we can have a look at our estimates, if we like. biden_support_by_state %&gt;% ggplot(aes(y = mean, x = stateicp, color = &quot;MRP estimate&quot;)) + geom_point() + geom_errorbar(aes(ymin = lower, ymax = upper), width = 0) + geom_point(data = nationscape_data %&gt;% group_by(stateicp, vote_biden) %&gt;% summarise(n = n()) %&gt;% group_by(stateicp) %&gt;% mutate(prop = n/sum(n)) %&gt;% filter(vote_biden==1), aes(y = prop, x = stateicp, color = &#39;Nationscape raw data&#39;)) + geom_hline(yintercept = 0.5, linetype = &#39;dashed&#39;) + labs(x = &#39;State&#39;, y = &#39;Estimated proportion support for Biden&#39;, color = &#39;Source&#39;) + theme_classic() + scale_color_brewer(palette = &#39;Set1&#39;) + coord_flip() 16.6 Concluding remarks and next steps In general, MRP is a good way to accomplish specific aims, but it’s not without trade-offs. If you have a good quality survey, then it may be a way to speak to disaggregated aspects of it. Or if you are concerned about uncertainty then it is a good way to think about that. If you have a biased survey then it’s a great place to start, but it’s not a panacea. There’s not a lot of work that’s been done with it, so there’s plenty of scope for exciting work from a variety of approaches: From a more statistical perspective, there is a lot of work to do in terms of thinking through how survey design and modelling approaches interact and the extent to which we are underestimating uncertainty. I’m also very interested in thinking through the implications of small samples and uncertainty in the post-stratification dataset. There’s an awful lot to do in terms of thinking through what the appropriate model is to use, and how do we even evaluate what ‘appropriate’ means here? Those with statistical interests, should probably go next to Gao et al. (2021) as well as pretty much anything by Yajuan Si, but Si (2020) would be a starting point. There’s a lot to be done from a sociology perspective in terms of survey responses and how we can better design our surveys, knowing they are going to be used for MRP and putting respect for our respondents first. From a political science perspective, we just have very little idea of the conditions under which we will have the stable preferences and relationships that are required for MRP to be accurate, and further understanding how this relates to uncertainty in survey design. For those with political science interests, a natural next step would be to go through Lauderdale et al. (2020) or Ghitza and Gelman (2020). Economists might be interested to think about how we could use MRP to better understand the inflation and unemployment rates at local levels. From a statistical software side of things, we really need to develop better packages around this. It’s from an information side of things, that I’m most excited about MRP. How do we best store and protect our datasets, yet retain the ability to have them correspond with each other? How do we put the levels together in a way that is meaningful? To what extent do people appreciate uncertainty estimates and how can we better communicate these estimates? More generally, we could pretty much use MRP anywhere we have samples. Determining the conditions under which we actually should, is the work of whole generations. I’d like to acknowledge and thank Lauren Kennedy and Monica Alexander, through whose generous sharing of code, data, and countless conversations my thoughts about MRP have developed.↩︎ You can work out which coefficients to be pass to gather_draws by using tidybayes::get_variables(model). (In this example I passed ‘b_.’ but the ones of interest to you may be different.)↩︎ I thank Chris Warshaw for putting me onto this dataset.↩︎ "],["text-as-data.html", "Chapter 17 Text as data 17.1 Introduction 17.2 Lasso regression 17.3 Topic models 17.4 Word embedding 17.5 Conclusion", " Chapter 17 Text as data Last updated: 29 March 2021. Required reading Hvitfeldt, Emil, and Julia Silge, 2021, Supervised Machine Learning for Text Analysis in R, Chapters 2, 5, 6, 7, https://smltar.com. Silge, Julia, and David Robinson, 2017, Text Mining with R, https://www.tidytextmining.com. Required viewing Recommended reading Amaka, Ofunne, and Amber Thomas, 2020, ‘The Naked Truth: How the names of 6,816 complexion products can reveal bias in beauty,’ The Pudding, March, https://pudding.cool/2021/03/foundation-names/. Key concepts/skills/etc Key libraries Key functions/etc Quiz 17.1 Introduction Text can be thought of as an unwieldy, but in generally 17.2 Lasso regression This subsection, and much of the code that is used, directly draws on Julia Silge’s notes, in particular: https://juliasilge.com/blog/tidy-text-classification/ (Silge 2018). One of the nice aspects of text is that we can adapt our existing methods to use it as an input. Here we are going to use a variation of logistics regression, along with text inputs, to forecast. If you want to learn more about Lasso regression, then you should consider taking Arik’s course over the summer, where he will dive into machine learning using Python. In this section we are going to have two different text inputs, train a model on a sample of text from each of them, and then try to use that model to forecast the text in a training set. Although this is a arbitrary example, you could imagine many real-world applications. For instance, if you work at Twitter then you may want to know if a tweet was likely written by a bot, or by a human. Or similarly, imagine that you work for a political party - you may like to know if an email was likely from an email campaign organised by a group, or from an individual. First we need to get some data. Julia Silge’s example, nicely, uses book text as input. Seeing as I am jointly appointed at a Faculty of Information, that seems especially nice. The wonderful thing about this is that there is an R package - gutenbergr - that makes it easy to get text from Project Gutenberg into R. The key function is gutenberg_download(), which needs a key for the book that you want. We’ll consider Jane Eyre and Alice’s Adventures in Wonderland, which have the keys of 1260 and 11, respectively. library(gutenbergr) alice_and_jane &lt;- gutenbergr::gutenberg_download(c(1260, 11), meta_fields = &quot;title&quot;) # Save the dataset so that we don&#39;t need to overwhelm the servers each time write_csv(alice_and_jane, &quot;inputs/books/alice_and_jane.csv&quot;) head(alice_and_jane) library(gutenbergr) alice_and_jane &lt;- read_csv(&quot;inputs/books/alice_and_jane.csv&quot;) head(alice_and_jane) Table 17.1: gutenberg_idtexttitle 11ALICE'S ADVENTURES IN WONDERLANDAlice's Adventures in Wonderland 11Alice's Adventures in Wonderland 11Lewis CarrollAlice's Adventures in Wonderland 11Alice's Adventures in Wonderland 11THE MILLENNIUM FULCRUM EDITION 3.0Alice's Adventures in Wonderland 11Alice's Adventures in Wonderland One of the great things about this is that the dataset is a tibble. So we can just work with all our familiar skills. The package has a lot more functionality, so I’d encourage you to look at the package’s website: https://github.com/ropensci/gutenbergr. Each line of the book is read in as a different row in the dataset. Notice that we have downloaded two books here at once, and so we added the title. The two books are one after each other. You can see that we have both by looking at some summary statistics. table(alice_and_jane$title) ## ## Alice&#39;s Adventures in Wonderland Jane Eyre: An Autobiography ## 3339 20659 So it looks like Jane Eyre is much longer than Alice in Wonderland, which isn’t a surprise to those who have read them. I don’t want to step into Digital Humanities too much, as I don’t know anything about it, but looking at things like the broader context of when these books were written, or other books that were written at similar times, is likely a fascinating area. We’ll just get rid of blank lines library(janitor) # TODO There&#39;s a way to do this within janitor, but I forget, need to look it up. alice_and_jane &lt;- alice_and_jane %&gt;% mutate(blank_line = if_else(text == &quot;&quot;, 1, 0)) %&gt;% filter(blank_line == 0) %&gt;% select(-blank_line) table(alice_and_jane$title) ## ## Alice&#39;s Adventures in Wonderland Jane Eyre: An Autobiography ## 2481 16395 There’s still an overwhelming amount of Jane Eyre in there. So we’ll just sample from Jane Eyre to make it more equal. set.seed(853) alice_and_jane$rows &lt;- c(1:nrow(alice_and_jane)) sample_from_me &lt;- alice_and_jane %&gt;% filter(title == &quot;Jane Eyre: An Autobiography&quot;) keep_me &lt;- sample(x = sample_from_me$rows, size = 2481, replace = FALSE) alice_and_jane &lt;- alice_and_jane %&gt;% filter(title == &quot;Alice&#39;s Adventures in Wonderland&quot; | rows %in% keep_me) %&gt;% select(-rows) table(alice_and_jane$title) ## ## Alice&#39;s Adventures in Wonderland Jane Eyre: An Autobiography ## 2481 2481 There’s a bunch of issues here, for instance, we have the whole of Alice, but we only have random bits of Jane, but nonetheless let’s continue and we’ll try to do something about that in a moment. Now we want to get a sample of text from each book. We will use the lines to distinguish these samples. So we use a counter that will add a line number. alice_and_jane &lt;- alice_and_jane %&gt;% group_by(title) %&gt;% mutate(line_number = paste(gutenberg_id, row_number(), sep = &quot;_&quot;)) %&gt;% ungroup() We now want to separate out the words. We’ll just use tidytext, because the focus here is on modelling, but there are a bunch of alternatives and one especially good one is the quanteda package, specifically, the tokens() function. library(tidytext) alice_and_jane_by_word &lt;- alice_and_jane %&gt;% unnest_tokens(word, text) %&gt;% group_by(word) %&gt;% filter(n() &gt; 10) %&gt;% ungroup() Notice here that we removed any word that wasn’t used more than 10 times. Nonetheless we still have a lot of unique words. (If we didn’t require that the word be used by the author at least 10 times then we end up with more than 6,000 words.) alice_and_jane_by_word$word %&gt;% unique() %&gt;% length() ## [1] 585 The reason this is relevant is because these are our independent variables. So where you may be used to having something less than 10 explanatory variables, in this case we are going to have 585 As such, we need a model that can handle this. However, as mentioned before, we are going to have some rows that essentially just had one word. While we could allow that, it might also be nice to give the model at least a few words to work with. alice_and_jane_by_word &lt;- alice_and_jane_by_word %&gt;% group_by(title, line_number) %&gt;% mutate(number_of_words_in_line = n()) %&gt;% ungroup() %&gt;% filter(number_of_words_in_line &gt; 2) %&gt;% select(-number_of_words_in_line) We’ll create a test/training split, and load in tidymodels. library(tidymodels) set.seed(853) alice_and_jane_by_word_split &lt;- alice_and_jane_by_word %&gt;% select(title, line_number) %&gt;% distinct() %&gt;% initial_split(prop = 3/4, strata = title) # alice_and_jane_by_word_train &lt;- training(alice_and_jane_by_word_split) %&gt;% select(line_number) # alice_and_jane_by_word_test &lt;- testing(alice_and_jane_by_word_split) # # rm(alice_and_jane_by_word_split) Now we need to create a document-term matrix. alice_and_jane_dtm_training &lt;- alice_and_jane_by_word %&gt;% count(line_number, word) %&gt;% inner_join(training(alice_and_jane_by_word_split) %&gt;% select(line_number)) %&gt;% cast_dtm(term = word, document = line_number, value = n) dim(alice_and_jane_dtm_training) ## [1] 3415 585 So we have our independent variables sorted, now we need our binary dependent variable, which is whether the book is Alice in Wonderland or Jane Eyre. response &lt;- data.frame(id = dimnames(alice_and_jane_dtm_training)[[1]]) %&gt;% separate(id, into = c(&quot;book&quot;, &quot;line&quot;, sep = &quot;_&quot;)) %&gt;% mutate(is_alice = if_else(book == 11, 1, 0)) predictor &lt;- alice_and_jane_dtm_training[] %&gt;% as.matrix() Now we can run our model. library(glmnet) model &lt;- cv.glmnet(x = predictor, y = response$is_alice, family = &quot;binomial&quot;, keep = TRUE ) save(model, file = &quot;outputs/models/alice_vs_jane.rda&quot;) load(&quot;outputs/models/alice_vs_jane.rda&quot;) library(glmnet) library(broom) coefs &lt;- model$glmnet.fit %&gt;% tidy() %&gt;% filter(lambda == model$lambda.1se) coefs %&gt;% head() Table 17.2: termstepestimatelambdadev.ratio (Intercept)36-0.335&nbsp;&nbsp;0.005970.562 in36-0.144&nbsp;&nbsp;0.005970.562 she360.39&nbsp;&nbsp;&nbsp;0.005970.562 so360.002490.005970.562 a36-0.117&nbsp;&nbsp;0.005970.562 about360.279&nbsp;&nbsp;0.005970.562 coefs %&gt;% group_by(estimate &gt; 0) %&gt;% top_n(10, abs(estimate)) %&gt;% ungroup() %&gt;% ggplot(aes(fct_reorder(term, estimate), estimate, fill = estimate &gt; 0)) + geom_col(alpha = 0.8, show.legend = FALSE) + coord_flip() + theme_minimal() + labs(x = &quot;Coefficient&quot;, y = &quot;Word&quot;) + scale_fill_brewer(palette = &quot;Set1&quot;) Perhaps unsurprisingly, if you mention Alice then it’s likely to be a Alice in Wonderland and if you mention Jane then it’s likely to be Jane Eyre. 17.3 Topic models A version of these notes was previously circulated as part of R. Alexander and Alexander (2020). 17.3.1 Overview Sometimes we have a statement and we want to know what it is about. Sometimes this will be easy, but we don’t always have titles for statements, and even when we do, sometimes we do not have titles that define topics in a well-defined and consistent way. One way to get consistent estimates of the topics of each statement is to use topic models. While there are many variants, one way is to use the latent Dirichlet allocation (LDA) method of Blei, Ng, and Jordan (2003), as implemented by the R package ‘topicmodels’ by Grün and Hornik (2011). The key assumption behind the LDA method is that each statement, ‘a document,’ is made by a person who decides the topics they would like to talk about in that document, and then chooses words, ‘terms,’ that are appropriate to those topics. A topic could be thought of as a collection of terms, and a document as a collection of topics. The topics are not specified ex ante; they are an outcome of the method. Terms are not necessarily unique to a particular topic, and a document could be about more than one topic. This provides more flexibility than other approaches such as a strict word count method. The goal is to have the words found in documents group themselves to define topics. 17.3.2 Document generation process The LDA method considers each statement to be a result of a process where a person first chooses the topics they want to speak about. After choosing the topics, the person then chooses appropriate words to use for each of those topics. More generally, the LDA topic model works by considering each document as having been generated by some probability distribution over topics. For instance, if there were five topics and two documents, then the first document may be comprised mostly of the first few topics; the other document may be mostly about the final few topics (Figure 17.1). Figure 17.1: Probability distributions over topics Similarly, each topic could be considered a probability distribution over terms. To choose the terms used in each document the speaker picks terms from each topic in the appropriate proportion. For instance, if there were ten terms, then one topic could be defined by giving more weight to terms related to immigration; and some other topic may give more weight to terms related to the economy (Figure 17.2). Figure 17.2: Probability distributions over terms Following Blei and Lafferty (2009), Blei (2012) and Griffiths and Steyvers (2004), the process by which a document is generated is more formally considered to be: There are \\(1, 2, \\dots, k, \\dots, K\\) topics and the vocabulary consists of \\(1, 2, \\dots, V\\) terms. For each topic, decide the terms that the topic uses by randomly drawing distributions over the terms. The distribution over the terms for the \\(k\\)th topic is \\(\\beta_k\\). Typically a topic would be a small number of terms and so the Dirichlet distribution with hyperparameter \\(0&lt;\\eta&lt;1\\) is used: \\(\\beta_k \\sim \\mbox{Dirichlet}(\\eta)\\).13 Strictly, \\(\\eta\\) is actually a vector of hyperparameters, one for each \\(K\\), but in practice they all tend to be the same value. Decide the topics that each document will cover by randomly drawing distributions over the \\(K\\) topics for each of the \\(1, 2, \\dots, d, \\dots, D\\) documents. The topic distributions for the \\(d\\)th document are \\(\\theta_d\\), and \\(\\theta_{d,k}\\) is the topic distribution for topic \\(k\\) in document \\(d\\). Again, the Dirichlet distribution with the hyperparameter \\(0&lt;\\alpha&lt;1\\) is used here because usually a document would only cover a handful of topics: \\(\\theta_d \\sim \\mbox{Dirichlet}(\\alpha)\\). Again, strictly \\(\\alpha\\) is vector of length \\(K\\) of hyperparameters, but in practice each is usually the same value. If there are \\(1, 2, \\dots, n, \\dots, N\\) terms in the \\(d\\)th document, then to choose the \\(n\\)th term, \\(w_{d, n}\\): Randomly choose a topic for that term \\(n\\), in that document \\(d\\), \\(z_{d,n}\\), from the multinomial distribution over topics in that document, \\(z_{d,n} \\sim \\mbox{Multinomial}(\\theta_d)\\). Randomly choose a term from the relevant multinomial distribution over the terms for that topic, \\(w_{d,n} \\sim \\mbox{Multinomial}(\\beta_{z_{d,n}})\\). Given this set-up, the joint distribution for the variables is (Blei (2012), p.6): \\[p(\\beta_{1:K}, \\theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}) = \\prod^{K}_{i=1}p(\\beta_i) \\prod^{D}_{d=1}p(\\theta_d) \\left(\\prod^N_{n=1}p(z_{d,n}|\\theta_d)p\\left(w_{d,n}|\\beta_{1:K},z_{d,n}\\right) \\right).\\] Based on this document generation process the analysis problem, discussed in the next section, is to compute a posterior over \\(\\beta_{1:K}\\) and \\(\\theta_{1:D}\\), given \\(w_{1:D, 1:N}\\). This is intractable directly, but can be approximated (Griffiths and Steyvers (2004) and Blei (2012)). 17.3.3 Analysis process After the documents are created, they are all that we have to analyse. The term usage in each document, \\(w_{1:D, 1:N}\\), is observed, but the topics are hidden, or ‘latent.’ We do not know the topics of each document, nor how terms defined the topics. That is, we do not know the probability distributions of Figures 17.1 or 17.2. In a sense we are trying to reverse the document generation process – we have the terms and we would like to discover the topics. If the earlier process around how the documents were generated is assumed and we observe the terms in each document, then we can obtain estimates of the topics (Steyvers and Griffiths (2006)). The outcomes of the LDA process are probability distributions and these define the topics. Each term will be given a probability of being a member of a particular topic, and each document will be given a probability of being about a particular topic. That is, we are trying to calculate the posterior distribution of the topics given the terms observed in each document (Blei (2012), p.7): \\[p(\\beta_{1:K}, \\theta_{1:D}, z_{1:D, 1:N} | w_{1:D, 1:N}) = \\frac{p\\left(\\beta_{1:K}, \\theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}\\right)}{p(w_{1:D, 1:N})}.\\] The initial practical step when implementing LDA given a corpus of documents is to remove ‘stop words.’ These are words that are common, but that don’t typically help to define topics. There is a general list of stop words such as: “a”; “a’s”; “able”; “about”; “above”… We also remove punctuation and capitalisation. The documents need to then be transformed into a document-term-matrix. This is essentially a table with a column of the number of times each term appears in each document. After the dataset is ready, the R package ‘topicmodels’ by Grün and Hornik (2011) can be used to implement LDA and approximate the posterior. It does this using Gibbs sampling or the variational expectation-maximization algorithm. Following Steyvers and Griffiths (2006) and Darling (2011), the Gibbs sampling process attempts to find a topic for a particular term in a particular document, given the topics of all other terms for all other documents. Broadly, it does this by first assigning every term in every document to a random topic, specified by Dirichlet priors with \\(\\alpha = \\frac{50}{K}\\) and \\(\\eta = 0.1\\) (Steyvers and Griffiths (2006) recommends \\(\\eta = 0.01\\)), where \\(\\alpha\\) refers to the distribution over topics and \\(\\eta\\) refers to the distribution over terms (Grün and Hornik (2011), p.7). It then selects a particular term in a particular document and assigns it to a new topic based on the conditional distribution where the topics for all other terms in all documents are taken as given (Grün and Hornik (2011), p.6): \\[p(z_{d, n}=k | w_{1:D, 1:N}, z&#39;_{d, n}) \\propto \\frac{\\lambda&#39;_{n\\rightarrow k}+\\eta}{\\lambda&#39;_{.\\rightarrow k}+V\\eta} \\frac{\\lambda&#39;^{(d)}_{n\\rightarrow k}+\\alpha}{\\lambda&#39;^{(d)}_{-i}+K\\alpha} \\] where \\(z&#39;_{d, n}\\) refers to all other topic assignments; \\(\\lambda&#39;_{n\\rightarrow k}\\) is a count of how many other times that term has been assigned to topic \\(k\\); \\(\\lambda&#39;_{.\\rightarrow k}\\) is a count of how many other times that any term has been assigned to topic \\(k\\); \\(\\lambda&#39;^{(d)}_{n\\rightarrow k}\\) is a count of how many other times that term has been assigned to topic \\(k\\) in that particular document; and \\(\\lambda&#39;^{(d)}_{-i}\\) is a count of how many other times that term has been assigned in that document. Once \\(z_{d,n}\\) has been estimated, then estimates for the distribution of words into topics and topics into documents can be backed out. This conditional distribution assigns topics depending on how often a term has been assigned to that topic previously, and how common the topic is in that document (Steyvers and Griffiths (2006)). The initial random allocation of topics means that the results of early passes through the corpus of document are poor, but given enough time the algorithm converges to an appropriate estimate. 17.3.4 Warnings and extensions The choice of the number of topics, k, affects the results, and must be specified a priori. If there is a strong reason for a particular number, then this can be used. Otherwise, one way to choose an appropriate number is to use a test and training set process. Essentially, this means running the process on a variety of possible values for k and then picking an appropriate value that performs well. One weakness of the LDA method is that it considers a ‘bag of words’ where the order of those words does not matter (Blei (2012)). It is possible to extend the model to reduce the impact of the bag-of-words assumption and add conditionality to word order. Additionally, alternatives to the Dirichlet distribution can be used to extend the model to allow for correlation. For instance, in Hansard topics related the army may be expected to be more commonly found with topics related to the navy, but less commonly with topics related to banking. 17.4 Word embedding 17.5 Conclusion Using text as data is exciting because of the quantity and variety of text that is available to us. In general, dealing with text datasets is messy. There is a lot of cleaning and preparation that is typically required. Often text datasets are large. As such, having a workflow in place, in which you work in a reproducible way, simulating data first, and then clearly communicating your findings becomes critical, if only to keep everything organised in your own mind. Nonetheless, it is an exciting area, and I encourage you to regularly use text analysis where possible. In terms of next steps there are two, related, concerns: data and analysis. In terms of data there are many places to get large amounts of text data relatively easily, including: The r package rtweets makes it easy to get Twitter data (although typically this is going to be looking forward from when you start using it, rather than being able to look back). Plenty of people at U of T work with Twitter data including Jia Xue in the iSchool, and Ludovic Rheault in political science. The inside Airbnb dataset that we used earlier provides text from reviews. We’ve seen the gutenbergr package already in these notes, which provides easy access to text from Project Gutenberg. We’ve seen scraping of Wikipedia, but if you are going to do a bit of this then you may find it better to use a package, for instance WikipediR. In terms of analysis: Start by going through the tidytext book, tidytext, as it has a lot of nice explanations, code, and examples. It would then be worthwhile working through the Quanteda package quanteda tutorials. Finally, consider packages such as text2vec, and spacyr. The Dirichlet distribution is a variation of the beta distribution that is commonly used as a prior for categorical and multinomial variables. If there are just two categories, then the Dirichlet and the beta distributions are the same. In the special case of a symmetric Dirichlet distribution, \\(\\eta=1\\), it is equivalent to a uniform distribution. If \\(\\eta&lt;1\\), then the distribution is sparse and concentrated on a smaller number of the values, and this number decreases as \\(\\eta\\) decreases. A hyperparameter is a parameter of a prior distribution.↩︎ "],["cloud.html", "Chapter 18 Cloud 18.1 Introduction 18.2 Google Colab 18.3 AWS 18.4 Google Compute Engine 18.5 Azure", " Chapter 18 Cloud Last updated: 30 January 2021. Required reading Recommended reading Edmondson, Mark, 2020, ‘googleComputeEngineR documentation,’ version 0.3.0.9000, freely available at: https://cloudyr.github.io/googleComputeEngineR/. McDermott, Grant R., 2020, ‘Cloud computing with Google Compute Engine,’ Data Science for Economists, freely available at: https://raw.githack.com/uo-ec607/lectures/master/14-gce/14-gce.html. Morris, Mitzi, 2020, ‘Stan Notebooks in the Cloud,’ freely available at: https://mc-stan.org/users/documentation/case-studies/jupyter_colab_notebooks_2020.html. Key concepts/skills/etc Benefits/costs of cloud. Getting started in the cloud. Starting virtual machines with R Studio. Stopping virtual machines. 18.1 Introduction Cloud benefits: - Costs can be reduced, or more easily amortized. - Can scale as you need. - Many platforms are already sorted out e.g. R Studio just works. I stole this from someone and I can’t remember who, but the cloud is another name for ‘someone else’s computer.’ That’s it. Nonetheless, learning to use someone else’s computer can be great for a number of reasons including: Scalability: It can be quite expensive to buy a new computer, especially if you only need it to run something every now and then, but by using someone else’s computer, you can just rent for a few hours or days. Portability: If you can shift your analysis workflow from your laptop to the cloud, then that suggests that you are likely doing good things in terms of reproducibility and portability. At the very least, your code is capable of running on your laptop and the cloud. Set-and-forget: If you are doing something that will take a while, then it can be great to not have to worry about your laptop’s fan running overnight, or your partner/baby/pet/housemate/etc accidently closing your computer, or not being able to watch Netflix on that same computer. When you use the cloud you are running your code on a ‘virtual machine.’ This is a part of a larger bunch of computers that has been designed to act like a computer with specific features. For instance you may specify that your virtual machine has 8 GB RAM, 128 storage, and 4 CPUs. Your VM would then act like a computer with those specifications. The cost to use cloud options increases based on the specifications of the virtual machine that you choose. There are a few downsides: Cost: While most cloud options are cheap, they are rarely free. (While there are free options, they tend to not be very powerful, and so you end up having to pay to get a computer that is better than your laptop.) To give you an idea of cost, when I use AWS, I typically end up spending five to ten dollars for a couple of days. So it’s fairly cheap, but it’s not nothing. It’s also pretty easy to accidently forget about something and run up an unexpected bill, especially initially. Public: It is pretty easy to make mistakes and accidently make everything public. Time: It takes time to get set-up and comfortable on the cloud. In these notes we are going to introduce the cloud starting with some options that pretty much anyone can (and should) take advantage of: Google Colab; and then moving to more general cloud options including Google Compute Engine, AWS, and Azure, which may be useful to some of you in some cases. If you want to get a job in industry, then the advice of pretty much every speaker from industry at the Toronto Data Workshop is that you learn at least one of those cloud options. For instance, Munich Re is an Azure shop, Receptiviti uses AWS, etc. 18.2 Google Colab Google Colab is similar to R Studio Cloud, in that it is set-up to allow you to just log in and get started. In this case, you need a Google account. It’s better than R Studio because they have more resources to put into its development and you can use GPUs, but but on the other hand it is designed for Python, and while we can use it for R, it’s not really focused on that. To get started you need to tell Google Colab that you want to use R. You can do this by using this: https://colab.research.google.com/notebook#create=true&amp;language=r. At this point you have a Jupyter notebook open that will run R. (But it is not a R Markdown document.) You can install packages as normal, e.g. install.packages(\"tidyverse\"), and then call the package e.g. library(tidyverse). Google Colab is a good option if you have a good reason for using the broader capabilities that it has. If you want to go deeper into that then the Morris reading has a bunch of options that you can explore, but as Morris puts it ‘Colab is a gateway drug - for large-scale processing pipelines you’ll need to move up to Google Cloud Platform or one of its competitors AWS, Azure, etc.’ and that is what we will do now. 18.3 AWS Amazon Web Services is a cloud service from Amazon. To get started you need an AWS Developer account which you can create here: https://aws.amazon.com/developer/. After you have created an account, you need to select a region where the computer that you will access is located. After this, you will want to “Launch a virtual machine” (with EC2). The first step is to choose an Amazon Machine Image (AMI). This provides the details of the computer that you will be using. For instance, your local computer may be a MacBook running Catalina. Helpfully, Louis Aslett provides a bunch of these already set up - http://www.louisaslett.com/RStudio_AMI/. You can either select the code for the region that you registered for, or you can click on the link. The benefit of this AMI is that they are set-up specifically for R Studio, however the trade-off is that they are a little out-dated, as they were compiled in May 2019. In the next step you can choose how powerful the computer will be. The free tier has a fairly basic computer, but you can choose better ones when you need them. At this point you can pretty much just launch the instance. If you start using AWS more seriously then you should look into different security settings. Your instance is now running. You can go to it by pasting the ‘public DNS’ into a browser. The username is ‘rstudio’ and the password is your instance ID. You should have R Studio running, which is exciting. The first thing to do is probably to change the default password using the instructions in the instance. You don’t need to install, say, the tidyverse, instead you can just call the library and keep going. You can see the list of packages that are installed with installed.packages(). For instance, rstan is already installed. And you can use GPUs if you want. Perhaps as important as being able to start an AWS instance is being able to stop it (so that you don’t get billed). The free tier is pretty great, but you do need to turn it off. To stop an instance, in the AWS instances page, select it, then ‘Actions -&gt; Instance State -&gt; Terminate.’ 18.4 Google Compute Engine The main R package related to Google Compute Engine seems to be: googleComputeEngineR. The reading from Grant McDermott is a pretty good walk-through. 18.5 Azure There are a bunch of R packages related to Azure here: https://github.com/Azure/AzureR. "],["deploy.html", "Chapter 19 Deploy 19.1 Introduction 19.2 Packages 19.3 Shiny 19.4 Plumber and model APIs", " Chapter 19 Deploy Last updated: 6 April 2021. Required reading Chip Huyen, 2020, ‘Machine learning is going real-time,’ 27 December, https://huyenchip.com/2020/12/27/real-time-machine-learning.html. Required viewing Blair, James, 2019, ‘Democratizing R with Plumber APIs,’ RStudio Conference, 24 January, https://www.rstudio.com/resources/rstudioconf-2019/democratizing-r-with-plumber-apis/. Nolis, Heather, and Jacqueline Nolis, ‘We’re hitting R a million times a day so we made a talk about it,’ RStudio Conference, 30 January, https://www.rstudio.com/resources/rstudioconf-2020/we-re-hitting-r-a-million-times-a-day-so-we-made-a-talk-about-it/. Recommended reading Key concepts/skills/etc Putting models into production requires a different set of skills to building a model. We need a familiarity with some cloud provider, APIs, and of course modelling. But the biggest difficulty, for me, is getting things set-up. Key libraries plumber shiny Key functions Quiz 19.1 Introduction A key troupe against R is that it’s not for production. I’m not here to convince you one way or another, however in this section we will go through a bunch of different tools that would allow you to do a lot in R if you wanted. The topics that we cover are: SQL databases. Docker. Plumber and APIs for models. Shiny Packages The general idea here is that you need to know the whole workflow. To this point, you’ve been able to scrape some data from a website, bring some order to that chaos, make some charts, appropriately model it, and write this all up. In most academic settings that is more than enough. But in many industry settings we’re going to want to use the model to do something. For instance, set-up a website that allows your model to be used to generate an insurance quote given several inputs. One way to deploy your model is to use Shiny, and we have seen examples of this earlier in the notes. That enables an individual to use your model. But it doesn’t really scale very well. For instance, if we wanted to sell our model forecasts to other businesses, then they might have their own way in which they would like users to interact with the results. The general problem is that we want our model results available to other machines and for that we will want to make an APIs. 19.2 Packages To this point we’ve largely been using R Packages to do things for us. However, another way is to have them loaded DoSS Toolkit 19.3 Shiny 19.4 Plumber and model APIs 19.4.1 Hello Toronto The general idea behind the plumber package (Schloerke and Allen 2021) is that we can train a model and make it available via an API that we can call when we want a forecast. It’s pretty great. Just to get something working, let’s make a function that returns ‘Hello Toronto’ regardless of the output. Open a new R file, add the following, and then save it as ‘plumber.R’ (you may need to install the plumber package if you’ve not done that yet). library(plumber) #* @get /print_toronto print_toronto &lt;- function() { result &lt;- &quot;Hello Toronto&quot; return(result) } After that is saved, in the top right of the editor you should get a button to ‘Run API.’ Click that, and your API should load. It’ll be a ‘Swagger’ application, which provides a GUI around our API. Expand the GET method, and then clik ‘Try it out’ and ‘Execute.’ In the response body, you should get ‘Toronto.’ To more closely reflect the fact that this is an API designed for computers, you can copy/paste the ‘request HTML’ into a browser and it should return ‘Hello Toronto.’ 19.4.2 Local model Now, we’re going to update the API so that it serves a model output, given some input. We’re going to follow Buhr (2017) fairly closely. At this point, I’d recommend starting a new R Project. To get started, let’s simulate some data and then train a model on it. In this case we’re interested in forecasting how long a baby may sleep overnight, given we know how long they slept during their afternoon nap. library(tidyverse) set.seed(853) number_of_observations &lt;- 1000 baby_sleep &lt;- tibble(afternoon_nap_length = rnorm(number_of_observations, 120, 5) %&gt;% abs(), noise = rnorm(number_of_observations, 0, 120), night_sleep_length = afternoon_nap_length * 4 + noise, ) baby_sleep %&gt;% ggplot(aes(x = afternoon_nap_length, y = night_sleep_length)) + geom_point(alpha = 0.5) + labs(x = &quot;Baby&#39;s afternoon nap length (minutes)&quot;, y = &quot;Baby&#39;s overnight sleep length (minutes)&quot;) + theme_classic() Let’s now use tidymodels to quickly make a dodgy model. set.seed(853) library(tidymodels) baby_sleep_split &lt;- rsample::initial_split(baby_sleep, prop = 0.80) baby_sleep_train &lt;- rsample::training(baby_sleep_split) baby_sleep_test &lt;- rsample::testing(baby_sleep_split) model &lt;- parsnip::linear_reg() %&gt;% parsnip::set_engine(engine = &quot;lm&quot;) %&gt;% parsnip::fit(night_sleep_length ~ afternoon_nap_length, data = baby_sleep_train ) write_rds(x = model, file = &quot;baby_sleep.rds&quot;) At this point, we have a model. One difference from what you might be used to is that we’ve saved the model as an ‘.rds’ file. We are going to read that in. Now that we have our model we want to put that into a file that we will use the API to access, again called ‘plumber.R.’ And we also want a file that sets up the API, called ‘server.R.’ So make an R script called ‘server.R’ and add the following content: library(plumber) serve_model &lt;- plumb(&quot;plumber.R&quot;) serve_model$run(port = 8000) Then in ‘plumber.R’ add the following content: library(plumber) library(tidyverse) model &lt;- readRDS(&quot;baby_sleep.rds&quot;) version_number &lt;- &quot;0.0.1&quot; variables &lt;- list( afternoon_nap_length = &quot;A value in minutes, likely between 0 and 240.&quot;, night_sleep_length = &quot;A forecast, in minutes, likely between 0 and 1000.&quot; ) #* @param afternoon_nap_length #* @get /survival predict_sleep &lt;- function(afternoon_nap_length=0) { afternoon_nap_length = as.integer(afternoon_nap_length) payload &lt;- data.frame(afternoon_nap_length=afternoon_nap_length) prediction &lt;- predict(model, payload) result &lt;- list( input = list(payload), response = list(&quot;estimated_night_sleep&quot; = prediction), status = 200, model_version = version_number) return(result) } Again, after you save the ‘plumber.R’ file you should have an option to ‘Run API.’ Click that and you can try out the API locally in the same way as before. 19.4.3 Cloud model To this point, we’ve got an API working on our own machine, but what we really want to do is to get it working on a computer such that the API can be accessed by anyone. To do this we are going to use DigitalOcean - https://www.digitalocean.com. It is a charged service, but when you create an account, it will come with $100 in credit, which will be enough to get started. This set-up process will be a pain and take some time, but you only need to do it once. Install two additional packages that will assist here: plumberDeploy (J. Allen 2021). analogsea (Chamberlain, Wickham, and Chang 2021). install.packages(&quot;plumberDeploy&quot;) remotes::install_github(&quot;sckott/analogsea&quot;) Now we need to connect your local computer with your DigitalOcean account. Get started with: analogsea::account() Now you need to authenticate the connection and this is done using a SSH public key. You can do this using: analogsea::key_create() But if this is your first time doing this then it may be more useful to have a visual process, in which case follow the instructions here: https://docs.digitalocean.com/products/droplets/how-to/add-ssh-keys/to-account/. What you want is to have a ‘.pub’ file on your computer. Then copy the public key aspect in that file, and add it to the SSH keys section in the account security settings. When you have the key on your local computer then you can check this using: ssh::ssh_key_info() Again, this will all take a while to validate. DigitalOcean calls every computer that you start a ‘droplet.’ So if you start three computers, then you’ll have started three droplets. You can check the droplets that you have running using: analogsea::droplets() If everything is set-up properly, then this will print the information about all droplets that you have associated with your account (which at this point, is probably none). To create a droplet, you run: id &lt;- plumberDeploy::do_provision(example = FALSE) Then you’ll get asked for your SSH passphrase and then it’ll just set-up a bunch of things. After this we’re going to need to install a whole bunch of things onto our droplet: analogsea::install_r_package(droplet = id, c(&quot;plumber&quot;, &quot;remotes&quot;, &quot;here&quot;)) analogsea::debian_apt_get_install(id, &quot;libssl-dev&quot;, &quot;libsodium-dev&quot;, &quot;libcurl4-openssl-dev&quot;) analogsea::debian_apt_get_install(id, &quot;libxml2-dev&quot;) analogsea::install_r_package(id, c(&quot;config&quot;, &quot;httr&quot;, &quot;urltools&quot;, &quot;plumber&quot;)) analogsea::install_r_package(id, c(&quot;xml2&quot;)) analogsea::install_r_package(id, c(&quot;tidyverse&quot;)) analogsea::install_r_package(id, c(&quot;tidymodels&quot;)) And then when that is finally set-up (it’ll seriously take 30 min or so) we can deploy our API! plumberDeploy::do_deploy_api(droplet = id, path = &quot;example&quot;, localPath = getwd(), port = 8000, docs = TRUE, overwrite=TRUE) "],["papers.html", "Chapter 20 Papers 20.1 ‘Mandatory Minimums’ 20.2 ‘These numbers mean dial it up’ 20.3 ‘The Short List’ 20.4 ‘Two Cathedrals’ 20.5 ‘A Proportional Response’ 20.6 ‘Mr Willis of Ohio’ 20.7 ‘Five Votes Down’ 20.8 ‘What’s next?’", " Chapter 20 Papers Last updated: 28 April 2021. 20.1 ‘Mandatory Minimums’ 20.1.1 Task Working individually and in an entirely reproducible way, please find a dataset of interest on Open Data Toronto and write a short paper telling a story about the data. 20.1.2 Guidance Find a dataset of interest on Open Data Toronto and download it in a reproducible way using the R package opendatatoronto (Gelfand 2020). Create a folder with appropriate sub-folders, add it to GitHub, and then prepare a PDF using R Markdown with these sections (you are welcome to use this starter folder: https://github.com/RohanAlexander/starter_folder): title, author, date, abstract, introduction, data, and references. In the data section thoroughly and precisely discuss the source of the data and the bias this brings (ethical, statistical, and otherwise). Comprehensively describe and summarize the data using text and at least one graph and one table. Graphs must be made in ggplot (Wickham 2016) and tables must be made using knitr::kable() (with or without kableExtra) or gt (Iannone, Cheng, and Schloerke 2020b). Make sure to cross-reference graphs and tables. Use bibtex to add references. Be sure to reference R and any R packages you use, as well as the dataset. Check that you have referenced everything. Strong submissions will draw on related literature and would be sure to also reference those. There are various options in R Markdown for references style; just pick one that you are used to. Go back and write an introduction. This should be two or three paragraphs. The last paragraph should set out the remainder of the paper. Add an abstract. This should be three or four sentences. And then add a descriptive title (Hint: ‘Paper 1’ is not descriptive.) Add a link to your GitHub repo via a footnote. Check that your GitHub repo is well-organized, and add an informative README. (Hint: Comment. Your. Code.). Make sure that you’ve got at least one R script in there, in addition, to your R Markdown file. Pull this all together as a PDF and check that the paper is well-written and able to be understood by the average reader of, say, FiveThirtyEight. This means that you are allowed to use mathematical notation, but you must explain all of it in plain language. All statistical concepts and terminology must be explained. Your reader is someone with a university education, but not necessarily someone who understands what a p-value is - explain everything that you use. Check there is no evidence that this is a class assignment. Via Quercus, submit the PDF. 20.1.3 Check offs points Check you’ve not included any R code or raw R output in the final PDF. Check that although you’ll probably have most of your code in the R Markdown, make sure that you have at least one R script in the scripts folder. Check there is thoroughly commented code that directly creates your PDF. Do not knit to html and then save as a PDF. Do not knit to Word and then save as a PDF Check that your graph and discussion are extremely clear, and of comparable quality to those of FiveThirtyEight. Check that the date is updated. Check your entire workflow is entirely reproducible. Check for typos. 20.1.4 FAQ Can I use a dataset from Kaggle instead? No, because too many people use Kaggle datasets so employers are sick of them. I can’t use code to download my dataset, can I just manually download it? No, because your entire workflow needs to be reproducible. Please fix the download problem or pick a different dataset. How much should I write? Most students submit something in the two-to-six-page range, but it’s really up to you. Be precise and thorough. My data is about apartment blocks/NBA/League of Legends so there’s no ethical or bias aspect, what do I do? Please re-read the readings to better understand bias and ethics. If you really can’t think of something, then it might be worth picking a different dataset. Can I use Python? No. If you already know Python then it doesn’t hurt to learn another language. Why do I need to cite R, when I don’t need to cite Word? R is a free statistical programming language with academic origins so it’s appropriate to acknowledge the work of others. It’s also important for reproducibility. 20.1.5 Rubric Go/no-go #1: R is cited - [1 ‘Yes,’ 0 ‘No’] Both referred to in the main content and included in the reference list. If not, no need to continue marking, just give paper 0 overall. Title - [2 ‘Exceptional,’ 1 ‘Yes,’ 0 ‘Poor or not done’] An informative title is included. Tell the reader what your story is - don’t waste their time. Ideally tell them what happens at the end of the story. ‘Problem Set X’ is not an informative title. There should be no evidence this is a school paper. Author, date, and repo - [2 ‘Yes,’ 0 ‘Poor or not done’] The author, date of submission, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: ‘Code and data supporting this analysis is available at: LINK’). Abstract - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor or not done’] An abstract is included and appropriately pitched to a general audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). If your abstract is longer than four sentences then you need to think a lot about whether it is too long. It may be fine (there are always exceptions) but you should probably have a good reason. Your abstract must tell the reader your top-level finding. What is the one thing that we learn about the world because of your paper? Introduction - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor or not done’] The introduction is self-contained and tells a reader everything they need to know, including putting it into a broader context. Your introduction should provide a bit of broader context to motivate the reader, as well as providing a bit more detail about what you’re interested in, what you did, what you found, why it’s important, etc. A reader should be able to read only your introduction and have a good idea about the research that you carried out and what you found. It would be rare that you would have tables or figures in your introduction (again there are always exceptions but think deeply about whether yours is one). It must outline the structure of the paper. For instance (and this is just a rough guide) an introduction for a 10 page paper, should probably be about 3 or 4 paragraphs, or 10 per cent, but it depends on specifics. Data - [10 ‘Exceptional,’ 8 ‘Great,’ 6 ‘Good,’ 4 ‘Some issues,’ 2 ‘Many issues,’ 0 ‘Poor or not done’] When you discuss the dataset (in the data section) you should make sure to discuss at least: The source of the data. The methodology and approach that is used to collect and process the data. The population, the frame, and the sample (as appropriate). Information about how respondents were found. What happened to non-response? What are its key features, strengths, and weaknesses about the survey generally. You should thoroughly discuss the variables in the dataset that you use. Are there any that are very similar that you nonetheless don’t use? Did you construct any variables by combining various ones? What do the data look like? Plot the actual data that you’re using (or as close as you can get to it). Discuss these plots and the other features of these data. These are just some of the issues strong submissions will consider. Show off your knowledge. If this becomes too detailed, then you should push some of this to footnotes or an appendix. ‘Exceptional’ means that when I read your submission I learn something about the dataset that I don’t learn from any other submission (within a reasonable measure of course). Numbering - [2 ‘Yes,’ 0 ‘Poor or not done’] All figures, tables, equations, etc are numbered and referred to in the text. Proofreading - [2 ‘Yes,’ 0 ‘Poor or not done’] All aspects of submission are free of noticeable typos. Graphs/tables/etc - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor or not done’] You must include graphs and tables in your paper and they must be to a high standard. They must be well formatted and camera-ready They should be clear and digestible. They must: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of labels/explanations, etc; and 3) appropriately sized and coloured (or appropriate significant figures in the case of stats). References - [4 ‘Perfect,’ 3 ‘One minor issue,’ 0 ‘Poor or not done’] All data/software/literature/etc are appropriately noted and cited. You must cite the software and software packages that you use. You must cite the datasets that you use. You must cite literature that you refer to (and you should refer to literature). If you take a small chunk of code from Stack Overflow then add the page in a comment next to the code. If you take a large chunk of code then cite it fully. 3 means one minor issue. More than one minor issue receives 0. Reproducibility - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor or not done’] The paper and analysis must be fully reproducible. A detailed README is included. All code should be thoroughly documented. An R project is used. Do not use setwd(). The code must appropriately read data, prepare it, create plots, conduct analysis, and generate documents. Seeds are used where needed. Code must have a preamble etc. You must appropriately document your scripts such that someone coming in could follow them. Your repo must be thoroughly organized. General excellence - [3 ‘Exceptional,’ 2 ‘Wow,’ 1 ‘Huh, that’s interesting,’ 0 ‘None’] There are always students that excel in a way that is not anticipated in the rubric. This item accounts for that. 20.1.6 Previous examples Some examples of papers that well in the past include those by: Amy Farrow, Morgaine Westin, and Rachel Lam. 20.2 ‘These numbers mean dial it up’ 20.2.1 Task Please consider this scenario: You are employed as a junior data scientist at Petit Poll - a Canadian polling company. Petit Poll has a contract with a ‘client’ - an Ontario government department - to provide them with advice. In particular, the client wants to understand the effect of COVID shut-downs on restaurant businesses and has asked Petit Poll to design an experiment about some aspect of this. Working as part of a small team of 1-3 people, and in an entirely reproducible way, please decide on an intervention, and some measurement strategies, and then write a short paper telling a story about the effect. 20.2.2 Guidance Working as part of a team of 1-3 people, prepare a PDF in R Markdown with the following features (you are welcome to use this starter folder: https://github.com/RohanAlexander/starter_folder): title, author/s, date, abstract, introduction, data, discussion, appendix with survey details, and references. Decide on an intervention. Some aspects to address include: How will it be designed and implemented? What will be random about it? How will you ensure the separation of treatment and non-treatment? How long will it run? You’ll need to run surveys to gather information about your intervention. Decide on a survey methodology. Some aspects to address include: What is the population, frame, and sample? What sampling methods will you use and why? What are some of the statistical properties that the method brings to the table? How are you going to reach your desired respondents? How much do you estimate this will cost? What steps will you take to deal with non-response and how will non-response affect your survey? How are you going to protect respondent privacy? Remember to consider all of this in the context of your ‘client’ - for instance, what are they interested in? Develop a survey on a platform that was introduced in class, or another that you’re familiar with. Be sure to test it yourselves. You will want to test this as much as possible, maybe even swap informally with another group? Now release the surveys into the (simulated) ‘field.’ Please do this by simulating an appropriate number of responses to your survey in R. Don’t forget to simulate in relation to the intervention that you proposed. Do you need two, or even more, surveys (and hence multiple sets of simulated results)? Show the results and discuss your ‘findings.’ Everything must be entirely reproducible. You may wish to scrape some data and/or use open data sources to appropriately parameterize your simulations. Don’t forget to cite them when you do this. Use R Markdown to write a PDF report about all of this. Discuss your intervention, results and findings, your survey design and motivations, etc - all of it. You are writing a report that will eventually go to the client, so you must set the scene, and use language that demonstrates your command of statistical concepts but brings the reader along with you. Be sure to include graphs and tables and reference them in your discussion. Be sure to be clear about weaknesses and biases, and opportunities for future work. Your report must be well written. You are allowed to, and should, use mathematical notation and statistical concepts, but you must explain all of it in plain language. Similarly, you can, and should, use experimental/survey/sampling/observational data terminology, but again, you need to explain it. In the data section you should specify the intervention and data gathering methodology. You should also show your data, with tables and graphs as necessary. In addition to summaries, be sure to plot your raw data to the extent possible. Your graphs and tables must be of an incredibly high standard. Graphs and tables should be well formatted and report-ready. They should be clean and digestible. Furthermore, you should label and describe each table/figure/equation. In the discussion section and any other relevant section, please be sure to discuss ethics and bias with reference to relevant literature. For instance, think about who is and who is not in your dataset. What are the statistical and ethical implications of this? Often folks struggle with the Discussion section of reports. For a 10 page report, we’re looking for about 2-3 pages of content. Here’s an example of sub-sections that you could include: A sub-section containing a brief overview of the paper, and also how it fits into the literature and improves existing contributions. Three sub-sections that detail the three main points that we learn about the world from the paper. A sub-section on limitations, but discussed in a sophisticated way. So this means not just stating them, but explaining and justifying. A sub-section on how it could be extended and future directions. Your client has stats graduates working for it who need to be impressed by the main content of the report, but also has people who barely know what an average is and these people need to be impressed also. Check that you have referenced everything, including R, R packages, and datasets. Strong submissions will draw on related literature and would be sure to also reference those. The style of references does not matter - there are various options in R Markdown - provided it is consistent and that bibtex has been used. Via Quercus, submit your PDF report. You must provide a link to the GitHub repo where the code that you used for this assignment lives. Comment. Your. Code. Your entire workflow must be entirely reproducible. Your repo should be clearly organised, and a useful README included. You must include a separate R script that accomplishes something (probably the simulations makes sense). And you must include the R Markdown file that produced the PDF in that repo. Please be sure to include a link to your survey/s in your report and screenshots of the survey/s in the appendix of your report. Everyone in the team receives the same mark. There should be no evidence that this is a class assignment. 20.2.3 Check offs points Check you’ve not included any R code or raw R output in the final PDF. Check you’ve cited R and any R packages used. Check that although you’ll probably have most of your code in the R Markdown, make sure that you have at least one R script in the scripts folder. Check there is thoroughly commented code that directly creates your PDF. Do not knit to html and then save as a PDF. Do not knit to Word and then save as a PDF Check that your graph and discussion are extremely clear, and of comparable quality to those of FiveThirtyEight. Check that the date is updated to the date of submission. Check your entire workflow is entirely reproducible. Check for typos. Check that you’ve got an appendix that details the survey/s and a link to the live survey. 20.2.4 FAQ Can I work by myself? Yes. But I recommend forming a group and the workload for the course assumes you’ll work on the second and third paper as part of a group of four. Can we switch groups for the third paper? Yes. How can I find a group? I will randomly create groups of four in Quercus. You are welcome to shift out of those groups and form your own groups if you’d like. Can I get a different mark to the rest of my group? No. Everyone in the group gets the same mark. I wrote my paper by myself, so can I be graded on a different scale? No. All papers are graded in the same way. How much should I write? Most students submit something in the 10-to-15-page range, but it’s really up to you. Be precise and thorough. How do students collaborate successfully? Groups that split up the work typically seem to do the best. So one student worries about the survey, one about simulating and analysing data, and another about the write-up. If you’re worried about using GitHub to collaborate, then just create different folders in GitHub to place your separate bits of work, and then have one person bring it together at the end. What intervention should we use? The intention is that you do something of interest to you. A well-written introduction would make the intervention clear. 20.2.5 Rubric Go/no-go #1: R is cited - [1 ‘Yes,’ 0 ‘No’] Both referred to in the main content and included in the reference list. If not, no need to continue marking, just give paper 0 overall. Title - [2 ‘Exceptional,’ 1 ‘Yes,’ 0 ‘Poor or not done’] An informative title is included. Tell the reader what your story is - don’t waste their time. Ideally tell them what happens at the end of the story. ‘Problem Set X’ is not an informative title. There should be no evidence this is a school paper. Author, date, and repo - [2 ‘Yes,’ 0 ‘Poor or not done’] The author, date of submission, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: ‘Code and data supporting this analysis is available at: LINK’). Abstract - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor or not done’] An abstract is included and appropriately pitched to a general audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). If your abstract is longer than four sentences then you need to think a lot about whether it is too long. It may be fine (there are always exceptions) but you should probably have a good reason. Your abstract must tell the reader your top-level finding. What is the one thing that we learn about the world because of your paper? Introduction - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor or not done’] The introduction is self-contained and tells a reader everything they need to know, including putting it into a broader context. Your introduction should provide a bit of broader context to motivate the reader, as well as providing a bit more detail about what you’re interested in, what you did, what you found, why it’s important, etc. A reader should be able to read only your introduction and have a good idea about the research that you carried out. It would be rare that you would have tables or figures in your introduction (again there are always exceptions but think deeply about whether yours is one). It must outline the structure of the paper. For instance (and this is just a rough guide) an introduction for a 10 page paper, should probably be about 3 or 4 paragraphs, or 10 per cent, but it depends on specifics. Data - [10 ‘Exceptional,’ 8 ‘Great,’ 6 ‘Good,’ 4 ‘Some issues,’ 2 ‘Many issues,’ 0 ‘Poor or not done’] The survey has a clear, detailed, and justified methodology that has been thoroughly discussed. The statistical basis for the approach that is used is clear. It would achieve the aims of the report. Population, frame, sample, and other key aspects are described. There is a detailed plan for reaching respondents. Cost is discussed and appropriate. Non-response is discussed and a plan for dealing with it clearly articulated. There is clear respect for the survey respondents. When you discuss the dataset (that you’ve likely largely simulated) you should make sure to discuss at least: The source of the data. The methodology and approach that is used to collect and process the data. The population, the frame, and the sample (as appropriate). Information about how respondents were found. What happened to non-response? What are its key features, strengths, and weaknesses about the survey generally. You should thoroughly discuss the variables in the dataset that you use. Are there any that are very similar that you nonetheless don’t use? Did you construct any variables by combining various ones? What do the data look like? Plot the actual data that you’re using (or as close as you can get to it). Discuss these plots and the other features of these data. These are just some of the issues strong submissions will consider. Show off your knowledge. If this becomes too detailed, then you should push some of this to footnotes or an appendix. ‘Exceptional’ means that when I read your submission I learn something about the dataset that I don’t learn from any other submission (within a reasonable measure of course). Discussion - [10 ‘Exceptional,’ 8 ‘Great,’ 6 ‘Good,’ 4 ‘Some issues,’ 2 ‘Many issues,’ 0 ‘Poor or not done’] Some questions that a good discussion would cover include: What is done in this this paper? What are the main points that we learn about the world? What are some weaknesses of what was done? What is left to learn? Details of the survey - [6 ‘Exceptional,’ 5 ‘Great,’ 4 ‘Good,’ 3 ‘Some issues,’ 2 ‘Many issues,’ 0 ‘Poor or not done’] A working link to survey is included in the appendix. Screenshots or at least the survey questions are included in the appendix. The survey has been well put together. The number and length of the questions is appropriate. The question type and potential answers are appropriate. It is clear how this survey could accomplish the aims of the report. The questions flow in an appropriate way. The simulation of survey responses is appropriate to the survey and the scenario - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor or not done’] It has been done in a reproducible way and either contained in a separate R file, or is in the report appendix. Numbering - [2 ‘Yes,’ 0 ‘Poor or not done’] All figures, tables, equations, etc are numbered and referred to in the text. Proofreading - [2 ‘Yes,’ 0 ‘Poor or not done’] All aspects of submission are free of noticeable typos. Graphs/tables/etc - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor or not done’] You must include graphs and tables in your paper and they must be to a high standard. They must be well formatted and camera-ready They should be clear and digestible. They must: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of labels/explanations, etc; and 3) appropriately sized and coloured (or appropriate significant figures in the case of stats). References - [4 ‘Perfect,’ 3 ‘One minor issue,’ 0 ‘Poor or not done’] All data/software/literature/etc are appropriately noted and cited. You must cite the software and software packages that you use. You must cite the datasets that you use. You must cite literature that you refer to (and you should refer to literature). If you take a small chunk of code from Stack Overflow then add the page in a comment next to the code. If you take a large chunk of code then cite it fully. 3 means one minor issue. More than one minor issue receives 0. Reproducibility - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor or not done’] The paper and analysis must be fully reproducible. A detailed README is included. All code should be thoroughly documented. An R project is used. Do not use setwd(). The code must appropriately read data, prepare it, create plots, conduct analysis, generate documents, etc. Seeds are used where needed. Code must have a preamble etc. You must appropriately document your scripts such that someone coming in could follow them. Your repo must be thoroughly organized and not contain extraneous files. General excellence - [3 ‘Exceptional,’ 2 ‘Wow,’ 1 ‘Huh, that’s interesting,’ 0 ‘None’] There are always students that excel in a way that is not anticipated in the rubric. This item accounts for that. 20.3 ‘The Short List’ 20.3.1 Task Working as part of a small team of 1-3 people, and in an entirely reproducible way, please pick a paper to reproduce from an approved list and then write a short paper telling a story based on this. Your story should both talk about the (reproduced) findings, but also (a bit more ‘meta’) about what you learnt from the process. 20.3.2 Guidance Working as part of a team of 1-3 people, prepare a PDF in R Markdown with the following features: title, author/s, date, abstract, introduction, data, model, results, discussion, and references. In the discussion section and any other relevant section, please be sure to discuss ethics and bias with reference to relevant literature. You should reproduce one of the following papers: Barari, Soubhik, Christopher Lucas, and Kevin Munger, 2021, ‘Political Deepfake Videos Misinform the Public, But No More than Other Fake Media,’ 13 January, https://osf.io/cdfh3/. Cohn, Alain, Michel André Maréchal, David Tannenbaum, and Christian Lukas Zünd, 2019, ‘Civic Honesty Around the Globe.’ Liran Einav, Amy Finkelstein, Tamar Oostrom, Abigail Ostriker, Heidi Williams, 2020, ‘Screening and Selection: The Case of Mammograms,’ American Economic Review. Pons, Vincent, 2018, ‘Will a Five-Minute Discussion Change Your Mind? A Countrywide Experiment on Voter Choice in France’ American Economic Review. Abramitzky, Ran, Leah Boustan, Katherine Eriksson, and Stephanie Hao. “Discrimination and the Returns to Cultural Assimilation in the Age of Mass Migration.” AEA Papers and Proceedings 110 (May 2020): 340–46. https://doi.org/10.1257/pandp.20201090. Chen, Yan, Ming Jiang, and Erin L. Krupka. “Hunger and the Gender Gap.” Experimental Economics 22, no. 4 (December 2019): 885–917. https://doi.org/10.1007/s10683-018-9589-9. Lise, Jeremy, and Fabien Postel-Vinay. “Multidimensional Skills, Sorting, and Human Capital Accumulation.” American Economic Review 110, no. 8 (August 2020): 2328–76. https://doi.org/10.1257/aer.20162002. Kolev, Julian, Yuly Fuentes-Medel, and Fiona Murray. “Gender Differences in Scientific Communication and Their Impact on Grant Funding Decisions.” AEA Papers and Proceedings 110 (May 2020): 245–49. https://doi.org/10.1257/pandp.20201043. Cowgill, Bo, Fabrizio Dell’Acqua, and Sandra Matz. “The Managerial Effects of Algorithmic Fairness Activism.” AEA Papers and Proceedings 110 (May 2020): 85–90. https://doi.org/10.1257/pandp.20201035. You should follow the lead of the author/s of the paper you’re reproducing, but thoroughly think about, and discuss, what is being done. Regardless of the particular model that you are using, and the (possibly lack of) extent to which this is done in the paper, your model must be well explained, thoroughly justified, explained as appropriate to the task at hand, and the results must be beautifully described. You must include a DAG (probably in the model section). You must have a discussion of power and experimental design (probably in the data section) Your paper must be well-written, draw on relevant literature, and show your statistical skills by explaining all statistical concepts that you draw on. You are welcome to use appendices for supporting, but not critical, material. Your discussion must include sub-sections that focus on three or four interesting points, and also sub-sections on weaknesses and next steps. In your report you must provide a link to a GitHub repo that fully contains your analysis. Your code must be entirely reproducible, documented, and readable. Your repo must be well-organised and appropriately use folders. Your graphs and tables must be of an incredibly high standard. Graphs and tables should be well formatted and report-ready. They should be clean and digestible. Furthermore, you should label and describe each table/figure. When you discuss the dataset (in the data section) you should make sure to discuss (at least): Its key features, strengths, and weaknesses generally. A discussion of the questionnaire - what is good and bad about it? A discussion of the methodology including how they find people to take the survey; what their population, frame, and sample were; what sampling approach they took and what some of the trade-offs may be; what they do about non-response; the cost. A discussion of the intervention and experimental design. These are just some of the issues strong submissions will consider. Show off your knowledge. If this becomes too detailed then you should push some of this to footnotes or an appendix. When you discuss your model (in the model section), you must be extremely careful to spell out the statistical model that you are using, defining and explaining each aspect and why it is important. (For a Bayesian model, a discussion of priors and regularization is almost always important.) You should mention the software that you used to run the model. You should be clear about model convergence, model checks, and diagnostic issues. How do the sampling and survey aspects that you discussed assert themselves in the modelling decisions that you make? Again, if it becomes too detailed then push some of the details to footnotes or an appendix. You have the original paper to guide you, but you’ll likely need to go well-beyond what is included. You should present model results, graphs, figures, etc, in the results section. This section should strictly relay results. Interpretation of these results and conclusions drawn from the results should be left for the discussion section. Your discussion should focus on your model results. Interpret them and explain what they mean. Put them in context. What do we learn about the world having understood your model and its results? What caveats could apply? To what extent does your model represent the small world and the large world (to use the language of McElreath, Ch 2)? What are some weaknesses and opportunities for future work? Additionally, as this is a reproduction you should include a sub-section on differences you found and difficulties that you had. Check that you have referenced everything. Strong submissions will draw on related literature in the discussion (and other sections) and would be sure to also reference those. The style of references does not matter, provided it is consistent. As a team, via Quercus, submit a PDF of your paper. Again, in your paper you must have a link to the associated GitHub repo. And you must include the R Markdown file that produced the PDF in that repo. And you must include the R Markdown file that produced the PDF in that repo. The repo must be well-organized and have a detailed README. A good way to work as a team would be to split up the work, so that one person is doing each section. The people doing the sections that rely on data (such as the analysis and the graphs) could just simulate it while they are waiting for the person putting together the data to finish. It is expected that your submission be well written and able to be understood by the average reader of say 538. This means that you are allowed to use mathematical notation, but you must be able to explain it all in plain English. Similarly, you can (and hint: you should) use survey, sampling, observational, and statistical terminology, but again you need to explain it. Your work should have flow and should be easy to follow and understand. To communicate well, anyone at the university level should be able to read your report once and relay back the methodology, overall results, findings, weaknesses and next steps without confusion. Everyone in the team receives the same mark. There should be no evidence that this is a class assignment. 20.3.3 Check offs points We have not just copy-pasted the code from the original paper, but instead have used that as a foundation to work from? 20.3.4 FAQ Do I have to stay in the same group as the second paper? No. You’re welcome to change. However, it’s important that you don’t change the second paper group on Quercus - be sure to only change the third paper group. Can we switch groups for the third paper? Yes. How much should I write? Most students submit something in the 10-to-15-page range, but it’s really up to you. Be precise and thorough. My paper doesn’t have a DAG, what do I do? You need to make the DAG. 20.3.5 Rubric Go/no-go #1: R is cited - [1 ‘Yes,’ 0 ‘No’] Both referred to in the main content and included in the reference list. If not, no need to continue marking, just give paper 0 overall. Title - [2 ‘Exceptional,’ 1 ‘Yes,’ 0 ‘Poor or not done’] An informative title is included. Tell the reader what your story is - don’t waste their time. Ideally tell them what happens at the end of the story. ‘Problem Set X’ is not an informative title. There should be no evidence this is a school paper. Author, date, and repo - [2 ‘Yes,’ 0 ‘Poor or not done’] The author, date of submission, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: ‘Code and data supporting this analysis is available at: LINK’). Abstract - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor or not done’] An abstract is included and appropriately pitched to a general audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). If your abstract is longer than four sentences then you need to think a lot about whether it is too long. It may be fine (there are always exceptions) but you should probably have a good reason. Your abstract must tell the reader your top-level finding. What is the one thing that we learn about the world because of your paper? Introduction - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor or not done’] The introduction is self-contained and tells a reader everything they need to know, including putting it into a broader context. Your introduction should provide a bit of broader context to motivate the reader, as well as providing a bit more detail about what you’re interested in, what you did, what you found, why it’s important, etc. A reader should be able to read only your introduction and have a good idea about the research that you carried out. It would be rare that you would have tables or figures in your introduction (again there are always exceptions but think deeply about whether yours is one). It must outline the structure of the paper. For instance (and this is just a rough guide) an introduction for a 10 page paper, should probably be about 3 or 4 paragraphs, or 10 per cent, but it depends on specifics. Data - [10 ‘Exceptional,’ 8 ‘Great,’ 6 ‘Good,’ 4 ‘Some issues,’ 2 ‘Many issues,’ 0 ‘Poor or not done’] You should thoroughly discuss the variables in the dataset that you use. Are there any that are very similar that you nonetheless don’t use? Did you construct any variables by combining various ones? What do the data look like? Plot the actual data that you’re using (or as close as you can get to it). Discuss these plots and the other features of these data. These are just some of the issues strong submissions will consider. Show off your knowledge. If this becomes too detailed, then you should push some of this to footnotes or an appendix. ‘Exceptional’ means that when I read your submission I learn something about the dataset that I don’t learn from any other submission (within a reasonable measure of course). Model - [10 ‘Exceptional,’ 8 ‘Great,’ 6 ‘Good,’ 4 ‘Some issues,’ 2 ‘Many issues,’ 0 ‘Poor or not done’] The model is nicely written out, well-explained, justified, and appropriate. When you discuss your model you must be extremely careful to spell out the statistical model that you are using defining and explaining each aspect and why it is important. Failure to do this suggests you don’t understand the model. The model is appropriately complex - that is, not too simple, but not unnecessarily complicated. The model has well-defined variables and these correspond to what is discussed in the data section. The model needs to be written out in appropriate mathematical notation but also in plain English. Every aspect of that notation must be defined otherwise the most this section can receive is poor. The model makes sense based on the substantive area, and also the form of the model. If the model is Bayesian, then priors need to be defined and sensible. Discussion needs to occur around how features enter the model and why. For instance, (and these are just examples) why use ages rather than age-groups, why does province have a levels effect, why is gender categorical, etc? In general, in order to be adequate, there needs to be a clear justification that this is the model for the situation. The assumptions underpinning the model are clearly discussed. Alternative models, or variants, must be discussed and strengths and weaknesses made clear. Why was this model chosen? You should mention the software that you used to run the model. There is some evidence of thought about the circumstances in which the model may not be appropriate. There is evidence of model validation and checking, whether that is out of sample or comparison to a straw man or RMSE, test/training, or appropriate sensitivity checks. You should be clear about model convergence, model checks, and diagnostic issues, but if this becomes too detailed then you could push some of this to an appendix. Great answers would discuss things such as, how do the aspects that you discussed in the data section assert themselves in the modelling decisions that you make. Again if it becomes too detailed then push some of the details to footnotes or an appendix. Again, explain what your model is and what is going on with it. Results - [10 ‘Exceptional,’ 8 ‘Great,’ 6 ‘Good,’ 4 ‘Some issues,’ 2 ‘Many issues,’ 0 ‘Poor or not done’] Results will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps. To be clear, you should also have text associated with all these aspects. Show the reader the results by plotting them. Talk about them. Explain them. That said, this section should strictly relay results. Discussion - [10 ‘Exceptional,’ 8 ‘Great,’ 6 ‘Good,’ 4 ‘Some issues,’ 2 ‘Many issues,’ 0 ‘Poor or not done’] Some questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page): What is done in this this paper? What is something that we learn about the world? What is another thing that we learn about the world? What are some weaknesses of what was done? What is left to learn or how should we proceed in the future? Numbering - [2 ‘Yes,’ 0 ‘Poor or not done’] All figures, tables, equations, etc are numbered and referred to in the text. Proofreading - [2 ‘Yes,’ 0 ‘Poor or not done’] All aspects of submission are free of noticeable typos. Graphs/tables/etc - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor or not done’] You must include graphs and tables in your paper and they must be to a high standard. They must be well formatted and camera-ready They should be clear and digestible. They must: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of labels/explanations, etc; and 3) appropriately sized and coloured (or appropriate significant figures in the case of stats). References - [4 ‘Perfect,’ 3 ‘One minor issue,’ 0 ‘Poor or not done’] All data/software/literature/etc are appropriately noted and cited. You must cite the software and software packages that you use. You must cite the datasets that you use. You must cite literature that you refer to (and you should refer to literature). If you take a small chunk of code from Stack Overflow then add the page in a comment next to the code. If you take a large chunk of code then cite it fully. 3 means one minor issue. More than one minor issue receives 0. Reproducibility - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor or not done’] The paper and analysis must be fully reproducible. A detailed README is included. All code should be thoroughly documented. An R project is used. Do not use setwd(). The code must appropriately read data, prepare it, create plots, conduct analysis, generate documents, etc. Seeds are used where needed. Code must have a preamble etc. You must appropriately document your scripts such that someone coming in could follow them. Your repo must be thoroughly organized and not contain extraneous files. General excellence - [3 ‘Exceptional,’ 2 ‘Wow,’ 1 ‘Huh, that’s interesting,’ 0 ‘None’] There are always students that excel in a way that is not anticipated in the rubric. This item accounts for that. 20.4 ‘Two Cathedrals’ 20.4.1 Task Working individually, please conduct original research that applies methods from statistics to a question that involves an experiment. 20.4.2 Guidance You have various options for topics (pick one): Develop a research question that is of interest to you and obtain or create a relevant dataset. This option involves developing your own research question based on your own interests, background, and expertise. I encourage you to take this option, but please discuss your plans with me. How does one come up with ideas? One way is to be question-driven, where you keep an informal log of small ideas, questions, and puzzles, that you have as you’re reading and working. Often, after dwelling on it for a while you can manage to find some questions of interest. Another way is to be data-driven - try to find some interesting dataset and then work backward. Finally, yet another way, is to be methods-driven - let’s say that you happen to understand Gaussian processes, then just apply that expertise. A replication exercise, being sure to use the paper as a foundation rather than as a ends-in-itself. You should know the expectations by now. If you need a refresher then review the past problem sets. But essentially: Everything is entirely reproducible. Your paper must be written in R Markdown. Your paper must have the following sections: Title, date, author, keywords, abstract, introduction, data, model, results, discussion, appendix (optional, for supporting, but not critical, material), and a reference list. Your paper must be well-written, draw on relevant literature, and show your statistical skills by explaining all statistical concepts that you draw on. The discussion needs to be substantial. For instance, if the paper were 10 pages long then a discussion should be at least 2.5 pages. In the discussion, the paper must include subsections on weaknesses and next steps - but these must be in proportion. The report must provide a link to a GitHub repo that contains everything (apart from any raw data that you git ignored if it is not yours to share). The code must be entirely reproducible, documented, and readable. The repo must be well-organised and appropriately use folders and README files. 20.4.3 Peer review submission My expectations for this paper are very high. I’m very excited to read what you submit. To help you achieve this standard, there is an initial ‘submission’ where you can get comments and feedback and then the final, actual, submission. Submit initial materials for peer-review. As an individual, via Quercus, submit a PDF of your rough draft on Quercus. At a minimum this must include: All top-matter (title, author (you can use a pseudonym if you want), date, keywords, abstract) completely filled out. A fully written Introduction section. All other sections must be present in your paper, but don’t have to be filled out (e.g. you must have a ‘Data’ heading, but you don’t need to have content in that section). To be clear - it is fine to later change any aspect of what you submit at this checkpoint. You will be awarded one percentage point just for submitting a draft that meets this minimum. The point of this is to get feedback on your work (and to make sure you have at least started thinking about this project) so you are more than welcome to (and so, if it is at all possible) include other sections that you wish to get feedback on. There will be no extensions granted for this submission since the following submission is dependent on this date. 20.4.4 Conduct peer-review As an individual, you will randomly be assigned a handful of rough drafts to provide feedback. You have three days to provide feedback to your peers. If you provide feedback to one peer you will receive one percentage point, if you provide feedback to two peers you will receive two percentage points, if you provide feedback to three (or more) peers you will receive the full three percentage points. Your feedback must include at least five comments (meaningful/useful bullet points). These must be well-written and thoughtful. There will be no extensions granted for this submission since the following submission is dependent on this date. Please remember that you are providing feedback here to help your colleagues. All comments should be professional and kind. It is challenging to receive criticism. Please remember that your goal here is to help your peers advance their writing/analysis. Any feedback that is inappropriate or not up to standard will receive a 0. 20.4.5 Check offs points Do you have a causal story, or at least a sub-section in the discussion that talks about causality and why you can’t speak to it, or what you would do if you could? 20.4.6 FAQ Can I work as part of a team? No. It’s important that you have some work that is entirely your own. You really need your own work to show off for job applications etc. How much should I write? Most students submit something that has 10-to-16-pages of main content, with additional pages devoted to appendices, but it’s really up to you. Be precise and thorough. 20.4.7 Rubric Go/no-go #1: R is cited - [1 ‘Yes,’ 0 ‘No’] Both referred to in the main content and included in the reference list. If not, no need to continue marking, just give paper 0 overall. Title - [2 ‘Exceptional,’ 1 ‘Yes,’ 0 ‘Poor or not done’] An informative title is included. Tell the reader what your story is - don’t waste their time. Ideally tell them what happens at the end of the story. ‘Problem Set X’ is not an informative title. There should be no evidence this is a school paper. Author, date, and repo - [2 ‘Yes,’ 0 ‘Poor or not done’] The author, date of submission, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: ‘Code and data supporting this analysis is available at: LINK’). Abstract - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor or not done’] An abstract is included and appropriately pitched to a general audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). If your abstract is longer than four sentences then you need to think a lot about whether it is too long. It may be fine (there are always exceptions) but you should probably have a good reason. Your abstract must tell the reader your top-level finding. What is the one thing that we learn about the world because of your paper? Introduction - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor or not done’] The introduction is self-contained and tells a reader everything they need to know, including putting it into a broader context. Your introduction should provide a bit of broader context to motivate the reader, as well as providing a bit more detail about what you’re interested in, what you did, what you found, why it’s important, etc. A reader should be able to read only your introduction and have a good idea about the research that you carried out. It would be rare that you would have tables or figures in your introduction (again there are always exceptions but think deeply about whether yours is one). It must outline the structure of the paper. For instance (and this is just a rough guide) an introduction for a 10 page paper, should probably be about 3 or 4 paragraphs, or 10 per cent, but it depends on specifics. Data - [10 ‘Exceptional,’ 8 ‘Great,’ 6 ‘Good,’ 4 ‘Some issues,’ 2 ‘Many issues,’ 0 ‘Poor or not done’] You should thoroughly discuss the variables in the dataset that you use. Are there any that are very similar that you nonetheless don’t use? Did you construct any variables by combining various ones? What do the data look like? Plot the actual data that you’re using (or as close as you can get to it). Discuss these plots and the other features of these data. These are just some of the issues strong submissions will consider. Show off your knowledge. If this becomes too detailed, then you should push some of this to footnotes or an appendix. ‘Exceptional’ means that when I read your submission I learn something about the dataset that I don’t learn from any other submission (within a reasonable measure of course). Model - [10 ‘Exceptional,’ 8 ‘Great,’ 6 ‘Good,’ 4 ‘Some issues,’ 2 ‘Many issues,’ 0 ‘Poor or not done’] The model is nicely written out, well-explained, justified, and appropriate. When you discuss your model you must be extremely careful to spell out the statistical model that you are using defining and explaining each aspect and why it is important. Failure to do this suggests you don’t understand the model. The model is appropriately complex - that is, not too simple, but not unnecessarily complicated. The model has well-defined variables and these correspond to what is discussed in the data section. The model needs to be written out in appropriate mathematical notation but also in plain English. Every aspect of that notation must be defined otherwise the most this section can receive is poor. The model makes sense based on the substantive area, and also the form of the model. If the model is Bayesian, then priors need to be defined and sensible. Discussion needs to occur around how features enter the model and why. For instance, (and these are just examples) why use ages rather than age-groups, why does province have a levels effect, why is gender categorical, etc? In general, in order to be adequate, there needs to be a clear justification that this is the model for the situation. The assumptions underpinning the model are clearly discussed. Alternative models, or variants, must be discussed and strengths and weaknesses made clear. Why was this model chosen? You should mention the software that you used to run the model. There is some evidence of thought about the circumstances in which the model may not be appropriate. There is evidence of model validation and checking, whether that is out of sample or comparison to a straw man or RMSE, test/training, or appropriate sensitivity checks. You should be clear about model convergence, model checks, and diagnostic issues, but if this becomes too detailed then you could push some of this to an appendix. Great answers would discuss things such as, how do the aspects that you discussed in the data section assert themselves in the modelling decisions that you make. Again if it becomes too detailed then push some of the details to footnotes or an appendix. Again, explain what your model is and what is going on with it. Results - [10 ‘Exceptional,’ 8 ‘Great,’ 6 ‘Good,’ 4 ‘Some issues,’ 2 ‘Many issues,’ 0 ‘Poor or not done’] Results will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps. To be clear, you should also have text associated with all these aspects. Show the reader the results by plotting them. Talk about them. Explain them. That said, this section should strictly relay results. Discussion - [10 ‘Exceptional,’ 8 ‘Great,’ 6 ‘Good,’ 4 ‘Some issues,’ 2 ‘Many issues,’ 0 ‘Poor or not done’] Some questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page): What is done in this this paper? What is something that we learn about the world? What is another thing that we learn about the world? What are some weaknesses of what was done? What is left to learn or how should we proceed in the future? Numbering - [2 ‘Yes,’ 0 ‘Poor or not done’] All figures, tables, equations, etc are numbered and referred to in the text. Proofreading - [2 ‘Yes,’ 0 ‘Poor or not done’] All aspects of submission are free of noticeable typos. Graphs/tables/etc - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor or not done’] You must include graphs and tables in your paper and they must be to a high standard. They must be well formatted and camera-ready They should be clear and digestible. They must: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of labels/explanations, etc; and 3) appropriately sized and coloured (or appropriate significant figures in the case of stats). References - [4 ‘Perfect,’ 3 ‘One minor issue,’ 0 ‘Poor or not done’] All data/software/literature/etc are appropriately noted and cited. You must cite the software and software packages that you use. You must cite the datasets that you use. You must cite literature that you refer to (and you should refer to literature). If you take a small chunk of code from Stack Overflow then add the page in a comment next to the code. If you take a large chunk of code then cite it fully. 3 means one minor issue. More than one minor issue receives 0. Reproducibility - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor or not done’] The paper and analysis must be fully reproducible. A detailed README is included. All code should be thoroughly documented. An R project is used. Do not use setwd(). The code must appropriately read data, prepare it, create plots, conduct analysis, generate documents, etc. Seeds are used where needed. Code must have a preamble etc. You must appropriately document your scripts such that someone coming in could follow them. Your repo must be thoroughly organized and not contain extraneous files. Enhancements - [4 ‘Exceptional,’ 3 ‘Great,’ 2 ‘Fine,’ 1 ‘Gets job done,’ 0 ‘Poor or not done’] You should pick at least one of the following and include it to enhance your submission: Datasheets for for your datasets (see: https://arxiv.org/abs/1803.09010) and model cards for your models (see: https://arxiv.org/pdf/1810.03993.pdf). Shiny application. R package. API for your model. There are always students that excel in a way that is not anticipated in the rubric. This item accounts for that. General excellence - [3 ‘Exceptional,’ 2 ‘Wow,’ 1 ‘Huh, that’s interesting,’ 0 ‘None’] There are always students that excel in a way that is not anticipated in the rubric. This item accounts for that. 20.4.8 Previous examples Some examples of papers that well in the past include those by: Amy Farrow, Laura Cline, and Hong Shi. 20.5 ‘A Proportional Response’ 20.5.1 Task Working in teams of one to four people, please consider this scenario: ‘You are employed as a junior statistician at Petit Poll - a Canadian polling company. Petit Poll has a contract with a Canadian political party to provide them with monthly polling updates.’ Working as part of a small team of 1-4 people, and in an entirely reproducible way, please write a short paper that tells the client a story about their standing. 20.5.2 Recommended steps Please pick a political party that you are ‘working for,’ and pick a geographic focus: 1) the overall election, 2) a particular province, or 3) a specific riding. Then decide on a survey methodology (hint: p. 13 of Wu &amp; Thompson provides a handy checklist). Some questions you should address include: What is the population, frame, and sample? What sampling methods will you use and why (e.g. you could choose SRSWOR, stratified, etc). What are some of the statistical properties that the method brings to the table (e.g. for SRSWOR you could discuss Wu &amp; Thompson, Theorem 2.2, etc, as appropriate)? How are you going to reach your desired respondents? How much do you estimate this will cost? What steps will you take to deal with non-response and how will non-response affect your survey? How are you going to protect respondent privacy? Remember to consider all of this in the context of your ‘client’ - for instance, who would be more interested in Alberta ridings: Bloc Québécois or the Conservatives? Who likely has more money to spend - the Liberals or the Greens? Develop a survey on a platform that was introduced in class. Be sure to test it yourselves. You will want to test this as much as possible, maybe even swap informally with another group? Now release the surveys into the (simulated) ‘field.’ Please do this by simulating an appropriate number of responses to your survey in R. Don’t forget to simulate in relation to the survey methodology that you proposed. Show the results and discuss your ‘findings.’ Everything must be entirely reproducible. You may like to consider linking your survey ‘responses’ with other data such as the census or GSS. Use R Markdown to write a PDF report about all of this. Discuss your results and findings, your survey design and motivations, etc - all of it. You are writing a report that will eventually go to the ‘client,’ so you must set the scene, and use language that demonstrates your command of statistical concepts but brings the reader along with you. Be sure to include graphs and tables and reference them in your discussion. Be sure to be clear about weaknesses and biases, and opportunities for future work. Your report must be well written. You are allowed to, and should, use mathematical notation, but you must explain all of it in plain english. Similarly, you can, and should, use surveys/sampling/observational data terminology, but again, you need to explain it. Your report must include at least the following aspects: title, date, authorship, non-technical executive summary, introduction, survey methodology, results, discussion, appendices that detail the survey, and references. Your ‘client’ has stats graduates working for it who need to be impressed by the main content of the report, but also has people who barely know what an average is and these people need to be impressed also. This is why your report should include a non-technical executive summary. In terms of length, this would typically be roughly 10 per cent of the report. It would be more detailed than an introduction, but still at a high level. Your graphs must be of an extremely high standard. Check that you have referenced everything. Strong submissions will draw on related literature in the discussion and would be sure to also reference those. The style of references does not matter, provided it is consistent. Via Quercus, submit a link to your PDF report which is hosted on GitHub. At some point in the introduction to your report, you must provide a link to the GitHub repo where the code that you used for this assignment lives (Hint: Comment. Your. Code.). Your entire workflow must be entirely reproducible. Please be sure to include a link to your survey in your report and screenshots of the survey in the appendix of your report. There should be no evidence that this is a class assignment. 20.5.3 Check offs points 20.5.4 FAQ 20.6 ‘Mr Willis of Ohio’ 20.6.1 Task Working in teams of one to four people, and in an entirely reproducible way, please use the Canadian General Social Survey (GSS) and a regression model to tell a story. 20.6.2 Recommended steps Depending on your focus and background, you may like to use a Bayesian hierarchical model, but regardless of the particular model that you use it must be well explained, thoroughly justified, appropriate to the task at hand, and the results must be beautifully described. You may focus on any year, aspect, or geography that is reasonable given the focus and constraints of the GSS. As a reminder, the GSS ‘program was designed as a series of independent, annual, cross-sectional surveys, each covering one topic in-depth.’ So please consider the topic and the year. The GSS is available to University of Toronto students via the library. In order to use it you need to clean and prepare it. Code to do this for one year is being distributed alongside this problem set and was discussed in lectures. You are welcome to simply use this code and this year, but the topic of that year will constrain your focus. Naturally, you are welcome to adapt the code to other years. If you use the code exactly as is then you must cite it. If you adapt the code then you don’t have to cite it, as it has a MIT license, but it would be appropriate to at least mention and acknowledge it, depending on how close your adaption is. Using R Markdown, please write a paper about your analysis and compile it into a PDF. Your paper must be well-written, draw on relevant literature, and show your statistical skills by explaining all statistical concepts that you draw on. Your paper must have the following sections: title, name/s, and date, abstract, introduction, data, model, results, discussion, and references. You are welcome to use appendices for supporting, but not critical, material. Your discussion must include sub-sections on weaknesses and next steps. In your report you must provide a link to a GitHub repo that fully contains your analysis. Your code must be entirely reproducible, documented, and readable. Your repo must be well-organised and appropriately use folders. Your graphs and tables must be of an incredibly high standard. Graphs and tables should be well formatted and report-ready. They should be clean and digestible. Furthermore, you should label and describe each table/figure. When you discuss the dataset (in the data section) you should make sure to discuss (at least): Its key features, strengths, and weaknesses generally. A discussion of the questionnaire - what is good and bad about it? A discussion of the methodology including how they find people to take the survey; what their population, frame, and sample were; what sampling approach they took and what some of the trade-offs may be; what they do about non-response; the cost. This is just some of the issues strong submissions will consider. Show off your knowledge. If this becomes too detailed then you should push some of this to footnotes or an appendix. When you discuss your model (in the model section), you must be extremely careful to spell out the statistical model that you are using, defining and explaining each aspect and why it is important. (For a Bayesian model, a discussion of priors and regularization is almost always important.) You should mention the software that you used to run the model. You should be clear about model convergence, model checks, and diagnostic issues. How do the sampling and survey aspects that you discussed assert themselves in the modelling decisions that you make? Again, if it becomes too detailed then push some of the details to footnotes or an appendix. You should present model results, graphs, figures, etc, in the results section. This section should strictly relay results. Interpretation of these results and conclusions drawn from the results should be left for the discussion section. Your discussion should focus on your model results. Interpret them and explain what they mean. Put them in context. What do we learn about the world having understood your model and its results? What caveats could apply? To what extent does your model represent the small world and the large world (to use the language of McElreath, Ch 2)? What are some weaknesses and opportunities for future work? Check that you have referenced everything. Strong submissions will draw on related literature in the discussion (and other sections) and would be sure to also reference those. The style of references does not matter, provided it is consistent. As a team, via Quercus, submit a PDF of your paper. Again, in your paper you must have a link to the associated GitHub repo in an appendix. And you must include the R Markdown file that produced the PDF in that repo. A good way to work as a team would be to split up the work, so that one person is doing each section. The people doing the sections that rely on data (such as the analysis and the graphs) could just simulate it while they are waiting for the person putting together the data to finish. It is expected that your submission be well written and able to be understood by the average reader of say 538. This means that you are allowed to use mathematical notation, but you must be able to explain it all in plain English. Similarly, you can (and hint: you should) use survey, sampling, observational, and statistical terminology, but again you need to explain it. Your work should have flow and should be easy to follow and understand. To communicate well, anyone at the university level should be able to read your report once and relay back the methodology, overall results, findings, weaknesses and next steps without confusion. 20.6.3 Check offs points It is recommended that you (informally) proofread one another’s sections - why not exchange papers with another group? Everyone in the team receives the same mark. There should be no evidence that this is a class assignment. 20.6.4 FAQ 20.7 ‘Five Votes Down’ 20.7.1 Task The primary goal of this paper is to predict the overall popular vote of the 2020 American presidential election using multilevel regression with post-stratification. 20.7.2 Recommended steps We expect you to work as part of a group of 4 people, but groups of size 1-4 are fine. We have suggested a split of the work based on a 4-person group, but these are just suggestions. Individual-level survey data: Request access to the Democracy Fund + UCLA Nationscape ‘Full Data Set’: https://www.voterstudygroup.org/publication/nationscape-data-set. This could take a day or two. Please start early. Given the expense of collecting this data, and the privilege of having access to it, if you don’t properly cite this dataset then you will get zero for this problem set. Once you have access then pick a survey of interest. We will use “ns20200102.dta” in the example (your number may be different). This will be a large file and is not yours to share. Do not push it to GitHub (use the .gitignore file - see here: https://carpentries-incubator.github.io/git-Rstudio-course/02-ignore/index.html). Use the example R code to get started preparing this dataset, and then go on cleaning and preparing it based on what you need. Make graphs and tables about the survey data and write beautiful sentences and paragraphs explaining everything. Post-stratification data: We will use the American Community Surveys (ACS). Please create an account with IPUMS: https://usa.ipums.org/usa/index.shtml You want the 2018 1-year ACS. Then you need to select some variables. This will depend on what you want to model and the survey data, but some options include: REGION, STATEICP, AGE, SEX, MARST, RACE, HISPAN, BPL, CITIZEN, EDUC, LABFORCE, INCTOT. Have a look around and see what you are interested in, remembering that you will need to establish a correspondence to the survey. Download the relevant post-stratification data (it’s probably easiest to change the data format to .dta). Again, this can take some time. Please start this early. This will be a large file and is not yours to share. Do not push it to GitHub (use the .gitignore file - see here: https://carpentries-incubator.github.io/git-Rstudio-course/02-ignore/index.html). Given the expense of collecting this data, and the privilege of having access to it, if you don’t properly cite this dataset then you will get zero for this problem set. Clean and prepare the post-stratification dataset. Remember that you need cell counts for the sub-populations in your model. See examples in the readings. (It may be efficient to start with simulated data while waiting for the real data) Modelling. You will want to explain vote intention based on a variety of explanatory variables. Construct the vote intention variable so that it is binary (either ‘supports Trump’ or ‘supports Biden’). You are welcome to use lm() but you would need to explain the nuances of this decision in the model section (Hint: start here: https://statmodeling.stat.columbia.edu/2020/01/10/linear-or-logistic-regression-with-binary-outcomes/). That said, you should probably use logistic regression if it is at all possible for you. If you don’t know where to start then look at (in increasing levels of complexity) glm(), lme4::glmer(), or brms::brm(). There are examples of each in the readings. Think very deeply about model fit, diagnostics, and other similar things that you need in order to convince someone that your model is appropriate. You have flexibility of the model that you use, (and hence the cells that you’ll need to create next). In general, the more cells the better, but you may want fewer cells for simplicity in the writing process and to ensure a decent sample in each cell. Apply your trained model to the post-stratification dataset to make the best estimate of the election result that you can. The specifics will depend on your modelling approach but will likely involve predict(), add_predicted_draws(), or similar. See the examples in the readings. We are primarily interested in the distribution of your forecast of the overall Presidential popular vote, and how the explanatory variables affect this. But great submissions would go beyond that. Also, you’re taking a statistics course, so if you just gave a central estimate and nothing else, then that would not be great. Create beautiful graphs and tables of your model and results. Create wonderful paragraphs talking about and explaining everything. (Again, it’s probably efficient to start with simulated data/results while waiting) Write up. Using R Markdown, please write a very thorough paper about your analysis and compile it into a PDF. The paper must be well-written, draw on relevant literature, and show your statistical skills by explaining all statistical concepts that you draw on. The paper must have the following sections: title, name/s, and date, abstract and keywords, introduction, data, model, results, discussion, and references. The paper may use appendices for supporting, but not critical, material. The discussion needs to be substantial. For instance, if the paper were 10 pages long then a discussion should be at least 2.5 pages. In the discussion, the paper must include subsections on weaknesses and next steps - but these must be in proportion. The report must provide a link to a GitHub repo that contains everything (apart from the raw data that you git ignored because it is not yours to share). The code must be entirely reproducible, documented, and readable. The repo must be well-organised and appropriately use folders and README files. The graphs and tables must be of an incredibly high standard, well formatted, and report-ready. They should be clean and digestible. Furthermore, you should label and describe each table/figure. When you discuss the datasets (in the data section) (remember there will be at least two datasets to discuss) you should make sure to discuss (at least): Their key features, strengths, and weaknesses generally. The survey questionnaire - what is good and bad about it? A discussion of the methodology including how they find people to take the survey; what their population, frame, and sample were; what sampling approach they took and what some of the trade-offs may be; what they do about non-response; the cost. This is just some of the issues strong submissions will consider. Show off your knowledge. If this becomes too detailed then you should push some of this to footnotes or an appendix. The dataset section is probably an appropriate place to include an explanation of what post-stratification is (in non-statistical language) and the strengths and weaknesses of it, although this discussion may fit more naturally in another section. Regardless, be sure to justify the inclusion of each explanatory variable. When you discuss your model (in the model section), you must be extremely careful to spell out the statistical model that you are using, defining and explaining each aspect and why it is important. (For a Bayesian model, a discussion of priors and regularization is almost always important.) You should mention the software that you used to run the model. You should be clear about model convergence, model checks, and diagnostic issues, although you may push the details of this to an appendix depending on how detailed you get. How do the sampling and survey aspects that you discussed assert themselves in the modelling decisions that you make? How can you convince a reader that you’ve neither overfit nor underfit the data? Again, if it becomes too detailed then push some of the details to footnotes or an appendix. You should present model results, graphs, figures, etc, in the results section. This section should strictly relay results. It must include text explaining all of these and summary statistics and similar. However, interpretation of these results and conclusions drawn from the results should be left for the discussion section. Your discussion should focus on your model results, but this time interpreting them, and explaining what they mean. Put them in context. What do we learn about the world having understood your model and its results? What caveats could apply? To what extent does your model represent the small world and the large world (to use the language of McElreath, Ch 2)? What are some weaknesses and opportunities for future work? Who is going to win the election? How confident are you in that forecast? Do you have a small or large distribution? What could that mean? Are you more confident in certain states? Do certain explanatory variables carry more weight than others? Etc. Check that you have referenced everything. Strong submissions will draw on related literature in the discussion (and other sections) and would be sure to also reference those. The style of references does not matter, but it must be consistent. If you don’t cite R then you will get zero for this problem set. As a team, via Quercus, submit a PDF of your paper. Again, in your paper you must have a link to the associated GitHub repo. And you must include the R Markdown file that produced the PDF in that repo. The RMarkdown file must exactly produce the PDF. Don’t edit it manually ex post - that isn’t reproducible. A good way to work as a team would be to split up the work, so that one person is doing each section. The people doing the sections that rely on data (such as the analysis and the graphs) could just simulate it while they are waiting for the person putting together the data to finish. We have recommended a split above, but you do what works for you. 20.7.3 Check offs points It is expected that your submission be well written and able to be understood by the average reader of say 538. This means that you are allowed to use mathematical notation, but you must be able to explain it all in plain English. Similarly, you can (and hint: you should) use survey, sampling, observational, and statistical terminology, but again you need to explain it. The average person doesn’t know what a p-value is nor what a confidence interval is. You need to explain all of this in plain language the first time you use it. Your work should have flow and should be easy to follow and understand. To communicate well, anyone at the university level should be able to read your report once and relay back the methodology, overall results, findings, weaknesses and next steps without confusion. It is recommended that you (informally) proofread one another’s work - why not exchange papers with another group? Everyone in the team receives the same mark. There should be no evidence that this is a class assignment. 20.7.4 FAQ 20.8 ‘What’s next?’ 20.8.1 Task Please work individually. In this paper, you will conduct original research that applies methods from statistics to a question involving surveys, sampling or observational data. 20.8.2 Recommended steps You have various options for topics (pick one): Develop a research question that is of interest to you and obtain or create a relevant dataset. This option involves developing your own research question based on your own interests, background, and expertise. I encourage you to take this option, but please discuss your plans with me. How does one come up with ideas? One way is to be question-driven, where you keep an informal log of small ideas, questions, and puzzles, that you have as you’re reading and working. Often, after dwelling on it for a while you can manage to find some questions of interest. Another way is to be data-driven - try to find some interesting dataset and then work backward. Finally, yet another way, is to be methods-driven - let’s say that you happen to understand Gaussian processes, then just apply that expertise. (Thanks to Jack Bailey for this idea.) Build a MRP model based on the CES and a post-stratification dataset that you obtain, to identify how the 2019 Canadian Federal Election would have been different if ‘everyone’ had voted. What do we learn about the importance of turnout based on your model and results? (This option involves logistic regression in either frequentist or Bayesian settings.) Reproduce a paper. This means that you download the data and then write your own code (using their code and paper as a guide if it’s available) to try to get their results and then write up what you did and found. Options include: Angelucci, Charles, and Julia Cagé, 2019, ‘Newspapers in times of low advertising revenues,’ American Economic Journal: Microeconomics, please see: https://www.openicpsr.org/openicpsr/project/116438/version/V1/view. (This option can be accomplished with just OLS. It is a ‘safe’ pick. I even already provided you with some code in class to get started - see the notes! ). Bailey, Michael A., Daniel J. Hopkins &amp; Todd Rogers, 2016, ‘Unresponsive and Unpersuaded: The Unintended Consequences of a Voter Persuasion Effort,’ Political Behavior. Clark, Sam, 2019, ‘A General Age-Specific Mortality Model With an Example Indexed by Child Mortality or Both Child and Adult Mortality,’ Demography, please see: https://github.com/sinafala/svd-comp. (This is an ambiguous pick!) Skinner, Ben, 2019, ‘Making the connection: Broadband access and online course enrollment at public open admissions institutions,’ Research in Higher Education, please see: https://github.com/btskinner/oa_online_broadband_rep. Pons, Vincent, 2018, ‘Will a Five-Minute Discussion Change Your Mind? A Countrywide Experiment on Voter Choice in France’ American Economic Review. Valencia Caicedo, Felipe, 2019, ‘The Mission: Human Capital Transmission, Economic Persistence, and Culture in South America,’ The Quarterly Journal of Economics, please see: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ML1155. If you have a favourite paper and want to reproduce it, then please let me know by the end of Week 12 so that I can check that it’s appropriate. Pretend that you work for Upworthy. Request the Upworthy dataset and then use it to evaluate the result of an A/B test. This request could take a week. Please plan ahead if you choose this option. Critique the following paper: AlShebli, Bedoor, Kinga Makovi &amp; Talal Rahwan, 2020, ‘The association between early career informal mentorship in academic collaborations and junior author performance,’ Nature Communications. You should be able to download the data here: https://github.com/bedoor/Mentorship and the paper here: https://www.nature.com/articles/s41467-020-19723-8. For background and starting points for your critique, please see: https://statmodeling.stat.columbia.edu/2020/11/19/are-female-scientists-worse-mentors-this-study-pretends-to-know/ and https://danieleweeks.github.io/Mentorship/#summary. (This option involves extensive data exploration and thinking really hard about what they are trying to do and how they are doing it.) Use this post by Andrew Whitby - https://andrewwhitby.com/2020/11/24/contact-tracing-biased/ - as a starting point to explore biased sampling and its effects on what we know about COVID and how this affects public policy. (This option involves extensive simulation.) In ‘Bias Behind Bars,’ data journalist Tom Cardoso finds that ‘(a)fter controlling for a number of variables, … Black and Indigenous inmates are more likely to get worse scores than white inmates, based solely on their race.’ The main story is here: https://www.theglobeandmail.com/canada/article-investigation-racial-bias-in-canadian-prison-risk-assessments/ The methodology discussion is here: https://www.theglobeandmail.com/canada/article-investigation-racial-bias-in-canadian-prisons-methodology/ The observational data is available here: https://www.theglobeandmail.com/files/editorial/News/nw-na-risk-1023/The_Globe_and_Mail_CSC_OMS_2012-2018_20201022235635.zip Your task is to follow the methodology that Tom published and attempt to replicate the results. Are you able to replicate them? Do the results change significantly under slightly different assumptions? (This option involves only frequentist logistic regression, although doing everything in a Bayesian setting would be lovely too). You should know the expectations by now. If you need a refresher then review the past problem sets. Everything is entirely reproducible. Your paper must be written in R Markdown. Your paper must have the following sections: Title, date, author, keywords, abstract, introduction, data, model, results, discussion, appendix (optional, for supporting, but not critical, material), and a reference list. Your paper must be well-written, draw on relevant literature, and show your statistical skills by explaining all statistical concepts that you draw on. The discussion needs to be substantial. For instance, if the paper were 10 pages long then a discussion should be at least 2.5 pages. In the discussion, the paper must include subsections on weaknesses and next steps - but these must be in proportion. The report must provide a link to a GitHub repo that contains everything (apart from any raw data that you git ignored if it is not yours to share). The code must be entirely reproducible, documented, and readable. The repo must be well-organised and appropriately use folders and README files. My expectations for this paper are very high. I’m very excited to read what you submit. To help you achieve this standard, there are two initial ‘submissions’ where you can get comments and feedback and then the final, actual, submission. Due dates: (Optional) December 9 11:59pm ET Submit initial materials for peer-review. As an individual, via Quercus, submit a PDF of your rough draft on Quercus by 11:59pm ET on Wednesday, December 9, 2020. At a minimum this must include: All top-matter (title, author (you can use a pseudonym if you want), date, keywords, abstract) completely filled out. A fully written Introduction section. All other sections must be present in your paper, but don’t have to be filled out (e.g. you must have a ‘Data’ heading, but you don’t need to have content in that section). To be clear - it is fine to later change any aspect of what you submit at this checkpoint. You will be awarded 1 percentage point just for submitting a draft that meets this minimum (that is 1 out of the 30 that are available for the final paper). If you don’t submit, then the percentage point will be pushed to part d). The point of this is to get feedback on your work (and to make sure you have at least started thinking about this project) so you are more than welcome to include other sections that you wish to get feedback on. There will be no extensions granted for this submission since the following submission is dependent on this date. (Optional) December 12 11:59pm ET Conduct peer-review. As an individual, on December 10, you will randomly be assigned a handful of rough drafts to provide feedback. You have until December 12, 2020 11:59pm ET to provide feedback to your peers. If you provide feedback to one peer you will receive 1 percentage point, if you provide feedback to two peers you will receive 2 percentage points, if you provide feedback to three (or more) peers you will receive the full 3 percentage points. You may complete this aspect whether or not you submitted something in part a). If you don’t complete it then the percentage points will be pushed to part d). Your feedback must include at least six comments (meaningful/useful bullet points). These must be well-written and thoughtful. There will be no extensions granted for this submission since the following submission is dependent on this date. Please remember that you are providing feedback here to help your colleagues. All comments should be professional and kind. It is challenging to receive criticism. Please remember that your goal here is to help your peers advance their writing/analysis. Any feedback that is inappropriate or not up to standard will receive a 0 and cannot be redeemed later. (Optional) December 16 11:59pm ET Submit materials for TA review. Submit a PDF to Quercus. The TA will provide high-level comments on December 17. At a minimum this must include: All top-matter. Fully written Introduction, Data, Model, and Results sections. All other sections must be present in your paper, but don’t have to be filled out (e.g. you must have a ‘Discussion’ heading, but you don’t need to have content in that section). To be clear - it is fine to later change any aspect of what you submit at this checkpoint. You receive 1 percentage point for submitting something that meets that minimum. If you don’t submit anything then this is pushed to the final paper. There are no extensions possible on this aspect. (Compulsory) December 20 11:59pm ET As an individual, via Quercus, submit a PDF of your paper. Again, in your paper, you must have a link to the associated GitHub repo. This submission will be graded based on a rubric that will be posted on Quercus and will be worth 25-30 percentage points depending on parts a) - c). 20.8.3 Check offs points 20.8.4 FAQ Do I have to submit an initial paper in order to do the peer-review? Yes. "],["references-1.html", "References", " References Acemoglu, Daron, Simon Johnson, and James A Robinson. 2001. “The Colonial Origins of Comparative Development: An Empirical Investigation.” American Economic Review 91 (5): 1369–1401. Akerlof, George A. 1978. “The Market for ’Lemons’: Quality Uncertainty and the Market Mechanism.” In Uncertainty in Economics, 235–51. Elsevier. Alexander, Monica. 2019. “Reproducibility in Demographic Research.” https://www.monicaalexander.com/posts/2019-10-20-reproducibility/. Alexander, Rohan, and Monica Alexander. 2020. “The Increased Effect of Elections and Changing Prime Ministers on Topics Discussed in the Australian Federal Parliament Between 1901 and 2018.” https://rohanalexander.com/pdfs/AlexanderAlexander-EffectofElectionsandPrimeMinisters.pdf. Allen, Eric J, Patricia M Dechow, Devin G Pope, and George Wu. 2017. “Reference-Dependent Preferences: Evidence from Marathon Runners.” Management Science 63 (6): 1657–72. Allen, Jeff. 2021. plumberDeploy: Plumber Deployment. https://CRAN.R-project.org/package=plumberDeploy. Alsan, Marcella, and Marianne Wanamaker. 2018. “Tuskegee and the Health of Black Men.” The Quarterly Journal of Economics 133 (1): 407–55. Arel-Bundock, Vincent. 2020. Modelsummary: Summary Tables and Plots for Statistical Models and Data: Beautiful, Customizable, and Publication-Ready. https://CRAN.R-project.org/package=modelsummary. ———. 2020. Modelsummary: Summary Tables and Plots for Statistical Models and Data: Beautiful, Customizable, and Publication-Ready. https://CRAN.R-project.org/package=modelsummary. Aschwanden, Christie. 2020. Artificial Intelligence Makes Bad Medicine Even Worse. https://www.wired.com/story/artificial-intelligence-makes-bad-medicine-even-worse/. Athey, Susan, and Guido W Imbens. 2017. “The State of Applied Econometrics: Causality and Policy Evaluation.” Journal of Economic Perspectives 31 (2): 3–32. Barrett, Malcolm. 2021a. Data Science as an Atomic Habit. https://malco.io/2021/01/04/data-science-as-an-atomic-habit/. ———. 2021b. Ggdag: Analyze and Create Elegant Directed Acyclic Graphs. https://CRAN.R-project.org/package=ggdag. Bastian, Hilda. 2020. “A Timeline of the Oxford-AstraZeneca Covid-19 Vaccine Trials.” http://hildabastian.net/index.php/100. Bates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015. “Fitting Linear Mixed-Effects Models Using lme4.” Journal of Statistical Software 67 (1): 1–48. https://doi.org/10.18637/jss.v067.i01. Berkson, Joseph. 1946. “Limitations of the Application of Fourfold Table Analysis to Hospital Data.” Biometrics Bulletin 2 (3): 47–53. http://www.jstor.org/stable/3002000. Blair, Graeme, Jasper Cooper, Alexander Coppock, and Macartan Humphreys. 2019. “Declaring and Diagnosing Research Designs.” American Political Science Review 113: 838–59. https://declaredesign.org/paper.pdf. Blei, David M. 2012. “Probabilistic Topic Models.” Communications of the ACM 55 (4): 77–84. Blei, David M, and John D Lafferty. 2009. “Topic Models.” In Text Mining, 101–24. Chapman; Hall/CRC. Blei, David M, Andrew Y Ng, and Michael I Jordan. 2003. “Latent Dirichlet Allocation.” Journal of Machine Learning Research 3 (Jan): 993–1022. Bloom, Howard, Andrew Bell, and Kayla Reiman. 2020. “Using Data from Randomized Trials to Assess the Likely Generalizability of Educational Treatment-Effect Estimates from Regression Discontinuity Designs.” Journal of Research on Educational Effectiveness, 1–30. https://doi.org/10.1080/19345747.2019.1634169. Brandt, Allan M. 1978. “Racism and Research: The Case of the Tuskegee Syphilis Study.” Hastings Center Report, 21–29. Bronte, Charlotte. 1847. Jane Eyre. https://www.gutenberg.org/files/1260/1260-h/1260-h.htm. Brook, Robert H, John E Ware, William H Rogers, Emmett B Keeler, Allyson Ross Davies, Cathy D Sherbourne, George A Goldberg, Kathleen N Lohr, Patricia Camp, and Joseph P Newhouse. 1984. “The Effect of Coinsurance on the Health of Adults: Results from the RAND Health Insurance Experiment.” Bryan, Jennifer, and Jim Hester. 2020. What They Forgot to Teach You about r. https://rstats.wtf/index.html. Bryan, Jennifer, Jim Hester, David Robinson, and Hadley Wickham. 2019. Reprex: Prepare Reproducible Example Code via the Clipboard. https://CRAN.R-project.org/package=reprex. Bryan, Jenny. 2020. Happy Git and GitHub for the useR. https://happygitwithr.com. Buhr, Ray. 2017. Using r as a Production Machine Learning Language (Part i). https://raybuhr.github.io/blog/posts/making-predictions-over-http/. Cambon, Jesse, and Christopher Belanger. 2021. “Tidygeocoder: Geocoding Made Easy.” Zenodo. https://doi.org/10.5281/zenodo.3981510. Chamberlain, Scott, Hadley Wickham, and Winston Chang. 2021. Analogsea: Interface to ’Digital Ocean’. Cohn, Alain. 2019. “Data and code for: Civic Honesty Around the Globe.” Harvard Dataverse. https://doi.org/10.7910/DVN/YKBODN. Cohn, Alain, Michel André Maréchal, David Tannenbaum, and Christian Lukas Zünd. 2019a. “Civic Honesty Around the Globe.” Science 365 (6448): 70–73. ———. 2019b. “Supplementary Materials for: Civic Honesty Around the Globe.” Science 365 (6448): 70–73. Cohn, Nate. 2016. We Gave Four Good Pollsters the Same Raw Data. They Had Four Different Results. Cooksey, Brian. 2014. “An Introduction to APIs.” Zapier. https://zapier.com/learn/apis/. Cox, Murray. 2021. “Inside Airbnb - Toronto Data.” http://insideairbnb.com/get-the-data.html. Cunningham, Scott. 2020. Causal Inference: The Mixtape. https://www.scunning.com/mixtape.html. ———. 2021. Causal Inference: The Mixtape. Yale Press. Dagan, Noa, Noam Barda, Eldad Kepten, Oren Miron, Shay Perchik, Mark A Katz, Miguel A Hernán, Marc Lipsitch, Ben Reis, and Ran D Balicer. 2021. “BNT162b2 mRNA Covid-19 Vaccine in a Nationwide Mass Vaccination Setting.” New England Journal of Medicine. Dahly, Darren. 2020. A Brief History of Medical Statistics and Its Impact on Reproducibility. https://statsepi.substack.com/p/a-brief-history-of-medical-statistics. Darling, William M. 2011. “A Theoretical and Practical Implementation Tutorial on Topic Modeling and Gibbs Sampling.” In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, 642–47. “Data Science Radar: How to Identify World-Class Data Science Capabilities.” 2020. Mango Solutions. https://www.mango-solutions.com/data-science-radar-how-to-identify-world-class-data-science-capabilities/. Farrugia, Patricia, Bradley A Petrisor, Forough Farrokhyar, and Mohit Bhandari. 2010. “Research Questions, Hypotheses and Objectives.” Canadian Journal of Surgery 53 (4): 278. Finkelstein, Amy, Sarah Taubman, Bill Wright, Mira Bernstein, Jonathan Gruber, Joseph P Newhouse, Heidi Allen, Katherine Baicker, and Oregon Health Study Group. 2012. “The Oregon Health Insurance Experiment: Evidence from the First Year.” The Quarterly Journal of Economics 127 (3): 1057–1106. Firke, Sam. 2020. Janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor. Fisher, Ronald. 1935. The Design of Experiments. Oliver; Boyd. Fitts, Alexis Sobel. 2014. “The King of Content: How Upworthy Aims to Alter the Web, and Could End up Altering the World.” Columbia Journalism Review. https://archives.cjr.org/feature/the_king_of_content.php. FT Visual &amp; Data Journalism team. 2020. “Coronavirus Tracked: See How Your Country Compares.” https://ig.ft.com/coronavirus-chart/. Gagolewski, Marek. 2020. R Package Stringi: Character String Processing Facilities. http://www.gagolewski.com/software/stringi/. Gao, Yuxiang, Lauren Kennedy, Daniel Simpson, Andrew Gelman, and others. 2021. “Improving Multilevel Regression and Poststratification with Structured Priors.” Bayesian Analysis. Gebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III au2, and Kate Crawford. 2020. “Datasheets for Datasets.” http://arxiv.org/abs/1803.09010. Gelfand, Sharla. 2020. Opendatatoronto: Access the City of Toronto Open Data Portal. https://CRAN.R-project.org/package=opendatatoronto. Gelman, Andrew. 2016. “What Has Happened down Here Is the Winds Have Changed.” https://statmodeling.stat.columbia.edu/2016/09/21/what-has-happened-down-here-is-the-winds-have-changed/. Gelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Gelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press. Gertler, Paul J, Sebastian Martinez, Patrick Premand, Laura B Rawlings, and Christel MJ Vermeersch. 2016. Impact Evaluation in Practice. The World Bank. Ghitza, Yair, and Andrew Gelman. 2020. “Voter Registration Databases and MRP: Toward the Use of Large-Scale Databases in Public Opinion Research.” Political Analysis 28 (4): 507–31. ———. 2020. “Voter Registration Databases and MRP: Toward the Use of Large-Scale Databases in Public Opinion Research.” Political Analysis 28 (4): 507–31. Goodrich, Ben, Jonah Gabry, Imad Ali, and Sam Brilleman. 2020. “Rstanarm: Bayesian Applied Regression Modeling via Stan.” https://mc-stan.org/rstanarm. Greenland, Sander, Stephen J Senn, Kenneth J Rothman, John B Carlin, Charles Poole, Steven N Goodman, and Douglas G Altman. 2016. “Statistical Tests, p Values, Confidence Intervals, and Power: A Guide to Misinterpretations.” European Journal of Epidemiology 31 (4): 337–50. Griffiths, Thomas, and Mark Steyvers. 2004. “Finding Scientific Topics.” PNAS 101: 5228–35. Grolemund, Garrett, and Hadley Wickham. 2011. “Dates and Times Made Easy with lubridate.” Journal of Statistical Software 40 (3): 1–25. http://www.jstatsoft.org/v40/i03/. Grün, Bettina, and Kurt Hornik. 2011. “topicmodels: An R Package for Fitting Topic Models.” Journal of Statistical Software 40 (13): 1–30. https://doi.org/10.18637/jss.v040.i13. Hanretty, Chris. 2020. “An Introduction to Multilevel Regression and Post-Stratification for Estimating Constituency Opinion.” Political Studies Review 18 (4): 630–45. Hastie, Trevor J, and Robert J Tibshirani. 1990. Generalized Additive Models. Vol. 43. CRC press. Henry, Lionel, and Hadley Wickham. 2020. Purrr: Functional Programming Tools. https://CRAN.R-project.org/package=purrr. Hernan, Miguel A, and James M Robins. 2020. What If. CRC Press. Hugh-Jones, David. 2020. Huxtable: Easily Create and Style Tables for LaTeX, HTML and Other Formats. https://CRAN.R-project.org/package=huxtable. Hulley, Stephen B. 2007. Designing Clinical Research. Lippincott Williams &amp; Wilkins. Iannone, Richard. 2020. DiagrammeR: Graph/Network Visualization. https://CRAN.R-project.org/package=DiagrammeR. Iannone, Richard, Joe Cheng, and Barret Schloerke. 2020a. Gt: Easily Create Presentation-Ready Display Tables. https://CRAN.R-project.org/package=gt. ———. 2020b. Gt: Easily Create Presentation-Ready Display Tables. https://CRAN.R-project.org/package=gt. Igelström, Erik. 2020. “Causal Graphs in r with DiagrammeR.” https://www.erikigelstrom.com/articles/causal-graphs-in-r-with-diagrammer/. Imai, Kosuke. 2017. Quantitative Social Science. Princeton University Press. James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2017. An Introduction to Statistical Learning with Applications in r. Kay, Matthew. 2020. tidybayes: Tidy Data and Geoms for Bayesian Models. https://doi.org/10.5281/zenodo.1308151. Kearney, Michael W. 2019. “Rtweet: Collecting and Analyzing Twitter Data.” Journal of Open Source Software 4 (42): 1829. https://doi.org/10.21105/joss.01829. Kennedy, Lauren, and Jonah Gabry. 2020. “MRP with Rstanarm.” https://mc-stan.org/rstanarm/articles/mrp.html. Kennedy, Lauren, Katharine Khanna, Daniel Simpson, and Andrew Gelman. 2020. “Using Sex and Gender in Survey Adjustment.” http://arxiv.org/abs/2009.14401. Keyes, Os. 2019. “Counting the Countless.” Real Life. https://reallifemag.com/counting-the-countless/. Kohavi, Ron, Diane Tang, and Ya Xu. 2020. Trustworthy Online Controlled Experiments: A Practical Guide to a/b Testing. Cambridge University Press. Kuhn, Max, and Hadley Wickham. 2020. Tidymodels: A Collection of Packages for Modeling and Machine Learning Using Tidyverse Principles. https://www.tidymodels.org. Lauderdale, Benjamin E, Delia Bailey, Jack Blumenau, and Douglas Rivers. 2020. “Model-Based Pre-Election Polling for National and Sub-National Outcomes in the US and UK.” International Journal of Forecasting 36 (2): 399–413. Levitt, Steven D. 1997. “Using Electoral Cycles in Police Hiring to Estimate the Effect of Police on Crime.” The American Economic Review 87 (3). ———. 2002. “Using Electoral Cycles in Police Hiring to Estimate the Effects of Police on Crime: Reply.” American Economic Review 92 (4): 1244–50. Locke, Steph, and Lucy D’Agostino McGowan. 2018. datasauRus: Datasets from the Datasaurus Dozen. https://CRAN.R-project.org/package=datasauRus. Lopp, Sean. 2017. “R for Enterprise: Understanding r’s Startup.” R Views. https://rviews.rstudio.com/2017/04/19/r-for-enterprise-understanding-r-s-startup/. Lumley, Thomas. 2020. “Survey: Analysis of Complex Survey Samples.” Lüdecke, Daniel, Dominique Makowski, Philip Waggoner, and Indrajeet Patil. 2020. “Performance: Assessment of Regression Models Performance.” CRAN. https://doi.org/10.5281/zenodo.3952174. Matias, J. Nathan, Kevin Munger, Marianne Aubin Le Quere, and Charles Ebersole. 2019. “The Upworthy Research Archive.” https://upworthy.natematias.com. Mattson, Greggor. 2017. “Artificial Intelligence Discovers Gayface. Sigh.” https://greggormattson.com/2017/09/09/artificial-intelligence-discovers-gayface/amp/. McCrary, Justin. 2002. “Using Electoral Cycles in Police Hiring to Estimate the Effect of Police on Crime: Comment.” American Economic Review 92 (4): 1236–43. McElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. CRC Press. Meng, Xiao-Li, and others. 2018. “Statistical Paradises and Paradoxes in Big Data (i): Law of Large Populations, Big Data Paradox, and the 2016 US Presidential Election.” The Annals of Applied Statistics 12 (2): 685–726. Mitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. “Model Cards for Model Reporting.” Proceedings of the Conference on Fairness, Accountability, and Transparency, January. https://doi.org/10.1145/3287560.3287596. Müller, Kirill. 2017b. Here: A Simpler Way to Find Your Files. https://CRAN.R-project.org/package=here. ———. 2017a. Here: A Simpler Way to Find Your Files. https://CRAN.R-project.org/package=here. Nelder, John Ashworth, and Robert WM Wedderburn. 1972. “Generalized Linear Models.” Journal of the Royal Statistical Society: Series A (General) 135 (3): 370–84. Obermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Science 366 (6464): 447–53. ———. 2019. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Science 366 (6464): 447–53. Ooms, Jeroen. 2019a. Pdftools: Text Extraction, Rendering and Converting of PDF Documents. https://CRAN.R-project.org/package=pdftools. ———. 2019b. Pdftools: Text Extraction, Rendering and Converting of PDF Documents. https://CRAN.R-project.org/package=pdftools. ———. 2019c. Tesseract: Open Source OCR Engine. https://CRAN.R-project.org/package=tesseract. Oostrom, Tamar. 2021. “Funding of Clinical Trials and Reported Drug Efficacy.” https://drive.google.com/file/d/1EQLCH0ns99IxYBkxPNbagcZtGgE9a8MQ/view. Oreopoulos, Philip, and Uros Petronijevic. 2018. “Student Coaching: How Far Can Technology Go?” Journal of Human Resources 53 (2): 299–329. https://doi.org/10.3368/jhr.53.2.1216-8439R. Oxford-AstraZeneca. 2020. “Azd1222 Vaccine Met Primary Efficacy Endpoint in Preventing COVID-19.” https://www.astrazeneca.com/media-centre/press-releases/2020/azd1222hlr.html. Pavlik, Kaylin. 2019. “Understanding + Classifying Genres Using Spotify Audio Features.” https://www.kaylinpavlik.com/classifying-songs-genres/. Pedersen, Thomas Lin. 2020. Patchwork: The Composer of Plots. https://CRAN.R-project.org/package=patchwork. Pitman, Jim. 1993. Probability. R Core Team. 2020. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Raju, Tonse. 2005. William Sealy Gosset and William a. Silverman: Two \"Students\" of Science. Pediatrics. Vol. 116. https://doi.org/10.1542/peds.2005-1134. Robinson, David, Alex Hayes, and Simon Couch. 2020. Broom: Convert Statistical Objects into Tidy Tibbles. https://CRAN.R-project.org/package=broom. Robinson, Emily, and Jacqueline Nolis. 2020. Build a Career in Data Science. https://livebook.manning.com/book/build-a-career-in-data-science?origin=product-look-inside. Salganik, Matthew. 2018. Bit by Bit: Social Research in the Digital Age. Princeton University Press. Schloerke, Barret, and Jeff Allen. 2021. Plumber: An API Generator for r. https://CRAN.R-project.org/package=plumber. Shetty, Shravya, and Daniel Tse. 2020. Using AI to Improve Breast Cancer Screening. https://blog.google/technology/health/improving-breast-cancer-screening/. Si, Yajuan. 2020. “On the Use of Auxiliary Variables in Multilevel Regression and Poststratification.” http://arxiv.org/abs/2011.00360. Silge, Julia. 2018. Text Classification with Tidy Data Principles. https://juliasilge.com/blog/tidy-text-classification/. Simpson, Dan. 2017. “It Seemed to Me That Most Destruction Was Being Done by Those Who Could Not Choose Between the Two.” https://statmodeling.stat.columbia.edu/2017/09/12/seemed-destruction-done-not-choose-two/. Simpson, Edward H. 1951. “The Interpretation of Interaction in Contingency Tables.” Journal of the Royal Statistical Society: Series B (Methodological) 13 (2): 238–41. Sjoberg, Daniel D., Michael Curry, Margie Hannum, Joseph Larmarange, Karissa Whiting, and Emily C. Zabor. 2021. Gtsummary: Presentation-Ready Data Summary and Analytic Result Tables. https://CRAN.R-project.org/package=gtsummary. Slowikowski, Kamil. 2021. Ggrepel: Automatically Position Non-Overlapping Text Labels with ’Ggplot2’. https://CRAN.R-project.org/package=ggrepel. Steyvers, Mark, and Tom Griffiths. 2006. “Probabilistic Topic Models.” In Latent Semantic Analysis: A Road to Meaning, edited by T. Landauer, D McNamara, S. Dennis, and W. Kintsch. Stock, James H, and Francesco Trebbi. 2003. “Retrospectives: Who Invented Instrumental Variable Regression?” Journal of Economic Perspectives 17 (3): 177–94. Taback, Nathan. 2020. Design of Experiments and Observational Studies. https://scidesign.github.io/designbook/. Taddy, Matt. 2019. Business Data Science. McGraw Hill. Thompson, Charlie, Josiah Parry, Donal Phipps, and Tom Wolff. 2020. Spotifyr: R Wrapper for the ’Spotify’ Web API. http://github.com/charlie86/spotifyr. Tierney, Nicholas. 2017. “Visdat: Visualising Whole Data Frames.” JOSS 2 (16): 355. https://doi.org/10.21105/joss.00355. Tukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67. Wang, Wei, David Rothschild, Sharad Goel, and Andrew Gelman. 2015. “Forecasting Elections with Non-Representative Polls.” International Journal of Forecasting 31 (3): 980–91. Ware, James. 1989. “Investigating Therapies of Potentially Great Benefit: ECMO.” Statistical Science, no. 4: 298–306. Wickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org. ———. 2017. Tidyverse: Easily Install and Load the ’Tidyverse’. https://CRAN.R-project.org/package=tidyverse. ———. 2019a. Httr: Tools for Working with URLs and HTTP. https://CRAN.R-project.org/package=httr. ———. 2019b. Rvest: Easily Harvest (Scrape) Web Pages. https://CRAN.R-project.org/package=rvest. ———. 2019c. Stringr: Simple, Consistent Wrappers for Common String Operations. https://CRAN.R-project.org/package=stringr. ———. 2020a. Forcats: Tools for Working with Categorical Variables (Factors). https://CRAN.R-project.org/package=forcats. ———. 2020b. Tidyverse. https://www.tidyverse.org/. Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019b. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686. ———, et al. 2019a. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686. Wickham, Hadley, and Jennifer Bryan. 2020. Usethis: Automate Package and Project Setup. https://CRAN.R-project.org/package=usethis. Wickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2020. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr. Wickham, Hadley, and Garrett Grolemund. 2017. R for Data Science. https://r4ds.had.co.nz/. Wickham, Hadley, Jim Hester, and Winston Chang. 2020. Devtools: Tools to Make Developing r Packages Easier. https://CRAN.R-project.org/package=devtools. Wickham, Hadley, and Evan Miller. 2020. Haven: Import and Export ’SPSS’, ’Stata’ and ’SAS’ Files. https://CRAN.R-project.org/package=haven. Wright, Philip G. 1928. The Tariff on Animal and Vegetable Oils. Macmillan Company. Wu, Changbao, and Mary E Thompson. 2020. Sampling Theory and Practice. Springer. Zhu, Hao. 2020. kableExtra: Construct Complex Table with ’Kable’ and Pipe Syntax. https://CRAN.R-project.org/package=kableExtra. "]]
