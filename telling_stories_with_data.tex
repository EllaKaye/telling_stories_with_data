% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Telling Stories With Data},
  pdfauthor={Rohan Alexander},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Telling Stories With Data}
\author{Rohan Alexander}
\date{2021-01-19}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{part-essentials}{%
\part{Essentials}\label{part-essentials}}

\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

\textbf{These notes are being actively developed.}

If you have comments or suggestions, or find mistakes, then please don't hesitate to \href{mailto:rohan.alexander@utoronto.ca}{get in touch}.

Version: 0.0.0.9000.

\hypertarget{welcome}{%
\section{Welcome}\label{welcome}}

Hi, I'm Rohan Alexander. You can find out more about me \href{https://rohanalexander.com/}{here}. These are notes that I wrote to support my teaching at the University of Toronto across the Faculty of Information and the Department of Statistical Sciences. The focus is on using quantitative methods to tell stories with data.

\hypertarget{structure}{%
\section{Structure}\label{structure}}

The parts of these notes are:

\begin{itemize}
\tightlist
\item
  Essentials

  \begin{itemize}
  \tightlist
  \item
    Hello world
  \item
    R essentials
  \item
    Workflow
  \end{itemize}
\item
  Communicate

  \begin{itemize}
  \tightlist
  \item
    Graphs, tables and text
  \item
    Maps
  \item
    Websites
  \item
    Shiny
  \end{itemize}
\item
  Hunting and gathering

  \begin{itemize}
  \tightlist
  \item
    Sampling and survey essentials
  \item
    APIs, scraping, PDFs and text
  \item
    RCTs, A/B testing
  \item
    Implementing surveys
  \item
    Open data
  \item
    Other
  \end{itemize}
\item
  Clean

  \begin{itemize}
  \tightlist
  \item
    Cleaning and preparing
  \item
    Storage and retrieval
  \item
    Dissemination and protection
  \end{itemize}
\item
  Model

  \begin{itemize}
  \tightlist
  \item
    Exploratory data analysis
  \item
    Regression essentials
  \item
    Matching and difference-in-differences
  \item
    Instrumental variables
  \item
    Regression discontinuity design
  \item
    Poll of polls
  \item
    Multilevel regression with post-stratification
  \item
    Text
  \end{itemize}
\item
  Other

  \begin{itemize}
  \tightlist
  \item
    Cloud
  \item
    Deploy
  \end{itemize}
\end{itemize}

\hypertarget{on-telling-stories}{%
\section{On telling stories}\label{on-telling-stories}}

Like many parents, when our child was born, one of the first things that my wife and I did regularly was read stories to him. In doing so we carried on a tradition that has occurred for millennia. Myths, fables, and fairy tales can be seen and heard all around us. Not only are they entertaining but they enable us to easily learn something about the world. While `The Very Hungry Caterpillar' may seem quite far from the world of quantitative analysis, there are similarities. Both are trying to tell the reader a story.

When conducting quantitative analysis we are trying to tell the reader a story that will convince them of something. It may be as exciting as predicting elections, as banal as increasing internet advertising click rates by 0.01 per cent, as serious as finding the cause of some disease, or as fun as forecasting the winner of a basketball game. In any case the key elements are the same. When writing fiction \href{https://en.wikipedia.org/wiki/Fiction_writing}{Wikipedia suggests} there are five key elements: character, plot, setting, theme, and style. When we are conducting quantitative analysis we have analogous concerns:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is the data? Who generated it and how?
\item
  What is the data trying to say? How can we let it say this?
\item
  What is the broader context surrounding the data? Where and when was it generated? Could other data have been generated?
\item
  What are we hoping others will see from this data?
\item
  How can you convince them of this?
\end{enumerate}

In the past, certain elements of telling stories with quantitative data were easier. For instance, experimental design has a long and robust tradition within traditional applications such as agricultural and medical sciences, physics, and chemistry. Student's t-distribution was identified by a chemist, William Sealy Gosset, who was working at Guinness and needed to assess the quality of the beer \citep{raju2005}! It would have been possible for him to randomly sample the beer and change one aspect at a time. Indeed, many of the fundamental statistical methods that we use today were developed in an agricultural setting. In the settings for which they were developed it was typically possible to establish control groups, randomize, and easily deal with any ethical concerns. In such a setting any subsequent story that is told with the resulting data is likely to be fairly convincing.

Unfortunately, such a set-up is rarely possible in modern applied statistics applications. On the other hand, there are many aspects that are easier today. For instance, we have well-developed statistical techniques, easier access to larger datasets, and open source statistical languages such as R. But the lack of ability to conduct traditional experiments means that we must turn to other aspects in order to tell a reader a convincing story about our data. These other aspects allow us to tell convincing stories even in the absence of a traditional experimental set-up.

\hypertarget{telling-stories-with-data}{%
\section{Telling stories with data}\label{telling-stories-with-data}}

The aim of these notes is to equip you with everything you need to be able to write short(ish), technical, memos, that convince a reader of the story you are telling. These notes encourage research-based, independent learning. This means that you should develop your own questions and answer them to the extent that you can. We focus on methods that can provide convincing stories even when we cannot conduct traditional experiments. Importantly, these approaches do not rely on `big data' (which is widely known by practitioners to not be a panacea \citep{meng2018statistical}), but instead on better using the data that are available. The purpose of the notes is to allow you to tell convincing stories using data and quantitative analysis. They blend theory and case studies to equip you to with practical skills, a sophisticated workflow, and an appreciation for how more-advanced methods build on what is covered here.

Data science is multi-disciplinary. It takes the `best' bits from fields such as statistics, data visualisation, programming, and experimental design (to name a few). As such, data science projects require a blend of these skills. These are hands-on notes in which you will learn these skills by conducting research projects using real-world data. This means that you will:

\begin{itemize}
\tightlist
\item
  obtain and clean relevant datasets;
\item
  develop your own research questions;
\item
  use statistical techniques to answer those questions; and
\item
  communicate your results in a meaningful way.
\end{itemize}

These notes were developed in collaboration with professional data scientists as well as academics from a variety of fields. They are designed around approaches that are used extensively in academia, government, and industry. Furthermore, they include many aspects, such as data cleaning and communication, that are critical, but rarely taught. However, these notes do not contain everything that you need. Your learning must be `active' when using these notes because that is the way you will continue to learn through the rest of your life and career. You need to seek out additional information, critically evaluate it, and apply it to your situation.

The workflow that we follow in these notes is:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Research question development.
\item
  Data collection.
\item
  Data cleaning.
\item
  Exploratory data analysis.
\item
  Statistical modelling.
\item
  Evaluation.
\item
  Communication.
\item
  Reproduce.
\end{enumerate}

All of these aspects are critical to being able to convince a reader of your story. Your ability to convince them of your story depends on the quality of all aspects of your workflow.

If we were to expand on this workflow then we roughly get the chapters that are covered in these notes, although they are re-ordered as necessary. From the first chapter we will have a workflow (make a graph then write about it convincingly) that allows us to tell a convincing (albeit likely basic) story. In each subsequent chapter we add aspects and depth to our workflow that will allow us to speak with increasing sophistication and credibility.

This workflow also aligns nicely with the skills that are sought in data scientists. For instance, Mango Solutions, a UK data science consultancy, describes `the six core capabilities of data scientists' as: 1. communicator; 2. data-wrangler; 3. programmer; 4. technologist; 5. modeller; and 6. visualiser \citep{mangosolutions}.

These notes are also designed to enable you to build a portfolio of work that you could show to a potential employer. This is arguably the most important thing that you should be doing. \citep[p.~55]{robinsonnolis2020} describe a portfolio as `a set of data science projects that you can show to people so they can see what kind of data science work you can do'. They describe this as a `step {[}that{]} can really help you be successful'.

\hypertarget{software}{%
\subsection{Software}\label{software}}

The software that we use in these notes is R \citep{citeR}. This language was chosen because it is open-source, widely used, general enough to cover the entire workflow, yet specific enough to have plenty of the tools that we need for statistical analysis built in. We do not assume that you have used R before, and so another reason for selecting R for these notes is the community of R users which is, in general, especially welcoming of new-comers and there are a lot of great beginner-friendly materials available.

If you don't have a programming language, then R is a great one to start with. If you have a preferred programming language already, then it wouldn't hurt to pick up R as well. That said, if you have a good reason to prefer another open source programming language (for instance you use Python daily at work) then you may wish to stick with that. However, all examples in these notes are in R.

Please download R and R Studio onto your own computer. You can download R for free here: \url{http://cran.utstat.utoronto.ca/}, and you can download R Studio Desktop for free here: \url{https://rstudio.com/products/rstudio/download/\#download}.

Please also create an account on R Studio Cloud: \url{https://rstudio.cloud/}. This will allow you to run R in the cloud, which will be helpful when we are getting started.

\hypertarget{assumed-background}{%
\subsection{Assumed background}\label{assumed-background}}

These notes assume familiarity with first-year statistics. For instance, if you have a taken a course or two where you covered hypothesis testing and similar concepts then that should be enough. That said, enthusiasm and interest can take you pretty far, so if you've got those then don't worry about too much else.

\hypertarget{structure-1}{%
\subsection{Structure}\label{structure-1}}

These notes are structured around a fairly dense 12-week course. Each chapter contains a list of required reading, as well as a list of recommended reading for those who are interested in the topic and want a starting place for further exploration. All chapters contain a summary of the key concepts and skills that are developed in that chapter. Code and technical chapters additionally contain a list of the main packages and functions that are used in the chapter. Many of the chapters also have a pre-quiz. This is a short quiz that you should complete after doing the required readings, but before going through the chapter to test your knowledge. After completing the chapter, you should go back through the lists and the pre-quiz to make sure that you understand each aspect.

There are problem sets throughout these notes. These are opportunities for you to conduct your own research on a topic that is of interest to you. Although the initial problem sets require you to use data from the Toronto Open Data Portal (\url{https://open.toronto.ca/}), after those first few you are able to use any appropriate dataset. Although open-ended research may be new to you, the extent to which you are able to develop your own questions, use quantitative methods to explore them, and communicate your story to a reader, is the true measure of the success of these notes.

\hypertarget{acknowledgements}{%
\section{Acknowledgements}\label{acknowledgements}}

Many people gave generously of their time, code, and data to help develop these notes.

Thank you to \href{https://www.monicaalexander.com/}{Monica Alexander}, Michael Chong, and \href{https://sharla.party/}{Sharla Gelfand} for allowing their code to be used.

Thank you to Kelly Lyons, Hareem Naveed, and Periklis Andritsos for helpful comments.

These notes have greatly benefited from the notes and teaching materials of others that are freely available online, especially:

\begin{itemize}
\tightlist
\item
  \href{https://cbail.github.io/textasdata/Text_as_Data.html}{Chris Bail}'s \emph{Text as Data};
\item
  \href{https://www.andrewheiss.com/}{Andrew Heiss}'s \emph{Program Evaluation for Public Service};
\item
  \href{https://grantmcdermott.com/}{Grant McDermott}'s \emph{Data Science for Economists};
\item
  \href{https://mimno.infosci.cornell.edu/info3350/}{David Mimno}'s \emph{Text Mining for History and Literature}
\item
  \href{http://edrub.in/}{Ed Rubin}'s \emph{PhD Econometrics (III)} and \emph{Introduction to Econometrics (II)};
\end{itemize}

Thank you to the following students who identified specific improvements in these notes: Aaron Miller, Amy Farrow, Cesar Villarreal Guzman, Faria Khandaker, Hong Shi, Mounica Thanam, and Wijdan Tariq.

Finally, thank you to the Winter 2020 and 2021 INF2178 and Fall 2020 Term STA304 students at the University of Toronto, whose feedback greatly improved all aspects.

\hypertarget{contact}{%
\section{Contact}\label{contact}}

Any comments or suggestions on these notes would be welcomed. You can contact me: \href{mailto:rohan.alexander@utoronto.ca}{\nolinkurl{rohan.alexander@utoronto.ca}}.

\hypertarget{drinking-from-a-fire-hose}{%
\chapter{Drinking from a fire hose}\label{drinking-from-a-fire-hose}}

\textbf{Required reading}

\begin{itemize}
\tightlist
\item
  Barrett, Malcolm, 2021, `Data science as an atomic habit', 16 January, \url{https://malco.io/2021/01/04/data-science-as-an-atomic-habit/}.
\item
  Bryan, Jennifer and Jim Hester, 2020, \emph{What they Forgot to Teach You About R}, Chapters 1 to 5, \url{https://rstats.wtf/debugging-r-code.html}.
\item
  Gelfand, Sharla, 2019, `Cleaning up after the federal election', \url{https://sharla.party/talk/2019-10-24-uoft-brown-bag/}.
\item
  Hoa, Karen, 2019, 'This is how AI bias really happens - and why it's so hard to fix, \emph{MIT Technology Review}, 4 February, \url{https://www.technologyreview.com/s/612876/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix/}.
\item
  Keyes, Os, 2019, `Counting the Countless', \emph{Real Life}, 8 April, freely available at: \url{https://reallifemag.com/counting-the-countless/}.
\item
  Wickham, Hadley, and Garrett Grolemund, 2017, \emph{R for Data Science}, Chapters 3 - 6, 8, 10, 11, 13, 14, 15, and 18, \url{https://r4ds.had.co.nz/}.
\end{itemize}

\textbf{Required viewing}

\begin{itemize}
\tightlist
\item
  Kuriwaki, Shiro, 2020, `Defining Custom Functions in R', \emph{Vimeo}, 2 February, \url{https://vimeo.com/388825332}.
\item
  Register, Yim, 2020, `Data Science Ethics in 6 Minutes', \emph{YouTube}, 29 December, \url{https://youtu.be/mA4gypAiRYU}.
\end{itemize}

\textbf{Alternative reading}

There are a lot of great alternative `getting started with R' type materials. Depending on your background and interests you may find some of the following useful:

\begin{itemize}
\tightlist
\item
  Arnold, Taylor, and Lauren Tilton, 2015, \emph{Humanities Data in R}, Springer, Chapters 1 to 5.
\item
  Hall, Megan, 2019, `An Introduction to R With Hockey Data', \url{https://hockey-graphs.com/2019/12/11/an-introduction-to-r-with-hockey-data/}.
\item
  Hanretty, Chris, 2020, `ConveRt', slides \url{http://chrishanretty.co.uk/conveRt/\#1}.
\item
  Phillips, Nathaniel D., 2018, \emph{YaRrr! The Pirate's Guide to R}, Chapter 2, \url{https://bookdown.org/ndphillips/YaRrr/started.html}.
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
  Alexander, Monica, 2019, `The concentration and uniqueness of baby names in Australia and the US', \url{https://www.monicaalexander.com/posts/2019-20-01-babynames/}.
\item
  Hvitfeldt, Emil, 2020, `Emoji in ggplot2', \url{https://www.hvitfeldt.me/blog/real-emojis-in-ggplot2/}.
\item
  Pavlik, Kaylin, 2018, `Dairy Queen Deserts in Minnesota', \url{https://www.kaylinpavlik.com/dairy-queen-deserts/}.
\item
  `R Studio Cloud Guide', \url{https://rstudio.cloud/learn/guide}.
\item
  Scherer, Cédric, 2019, `Best TidyTuesday 2019', \url{https://cedricscherer.netlify.com/2019/12/30/best-tidytuesday-2019/}.
\item
  Silge, Julia, 2019, `Reordering and facetting for ggplot2', \url{https://juliasilge.com/blog/reorder-within/}.
\item
  Smale, David, 2019, `Happy Days', \url{https://davidsmale.netlify.com/portfolio/happy-days/}.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{ggplot2}
\item
  \texttt{tidyverse}
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\tightlist
\item
  R is fun and allows you to accomplish really interesting projects.
\item
  But like any language it is a slow path to mastery.
\item
  The way to learn is to start with a really small project in mind, and break down the steps required to achieve it. Look at other people's code to work out how you might deal with the steps. Copy, paste, and modify someone else's code to achieve each step. Don't worry about perfection, just worry about achieving each step. Complete that project and move onto the next project. Rinse and repeat. Each project you'll get a little better.
\item
  Tibbles
\item
  Importing data
\item
  Joining data
\item
  Strings
\item
  Factors
\item
  Dates
\item
  Pivot
\end{itemize}

\textbf{Key functions}

\begin{itemize}
\tightlist
\item
  \texttt{\%\textgreater{}\%} `pipe'
\item
  \texttt{dplyr::arrange()}
\item
  \texttt{dplyr::filter()}
\item
  \texttt{dplyr::group\_by()}
\item
  \texttt{dplyr::mutate()}
\item
  \texttt{dplyr::select()}
\item
  \texttt{dplyr::summarise()}
\item
  \texttt{dplyr::ungroup()}
\item
  \texttt{ggplot::facet\_wrap()}
\item
  \texttt{ggplot::geom\_histogram()}
\item
  \texttt{class()}
\item
  \texttt{dplyr::case\_when()}
\item
  \texttt{ggplot::facet\_wrap()}
\item
  \texttt{ggplot::geom\_density()}
\item
  \texttt{ggplot::geom\_histogram()}
\item
  \texttt{ggplot::geom\_point()}
\item
  \texttt{janitor::clean\_names()}
\item
  \texttt{skimr::skim()}
\item
  \texttt{tidyr::pivot\_longer()}
\item
  \texttt{tidyr::pivot\_wider()}
\end{itemize}

\textbf{Quiz}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  If I had a dataset with the following columns: \texttt{name}, \texttt{age} and wanted to focus on \texttt{name}, then which verb should I use (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{tidyverse::select()}.
  \item
    \texttt{tidyverse::mutate()}.
  \item
    \texttt{tidyverse::filter()}.
  \item
    \texttt{tidyverse::rename()}.
  \end{enumerate}
\item
  If I want to cite R then how do I find a recommended citation (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{cite(\textquotesingle{}R\textquotesingle{})}.
  \item
    \texttt{cite()}.
  \item
    \texttt{citation(\textquotesingle{}R\textquotesingle{})}.
  \item
    \texttt{citation()}.
  \end{enumerate}
\item
  According to Register, 2020, data decisions affect (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Real people.
  \item
    No one.
  \item
    Those in the training set.
  \item
    Those in the test set.
  \end{enumerate}
\item
  In your own words, what is data science?
\item
  According to Keyes, 2019, what is perhaps a more accurate accurate definition of data science (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    `The inhumane reduction of humanity down to what can be counted.';
  \item
    `The quantitative analysis of large amounts of data for the purpose of decision-making.';
  \item
    `Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data.'
  \end{enumerate}
\item
  Imagine that you have a job in which including `race' as an explanatory variable improves the performance of your model. What types of issues would you consider when deciding whether to include this variable in production? What if the variable was sexuality? Please be sure to refer to Mullainathan, 2019, in your answer.
\item
  What are three advantages of R? What are three disadvantages?
\item
  What is R Studio?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    An integrated development environment (IDE)
  \item
    A closed source paid program
  \item
    A programming language created by Guido van Rossum
  \item
    A statistical programming language
  \end{enumerate}
\item
  What is R?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A open source statistical programming language
  \item
    A programming language created by Guido van Rossum
  \item
    A closed source statistical programming language
  \item
    An integrated development environment (IDE)
  \end{enumerate}
\item
  Which of the following are not tidyverse verbs (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    select().
  \item
    filter().
  \item
    arrange().
  \item
    mutate().
  \item
    visualize().
  \end{enumerate}
\item
  If I wanted to make a new column which verb should I use (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    select().
  \item
    filter().
  \item
    arrange().
  \item
    mutate().
  \item
    visualize().
  \end{enumerate}
\item
  If I wanted to focus on particular rows which verb should I use (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    select().
  \item
    filter().
  \item
    arrange().
  \item
    mutate().
  \item
    summarise()
  \end{enumerate}
\item
  If I wanted a summary of the data that gave me the mean by sex, which two verbs should I use (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    summarise().
  \item
    filter().
  \item
    arrange().
  \item
    mutate().
  \item
    group\_by().
  \end{enumerate}
\item
  What are the three key aspects of the grammar of graphics (select all)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    data.
  \item
    aesthetics.
  \item
    type.
  \item
    geom\_histogram().
  \end{enumerate}
\item
  What is not one of the four challenges for mitigating bias mentioned in Hao 2019 (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Unknown unknowns.
  \item
    Imperfect processes.
  \item
    The definitions of fairness.
  \item
    Lack of social context.
  \item
    Disinterest given profit considerations.
  \end{enumerate}
\item
  What would be the output of \texttt{class("edward")} (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    ``numeric''.
  \item
    ``character''.
  \item
    ``data.frame''.
  \item
    ``vector''.
  \end{enumerate}
\item
  How can I simulate 10,000 draws from a normal distribution with a mean of 27 and a standard deviation of 3 (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{rnorm(10000,\ mean\ =\ 27,\ sd\ =\ 3)}.
  \item
    \texttt{rnorm(27,\ mean\ =\ 10000,\ sd\ =\ 3)}.
  \item
    \texttt{rnorm(3,\ mean\ =\ 10000,\ sd\ =\ 27)}.
  \item
    \texttt{rnorm(27,\ mean\ =\ 3,\ sd\ =\ 1000)}.
  \end{enumerate}
\end{enumerate}

\hypertarget{hello-world}{%
\section{Hello world}\label{hello-world}}

To jump in we will get some data from the wild, make a graph with it, and then use this to tell a story. Some of the code may be a bit unfamiliar to you if it's your first-time using R. It'll all soon be familiar. But the only way to learn how to code is to code. Please try to get this working on your own computer, typing out (not copy/pasting) all the code that you need. It's important and normal to realise that you're going to be bad at this for a while.

\begin{quote}
Whenever you're learning a new tool, for a long time, you're going to suck\ldots{} But the good news is that is typical; that's something that happens to everyone, and it's only temporary.

Hadley Wickham as quoted by \citet{citeBarrett}.
\end{quote}

One of the great things about graphs is that sometimes this is all you need to have a convincing story, as Figure \ref{fig:covid} from \citet{ftcoronavirus} show.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/covid} \caption{New confirmed cases of Covid-19 in United States, United Kingdom, Canada and Australia, as at 22 December 2020.}\label{fig:covid}
\end{figure}

In this section we are going to focus on making a table and a graph from our data. Although you will be guided thoroughly to achieve this, hopefully by seeing the power of quantitative analysis with R you will be motivated to stick with it when you run into difficulties later on.

\hypertarget{case-study---canadian-elections}{%
\section{Case study - Canadian elections}\label{case-study---canadian-elections}}

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/county_election} \caption{The County Election (1851–52), by George Caleb Bingham (American, 1811 - 1879), as downloaded from https://artvee.com/dl/the-county-election.}\label{fig:canadianelections}
\end{figure}

\hypertarget{getting-started}{%
\subsection{Getting started}\label{getting-started}}

To get started you should open a new R Markdown file (File -\textgreater{} New File -\textgreater{} R Markdown). As this is our first attempt at using R in the wild, we will just have everything in the one R Markdown document. (In later projects we will move to a more robust set-up.) Then you should create a new R code chunk (keyboard shortcut: Command + Option + I) and add some preamble documentation. I like to specify the purpose of the document, who the author is and their contact details, when the file was written or last updated, and pre-requisites that the file relies on. You may also like to include a license, and list outstanding issues or todos. Remember that in R, lines that start with `\#' are comments - they won't run.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#### Preamble ####}
\CommentTok{# Purpose: Read in voting data from the 2019 Canadian Election and output a }
\CommentTok{# dataset that can be used for analysis.}
\CommentTok{# Author: Rohan Alexander}
\CommentTok{# Email: rohan.alexander@utoronto.ca}
\CommentTok{# Date: 9 January 2019}
\CommentTok{# Prerequisites: Need the text file from the Canadian elections website}
\CommentTok{# Issues: }
\CommentTok{# To do:}
\end{Highlighting}
\end{Shaded}

After this I typically set-up my workspace. This usually involves installing and/or reading in any packages, and possibly updating them. Remember that you only need to install a package once for each computer. But you need to call it every time you want to use it. (Here I've added excessive comments so that you know what is going on and why - in general I wouldn't explain what tidyverse is.)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#### Workspace set-up ####}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{) }\CommentTok{# Only need to do this once}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"janitor"}\NormalTok{) }\CommentTok{# Only need to do this once}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"here"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

In this case we are going to use \texttt{tidyverse} \citet{Wickham2017}, \texttt{janitor} \citet{janitor}, and \texttt{here} \citet{here}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#### Workspace set-up ####}
\CommentTok{# tidyverse is a collection of packages}
\CommentTok{# Try ?tidyverse to see more}
\KeywordTok{library}\NormalTok{(tidyverse) }\CommentTok{# Calls the tidyverse - need to do this each time.}
\KeywordTok{library}\NormalTok{(janitor) }\CommentTok{# janitor helps us clean datasets}
\KeywordTok{library}\NormalTok{(here) }\CommentTok{# here helps us to know where files are}
\CommentTok{# update.packages() # You can uncomment this if you want to update your packages. }
\end{Highlighting}
\end{Shaded}

\hypertarget{get-the-data}{%
\subsection{Get the data}\label{get-the-data}}

We read in the dataset from the Elections Canada website. We can actually pass a website to the \texttt{read\_tsv()} function, which saves a lot of time.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#### Read in the data ####}
\CommentTok{# Read in the data using read_tsv from the readr package (part of the tidyverse)}
\CommentTok{# The '<-' is assigning the output of readr::read_tsv to a object called raw_data. }
\NormalTok{raw_elections_data <-}\StringTok{ }\NormalTok{readr}\OperatorTok{::}\KeywordTok{read_tsv}\NormalTok{(}\DataTypeTok{file =} \StringTok{"http://enr.elections.ca/DownloadResults.aspx"}\NormalTok{,}
                            \DataTypeTok{skip =} \DecValTok{1}\NormalTok{) }
\CommentTok{# There is some debris on the first line so we skip them.}
\CommentTok{# We have read the data from the Elections Canada website. We may like to save }
\CommentTok{# it just in case something happens and they move it. }
\KeywordTok{write_csv}\NormalTok{(raw_elections_data, }\KeywordTok{here}\NormalTok{(}\StringTok{"inputs/data/canadian_voting.csv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

(Note that Elections Canada updates this link with the latest elections. When I run this on 31 December 2020, I get the results of a Toronto by-election. While I'll certainly update these notes from time to time, it may be that there's been an election between now and when you run these notes, so your specific results may be slightly different.)

\hypertarget{clean-the-data}{%
\subsection{Clean the data}\label{clean-the-data}}

Now we'd like to clean the data so that we can use it.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#### Basic cleaning ####}
\NormalTok{raw_elections_data <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\KeywordTok{here}\NormalTok{(}\StringTok{"inputs/data/canadian_voting.csv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## -- Column specification --------------------------------------------------------
## cols(
##   `Electoral district number - Numéro de la circonscription` = col_character(),
##   `Electoral district name` = col_character(),
##   `Nom de la circonscription` = col_character(),
##   `Type of results*` = col_character(),
##   `Type de résultats**` = col_character(),
##   `Surname - Nom de famille` = col_character(),
##   `Middle name(s) - Autre(s) prénom(s)` = col_logical(),
##   `Given name - Prénom` = col_character(),
##   `Political affiliation` = col_character(),
##   `Appartenance politique` = col_character(),
##   `Votes obtained - Votes obtenus` = col_double(),
##   `% Votes obtained - Votes obtenus %` = col_double(),
##   `Rejected ballots - Bulletins rejetés***` = col_double(),
##   `Total number of ballots cast - Nombre total de votes déposés` = col_double()
## )
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# If you called the library (as we did) then you don't need to use this set-up }
\CommentTok{# of janitor::clean_names, you could just use clean_names, but I'm making it }
\CommentTok{# explicit here, but won't in the future.}
\NormalTok{cleaned_elections_data <-}\StringTok{ }\NormalTok{janitor}\OperatorTok{::}\KeywordTok{clean_names}\NormalTok{(raw_elections_data)}
\CommentTok{# One thing to notice for those who have a Stata background is that we just }
\CommentTok{# overwrote the name - that's fine in R.}

\CommentTok{# The pipe operator - %>% - pushes the output from one line to be an input to the }
\CommentTok{# next line.}
\NormalTok{cleaned_elections_data <-}\StringTok{ }
\StringTok{  }\NormalTok{cleaned_elections_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\CommentTok{# Filter to only have certain rows}
\StringTok{  }\KeywordTok{filter}\NormalTok{(type_of_results }\OperatorTok{==}\StringTok{ "validated"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\CommentTok{# Select only certain columns}
\StringTok{  }\KeywordTok{select}\NormalTok{(electoral_district_number_numero_de_la_circonscription,}
\NormalTok{         electoral_district_name,}
\NormalTok{         political_affiliation,}
\NormalTok{         surname_nom_de_famille,}
\NormalTok{         percent_votes_obtained_votes_obtenus_percent}
\NormalTok{         ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\CommentTok{# Rename the columns to be a bit shorter}
\StringTok{  }\KeywordTok{rename}\NormalTok{(}\DataTypeTok{riding_number =}\NormalTok{ electoral_district_number_numero_de_la_circonscription,}
         \DataTypeTok{riding =}\NormalTok{ electoral_district_name,}
         \DataTypeTok{party =}\NormalTok{ political_affiliation,}
         \DataTypeTok{surname =}\NormalTok{ surname_nom_de_famille,}
         \DataTypeTok{votes =}\NormalTok{ percent_votes_obtained_votes_obtenus_percent)}

\KeywordTok{head}\NormalTok{(cleaned_elections_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 5
##   riding_number riding         party                    surname     votes
##   <chr>         <chr>          <chr>                    <chr>       <dbl>
## 1 35108         Toronto Centre People's Party - PPC     Bawa          1.1
## 2 35108         Toronto Centre Free Party Canada        Cappelletti   0.3
## 3 35108         Toronto Centre NDP-New Democratic Party Chang        17  
## 4 35108         Toronto Centre Independent              Clarke        0.5
## 5 35108         Toronto Centre Liberal                  Ien          42  
## 6 35108         Toronto Centre Libertarian              Komar         0.5
\end{verbatim}

Finally we may like to save our cleaned dataset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#### Save ####}
\NormalTok{readr}\OperatorTok{::}\KeywordTok{write_csv}\NormalTok{(cleaned_elections_data, }\StringTok{"outputs/data/cleaned_elections_data.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{make-a-graph}{%
\subsection{Make a graph}\label{make-a-graph}}

First we need to read in the dataset, we then filter the number of parties to a smaller number.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#### Read in the data ####}
\NormalTok{cleaned_elections_data <-}\StringTok{ }
\StringTok{  }\NormalTok{readr}\OperatorTok{::}\KeywordTok{read_csv}\NormalTok{(}\StringTok{"outputs/data/cleaned_elections_data.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## -- Column specification --------------------------------------------------------
## cols(
##   riding_number = col_double(),
##   riding = col_character(),
##   party = col_character(),
##   surname = col_character(),
##   votes = col_double()
## )
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Make a graph just considers Toronto riding}
\NormalTok{cleaned_elections_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(party }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Bloc Québécois"}\NormalTok{, }
                      \StringTok{"Conservative"}\NormalTok{, }
                      \StringTok{"Liberal"}\NormalTok{,}
                      \StringTok{"NDP-New Democratic Party"}\NormalTok{)}
\NormalTok{  ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ riding, }\DataTypeTok{y =}\NormalTok{ votes, }\DataTypeTok{color =}\NormalTok{ party)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}\StringTok{ }\CommentTok{# Make the theme neater}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{90}\NormalTok{, }\DataTypeTok{hjust =} \DecValTok{1}\NormalTok{)) }\OperatorTok{+}\StringTok{ }\CommentTok{# Change the angle}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Riding"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Votes (%)"}\NormalTok{,}
       \DataTypeTok{color =} \StringTok{"Party"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-8-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Save the graph}
\KeywordTok{ggsave}\NormalTok{(}\StringTok{"outputs/figures/toronto_results.pdf"}\NormalTok{, }\DataTypeTok{width =} \DecValTok{40}\NormalTok{, }\DataTypeTok{height =} \DecValTok{20}\NormalTok{, }\DataTypeTok{units =} \StringTok{"cm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{make-a-table}{%
\subsection{Make a table}\label{make-a-table}}

There are an awful lot of ways to make a table in R. First we'll try the built-in function \texttt{summary()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#### Read in the data ####}
\NormalTok{cleaned_elections_data <-}\StringTok{ }
\StringTok{  }\NormalTok{readr}\OperatorTok{::}\KeywordTok{read_csv}\NormalTok{(}\StringTok{"outputs/data/cleaned_elections_data.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## -- Column specification --------------------------------------------------------
## cols(
##   riding_number = col_double(),
##   riding = col_character(),
##   party = col_character(),
##   surname = col_character(),
##   votes = col_double()
## )
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#### Make some tables ####}
\CommentTok{# Try some different default summary table}
\KeywordTok{summary}\NormalTok{(cleaned_elections_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  riding_number      riding             party             surname         
##  Min.   :35108   Length:15          Length:15          Length:15         
##  1st Qu.:35108   Class :character   Class :character   Class :character  
##  Median :35108   Mode  :character   Mode  :character   Mode  :character  
##  Mean   :35112                                                           
##  3rd Qu.:35118                                                           
##  Max.   :35118                                                           
##      votes      
##  Min.   : 0.20  
##  1st Qu.: 0.55  
##  Median : 3.60  
##  Mean   :13.34  
##  3rd Qu.:24.85  
##  Max.   :45.70
\end{verbatim}

Now we can try a \texttt{group\_by()} and \texttt{summarise()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Make our own}
\NormalTok{cleaned_elections_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\CommentTok{# Using group_by and summarise means that whatever summary statistics we }
\StringTok{  }\CommentTok{# construct will be on a party basis. We could group_by multiple variables and}
\StringTok{  }\CommentTok{# similarly, we could create a bunch of different other summary statistics.}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(party) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{min =} \KeywordTok{min}\NormalTok{(votes),}
            \DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(votes),}
            \DataTypeTok{max =} \KeywordTok{max}\NormalTok{(votes))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `summarise()` ungrouping output (override with `.groups` argument)
\end{verbatim}

\begin{verbatim}
## # A tibble: 9 x 4
##   party                      min  mean   max
##   <chr>                    <dbl> <dbl> <dbl>
## 1 Conservative               5.7 23.8   41.8
## 2 Free Party Canada          0.3  0.3    0.3
## 3 Green Party                2.6 17.7   32.7
## 4 Independent                0.5  0.55   0.6
## 5 Liberal                   42   43.8   45.7
## 6 Libertarian                0.5  0.5    0.5
## 7 NDP-New Democratic Party   5.8 11.4   17  
## 8 No Affiliation             0.2  0.2    0.2
## 9 People's Party - PPC       1.1  2.35   3.6
\end{verbatim}

\hypertarget{case-study---toronto-homelessness}{%
\section{Case study - Toronto homelessness}\label{case-study---toronto-homelessness}}

Toronto has a large homeless population, and of course given the pandemic and winter it is critical that there are enough places in shelters. Unfortunately, as we will see in this case study, there are not enough places. However, the one good thing is that we have the data to see this is a problem.

In this case study we are going to use data on the number of people in Toronto shelters to make a graph of usage. This will also introduce us to Tidy Tuesday! In my experience, people are most successful at `learning R' when they are learning it to achieve something else. If you're in a university course then that might be `pass the course', but often it's nice to have some other projects. TidyTuesday is a weekly event in which the R community comes together around a dataset and shares code and approaches. You can learn more about it here: \url{https://github.com/rfordatascience/tidytuesday}.

\hypertarget{getting-started-1}{%
\subsection{Getting started}\label{getting-started-1}}

Again, open a new R Markdown file: (File -\textgreater{} New File -\textgreater{} R Markdown). Update the details so that they reflect your own.

Again, add some top matter with some comments and explanations of the code.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#### Preamble ####}
\CommentTok{# Purpose: Read in Toronto homelessness data and output a graph.}
\CommentTok{# Author: Rohan Alexander}
\CommentTok{# Email: rohan.alexander@utoronto.ca}
\CommentTok{# Date: 22 December 2020}
\CommentTok{# Prerequisites: }
\CommentTok{# Issues: }
\CommentTok{# To do:}
\end{Highlighting}
\end{Shaded}

I want to talk a little about the libraries this time. Libraries are bits of code that other people have written. There are a few common ones that you'll see regularly, especially the \texttt{tidyverse}. To use a package we first have to install it and then we need to load it. Jenny Bryan has a wonderful analogy of installing a lightbulb - \texttt{install.packages("tidyverse")}. You only need to do this once, but then if you want light then you need to turn on the switch - \texttt{library(tidyverse)}. So because we installed everything earlier we won't need to do it again, we can just call the library.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#### Workspace set-up ####}
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

Given that a lot of people gave up their time to make \texttt{R} and the packages, it's important to cite them. Luckily, it's easy to get the information that you need to properly cite them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# To get the citation for R run:}
\KeywordTok{citation}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## To cite R in publications use:
## 
##   R Core Team (2020). R: A language and environment for statistical
##   computing. R Foundation for Statistical Computing, Vienna, Austria.
##   URL https://www.R-project.org/.
## 
## A BibTeX entry for LaTeX users is
## 
##   @Manual{,
##     title = {R: A Language and Environment for Statistical Computing},
##     author = {{R Core Team}},
##     organization = {R Foundation for Statistical Computing},
##     address = {Vienna, Austria},
##     year = {2020},
##     url = {https://www.R-project.org/},
##   }
## 
## We have invested a lot of time and effort in creating R, please cite it
## when using it for data analysis. See also 'citation("pkgname")' for
## citing R packages.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# And to get the citation for a package run that function with the package name. For instance:}
\KeywordTok{citation}\NormalTok{(}\StringTok{'tidyverse'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##   Wickham et al., (2019). Welcome to the tidyverse. Journal of Open
##   Source Software, 4(43), 1686, https://doi.org/10.21105/joss.01686
## 
## A BibTeX entry for LaTeX users is
## 
##   @Article{,
##     title = {Welcome to the {tidyverse}},
##     author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},
##     year = {2019},
##     journal = {Journal of Open Source Software},
##     volume = {4},
##     number = {43},
##     pages = {1686},
##     doi = {10.21105/joss.01686},
##   }
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Again, don't worry too much about the details - we'll get into them later.}
\end{Highlighting}
\end{Shaded}

\hypertarget{get-the-data-1}{%
\subsection{Get the data}\label{get-the-data-1}}

We are going to grab some data that has been made available about Toronto homeless shelters. Again, don't worry too much about the details for now, but what we are saying here, is that there's a CSV that has been made available to us on GitHub and this code downloads it to our own computer. After we download it we can quickly look at the data using \texttt{head()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{toronto_shelters <-}\StringTok{ }
\StringTok{  }\NormalTok{readr}\OperatorTok{::}\KeywordTok{read_csv}\NormalTok{(}
    \StringTok{'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-12-01/shelters.csv'}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## -- Column specification --------------------------------------------------------
## cols(
##   id = col_double(),
##   occupancy_date = col_datetime(format = ""),
##   organization_name = col_character(),
##   shelter_name = col_character(),
##   shelter_address = col_character(),
##   shelter_city = col_character(),
##   shelter_province = col_character(),
##   shelter_postal_code = col_character(),
##   facility_name = col_character(),
##   program_name = col_character(),
##   sector = col_character(),
##   occupancy = col_double(),
##   capacity = col_double()
## )
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(toronto_shelters)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 13
##      id occupancy_date      organization_na~ shelter_name shelter_address
##   <dbl> <dttm>              <chr>            <chr>        <chr>          
## 1     1 2017-01-01 00:00:00 COSTI Immigrant~ COSTI Recep~ 100 Lippincott~
## 2     2 2017-01-01 00:00:00 Christie Ossing~ Christie Os~ 973 Lansdowne ~
## 3     3 2017-01-01 00:00:00 Christie Ossing~ Christie Os~ 973 Lansdowne ~
## 4     4 2017-01-01 00:00:00 Christie Refuge~ Christie Re~ 43 Christie St~
## 5     5 2017-01-01 00:00:00 City of Toronto  Birchmount ~ 1673 Kingston ~
## 6     6 2017-01-01 00:00:00 City of Toronto  Birkdale Re~ 1229 Ellesmere~
## # ... with 8 more variables: shelter_city <chr>, shelter_province <chr>,
## #   shelter_postal_code <chr>, facility_name <chr>, program_name <chr>,
## #   sector <chr>, occupancy <dbl>, capacity <dbl>
\end{verbatim}

\hypertarget{make-a-graph-1}{%
\subsection{Make a graph}\label{make-a-graph-1}}

The dataset is on a daily basis for each shelter. What I'd like to do is to get an overall picture of the availability of shelter places in Toronto. This code is based on code by \href{https://florencevdubois.github.io}{Florence Vallée-Dubois}\footnote{\url{https://github.com/florencevdubois/MyTidyTuesdays/blob/master/2020.12.01.R}} and \href{https://lisalendway.netlify.app}{Lisa Lendway}\footnote{\url{https://github.com/llendway/tidy_tuesday_in_thirty/blob/main/2020_12_01_tidy_tuesday.Rmd}}.

Now, I'd like to first do some data manipulation before we graph it this time. I'm going to introduce a few new functions here that we'll see a lot more soon. Again, don't worry if this doesn't all make sense right now. You're learning a brand-new language! Just try to focus on picking up a word or two and staying motivated!

Finally, because it's Christmas, we can grab a \href{https://github.com/elb0/decemberLB}{seasonally-appropriate theme} from my colleague \href{https://twitter.com/liza_bolton}{Liza Bolton}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# In contrast to the earlier packages, which were located in a central repository }
\CommentTok{# of packages called CRAN, Liza's is available on her GitHub. Again, don't worry }
\CommentTok{# about the details for now, it'll all be clarified later.}
\NormalTok{devtools}\OperatorTok{::}\KeywordTok{install_github}\NormalTok{(}\StringTok{"elb0/decemberLB"}\NormalTok{, }\DataTypeTok{ref =} \StringTok{"main"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Skipping install of 'decemberLB' from a github remote, the SHA1 (2cc38a25) has not changed since last install.
##   Use `force = TRUE` to force installation
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Once it's installed we call the library as usual.}
\KeywordTok{library}\NormalTok{(decemberLB)}

\NormalTok{toronto_shelters }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{tidyr}\OperatorTok{::}\KeywordTok{drop_na}\NormalTok{(occupancy, capacity) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# We only want rows that have data}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(occupancy_date, sector) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# We want to know the occupancy by date and sector}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{the_sum =} \KeywordTok{sum}\NormalTok{(occupancy),}
            \DataTypeTok{the_capacity =} \KeywordTok{sum}\NormalTok{(capacity),}
            \DataTypeTok{the_usage =}\NormalTok{ the_sum }\OperatorTok{/}\StringTok{ }\NormalTok{the_capacity, }\DataTypeTok{.groups =} \StringTok{'drop'}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ occupancy_date, }\DataTypeTok{y =}\NormalTok{ the_usage, }\DataTypeTok{color =}\NormalTok{ sector)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{group =}\NormalTok{ sector), }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{limits =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\OtherTok{NA}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{color =} \StringTok{"Type"}\NormalTok{,}
       \DataTypeTok{x =} \StringTok{"Date"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Occupancy rate"}\NormalTok{,}
       \DataTypeTok{title =} \StringTok{"Toronto shelters"}\NormalTok{,}
       \DataTypeTok{subtitle =} \StringTok{"Occupancy per day"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_december}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"xmas"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'
\end{verbatim}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-15-1.pdf}

\hypertarget{r-essentials}{%
\section{R essentials}\label{r-essentials}}

This section is the basics of using R. Some of it may not make sense at first, but these are commands that we will come back to throughout these notes. You should initially just go through this chapter quickly, noting aspects that you don't understand. Then start to play around with some of the initial case studies. Then maybe come back to this chapter. That way you will see how the various bits fit into context, and hopefully be more motivated to pick up various aspects. We will come back to everything in this chapter in more detail at some point in these notes.

R is an open source language that is useful for statistical programming

You can download R for free here: \url{http://cran.utstat.utoronto.ca/}, and you can download R Studio Desktop for free here: \url{https://rstudio.com/products/rstudio/download/\#download}.

When you are using R you will run into trouble at some point. To work through that trouble:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Look at the help file for the function by putting ? before the function e.g.~\texttt{?pivot\_wider}.
\item
  Check the class of your data, by \texttt{class(data\_set\$data\_column)}.
\item
  Check for typos.
\item
  Google the error.
\item
  Google what you are trying to do.
\item
  Restart R (\texttt{Session} -\textgreater{} \texttt{Restart\ R\ and\ Clear\ Output}).
\item
  Try to make a small example and see if you have the same issues.
\item
  Restart your computer.
\end{enumerate}

The past ten years or so of R have been characterised by the rise of the tidyverse. This is `\ldots{} an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.' \citet{tidyversewebsite}. There are three distinctions here: the original R language, typically referred to as `base'; the `tidyverse' which is a collection of packages that build on top of the R language; and other packages.

Pretty much everything that you can do in the tidyverse, you can also do in base. However, as the tidyverse was built especially for modern data science it is usually easier to use the tidyverse, especially when you are setting out. Additionally, pretty much everything that you can do in the tidyverse, you can also do with other packages. However, as the tidyverse is a coherent collection of packages, it is often easier to use the tidyverse, especially when you are setting out. Eventually you will start to see cases where it makes sense to trade-off the convenience and coherence of the tidyverse for some features of base or other packages. Indeed you'll see that at various points in these notes. For instance, the tidyverse can be slow, and so if you need to import thousands of CSVs then it can make sense to switch away from \texttt{read\_csv()}. That is great and the appropriate use of base and non-tidyverse packages, rather than dogmatic insistence on a solution, is a sign of your development as an applied statistician.

Get started by loading the \texttt{tidyverse} package.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

The general workflow that we will use involves:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Import
\item
  Tidy
\item
  Transforming, descriptive
\item
  Plot
\item
  Model
\item
  Repeat 3/4
\end{enumerate}

People like Keyes have tried to tell us this for a long time, but COVID-19 make it very clear to everyone - most of the data that we use will have humans at the heart of it. It's vitally important that you keep that in mind and grapple with it in everything that you do with R. It can be really easy to forget that almost every point in our dataset is likely a person.

\hypertarget{social-impact}{%
\section{Social impact}\label{social-impact}}

\begin{quote}
``We shouldn't have to think about the societal impact of our work because it's hard and other people can do it for us'' is a really bad argument. I stopped doing CV research because I saw the impact my work was having. I loved the work but the military applications and privacy concerns eventually became impossible to ignore. But basically all facial recognition work would not get published if we took Broader Impacts sections seriously. There is almost no upside and enormous downside risk. To be fair though i should have a lot of humility here. For most of grad school I bought in to the myth that science is apolitical and research is objectively moral and good no matter what the subject is.

Joe Redmon, \href{https://twitter.com/pjreddie/status/1230524770350817280}{20 February 2020}.
\end{quote}

Although the term `data science' is ubiquitous in academia, industry, and even more generally, it is difficult to define. One deliberately antagonistic definition of data science is `{[}t{]}he inhumane reduction of humanity down to what can be counted' \citep{keyes2009}. While purposefully controversial, this definition highlights one reason for the increased demand for data science and quantitative methods over the past decade---individuals and their behaviour are now at the heart of it. Many of the techniques have been around for many decades, but what makes them popular now is this human focus.

Unfortunately, even though much of the work may be focused on individuals, issues of privacy and consent, and ethical concerns more broadly, rarely seem front of mind. While there are some exceptions, in general, even at the same time as claiming that AI, machine learning, and data science are going to revolutionise society, consideration of these types of issues appears to have been largely treated as something that would be nice to have, rather than something that we may like to think of before we embrace the revolution.

For the most part, these are not new issues. In the sciences, there has been considerable recent ethical consideration around CRISPR technology and gene editing, but in an earlier time similar conversations were had, for instance, about Wernher von Braun being allowed to building rockets for the US. In medicine, of course, these concerns have been front-of-mind for some time. Data science seems determined to have its own Tuskegee syphilis experiment moment rather than think about and deal appropriately with these issues, based on the experiences of other fields, before they occur.

That said, there is some evidence that data scientists are beginning to be more concerned about the ethics surrounding the practice. For instance, NeurIPS, the most prestigious machine learning conference, now requires a statement on ethics to accompany all submissions.

\begin{quote}
In order to provide a balanced perspective, authors are required to include a statement of the potential broader impact of their work, including its ethical aspects and future societal consequences. Authors should take care to discuss both positive and negative outcomes.

\href{https://neurips.cc/Conferences/2020/CallForPapers}{NeurIPS call for papers}, as accessed 26 February 2020.
\end{quote}

Ethical considerations will be mentioned throughout these notes rather than clumped in one easily ignorable part that can be thrown away after `ethics week'. The purpose is not to prescriptively rule things in or out, but to provide an opportunity to raise some issues that should be front of mind. The variety of data science applications, the relative youth of the field, and the speed of change, mean that ethical considerations can sometimes be set aside when it comes to data science. This is in contrast to fields such as science, medicine, engineering, and accounting where there is a long history. Nonetheless it can helpful to think through some ethical considerations that you may encounter in the content of a usual data science project.

\begin{figure}
\includegraphics[width=0.9\linewidth]{figures/probability} \caption{Probability, from https://xkcd.com/881/.}\label{fig:unnamed-chunk-17}
\end{figure}

\hypertarget{r-r-studio-and-r-studio-cloud}{%
\section{R, R Studio, and R Studio Cloud}\label{r-r-studio-and-r-studio-cloud}}

My colleague \href{https://twitter.com/liza_bolton}{Liza Bolton} has a lovely analogy here on the relationship between R and R Studio which I really like. R is like a car engine and R Studio is like the car. Although some of us can use a car engine directly, most of us use a car to interact with the engine.

\hypertarget{r}{%
\subsection{R}\label{r}}

R - \url{https://www.r-project.org/} - is an open source and free programming language that is focused on general statistics. (Free in this context doesn't refer to a price of zero, but instead to `freedom', but it also does have a price of zero). This is in contrast with a open source programming language that is designed for general purpose, such as Python, or an open source programming language that is focused on probability, such as Stan. It was created by Ross Ihaka and Robert Gentleman at the University of Auckland in New Zealand. It is maintained by the R Core Team and changes to this `base' of code occur methodically and with concern given to a variety of different priorities.

If you are in Canada then you can download R here: \url{http://cran.utstat.utoronto.ca/}, if you are in Australia then you can download R here: \url{https://cran.csiro.au/}, otherwise you should go here - \url{https://cran.r-project.org/mirrors.html} - and find a location that suits you. (It doesn't really matter where you get it from, it's just that it may be slightly faster to use a closer option.)

Many people build on this stable base, to extend the capabilities of R to better and more quickly suit their needs. They do this by creating packages. Typically, although not always, a package is a collection R code, and this allows you to more easily do things that you want to do. These packages are managed by the Comprehensive R Archive Network (CRAN) - \url{https://cran.r-project.org/}, and other repositories. CRAN is built into the download of R that you just got, so you can use it straight away.

If you want to use a package then you need to firstly install it in your computer, and then you need to load it when you want to use it. \href{http://dicook.org/}{Di Cook}, who is a Professor of Business Analytics at Monash University in Australia, \href{https://twitter.com/visnut/status/1248087845589274624}{describes} this as analogous to a lightbulb: if you want light in your house, first you need to screw in the lightbulb, and you need to turn the switch on. You only need to screw in the lightbulb once per house, but you need to turn the switch on every time you want to use the light.

To install a package on your computer (again, you'll need to do this only once per computer) you use the code:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then when you want to use a package, you need to call it with this code:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

You can open R and use it on your computer. It is primarily designed to be interacted with through the command line. This is how I had to start with R, and it's fine, but it can be useful to have a richer environment than the command line provides. In particular, it can be useful to install an Integrated Development Environment (IDE), which is an application that brings together various bits and pieces that you'll use all the time. The one that we will use is R Studio.

\hypertarget{r-studio}{%
\subsection{R Studio}\label{r-studio}}

R Studio is distinct to R and they are different entities. R Studio builds on top of R to make it easier for you to use R. This is in the same way that you can use the internet from the command line, but most of us use a browser such as Chrome, Firefox, or Safari.

R Studio is free in the sense that you don't pay anything for it. It is also free in the sense of being able to take the code, modify it, and distribute that code provided others are similarly allowed to take your code and modify it and distribute, etc. However, it is important to recognise that R Studio is an entity and so it is possible that in the future the current situation could change.

You can download R Studio here: \url{https://rstudio.com/products/rstudio/download/\#download}.

When you open R Studio it will look like Figure \ref{fig:first}.

\begin{figure}
\includegraphics[width=1\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/01-03_r_essentials/01} \caption{Opening R Studio for the first time}\label{fig:first}
\end{figure}

The left pane is a console in which you can type and execute R code line by line. Try it with 2+2 by clicking next to the prompt `\textgreater{}' and typing that out then pressing enter. The code that you type should be:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2} \OperatorTok{+}\StringTok{ }\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4
\end{verbatim}

And hopefully you get the same answer printed in the console.

The pane on the top right has information about your environment. For instance, when we create variables a list of their names and some properties will appear there. Try to type the following code, replacing my name with your name, next to the prompt, and again press enter:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_name <-}\StringTok{ "Rohan"}
\end{Highlighting}
\end{Shaded}

You should notice a new value in the environment pane with the variable name and its value.

The pane in the bottom right is a file manager. At the moment it should just have two files - an R History file and a R Project file. We'll get to what these are later, but for now we will create and save a file.

Type out the following code (don't worry too much about the details for now):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{saveRDS}\NormalTok{(}\DataTypeTok{object =}\NormalTok{ my_name, }\DataTypeTok{file =} \StringTok{"my_first_file.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

And you should see a new `.rds' file in your list of files.

\hypertarget{r-studio-cloud}{%
\subsection{R Studio Cloud}\label{r-studio-cloud}}

While you can download R Studio to your own computer, initially we will us R Studio Cloud, which is an online version that is provided by R Studio. We will use this so that you can focus on getting comfortable with R and R Studio in an environment that is consistent. This way you don't have to worry about what computer you have or installation permissions while you are still getting used to the basics.

The R Studio Cloud - \url{https://rstudio.cloud/} - is as easy as it gets in terms of moving to the cloud. The trade-off is that it is not very powerful and it is sometimes slow, but for the purposes of the initial sections of these notes that will be fine.

To get started, go to \url{https://rstudio.cloud/} and create an account. If you are going to be a student for a while then it might be worthwhile using a university email account, because although they don't yet charge for it, they will probably start charging soon, but with some luck they will offer education discounts.

Once you have an account and log in, then it should look something like Figure \ref{fig:second}.

\begin{figure}
\includegraphics[width=1\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/01-03_r_essentials/02} \caption{Opening R Studio Cloud for the first time}\label{fig:second}
\end{figure}

(You'll be in `Your Workspace', and you won't have a `Example Workspace'.) From here you should start a `New Project'. You can give the project a name by clicking on `Untitled Project' and replacing it. We can now use R Studio in the cloud.

While working line-by-line in the console is fine, it is easier to write out a whole script that can then be executed. We will do this by making an R Script. To do this go to: File -\textgreater{} New File -\textgreater{} R Script, or use the shortcut Command + Shift + N. The console pane will fall to the bottom left and an R Script will open in the top left. Let's write some code that will grab all of the Australian politicians and then construct a small table about the genders of the prime ministers.

(Some of this code won't make sense at this stage, but just type it all out to get in the habit and then run it, by selecting all of the code and clicking `Run' (or using the keyboard shortcut: Command + Return)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Install the packages that we'll need}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"devtools"}\NormalTok{)}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Load the packages that we need to use this time}
\KeywordTok{library}\NormalTok{(devtools)}
\KeywordTok{library}\NormalTok{(tidyverse)}

\CommentTok{# Grab the data on Australian politicians}
\KeywordTok{install_github}\NormalTok{(}\StringTok{"RohanAlexander/AustralianPoliticians"}\NormalTok{)}

\CommentTok{# Make a table of the counts of genders of the prime ministers}
\NormalTok{AustralianPoliticians}\OperatorTok{::}\NormalTok{all }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{as_tibble}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(gender, wasPrimeMinister)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4 x 3
##   gender wasPrimeMinister     n
##   <chr>             <int> <int>
## 1 female                1     1
## 2 female               NA   235
## 3 male                  1    29
## 4 male                 NA  1511
\end{verbatim}

You can save your R Script as `my\_first\_r\_script.R' using File -\textgreater{} Save As (or the keyboard shortcut: Command + S). When you're done your workspace should look something like Figure \ref{fig:third}.

\begin{figure}
\includegraphics[width=1\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/01-03_r_essentials/03} \caption{After running an R Script}\label{fig:third}
\end{figure}

One thing to be aware of is that each R Studio Cloud workspace is essentially a new computer. Because of this, you'll need to install any package that you want to use for each workspace. For instance, before you can use the tidyverse, you need to install.packages(``tidyverse''). This is in contrast to when you use your own computer.

A few final notes on R Studio Cloud for you to keep in the back of your mind:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  In the Australian politicians example we got our data from the website GitHub, but you can get data into your workspace from your local computer in a variety of ways. One way is to use the `upload' button in the Files panel.
\item
  R Studio Cloud allows some degree of collaboration. For instance, you can give someone else access to a workspace that you create. This could be useful for collaborating on an assignment, although it is not quite full featured yet and you cannot both be in the workspace at the same time (in contrast to, say, Google Docs).
\item
  There are a variety of weaknesses of R Studio Cloud, in particular at the moment there is a 1GB limit on RAM. Additionally, it is still under-developed and things break from time to time. The R Studio Community page that is focused on R Studio Cloud can be helpful sometimes: \url{https://community.rstudio.com/c/rstudio-cloud}.
\end{enumerate}

\hypertarget{tidyverse-i}{%
\section{Tidyverse I}\label{tidyverse-i}}

\textbf{Aspects of `Tidyverse I' were written with \href{https://www.monicaalexander.com/}{Monica Alexander}.}

One of the key packages that we use in these notes is the \texttt{tidyverse} \citet{tidyverse}. The \texttt{tidyverse} is actually a package of packages (i.e.~when you install \texttt{tidyverse}, you are actually installing a whole bunch of different packages). The key package in the \texttt{tidyverse} in terms of manipulating data is \texttt{dplyr} \citet{citedplyr}, and the key package in the \texttt{tidyverse} in terms of creating graphs is \texttt{ggplot2} \citet{citeggplot}.

In this section we are going to cycle through some essentials from the Tidyverse. You'll come back to the functions in this section regularly.

I want to keep this section self-contained, so let's start by installing the \texttt{tidyverse} (again, to use Di Cook's analogy, this is the equivalent of screwing in the light-bulb). If you just did it, then you don't need to do it again.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now we can load the \texttt{tidyverse} (again, to use Di Cook's analogy, the equivalent of turning on the light-switch).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

Here we are going to download the data about Australian politicians using the function \texttt{read\_csv()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians <-}\StringTok{ }
\StringTok{  }\KeywordTok{read_csv}\NormalTok{(}
    \DataTypeTok{file =} 
      \StringTok{"https://raw.githubusercontent.com/RohanAlexander/telling_stories_with_data/master/inputs/data/australian_politicians.csv"}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## -- Column specification --------------------------------------------------------
## cols(
##   .default = col_character(),
##   birthDate = col_date(format = ""),
##   birthYear = col_double(),
##   deathDate = col_date(format = ""),
##   member = col_double(),
##   senator = col_double(),
##   wasPrimeMinister = col_double()
## )
## i Use `spec()` for the full column specifications.
\end{verbatim}

We will now cover the pipe and six functions that are useful to know and that we will use all the time:

\begin{itemize}
\tightlist
\item
  \texttt{select()}
\item
  \texttt{filter()}
\item
  \texttt{arrange()}
\item
  \texttt{mutate()}
\item
  \texttt{summarise()/summarize()}
\item
  \texttt{group\_by()}
\end{itemize}

\hypertarget{the-pipe}{%
\subsection{The pipe}\label{the-pipe}}

One key tidyverse helper is the `pipe': \texttt{\%\textgreater{}\%}. Read it as ``and then'' (keyboard shortcut: Command + Shift + M). This takes the output of a line of code and uses it as an input to the next line of code. You don't have to use it, but it tends to make your code more readable.

The idea of the pipe is that you take your dataset, \textbf{and then}, do something to it. In this case, we will look at the first few lines of our dataset by piping \texttt{australian\_politicians} through to the \texttt{head()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 20
##   uniqueID surname allOtherNames firstName commonName displayName
##   <chr>    <chr>   <chr>         <chr>     <chr>      <chr>      
## 1 Abbott1~ Abbott  Richard Hart~ Richard   <NA>       Abbott, Ri~
## 2 Abbott1~ Abbott  Percy Phipps  Percy     <NA>       Abbott, Pe~
## 3 Abbott1~ Abbott  Macartney     Macartney Mac        Abbott, Mac
## 4 Abbott1~ Abbott  Charles Lydi~ Charles   Aubrey     Abbott, Au~
## 5 Abbott1~ Abbott  Joseph Palmer Joseph    <NA>       Abbott, Jo~
## 6 Abbott1~ Abbott  Anthony John  Anthony   Tony       Abbott, To~
## # ... with 14 more variables: earlierOrLaterNames <chr>, title <chr>,
## #   gender <chr>, birthDate <date>, birthYear <dbl>, birthPlace <chr>,
## #   deathDate <date>, member <dbl>, senator <dbl>, wasPrimeMinister <dbl>,
## #   wikidataID <chr>, wikipedia <chr>, adb <chr>, comments <chr>
\end{verbatim}

\hypertarget{selecting}{%
\subsection{Selecting}\label{selecting}}

The \texttt{select()} function is used to get a particular column of a dataset. For instance, we might like to select the first names column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(firstName) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 1
##   firstName
##   <chr>    
## 1 Richard  
## 2 Percy    
## 3 Macartney
## 4 Charles  
## 5 Joseph   
## 6 Anthony
\end{verbatim}

In R, there are many ways to do things. Another way to get a particular column of a dataset is to use the dollar sign. This is from base R (as opposed to \texttt{select()} which is from the \texttt{tidyverse} package).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians}\OperatorTok{$}\NormalTok{firstName }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Richard"   "Percy"     "Macartney" "Charles"   "Joseph"    "Anthony"
\end{verbatim}

The two are almost equivalent and differ only in the class of what they return (we'll talk more about class later in the notes).

For the sake of completeness, if you combine \texttt{select()} with \texttt{pull()} then you will get the same class of output as if you use the dollar sign.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(firstName) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{pull}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Richard"   "Percy"     "Macartney" "Charles"   "Joseph"    "Anthony"
\end{verbatim}

You can also use \texttt{select} to get rid of columns, by selecting in a negative sense.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{firstName)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,776 x 19
##    uniqueID surname allOtherNames commonName displayName earlierOrLaterN~ title
##    <chr>    <chr>   <chr>         <chr>      <chr>       <chr>            <chr>
##  1 Abbott1~ Abbott  Richard Hart~ <NA>       Abbott, Ri~ <NA>             <NA> 
##  2 Abbott1~ Abbott  Percy Phipps  <NA>       Abbott, Pe~ <NA>             <NA> 
##  3 Abbott1~ Abbott  Macartney     Mac        Abbott, Mac <NA>             <NA> 
##  4 Abbott1~ Abbott  Charles Lydi~ Aubrey     Abbott, Au~ <NA>             <NA> 
##  5 Abbott1~ Abbott  Joseph Palmer <NA>       Abbott, Jo~ <NA>             <NA> 
##  6 Abbott1~ Abbott  Anthony John  Tony       Abbott, To~ <NA>             <NA> 
##  7 Abel1939 Abel    John Arthur   <NA>       Abel, John  <NA>             <NA> 
##  8 Abetz19~ Abetz   Eric          <NA>       Abetz, Eric <NA>             <NA> 
##  9 Adams19~ Adams   Judith Anne   <NA>       Adams, Jud~ nee Bird         <NA> 
## 10 Adams19~ Adams   Dick Godfrey~ <NA>       Adams, Dick <NA>             <NA> 
## # ... with 1,766 more rows, and 12 more variables: gender <chr>,
## #   birthDate <date>, birthYear <dbl>, birthPlace <chr>, deathDate <date>,
## #   member <dbl>, senator <dbl>, wasPrimeMinister <dbl>, wikidataID <chr>,
## #   wikipedia <chr>, adb <chr>, comments <chr>
\end{verbatim}

Finally, you can select, based on conditions. For instance, selecting all all of the columns that start with something, for instance, `birth'.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(}\KeywordTok{starts_with}\NormalTok{(}\StringTok{"birth"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,776 x 3
##    birthDate  birthYear birthPlace  
##    <date>         <dbl> <chr>       
##  1 NA              1859 Bendigo     
##  2 1869-05-14      1869 Hobart      
##  3 1877-07-03      1877 Murrurundi  
##  4 1886-01-04      1886 St Leonards 
##  5 1891-10-18      1891 North Sydney
##  6 1957-11-04      1957 London      
##  7 1939-06-25      1939 Sydney      
##  8 1958-01-25      1958 Stuttgart   
##  9 1943-04-11      1943 Picton      
## 10 1951-04-29      1951 Launceston  
## # ... with 1,766 more rows
\end{verbatim}

\hypertarget{filtering}{%
\subsection{Filtering}\label{filtering}}

The \texttt{filter()} function is used to get particular rows from a dataset. For instance, we might like to filter to only politicians that became prime minister.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(wasPrimeMinister }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 30 x 20
##    uniqueID surname allOtherNames firstName commonName displayName
##    <chr>    <chr>   <chr>         <chr>     <chr>      <chr>      
##  1 Abbott1~ Abbott  Anthony John  Anthony   Tony       Abbott, To~
##  2 Barton1~ Barton  Edmund        Edmund    <NA>       Barton, Ed~
##  3 Bruce18~ Bruce   Stanley Melb~ Stanley   <NA>       Bruce, Sta~
##  4 Chifley~ Chifley Joseph Bened~ Joseph    Ben        Chifley, B~
##  5 Cook1860 Cook    Joseph        Joseph    <NA>       Cook, Jose~
##  6 Curtin1~ Curtin  John Joseph ~ John      <NA>       Curtin, Jo~
##  7 Deakin1~ Deakin  Alfred        Alfred    <NA>       Deakin, Al~
##  8 Fadden1~ Fadden  Arthur Willi~ Arthur    Arthur     Fadden, Ar~
##  9 Fisher1~ Fisher  Andrew        Andrew    <NA>       Fisher, An~
## 10 Forde18~ Forde   Francis Mich~ Francis   Frank      Forde, Fra~
## # ... with 20 more rows, and 14 more variables: earlierOrLaterNames <chr>,
## #   title <chr>, gender <chr>, birthDate <date>, birthYear <dbl>,
## #   birthPlace <chr>, deathDate <date>, member <dbl>, senator <dbl>,
## #   wasPrimeMinister <dbl>, wikidataID <chr>, wikipedia <chr>, adb <chr>,
## #   comments <chr>
\end{verbatim}

The \texttt{filter()} function also accepts two conditions. For instance, we can look at politicians who were prime minister and were named Joseph.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(wasPrimeMinister }\OperatorTok{==}\StringTok{ }\DecValTok{1} \OperatorTok{&}\StringTok{ }\NormalTok{firstName }\OperatorTok{==}\StringTok{ "Joseph"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 20
##   uniqueID surname allOtherNames firstName commonName displayName
##   <chr>    <chr>   <chr>         <chr>     <chr>      <chr>      
## 1 Chifley~ Chifley Joseph Bened~ Joseph    Ben        Chifley, B~
## 2 Cook1860 Cook    Joseph        Joseph    <NA>       Cook, Jose~
## 3 Lyons18~ Lyons   Joseph Aloys~ Joseph    <NA>       Lyons, Jos~
## # ... with 14 more variables: earlierOrLaterNames <chr>, title <chr>,
## #   gender <chr>, birthDate <date>, birthYear <dbl>, birthPlace <chr>,
## #   deathDate <date>, member <dbl>, senator <dbl>, wasPrimeMinister <dbl>,
## #   wikidataID <chr>, wikipedia <chr>, adb <chr>, comments <chr>
\end{verbatim}

We would get the same result if we use a comma instead of an ampersand.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(wasPrimeMinister }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{, firstName }\OperatorTok{==}\StringTok{ "Joseph"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 20
##   uniqueID surname allOtherNames firstName commonName displayName
##   <chr>    <chr>   <chr>         <chr>     <chr>      <chr>      
## 1 Chifley~ Chifley Joseph Bened~ Joseph    Ben        Chifley, B~
## 2 Cook1860 Cook    Joseph        Joseph    <NA>       Cook, Jose~
## 3 Lyons18~ Lyons   Joseph Aloys~ Joseph    <NA>       Lyons, Jos~
## # ... with 14 more variables: earlierOrLaterNames <chr>, title <chr>,
## #   gender <chr>, birthDate <date>, birthYear <dbl>, birthPlace <chr>,
## #   deathDate <date>, member <dbl>, senator <dbl>, wasPrimeMinister <dbl>,
## #   wikidataID <chr>, wikipedia <chr>, adb <chr>, comments <chr>
\end{verbatim}

Similarly, we can look at politicians who were named Myles or Ruth.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(firstName }\OperatorTok{==}\StringTok{ "Ruth"} \OperatorTok{|}\StringTok{ }\NormalTok{firstName }\OperatorTok{==}\StringTok{ "Myles"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 20
##   uniqueID surname allOtherNames firstName commonName displayName
##   <chr>    <chr>   <chr>         <chr>     <chr>      <chr>      
## 1 Coleman~ Coleman Ruth Nancy    Ruth      <NA>       Coleman, R~
## 2 Ferrick~ Ferric~ Myles Aloysi~ Myles     <NA>       Ferricks, ~
## 3 Webber1~ Webber  Ruth Stephan~ Ruth      <NA>       Webber, Ru~
## # ... with 14 more variables: earlierOrLaterNames <chr>, title <chr>,
## #   gender <chr>, birthDate <date>, birthYear <dbl>, birthPlace <chr>,
## #   deathDate <date>, member <dbl>, senator <dbl>, wasPrimeMinister <dbl>,
## #   wikidataID <chr>, wikipedia <chr>, adb <chr>, comments <chr>
\end{verbatim}

We can also pipe the results, for instance, pipe from the \texttt{filter()} to \texttt{select()}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(firstName }\OperatorTok{==}\StringTok{ "Ruth"} \OperatorTok{|}\StringTok{ }\NormalTok{firstName }\OperatorTok{==}\StringTok{ "Myles"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(firstName, surname)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 2
##   firstName surname 
##   <chr>     <chr>   
## 1 Ruth      Coleman 
## 2 Myles     Ferricks
## 3 Ruth      Webber
\end{verbatim}

Finally, we can \texttt{filter()} to a particular row number, for instance, in this case row 853.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\KeywordTok{row_number}\NormalTok{() }\OperatorTok{==}\StringTok{ }\DecValTok{853}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 20
##   uniqueID surname allOtherNames firstName commonName displayName
##   <chr>    <chr>   <chr>         <chr>     <chr>      <chr>      
## 1 Jarman1~ Jarman  Alan William  Alan      <NA>       Jarman, Al~
## # ... with 14 more variables: earlierOrLaterNames <chr>, title <chr>,
## #   gender <chr>, birthDate <date>, birthYear <dbl>, birthPlace <chr>,
## #   deathDate <date>, member <dbl>, senator <dbl>, wasPrimeMinister <dbl>,
## #   wikidataID <chr>, wikipedia <chr>, adb <chr>, comments <chr>
\end{verbatim}

But there is also a dedicated function to do this, which is \texttt{slice()}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{slice}\NormalTok{(}\DecValTok{853}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 20
##   uniqueID surname allOtherNames firstName commonName displayName
##   <chr>    <chr>   <chr>         <chr>     <chr>      <chr>      
## 1 Jarman1~ Jarman  Alan William  Alan      <NA>       Jarman, Al~
## # ... with 14 more variables: earlierOrLaterNames <chr>, title <chr>,
## #   gender <chr>, birthDate <date>, birthYear <dbl>, birthPlace <chr>,
## #   deathDate <date>, member <dbl>, senator <dbl>, wasPrimeMinister <dbl>,
## #   wikidataID <chr>, wikipedia <chr>, adb <chr>, comments <chr>
\end{verbatim}

\hypertarget{arranging}{%
\subsection{Arranging}\label{arranging}}

We can change the order of the dataset based on the values in a particular column using the \texttt{arrange()} function. For instance, we may like to arrange the data by year of birth.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(surname)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,776 x 20
##    uniqueID surname allOtherNames firstName commonName displayName
##    <chr>    <chr>   <chr>         <chr>     <chr>      <chr>      
##  1 Abbott1~ Abbott  Richard Hart~ Richard   <NA>       Abbott, Ri~
##  2 Abbott1~ Abbott  Percy Phipps  Percy     <NA>       Abbott, Pe~
##  3 Abbott1~ Abbott  Macartney     Macartney Mac        Abbott, Mac
##  4 Abbott1~ Abbott  Charles Lydi~ Charles   Aubrey     Abbott, Au~
##  5 Abbott1~ Abbott  Joseph Palmer Joseph    <NA>       Abbott, Jo~
##  6 Abbott1~ Abbott  Anthony John  Anthony   Tony       Abbott, To~
##  7 Abel1939 Abel    John Arthur   John      <NA>       Abel, John 
##  8 Abetz19~ Abetz   Eric          Eric      <NA>       Abetz, Eric
##  9 Adams19~ Adams   Judith Anne   Judith    <NA>       Adams, Jud~
## 10 Adams19~ Adams   Dick Godfrey~ Dick      <NA>       Adams, Dick
## # ... with 1,766 more rows, and 14 more variables: earlierOrLaterNames <chr>,
## #   title <chr>, gender <chr>, birthDate <date>, birthYear <dbl>,
## #   birthPlace <chr>, deathDate <date>, member <dbl>, senator <dbl>,
## #   wasPrimeMinister <dbl>, wikidataID <chr>, wikipedia <chr>, adb <chr>,
## #   comments <chr>
\end{verbatim}

We can also use the \texttt{desc()} function to arrange in descending order.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\KeywordTok{desc}\NormalTok{(surname))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,776 x 20
##    uniqueID surname allOtherNames firstName commonName displayName
##    <chr>    <chr>   <chr>         <chr>     <chr>      <chr>      
##  1 Zimmerm~ Zimmer~ Trent Moir    Trent     <NA>       Zimmerman,~
##  2 Zeal1830 Zeal    William Aust~ William   <NA>       Zeal, Will~
##  3 Zappia1~ Zappia  Antonio       Antonio   Tony       Zappia, To~
##  4 Zammit1~ Zammit  Paul John     Paul      <NA>       Zammit, Pa~
##  5 Zakharo~ Zakhar~ Alice Olive   Alice     Olive      Zakharov, ~
##  6 Zahra19~ Zahra   Christian Jo~ Christian <NA>       Zahra, Chr~
##  7 Young19~ Young   Harold Willi~ Harold    <NA>       Young, Har~
##  8 Young19~ Young   Michael Jero~ Michael   Mick       Young, Mick
##  9 Young19~ Young   Terry James   Terry     <NA>       Young, Ter~
## 10 Yates18~ Yates   George Edwin  George    Gunner     Yates, Gun~
## # ... with 1,766 more rows, and 14 more variables: earlierOrLaterNames <chr>,
## #   title <chr>, gender <chr>, birthDate <date>, birthYear <dbl>,
## #   birthPlace <chr>, deathDate <date>, member <dbl>, senator <dbl>,
## #   wasPrimeMinister <dbl>, wikidataID <chr>, wikipedia <chr>, adb <chr>,
## #   comments <chr>
\end{verbatim}

We can also arrange based on more than one column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(firstName, surname)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,776 x 20
##    uniqueID surname allOtherNames firstName commonName displayName
##    <chr>    <chr>   <chr>         <chr>     <chr>      <chr>      
##  1 Blain18~ Blain   Adair Macali~ Adair     <NA>       Blain, Ada~
##  2 Armstro~ Armstr~ Adam Alexand~ Adam      Bill       Armstrong,~
##  3 Bandt19~ Bandt   Adam Paul     Adam      <NA>       Bandt, Adam
##  4 Dein1889 Dein    Adam Kemball  Adam      Dick       Dein, Dick 
##  5 Ridgewa~ Ridgew~ Aden Derek    Aden      <NA>       Ridgeway, ~
##  6 Bennett~ Bennett Adrian Frank  Adrian    <NA>       Bennett, A~
##  7 Gibson1~ Gibson  Adrian        Adrian    <NA>       Gibson, Ad~
##  8 Wynne18~ Wynne   Agar          Agar      <NA>       Wynne, Agar
##  9 Roberts~ Robert~ Agnes Robert~ Agnes     <NA>       Robertson,~
## 10 Bird1906 Bird    Alan Charles  Alan      <NA>       Bird, Alan 
## # ... with 1,766 more rows, and 14 more variables: earlierOrLaterNames <chr>,
## #   title <chr>, gender <chr>, birthDate <date>, birthYear <dbl>,
## #   birthPlace <chr>, deathDate <date>, member <dbl>, senator <dbl>,
## #   wasPrimeMinister <dbl>, wikidataID <chr>, wikipedia <chr>, adb <chr>,
## #   comments <chr>
\end{verbatim}

We can pipe \texttt{arrange()} to another \texttt{arrange()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(firstName) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(surname)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,776 x 20
##    uniqueID surname allOtherNames firstName commonName displayName
##    <chr>    <chr>   <chr>         <chr>     <chr>      <chr>      
##  1 Abbott1~ Abbott  Anthony John  Anthony   Tony       Abbott, To~
##  2 Abbott1~ Abbott  Charles Lydi~ Charles   Aubrey     Abbott, Au~
##  3 Abbott1~ Abbott  Joseph Palmer Joseph    <NA>       Abbott, Jo~
##  4 Abbott1~ Abbott  Macartney     Macartney Mac        Abbott, Mac
##  5 Abbott1~ Abbott  Percy Phipps  Percy     <NA>       Abbott, Pe~
##  6 Abbott1~ Abbott  Richard Hart~ Richard   <NA>       Abbott, Ri~
##  7 Abel1939 Abel    John Arthur   John      <NA>       Abel, John 
##  8 Abetz19~ Abetz   Eric          Eric      <NA>       Abetz, Eric
##  9 Adams19~ Adams   Dick Godfrey~ Dick      <NA>       Adams, Dick
## 10 Adams19~ Adams   Judith Anne   Judith    <NA>       Adams, Jud~
## # ... with 1,766 more rows, and 14 more variables: earlierOrLaterNames <chr>,
## #   title <chr>, gender <chr>, birthDate <date>, birthYear <dbl>,
## #   birthPlace <chr>, deathDate <date>, member <dbl>, senator <dbl>,
## #   wasPrimeMinister <dbl>, wikidataID <chr>, wikipedia <chr>, adb <chr>,
## #   comments <chr>
\end{verbatim}

It is just important to be clear about the precedence of each.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(surname, firstName)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,776 x 20
##    uniqueID surname allOtherNames firstName commonName displayName
##    <chr>    <chr>   <chr>         <chr>     <chr>      <chr>      
##  1 Abbott1~ Abbott  Anthony John  Anthony   Tony       Abbott, To~
##  2 Abbott1~ Abbott  Charles Lydi~ Charles   Aubrey     Abbott, Au~
##  3 Abbott1~ Abbott  Joseph Palmer Joseph    <NA>       Abbott, Jo~
##  4 Abbott1~ Abbott  Macartney     Macartney Mac        Abbott, Mac
##  5 Abbott1~ Abbott  Percy Phipps  Percy     <NA>       Abbott, Pe~
##  6 Abbott1~ Abbott  Richard Hart~ Richard   <NA>       Abbott, Ri~
##  7 Abel1939 Abel    John Arthur   John      <NA>       Abel, John 
##  8 Abetz19~ Abetz   Eric          Eric      <NA>       Abetz, Eric
##  9 Adams19~ Adams   Dick Godfrey~ Dick      <NA>       Adams, Dick
## 10 Adams19~ Adams   Judith Anne   Judith    <NA>       Adams, Jud~
## # ... with 1,766 more rows, and 14 more variables: earlierOrLaterNames <chr>,
## #   title <chr>, gender <chr>, birthDate <date>, birthYear <dbl>,
## #   birthPlace <chr>, deathDate <date>, member <dbl>, senator <dbl>,
## #   wasPrimeMinister <dbl>, wikidataID <chr>, wikipedia <chr>, adb <chr>,
## #   comments <chr>
\end{verbatim}

\hypertarget{grouping}{%
\subsection{Grouping}\label{grouping}}

We can group variables using the function \texttt{group\_by()} and then apply some other function within those groups. For instance, we could arrange by first name within gender, and then get the first three results.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(gender) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(firstName) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{slice}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 20
## # Groups:   gender [2]
##   uniqueID surname allOtherNames firstName commonName displayName
##   <chr>    <chr>   <chr>         <chr>     <chr>      <chr>      
## 1 Roberts~ Robert~ Agnes Robert~ Agnes     <NA>       Robertson,~
## 2 MacTier~ MacTie~ Alannah Joan~ Alannah   <NA>       MacTiernan~
## 3 Zakharo~ Zakhar~ Alice Olive   Alice     Olive      Zakharov, ~
## 4 Blain18~ Blain   Adair Macali~ Adair     <NA>       Blain, Ada~
## 5 Armstro~ Armstr~ Adam Alexand~ Adam      Bill       Armstrong,~
## 6 Bandt19~ Bandt   Adam Paul     Adam      <NA>       Bandt, Adam
## # ... with 14 more variables: earlierOrLaterNames <chr>, title <chr>,
## #   gender <chr>, birthDate <date>, birthYear <dbl>, birthPlace <chr>,
## #   deathDate <date>, member <dbl>, senator <dbl>, wasPrimeMinister <dbl>,
## #   wikidataID <chr>, wikipedia <chr>, adb <chr>, comments <chr>
\end{verbatim}

\hypertarget{mutating}{%
\subsection{Mutating}\label{mutating}}

The \texttt{mutate()} function is used to make a new column. For instance, perhaps we want to make a new column that is 1 if a person was a member and a senator and 0 otherwise.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians <-}\StringTok{ }
\StringTok{  }\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{was_both =} \KeywordTok{if_else}\NormalTok{(member }\OperatorTok{==}\StringTok{ }\DecValTok{1} \OperatorTok{&}\StringTok{ }\NormalTok{senator }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))}

\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(member, senator, was_both) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 3
##   member senator was_both
##    <dbl>   <dbl>    <dbl>
## 1      0       1        0
## 2      1       1        1
## 3      0       1        0
## 4      1       0        0
## 5      1       0        0
## 6      1       0        0
\end{verbatim}

\hypertarget{summarise}{%
\subsection{Summarise}\label{summarise}}

The function \texttt{summarise()} is used to create new summary variables. For instance, looking at the maximum of birth year to find who the most recently born politicians are.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{youngest_politicians_birth_year =} \KeywordTok{max}\NormalTok{(birthYear, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 1
##   youngest_politicians_birth_year
##                             <dbl>
## 1                            1994
\end{verbatim}

And we can check that using \texttt{arrange()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\OperatorTok{-}\NormalTok{birthYear) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(uniqueID, surname, allOtherNames, birthYear) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{slice}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 4
##   uniqueID       surname     allOtherNames    birthYear
##   <chr>          <chr>       <chr>                <dbl>
## 1 SteeleJohn1994 Steele-John Jordon Alexander      1994
## 2 Chandler1990   Chandler    Claire                1990
## 3 Roy1990        Roy         Wyatt Beau            1990
\end{verbatim}

The \texttt{summarise()} function is particularly powerful in conjunction with \texttt{group\_by()}. For instance, let's look at the year of birth of the youngest by gender.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(gender) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{youngest_politician_birth_year =} \KeywordTok{max}\NormalTok{(birthYear, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `summarise()` ungrouping output (override with `.groups` argument)
\end{verbatim}

\begin{verbatim}
## # A tibble: 2 x 2
##   gender youngest_politician_birth_year
##   <chr>                           <dbl>
## 1 female                           1990
## 2 male                             1994
\end{verbatim}

Let's look at mean of age at death by gender.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{days_lived =}\NormalTok{ deathDate }\OperatorTok{-}\StringTok{ }\NormalTok{birthDate) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(days_lived)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(gender) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean_days_lived =} \KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(days_lived), }\DecValTok{2}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\OperatorTok{-}\NormalTok{mean_days_lived)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `summarise()` ungrouping output (override with `.groups` argument)
\end{verbatim}

\begin{verbatim}
## # A tibble: 2 x 2
##   gender mean_days_lived
##   <chr>  <drtn>         
## 1 female 28857.30 days  
## 2 male   27372.89 days
\end{verbatim}

We can use \texttt{group\_by()} for more than one group for instance, looking again at average number of days lived by gender and by which house.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{days_lived =}\NormalTok{ deathDate }\OperatorTok{-}\StringTok{ }\NormalTok{birthDate) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(days_lived)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(gender, wasPrimeMinister) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean_days_lived =} \KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(days_lived), }\DecValTok{2}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\OperatorTok{-}\NormalTok{mean_days_lived)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `summarise()` regrouping output by 'gender' (override with `.groups` argument)
\end{verbatim}

\begin{verbatim}
## # A tibble: 3 x 3
## # Groups:   gender [2]
##   gender wasPrimeMinister mean_days_lived
##   <chr>             <dbl> <drtn>         
## 1 female               NA 28857.30 days  
## 2 male                  1 28446.61 days  
## 3 male                 NA 27345.20 days
\end{verbatim}

\hypertarget{counting}{%
\subsection{Counting}\label{counting}}

We can use the function \texttt{count()} to create counts by groups. For instance, the number of politicians by gender.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(gender) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 2
## # Groups:   gender [2]
##   gender     n
##   <chr>  <int>
## 1 female   236
## 2 male    1540
\end{verbatim}

\hypertarget{proportions}{%
\subsection{Proportions}\label{proportions}}

Finally, often calculating proportions is a combination of \texttt{summarise()} and \texttt{mutate()} (and \texttt{group\_by()}).

Let's calculate the proportion of genders.

Note here, that we needed to \texttt{ungroup()} the data before mutating.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(gender) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ungroup}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{prop =}\NormalTok{ n}\OperatorTok{/}\NormalTok{(}\KeywordTok{sum}\NormalTok{(n)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 3
##   gender     n  prop
##   <chr>  <int> <dbl>
## 1 female   236 0.133
## 2 male    1540 0.867
\end{verbatim}

\hypertarget{base}{%
\section{Base}\label{base}}

\hypertarget{class}{%
\subsection{Class}\label{class}}

A class is the broader type of object that something is. For instance, your class is probably `human', which is itself a `animal'. Similarly, if we create a number in R we can use \texttt{class()} to work out its class, which in this case will be numeric.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_number <-}\StringTok{ }\DecValTok{8}
\KeywordTok{class}\NormalTok{(my_number)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "numeric"
\end{verbatim}

Or we could make it a character.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_name <-}\StringTok{ "rohan"}
\KeywordTok{class}\NormalTok{(my_name)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "character"
\end{verbatim}

Finally, we can often coerce classes to be something else.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_number_as_character <-}\StringTok{ }\KeywordTok{as.character}\NormalTok{(my_number)}
\KeywordTok{class}\NormalTok{(my_number_as_character)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "character"
\end{verbatim}

There are many ways for your code to not run, but having an issue with the classes is the almost always the first thing to check.

\hypertarget{simulating-data}{%
\subsection{Simulating data}\label{simulating-data}}

Simulating data is a key skill for statistics. We will use the following functions all the time: \texttt{rnorm()}, \texttt{sample()}, and \texttt{runi()}. Arguably the most important function is \texttt{set.seed()}, which we need because while we want our data to be random, we want it to be repeatable.

Let's get 10 observations from the standard normal.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{number_of_observations <-}\StringTok{ }\DecValTok{10}

\NormalTok{simulated_data <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{person =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{number_of_observations),}
                         \DataTypeTok{observation =} \KeywordTok{rnorm}\NormalTok{(number_of_observations, }
                                             \DataTypeTok{mean =} \DecValTok{0}\NormalTok{, }
                                             \DataTypeTok{sd =} \DecValTok{1}\NormalTok{)}
\NormalTok{                         )}
\end{Highlighting}
\end{Shaded}

Then let's add 10 draws from the uniform distribution between 0 and 10.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated_data}\OperatorTok{$}\NormalTok{another_observation <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(number_of_observations, }
                                            \DataTypeTok{min =} \DecValTok{0}\NormalTok{, }
                                            \DataTypeTok{max =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Finally, let's use sample, which allows use to pick from a list of items, to add a favourite colour to each observation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated_data}\OperatorTok{$}\NormalTok{fav_colour <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{" white "}\NormalTok{), }
                                    \DataTypeTok{size =}\NormalTok{ number_of_observations,}
                                    \DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We set the option \texttt{replace} to \texttt{TRUE} because we are only choosing between two items, but we want ten outcomes. Depending on the simulation you should think about whether you need it \texttt{TRUE} or \texttt{FALSE}. Also, there is another useful option to adjust the probability with which each item is drawn. In particular, the default is that both options are equally likely, but perhaps we might like to have 10 per cent \texttt{blue} with 90 per cent \texttt{white}. The way to do this is to set the option \texttt{prob}. As always with functions, you can find more in the help with \texttt{?sample}.

\hypertarget{functions}{%
\subsection{Functions}\label{functions}}

There are a lot of functions in R, and almost any common task that you might need to do is likely already done. But you will need to write your own functions. The way to do this is to define a function and give it a name. Your function will probably have some inputs (note that these inputs can have default values). Your function will then do something with these inputs and then return something.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_function <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(some_names) \{}
  \KeywordTok{print}\NormalTok{(some_names)}
\NormalTok{\}}

\KeywordTok{my_function}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"rohan"}\NormalTok{, }\StringTok{"monica"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "rohan"  "monica"
\end{verbatim}

\hypertarget{ggplot-essentials}{%
\section{ggplot essentials}\label{ggplot-essentials}}

The \texttt{ggplot} package is the plotting package that is part of the \texttt{tidyverse} collection of packages.

In a similar way to piping, it works in layers. But instead of using the pipe (\texttt{\%\textgreater{}\%}) ggplot uses \texttt{+}.

\hypertarget{main-features}{%
\subsection{Main features}\label{main-features}}

There are three key aspects:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  data;
\item
  aesthetics / mapping; and
\item
  type.
\end{enumerate}

For instances, let's build up a histogram of age of death with increasing complexity.

Starts with a grey box:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{days_lived =} \KeywordTok{as.integer}\NormalTok{(deathDate }\OperatorTok{-}\StringTok{ }\NormalTok{birthDate)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(days_lived)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ days_lived))}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-63-1.pdf}

We need to tell it what we want to plot. This is where \texttt{geom} comes in

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{days_lived =} \KeywordTok{as.integer}\NormalTok{(deathDate }\OperatorTok{-}\StringTok{ }\NormalTok{birthDate)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(days_lived)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ days_lived)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{365}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-64-1.pdf}

Now let's color the bars by gender, which means adding an aesthetic.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{days_lived =} \KeywordTok{as.integer}\NormalTok{(deathDate }\OperatorTok{-}\StringTok{ }\NormalTok{birthDate)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(days_lived)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ days_lived, }\DataTypeTok{fill =}\NormalTok{ gender)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{365}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-65-1.pdf}

We can add some labels, change the color, and background.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{days_lived =} \KeywordTok{as.integer}\NormalTok{(deathDate }\OperatorTok{-}\StringTok{ }\NormalTok{birthDate)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(days_lived)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ days_lived, }\DataTypeTok{fill =}\NormalTok{ gender)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{365}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{"Length of life of Australian politicians"}\NormalTok{, }
       \DataTypeTok{x =} \StringTok{"Age of deaths (days)"}\NormalTok{, }
       \DataTypeTok{y =} \StringTok{"Number"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-66-1.pdf}

I forget who said this but, `\texttt{ggplot} makes it so easy to have nicely labelled axes, there's no real excuse not to'.

\hypertarget{facets}{%
\subsection{Facets}\label{facets}}

Facets are subplots and are invaluable because they allow you to add another variable to your plot without having to make a 3D plot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian_politicians }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{days_lived =} \KeywordTok{as.integer}\NormalTok{(deathDate }\OperatorTok{-}\StringTok{ }\NormalTok{birthDate)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(days_lived)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ days_lived)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{365}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{"Length of life of Australian politicians"}\NormalTok{, }
       \DataTypeTok{x =} \StringTok{"Age of deaths (days)"}\NormalTok{, }
       \DataTypeTok{y =} \StringTok{"Number"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\NormalTok{gender)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-67-1.pdf}

\hypertarget{tidyverse-ii}{%
\section{Tidyverse II}\label{tidyverse-ii}}

\hypertarget{tibbles}{%
\subsection{Tibbles}\label{tibbles}}

A tibble is a data frame, but it is a data frame with some particular changes that make it easier to work with. You should read Chapter 10 of \citet{r4ds} for more detail. The main difference is that compared with a dataframe, a tibble doesn't convert strings to factors, and it prints nicely, including letting you know the class of a column.

You can make a tibble manually if you need, for instance this can be handy for simulating data, but usually we will just import data as a tibble.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{people <-}\StringTok{ }
\StringTok{  }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{names =} \KeywordTok{c}\NormalTok{(}\StringTok{"rohan"}\NormalTok{, }\StringTok{"monica"}\NormalTok{),}
         \DataTypeTok{website =} \KeywordTok{c}\NormalTok{(}\StringTok{"rohanalexander.com"}\NormalTok{, }\StringTok{"monicaalexander.com"}\NormalTok{),}
         \DataTypeTok{fav_colour =} \KeywordTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{" white "}\NormalTok{),}
\NormalTok{         )}
\NormalTok{people}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 3
##   names  website             fav_colour
##   <chr>  <chr>               <chr>     
## 1 rohan  rohanalexander.com  "blue"    
## 2 monica monicaalexander.com " white "
\end{verbatim}

\hypertarget{importing-data}{%
\subsection{Importing data}\label{importing-data}}

There are a variety of ways to import data. If you are dealing with CSV files then try \texttt{read\_csv()} in the first instance. There were examples of that in earlier sections.

\hypertarget{joining-data}{%
\subsection{Joining data}\label{joining-data}}

We can join two datasets together in a variety of ways. The most common join that I use is \texttt{left\_join()}, where I have one main dataset and I want to join another to it based on some common column names. Here we'll join two datasets based on favourite colour.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{both <-}\StringTok{ }
\StringTok{  }\NormalTok{simulated_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{left_join}\NormalTok{(people, }\DataTypeTok{by =} \StringTok{"fav_colour"}\NormalTok{)}

\NormalTok{both}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 10 x 6
##    person observation another_observation fav_colour names  website            
##     <int>       <dbl>               <dbl> <chr>      <chr>  <chr>              
##  1      1     -0.360                9.52  "blue"     rohan  rohanalexander.com 
##  2      2     -0.0406               0.586 " white "  monica monicaalexander.com
##  3      3     -1.78                 2.48  "blue"     rohan  rohanalexander.com 
##  4      4     -1.12                 5.80  " white "  monica monicaalexander.com
##  5      5     -1.00                 5.26  "blue"     rohan  rohanalexander.com 
##  6      6      1.78                 4.09  "blue"     rohan  rohanalexander.com 
##  7      7     -1.39                 3.97  "blue"     rohan  rohanalexander.com 
##  8      8     -0.497                2.52  " white "  monica monicaalexander.com
##  9      9     -0.558                6.29  "blue"     rohan  rohanalexander.com 
## 10     10     -0.824                8.57  "blue"     rohan  rohanalexander.com
\end{verbatim}

\hypertarget{strings}{%
\subsection{Strings}\label{strings}}

We've seen a string earlier, but it is an object that is created with single or double quotes. String manipulation is an entire book in itself, but you should start with the stringr package \citep{citestringr}.

I'll just cover a few essentials: \texttt{stringr::str\_detect()}, \texttt{stringr::str\_replace()}, \texttt{stringr::str\_squish()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(people)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 3
##   names  website             fav_colour
##   <chr>  <chr>               <chr>     
## 1 rohan  rohanalexander.com  "blue"    
## 2 monica monicaalexander.com " white "
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{people <-}\StringTok{ }
\StringTok{  }\NormalTok{people }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{is_rohan =}\NormalTok{ stringr}\OperatorTok{::}\KeywordTok{str_detect}\NormalTok{(names, }\StringTok{"rohan"}\NormalTok{),}
         \DataTypeTok{make_howlett =}\NormalTok{ stringr}\OperatorTok{::}\KeywordTok{str_replace}\NormalTok{(website, }\StringTok{"alexander"}\NormalTok{, }\StringTok{"howlett"}\NormalTok{),}
         \DataTypeTok{fav_colour_trim =}\NormalTok{ stringr}\OperatorTok{::}\KeywordTok{str_squish}\NormalTok{(fav_colour)}
\NormalTok{         )}

\KeywordTok{head}\NormalTok{(people)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 6
##   names  website            fav_colour is_rohan make_howlett     fav_colour_trim
##   <chr>  <chr>              <chr>      <lgl>    <chr>            <chr>          
## 1 rohan  rohanalexander.com "blue"     TRUE     rohanhowlett.com blue           
## 2 monica monicaalexander.c~ " white "  FALSE    monicahowlett.c~ white
\end{verbatim}

\hypertarget{pivot}{%
\subsection{Pivot}\label{pivot}}

Datasets tend to be either long or wide. Generally, in the tidyverse, and certainly for ggplot, we need long data. To go from one to the other you can use the \texttt{pivot\_longer()} and \texttt{pivot\_wider()} functions.

Let's see an example with some data on whether red team or blue team won a competition in some year.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pivot_example_data <-}\StringTok{ }
\StringTok{  }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{year =} \KeywordTok{c}\NormalTok{(}\DecValTok{2019}\NormalTok{, }\DecValTok{2020}\NormalTok{, }\DecValTok{2021}\NormalTok{),}
         \DataTypeTok{blue_team =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{),}
         \DataTypeTok{red_team =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}

\KeywordTok{head}\NormalTok{(pivot_example_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 3
##    year blue_team red_team
##   <dbl>     <dbl>    <dbl>
## 1  2019         1        2
## 2  2020         2        1
## 3  2021         1        2
\end{verbatim}

This dataset is in wide format at the moment. To get it into long format, what we'd like is to have a column that specifies the team, and then another that specifies the result. We'll use \texttt{tidyr::pivot\_longer}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data_pivoted_longer <-}\StringTok{ }
\StringTok{  }\NormalTok{pivot_example_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{tidyr}\OperatorTok{::}\KeywordTok{pivot_longer}\NormalTok{(}\DataTypeTok{cols =} \KeywordTok{c}\NormalTok{(}\StringTok{"blue_team"}\NormalTok{, }\StringTok{"red_team"}\NormalTok{),}
              \DataTypeTok{names_to =} \StringTok{"team"}\NormalTok{,}
               \DataTypeTok{values_to =} \StringTok{"position"}\NormalTok{)}

\KeywordTok{head}\NormalTok{(data_pivoted_longer)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 3
##    year team      position
##   <dbl> <chr>        <dbl>
## 1  2019 blue_team        1
## 2  2019 red_team         2
## 3  2020 blue_team        2
## 4  2020 red_team         1
## 5  2021 blue_team        1
## 6  2021 red_team         2
\end{verbatim}

Occasionally, you'll need to go from long data to wide data. We accomplish this with \texttt{tidyr::pivot\_wider}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data_pivoted_wider <-}\StringTok{ }
\StringTok{  }\NormalTok{data_pivoted_longer }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{tidyr}\OperatorTok{::}\KeywordTok{pivot_wider}\NormalTok{(}\DataTypeTok{id_cols =} \KeywordTok{c}\NormalTok{(}\StringTok{"year"}\NormalTok{, }\StringTok{"team"}\NormalTok{),}
                     \DataTypeTok{names_from =} \StringTok{"team"}\NormalTok{,}
                     \DataTypeTok{values_from =} \StringTok{"position"}\NormalTok{)}

\KeywordTok{head}\NormalTok{(data_pivoted_wider)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 3
##    year blue_team red_team
##   <dbl>     <dbl>    <dbl>
## 1  2019         1        2
## 2  2020         2        1
## 3  2021         1        2
\end{verbatim}

\hypertarget{factors}{%
\subsection{Factors}\label{factors}}

A factor is a string that has an inherent ordering. For instance, the days of the week have an order - Monday, Tuesday, Wednesday,\ldots{} - which is not alphabetical. Factors feature prominently in base, but they often add more complication than they are worth and so the tidyverse gives them a less prominent role. Nonetheless taking advantage of factors is useful in certain circumstances, for instance when plotting the days of the week we probably want them in the usual ordering than in the alphabetical ordering that would result if we had them as characters. The package that we use to deal with factors is \texttt{forcats} \citep{citeforcats}.

Sometimes you will have a character vector and you will want it ordered in a particular way. The default is that a character vector is ordered alphabetically, but you may not want that, for instance, the days of the week would look strange on a graph if they were alphabetically ordered: Friday, Monday, Saturday, Sunday, Thursday, Tuesday, Wednesday!

The way to change the ordering is to change the variable from a character to a factor. I would then use the \texttt{forcats} package to specify an ordering by hand. The help page is here: \url{https://forcats.tidyverse.org/reference/fct_relevel.html}.

Let's look at a concrete example.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_data <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{all_names =} \KeywordTok{c}\NormalTok{(}\StringTok{"Rohan"}\NormalTok{, }\StringTok{"Monica"}\NormalTok{, }\StringTok{"Edward"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

If we plotted this then Edward would be first, because it would be alphabetical. But if instead I want to be first as I am the oldest then we could use \texttt{forcats} in the following way.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(forcats) }\CommentTok{# (BTW you'll probably have to install that one)}
\KeywordTok{library}\NormalTok{(tidyverse)}

\NormalTok{my_data <-}
\StringTok{  }\NormalTok{my_data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{all_names =} \KeywordTok{factor}\NormalTok{(all_names), }\CommentTok{# Change to factor}
         \DataTypeTok{all_names_releveled =} \KeywordTok{fct_relevel}\NormalTok{(all_names, }\StringTok{"Rohan"}\NormalTok{, }\StringTok{"Monica"}\NormalTok{)) }\CommentTok{# Change the levels}

\CommentTok{# Then compare the two}
\NormalTok{my_data}\OperatorTok{$}\NormalTok{all_names}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] Rohan  Monica Edward
## Levels: Edward Monica Rohan
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_data}\OperatorTok{$}\NormalTok{all_names_releveled}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] Rohan  Monica Edward
## Levels: Rohan Monica Edward
\end{verbatim}

\hypertarget{cases}{%
\subsection{Cases}\label{cases}}

If you need to write a few conditional statements then \texttt{case\_when} is the way to go.

Let's start with a tibble of dates and pretend that we want to group them into `pre-1950', `1950-2000', `2000-onwards'

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{case_when_example <-}\StringTok{ }
\StringTok{  }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{some_dates =} \KeywordTok{c}\NormalTok{(}\StringTok{"1909-12-31"}\NormalTok{, }\StringTok{"1919-12-31"}\NormalTok{, }\StringTok{"1929-12-31"}\NormalTok{, }\StringTok{"1939-12-31"}\NormalTok{, }
                        \StringTok{"1949-12-31"}\NormalTok{, }\StringTok{"1959-12-31"}\NormalTok{, }\StringTok{"1969-12-31"}\NormalTok{, }\StringTok{"1979-12-31"}\NormalTok{, }
                        \StringTok{"1989-12-31"}\NormalTok{, }\StringTok{"1999-12-31"}\NormalTok{, }\StringTok{"2009-12-31"}\NormalTok{)}
\NormalTok{         )}

\NormalTok{case_when_example <-}\StringTok{ }
\StringTok{  }\NormalTok{case_when_example }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{some_dates =}\NormalTok{ lubridate}\OperatorTok{::}\KeywordTok{ymd}\NormalTok{(some_dates)}
\NormalTok{         )}

\KeywordTok{head}\NormalTok{(case_when_example)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 1
##   some_dates
##   <date>    
## 1 1909-12-31
## 2 1919-12-31
## 3 1929-12-31
## 4 1939-12-31
## 5 1949-12-31
## 6 1959-12-31
\end{verbatim}

Now we'll use \texttt{dplyr::case\_when()} to group these.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{case_when_example <-}\StringTok{ }
\StringTok{  }\NormalTok{case_when_example }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{year_group =} 
           \KeywordTok{case_when}\NormalTok{(}
\NormalTok{             some_dates }\OperatorTok{<}\StringTok{ }\NormalTok{lubridate}\OperatorTok{::}\KeywordTok{ymd}\NormalTok{(}\StringTok{"1950-01-01"}\NormalTok{) }\OperatorTok{~}\StringTok{ "pre-1950"}\NormalTok{,}
\NormalTok{             some_dates }\OperatorTok{<}\StringTok{ }\NormalTok{lubridate}\OperatorTok{::}\KeywordTok{ymd}\NormalTok{(}\StringTok{"2000-01-01"}\NormalTok{) }\OperatorTok{~}\StringTok{ "1950-2000"}\NormalTok{,}
\NormalTok{             some_dates }\OperatorTok{>=}\StringTok{ }\NormalTok{lubridate}\OperatorTok{::}\KeywordTok{ymd}\NormalTok{(}\StringTok{"2000-01-01"}\NormalTok{) }\OperatorTok{~}\StringTok{ "2000-onwards"}\NormalTok{,}
             \OtherTok{TRUE} \OperatorTok{~}\StringTok{ "CHECK ME"}
\NormalTok{             )}
\NormalTok{  )}

\KeywordTok{head}\NormalTok{(case_when_example)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 2
##   some_dates year_group
##   <date>     <chr>     
## 1 1909-12-31 pre-1950  
## 2 1919-12-31 pre-1950  
## 3 1929-12-31 pre-1950  
## 4 1939-12-31 pre-1950  
## 5 1949-12-31 pre-1950  
## 6 1959-12-31 1950-2000
\end{verbatim}

We could accomplish this with a series of \texttt{if\_else} statements, but \texttt{case\_when} is just a bit cleaner. The only thing to be aware of is that statements are evaluated in order. So as soon as something matches it doesn't continue down the list of conditions. That's why we have that catch-all at the end - if the date doesn't fit any of the earlier conditions then we've got a problem and want to know about it.

\hypertarget{workflow}{%
\chapter{Workflow}\label{workflow}}

\textbf{Required reading}

\begin{itemize}
\tightlist
\item
  Alexander, Monica, 2019, `Reproducibility in demographic research', \url{https://www.monicaalexander.com/posts/2019-10-20-reproducibility/}.
\item
  Bailey, Jack, 2020, `UK Voting Intention Poll Tracker', \url{https://github.com/jackobailey/poll_tracker}.
\item
  Bryan, Jennifer and Jim Hester, 2020, `What they Forgot to Teach You About R', Chapters 11, 12, and 13, \url{https://rstats.wtf/debugging-r-code.html}.
\item
  Bryan, Jenny, 2020, \emph{Happy Git and GitHub for the useR}, Chapters 4, 6, 7, 9, \url{https://happygitwithr.com/}.
\item
  Fan, Jean, `R Style Guide', \url{https://jef.works/R-style-guide/}.
\item
  Gelman, Andrew, 2016, `What has happened down here is the winds have changed', \url{https://statmodeling.stat.columbia.edu/2016/09/21/what-has-happened-down-here-is-the-winds-have-changed/}.
\item
  Healy, Kieran, 2020, `The Kitchen Counter Observatory', 21 May, \url{https://kieranhealy.org/blog/archives/2020/05/21/the-kitchen-counter-observatory/}.
\item
  Hill, Alison, 2020, `How I Teach R Markdown', 28 May, \url{https://alison.rbind.io/post/2020-05-28-how-i-teach-r-markdown/}.
\item
  Mostipak, Jesse, 2018, `So you've been asked to make a reprex', 24 February, \url{https://www.jessemaegan.com/post/so-you-ve-been-asked-to-make-a-reprex/}.
\item
  Phillips, Nathaniel D., 2018, \emph{YaRrr! The Pirate's Guide to R}, Chapter 4.3, \url{https://bookdown.org/ndphillips/YaRrr/a-brief-style-guide-commenting-and-spacing.html}.
\item
  Taback, Nathan, 2019, `R Markdown for Class Reports', \url{https://scidesign.github.io/Rmarkdownforclassreports.html}.
\item
  Tierney, Nicholas, 2020, \emph{RMarkdown for Scientists}, 9 September, Chapters 7-10, 12-15, \url{https://rmd4sci.njtierney.com}.
\item
  Wickham, Hadley, \emph{Advanced R}, Chapter on Style, \url{http://adv-r.had.co.nz/Style.html}.
\item
  Wickham, Hadley, and Garrett Grolemund, 2017, \emph{R for Data Science}, Chapter 27, \url{https://r4ds.had.co.nz/}.
\item
  Wilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, Tracy K. Teal, 2017, `Good enough practices in scientific computing', \emph{PLoS Computational Biology}, 13(6).
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
  AirbnbEng, 2016, `Scaling Knowledge at Airbnb', 25 February, \url{https://medium.com/airbnb-engineering/scaling-knowledge-at-airbnb-875d73eff091}.
\item
  Bik, Elisabeth, 2020, `The Tadpole Paper Mill', \emph{Science Integrity Digest}, 21 February, \url{https://scienceintegritydigest.com/2020/02/21/the-tadpole-paper-mill/}.
\item
  Chan, Martin, 2020, `RStudio Projects and Working Directories: A Beginner's Guide', \url{https://martinctc.github.io/blog/rstudio-projects-and-working-directories-a-beginner's-guide/}.
\item
  Daly, Darren, 2020, `A brief history of medical statistics and its impact on reproducibility', 2 February, \url{https://medium.com/@darren_dahly/a-brief-history-of-medical-statistics-and-its-role-in-reproducibility-23e31082f024}.
\item
  Ebert, Philip A. ``Bayesian reasoning in avalanche terrain: a theoretical investigation.'' Journal of Adventure Education and Outdoor Learning 19, no. 1 (2019): 84-95.
\item
  Gertler, Paul, Sebastian Martinez, Patrick Premand, Laura Rawlings, and Christel Vermeersch, `Impact Evaluation in Practice', Chapters 1, 2.
\item
  Guzey, Alexey, 2020, `How Life Sciences Actually Work: Findings of a Year-Long Investigation', \url{https://guzey.com/how-life-sciences-actually-work/}.
\item
  King, Gary, `How to Write a Publishable Paper as a Class Project', \url{https://gking.harvard.edu/papers}.
\item
  King, Gary, 2006, `Publication, publication', \emph{PS: Political Science \& Politics}, vol.~39, pp.~119-125, \url{https://gking.harvard.edu/files/gking/files/paperspub.pdf}.
\item
  Lowndes, Julia S. Stewart, Benjamin D. Best, Courtney Scarborough, Jamie C. Afflerbach, Melanie R. Frazier, Casey C. O'Hara, Ning Jiang, and Benjamin S. Halpern, 2017, `Our path to better science in less time using open data science tools', Nature ecology \& evolution, 1(6).
\item
  Miyakawa, Tsuyoshi, 2020, `No raw data, no science: another possible source of the reproducibility crisis', \emph{Molecular Brain}, 13, 24. \url{https://doi.org/10.1186/s13041-020-0552-2}
\item
  Peek, Ryan, 2020, `10 tips to souping up rmarkdown', 20 February, \url{https://ryanpeek.github.io/2020-02-20-10-tips-to-souping-up-rmarkdown/\#S8}.
\item
  Riederer, Emily, 2019, `Resource Round-Up: Reproducible Research Edition', 29 August, \url{https://emilyriederer.netlify.com/post/resource-round-up-reproducible-research-edition/}.
\item
  Riederer, Emily, 2019, `RMarkdown Driven Development (RmdDD)', 4 May, \url{https://emilyriederer.netlify.com/post/rmarkdown-driven-development/}.
\item
  Silver, Nate, 2020, `We Fixed An Issue With How Our Primary Forecast Was Calculating Candidates' Demographic Strengths', 19 February, \url{https://fivethirtyeight.com/features/we-fixed-a-mistake-in-how-our-primary-forecast-was-calculating-candidates-demographic-strengths/}.
\item
  Soetewey, Antoine, 2020, `Getting started in R markdown', 18 February, \url{https://www.statsandr.com/blog/getting-started-in-r-markdown/}.
\item
  Varian, Hal, `How to Build an Economic Model in Your Spare Time', \url{http://people.ischool.berkeley.edu/~hal/Papers/how.pdf}. (This paper was written a while ago for economists, please just ignore the economics specific parts.)
\item
  Wayne, Hillel, 2017, `How do we trust our science code?', 14 August, \url{https://www.hillelwayne.com/post/how-do-we-trust-science-code/}.
\item
  Wayne, Hillel, 2020, `This is how science happens', 9 March, \url{https://www.hillelwayne.com/post/this-is-how-science-happens/}.
\end{itemize}

\textbf{Required viewing}

\begin{itemize}
\tightlist
\item
  Bryan, Jenny, `Debugging in R', \url{https://resources.rstudio.com/rstudio-conf-2020/object-of-type-closure-is-not-subsettable-jenny-bryan}.
\end{itemize}

\textbf{Recommended viewing}

\begin{itemize}
\tightlist
\item
  Gelfand, Sharla, 2020, `Don't repeat yourself, talk to yourself! Repeated reporting in the R universe', 30 January, talk given at rstudio::conf, San Francisco, \url{https://resources.rstudio.com/rstudio-conf-2020/dont-repeat-yourself-talk-to-yourself-repeated-reporting-in-the-r-universe-sharla-gelfand}.
\end{itemize}

\textbf{Recommended activity}

\begin{itemize}
\tightlist
\item
  `First Contributions' \url{https://github.com/forwards/first-contributions}.
\end{itemize}

\textbf{Fun song}

\begin{itemize}
\tightlist
\item
  The Magnetic Fields, 2016, `'86 How I Failed Ethics', 17 November, \url{https://youtu.be/Hu5dEXZ7DOY} (thanks to Paul Hodgetts).
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\tightlist
\item
  Restart R often (Session -\textgreater{} Restart R and Clear Output).
\item
  Debugging is a skill, and you will get better at it with time and practice.
\item
  Start with reading the error message.
\item
  Check the class.
\item
  You may get frustrated at times, this is normal.
\item
  There are various tools that can help. Google is your friend.
\item
  Make a small example and try to get the code running on that.
\item
  Cultivating a tenacious mentality may help.
\item
  Writing code that future-you can understand.
\item
  Developing important questions.
\item
  Reproducibility and replicability
\item
  The importance of data and code access
\item
  The importance of version control in a modern scientific workflow.
\item
  The basics of Git and GitHub, as a solo data scientist.
\end{itemize}

\textbf{Key GitHub workflow with commands}

\begin{itemize}
\tightlist
\item
  Get the latest changes: \texttt{git\ pull}.
\item
  Add your updates: \texttt{git\ add\ -A}.
\item
  Check on everything: \texttt{git\ status}.
\item
  Commit your changes: \texttt{git\ commit\ -m\ "Short\ description\ of\ changes"}.
\item
  Push your changes to GitHub: \texttt{git\ push}.
\end{itemize}

\textbf{Quiz}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What are three features of a good research question (write a paragraph or two)?
\item
  What is a counterfactual (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    If-then statements in which the if didn't happen.
  \item
    If-then statements in which the if happen.
  \item
    Statements that are either true or false.
  \item
    Statements that are neither true or false.
  \end{enumerate}
\item
  How do you hide the warnings in a R Markdown R chunk (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{echo\ =\ FALSE}
  \item
    \texttt{include\ =\ FALSE}
  \item
    \texttt{eval\ =\ FALSE}
  \item
    \texttt{warning\ =\ FALSE}
  \item
    \texttt{message\ =\ FALSE}
  \end{enumerate}
\item
  What is a reprex and why is it important to be able to make one (select all)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A reproducible example that enables your error to be reproduced.
  \item
    A reproducible example that helps others help you.
  \item
    A reproducible example during the construction of which you may solve your own problem.
  \item
    A reproducible example that demonstrates you've actually tried to help yourself.
  \end{enumerate}
\item
  Why are R Projects important (select all)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    They help with reproducibility.
  \item
    They make it easier to share code.
  \item
    They make your workspace more organized.
  \item
    They are all that needs to be done.
  \end{enumerate}
\item
  Consider this sequence: `\texttt{git\ pull}, \texttt{git\ status}, \_\_\_\_\_\_\_\_, \texttt{git\ status}, \texttt{git\ commit\ -m\ "My\ message"}, \texttt{git\ push}'. What is the missing step (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{git\ add\ -A}.
  \item
    \texttt{git\ status}.
  \item
    \texttt{git\ pull}.
  \item
    \texttt{git\ push}.
  \end{enumerate}
\end{enumerate}

\hypertarget{introduction-1}{%
\section{Introduction}\label{introduction-1}}

\begin{quote}
Suppose you have cancer and you have to choose between a black box AI surgeon that cannot explain how it works but has a 90\% cure rate and a human surgeon with an 80\% cure rate. Do you want the AI surgeon to be illegal?

\href{https://twitter.com/geoffreyhinton/status/1230592238490615816?s=21}{Geoffrey Hinton}, 20 February 2020.
\end{quote}

\begin{quote}
The number one thing to keep in mind about machine learning is that performance is evaluated on samples from one dataset, but the model is used in production on samples that may not necessarily follow the same characteristics\ldots{} The finance industry has a saying for this: ``past performance is no guarantee of future results''. Your model scoring X on your test dataset doesn't mean it will perform at level X on the next N situations it encounters in the real world. The future may not be like the past.

So when asking the question, ``would you rather use a model that was evaluated as 90\% accurate, or a human that was evaluated as 80\% accurate'', the answer depends on whether your data is typical per the evaluation process. Humans are adaptable, models are not. If significant uncertainty is involved, go with the human. They may have inferior pattern recognition capabilities (versus models trained on enormous amounts of data), but they understand what they do, they can reason about it, and they can improvise when faced with novelty

If every possible situation is known and you want to prioritize scalability and cost-reduction, go with the model. Models exist to encode and operationalize human cognition in well-understood situations. (``well understood'' meaning either that it can be explicitly described by a programmer, or that you can amass a dataset that densely samples the distribution of possible situations -- which must be static)

\href{https://twitter.com/fchollet/status/1230611916701229064?s=21}{François Chollet}, 20 February 2020.
\end{quote}

If science is about systematically building and organising knowledge in terms of testable explanations and predictions, then data science takes this and focuses on data. Fundamentally data science is still science, and as such, building and organising knowledge is a critical aspect. Being able to do something yourself, once, does not achieve this. Hence, the focus on reproducibility and replicability.

\citet{Alexander2019} says `Research is reproducible if it can be reproduced exactly, given all the materials used in the study.' `{[}Hence{]} materials need to be provided!'. '`{[}M{]}aterials' usually means data, code and software.' The minimum requirement is to be able to `\protect\hyperlink{r}{r}eproduce the data, methods and results (including figures, tables)'.

Similarly, from \citet{Gelman2016} you should have noticed that this has been an issue in other sciences. e.g.~psychology. The issue with not being reproducible is that we are not contributing to knowledge. We no longer have any idea was is fact in the field of psychology. (This is coming for other fields too, including the field that I was trained in - economics.) Do this matter? Yes.

Some of the examples that \citet{Gelman2016} cites (which turned out to be dodgy) don't really matter e.g.~ESP or the power pose. It doesn't really matter. But increasingly the same methods are being applied in areas where they do matter e.g.~`nudge' units. Similarly, \citet{Simpson2017} makes it clear that it's a big problem in data science.

The `gay face' paper that \citet{Simpson2017} writes about has not released their dataset. We have no way of knowing what is going on with it. They have found a certain set of results based on that dataset, their methods, and what they did, but we have no way of knowing how much that matters. As \citet{Simpson2017} says `the paper itself does some things right. It has a detailed discussion of the limitations of the data and the method'. You must do this in everything that you write, but it is not enough.

Without the data, we don't know what their results speak to as we don't understand how representative the sample is. If the dataset is biased, then that undermines their claims. There's a reason that while initial medical trials are done on mice, etc, eventually human trials are required.

In order to do the study they needed a trained dataset. They trained it using Mechanical Turk. Figure \ref{fig:mattsonexamplemap} is from \citet{Mattson2017}.

\begin{figure}
\includegraphics[width=0.9\linewidth]{figures/gayface_extract} \caption{Instructions for workers to do classification piecework on the Amazon Mechanical Turk platform, p. 46. From Mattson.}\label{fig:mattsonexamplemap}
\end{figure}

\citet{Mattson2017} comments:

\begin{quote}
The problems here are legion: Barack Obama is biracial but simply ``Black'' by American cultural norms. ``Clearly Latino'' begs the question ``to whom?'' Latino is an ethnic category, not a racial one: many Latinos already are Caucasian, and increasingly so. By training their workers according to stereotypical American categories, WAK's algorithm can only spit out the garbage they put in.
\end{quote}

`WAK's algorithm can only spit out the garbage they put in.' I would encourage you to print out that statement and paste it somewhere that you will see it every time you get a data science result.

What steps can we take to make our work reproducible?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ensure your entire workflow is documented. How did you get the raw data? Can you save the raw data? Will the raw data always be available? Is the raw data available to others? What steps are you taking to transform the raw data in data that can be analysed? How are you analysing the data? How are you building the report?
\item
  Try to improve each time. Can you run your entire workflow again? Can `another person' run your entire workflow again? Can a future you run your entire workflow again? Can a future `another person' run your entire workflow again? Each of these requirements is increasingly more onerous. We are going to start with worrying about the first. The way we are going to do this is by using R Markdown.
\end{enumerate}

\hypertarget{r-markdown}{%
\section{R Markdown}\label{r-markdown}}

\hypertarget{getting-started-2}{%
\subsection{Getting started}\label{getting-started-2}}

R Markdown is a mark-up language similar to html or LaTeX, in comparison to a WYSIWYG language, such as Word. This means that all of the aspects are consistent, for instance, all `main headings' will look the same. However it means that use symbols to designate how you would like certain aspects to appear, and it is only when you compile it that you get to see it.

R Markdown is a variant of regular markdown that is specifically designed to allow R code chunks to be included. The advantage is that you can get a `live' document in which code executes and is then printed to a document. The disadvantage is that it can take a while for the document to compile because all of the code needs to run.

You can create a new R Markdown document within R Studio (File -\textgreater{} New File -\textgreater{} R Markdown Document). Another advantage of R Markdown is that very similar code can compile into a variety of documents, including html pages and PDFs. R Markdown also has default options set up for including a title, author, and date sections.

\hypertarget{basic-commands}{%
\subsection{Basic commands}\label{basic-commands}}

If you ever need a reminder of the basics of R Markdown then this is built into R Studio (Help -\textgreater{} Markdown Quick Reference). This provides the code for commonly needed commands:

\begin{itemize}
\tightlist
\item
  Emphasis: \texttt{*italic*,\ **bold**,\ \_italic\_,\ \_\_bold\_\_}
\item
  Headers (these need to go on their own line with a line before and after): \texttt{\#\ Header\ 1,\ \#\#\ Header\ 2,\ \#\#\#\ Header\ 3}
\item
  Lists:
\end{itemize}

\begin{verbatim}
Unordered List
* Item 1
* Item 2
    + Item 2a
    + Item 2b
Ordered List
1. Item 1
2. Item 2
3. Item 3
    + Item 3a
    + Item 3b
\end{verbatim}

\begin{itemize}
\tightlist
\item
  URLs: Can just include an address: \url{http://example.com}, or can include a \texttt{{[}linked\ phrase{]}(http://example.com)}.
\item
  Basic images can just be included either from the internet: \texttt{!{[}alt\ text{]}(http://example.com/logo.png)} or from a local file: \texttt{!{[}alt\ text{]}(figures/img.png)}.
\end{itemize}

In order to create an actual document, once you have these pieces set up, click `Knit'.

\hypertarget{r-chunks}{%
\subsection{R chunks}\label{r-chunks}}

You can include R (and a bunch of other languages) code in code chunks within your R Markdown document. Then when you knit your document, the R code will run and be included in your document.

To create an R chunk start with three backticks and then within curly braces tell markdown that this is an R chunk. Anything inside this chunk will be considered R code and run as such.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ diamonds) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ price, }\DataTypeTok{y =}\NormalTok{ carat))}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-79-1.pdf}

There are various evaluation options that are available in chunks. You include these by putting a comma after \texttt{r} and then specifying any options before the closing curly brace. Helpful options include:

\begin{itemize}
\tightlist
\item
  \texttt{echo\ =\ FALSE}: run the code and include the output, but don't print the code in the document.
\item
  \texttt{include\ =\ FALSE}: run the code but don't output anything and don't print the code in the document.
\item
  \texttt{eval\ =\ FALSE}: don't run the code, and hence don't include the outputs, but do print the code in the document.
\item
  \texttt{warning\ =\ FALSE}: don't display warnings.
\item
  \texttt{message\ =\ FALSE}: don't display messages.
\end{itemize}

\hypertarget{abstracts-and-pdf-outputs}{%
\subsection{Abstracts and PDF outputs}\label{abstracts-and-pdf-outputs}}

In the default header, you can add a section for a header, so that it would look like this:

\begin{verbatim}
---
title: My document
author: Rohan Alexander
date: 5 January 2020
output: html_document
abstract: "This is my abstract."
---
\end{verbatim}

Similarly, you can change the output from \texttt{html\_document} to \texttt{pdf\_document} in order to produce a PDF. This uses LaTeX in the background so you may need to install a bunch of related packages.

\hypertarget{references}{%
\subsection{References}\label{references}}

You can reference a bibliography by including one in the preamble and then calling it in the text when you need.

\begin{verbatim}
---
title: My document
author: Rohan Alexander
date: 5 January 2020
output: html_document
abstract: "This is my abstract."
bibliography: bibliography.bib
---
\end{verbatim}

You need to make a separate file called \texttt{bibliography.bib}. In that you need an entry for the item that you want to reference. R and R packages usually provide this for you for instance, if you run \texttt{citation()} then it tells you the entry to put in your bibtex file:

\begin{verbatim}
@Manual{,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2020},
    url = {https://www.R-project.org/},
  }
\end{verbatim}

You need to create a unique key that you'll refer to it with in the text. This can be anything if it's unique, but I try to use meaningful ones, so that bibtex entry could become:

\begin{verbatim}
@Manual{citeR,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2020},
    url = {https://www.R-project.org/},
  }
\end{verbatim}

And to cite R you'd then include the following: \texttt{@citeR}, which would put the brackets around the year, like this: \citet{citeR} or \texttt{{[}@citeR{]}}, which would put the brackets around the whole thing, like this: \citep{citeR}.

\hypertarget{cross-references}{%
\subsection{Cross-references}\label{cross-references}}

Finally, it can be useful to cross-reference figures, tables and equations. This makes it easier to refer to them in the text. To do this for a figure you refer to the name of the R chunk that creates/contains the figure. For instance, \texttt{(Figure\ \textbackslash{}@ref(fig:billssssss))} will produce: (Figure \ref{fig:billssssss}) as the name of the R chunk is \texttt{billssssss} (don't forget to add the \texttt{fig} in front of the chunk name.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(palmerpenguins)}
\KeywordTok{ggplot}\NormalTok{(penguins, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ island, }\DataTypeTok{fill =}\NormalTok{ species)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.8}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_manual}\NormalTok{(}\DataTypeTok{values =} \KeywordTok{c}\NormalTok{(}\StringTok{"darkorange"}\NormalTok{,}\StringTok{"purple"}\NormalTok{,}\StringTok{"cyan4"}\NormalTok{),}
                    \DataTypeTok{guide =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\NormalTok{species, }\DataTypeTok{ncol =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{telling_stories_with_data_files/figure-latex/billssssss-1.pdf}
\caption{\label{fig:billssssss}More bills of penguins}
\end{figure}

You can do similar for tables and equations e.g \texttt{(Table\ \textbackslash{}@ref(tab:penguinhead))} will produce: (Table \ref{tab:penguinhead}) (again, don't forget to ad \texttt{tab} in front).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(species, bill_length_mm, bill_depth_mm) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{slice}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(}\DataTypeTok{caption =} \StringTok{"A penguin table"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:penguinhead}A penguin table}
\centering
\begin{tabular}[t]{l|r|r}
\hline
species & bill\_length\_mm & bill\_depth\_mm\\
\hline
Adelie & 39.1 & 18.7\\
\hline
Adelie & 39.5 & 17.4\\
\hline
Adelie & 40.3 & 18.0\\
\hline
Adelie & NA & NA\\
\hline
Adelie & 36.7 & 19.3\\
\hline
\end{tabular}
\end{table}

And finally, you can, and should, cross-reference equations also, but this time you need to add a tag \texttt{(\textbackslash{}\#eq:slope)} and then reference that e.g.~use \texttt{Equation\ \textbackslash{}@ref(eq:slope).} to produce Equation \eqref{eq:slope}.

\begin{verbatim}
\begin{equation}
Y = a + b X \label{eq:slope}
\end{equation}
\end{verbatim}

\begin{equation}
Y = a + b X \label{eq:slope}
\end{equation}

\hypertarget{r-projects}{%
\section{R projects}\label{r-projects}}

RStudio has the option of creating a project, which allows you to keep all the files (data, analysis, report etc) associated with a particular project together. To create a project, click Click File \textgreater{} New Project, then select empty project, name your project and think about where you want to save it. For example, if you are creating a project for Problem Set 2, you might call it \texttt{ps2} and save it in a sub-folder called \texttt{PS2} in your INF2178 folder.

Once you have created a project, a new file with the extension \texttt{.RProj} will appear in that file. As an example, download the R help folder. Whenever I work on class materials, I open the project file and work from that.

The main advantage of projects is that you don't have to set the working directory or type the whole file path to read in a file (for example, a data file). So instead of reading a csv from \texttt{"\textasciitilde{}/Documents/toronto/teaching/INF2178/data/"} you can just read it in from \texttt{data/}.

To meet even the minimal expected level of reproducibility, you must use R projects. You must not use \texttt{setwd()} because that ties your work to your computer.

\hypertarget{git-and-github}{%
\section{Git and GitHub}\label{git-and-github}}

\hypertarget{introduction-2}{%
\subsection{Introduction}\label{introduction-2}}

Here we introduce Git and GitHub. These are tools that:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  enhance the reproducibility of your work by making it easier to share your code and data;
\item
  make it easier to show off your work;
\item
  improve your workflow by encouraging you to think systematically about your approach; and
\item
  (although we won't take advantage of this) make it easier to work in teams.
\end{enumerate}

Git is a version control system. One way you might be used to doing version control is to have various versions of your files: \texttt{first\_go.R}, \texttt{first\_go-fixed.R}, \texttt{first\_go-fixed-with-mons-edits.R}. But this soon becomes cumbersome. Some of you may use dates, for instance: \texttt{2020-03-12-analysis.R}, \texttt{2020-03-13-analysis.R}, \texttt{2020-03-14-analysis.R}, etc. While this keeps a record it can be difficult to search if you need to go back - will you really remember the date of some change in a week? How about a month or a year? It gets unwieldy fairly quickly.

Instead of this, Git allows you to only have one version of the file \texttt{analysis.R} and it keeps a record of the changes to that file, and a snapshot of that file at a given point in time. When it takes that snapshot is determined by you. When you want Git to take a snapshot you additionally include a message, saying what changed between this snapshot and the last. In that way, there is only ever one version of the file, but the history can be more easily searched.

The issue is that Git was designed for software developers. As such, while it works, it can be a little ungainly for non-developers (Figure \ref{fig:hackernews}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/hacker_news} \caption{An infamous response to the launch of Dropbox in 2007, trivialising the use-case for Dropbox, and while this actually would work, it wouldn't for most of us.}\label{fig:hackernews}
\end{figure}

Hence, GitHub, GitLab, and various other companies offer easier-to-use services that build on Git. GitHub used to be the weapon of choice, but they were sold to Microsoft in 2018 and since then other variants such as GitLab have risen in popularity. We will introduce GitHub here because it remains the most popular, and it is built into RStudio, however you should feel free to explore other options.

One of the hardest aspects of Git, and the rest, for me was the terminology. Folders are called `repos'. Saving is called a `commit'. You'll get used to it eventually, but just so you know - it's not you, it's Git - feeling confused it entirely normal

These are brief notes and you should refer to Jenny Bryan's book for further detail. Frankly, I can't improve on Bryan's book and I use them regularly myself.

\hypertarget{git}{%
\subsection{Git}\label{git}}

Check if you have Git installed by opening R Studio and then going to the Terminal and typing the following and then return.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ --version}
\end{Highlighting}
\end{Shaded}

If you get a version number, then you are done (Figure \ref{fig:gitone}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/git_one} \caption{How to access the Terminal within R Studio}\label{fig:gitone}
\end{figure}

If you have a Mac then Git should come pre-installed, if you have Windows then there's a chance, and if you have Linux then you probably don't need this guide. If you don't get a version number, then you need to install it. Please go to Chapter 5 of \citet{happygit} for some instructions based on your operating system.

After you have Git, then you need to tell it your username and email. You need this because Git adds this information whenever you take a `snapshot', or to use Git's language whenever you make a commit.

Again, within the Terminal, type the following, replacing the details with yours, and then return after each line.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ config --global user.name }\StringTok{'Jane Doe'}
\FunctionTok{git}\NormalTok{ config --global user.email }\StringTok{'jane@example.com'}
\FunctionTok{git}\NormalTok{ config --global --list}
\end{Highlighting}
\end{Shaded}

The details that you enter here will be public (there are various ways to hide your email address if you need to do this and GitHub provides instructions about this).

Again, if you have issues or need more detailed instructions please go to Chapter 7 of \citet{happygit}.

\hypertarget{github}{%
\subsection{GitHub}\label{github}}

The first step is to create an account on GitHub (\url{https://github.com}) (Figure \ref{fig:githubone}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/github_1} \caption{Sign up screen at GitHub}\label{fig:githubone}
\end{figure}

GitHub doesn't have the most intuitive user experience in the world, but we are now going to make a new folder (which is called a `repo' in Git). You are looking for a plus sign in the top right, and then select `New Repository' (Figure \ref{fig:githubtwo}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/github_2} \caption{Create a new repository}\label{fig:githubtwo}
\end{figure}

At this point you can add a sensible name for your repo. Leave it as public (you can delete it later if you want). And check the box to initialize with a readme. In the `Add .gitignore' option you can leave it for now, but if you start using GitHub more regularly then you may like to select the R option here. (That just tells Git to ignore various files.) After that, just click the button to create a new repository (Figure \ref{fig:githubthree}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/github_3} \caption{Create a new repository, really}\label{fig:githubthree}
\end{figure}

You'll now be taken to a screen that is fairly empty, but the details that you need are in the green `Clone or Download' button, then click the clipboard (Figure \ref{fig:githubfour}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/github_4} \caption{Get the details of your new repository}\label{fig:githubfour}
\end{figure}

Now you need to open Terminal, and use \texttt{cd} to get to where you want to save the folder, then:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ clone https://github.com/RohanAlexander/test.git}
\end{Highlighting}
\end{Shaded}

At this point, a new folder has been created. We can now interact with it.

The first step is almost always to pull the latest changes with \texttt{git\ pull} (this is slightly pointless in this example because it's just us but it's a good habit to get into). We can then make a change to the folder, for instance, update the readme, and then save it as usual. Once this is done, we need to add, commit, and push. As before, use \texttt{cd} to navigate to your folder, then \texttt{git\ status} to see if there is anything going on (you should see some reference to the change you made). Then \texttt{git\ add\ -A} adds the changes to the staging area (this seems pointless, and it is in this context, but this allows you to specify specific files and similar if needed). Then \texttt{git\ status} to check what has happened. Then \texttt{git\ commit\ -m\ "Minor\ update\ to\ readme"}, and then \texttt{git\ status} to check on everything, and finally \texttt{git\ push}.

To summarise (assuming you are in the relevant folder):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ pull}
\FunctionTok{git}\NormalTok{ status}
\FunctionTok{git}\NormalTok{ add -A}
\FunctionTok{git}\NormalTok{ status}
\FunctionTok{git}\NormalTok{ commit -m }\StringTok{"Short commit message"}
\FunctionTok{git}\NormalTok{ status}
\FunctionTok{git}\NormalTok{ push}
\end{Highlighting}
\end{Shaded}

\hypertarget{using-git-within-rstudio}{%
\subsection{Using Git within RStudio}\label{using-git-within-rstudio}}

I promised that GitHub was built into RStudio, but so far we've not really taken advantage of that. The way to do it is to create a new repo in GitHub, and copy the information, as before.

At this point, you open RStudio, select \texttt{Files}, \texttt{New\ Project}, \texttt{Version\ Control}, \texttt{Git}, and paste the information for the repo. Go through the rest of it, saving the folder somewhere sensible, and clicking `Open in new session'. This will then create a new folder on your computer which will be a Git folder that is linked to the GitHub repo that you created.

At this point, you'll have a `Git' tab (Figure \ref{fig:rstudiogit}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/RStudio_git} \caption{The Git pane in R Studio}\label{fig:rstudiogit}
\end{figure}

First pull (click the blue down arrow). Now you want to tick the `staged' box against the files that you want to commit. Then click `Commit'. Type a message in the `Commit message' box and then click `Commit'. Finally `Push'. Again, the details are in \citet{happygit}, especially Chapter 12.

\hypertarget{next-steps}{%
\subsection{Next steps}\label{next-steps}}

We haven't really taken advantage of GitHub's features in terms of teams or branches. That is certainly something that you should get into once you have more confidence with these basics. As always, when it comes to Git for data scientists who use R, you should go to the relevant sections of \citet{happygit}.

\hypertarget{using-r-in-practice}{%
\section{Using R in practice}\label{using-r-in-practice}}

\hypertarget{introduction-3}{%
\subsection{Introduction}\label{introduction-3}}

This section is what do when your code doesn't do what you want, discusses a mindset that may help when doing quantitative analysis with R, and finally, some recommendations around how to write your code.

\hypertarget{getting-help}{%
\subsection{Getting help}\label{getting-help}}

Programming is hard and everyone struggles sometimes. At some point your code won't run or will throw an error. This is normal, and it happens to everyone. It happens to me on a daily, sometimes hourly, basis. Everyone gets frustrated. There are a few steps that are worthwhile taking when this happens:

\begin{itemize}
\tightlist
\item
  Sometimes the error messages in R are useful. Read it carefully and see if there's anything of use in it. At the very least, if you get the same message in the future, hopefully you might remember how you solved the problem this time!
\item
  If you're getting an error then try googling it, (I find it can help to include the term `R' or `tidyverse' or the relevant package name).
\item
  If there's a particular function that seems to be giving trouble, have a look at the help file for it. Sometimes you might be putting in the arguments in the wrong order. You can do this with `?function' e.g.~for help with select, you would type `\texttt{?select}' and then run that line.
\item
  Check the class of the object. Sometimes R is a little fussy and converting the class can help.
\item
  If your code just isn't running, then try searching for what you are trying to do, e.g.~`save PDF of graph in R made using ggplot'. Almost always there are relevant blog posts or Stack Overflow answers that will help.
\item
  Try to restart R and R Studio and load everything again.
\item
  Try to restart your computer.
\end{itemize}

There are a few small mistakes that I often make and may be worth checking in case you make them too:

\begin{itemize}
\tightlist
\item
  check the class e.g.~\texttt{class(my\_dataset\$its\_column)} to make sure that is what it should be;
\item
  when you're using ggplot make sure you use `\texttt{+}' not `\texttt{\%\textgreater{}\%}'; and
\item
  check whether you are using `.' when you shouldn't be, or vice versa.
\end{itemize}

It's almost always helpful to take a break and come back the next day.

Asking for help is a skill that you will get better at. Try not to say `this doesn't work', or `I tried everything' or `here's the error message, what do I do?'. In general, it's not possible to help based on that because there are too many possibilities. Instead:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Provide a small example of your data, and code, and detail what is going wrong.
\item
  Document what you have tried so far - what Stack Overflow pages have you looked at and why are they not quite what you're after? What RStudio Community pages have you tried?
\item
  Be clear about the outcome that you would like.
\end{enumerate}

As the RStudio Community welcome page \href{https://community.rstudio.com/t/welcome-to-the-rstudio-community/8}{says} `your job is to make it as easy as possible for others to help you'. To enable that to happen you need to `create a minimal reproducible example, or reprex for short' You can get more information about this \href{https://www.tidyverse.org/help/\#reprex}{here}, but basically what is needed is that you code is reproducible (so include things like \texttt{library()}, etc) and that is it minimal - that means making a very simple smaller example and reproducing the error on that small example.

Usually doing this actually allows you to solve your own problem. If it doesn't then it'll allow someone else a fighting chance as being able to help you. I especially recommend the \texttt{reprex} package \citep{citereprex}.

There's almost no chance that you've got a problem that someone hasn't addressed before, it's just a matter of finding the answer! Try to be tenacious with this and learn how to solve your own problems.

\hypertarget{mentality}{%
\subsection{Mentality}\label{mentality}}

\begin{quote}
(Y)ou are a real, valid, \emph{competent} user and programmer no matter what IDE you develop in or what tools you use to make your work work for you

(L)et's break down the gates, there's enough room for everyone

Sharla Gelfand, \href{https://twitter.com/sharlagelfand/status/1237365576701542400}{10 March 2020}.
\end{quote}

I'm a little hesitant to make suggestions with regard to mentality. If you write code, then you're coder regardless of how you do it, what you're using it for, or who you are. But I want to share a few traits that I have found have been useful to cultivate in myself. That said, entirely, whatever works for you is great, so take or leave this section.

\begin{itemize}
\tightlist
\item
  Focused: I've found that having an aim to `learn R' or something similar tends to be problematic, because there's no real end point to that. Instead I would recommend smaller, more specific goals, such as `make a histogram about the 2019 Canadian Election with ggplot'. That is something that you can focus on and achieve. With more nebulous goals it becomes easier to get lost on tangents, much more difficult to get help, and I've noticed that people who have nebulous goals seem to give up.
\item
  Curious: I've found that it's useful to just have a go. In general, the worst that happens is that you waste your time and have to give up. You can rarely break something irreparably with code. If you want to know what happens if you pass a `vector' instead of a `dataframe' to \texttt{ggplot} then just try it.
\item
  Pragmatic: At the same time, I've found that it's best to try to stick within the bounds of what I know and just make one small change each time. For instance if you're wanting to do some regression, and curious about the \texttt{tidymodels} package \citep{citeTidymodels} instead of \texttt{lm()}. Perhaps you could just use one aspect from the \texttt{tidymodels} package initially and then make another change next time. In my opinion ugly code that gets the job done, is better than beautiful code that is never finished.
\item
  Tenacious: This is a balancing act. I always find there are unexpected problems and issues with every project. On the one hand persevering despite these is a good tendency. But on the other hand I've learned that sometimes I need to be prepared to give up on something if it doesn't seem like a break-through is possible. This is where I have found that mentors can be useful as they tend to have a better idea. This is also where planning comes in.
\item
  Planned: I have found it is very useful to plan out what you are going to do. For instance, you may want to make a histogram of the 2019 Canadian Election. I find it useful to plan the steps that are needed and even to sketch out how I might implement each step. For instance, the first step is to get the data. What packages might be useful? Where might the data be? What is our back-up plan for if we can't find the data in that initial spot?
\item
  Done is better than perfect: We all have various perfectionist tendencies to a certain extent, but I recommend that you try to turn them off to a certain extent when it comes to R. In the first instance just try to write code that works, especially in the early days. You can always come back and improve aspects of it. But it is important to actually ship.
\end{itemize}

\hypertarget{code-comments}{%
\subsection{Code comments}\label{code-comments}}

Comment your code.

There is no one way to write code, especially in R. However, there are some general guidelines that will make it easier for you even if you're just working on your own.

Comment your code.

Comments in R can be added by including the \# symbol. The shortcut on Mac is `Command + Shift + m'. You don't have to put a comment at the start of the line, it can be midway through. In general, you don't need to comment what every aspect of your code is doing but you should comment parts that are not obvious. For instance, if you read in some value then you may like to comment where it is coming from.

Comment your code.

You should comment why you are doing something. What are you trying to achieve?

Comment your code.

You must comment to explain weird things. Like if you're removing some specific row, say row 27, then why are you removing that row? It may seem obvious in the moment, but future-you in six months won't remember.

Comment your code.

I like to break my code into sections. For instance, setting up my workspace, reading in datasets, manipulating and cleaning the dataset, analysing the datasets, and finally producing tables and figures. While it can be difficult to speak generally, I usually separate each of those certainly with comments explaining what is going on, and sometimes into separate files, depending on the length.

Comment your code.

Additionally, at the top of each file I put basic information, such as the purpose of the file, and pre-requisites or dependencies, the date, the author and contact information, and finally and red-flags, bodies, or todos.

Comment your code.

At the very least I recommend something like the following for every R script:

\begin{verbatim}
#### Preamble ####
# Purpose: Brief sentence about what this script does
# Author: Your name
# Data: The date it was written
# Contact: Add your email
# License: Think about how your code may be used
# Pre-requisites: 
# - Maybe you need some data or some other script to have been run?


#### Workspace setup ####
# Don't keep the install.packages line - just comment out if need be
# Load libraries
library(tidyverse)

# Read in the raw data. 
raw_data <- readr::read_csv("inputs/data/raw_data.csv")


#### Next section ####
...
\end{verbatim}

\hypertarget{learning-more}{%
\subsection{Learning more}\label{learning-more}}

One of the great aspects of R is that there is a friendly community of people who use it. There are a variety of ways that I learn about new tricks, functions, and packages including:

\begin{itemize}
\tightlist
\item
  \href{https://rladies.org/}{R Ladies} `is a world-wide organization to promote gender diversity in the R community'. Whether that is your cup of tea or not, they tend to share fantastic material on their twitter feeds: \url{https://twitter.com/rladiesglobal} and \url{https://twitter.com/WeAreRLadies} (where someone takes over the account for the week, so the enthusiasm never wanes). If you're in Toronto, then the local chapter is: \url{https://rladies.org/canada-rladies/locality/Toronto/} and \url{https://www.meetup.com/rladies-toronto/}.
\item
  \href{https://rweekly.org/}{R Weekly} is a weekly newsletter that highlights new things that have happened recently.
\item
  A popular R podcast is \href{http://nssdeviations.com/}{Not So Standard Deviations}.
\end{itemize}

Another great way to learn is by exchanging your code with others. Initially, just have them read it and give you feedback about it. But after you get a bit more confident run each other's code. The most efficiently I've ever improved in my R journey has been by having Monica try to run my code.

\hypertarget{developing-research-questions}{%
\section{Developing research questions}\label{developing-research-questions}}

Both qualitative and quantitative approaches have their place, but here we focus on quantitative approaches. (Qualitative research is important as well, and often the most interesting work has a little of both - `mixed methods'.) This means that we are subject to issues surrounding data quality, scales, measures, sources, etc. We are especially interested in trying to tease out causality.

Broadly there are two ways to go about research:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  data-first,
\item
  question-first.
\end{enumerate}

If you get a job somewhere typically you will initially be data-first. This means that you will need to work out the questions that you can reasonably answer with the data available to you. After you show some promise, you may be given the latitude to explore specific questions, possibly even gathering data specifically for that purpose. Contrast this with the example of the Behavioural Insights Team, \citep[p.~23]{gertler2016impact} who got to design and then carry out experiments given the remit of the entire British government (as they were spun out of the prime minister's office).

When deciding the questions that you can reasonably answer with the data that are available, you need to think about:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Theory: Do you have a reasonable expectation that there is something causal that could be determined? Charting the stock market - maybe, but might be better with haruspex because at least that way you have something you could eat. You need a reasonable theory of how \(x\) may be affecting \(y\).
\item
  Importance: There are plenty of trivial questions that you could ask, but it's important to not waste your time. The importance of a question also helps with motivation when you are on your fourth straight week of cleaning data and de-bugging your code. It also (and this becomes important) makes it easier to get talented people to work with you, or similarly to convince people to fund you or allow you to work on this project.
\item
  Availability: Can you reasonably expect get more data about this research question in the future or is this the extent of the data that could be gathered?
\item
  Iteration: Is this something that you can come back to and run often or is this a once-off analysis?
\end{enumerate}

The `FINER framework' as a mnemonic device used in medicine.\footnote{Thanks to Aaron Miller for pointing me to this.} This framework reminds us to ask questions that are \citep{hulley2007designing}:

\begin{quote}
\begin{itemize}
\tightlist
\item
  Feasible: Adequate number of subjects; adequate technical expertise; affordable in time and money; manageable in scope.
\item
  Interesting: Getting the answer intrigues investigator, peers and community.
\item
  Novel: Confirms, refutes or extends previous findings
\item
  Ethical: Amenable to a study that institutional review board will approve.
\item
  Relevant: To scientific knowledge; to clinical and health policy; to future research.
\end{itemize}
\end{quote}

\citet{farrugia2010research} build on this in terms of developing research questions and recommend `PICOT':

\begin{quote}
\begin{itemize}
\tightlist
\item
  Population: What specific population are you interested in?
\item
  Intervention: What is your investigational intervention?
\item
  Comparison group: What is the main alternative to compare with the intervention?
\item
  Outcome of interest: What do you intend to accomplish, measure, improve or affect?
\item
  Time: What is the appropriate follow-up time to assess outcome
\end{itemize}
\end{quote}

Often time will be constrained, possibly in interesting ways and these can guide your research. If we are interested in the effect of Trump's tweets on the stock market, then that can be done just by looking at the minutes (milliseconds?) after he tweets. But what if we are interested in the effect of a cancer drug on long term outcomes? If the effect takes 20 years then we either have to wait a while, or we need to look at people who were treated in 2000, but then we have selection effects and different circumstances to if we give the drug today. Often the only reasonable thing to do is to build a statistical model, but then we need adequate sample sizes, etc.

Usually the creation of a counterfactual is crucial. We'll discuss counterfactuals a lot more later, but briefly, a counterfactual is an if-then statement in which the `if' is false. Consider the example of Humpty Dumpty from Lewis Carroll's Through the Looking-Glass:

\begin{figure}
\includegraphics[width=0.9\linewidth]{figures/humpty} \caption{Humpty Dumpty example}\label{fig:humpty}
\end{figure}

Humpty is satisfied with what would happen if he were to fall off, even though he is similarly satisfied that this would never happen. (I won't ruin the story for you.) The comparison group often determines your results e.g.~the relationship between VO2 and athletic outcomes, compared with elite athletic outcomes.

Finally, we can often dodge ethics boards in data science, especially once you leave university. Typically, ethics guides from medicine and other fields are focused on ethics boards. But we often don't have those in data science applications. Even if your intentions are unimpeachable, I want to suggest one additional aspect to think about, and that is Bayes theorem:
\[P(A|B) = \frac{P(B|A)\times P(A)}{P(B)}\]
(The probability of A given B depends on the probability of B given A, the probability of A, and the probability of B.)

To see why this may be relevant, let's go to the canonical Bayes example: There is some test for a disease that is 99 per cent accurate both ways (that is, if a person actually has the disease there is a 99 per cent chance the test says positive, and is a person does not have the disease then there is a 99 per cent chance the test says negative). Let's just say that only 0.005 of the population has the disease. Then if we randomly pick someone from the general population then the chance that they have the disease is outstandingly low. This is even if they test positive:
\[\frac{0.99\times0.005}{0.99\times0.005 + 0.01\times0.995} \approx 33.2\]

To see why this may be relevant, consider the example of Google's AI cancer testing \citep{citeGoogleAIbreastcancer}. Basically what they have done is to train a model that can identify breast cancer. They claim `greater accuracy, fewer false positives, and fewer false negatives than experts'.

I, and many others \citep{citeWiredonGoogle}, would argue this is probably not where we would want these resources directed at this point. Even when perfectly healthy people go and get screened they tend to find various things that are `wrong' with them. The issue is that they're perfectly healthy and that we've rarely got a good idea as to whether that aspect that was flagged by the test is a big deal or not.

Given low prevalence in the community, we probably don't want wide-spread use of a particular testing regime that only looks at one aspect (i.e.~the mammogram in this case). Bayes rule guides us that the danger caused by the unnecessary `treatment' would probably outweigh the benefits. The authors of that Google blog post likely have unimpeachable ethics, but they may not understand Bayes rule.

\hypertarget{part-communicate}{%
\part{Communicate}\label{part-communicate}}

\hypertarget{static}{%
\chapter{Static}\label{static}}

\textbf{Required reading}

\begin{itemize}
\tightlist
\item
  Healy, Kieran, 2019, \emph{Data Visualization: A Practical Introduction}, Princeton University Press, Chapters 3 and 4, \url{https://socviz.co/}.
\item
  Wickham, Hadley, and Garrett Grolemund, 2017, \emph{R for Data Science}, Chapter 28, \url{https://r4ds.had.co.nz/}.
\item
  Hodgetts, Paul, 2020, `The ggfortify Package', 31 December, \url{https://www.hodgettsp.com/posts/r-ggfortify/}.
\item
  Zinsser, William, 1976 {[}2016{]}, \emph{On Writing Well}. (Any edition is fine. This book is first, despite alphabetical order because if you're serious about improving your writing then you should start with this book. It only takes a few hours to read. You'll go onto other books, but start with this one.)
\item
  Zinsser, William, 2009, `Writing English as a Second Language', Lecture, Columbia Graduate School of Journalism, 11 August, \url{https://theamericanscholar.org/writing-english-as-a-second-language/}. (I'm realistic enough to realise that requiring a book, even though I've said it's great and it's short, is a bit of a stretch. If you really don't want to commit to reading the Zinsser, then please at least read this `crib notes' version of it.)
\item
  Alexander, Monica, 2019, `The concentration and uniqueness of baby names in Australia and the US', \url{https://www.monicaalexander.com/posts/2019-20-01-babynames/}. (Look at how Monica explains concepts, especially the Gini coefficient, in a way that you can understand even if you've never heard of it before.)
\item
  Bronner, Laura, 2020, `Quant Editing', \url{http://www.laurabronner.com/quant-editing}. (Read these points and evaluate your own writing against them. It's fine to not comply with them if you have a good reason, but you need to know that you're not complying with them).
\item
  (The) Economist, 2013, `Johnson: Those six little rules', Prospero, 29 July 2013, available at: \url{https://www.economist.com/prospero/2013/07/29/johnson-those-six-little-rules}.
\item
  Girouard, Dave, 2020, `A Founder's Guide to Writing Well', First Round Review, 4 August, \url{https://firstround.com/review/a-founders-guide-to-writing-well/}.
\item
  Graham, Paul, 2020, `How to Write Usefully', \url{http://paulgraham.com/useful.html}. (Graham is good at writing for a programmer, but if you have a similar background then you may like this.)
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
  (The) Economist, 1991 {[}2014{]}, `The Economist Style Guide', Twelfth edition. (Any edition is fine. Pick a point or two each day and think about how it related to your own writing.)
\item
  Cochrane, John H., 2005, `Writing Tips for Ph. D. Students', \url{https://faculty.chicagobooth.edu/john.cochrane/research/papers/phd_paper_writing.pdf}. (This is aimed at academic research papers, but parts are still broadly relevant. And if you're going into academia then this is very relevant.)
\item
  Codrey, Laura, 2013, `Churchill's call for brevity', 17 October, \url{https://blog.nationalarchives.gov.uk/churchills-call-for-brevity/}.
\item
  Five Thirty Eight, 2020, Pick almost any article in their sports (\url{https://fivethirtyeight.com/sports/}) or politics (\url{https://fivethirtyeight.com/politics/}) sections. (The people at 538 write beautifully. Look at how their titles tell you exactly what is going on, or what they found. Look at how nicely their first paragraphs motivates you to read the rest of the article. Why am I \href{https://fivethirtyeight.com/features/byu-is-scorching-the-nets-from-even-farther-back/}{reading} about BYU basketball when I'm indifferent to both BYU and college basketball? Because that title and first paragraph hooked me.)
\item
  Graham, Paul, 2005, `Writing, Briefly', \url{http://paulgraham.com/writing44.html}.
\item
  Patrick, Cameron, 2019, `Plotting multiple variables at once using ggplot2 and tidyr', 26 November, \url{https://cameronpatrick.com/post/2019/11/plotting-multiple-variables-ggplot2-tidyr/}.
\item
  Patrick, Cameron, 2020, `Making beautiful bar charts with ggplot', 15 March, \url{https://cameronpatrick.com/post/2020/03/beautiful-bar-charts-ggplot/}.
\item
  Shapiro, Jesse M., `Four Steps to an Applied Micro Paper', \url{https://www.brown.edu/Research/Shapiro/pdfs/foursteps.pdf}. (This is mostly recommended for the part about `the robot' with regard to your data section.)
\item
  Shapiro, Julian, `Writing Well', \url{https://www.julian.com/guide/write/intro}.
\item
  Strunk, William Jr., 1959 {[}2009{]} `The Elements of Style'. (Any edition is fine. Eventually you'll move beyond this, but it's important to know the rules before you break them).
\item
  Vanderplas, Susan, Dianne Cook, and Heike Hofmann, 2020, `Testing Statistical Charts: What Makes a Good Graph?', \emph{Annual Review of Statistics and Its Application}, \url{https://www.annualreviews.org/doi/abs/10.1146/annurev-statistics-031219-041252}
\end{itemize}

\textbf{Examples of well-written papers}

\begin{itemize}
\tightlist
\item
  Barron, Alexander TJ, Jenny Huang, Rebecca L. Spang, and Simon DeDeo. ``Individuals, institutions, and innovation in the debates of the French Revolution.'' Proceedings of the National Academy of Sciences 115, no. 18 (2018): 4607-4612.
\item
  Chambliss, Daniel F. ``The Mundanity of Excellence: An Ethnographic Report on Stratification and Olympic Swimmers.'' Sociological Theory 7, no. 1 (1989): 70-86. \url{doi:10.2307/202063}.
\item
  Joyner, Michael J. ``Modeling: optimal marathon performance on the basis of physiological factors.'' Journal of Applied Physiology 70, no. 2 (1991): 683-687.
\item
  Kharecha, Pushker A., and James E. Hansen, 2013, `Prevented mortality and greenhouse gas emissions from historical and projected nuclear power', \emph{Environmental science \& technology}, 47, no. 9, pp.~4889-4895.
\item
  Samuel, Arthur L., 1959, `Some studies in machine learning using the game of checkers', \emph{IBM Journal of research and development}, 3, no. 3, pp.~210-229.
\item
  Wardrop, Robert L., 1995, `Simpson's paradox and the hot hand in basketball', \emph{The American Statistician}, 49, no. 1, 24-28.
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\tightlist
\item
  Show the reader your raw data, or as close as you can come to it.
\item
  Use either \texttt{geom\_point} or \texttt{geom\_bar} initially.
\item
  Writing efficiently and effectively is a requirement if you want your work to be convincing.
\item
  Don't waste your reader's time.
\item
  A good title says what the paper is about, a great title says what the paper found.
\item
  For a six-page paper, a good abstract is a three to five sentence paragraph. For a longer paper your abstract can be slightly longer.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{ggplot}
\item
  \texttt{patchwork}
\end{itemize}

\textbf{Key functions/etc}

\begin{itemize}
\tightlist
\item
  \texttt{ggplot::geom\_point()}
\item
  \texttt{ggplot::geom\_bar()}
\end{itemize}

\textbf{Quiz}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  I have a dataset that contains measurements of height (in cm) for a sample of 300 penguins, who are either the Adeline or Emperor species. I am interested in visualizing the distribution of heights by species in a graphical way. Please discuss whether a pie chart is an appropriate type of graph to use. What about a box and whisker plot? Finally, what are some considerations if you made a histogram? {[}Please write a paragraph or two for each aspect.{]}
\item
  Assume the dataset and columns exist. Would this code work? \texttt{data\ \%\textgreater{}\%\ ggplot(aes(x\ =\ col\_one))\ \%\textgreater{}\%\ geom\_point()} (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Yes
  \item
    No
  \end{enumerate}
\item
  If I have categorical data, which geom should I use to plot it (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{geom\_bar()}
  \item
    \texttt{geom\_point()}
  \item
    \texttt{geom\_abline()}
  \item
    \texttt{geom\_boxplot()}
  \end{enumerate}
\item
  Why are box plots often inappropriate (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    They hide the full distribution of the data.
  \item
    They are hard to make.
  \item
    They are ugly.
  \item
    The mode is clearly displayed.
  \end{enumerate}
\item
  Which of the following is the best title (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    ``Problem Set 1''
  \item
    ``Unemployment''
  \item
    ``Examining Canada's Unemployment (2010-2020)''
  \item
    ``Canada's Unemployment Increased between 2010 and 2020''
  \end{enumerate}
\end{enumerate}

\hypertarget{introduction-4}{%
\section{Introduction}\label{introduction-4}}

In order to convince someone of your story, your paper must be well-written, well-organized, and easy to follow. It should flow easily from one point to the next. It should have proper sentence structure, spelling, vocabulary, and grammar. Each point should be articulated clearly and completely without being overly verbose. Papers should demonstrate your understanding of the topics you are writing about and your confidence in discussing the terms, techniques and issues that are relevant. References must be included and properly cited because this enhances your credibility.

\begin{quote}
People who need to write: founders, VCs, lawyers, software engineers, designers, painters, data scientists, musicians, filmmakers, creative directors, physical trainers, teachers, writers.
Learn to write.

\href{https://twitter.com/shl/status/1224346643585089536}{Sahil Lavingia}.
\end{quote}

\begin{quote}
This is great advice. Writing well has done just as much for me as knowing how to code. I'd add that if you're intimidated by writing, start a blog and write often about something you're interested in. You'll get better. At least that's what I've done for the past 10 years. :)

\href{https://twitter.com/vboykis/status/1224351069771386881?s=21}{Vicki Boykis}.
\end{quote}

This chapter is about writing. By the end of it you will have a better idea of how to write short, detailed, quantitative papers that communicate exactly what you want them to and don't waste the time of your reader.

One critical part of telling stories with \emph{data}, is that it's ultimately the data that has to convince them. You're the medium, but the data are the message. To that end, the easiest way to try to convince someone of your story is to show them the data that allowed you to come to that story. Plot your raw data, or as close to it as possible.

While \texttt{ggplot} is a fantastic tool for doing this, there is a lot to that package and so it can be difficult to know where to start. My recommendation is that you start with either a scatter plot or a bar chart. What is critical is that you show the reader your raw data. These notes run through how to do that. It then discusses some more advanced options, but the important thing is that you show the reader your raw data (or as close to it as you can). Students seem to get confused what `raw' means; I'm using it to refer to as close to the original dataset as possible, so no sums, or averages, etc, if possible. Sometimes your data are too disperse for that or you've got other constraints, so there needs to be an element of manipulation. The main point is that you, at the very least, need to plot the data that you're going to be modelling. If you are dealing with larger datasets then just take a 10/1/0.1/etc per cent sample.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/jerry} \caption{Show me the data!}\label{fig:unnamed-chunk-85}
\end{figure}

Source: YouTube screenshot.

\hypertarget{graphs}{%
\section{Graphs}\label{graphs}}

Graphs are critical to tell a compelling story. And the most important thing with your graphs is to plot your raw data. Again: Plot. Your. Raw. Data.

Let's look at a somewhat fun example from the \texttt{datasauRus} package \citep{citedatasauRus}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(datasauRus)}

\CommentTok{# Code from: https://juliasilge.com/blog/datasaurus-multiclass/}
\NormalTok{datasaurus_dozen }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(dataset }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"dino"}\NormalTok{, }\StringTok{"star"}\NormalTok{, }\StringTok{"away"}\NormalTok{, }\StringTok{"bullseye"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(dataset) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\KeywordTok{across}\NormalTok{(}\KeywordTok{c}\NormalTok{(x, y), }\KeywordTok{list}\NormalTok{(}\DataTypeTok{mean =}\NormalTok{ mean, }\DataTypeTok{sd =}\NormalTok{ sd)),}
    \DataTypeTok{x_y_cor =} \KeywordTok{cor}\NormalTok{(x, y)}
\NormalTok{  ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4 x 6
##   dataset  x_mean  x_sd y_mean  y_sd x_y_cor
##   <chr>     <dbl> <dbl>  <dbl> <dbl>   <dbl>
## 1 away       54.3  16.8   47.8  26.9 -0.0641
## 2 bullseye   54.3  16.8   47.8  26.9 -0.0686
## 3 dino       54.3  16.8   47.8  26.9 -0.0645
## 4 star       54.3  16.8   47.8  26.9 -0.0630
\end{verbatim}

And despite these similarities at a summary statistic level, they're actually very different, well, beasts, when you plot the raw data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{datasaurus_dozen }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(dataset }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"dino"}\NormalTok{, }\StringTok{"star"}\NormalTok{, }\StringTok{"away"}\NormalTok{, }\StringTok{"bullseye"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x, }\DataTypeTok{y=}\NormalTok{y, }\DataTypeTok{colour=}\NormalTok{dataset)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\KeywordTok{vars}\NormalTok{(dataset), }\DataTypeTok{nrow =} \DecValTok{2}\NormalTok{, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{colour =} \StringTok{"Dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-87-1.pdf}

\hypertarget{bar-chart}{%
\subsection{Bar chart}\label{bar-chart}}

Bar charts are useful when you have one variable that you want to focus on. Hint: you almost always have one variable that you want to focus on. Hence, you should almost always include at least one (and likely many) bar charts. Bar charts go by a variety of names, depending on their specifics. I recommend the \href{https://github.com/rstudio/cheatsheets/raw/master/data-visualization-2.1.pdf}{R Studio Data Viz Cheat Sheet}.

To get started, let's simulate some data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{number_of_observation <-}\StringTok{ }\DecValTok{10000}

\NormalTok{example_data <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{person =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{number_of_observation),}
                       \DataTypeTok{smoker =} \KeywordTok{sample}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\StringTok{"Smoker"}\NormalTok{, }\StringTok{"Non-smoker"}\NormalTok{),}
                                       \DataTypeTok{size =}\NormalTok{ number_of_observation, }
                                       \DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{),}
                       \DataTypeTok{age_died =} \KeywordTok{runif}\NormalTok{(number_of_observation,}
                                        \DataTypeTok{min =} \DecValTok{0}\NormalTok{,}
                                        \DataTypeTok{max =} \DecValTok{100}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{round}\NormalTok{(}\DataTypeTok{digits =} \DecValTok{0}\NormalTok{),}
                       \DataTypeTok{height =} \KeywordTok{sample}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\DecValTok{50}\OperatorTok{:}\DecValTok{220}\NormalTok{), }
                                       \DataTypeTok{size =}\NormalTok{  number_of_observation, }
                                       \DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{),}
                       \DataTypeTok{num_children =} \KeywordTok{sample}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{5}\NormalTok{),}
                                             \DataTypeTok{size =}\NormalTok{ number_of_observation, }
                                             \DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{,}
                                             \DataTypeTok{prob =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.40}\NormalTok{, }\FloatTok{0.15}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.05}\NormalTok{))}
\NormalTok{                       )}
\end{Highlighting}
\end{Shaded}

First, let's have a look at the data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(example_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 5
##   person smoker     age_died height num_children
##    <int> <chr>         <dbl>  <int>        <int>
## 1      1 Smoker           55     80            3
## 2      2 Non-smoker       54     78            2
## 3      3 Non-smoker       84    109            1
## 4      4 Smoker           75    114            4
## 5      5 Smoker           32    135            1
## 6      6 Smoker           37    220            0
\end{verbatim}

Now let's plot the age distribution. Based on our simulated data, we're expecting a fairly uniform plot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ age_died)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-90-1.pdf}

Now let's make it look a little better. There are themes that are built into ggplot, or you can install other themes from other packages, or you can edit aspects yourself. I'd recommend starting with the \texttt{ggthemes} package for some fun ones, but I tend to just use classic or minimal. Remember that you must always refer to your graphs in your text (Figure \ref{fig:myfrstgraph}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ age_died)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Age died"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Number"}\NormalTok{,}
       \DataTypeTok{title =} \StringTok{"Number of people who died at each age"}\NormalTok{,}
       \DataTypeTok{caption =} \StringTok{"Source: Simulated data."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{telling_stories_with_data_files/figure-latex/myfrstgraph-1.pdf}
\caption{\label{fig:myfrstgraph}Number of people who died at each age}
\end{figure}

We may want to facet by some variable, in this case whether the person is a smoker (Figure \ref{fig:mysecndgraph}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ age_died)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\KeywordTok{vars}\NormalTok{(smoker)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Age died"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Number"}\NormalTok{,}
       \DataTypeTok{title =} \StringTok{"Number of people who died at each age, by whether they smoke"}\NormalTok{,}
       \DataTypeTok{caption =} \StringTok{"Source: Simulated data."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{telling_stories_with_data_files/figure-latex/mysecndgraph-1.pdf}
\caption{\label{fig:mysecndgraph}Number of people who died at each age, by whether they smoke}
\end{figure}

Alternatively, we may wish to colour by that instead (Figure \ref{fig:mysthidgraph}). I'll filter to just a handful of age-groups to keep it tractable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(age_died }\OperatorTok{<}\StringTok{ }\DecValTok{25}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ age_died, }\DataTypeTok{fill =}\NormalTok{ smoker)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Age died"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Number"}\NormalTok{,}
       \DataTypeTok{fill =} \StringTok{"Smoker"}\NormalTok{,}
       \DataTypeTok{title =} \StringTok{"Number of people who died at each age, by whether they smoke"}\NormalTok{,}
       \DataTypeTok{caption =} \StringTok{"Source: Simulated data."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{telling_stories_with_data_files/figure-latex/mysthidgraph-1.pdf}
\caption{\label{fig:mysthidgraph}Number of people who died at each age, by whether they smoke}
\end{figure}

It's important to recognise that a boxplot hides the full distribution of a variable. Unless you need to communicate the general distribution of many variables at once then you should not use them. The same box plot can apply to very different distributions.

\hypertarget{scatter-plot}{%
\subsection{Scatter plot}\label{scatter-plot}}

Often, we are also interested in the relationship between two series. We'll do that with a scatter plot. In this case, let's simulate some data, say years of education and income.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{number_of_observation <-}\StringTok{ }\DecValTok{500}

\NormalTok{scatter_data <-}\StringTok{ }
\StringTok{  }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{years_of_education =} \KeywordTok{runif}\NormalTok{(}\DataTypeTok{n =}\NormalTok{ number_of_observation, }\DataTypeTok{min =} \DecValTok{10}\NormalTok{, }\DataTypeTok{max =} \DecValTok{25}\NormalTok{),}
         \DataTypeTok{error =} \KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n=}\NormalTok{ number_of_observation, }\DataTypeTok{mean =} \DecValTok{0}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{10000}\NormalTok{),}
\NormalTok{         ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{income =}\NormalTok{ years_of_education }\OperatorTok{*}\StringTok{ }\DecValTok{5000} \OperatorTok{+}\StringTok{ }\NormalTok{error,}
         \DataTypeTok{income =} \KeywordTok{if_else}\NormalTok{(income }\OperatorTok{<}\StringTok{ }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, income))}

\KeywordTok{head}\NormalTok{(scatter_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 3
##   years_of_education   error income
##                <dbl>   <dbl>  <dbl>
## 1               15.4 -13782. 63180.
## 2               11.8   7977. 66985.
## 3               17.3  -9787. 76498.
## 4               14.7  12999. 86689.
## 5               10.6  -1500. 51302.
## 6               16.1   1911. 82202.
\end{verbatim}

Now let's look at income as a function of years of education (Figure \ref{fig:scattorplot}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scatter_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ years_of_education, }\DataTypeTok{y =}\NormalTok{ income)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Years of education"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Income"}\NormalTok{,}
       \DataTypeTok{title =} \StringTok{"Relationship between income and years of education"}\NormalTok{,}
       \DataTypeTok{caption =} \StringTok{"Source: Simulated data."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{telling_stories_with_data_files/figure-latex/scattorplot-1.pdf}
\caption{\label{fig:scattorplot}Relationship between income and years of education}
\end{figure}

\hypertarget{other}{%
\subsection{Other}\label{other}}

\hypertarget{best-fit}{%
\subsubsection{Best fit}\label{best-fit}}

If we're interested in quickly adding a line of best fit then, continuing with the earlier income example, we can do that with \texttt{geom\_smooth()} (Figure \ref{fig:scattorplottwo}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scatter_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ years_of_education, }\DataTypeTok{y =}\NormalTok{ income)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =}\NormalTok{ lm, }\DataTypeTok{color =} \StringTok{"black"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Years of education"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Income"}\NormalTok{,}
       \DataTypeTok{title =} \StringTok{"Relationship between income and years of education"}\NormalTok{,}
       \DataTypeTok{caption =} \StringTok{"Source: Simulated data."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\begin{figure}
\centering
\includegraphics{telling_stories_with_data_files/figure-latex/scattorplottwo-1.pdf}
\caption{\label{fig:scattorplottwo}Relationship between income and years of education}
\end{figure}

\hypertarget{histogram}{%
\subsubsection{Histogram}\label{histogram}}

If we want to get counts by groups then we may want to use a histogram. Figure \ref{fig:hisogramone} shows the counts for our simulated incomes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scatter_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ income)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Income"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Number"}\NormalTok{,}
       \DataTypeTok{title =} \StringTok{"Distribution of income"}\NormalTok{,}
       \DataTypeTok{caption =} \StringTok{"Source: Simulated data."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
\end{verbatim}

\begin{figure}
\centering
\includegraphics{telling_stories_with_data_files/figure-latex/hisogramone-1.pdf}
\caption{\label{fig:hisogramone}Distribution of income}
\end{figure}

\hypertarget{multiple-plots}{%
\subsubsection{Multiple plots}\label{multiple-plots}}

Finally, let's try putting them together. We're going to use the \texttt{patchwork} package \citep{citepatchwork} and the \texttt{penguins} package for data. Don't forget \texttt{install.packages("palmerpenguins")} as this is probably the first time you've used the package.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(patchwork)}
\KeywordTok{library}\NormalTok{(palmerpenguins)}

\NormalTok{p1 <-}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(palmerpenguins}\OperatorTok{::}\NormalTok{penguins) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(bill_length_mm, bill_depth_mm)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Bill length (mm)"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Bill depth (mm)"}\NormalTok{)}
\NormalTok{p2 <-}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(palmerpenguins}\OperatorTok{::}\NormalTok{penguins) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(species)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Species"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Number"}\NormalTok{)}

\NormalTok{p1 }\OperatorTok{+}\StringTok{ }\NormalTok{p2}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-92-1.pdf}

And we can make things fairly involved fairly quickly.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(p1 }\OperatorTok{|}\StringTok{ }\NormalTok{p2) }\OperatorTok{/}
\StringTok{  }\NormalTok{p2}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-93-1.pdf}

\hypertarget{tables}{%
\section{Tables}\label{tables}}

Tables are also critical to tell a compelling story. We may prefer a table to a graph when there are only a few features that we want to focus on. We'll use \texttt{knitr::kable()} alongside the `kableExtra' package and also the \texttt{gt} package.

Let's start with the kable package and the summary dinosaur data from earlier.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example_data <-}\StringTok{ }
\StringTok{  }\NormalTok{datasaurus_dozen }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(dataset }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"dino"}\NormalTok{, }\StringTok{"star"}\NormalTok{, }\StringTok{"away"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(dataset) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarize}\NormalTok{(}
    \DataTypeTok{Mean    =} \KeywordTok{mean}\NormalTok{(x),}
    \DataTypeTok{Std_dev =} \KeywordTok{sd}\NormalTok{(x),}
\NormalTok{    ) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `summarise()` ungrouping output (override with `.groups` argument)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r}
\hline
dataset & Mean & Std\_dev\\
\hline
away & 54.26610 & 16.76983\\
\hline
dino & 54.26327 & 16.76514\\
\hline
star & 54.26734 & 16.76896\\
\hline
\end{tabular}

Even the defaults are pretty good, but we can add a few tweaks to make the table better. The first is that this many significant digits is inappropriate, we may also like to add a caption, make the column names consistent, and change the alignment.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(}\DataTypeTok{digits =} \DecValTok{2}\NormalTok{, }
               \DataTypeTok{caption =} \StringTok{"My first table."}\NormalTok{, }
               \DataTypeTok{col.names =} \KeywordTok{c}\NormalTok{(}\StringTok{"Dataset"}\NormalTok{, }\StringTok{"Mean"}\NormalTok{, }\StringTok{"Standard deviation"}\NormalTok{),}
               \DataTypeTok{align =} \KeywordTok{c}\NormalTok{(}\StringTok{'l'}\NormalTok{, }\StringTok{'l'}\NormalTok{, }\StringTok{'l'}\NormalTok{)}
\NormalTok{               )}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-95}My first table.}
\centering
\begin{tabular}[t]{l|l|l}
\hline
Dataset & Mean & Standard deviation\\
\hline
away & 54.27 & 16.77\\
\hline
dino & 54.26 & 16.77\\
\hline
star & 54.27 & 16.77\\
\hline
\end{tabular}
\end{table}

The `'kableExtra' package builds extra functionality \citep{citekableextra}.

The \texttt{gt} package \citep{citegt} is a newer package that brings a lot of exciting features. However, being newer it sometimes has issues with PDF output.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(gt)}

\NormalTok{example_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{gt}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\captionsetup[table]{labelformat=empty,skip=1pt}
\begin{longtable}{lrr}
\toprule
dataset & Mean & Std\_dev \\ 
\midrule
away & 54.26610 & 16.76982 \\ 
dino & 54.26327 & 16.76514 \\ 
star & 54.26734 & 16.76896 \\ 
\bottomrule
\end{longtable}

We could add sub-titles easily.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{gt}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{tab_header}\NormalTok{(}
    \DataTypeTok{title =} \StringTok{"Summary stats can be misleading"}\NormalTok{,}
    \DataTypeTok{subtitle =} \StringTok{"With an example from a dinosaur!"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\captionsetup[table]{labelformat=empty,skip=1pt}
\begin{longtable}{lrr}
\caption*{
\large Summary stats can be misleading\\ 
\small With an example from a dinosaur!\\ 
} \\ 
\toprule
dataset & Mean & Std\_dev \\ 
\midrule
away & 54.26610 & 16.76982 \\ 
dino & 54.26327 & 16.76514 \\ 
star & 54.26734 & 16.76896 \\ 
\bottomrule
\end{longtable}

One common reason for needing a table is to report regression results. You should consider \texttt{gtsummary}, \texttt{stargazer}, and \texttt{modelsummary}. But at the moment, my favourite is \texttt{modelsummary} \citep{citemodelsummary}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(modelsummary)}

\NormalTok{mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x, datasaurus_dozen)}
\KeywordTok{modelsummary}\NormalTok{(mod)}
\end{Highlighting}
\end{Shaded}

\begin{table}[H]
\centering
\begin{tabular}[t]{lc}
\toprule
  & Model 1\\
\midrule
(Intercept) & 53.590\\
 & (2.119)\\
x & -0.106\\
 & (0.037)\\
\midrule
Num.Obs. & 1846\\
R2 & 0.004\\
R2 Adj. & 0.004\\
AIC & 17383.0\\
BIC & 17399.6\\
Log.Lik. & -8688.506\\
F & 8.072\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{title-abstract-and-introduction}{%
\section{Title, abstract, and introduction}\label{title-abstract-and-introduction}}

A title is the first opportunity that you have to tell the reader your story. Ideally you will tell the reader exactly what you found. An effective title is critical in order to get your work read when there are other competing priorities. A title doesn't have to be `cute' to be great.

\begin{itemize}
\tightlist
\item
  Good: `On the 2019 Canadian Federal Election'. (At least the reader knows what the paper is about.)
\item
  Better: `The Liberal Party performance in the 2019 Canadian Federal Election'. (At least the reader knows what the paper is about more specifically.)
\item
  Even better: `The Liberal Party did poorly in rural areas in the 2019 Canadian Federal Election'. (The reader knows what the paper is about.)
\end{itemize}

You should put your name and the date on the paper because this provides an important context to the paper.

For a six-page paper, a good abstract is a three to five sentence paragraph. For a longer paper your abstract can be slightly longer. The abstract should say: What you did, what you found, and why the reader should care. Each of these should just be a sentence or two, so keep it very high level.

You should then have an introduction that tells the reader everything they need to know. You are not writing a mystery story - tell the reader the most important points in the introduction. For a six-page paper, your introduction may be two or three paragraphs. Four would likely be too much, but it depends on the context.

Your introduction should set the scene and give the reader some background. For instance, you may like to start of a little broader, to provide some context to your paper. You should then describe how your paper fits into that context. Then give some high-level results - provide more detail than you provided in the abstract, but don't get into the weeds - and finally broadly discuss next steps or glaring weaknesses. With regard to that high-level result: you need to pick one. If you have a bunch of interesting findings, then good for you, but pick one and write your introduction around that. If it's compelling enough then the reader will end up reading all your other interesting findings in the discussion/results sections. Finally, you should highlight the remainder of the paper.

As an example:

\begin{quote}
The Canadian Liberal Party has always struggled in rural ridings. In the past 100 years they have never won more than 25 per cent of them. But even by those standards the 2019 Federal Election was a disappointment with the Liberal Party winning only 2 of the 40 rural ridings.

In this paper we look at why the performance of the Liberal Party in this most recent election was so poor. We construct a model in which whether the Liberal Party won the riding is explained by the number of farms in the riding, the average internet connectivity, and the median age. We find that as the median age of a riding increases, the likelihood that a riding was won by the Liberal Party decreases by 14 percentage points. Future work could expand the time horizon that is considered which would allow a more nuanced understanding of these effects.

The remainder of this paper is structured as follows: Section 2 discusses the data, Section 3 discusses the model, Section 4 presents the results, and finally Section 5 discusses our findings and some weaknesses.
\end{quote}

The recommended readings provide some lovely examples of titles, abstracts, and introductions. Please take the time to briefly read these papers.

\hypertarget{figures-tables-equations-and-technical-terms}{%
\section{Figures, tables, equations, and technical terms}\label{figures-tables-equations-and-technical-terms}}

Figure and tables are a critical aspect of convincing people of your story. In a graph you can show your data and then let people decide for themselves. And in a table you can more easily summarise your data.

Figures, tables, equations, etc, should be numbered and then referenced in the text e.g.~``Figure 1 shows\ldots{}'' and then have Figure 1.

You should make sure that all aspects of your graph are legible. Always label all of the axes. Your graphs should have titles, and the point that you want to communicate should be clear.

If you use a technical term, then it should be briefly explained in plain language for readers who might not be familiar with it. A great example of this is \href{https://www.monicaalexander.com/posts/2019-20-01-babynames/}{this post} by Monica Alexander where she explains the Gini coefficient:

\begin{quote}
To look at the concentration of baby names, let's calculate the Gini coefficient for each country, sex and year. The Gini coefficient measures dispersion or inequality among values of a frequency distribution. It can take any value between 0 and 1. In the case of income distributions, a Gini coefficient of 1 would mean one person has all the income. In this case, a Gini coefficient of 1 would mean that all babies have the same name. In contrast, a Gini coefficient of 0 would mean names are evenly distributed across all babies.
\end{quote}

\hypertarget{on-brevity}{%
\section{On brevity}\label{on-brevity}}

\begin{figure}
\includegraphics[width=0.9\linewidth]{figures/johnson} \caption{'No more than four pages, or he's never going to read it. Two pages is preferable.'}\label{fig:unnamed-chunk-99}
\end{figure}

Source: Shipman, Tim, 2020, "The prime minister's vanishing briefs', The Sunday Times, 23 February, available at: \url{https://www.thetimes.co.uk/article/the-prime-ministers-vanishing-briefs-67mt0bg95} via \href{https://twitter.com/sarahjnickson}{Sarah Nickson}.

\begin{quote}
Insisting on two page briefs is sensible - not `government by ADHD'. PM has to be across lots of issues - cannot and should not be across (most of) them in the same depth as secretaries of state. Danger lies in PM trying to take on too much and getting bogged down in detail.

This might irk officials who lack a sense of where their issue sits within the PM's list of priorities - or the writing skills to draft a succinct brief. But there'd be very few occasions when a brief to the PM warrants more than two pages.

This is not something peculiar to the current PM - other ministers have raised the same in interviews with @instituteforgov Oliver Letwin complained of `huge amount of terrible guff, at huge, colossal, humungous length coming from some departments'
\url{https://www.instituteforgovernment.org.uk/ministers-reflect/person/oliver-letwin/}

Letwin sent briefs back and asked they be re-drafted to one quarter of the length. `Somewhere along the line the Civil Service had got used to splurge of the meaningless kind' Similarly, Theresa Villiers talked about the civil service's `frustrating tendency to produce six pages of obscure and rather impenetrable text' and wishes she'd be firmer in sending documents back for re-drafting:
\url{https://www.instituteforgovernment.org.uk/ministers-reflect/person/theresa-villiers/}

Sarah Nickson, \href{https://twitter.com/sarahjnickson/status/1231518746398908421}{23 Feb 2020}.
\end{quote}

Brevity is important. Partly this because you are writing for the reader, not yourself, and your reader has other priorities. But it is also because as the writer it focuses you to consider what your most important points are, how you can best support them, and where your arguments are weakest.

If you don't think that examples from government are persuasive, then please consider \href{https://www.sec.gov/Archives/edgar/data/1018724/000119312518121161/d456916dex991.htm}{Amazon's 2017 Letter to Shareholders}, or other statements about Bezos and memo writing, for instance:

\begin{quote}
Well structured, narrative text is what we're after rather than just text\ldots{} The reason writing a 4 page memo is harder than ``writing'' a 20 page powerpoint is because the narrative structure of a good memo forces better thought and better understanding of what's more important than what, and how things are related.

Jeff Bezos, 9 June 2004.
\end{quote}

\hypertarget{other-1}{%
\section{Other}\label{other-1}}

Typos and other grammatical mistakes affect the credibility of your claims. If the reader can't trust you to use a spell-checker then why should they trust you to use logistic regression? Microsoft Word has a fantastic spell-checker that is much better than what is available for R Markdown: copy/paste your work into there, look for the red lines and fix them in your R Markdown. Then look for the green lines and think about if you need to fix them in your R Markdown. If you don't have Word then Google Docs is pretty good and so is Apple's Pages.

A few other general tips that I have stolen from various people including the Reserve Bank of Australia's style guide:

\begin{itemize}
\tightlist
\item
  Think about what you are writing. Aim to write everything as though it were on the front page of the newspaper, because one day it could be.
\item
  Be concise. Remove as many words as possible.
\item
  Be direct. Think about the structure of your story, and identify the key pieces of information and arrange them so that your paper flows logically from one to the next. You should use sub-headings if you need.
\item
  Be precise. For instance, the stock-market didn't improve or worsen, it rose or fell. Distinguish levels from rates of change.
\item
  Be clear.
\item
  Write simply.
\item
  Use short sentence where possible.
\item
  Avoid jargon.
\end{itemize}

You should break these rules when you need to. But the only way to know whether you need to break a rule is to know the rules in the first instance.

\hypertarget{maps}{%
\chapter{Maps}\label{maps}}

\begin{itemize}
\tightlist
\item
  TODO: Add something about geocoding.
\end{itemize}

\textbf{Required reading}

\begin{itemize}
\tightlist
\item
  Cooley, David, 2020, `mapdeck', freely available at: \url{https://symbolixau.github.io/mapdeck/index.html}.
\item
  Kolb, Jan-Philipp, 2019, `Using Web Services to Work with Geodata in R', \emph{The R Journal}, 11:2, pages 6-23, freely available at: \url{https://journal.r-project.org/archive/2019/RJ-2019-041/index.html}.
\item
  Gabrielle, 2019, `Visualising spatial data using sf and mapdeck - part one', 4 December, freely available at: \url{https://resources.symbolix.com.au/2019/12/04/mapdeck-1/}.
\item
  `Leaflet for R', freely available at: \url{https://rstudio.github.io/leaflet/}.
\end{itemize}

\textbf{Required viewing}

\begin{itemize}
\tightlist
\item
  Kuriwaki, Shiro, 2020, `Making maps in R with sf', 1 March, freely available at: \url{https://vimeo.com/394800836}.
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
  Engel, Claudia A, 2019, \emph{Using Spatial Data with R}, 11 February, Chapter 3 Making Maps in R, freely available at: \url{https://cengel.github.io/R-spatial/mapping.html}.
\item
  Lovelace, Robin, Jakub Nowosad, Jannes Muenchow, 2020, \emph{Geocomputation with R}, 29 March, Chapter 8, Making maps with R, freely available at: \url{https://geocompr.robinlovelace.net/adv-map.html}.
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\tightlist
\item
  Thinking of maps as a (often fiddly, but strangely enjoyable) variant of a usual ggplot.
\item
  Broadening the data that we make available via interactive maps, while still telling a clear story.
\item
  Becoming comfortable with (and excited about) creating both static and interactive maps.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{ggmap}
\item
  \texttt{leaflet}
\item
  \texttt{maps}
\item
  \texttt{mapdeck}
\end{itemize}

\textbf{Key functions/etc}

\begin{itemize}
\tightlist
\item
  \texttt{add\_arc()}
\item
  \texttt{addCircleMarkers()}
\item
  \texttt{addMarkers()}
\item
  \texttt{addTiles()}
\item
  \texttt{canada.cities}
\item
  \texttt{geom\_polygon()}
\item
  \texttt{get\_stamenmap()}
\item
  \texttt{ggmap()}
\item
  \texttt{leaflet()}
\item
  \texttt{map()}
\item
  \texttt{map\_data()}
\item
  \texttt{mapdeck()}
\item
  \texttt{mapdeck\_style()}
\end{itemize}

\hypertarget{introduction-5}{%
\section{Introduction}\label{introduction-5}}

In many ways maps can be thought of as a fancy graph, where the x-axis is latitude, the y-axis is longitude, and there is some outline or a background image. We are used to this type of set-up, for instance, in a ggplot setting that is quite familiar.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_polygon}\NormalTok{( }\CommentTok{# First draw an outline}
    \DataTypeTok{data =}\NormalTok{ some_data, }
    \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ latitude, }
        \DataTypeTok{y =}\NormalTok{ longitude,}
        \DataTypeTok{group =}\NormalTok{ group}
\NormalTok{        )) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{( }\CommentTok{# Then add points of interest}
    \DataTypeTok{data =}\NormalTok{ some_other_data, }
    \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ latitude, }
        \DataTypeTok{y =}\NormalTok{ longitude)}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

And while there are some small complications, for the most part it is as straight-forward as that. The first step is to get some data. And helpfully, there is some geographic data built into ggplot, and there is some other information built into a package called \texttt{maps}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(maps)}
\KeywordTok{library}\NormalTok{(tidyverse)}

\NormalTok{canada <-}\StringTok{ }\KeywordTok{map_data}\NormalTok{(}\DataTypeTok{database =} \StringTok{"world"}\NormalTok{, }\DataTypeTok{regions =} \StringTok{"canada"}\NormalTok{)}
\NormalTok{canadian_cities <-}\StringTok{ }\NormalTok{maps}\OperatorTok{::}\NormalTok{canada.cities}

\KeywordTok{head}\NormalTok{(canada)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        long      lat group order region    subregion
## 1 -59.78760 43.93960     1     1 Canada Sable Island
## 2 -59.92227 43.90391     1     2 Canada Sable Island
## 3 -60.03775 43.90664     1     3 Canada Sable Island
## 4 -60.11426 43.93911     1     4 Canada Sable Island
## 5 -60.11748 43.95337     1     5 Canada Sable Island
## 6 -59.93604 43.93960     1     6 Canada Sable Island
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(canadian_cities)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            name country.etc    pop   lat    long capital
## 1 Abbotsford BC          BC 157795 49.06 -122.30       0
## 2      Acton ON          ON   8308 43.63  -80.03       0
## 3 Acton Vale QC          QC   5153 45.63  -72.57       0
## 4    Airdrie AB          AB  25863 51.30 -114.02       0
## 5    Aklavik NT          NT    643 68.22 -135.00       0
## 6    Albanel QC          QC   1090 48.87  -72.42       0
\end{verbatim}

With that information in hand we can then create a map of Canada that shows the cities with a population over 1,000. (The \texttt{geom\_polygon()} function within \texttt{ggplot} draws shapes, by connecting points within groups. And the \texttt{coord\_map()} function adjusts for the fact that we are making something that is 2D map to represent something that is 3D.)

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_polygon}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ canada,}
               \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ long,}
                   \DataTypeTok{y =}\NormalTok{ lat,}
                   \DataTypeTok{group =}\NormalTok{ group),}
               \DataTypeTok{fill =} \StringTok{"white"}\NormalTok{, }
               \DataTypeTok{colour =} \StringTok{"grey"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_map}\NormalTok{(}\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{40}\NormalTok{, }\DecValTok{70}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ canadian_cities}\OperatorTok{$}\NormalTok{long, }
                 \DataTypeTok{y =}\NormalTok{ canadian_cities}\OperatorTok{$}\NormalTok{lat),}
             \DataTypeTok{alpha =} \FloatTok{0.3}\NormalTok{,}
             \DataTypeTok{color =} \StringTok{"black"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Longitude"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Latitude"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-103-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# If I'm being honest, this 'simple example' took me six hours to work out. Firstly }
\CommentTok{# to find Canada and then to find Canadian cities.}
\end{Highlighting}
\end{Shaded}

In this section we will go through two types of maps: static and interactive. Static maps will be useful for printed output, such as a PDF or Word report, or where there is something in particular that you want to illustrate. Interactive maps will be more useful in an online setting, where you want your users to be able to explore the data themselves. (Further, they are a great way to take advantage of having your own website.)

\hypertarget{static-maps}{%
\section{Static maps}\label{static-maps}}

As is often the case with R, there are many different ways to get started create static maps. We've already seen how they can be built using simply ggplot, but here we'll explore one package that has a bunch of functionality built in that will make things easier: \texttt{ggmap}.

There are two essential components to a map: 1) some border or background image (also known as a tile); and 2) something of interest within that border or on top of that tile. In \texttt{ggmap}, we will use an open source option for our tile, Stamen Maps (maps.stamen.com), and we will use plot points based on latitude and longitude.

\hypertarget{australian-polling-places}{%
\subsubsection{Australian polling places}\label{australian-polling-places}}

Like Canada, in Australia people go to specific locations, called booths, to vote. These booths have latitudes and longitudes and so we can plot these. One reason we may like to do this is to notice patterns over geographies.

To get started we need to get a tile. We are going to use \texttt{ggmap} to get a tile from Stamen Maps, which builds on OpenStreetMap (openstreetmap.org). The main argument to this function is to specify a bounding box. This requires two latitudes - one for the top of the box and one for the bottom of the box - and two longitudes - one for the left of the box and one for the right of the box. (It can be useful to use Google Maps, or an alternative, to find the values of these that you need.) The bounding box provides the coordinates of the edges that you are interested in. In this case I have provided it with coordinates such that it will be centered around Canberra, Australia (our equivalent of Ottawa - a small city that was created for the purposes of being the capital).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggmap)}

\NormalTok{bbox <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DataTypeTok{left =} \FloatTok{148.95}\NormalTok{, }\DataTypeTok{bottom =} \FloatTok{-35.5}\NormalTok{, }\DataTypeTok{right =} \FloatTok{149.3}\NormalTok{, }\DataTypeTok{top =} \FloatTok{-35.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Once you have defined the bounding box, then the function \texttt{get\_stamenmap()} will get the tiles in that area. The number of tiles that it needs to get depends on the zoom, and the type of tiles that it gets depends on the maptype. I've chosen the maptype that I like here - the black and white option - but the helpfile specifies a few others that you may like. At this point you can pass your maps to ggmap and it will plot the tile! It will be actively downloading these tiles, so you need an internet connection.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{canberra_stamen_map <-}\StringTok{ }\KeywordTok{get_stamenmap}\NormalTok{(bbox, }\DataTypeTok{zoom =} \DecValTok{11}\NormalTok{, }\DataTypeTok{maptype =} \StringTok{"toner-lite"}\NormalTok{)}

\KeywordTok{ggmap}\NormalTok{(canberra_stamen_map)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-105-1.pdf}

Once we have a map then we can use \texttt{ggmap()} to plot it. (That circle in the middle of the map is where the Australian Parliament House is\ldots{} yes, our parliament is surrounded by circular roads (we call them `roundabouts'), actually it's surrounded by two of them.)

Now we want to get some data that we will plot on top of our tiles. We will just plot the location of the polling places, based on which `division' (the Australian equivalent to `ridings' in Canada) it is. This is available here: \url{https://results.aec.gov.au/20499/Website/Downloads/HouseTppByPollingPlaceDownload-20499.csv}. (The Australian Electoral Commission (AEC) is the official government agency that is responsible for elections in Australia.)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Read in the booths data for each year}
\NormalTok{booths <-}\StringTok{ }\NormalTok{readr}\OperatorTok{::}\KeywordTok{read_csv}\NormalTok{(}\StringTok{"https://results.aec.gov.au/24310/Website/Downloads/GeneralPollingPlacesDownload-24310.csv"}\NormalTok{, }
                          \DataTypeTok{skip =} \DecValTok{1}\NormalTok{, }
                          \DataTypeTok{guess_max =} \DecValTok{10000}\NormalTok{)}

\KeywordTok{head}\NormalTok{(booths)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 15
##   State DivisionID DivisionNm PollingPlaceID PollingPlaceTyp~ PollingPlaceNm
##   <chr>      <dbl> <chr>               <dbl>            <dbl> <chr>         
## 1 ACT          318 Bean                93925                5 Belconnen BEA~
## 2 ACT          318 Bean                93927                5 BLV Bean PPVC 
## 3 ACT          318 Bean                11877                1 Bonython      
## 4 ACT          318 Bean                11452                1 Calwell       
## 5 ACT          318 Bean                 8761                1 Chapman       
## 6 ACT          318 Bean                 8763                1 Chisholm      
## # ... with 9 more variables: PremisesNm <chr>, PremisesAddress1 <chr>,
## #   PremisesAddress2 <chr>, PremisesAddress3 <chr>, PremisesSuburb <chr>,
## #   PremisesStateAb <chr>, PremisesPostCode <chr>, Latitude <dbl>,
## #   Longitude <dbl>
\end{verbatim}

This dataset is for the whole of Australia, but as we are just going to plot the area around Canberra we will filter to that and only to booths that are geographic (the AEC has various options for people who are in hospital, or not able to get to a booth, etc, and these are still `booths' in this dataset).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Reduce the booths data to only rows with that have latitude and longitude}
\NormalTok{booths_reduced <-}
\StringTok{  }\NormalTok{booths }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(State }\OperatorTok{==}\StringTok{ "ACT"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(PollingPlaceID, DivisionNm, Latitude, Longitude) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(Longitude)) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# Remove rows that don't have a geography}
\StringTok{  }\KeywordTok{filter}\NormalTok{(Longitude }\OperatorTok{<}\StringTok{ }\DecValTok{165}\NormalTok{) }\CommentTok{# Remove Norfolk Island}
\end{Highlighting}
\end{Shaded}

Now we can use \texttt{ggmap} in the same way as before to plot our underlying tiles, and then build on that using \texttt{geom\_point()} to add our points of interest.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggmap}\NormalTok{(canberra_stamen_map, }
      \DataTypeTok{extent =} \StringTok{"normal"}\NormalTok{, }
      \DataTypeTok{maprange =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ booths_reduced,}
             \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Longitude, }
                 \DataTypeTok{y =}\NormalTok{ Latitude, }
                 \DataTypeTok{colour =}\NormalTok{ DivisionNm),}
\NormalTok{             ) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{name =} \StringTok{"2019 Division"}\NormalTok{, }\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_map}\NormalTok{(}\DataTypeTok{projection=}\StringTok{"mercator"}\NormalTok{,}
            \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\KeywordTok{attr}\NormalTok{(map, }\StringTok{"bb"}\NormalTok{)}\OperatorTok{$}\NormalTok{ll.lon, }\KeywordTok{attr}\NormalTok{(map, }\StringTok{"bb"}\NormalTok{)}\OperatorTok{$}\NormalTok{ur.lon),}
            \DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\KeywordTok{attr}\NormalTok{(map, }\StringTok{"bb"}\NormalTok{)}\OperatorTok{$}\NormalTok{ll.lat, }\KeywordTok{attr}\NormalTok{(map, }\StringTok{"bb"}\NormalTok{)}\OperatorTok{$}\NormalTok{ur.lat)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Longitude"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Latitude"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{panel.grid.major =} \KeywordTok{element_blank}\NormalTok{(),}
        \DataTypeTok{panel.grid.minor =} \KeywordTok{element_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-108-1.pdf}

We may like to save the map so that we don't have to draw it every time, and we can do that in the same way as any other graph, using \texttt{ggsave()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggsave}\NormalTok{(}\StringTok{"outputs/figures/map.pdf"}\NormalTok{, }\DataTypeTok{width =} \DecValTok{20}\NormalTok{, }\DataTypeTok{height =} \DecValTok{10}\NormalTok{, }\DataTypeTok{units =} \StringTok{"cm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Finally, the reason that I used Stamen Maps and OpenStreetMap is because it is open source, however you can also use Google Maps if you want. This requires you to first register a credit card with Google, and specify a key, but with low usage should be free. The \texttt{get\_googlemap()} function with \texttt{ggmap}, brings some nice features that \texttt{get\_stamenmap()} does not have. For instance, you can enter a placename and it'll do it's best to find it rather than needing to specify a bounding box.

\hypertarget{toronto-bike-parking}{%
\subsubsection{Toronto bike parking}\label{toronto-bike-parking}}

Let's see another example of a static map, this time using Toronto data accessed via the \texttt{opendatatoronto} package. The dataset that we are going to plot is available here: \url{https://open.toronto.ca/dataset/street-furniture-bicycle-parking/}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# This code is based on code from: https://open.toronto.ca/dataset/street-furniture-bicycle-parking/.}
\KeywordTok{library}\NormalTok{(opendatatoronto)}
\CommentTok{# (The string identifies the package.)}
\NormalTok{resources <-}\StringTok{ }\KeywordTok{list_package_resources}\NormalTok{(}\StringTok{"71e6c206-96e1-48f1-8f6f-0e804687e3be"}\NormalTok{)}
\CommentTok{# In this case there is only one dataset within this resource so just need the first one    }
\NormalTok{raw_data <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(resources, }\KeywordTok{row_number}\NormalTok{()}\OperatorTok{==}\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{get_resource}\NormalTok{()}
\KeywordTok{write_csv}\NormalTok{(raw_data, }\StringTok{"inputs/data/bike_racks.csv"}\NormalTok{)}
\KeywordTok{head}\NormalTok{(raw_data)}
\end{Highlighting}
\end{Shaded}

Now that we've saved a copy of the data, we can use that one. First we need to clean it up a bit. There are some clear errors in the ADDRESSNUMBERTEXT field, but not too many, so we'll just ignore it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{raw_data <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"inputs/data/bike_racks.csv"}\NormalTok{)}
\CommentTok{# We'll just focus on the data that we want}
\NormalTok{bike_data <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{ward =}\NormalTok{ raw_data}\OperatorTok{$}\NormalTok{WARD,}
                    \DataTypeTok{id =}\NormalTok{ raw_data}\OperatorTok{$}\NormalTok{ID,}
                    \DataTypeTok{status =}\NormalTok{ raw_data}\OperatorTok{$}\NormalTok{STATUS,}
                    \DataTypeTok{street_address =} \KeywordTok{paste}\NormalTok{(raw_data}\OperatorTok{$}\NormalTok{ADDRESSNUMBERTEXT, raw_data}\OperatorTok{$}\NormalTok{ADDRESSSTREET),}
                    \DataTypeTok{latitude =}\NormalTok{ raw_data}\OperatorTok{$}\NormalTok{LATITUDE,}
                    \DataTypeTok{longitude =}\NormalTok{ raw_data}\OperatorTok{$}\NormalTok{LONGITUDE)}
\KeywordTok{rm}\NormalTok{(raw_data)}
\end{Highlighting}
\end{Shaded}

Some of the bike racks were temporary so remove them and also let's just look at the area around the university, which is Ward 11

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Only keep ones that still exist}
\NormalTok{bike_data <-}\StringTok{ }
\StringTok{  }\NormalTok{bike_data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(status }\OperatorTok{==}\StringTok{ "Existing"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{status)}

\NormalTok{bike_data <-}\StringTok{ }\NormalTok{bike_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(ward }\OperatorTok{==}\StringTok{ }\DecValTok{11}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{ward)}
\end{Highlighting}
\end{Shaded}

If you look at the dataset at this point then you'll notice that there is a row for every bike parking spot. But we don't really need to know that, because sometimes there are lots right next to each other. Instead we'd just like the one point (we'll take advantage of this in an interactive graph in a moment). So we want to create a count by address, and then just get one instance per address.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bike_data <-}\StringTok{ }
\StringTok{  }\NormalTok{bike_data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(street_address) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{number_of_spots =} \KeywordTok{n}\NormalTok{(),}
         \DataTypeTok{running_total =} \KeywordTok{row_number}\NormalTok{()}
\NormalTok{         ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ungroup}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(running_total }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{id, }\OperatorTok{-}\NormalTok{running_total)}

\KeywordTok{head}\NormalTok{(bike_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 4
##   street_address   latitude longitude number_of_spots
##   <chr>               <dbl>     <dbl>           <int>
## 1 8 Kensington Ave     43.7     -79.4               1
## 2 87 Avenue Rd         43.7     -79.4               4
## 3 162 Mc Caul St       43.7     -79.4               1
## 4 147 Baldwin St       43.7     -79.4               2
## 5 888 Yonge St         43.7     -79.4               1
## 6 180 Elizabeth St     43.7     -79.4              10
\end{verbatim}

Now we can grab our tile, and add our bike rack data onto it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bbox <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DataTypeTok{left =} \FloatTok{-79.420390}\NormalTok{, }\DataTypeTok{bottom =} \FloatTok{43.642658}\NormalTok{, }\DataTypeTok{right =} \FloatTok{-79.383354}\NormalTok{, }\DataTypeTok{top =} \FloatTok{43.672557}\NormalTok{)}

\NormalTok{toronto_stamen_map <-}\StringTok{ }\KeywordTok{get_stamenmap}\NormalTok{(bbox, }\DataTypeTok{zoom =} \DecValTok{14}\NormalTok{, }\DataTypeTok{maptype =} \StringTok{"toner-lite"}\NormalTok{)}

\KeywordTok{ggmap}\NormalTok{(toronto_stamen_map,  }\DataTypeTok{maprange =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ bike_data,}
             \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ longitude, }
                 \DataTypeTok{y =}\NormalTok{ latitude),}
             \DataTypeTok{alpha =} \FloatTok{0.3}
\NormalTok{             ) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Longitude"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Latitude"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_minimal}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-114-1.pdf}

\hypertarget{interactive-maps}{%
\section{Interactive maps}\label{interactive-maps}}

The nice thing about interactive maps is that you can let your users decide what they are interested in. Additionally, if there is a lot of information then you may like to leave it to your users as to selectively focus on what they are interested in. For instance, in the case of Canadian politics, some people will be interested in Toronto ridings, while others will be interested in Manitoba, etc. But it would be difficult to present a map that focuses on both of those, so an interactive map is a great option for allowing users to zoom in on what they want.

\hypertarget{leaflet}{%
\subsubsection{Leaflet}\label{leaflet}}

The \texttt{leaflet} package is originally a JavaScript library of the same name that has been brought over to R. It makes it easy to make interactive maps. The basics are fairly similar to the \texttt{ggmap} set-up, but of course after that, there are many, many, options.

Let's redo the bike map from earlier, and possibly the interaction will allow us to see what the issue is with the data.

In the same way as a graph in \texttt{ggplot} begins with the \texttt{ggplot()} function, a map in the \texttt{leaflet} package begins with a call to the \texttt{leaflet()} function. This allows you to specify data, and a bunch of other options such as width and height. After this, we add `layers', in the same way that we added them in \texttt{ggplot}. The first layer that we'll add is a tile with the function \texttt{addTiles()}. In this case, the default is from OpenStreeMap. After that we'll add markers that show the location of each bike parking spot with \texttt{addMarkers()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(leaflet)}

\KeywordTok{leaflet}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ bike_data) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{addTiles}\NormalTok{() }\OperatorTok{%>%}\StringTok{  }\CommentTok{# Add default OpenStreetMap map tiles}
\StringTok{  }\KeywordTok{addMarkers}\NormalTok{(}\DataTypeTok{lng =}\NormalTok{ bike_data}\OperatorTok{$}\NormalTok{longitude, }
             \DataTypeTok{lat =}\NormalTok{ bike_data}\OperatorTok{$}\NormalTok{latitude, }
             \DataTypeTok{popup =}\NormalTok{ bike_data}\OperatorTok{$}\NormalTok{street_address,}
             \DataTypeTok{label =} \OperatorTok{~}\KeywordTok{as.character}\NormalTok{(bike_data}\OperatorTok{$}\NormalTok{number_of_spots))}
\end{Highlighting}
\end{Shaded}

There are two options here that may not be familiar. The first is popup, and this is what happens when you click on the marker. In this example this is giving the address. The second is label, which is what happens when you hover over the marker. In this example it is given the number of spots.

\hypertarget{covid-19}{%
\subsubsection{COVID-19}\label{covid-19}}

Let's have another go, this time with Ontario data on COVID-19.

We can download the latest data from the Ontario Data Catalogue. This is a fast moving situation in which they are likely to make breaking changes to this dataset. To ensure these notes work, I will save and then use the dataset as at 4 April 2020, but you are able to get the up-to-date dataset using the link and the code.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ontario_covid <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"https://data.ontario.ca/datastore/dump/455fd63b-603d-4608-8216-7d8647f43350?bom=True"}\NormalTok{)}
\KeywordTok{write_csv}\NormalTok{(ontario_covid, }\StringTok{"inputs/data/ontario_covid_2020-04-04.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ontario_covid <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"inputs/data/ontario_covid_2020-04-04.csv"}\NormalTok{)}
\KeywordTok{head}\NormalTok{(ontario_covid)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 14
##   `_id` ROW_ID ACCURATE_EPISODE_D~ Age_Group CLIENT_GENDER CASE_ACQUISITIO~
##   <dbl>  <dbl> <dttm>              <chr>     <chr>         <chr>           
## 1     1      1 2020-03-07 00:00:00 40s       MALE          Neither         
## 2     2      2 2020-03-08 00:00:00 20s       MALE          Neither         
## 3     3      3 2020-03-10 00:00:00 40s       FEMALE        Neither         
## 4     4      4 2020-03-11 00:00:00 50s       FEMALE        Neither         
## 5     5      5 2020-03-12 00:00:00 30s       FEMALE        Neither         
## 6     6      6 2020-03-15 00:00:00 50s       MALE          Neither         
## # ... with 8 more variables: OUTCOME1 <chr>, Reporting_PHU <chr>,
## #   Reporting_PHU_Address <chr>, Reporting_PHU_City <chr>,
## #   Reporting_PHU_Postal_Code <chr>, Reporting_PHU_Website <chr>,
## #   Reporting_PHU_Latitude <dbl>, Reporting_PHU_Longitude <dbl>
\end{verbatim}

There is a lot of information here, but we'll just plot the number of cases, by the reporting area (health areas). So this isn't the location of the person, but the location of the responsible health unit. Because of this, we'll add a little bit of noise so that the marker for each person can be seen. We do this with \texttt{jitter()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ontario_covid <-}\StringTok{ }
\StringTok{  }\NormalTok{ontario_covid }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Reporting_PHU_Latitude =} \KeywordTok{jitter}\NormalTok{(Reporting_PHU_Latitude, }\DataTypeTok{amount =} \FloatTok{0.1}\NormalTok{),}
         \DataTypeTok{Reporting_PHU_Longitude =} \KeywordTok{jitter}\NormalTok{(Reporting_PHU_Longitude, }\DataTypeTok{amount =} \FloatTok{0.1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

We will introduce a different type of marker here, which is circles. This will allow us to use different colours for the outcomes of each case. There are three possible outcomes: the case is resolved, it is not resolved, or it was fatal.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(leaflet)}

\NormalTok{pal <-}\StringTok{ }\KeywordTok{colorFactor}\NormalTok{(}\StringTok{"Dark2"}\NormalTok{, }\DataTypeTok{domain =}\NormalTok{ ontario_covid}\OperatorTok{$}\NormalTok{OUTCOME1 }\OperatorTok{%>%}\StringTok{ }\KeywordTok{unique}\NormalTok{())}

\KeywordTok{leaflet}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{addTiles}\NormalTok{() }\OperatorTok{%>%}\StringTok{  }\CommentTok{# Add default OpenStreetMap map tiles}
\StringTok{  }\KeywordTok{addCircleMarkers}\NormalTok{(}
    \DataTypeTok{data =}\NormalTok{ ontario_covid,}
    \DataTypeTok{lng =}\NormalTok{ ontario_covid}\OperatorTok{$}\NormalTok{Reporting_PHU_Longitude, }
    \DataTypeTok{lat =}\NormalTok{ ontario_covid}\OperatorTok{$}\NormalTok{Reporting_PHU_Latitude, }
    \DataTypeTok{color =} \KeywordTok{pal}\NormalTok{(ontario_covid}\OperatorTok{$}\NormalTok{OUTCOME1),}
    \DataTypeTok{popup =} \KeywordTok{paste}\NormalTok{(}\StringTok{"<b>Age-group:</b>"}\NormalTok{, }\KeywordTok{as.character}\NormalTok{(ontario_covid}\OperatorTok{$}\NormalTok{Age_Group), }\StringTok{"<br>"}\NormalTok{,}
                  \StringTok{"<b>Gender:</b>"}\NormalTok{, }\KeywordTok{as.character}\NormalTok{(ontario_covid}\OperatorTok{$}\NormalTok{CLIENT_GENDER), }\StringTok{"<br>"}\NormalTok{,}
                  \StringTok{"<b>Acquisition:</b>"}\NormalTok{, }\KeywordTok{as.character}\NormalTok{(ontario_covid}\OperatorTok{$}\NormalTok{CASE_ACQUISITIONINFO), }\StringTok{"<br>"}\NormalTok{,}
                  \StringTok{"<b>Episode date:</b>"}\NormalTok{, }\KeywordTok{as.character}\NormalTok{(ontario_covid}\OperatorTok{$}\NormalTok{ACCURATE_EPISODE_DATE), }\StringTok{"<br>"}\NormalTok{)}
\NormalTok{    ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{addLegend}\NormalTok{(}\StringTok{"bottomright"}\NormalTok{, }
            \DataTypeTok{pal =}\NormalTok{ pal, }
            \DataTypeTok{values =}\NormalTok{ ontario_covid}\OperatorTok{$}\NormalTok{OUTCOME1 }\OperatorTok{%>%}\StringTok{ }\KeywordTok{unique}\NormalTok{(),}
    \DataTypeTok{title =} \StringTok{"Case outcome"}\NormalTok{,}
    \DataTypeTok{opacity =} \DecValTok{1}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{mapdeck}{%
\subsubsection{mapdeck}\label{mapdeck}}

\textbf{Thank you to \href{https://shaunratcliff.com/}{Shaun Ratcliff} for introducing me to mapdeck.}

The package \texttt{mapdeck} is an R package that is built on top of Mapbox (\url{https://www.mapbox.com}). It is based on WebGL, which means that your web browser does a lot of work for you. The nice thing is that because of this, it can do a bunch of things that leaflet struggles with, especially dealing with larger datasets. Mapbox is a full-featured application that many businesses that you may have heard of use: \url{https://www.mapbox.com/showcase}. To close out these notes on mapping, I want to briefly touch on \texttt{mapdeck}, as it is a newer, but very exciting, package.

To this point we have used stamen maps as our tile, but mapdeck uses mapbox - \url{https://www.mapbox.com/} - and so you need to register and get a token for this. (It's free.) Once you have that token you add it to R using:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(mapdeck)}
\KeywordTok{set_token}\NormalTok{(}\StringTok{"asdf"}\NormalTok{) }\CommentTok{# replace asdf with your token.}
\KeywordTok{mapdeck_tokens}\NormalTok{()}

\KeywordTok{set_token}\NormalTok{(test}\OperatorTok{$}\NormalTok{key)}
\end{Highlighting}
\end{Shaded}

(Don't add it into your script otherwise everyone will be able to take it and use it, especially once you add it to GitHub.)

Then we need some data. Here we're going to just use the example dataset, which is about flights.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Code taken from the example: https://github.com/SymbolixAU/mapdeck}
\KeywordTok{library}\NormalTok{(mapdeck)}

\NormalTok{url <-}\StringTok{ 'https://raw.githubusercontent.com/plotly/datasets/master/2011_february_aa_flight_paths.csv'}
\NormalTok{flights <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(url)}
\NormalTok{flights}\OperatorTok{$}\NormalTok{info <-}\StringTok{ }\KeywordTok{paste0}\NormalTok{(}\StringTok{"<b>"}\NormalTok{,flights}\OperatorTok{$}\NormalTok{airport1, }\StringTok{" - "}\NormalTok{, flights}\OperatorTok{$}\NormalTok{airport2, }\StringTok{"</b>"}\NormalTok{)}

\KeywordTok{head}\NormalTok{(flights)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   start_lat start_lon  end_lat    end_lon airline airport1 airport2 cnt
## 1  32.89595 -97.03720 35.04022 -106.60919      AA      DFW      ABQ 444
## 2  41.97960 -87.90446 30.19453  -97.66987      AA      ORD      AUS 166
## 3  32.89595 -97.03720 41.93887  -72.68323      AA      DFW      BDL 162
## 4  18.43942 -66.00183 41.93887  -72.68323      AA      SJU      BDL  56
## 5  32.89595 -97.03720 33.56294  -86.75355      AA      DFW      BHM 168
## 6  25.79325 -80.29056 36.12448  -86.67818      AA      MIA      BNA  56
##               info
## 1 <b>DFW - ABQ</b>
## 2 <b>ORD - AUS</b>
## 3 <b>DFW - BDL</b>
## 4 <b>SJU - BDL</b>
## 5 <b>DFW - BHM</b>
## 6 <b>MIA - BNA</b>
\end{verbatim}

Finally, we can call the map. Again, this is just the example in the package's website.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mapdeck}\NormalTok{(}\DataTypeTok{style =} \KeywordTok{mapdeck_style}\NormalTok{(}\StringTok{'dark'}\NormalTok{)}
\NormalTok{        ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{add_arc}\NormalTok{(}
    \DataTypeTok{data =}\NormalTok{ flights}
\NormalTok{    , }\DataTypeTok{origin =} \KeywordTok{c}\NormalTok{(}\StringTok{"start_lon"}\NormalTok{, }\StringTok{"start_lat"}\NormalTok{)}
\NormalTok{    , }\DataTypeTok{destination =} \KeywordTok{c}\NormalTok{(}\StringTok{"end_lon"}\NormalTok{, }\StringTok{"end_lat"}\NormalTok{)}
\NormalTok{    , }\DataTypeTok{stroke_from =} \StringTok{"airport1"}
\NormalTok{    , }\DataTypeTok{stroke_to =} \StringTok{"airport2"}
\NormalTok{    , }\DataTypeTok{tooltip =} \StringTok{"info"}
\NormalTok{    , }\DataTypeTok{layer_id =} \StringTok{'arclayer'}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

And this is pretty nice!

\hypertarget{making-a-website}{%
\chapter{Making a website}\label{making-a-website}}

\hypertarget{getting-started-with-blogdown}{%
\section{Getting started with Blogdown}\label{getting-started-with-blogdown}}

\textbf{Required reading}

\begin{itemize}
\tightlist
\item
  Hill, Alison, 2017, `Up \& Running with blogdown', 12 June, freely available at: \url{https://alison.rbind.io/post/2017-06-12-up-and-running-with-blogdown/}.
\item
  Salmon, Maëlle, 2020, `What to know before you adopt Hugo/blogdown', 29 February, freely available at: \url{https://masalmon.eu/2020/02/29/hugo-maintenance/}.
\item
  Xie, Yihui, Amber Thomas, and Alison Presmanes Hill, 2020, \emph{blogdown: Creating Websites with R Markdown}', as at 6 February, freely available at: \url{https://bookdown.org/yihui/blogdown/}.
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
  Hill, Alison, 2019, `A Spoonful of Hugo: Troubleshooting Your Build', 4 March, freely available at: \url{https://alison.rbind.io/post/2019-03-04-hugo-troubleshooting/}.
\item
  De Leon, Desirée, 2019, `Trying out Blogdown', 4 September, freely available at: \url{http://desiree.rbind.io/post/2019/trying-out-blogdown/}.
\item
  Navarro, Danielle, 2018, `Day 1: Getting started with blogdown', 27 April, freely available at: \url{https://djnavarro.net/post/starting-blogdown/}.
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\tightlist
\item
  Building a website using blogdown and hugo.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{blogdown}
\item
  \texttt{tidyverse}
\end{itemize}

\hypertarget{introduction-6}{%
\section{Introduction}\label{introduction-6}}

\emph{These notes were originally developed for a workshop at the ANU delivered in 2017, and published to my website. Thank you to Minhee Chae and Peter Gibbard for helpful comments.}

A website is a critical part of communication. If you are searching for a job then it acts as one place to bring everything that you can do together. If you are using R, then you might like a website that makes it easy to share your work. This is where \texttt{blogdown} helps.

\texttt{blogdown} is a package that allows you to make websites (not just blogs, notwithstanding its name) largely within R Studio. It builds on Hugo, which is a popular tool for making websites. \texttt{blogdown} lets you freely and quickly get a website up-and-running. It is easy to add content from time-to-time. It integrates with R Markdown which lets you easily share your work. And the separation of content and styling allows you to relatively quickly change your website's design.

That said, using \texttt{blogdown} is more work than Google sites or Squarespace. It requires a little more knowledge than using a basic Wordpress site. And if you want to customise many aspects of your website, or need everything to be `just so' then \texttt{blogdown} may not be for you. \texttt{blogdown} is still under active development and various aspects may break in future releases. That said, the investment of time required to set up a \texttt{blogdown} website is unlikely to be wasted. Even if \texttt{blogdown} were shuttered tomorrow most of the content could be repurposed for a regular Hugo website.

This post is a simplified version of the \texttt{blogdown} user-guide and the blog post by Alison Presmanes Hill. It sticks to the basics and doesn't require much decision-making. The purpose is to allow someone without much experience to use \texttt{blogdown} to get a website up-and-running. Head to those two resources once you've got a website working and want to dive a bit deeper.

\hypertarget{foundations}{%
\section{Foundations}\label{foundations}}

To use \texttt{blogdown} you need R and R Studio, but if you have made it this far in the course then you probably know that. We'll install the \texttt{blogdown} package then use GitHub to create a new folder where we'll create our website.

First install blogdown:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"blogdown"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now we want to create a folder in GitHub (because it will be easier to put your website onto the internet if you have a GitHub account). We have seen GitHub earlier in this notes, but if you didn't do this at that point, then please create a free GitHub account at \url{https://github.com/}.

Once you have an account, create a new repository by clicking on the plus and call it `my\_website' (or whatever you want).

\includegraphics[width=29.89in]{figures/images/blogdown_github_1}

Don't worry about including a readme or gitignore. Once you get to the `Quick setup' page, copy the website address.

\includegraphics[width=29.36in]{figures/images/blogdown_github_2}

At this point, we want to get that folder onto our own computer. So open RStudio, select \texttt{Files}, \texttt{New\ Project}, \texttt{Version\ Control}, \texttt{Git}, and paste the information for the repo. Go through the rest of it, saving the folder somewhere sensible, and clicking `Open in new session'. This will then create a new folder on your computer which will be a Git folder that is linked to the GitHub repo that you created.

We can now construct a frame for our website in that folder.

\hypertarget{build-the-frame}{%
\section{Build the frame}\label{build-the-frame}}

Open R Studio and install Hugo via the blogdown package with the following code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{blogdown}\OperatorTok{::}\KeywordTok{install_hugo}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

In R Studio create a new project in the folder that you just created `my\_website'. To do this click on: File -\textgreater{} New Project -\textgreater{} Existing Directory. Then navigate to the folder `my\_website'. This will open a new R Studio session. Creating a project just adds a .proj file in the folder that makes it easier to come back to your website later.

Using that new R Studio session create your website with the following code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{blogdown}\OperatorTok{::}\KeywordTok{new_site}\NormalTok{(}\DataTypeTok{theme =} \StringTok{"gcushen/hugo-academic"}\NormalTok{, }\DataTypeTok{theme_example =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This will:

\begin{itemize}
\tightlist
\item
  download files into your `my\_website' folder;
\item
  open a R Markdown file that you can close for now; and
\item
  begin serving the site in your R Studio viewer.
\end{itemize}

The console and viewer of your R Studio session should look like this:

\includegraphics[width=32.78in]{figures/images/blogdown_serve_site}

Now that we have a frame, we can add our own content.

\hypertarget{add-content}{%
\section{Add content}\label{add-content}}

At this point, the default website is being `served' locally. This means that changes you make will be reflected in the website that you see in your R Studio Viewer. To see the website in a web browser click the `show in new window' button on the top left of the Viewer. This is circled in the above image. That will open the website using the address that the R Studio also tells you.

\hypertarget{headshot}{%
\subsubsection{Headshot}\label{headshot}}

The first change to make is to update the headshot. In your folder, go to my\_website -\textgreater{} static -\textgreater{} img. Replace `portrait.jpg' with your own square headshot jpg. If you do this correctly then when you go back to your website the image will have updated.

\hypertarget{personal-details-contacts-and-main-menu}{%
\subsubsection{Personal details, contacts, and main menu}\label{personal-details-contacts-and-main-menu}}

To update the biography and other details in that first pane, go to File -\textgreater{} Open File in the R Studio menu and open config.toml which is in my\_website -\textgreater{} config.toml. This file will either open in a text editor or in R Studio -- it doesn't matter which. When you save the file the changes will be reflected in the website.

Search for `title' or go to line 2. It should say:

\begin{Shaded}
\begin{Highlighting}[]
\StringTok{'title = "Academic"'}
\end{Highlighting}
\end{Shaded}

Change that to:

\begin{Shaded}
\begin{Highlighting}[]
\StringTok{'title = "Your Name"'}
\end{Highlighting}
\end{Shaded}

Search for `{[}params{]}' or go to line 21. There you can update parameters such as name, role, and contact details. If you don't want a particular parameter to show up on your website then set it equal to "". (An example of this is on line 33.)

Once you've updated these parameters, search for `{[}{[}params.social{]}{]}' or go to line 126. There you can update your contact details, such as email, twitter, etc. Just delete or comment out the full four lines if you don't want a particular contact type displayed on your website. You can always add more later.

Finally, search for `{[}{[}menu.main{]}{]}' or go to line 152. There you can change the menu items that are displayed on the top right of your website. For instance if you don't want a blog then delete or comment out the four lines:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{[[menu.main]]}
\NormalTok{  name =}\StringTok{ "Posts"}
\NormalTok{  url =}\StringTok{ "#posts"}
\NormalTok{  weight =}\StringTok{ }\DecValTok{3}
\end{Highlighting}
\end{Shaded}

If you want to change the order of the items then change the `weight'. Ascending values from left to right.

\hypertarget{biography}{%
\subsubsection{Biography}\label{biography}}

In your folder, go to my\_website -\textgreater{} content -\textgreater{} home -\textgreater{} about.md. That should open in R Studio or your text editor. Any changes that you save should immediately show up in your website.

Search for `\# List your academic interests.' or go to line 12. There you can change your academic interests. If you don't want this to show up on your website then you can just delete or comment out lines 12-18.

Search for `\# List your qualifications (such as academic degrees).' or go to line 20. There you can change your academic qualification. If you don't want this to show up on your website then you can just delete or comment out these lines.

The `year' is a numeric field. If you'd prefer to include duration (e.g.~2013 -- 2017), then replace the `2012' with `\,``2013 -- 2017''\,' (the "" are important). Or similarly, if you are expecting a degree then you could replace the `year' with `\,``Expected month year''\,'.

Search for `\# Biography' or go to line 43. There you can add a brief biography.

\hypertarget{teaching}{%
\subsubsection{Teaching}\label{teaching}}

Most of the other files in my\_website -\textgreater{} content -\textgreater{} home just display content from elsewhere. This is because of the setup of the website. The exception is teaching.md. Open that and edit everything after line 15.

\hypertarget{publications}{%
\subsubsection{Publications}\label{publications}}

In your folder, go to my\_website -\textgreater{} content -\textgreater{} publication. There are two default publications added there. You can edit those and then copy them to add extra publications.

\hypertarget{posts}{%
\subsubsection{Posts}\label{posts}}

If you want a blog in your website then the content is saved in: my\_website -\textgreater{} content -\textgreater{} post. If you don't want a blog then just delete this folder and comment out the posts menu item from my\_website -\textgreater{} config.toml file so it doesn't show up in the menu.

Once your website is working, if you want a new blog post, then you can simply use the R Studio menu bar: Tools -\textgreater{} Addins -\textgreater{} New Post.

\hypertarget{etc}{%
\subsubsection{Etc}\label{etc}}

Go through the different parts and change it as you need.

\hypertarget{subsequent-editing}{%
\subsubsection{Subsequent editing}\label{subsequent-editing}}

To come back to editing your website once you've closed R Studio, go to the `my\_website' folder and then double-click on the Rproj file, `blogdown\_test.Rproj'. That will open a new instance of R Studio.

From there you can type `blogdown:::serve\_site()' into the console to serve your site and then continue editing, or you could use the R Studio menu bar:
Tools -\textgreater{} Addins -\textgreater{} Serve Site.

\hypertarget{making-your-website-public}{%
\section{Making your website public}\label{making-your-website-public}}

\hypertarget{commit}{%
\subsubsection{Commit}\label{commit}}

So far everything has happened on your own computer. The first step to making your website public is to commit these changes to GitHub. To do this open Terminal again and as before use cd and ls to navigate to `my\_website'.

Once there, type each of the following lines (adding your own description) and follow each by `return'

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{git add }\OperatorTok{-}\NormalTok{A}
\NormalTok{git commit }\OperatorTok{-}\NormalTok{m }\StringTok{"DESCRIBE THE CHANGE YOU ARE ADDING"}
\NormalTok{git push}
\end{Highlighting}
\end{Shaded}

(You may be asked for your GitHub password. Terminal is a bit tricky to type passwords into because you don't know how many characters you've typed, but have a go and follow it by `return'.)

\hypertarget{netlify}{%
\subsubsection{Netlify}\label{netlify}}

There are many ways to make your website public, but one way is to use Netlify. What we are going to do is to link GitHub and Netlify, so that when you make a change in GitHub then Netlify automatically updates. Once you have an account then click ``New site from Git'' and at that point you can link your GitHub account. Once it has been authorised, then you should select the repo that you want to make public. The publish director is `public'.

Once this is all specified then you can ``Deploy site''. Netlify gives you a default address, but you can change this. However it will still have .netlify.com. To get rid of this you need to purchase a domain, and then go through the custom domains setting in Netlify.

\hypertarget{shiny}{%
\chapter{Shiny}\label{shiny}}

\textbf{TODO: ADD CONTENT}

\hypertarget{part-hunting-and-gathering-data}{%
\part{Hunting and Gathering (Data)}\label{part-hunting-and-gathering-data}}

\hypertarget{gathering-data}{%
\chapter{Gathering data}\label{gathering-data}}

\textbf{Required reading}

\begin{itemize}
\tightlist
\item
  Cooksey, Brian, 2014, `An Introduction to APIs', Zapier, 22 April, \url{https://zapier.com/learn/apis/}.
\item
  Gelfand, Sharla, 2019, `Crying @ Sephora', 8 November, \url{https://sharla.party/post/crying-sephora/}.
\item
  Luscombe, Alex, 2020, `Getting your .pdfs into R', 5 August, \url{https://alexluscombe.ca/post/r-pdftools/}.
\item
  Luscombe, Alex, 2020, `Parsing your .pdfs in R', 10 August, \url{https://alexluscombe.ca/post/parsing-pdfs/}.
\item
  Silge, Julia and David Robinson, 2020, \emph{Text Mining with R}, Chapters 1, 3, and 6, \url{https://www.tidytextmining.com/}.
\item
  Wickham, Hadley, nd, `Getting started with httr', \url{https://cran.r-project.org/web/packages/httr/vignettes/quickstart.html}.
\item
  Wickham, Hadley, 2014, `rvest: easy web scraping with R', 24 November, \url{https://blog.rstudio.com/2014/11/24/rvest-easy-web-scraping-with-r/}.
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
  Alexander, Rohan, 2019, `Gathering and analysing text data', 3 January, \url{https://rohanalexander.com/posts/2019-01-03-gathering-and-analysing-text-data/}. (Example of web-scraping.)
\item
  Benoit, Kenneth, 2019, `Text as data: An overview', 17 July, \url{https://kenbenoit.net/pdfs/28\%20Benoit\%20Text\%20as\%20Data\%20draft\%202.pdf}.
\item
  Bolton, Liza, 2019, `A quick look at museums per capita', 26 March, \url{http://blog.dataembassy.co.nz/museums-per-capita/}. (Example of web-scraping.)
\item
  Bryan, Jennifer, and Jim Hester, 2020, `What They Forgot to Teach You About R', chapter 7, \url{https://rstats.wtf/index.html}.
\item
  Clavelle, Tyler, 2017, `Using R to extract data from web APIs', 5 June, \url{https://www.tylerclavelle.com/code/2017/randapis/}.
\item
  Dogucu, Mine, and Mine Çetinkaya-Runde, 2020,`Web Scraping in the Statistics and Data Science Curriculum: Challenges and Opportunities', 6 May. (Walks through some basics.)
\item
  Goldman, Shayna, 2019, `How Much Do NHL Players Really Make? Part 2: Taxes', \url{https://hockey-graphs.com/2019/01/08/how-much-do-nhl-players-really-make-part-2-taxes/}. (Example of web-scraping.)
\item
  Graham, Shawn, 2019, `Scraping with rvest', 7 November, \url{https://electricarchaeology.ca/2019/11/07/scraping-with-rvest/}. (Example of web-scraping.)
\item
  Henze, Martin, 2020, `Web Scraping with rvest + Astro Throwback', 23 January, \url{https://heads0rtai1s.github.io/2020/01/23/rvest-intro-astro/}. (Example of web-scraping.)
\item
  Hudon, Caitlin, 2017, `'Blue Christmas: A data-driven search for the most depressing Christmas song', 22 December, \url{https://caitlinhudon.com/2017/12/22/blue-christmas/}.
\item
  James, Gareth, Daniela Witten, Trevor Hastie and Robert Tibshirani, 2017, \emph{An Introduction to Statistical Learning with Applications in R}, Chapter 6, \url{http://faculty.marshall.usc.edu/gareth-james/ISL/}.
\item
  Luscombe, Alex, 2020, `A Gentle Introduction to Tesseract OCR', 3 June, \url{https://alexluscombe.ca/post/ocr-tutorial/}.
\item
  Marshall, James, `HTML Made Really Easy', \url{https://www.jmarshall.com/easy/html/}. (Primer on HTML.)
\item
  Marshall, James, `HTTP Made Really Easy', \url{https://www.jmarshall.com/easy/http/}.
\item
  Nakagawara, Ryo, 2020, `Intro to \{polite\} Web Scraping of Soccer Data with R!', 14 May, \url{https://ryo-n7.github.io/2020-05-14-webscrape-soccer-data-with-R/}. (Introduction to the \texttt{polite} package.)
\item
  Pavlik, Kaylin, 2020, `How do fiber types appear together in yarn blends?', \url{https://www.kaylinpavlik.com/ravelry-yarn-fibers/}.
\item
  Silge, Julia, 2017, `Scraping CRAN with rvest', 5 March, \url{https://juliasilge.com/blog/scraping-cran/}. (Example of web-scraping.)
\item
  Silge, Julia, 2018, `The game is afoot! Topic modeling of Sherlock Holmes stories', 25 January, \url{https://juliasilge.com/blog/sherlock-holmes-stm/}.
\item
  Silge, Julia, 2018, `Training, evaluating, and interpreting topic models', 8 September, \url{https://juliasilge.com/blog/evaluating-stm/}.
\item
  Smale, David, 2020, `Daniel Johnston', \url{https://davidsmale.netlify.com/portfolio/daniel-johnston/}.
\item
  Taddy, Matt, 2019, \emph{Business Data Science}, Chapter 8, pp.~231-259.
\item
  Wickham, Hadley, `Managing Secrets', \url{https://cran.r-project.org/web/packages/httr/vignettes/secrets.html}.
\end{itemize}

\textbf{Recommended viewing}

\begin{itemize}
\tightlist
\item
  D'Agostino McGowan, Lucy, 2020 `Harnessing the Power of the Web via R Clients for Web APIs', talk at ASA Joint Statistical Meeting 2018, \url{https://www.lucymcgowan.com/talk/asa_joint_statistical_meeting_2018/}.
\item
  Tatman, Rachel, 2018, `Character Encoding and You', 21 February, \url{https://youtu.be/2U9EHYqc59Y}.
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\tightlist
\item
  Use APIs where possible because the data provider has specified the data they would like to make available to you, and the conditions under which they are making it available.
\item
  Often R packages have been written to make it easier to use APIs.
\item
  Use R environments to manage your keys.
\item
  Using the verb GET (`a GET request') means providing a URL and the server will return something, using the verb POST (a POST request') means providing some data and the server will deal with that data.
\item
  Cleaning data
\item
  Graphing data to tell a story
\item
  Respectfully scraping data
\item
  Approaching extracting text from PDFs as a workflow.
\item
  Planning what is needed at the start.
\item
  Starting small and then iterating.
\item
  Putting in place checks.
\item
  Gathering text data.
\item
  Preparing text datasets.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{babynames}
\item
  \texttt{broom}
\item
  \texttt{dplyr}
\item
  \texttt{ggplot2}
\item
  \texttt{glmnet}
\item
  \texttt{gutenbergr}
\item
  \texttt{janitor}
\item
  \texttt{jsonlite}
\item
  \texttt{pdftools}
\item
  \texttt{purrr}
\item
  \texttt{rtweet}
\item
  \texttt{rvest}
\item
  \texttt{spotifyr}
\item
  \texttt{stringi}
\item
  \texttt{tidymodels}
\item
  \texttt{tidytext}
\item
  \texttt{tidyverse}
\item
  \texttt{usethis}
\end{itemize}

\textbf{Key functions/etc}

\begin{itemize}
\tightlist
\item
  \texttt{augment()}
\item
  \texttt{bind\_tf\_idf()}
\item
  \texttt{cast\_dtm()}
\item
  \texttt{edit\_r\_environ()}
\item
  \texttt{fromJSON()}
\item
  \texttt{geom\_segment()}
\item
  \texttt{get\_artist\_audio\_features()}
\item
  \texttt{get\_favorites()}
\item
  \texttt{get\_my\_top\_artists\_or\_tracks()}
\item
  \texttt{glance()}
\item
  \texttt{glmnet()}
\item
  \texttt{gutenberg\_download()}
\item
  \texttt{html\_nodes()}
\item
  \texttt{html\_text()}
\item
  \texttt{map\_dfr()}
\item
  \texttt{pdf\_text()}
\item
  \texttt{pmap\_dfr()}
\item
  \texttt{read\_csv()}
\item
  \texttt{read\_html()}
\item
  \texttt{safely()}
\item
  \texttt{search\_tweets()}
\item
  \texttt{SelectorGadget}
\item
  \texttt{separate()}
\item
  \texttt{separate\_rows()}
\item
  \texttt{str\_detect()}
\item
  \texttt{str\_replace()}
\item
  \texttt{str\_squish()}
\item
  \texttt{tidy()}
\item
  \texttt{tokens()}
\item
  \texttt{unnest\_tokens()}
\item
  \texttt{walk2()}
\end{itemize}

\textbf{Quiz}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In your own words, what is an API (write a paragraph or two)?
\item
  Find two APIs and discuss how you could use them to tell interesting stories (write a paragraph or two for each).
\item
  Find two APIs that have an R packages written around them. How could you use these to tell interesting stories? (Write a paragraph or two for each.)
\item
  What is the main argument to the GET method (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
  \item
  \item
  \item
  \end{enumerate}
\item
  Name three reasons why we should be respectful when getting scraping data from websites (write a paragraph or two).
\item
  What features of a website do we typically take advantage of when we parse the code (select all)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
  \item
  \item
  \item
  \end{enumerate}
\item
  What are three advantages and three disadvantages of scraping compared with using an API (write a paragraph or two)?
\item
  Which of these are functions from the \texttt{purrr} package (select all)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
  \item
  \item
  \item
  \end{enumerate}
\item
  What are three delimiters that could be useful when trying to bring order to the PDF that you read in as a character vector (write a paragraph or two)?
\item
  What do I need to put inside ``SOMETHING\_HERE'' if I want to match regular expressions for a full stop i.e.~``.'' (pick one)? (Hint: See the `strings' cheatsheet.)

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
  \item
  \item
  \item
  \end{enumerate}
\item
  Name three reasons for sketching out what you want before starting to try to extract data from a PDF (write a paragraph or two for each).
\item
  If you are interested in demographic data then what are three checks that you might like to do? What are three if you are interested in economic data such as GDP, interest rates, and exchange rates? (Write an explanation for each.)
\item
  What does the \texttt{purrr} package do (select all)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
  \item
  \item
  \item
  \end{enumerate}
\item
  Why should we use \texttt{safely()} when scraping data (select all)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
  \item
  \item
  \item
  \end{enumerate}
\end{enumerate}

\hypertarget{apis}{%
\section{APIs}\label{apis}}

\hypertarget{introduction-7}{%
\subsection{Introduction}\label{introduction-7}}

In everyday language, and for our purposes, an Application Programming Interface (API) is simply a situation in which someone has set up specific files on their computer such that you can follow their instructions to get them. For instance, when you use a gif on Slack, Slack asks Giphy's server for the appropriate gif, Giphy's server gives that gif to Slack and then Slack inserts it into your chat. The way in which Slack and Giphy interact is determined by Giphy's API. More strictly, an API is just an application that runs on a server that we access using the HTTP protocol.

In our case, we are going to focus on using APIs for gathering data. So I'll tailor the language that I use toward that, and so:

\begin{quote}
{[}a{]}n API is the tool that makes a website's data digestible for a computer. Through it, a computer can view and edit data, just like a person can by loading pages and submitting forms. \citep[Chapter 1]{zapierapis}
\end{quote}

For instance, you could go to \href{https://www.google.ca/maps}{Google Maps} and then scroll and click and drag to center the map on Canberra, Australia, or you could just paste this into your browser: \url{https://www.google.ca/maps/@-35.2812958,149.1248113,16z}. You just used the Google Maps API.\footnote{There are at least six great coffee shops shown just in this section of map including: Mocan \& Green Grout; The Cupping Room; Barrio Collective Coffee; Lonsdale Street Cafe; Two Before Ten; and Red Brick. There are also two coffee shops that I love but that most wouldn't classify as `great' including: The Street Theatre Cafe; and the CBE Cafe.}

The advantage of using an API is that the data provider specifies exactly the data that they are willing to provide, and the terms under which they will provide it. These terms may include things like rate limits (i.e.~how often you can ask for data), and what you can do with the data (e.g.~maybe you're not allowed to use it for commercial purposes, or to republish it, or whatever). Additionally, because the API is being provided specifically for you to use it, it is less likely to be subject to unexpected changes. Because of this it is ethically and legally clear that when an API is available you should try to use it.

In this chapter we introduce some APIs in R. In particular, we will first introduce some R packages that wrap around APIs and make it easier to use an API, and we will then deal directly with APIs.

\hypertarget{r-packages-that-wrap-around-apis}{%
\subsection{R packages that wrap around APIs}\label{r-packages-that-wrap-around-apis}}

There are a lot of R packages that wrap around APIs making it easier for you to use an API within `familiar surroundings'. Here, I'll run through some useful and/or fun ones.

\hypertarget{using-apis-directly}{%
\subsection{Using APIs directly}\label{using-apis-directly}}

In this section we introduce GET requests in which we use an API directly. We will use the \texttt{httr} package \citet{citehttr}. A GET request tries to obtain some specific data.

You make a GET request, using, \texttt{GET()}, which takes a URL as an argument.

\textbf{TBD}

\hypertarget{case-study---rtweet}{%
\section{Case study - rtweet}\label{case-study---rtweet}}

Twitter is a rich source of text and other data. The Twitter API is the way in which Twitter ask that you interact with Twitter in order to gather these data. The \texttt{rtweet} package \citep{rtweet-package} is built around this API and allows us to interact with it in ways that are similar to using any other R package. Initially all you need a regular Twitter account.

Get started by install the library if you need and then calling it.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install.packages('rtweet')}
\KeywordTok{library}\NormalTok{(rtweet)}
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

To get started we need to authorise rtweet. We start that process by calling a function from the package.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{get_favorites}\NormalTok{(}\DataTypeTok{user =} \StringTok{"RohanAlexander"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This will open a browser on your computer, and you will then have to log into your regular Twitter account as shown in Figure \ref{fig:rtweetlogin}.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/rtweet} \caption{rtweet authorisation page}\label{fig:rtweetlogin}
\end{figure}

Once that is done we can actually get my favourites and then save them.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rohans_favs <-}\StringTok{ }\KeywordTok{get_favorites}\NormalTok{(}\StringTok{"RohanAlexander"}\NormalTok{)}

\KeywordTok{saveRDS}\NormalTok{(rohans_favs, }\StringTok{"dont_push/rohans_favs.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

And then looking at the most recent favourite, we can see it was when Professor Bolton tweeted about one of the stellar students in ISSC.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rohans_favs }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\KeywordTok{desc}\NormalTok{(created_at)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{slice}\NormalTok{(}\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(screen_name, text)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 2
##   screen_name text                                                              
##   <chr>       <chr>                                                             
## 1 Liza_Bolton One of our awesome @UofTStatSci students! I 💜 learning about the ~
\end{verbatim}

Let's look at who is tweeting about R, using one of the common R hashtags: \#rstats. I've removed retweets so that we hopefully get some actual interesting projects.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rstats_tweets <-}\StringTok{ }\KeywordTok{search_tweets}\NormalTok{(}
  \DataTypeTok{q =} \StringTok{"#rstats"}\NormalTok{,}
  \DataTypeTok{include_rts =} \OtherTok{FALSE}
\NormalTok{)}

\KeywordTok{saveRDS}\NormalTok{(rstats_tweets, }\StringTok{"dont_push/rstats_tweets.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

And then have a look at them.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(rstats_tweets)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "user_id"                 "status_id"              
##  [3] "created_at"              "screen_name"            
##  [5] "text"                    "source"                 
##  [7] "display_text_width"      "reply_to_status_id"     
##  [9] "reply_to_user_id"        "reply_to_screen_name"   
## [11] "is_quote"                "is_retweet"             
## [13] "favorite_count"          "retweet_count"          
## [15] "quote_count"             "reply_count"            
## [17] "hashtags"                "symbols"                
## [19] "urls_url"                "urls_t.co"              
## [21] "urls_expanded_url"       "media_url"              
## [23] "media_t.co"              "media_expanded_url"     
## [25] "media_type"              "ext_media_url"          
## [27] "ext_media_t.co"          "ext_media_expanded_url" 
## [29] "ext_media_type"          "mentions_user_id"       
## [31] "mentions_screen_name"    "lang"                   
## [33] "quoted_status_id"        "quoted_text"            
## [35] "quoted_created_at"       "quoted_source"          
## [37] "quoted_favorite_count"   "quoted_retweet_count"   
## [39] "quoted_user_id"          "quoted_screen_name"     
## [41] "quoted_name"             "quoted_followers_count" 
## [43] "quoted_friends_count"    "quoted_statuses_count"  
## [45] "quoted_location"         "quoted_description"     
## [47] "quoted_verified"         "retweet_status_id"      
## [49] "retweet_text"            "retweet_created_at"     
## [51] "retweet_source"          "retweet_favorite_count" 
## [53] "retweet_retweet_count"   "retweet_user_id"        
## [55] "retweet_screen_name"     "retweet_name"           
## [57] "retweet_followers_count" "retweet_friends_count"  
## [59] "retweet_statuses_count"  "retweet_location"       
## [61] "retweet_description"     "retweet_verified"       
## [63] "place_url"               "place_name"             
## [65] "place_full_name"         "place_type"             
## [67] "country"                 "country_code"           
## [69] "geo_coords"              "coords_coords"          
## [71] "bbox_coords"             "status_url"             
## [73] "name"                    "location"               
## [75] "description"             "url"                    
## [77] "protected"               "followers_count"        
## [79] "friends_count"           "listed_count"           
## [81] "statuses_count"          "favourites_count"       
## [83] "account_created_at"      "verified"               
## [85] "profile_url"             "profile_expanded_url"   
## [87] "account_lang"            "profile_banner_url"     
## [89] "profile_background_url"  "profile_image_url"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rstats_tweets }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(screen_name, text) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 2
##   screen_name     text                                                          
##   <chr>           <chr>                                                         
## 1 CRANberriesFeed CRAN updates: gstat hdme opalr tinytest https://t.co/y5W2NTKS~
## 2 CRANberriesFeed New CRAN package FeatureImpCluster with initial version 0.1.2~
## 3 CRANberriesFeed CRAN updates: crfsuite https://t.co/y5W2NTKSXT #rstats        
## 4 CRANberriesFeed CRAN updates: DSI https://t.co/y5W2NTKSXT #rstats             
## 5 CRANberriesFeed New CRAN package arcos with initial version 1.1 https://t.co/~
## 6 CRANberriesFeed CRAN updates: COVID19 gaiah mosaic smoothSurv https://t.co/y5~
\end{verbatim}

There is a bunch of other things that you can do just using a regular user account, and if you're interested then you should try the examples in the \texttt{rtweet} package documentation: \url{https://rtweet.info/index.html}. But more is available once you register as a developer (\url{https://developer.twitter.com/en/apply-for-access}). The Twitter API document is surprisingly readable and you may enjoy some of it: \url{https://developer.twitter.com/en/docs}.

When I introduced APIs I said that the `data provider specifies exactly the data that they are willing to provide\ldots{}' and we have certainly been able to take advantage of what they provide But I continued `\ldots and the terms under which they will provide it' and here we haven't done our part. In particular, I took some tweets and saved them. If I had pushed these to GitHub then it's possible I may have accidently stored sensitive information if there happened to be some in the tweets. Or if I had taken enough tweets to start to do some reasonable statistical analysis then even if there wasn't sensitive information I may have violated the terms if I had pushed those saved tweets to GitHub. Finally, I linked a Twitter user name, in this case \texttt{@Liza\_Bolton} with Professor Bolton. I happened to ask her if this was okay, but if I hadn't done that then I would have been violating the Twitter terms of service.

If you use Twitter data, please take a moment to look at the terms: \url{https://developer.twitter.com/en/developer-terms/more-on-restricted-use-cases}.

\hypertarget{case-study---spotifyr}{%
\section{Case study - spotifyr}\label{case-study---spotifyr}}

For the next example I will introduce the \texttt{spotifyr} package \citep{spotifyr}. Again, this is a wrapper that has been developed around an API, in this case the Spotify API.

\url{https://www.rcharlie.com/spotifyr/}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# devtools::install_github('charlie86/spotifyr')}
\KeywordTok{library}\NormalTok{(spotifyr)}
\end{Highlighting}
\end{Shaded}

In order to use this account you need a Spotify Developer Account, which you can set-up here: \url{https://developer.spotify.com/dashboard/}. That'll have you log in with your Spotify details and then accept their terms (it's worth looking at some of these and I'll follow up on a few below) as in Figure \ref{fig:spotifyaccept}.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/spotify} \caption{rtweet authorisation page}\label{fig:spotifyaccept}
\end{figure}

What we need from here is a `Client ID' and you can just fill out some basic details. In our case we probably `don't know' what we're building, which means that Spotify requires us to use a non-commercial agreement, which is fine. In order to use the Spotify API we need a Client ID and a Client Secret.

These are things that you want to keep to yourself. There are a variety of ways of keeping this secret, (and my understanding is that a helpful package is on its way) but we'll keep them in our System Environment. In this way, when we push to GitHub they won't be included. To do this we need to be careful about the naming, because \texttt{spotifyr} will look in our environment for specifically named keys.

To do this we are going to use the \texttt{usethis} package \citet{citeusethis}. So if you don't have that then please install it. There is a file called `.Renviron' which we will open and add our secrets to. This file also controls things like your default library location and more information is available at \citet{renvironrstudio} and \citet{whattheyforgot}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{usethis}\OperatorTok{::}\KeywordTok{edit_r_environ}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

When you run that function it will open a file. There you can add your Spotify secrets.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SPOTIFY_CLIENT_ID =}\StringTok{ 'PUT_YOUR_CLIENT_ID_HERE'}
\NormalTok{SPOTIFY_CLIENT_SECRET =}\StringTok{ 'PUT_YOUR_SECRET_HERE'}
\end{Highlighting}
\end{Shaded}

Save your `.Renviron' file, and then restart R (Session -\textgreater{} Restart R). You can now draw on that variable when you need.

Some functions that require your secrets as arguments will now just work. For instance, we will get information about Radiohead using \texttt{get\_artist\_audio\_features()}. One of the arguments is \texttt{authorization}, but as that is set to default to look at the R Environment, we don't need to do anything further.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{radiohead <-}\StringTok{ }\KeywordTok{get_artist_audio_features}\NormalTok{(}\StringTok{'radiohead'}\NormalTok{)}
\KeywordTok{saveRDS}\NormalTok{(radiohead, }\StringTok{"inputs/radiohead.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{radiohead <-}\StringTok{ }\KeywordTok{readRDS}\NormalTok{(}\StringTok{"inputs/radiohead.rds"}\NormalTok{)}

\KeywordTok{names}\NormalTok{(radiohead)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "artist_name"                  "artist_id"                   
##  [3] "album_id"                     "album_type"                  
##  [5] "album_images"                 "album_release_date"          
##  [7] "album_release_year"           "album_release_date_precision"
##  [9] "danceability"                 "energy"                      
## [11] "key"                          "loudness"                    
## [13] "mode"                         "speechiness"                 
## [15] "acousticness"                 "instrumentalness"            
## [17] "liveness"                     "valence"                     
## [19] "tempo"                        "track_id"                    
## [21] "analysis_url"                 "time_signature"              
## [23] "artists"                      "available_markets"           
## [25] "disc_number"                  "duration_ms"                 
## [27] "explicit"                     "track_href"                  
## [29] "is_local"                     "track_name"                  
## [31] "track_preview_url"            "track_number"                
## [33] "type"                         "track_uri"                   
## [35] "external_urls.spotify"        "album_name"                  
## [37] "key_name"                     "mode_name"                   
## [39] "key_mode"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{radiohead }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(artist_name, track_name, album_name) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   artist_name                               track_name
## 1   Radiohead                      Airbag - Remastered
## 2   Radiohead            Paranoid Android - Remastered
## 3   Radiohead Subterranean Homesick Alien - Remastered
## 4   Radiohead     Exit Music (For a Film) - Remastered
## 5   Radiohead                    Let Down - Remastered
## 6   Radiohead                Karma Police - Remastered
##                      album_name
## 1 OK Computer OKNOTOK 1997 2017
## 2 OK Computer OKNOTOK 1997 2017
## 3 OK Computer OKNOTOK 1997 2017
## 4 OK Computer OKNOTOK 1997 2017
## 5 OK Computer OKNOTOK 1997 2017
## 6 OK Computer OKNOTOK 1997 2017
\end{verbatim}

Let's just make a quick graph looking at track length over time.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{radiohead }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ album_release_year, }\DataTypeTok{y =}\NormalTok{ duration_ms)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-144-1.pdf}

Just because we can, let's settle an argument. I've always said that Radiohead of quite depressing, but they're my wife's favourite band. So let's see how depressing they are. Spotify provides various information about each track, including `valence', which Spotify \href{https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/}{define} as `(a) measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g.~happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g.~sad, depressed, angry).' So higher values are happier. Let's compare someone who we know it likely to be happy - Taylor Swift - with Radiohead.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{swifty <-}\StringTok{ }\KeywordTok{get_artist_audio_features}\NormalTok{(}\StringTok{'taylor swift'}\NormalTok{)}
\KeywordTok{saveRDS}\NormalTok{(swifty, }\StringTok{"inputs/swifty.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{swifty <-}\StringTok{ }\KeywordTok{readRDS}\NormalTok{(}\StringTok{"inputs/swifty.rds"}\NormalTok{)}

\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{name =} \KeywordTok{c}\NormalTok{(swifty}\OperatorTok{$}\NormalTok{artist_name, radiohead}\OperatorTok{$}\NormalTok{artist_name),}
       \DataTypeTok{year =} \KeywordTok{c}\NormalTok{(swifty}\OperatorTok{$}\NormalTok{album_release_year, radiohead}\OperatorTok{$}\NormalTok{album_release_year),}
       \DataTypeTok{valence =} \KeywordTok{c}\NormalTok{(swifty}\OperatorTok{$}\NormalTok{valence, radiohead}\OperatorTok{$}\NormalTok{valence)}
\NormalTok{               ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ year, }\DataTypeTok{y =}\NormalTok{ valence, }\DataTypeTok{color =}\NormalTok{ name)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Year"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Valence"}\NormalTok{,}
       \DataTypeTok{color =} \StringTok{"Name"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-146-1.pdf}

Finally, for the sake of embarrassment, let's look at our most played artists.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{top_artists <-}\StringTok{ }\KeywordTok{get_my_top_artists_or_tracks}\NormalTok{(}\DataTypeTok{type =} \StringTok{'artists'}\NormalTok{, }\DataTypeTok{time_range =} \StringTok{'long_term'}\NormalTok{, }\DataTypeTok{limit =} \DecValTok{20}\NormalTok{)}

\KeywordTok{saveRDS}\NormalTok{(top_artists, }\StringTok{"inputs/top_artists.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{top_artists <-}\StringTok{ }\KeywordTok{readRDS}\NormalTok{(}\StringTok{"inputs/top_artists.rds"}\NormalTok{)}

\NormalTok{top_artists }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(name, popularity)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                   name popularity
## 1            Radiohead         81
## 2  Bombay Bicycle Club         66
## 3                Drake        100
## 4        Glass Animals         74
## 5                JAY-Z         85
## 6        Laura Marling         65
## 7       Sufjan Stevens         75
## 8      Vampire Weekend         73
## 9     Sturgill Simpson         65
## 10          Nick Drake         66
## 11        Dire Straits         78
## 12               Lorde         80
## 13         Marian Hill         65
## 14       José González         68
## 15       Stevie Wonder         79
## 16          Disclosure         82
## 17      Ben Folds Five         52
## 18       Ainslie Wills         40
## 19            Coldplay         89
## 20               alt-J         75
\end{verbatim}

So pretty much my wife and I like what everyone else likes, with the exception of Ainslie Wills, who is an Australian and I suspect we used to listen to her when we were homesick.

How amazing that we live in a world that all that information is available with very little effort or cost.

Again, there is a lot more at the package's website: \url{https://www.rcharlie.com/spotifyr/}. A very nice little application of the Spotify API using some statistical analysis is \citet{kaylinpavlik}.

\hypertarget{scraping}{%
\section{Scraping}\label{scraping}}

\hypertarget{introduction-8}{%
\subsection{Introduction}\label{introduction-8}}

Web-scraping is a way to get data from websites into R. Rather than going to a website ourselves through a browser, we write code that does it for us. This opens up a lot of data to us, but on the other hand, it is not typically data that is being made available for these purposes and so it is important to be respectful of it. While generally not illegal, the specifics with regard to the legality of web-scraping depends on jurisdictions and the specifics of what you're doing, and so it is also important to be mindful of this. And finally, web-scraping imposes a cost on the website host, and so it is important to reduce this to the extent that it's possible.

That all said, web-scraping is an invaluable source of data. But they are typically a datasets that can be created as a by-product of someone trying to achieve another aim. For instance, a retailer may have a website with their products and their prices. That has not been created deliberately as a source of data, but we can scrape it to create a dataset. As such, the following principals guide my web-scraping.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Avoid it. Try to use an API wherever possible.
\item
  Abide by their desires. Some websites have a file `robots.txt' that contains information about what they are comfortable with scrapers doing, for instance `\url{https://www.google.com/robots.txt}'. If they have one of these then you should read it and abide by it.
\item
  Reduce the impact.

  \begin{itemize}
  \tightlist
  \item
    Firstly, slow down your scraper, for instance, rather than having it visit the website every second, slow it down (using \texttt{sys.sleep()}). If you're only after a few hundred files then why not just have it visit once a minute, running in the background overnight?
  \item
    Secondly, consider the timing of when you run the scraper. For instance, if it's a retailer then why not set your script to run from 10pm through to the morning, when fewer customers are likely to need the site? If it's a government website and they have a big monthly release then why not avoid that day?
  \end{itemize}
\item
  Take only what you need. For instance, don't scrape the entire of Wikipedia if all you need is to know the names of the 10 largest cities in Canada. This reduces the impact on their website and allows you to more easily justify what you are doing.
\item
  Only scrape once. Save everything as you go so that you don't have to re-collect data. Similarly, once you have the data, you should keep that separate and not modify it. Of course, if you need data over time then you will need to go back, but this is different to needlessly re-scraping a page.
\item
  Don't republish the pages that you scraped. (This is in contrast to datasets that you create from it.)
\item
  Take ownership and ask permission if possible. At a minimum level your scripts should have your contact details in them. Depending on the circumstances, it may be worthwhile asking for permission before you scrape.
\end{enumerate}

\hypertarget{getting-started-3}{%
\subsection{Getting started}\label{getting-started-3}}

Webscraping is possible by taking advantage of the underlying structure of a webpage. We use patterns in the HTML/CSS to get the data that we want. To look at the underlying HTML/CSS you can either: 1) open a browser, right-click, and choose something like `Inspect'; or 2) save the website and then open it with a text editor rather than a browser.

HTML/CSS is a markup language comprised of matching tags. So if you want text to be bold then you would use something like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{<b}\OperatorTok{>}\NormalTok{My bold text</b}\OperatorTok{>}
\end{Highlighting}
\end{Shaded}

Similarly, if you want a list then you start and end the list as well as each item.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{<ul}\OperatorTok{>}
\NormalTok{  <li}\OperatorTok{>}\NormalTok{Learn webscraping</li}\OperatorTok{>}
\NormalTok{  <li}\OperatorTok{>}\NormalTok{Do data science</li}\OperatorTok{>}
\NormalTok{  <li}\OperatorTok{>}\NormalTok{Proft</li}\OperatorTok{>}
\NormalTok{</ul}\OperatorTok{>}
\end{Highlighting}
\end{Shaded}

When webscraping we will search for these tags.

To get started, this is some HTML/CSS from my website. Let's say that we want to grab my name from it. We can see that the name is in bold, so we want to probably focus on that feature and extract it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{website_extract <-}\StringTok{ "<p>Hi, I’m <b>Rohan</b> Alexander.</p>"}
\end{Highlighting}
\end{Shaded}

We will use the \texttt{rvest} package \citet{citervest}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install.packages("rvest")}
\KeywordTok{library}\NormalTok{(rvest)}

\NormalTok{rohans_data <-}\StringTok{ }\KeywordTok{read_html}\NormalTok{(website_extract)}

\NormalTok{rohans_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## {html_document}
## <html>
## [1] <body><p>Hi, I’m <b>Rohan</b> Alexander.</p></body>
\end{verbatim}

The language used by \texttt{rvest} to look for tags is `node', so we will focus on bold nodes. By default \texttt{html\_nodes()} returns the tags as well. So we can focus on the text that they contain, using \texttt{html\_text()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rohans_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{html_nodes}\NormalTok{(}\StringTok{"b"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## {xml_nodeset (1)}
## [1] <b>Rohan</b>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{first_name <-}\StringTok{ }
\StringTok{  }\NormalTok{rohans_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{html_nodes}\NormalTok{(}\StringTok{"b"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{html_text}\NormalTok{()}

\NormalTok{first_name}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Rohan"
\end{verbatim}

The result is that we learn my first name.

\hypertarget{case-study---rohans-books}{%
\section{Case study - Rohan's books}\label{case-study---rohans-books}}

\hypertarget{introduction-9}{%
\subsection{Introduction}\label{introduction-9}}

In this case study we are going to scrape a list of books that I own, clean it, and look at the distribution of the first letters of author surnames. It is slightly more complicated than the example above, but the underlying approach is the same - download the website, look for the nodes of interest, extract the information, clean it.

\hypertarget{gather}{%
\subsection{Gather}\label{gather}}

Again, the key library that we are using is the \texttt{rvest} library. This makes it easier to download a website, and to then navigate the html to find the aspects that we are interested in. You should create a new project in a new folder (File -\textgreater{} New Project). Within that new folder you should make three new folders: \texttt{inputs}, \texttt{outputs}, and \texttt{scripts.}

In the scripts folder you should write and save a script along these lines. This script loads the libraries that we need, then visits my website, and saves a local copy.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#### Contact details ####}
\CommentTok{# Title: Get data from rohanalexander.com}
\CommentTok{# Purpose: This script gets data from Rohan's website about the books that he }
\CommentTok{# owns. It calls his website and then saves the dataset to inputs.}
\CommentTok{# Author: Rohan Alexander}
\CommentTok{# Contact: rohan.alexander@utoronto.ca}
\CommentTok{# Last updated: 20 May 2020}


\CommentTok{#### Set up workspace ####}
\KeywordTok{library}\NormalTok{(rvest)}
\KeywordTok{library}\NormalTok{(tidyverse)}


\CommentTok{#### Get html ####}
\NormalTok{rohans_data <-}\StringTok{ }\KeywordTok{read_html}\NormalTok{(}\StringTok{"https://rohanalexander.com/bookshelf.html"}\NormalTok{)}
\CommentTok{# This takes a website as an input and will read it into R, in the same way that we }
\CommentTok{# can read a, say, CSV into R.}

\KeywordTok{write_html}\NormalTok{(rohans_data, }\StringTok{"inputs/my_website/raw_data.html"}\NormalTok{) }
\CommentTok{# Always save your raw dataset as soon as you get it so that you have a record }
\CommentTok{# of it. This is the equivalent of, say, write_csv() that we have used earlier.}
\end{Highlighting}
\end{Shaded}

\hypertarget{clean}{%
\subsection{Clean}\label{clean}}

Now we need to navigate the HTML to get the aspects that we want, and to then put them into some sensible structure. I always try to get the data into a tibble as early as possible. While it's possible to work with the nested data, I move to a tibble so that the usual verbs that I'm used to can be used.

In the scripts folder you should write and save a new R script along these lines. First, we need to add the top matter, read in the libraries and the data that we scraped.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#### Contact details ####}
\CommentTok{# Title: Clean data from rohanaledander.com}
\CommentTok{# Purpose: This script cleans data that was downloaded in 01-get_data.R.}
\CommentTok{# Author: Rohan Alexander}
\CommentTok{# Contact: rohan.alexander@utoronto.ca}
\CommentTok{# Pre-requisites: Need to have run 01_get_data.R and have saved the data.}
\CommentTok{# Last updated: 20 May 2020}


\CommentTok{#### Set up workspace ####}
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(rvest)}

\NormalTok{rohans_data <-}\StringTok{ }\KeywordTok{read_html}\NormalTok{(}\StringTok{"inputs/my_website/raw_data.html"}\NormalTok{)}

\NormalTok{rohans_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## {html_document}
## <html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
## [1] <head>\n<meta http-equiv="Content-Type" content="text/html; charset=UTF-8 ...
## [2] <body>\n\n<!--radix_placeholder_front_matter-->\n\n<script id="distill-fr ...
\end{verbatim}

Now we need to identify the data that we are interested in using html tags and convert it to a tibble. If you look at the website, then you should notice that we are likely trying to focus on list items (Figure \ref{fig:rohansbooks}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/rohansbooks} \caption{Some of Rohan's books}\label{fig:rohansbooks}
\end{figure}

Let's look at the source (Figure \ref{fig:rohanssourceone}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/sourcetop} \caption{Source code for top of the page}\label{fig:rohanssourceone}
\end{figure}

There's a lot of debris, but scrolling down we eventually get to a list (Figure \ref{fig:rohanssourcetwo}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/sourcelist} \caption{Source code for list}\label{fig:rohanssourcetwo}
\end{figure}

The tag for a list item is `li', so we modify the earlier code to focus on that and to get the text.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#### Clean data ####}
\CommentTok{# Identify the lines that have books on them based on the list html tag}
\NormalTok{text_data <-}\StringTok{ }\NormalTok{rohans_data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{html_nodes}\NormalTok{(}\StringTok{"li"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{html_text}\NormalTok{()}

\NormalTok{all_books <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{books =}\NormalTok{ text_data)}

\KeywordTok{head}\NormalTok{(all_books)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 1
##   books                                                                         
##   <chr>                                                                         
## 1 "-“A Little Life”, Hanya Yanighara. Recommended by Lauren."                   
## 2 "“The Andromeda Strain”, Michael Crichton."                                   
## 3 "“Is There Life After Housework”, Don Aslett.\nGot given this at the Museum o~
## 4 "“The Chosen”, Chaim Potok."                                                  
## 5 "“The Forsyth Saga”, John Galsworthy."                                        
## 6 "“Freakonomics”, Steven Levitt and Stephen Dubner."
\end{verbatim}

We now need to clean the data. First we want to separate the title and the author

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# All content is just one string, so need to separate title and author}
\NormalTok{all_books <-}
\StringTok{  }\NormalTok{all_books }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{separate}\NormalTok{(books, }\DataTypeTok{into =} \KeywordTok{c}\NormalTok{(}\StringTok{"title"}\NormalTok{, }\StringTok{"author"}\NormalTok{), }\DataTypeTok{sep =} \StringTok{"”"}\NormalTok{)}

\CommentTok{# Remove leading comma and clean up the titles a little}
\NormalTok{all_books <-}
\StringTok{  }\NormalTok{all_books }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{author =} \KeywordTok{str_remove_all}\NormalTok{(author, }\StringTok{"^, "}\NormalTok{),}
         \DataTypeTok{author =} \KeywordTok{str_squish}\NormalTok{(author),}
         \DataTypeTok{title =} \KeywordTok{str_remove}\NormalTok{(title, }\StringTok{"“"}\NormalTok{),}
         \DataTypeTok{title =} \KeywordTok{str_remove}\NormalTok{(title, }\StringTok{"^-"}\NormalTok{)}
\NormalTok{         )}

\KeywordTok{head}\NormalTok{(all_books)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 2
##   title                  author                                                 
##   <chr>                  <chr>                                                  
## 1 A Little Life          Hanya Yanighara. Recommended by Lauren.                
## 2 The Andromeda Strain   Michael Crichton.                                      
## 3 Is There Life After H~ Don Aslett. Got given this at the Museum of Clean in P~
## 4 The Chosen             Chaim Potok.                                           
## 5 The Forsyth Saga       John Galsworthy.                                       
## 6 Freakonomics           Steven Levitt and Stephen Dubner.
\end{verbatim}

Finally, some specific cleaning is needed.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Some authors have comments after their name, so need to get rid of them, although there are some exceptions that will not work}
\CommentTok{# J. K. Rowling.}
\CommentTok{# M. Mitchell Waldrop.}
\CommentTok{# David A. Price}
\NormalTok{all_books <-}
\StringTok{  }\NormalTok{all_books }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{author =} \KeywordTok{str_replace_all}\NormalTok{(author,}
                              \KeywordTok{c}\NormalTok{(}\StringTok{"J. K. Rowling."}\NormalTok{ =}\StringTok{ "J K Rowling."}\NormalTok{,}
                                \StringTok{"M. Mitchell Waldrop."}\NormalTok{ =}\StringTok{ "M Mitchell Waldrop."}\NormalTok{,}
                                \StringTok{"David A. Price"}\NormalTok{ =}\StringTok{ "David A Price"}\NormalTok{)}
\NormalTok{                              )}
\NormalTok{         ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{separate}\NormalTok{(author, }\DataTypeTok{into =} \KeywordTok{c}\NormalTok{(}\StringTok{"author_correct"}\NormalTok{, }\StringTok{"throw_away"}\NormalTok{), }\DataTypeTok{sep =} \StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{."}\NormalTok{, }\DataTypeTok{extra =} \StringTok{"drop"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{throw_away) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{rename}\NormalTok{(}\DataTypeTok{author =}\NormalTok{ author_correct)}

\CommentTok{# Some books have multiple authors, so need to separate them}
\CommentTok{# One has multiple authors:}
\CommentTok{# "Daniela Witten, Gareth James, Robert Tibshirani, and Trevor Hastie"}
\NormalTok{all_books <-}
\StringTok{  }\NormalTok{all_books }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{author =} \KeywordTok{str_replace}\NormalTok{(author,}
                              \StringTok{"Daniela Witten, Gareth James, Robert Tibshirani, and Trevor Hastie"}\NormalTok{,}
                              \StringTok{"Daniela Witten and Gareth James and Robert Tibshirani and Trevor Hastie"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{separate}\NormalTok{(author, }\DataTypeTok{into =} \KeywordTok{c}\NormalTok{(}\StringTok{"author_first"}\NormalTok{, }\StringTok{"author_second"}\NormalTok{, }\StringTok{"author_third"}\NormalTok{, }\StringTok{"author_fourth"}\NormalTok{), }\DataTypeTok{sep =} \StringTok{" and "}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"right"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{pivot_longer}\NormalTok{(}\DataTypeTok{cols =} \KeywordTok{starts_with}\NormalTok{(}\StringTok{"author_"}\NormalTok{),}
               \DataTypeTok{names_to =} \StringTok{"author_position"}\NormalTok{,}
               \DataTypeTok{values_to =} \StringTok{"author"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{author_position) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(author))}

\KeywordTok{head}\NormalTok{(all_books)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 2
##   title                         author          
##   <chr>                         <chr>           
## 1 A Little Life                 Hanya Yanighara 
## 2 The Andromeda Strain          Michael Crichton
## 3 Is There Life After Housework Don Aslett      
## 4 The Chosen                    Chaim Potok     
## 5 The Forsyth Saga              John Galsworthy 
## 6 Freakonomics                  Steven Levitt
\end{verbatim}

It looks there is some at the end because I have a best of. I'll just get rid of those manually because it's not the focus.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all_books <-}\StringTok{ }
\StringTok{  }\NormalTok{all_books }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{slice}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{118}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{explore}{%
\subsection{Explore}\label{explore}}

Finally, just because we have the data now, so we may as well try to do something with it, let's look at the distribution of the first letter of the author names.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all_books }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{first_letter =} \KeywordTok{str_sub}\NormalTok{(author, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{    ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(first_letter) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 21 x 2
## # Groups:   first_letter [21]
##    first_letter     n
##    <chr>        <int>
##  1 ""               1
##  2 "A"              8
##  3 "B"              5
##  4 "C"              4
##  5 "D"             10
##  6 "E"              3
##  7 "F"              1
##  8 "G"             10
##  9 "H"              6
## 10 "I"              1
## # ... with 11 more rows
\end{verbatim}

\hypertarget{case-study---canadian-prime-ministers}{%
\section{Case study - Canadian Prime Ministers}\label{case-study---canadian-prime-ministers}}

\hypertarget{introduction-10}{%
\subsection{Introduction}\label{introduction-10}}

In this case study we are interested in how long Canadian prime ministers lived, based on the year that they were born. We will scrape data from Wikipedia, clean it, and then make a graph.

The key library that we will use for scraping is \texttt{rvest}. This adds a lot of functions that will make life easier. That said, every time you scrape a website things will change. Each scrape will largely be bespoke, even if you can borrow some code from earlier projects that you have completed. It is completely normal to feel frustrated at times. It helps to begin with an end in mind.

To that end, let's generate some simulated data. Ideally, we want a table that has a row for each prime minister, a column for their name, and a column each for the birth and death years. If they are still alive, then that death year can be empty. We know that birth and death years should be somewhere between 1700 and 1990, and that death year should be larger than birth year. Finally, we also know that the years should be integers, and the names should be characters. So, we want something that looks roughly like this:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(babynames)}
\KeywordTok{library}\NormalTok{(tidyverse)}

\NormalTok{simulated_dataset <-}\StringTok{ }
\StringTok{  }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{prime_minister =} \KeywordTok{sample}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ babynames }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(prop }\OperatorTok{>}\StringTok{ }\FloatTok{0.01}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{                                   }\KeywordTok{select}\NormalTok{(name) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{unique}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{unlist}\NormalTok{(), }
                                 \DataTypeTok{size =} \DecValTok{10}\NormalTok{, }\DataTypeTok{replace =} \OtherTok{FALSE}\NormalTok{),}
         \DataTypeTok{birth_year =} \KeywordTok{sample}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\DecValTok{1700}\OperatorTok{:}\DecValTok{1990}\NormalTok{), }\DataTypeTok{size =} \DecValTok{10}\NormalTok{, }\DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{),}
         \DataTypeTok{years_lived =} \KeywordTok{sample}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\DecValTok{50}\OperatorTok{:}\DecValTok{100}\NormalTok{), }\DataTypeTok{size =} \DecValTok{10}\NormalTok{, }\DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{),}
         \DataTypeTok{death_year =}\NormalTok{ birth_year }\OperatorTok{+}\StringTok{ }\NormalTok{years_lived) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(prime_minister, birth_year, death_year, years_lived) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(birth_year)}

\KeywordTok{head}\NormalTok{(simulated_dataset)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 4
##   prime_minister birth_year death_year years_lived
##   <chr>               <int>      <int>       <int>
## 1 Cynthia              1707       1777          70
## 2 Joyce                1715       1805          90
## 3 Emily                1716       1801          85
## 4 Jennifer             1731       1813          82
## 5 Frances              1743       1838          95
## 6 Cora                 1768       1823          55
\end{verbatim}

One of the advantages of generating a simulated dataset is that if you are working in groups then one person can start making the graph, using the simulated dataset, while the other person gathers the data. In terms of a graph, we want something like Figure \ref{fig:pmsgraphexample}.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/IMG_4185} \caption{Sketch of planned graph.}\label{fig:pmsgraphexample}
\end{figure}

\hypertarget{gather-1}{%
\subsection{Gather}\label{gather-1}}

We are starting with a question that is of interest, which how long each Canadian prime minister lived. As such, we need to identify a source of data While there are likely to be plenty of data sources that have the births and deaths of each prime minister, we want one that we can trust, and as we are going to be scraping, we want one that has some structure to it. The Wikipedia page (\url{https://en.wikipedia.org/wiki/List_of_prime_ministers_of_Canada}) fits both these criteria. As it is a popular page the information is more likely to be correct, and the data are available in a table.

We load the library and then we read in the data from the relevant page. The key function here is \texttt{read\_html()}, which you can use in the same way as, say, \texttt{read\_csv()}, except that it takes a html page as an input. Once you call \texttt{read\_html()} then the page is downloaded to your own computer, and it is usually a good idea to save this, using \texttt{write\_html()} as it is your raw data. Saving it also means that we don't have to keep visiting the website when we want to start again with our cleaning, and so it is part of being polite. However, it is likely not our property (in the case of Wikipedia, we might be okay), and so you should probably not share it.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rvest)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{raw_data <-}\StringTok{ }\KeywordTok{read_html}\NormalTok{(}\StringTok{"https://en.wikipedia.org/wiki/List_of_prime_ministers_of_Canada"}\NormalTok{)}
\KeywordTok{write_html}\NormalTok{(raw_data, }\StringTok{"inputs/wiki/pms.html"}\NormalTok{) }\CommentTok{# Note that we save the file as a html file.}
\end{Highlighting}
\end{Shaded}

\hypertarget{clean-1}{%
\subsection{Clean}\label{clean-1}}

Websites are made up of html, which is a markup language. We are looking for patterns in the mark-up that we can use to help us get closer to the data that we want. This is an iterative process and requires a lot of trial and error. Even simple examples will take time. You can look at the html by using a browser, right clicking, and then selecting \texttt{view\ page\ source}. Similarly, you could open the html file using a text editor.

\hypertarget{by-inspection}{%
\subsubsection{By inspection}\label{by-inspection}}

We are looking for patterns that we can use to select the information that is of interest - names, birth year, and death year. When we look at the html it looks like there is something going on with \texttt{\textless{}tr\textgreater{}}, and then \texttt{\textless{}td\textgreater{}} (thanks to Thomas Rosenthal for identifying this). We select those nodes using \texttt{html\_nodes()}, which takes the tags as an input. If you only want the first one then there is a singular version, \texttt{html\_node()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Read in our saved data}
\NormalTok{raw_data <-}\StringTok{ }\KeywordTok{read_html}\NormalTok{(}\StringTok{"inputs/wiki/pms.html"}\NormalTok{)}

\CommentTok{# We can parse tags in order}
\NormalTok{parse_data_inspection <-}\StringTok{ }
\StringTok{  }\NormalTok{raw_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{html_nodes}\NormalTok{(}\StringTok{"tr"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{html_nodes}\NormalTok{(}\StringTok{"td"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{html_text}\NormalTok{() }\CommentTok{# html_text removes any remaining html tags}

\CommentTok{# But this code does exactly the same thing - the nodes are just pushed into }
\CommentTok{# the one function call}
\NormalTok{parse_data_inspection <-}\StringTok{ }
\StringTok{  }\NormalTok{raw_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{html_nodes}\NormalTok{(}\StringTok{"tr td"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{html_text}\NormalTok{()}

\KeywordTok{head}\NormalTok{(parse_data_inspection)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Abbreviation key:"                                                                                                                                                                                                                              
## [2] "No.: Incumbent number, Min.: Ministry, Refs: References\n"                                                                                                                                                                                      
## [3] "Colour key:"                                                                                                                                                                                                                                    
## [4] "\n\n  Liberal Party of Canada\n \n  Historical Conservative parties (including Liberal-Conservative, Conservative (Historical),     Unionist, National Liberal and Conservative, Progressive Conservative) \n  Conservative Party of Canada\n\n"
## [5] "Provinces key:"                                                                                                                                                                                                                                 
## [6] "AB: Alberta, BC: British Columbia, MB: Manitoba, NS: Nova Scotia,ON: Ontario, QC: Quebec, SK: Saskatchewan\n"
\end{verbatim}

At this point our data is in a character vector, we want to convert it to a table, and reduce the data down to just the information that we want. The key that is going to allow us to do this is the fact that there seems to be a blank line (which in html is denoted by \texttt{\textbackslash{}n}) before the key information that we need. So, once we identify that line then we can filter to just the line below it!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{parsed_data <-}\StringTok{ }
\StringTok{  }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{raw_text =}\NormalTok{ parse_data_inspection) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# Convert the character vector to a table}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{is_PM =} \KeywordTok{if_else}\NormalTok{(raw_text }\OperatorTok{==}\StringTok{ "}\CharTok{\textbackslash{}n\textbackslash{}n}\StringTok{"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{), }\CommentTok{# Look for the blank line that is }
         \CommentTok{# above the row that we want}
         \DataTypeTok{is_PM =} \KeywordTok{lag}\NormalTok{(is_PM, }\DataTypeTok{n =} \DecValTok{1}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# Identify the actual row that we want}
\StringTok{  }\KeywordTok{filter}\NormalTok{(is_PM }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) }\CommentTok{# Just get the rows that we want}

\KeywordTok{head}\NormalTok{(parsed_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 2
##   raw_text                                                                 is_PM
##   <chr>                                                                    <dbl>
## 1 "\nSir John A. MacDonald(1815–1891)MP for Kingston, ON\n"                    1
## 2 "\nAlexander Mackenzie(1822–1892)MP for Lambton, ON\n"                       1
## 3 "\nSir John A. MacDonald(1815–1891)MP for Victoria, BC until 1882MP for~     1
## 4 "\nSir John Abbott(1821–1893)Senator for Quebec\n"                           1
## 5 "\nSir John Thompson(1845–1894)MP for Antigonish, NS\n"                      1
## 6 "\nSir Mackenzie Bowell(1823–1917)Senator for Ontario\n"                     1
\end{verbatim}

\hypertarget{using-the-selector-gadget}{%
\subsubsection{Using the selector gadget}\label{using-the-selector-gadget}}

If you are comfortable with html then you might be able to see patterns, but one tool that may help is the SelectorGadget: \url{https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html}. This allows you to pick and choose the elements that you want, and then gives you the input to give to \texttt{html\_nodes()} (Figure \ref{fig:selectorgadget})

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/selectorgadget} \caption{Using the Selector Gadget to identify the tag, as at 13 March 2020.}\label{fig:selectorgadget}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Read in our saved data}
\NormalTok{raw_data <-}\StringTok{ }\KeywordTok{read_html}\NormalTok{(}\StringTok{"inputs/wiki/pms.html"}\NormalTok{)}

\CommentTok{# We can parse tags in order}
\NormalTok{parse_data_selector_gadget <-}\StringTok{ }
\StringTok{  }\NormalTok{raw_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{html_nodes}\NormalTok{(}\StringTok{"td:nth-child(3)"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{html_text}\NormalTok{() }\CommentTok{# html_text removes any remaining html tags}

\KeywordTok{head}\NormalTok{(parse_data_selector_gadget)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "\nSir John A. MacDonald(1815–1891)MP for Kingston, ON\n"                                                            
## [2] "\nAlexander Mackenzie(1822–1892)MP for Lambton, ON\n"                                                               
## [3] "\nSir John A. MacDonald(1815–1891)MP for Victoria, BC until 1882MP for Carleton, ON until 1887MP for Kingston, ON\n"
## [4] "\nSir John Abbott(1821–1893)Senator for Quebec\n"                                                                   
## [5] "\nSir John Thompson(1845–1894)MP for Antigonish, NS\n"                                                              
## [6] "\nSir Mackenzie Bowell(1823–1917)Senator for Ontario\n"
\end{verbatim}

In this case there is one prime minister - Robert Borden - who changed party and we would need to filter away that row: \texttt{\textbackslash{}nUnionist\ Party\textbackslash{}n"}.

\hypertarget{clean-data}{%
\subsubsection{Clean data}\label{clean-data}}

Now that we have the parsed data, we need to clean it to match what we wanted. In particular we want a names column, as well as columns for birth year and death year. We will use \texttt{separate()} to take advantage of the fact that it looks like the dates are distinguished by brackets.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{initial_clean <-}\StringTok{ }
\StringTok{  }\NormalTok{parsed_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{separate}\NormalTok{(raw_text, }
            \DataTypeTok{into =} \KeywordTok{c}\NormalTok{(}\StringTok{"Name"}\NormalTok{, }\StringTok{"not_name"}\NormalTok{), }
            \DataTypeTok{sep =} \StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{("}\NormalTok{,}
            \DataTypeTok{remove =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# The remove = FALSE option here means that we }
\StringTok{  }\CommentTok{# keep the original column that we are separating.}
\StringTok{  }\KeywordTok{separate}\NormalTok{(not_name, }
            \DataTypeTok{into =} \KeywordTok{c}\NormalTok{(}\StringTok{"Date"}\NormalTok{, }\StringTok{"all_the_rest"}\NormalTok{), }
            \DataTypeTok{sep =} \StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{)"}\NormalTok{,}
            \DataTypeTok{remove =} \OtherTok{FALSE}\NormalTok{)}

\KeywordTok{head}\NormalTok{(initial_clean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 6
##   raw_text           Name     not_name          Date  all_the_rest         is_PM
##   <chr>              <chr>    <chr>             <chr> <chr>                <dbl>
## 1 "\nSir John A. Ma~ "\nSir ~ "1815–1891)MP fo~ 1815~ "MP for Kingston, O~     1
## 2 "\nAlexander Mack~ "\nAlex~ "1822–1892)MP fo~ 1822~ "MP for Lambton, ON~     1
## 3 "\nSir John A. Ma~ "\nSir ~ "1815–1891)MP fo~ 1815~ "MP for Victoria, B~     1
## 4 "\nSir John Abbot~ "\nSir ~ "1821–1893)Senat~ 1821~ "Senator for Quebec~     1
## 5 "\nSir John Thomp~ "\nSir ~ "1845–1894)MP fo~ 1845~ "MP for Antigonish,~     1
## 6 "\nSir Mackenzie ~ "\nSir ~ "1823–1917)Senat~ 1823~ "Senator for Ontari~     1
\end{verbatim}

Finally, we need to clean up the columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned_data <-}\StringTok{ }
\StringTok{  }\NormalTok{initial_clean }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(Name, Date) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{separate}\NormalTok{(Date, }\DataTypeTok{into =} \KeywordTok{c}\NormalTok{(}\StringTok{"Birth"}\NormalTok{, }\StringTok{"Died"}\NormalTok{), }\DataTypeTok{sep =} \StringTok{"–"}\NormalTok{, }\DataTypeTok{remove =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# The }
\StringTok{  }\CommentTok{# PMs who have died have their birth and death years separated by a hyphen, but }
\StringTok{  }\CommentTok{# you need to be careful with the hyphen as it seems to be a slightly odd type of }
\StringTok{  }\CommentTok{# hyphen and you need to copy/paste it.}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Birth =} \KeywordTok{str_remove}\NormalTok{(Birth, }\StringTok{"b. "}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# Alive PMs have slightly different format}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{Date) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Name =} \KeywordTok{str_remove}\NormalTok{(Name, }\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# Remove some html tags that remain}
\StringTok{  }\KeywordTok{mutate_at}\NormalTok{(}\KeywordTok{vars}\NormalTok{(Birth, Died), }\OperatorTok{~}\KeywordTok{as.integer}\NormalTok{(.)) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# Change birth and death to integers}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Age_at_Death =}\NormalTok{ Died }\OperatorTok{-}\StringTok{ }\NormalTok{Birth) }\OperatorTok{%>%}\StringTok{  }\CommentTok{# Add column of the number of years they lived}
\StringTok{  }\KeywordTok{distinct}\NormalTok{() }\CommentTok{# Some of the PMs had two goes at it.}

\KeywordTok{head}\NormalTok{(cleaned_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 4
##   Name                  Birth  Died Age_at_Death
##   <chr>                 <int> <int>        <int>
## 1 Sir John A. MacDonald  1815  1891           76
## 2 Alexander Mackenzie    1822  1892           70
## 3 Sir John Abbott        1821  1893           72
## 4 Sir John Thompson      1845  1894           49
## 5 Sir Mackenzie Bowell   1823  1917           94
## 6 Sir Charles Tupper     1821  1915           94
\end{verbatim}

\hypertarget{explore-1}{%
\subsection{Explore}\label{explore-1}}

At this point we'd like to make a graph that illustrates how long each prime minister lived. If they are still alive then we would like them to run to the end, but we would like to colour them differently.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{still_alive =} \KeywordTok{if_else}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(Died), }\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{),}
         \DataTypeTok{Died =} \KeywordTok{if_else}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(Died), }\KeywordTok{as.integer}\NormalTok{(}\DecValTok{2020}\NormalTok{), Died)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Name =} \KeywordTok{as_factor}\NormalTok{(Name)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Birth, }
             \DataTypeTok{xend =}\NormalTok{ Died,}
             \DataTypeTok{y =}\NormalTok{ Name,}
             \DataTypeTok{yend =}\NormalTok{ Name, }
             \DataTypeTok{color =}\NormalTok{ still_alive)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_segment}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Year of birth"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Prime minister"}\NormalTok{,}
       \DataTypeTok{color =} \StringTok{"PM is alive"}\NormalTok{,}
       \DataTypeTok{title =} \StringTok{"Canadian Prime Minister, by year of birth"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-169-1.pdf}

\hypertarget{pdfs}{%
\section{PDFs}\label{pdfs}}

\hypertarget{introduction-11}{%
\subsection{Introduction}\label{introduction-11}}

In contrast to an API, a PDF is usually only produced for human (not computer) consumption. The nice thing about PDFs is that they are static and constant. And it is nice that they make data available at all. But the trade-off is that:

\begin{itemize}
\tightlist
\item
  It is not overly useful to do larger-scale statistical analysis.
\item
  We don't know how the PDF was put together so we don't know whether we can trust it.
\item
  We can't manipulate the data to get results that we are interested in.
\end{itemize}

Indeed, sometimes governments publish data as PDFs because they don't actually want you to be able to analyse it! Being able to get data from PDFs opens up a large number of dataset for you, some of which we'll see in this chapter.

There are two important aspects to keep in mind when approaching a PDF with a mind to extracting data from it:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Begin with an end in mind. Planning and then literally sketching out what you want from a final dataset/graph/paper stops you wasting time and keeps you focused.
\item
  Start simple, then iterate. The quickest way to make a complicated model is often to first build a simple model and then complicate it. Start with just trying to get one page of the PDF working or even just one line. Then iterate from there.
\end{enumerate}

In this chapter we start by walking through several examples and then go through three case studies of varying difficulty.

\hypertarget{getting-started-4}{%
\subsection{Getting started}\label{getting-started-4}}

Figure \ref{fig:firstpdfexample} is a PDF that consists of just the first sentence from Jane Eyre taken from Project Gutenberg \citet{janeeyre}.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/inputs/pdfs/first_example} \caption{First sentence of Jane Eyre}\label{fig:firstpdfexample}
\end{figure}

We will use the package \texttt{pdftools} \citet{citepdftools} to get the text in this one page PDF into R.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install.packages("pdftools")}
\KeywordTok{library}\NormalTok{(pdftools)}
\KeywordTok{library}\NormalTok{(tidyverse)}

\NormalTok{first_example <-}\StringTok{ }\NormalTok{pdftools}\OperatorTok{::}\KeywordTok{pdf_text}\NormalTok{(}\StringTok{"inputs/pdfs/first_example.pdf"}\NormalTok{)}

\NormalTok{first_example}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "There was no possibility of taking a walk that day."
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(first_example)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "character"
\end{verbatim}

We can see that the PDF has been correctly read in, as a character vector.

We will now try a slightly more complicated example that consists of the first few paragraphs of Jane Eyre (Figure \ref{fig:secondpdfexample}). Also notice that now we have the chapter heading as well.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/inputs/pdfs/second_example} \caption{First few paragraphs of Jane Eyre}\label{fig:secondpdfexample}
\end{figure}

We use the same function as before.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{second_example <-}\StringTok{ }\NormalTok{pdftools}\OperatorTok{::}\KeywordTok{pdf_text}\NormalTok{(}\StringTok{"inputs/pdfs/second_example.pdf"}\NormalTok{)}

\NormalTok{second_example}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "CHAPTER I\nThere was no possibility of taking a walk that day. We had been wandering, indeed, in the\nleafless shrubbery an hour in the morning; but since dinner (Mrs. Reed, when there was no\ncompany, dined early) the cold winter wind had brought with it clouds so sombre, and a rain so\npenetrating, that further out-door exercise was now out of the question.\nI was glad of it: I never liked long walks, especially on chilly afternoons: dreadful to me was the\ncoming home in the raw twilight, with nipped fingers and toes, and a heart saddened by the\nchidings of Bessie, the nurse, and humbled by the consciousness of my physical inferiority to\nEliza, John, and Georgiana Reed.\nThe said Eliza, John, and Georgiana were now clustered round their mama in the drawing-room:\nshe lay reclined on a sofa by the fireside, and with her darlings about her (for the time neither\nquarrelling nor crying) looked perfectly happy. Me, she had dispensed from joining the group;\nsaying, “She regretted to be under the necessity of keeping me at a distance; but that until she\nheard from Bessie, and could discover by her own observation, that I was endeavouring in good\nearnest to acquire a more sociable and childlike disposition, a more attractive and sprightly\nmanner—something lighter, franker, more natural, as it were—she really must exclude me from\nprivileges intended only for contented, happy, little children.”\n“What does Bessie say I have done?” I asked.\n“Jane, I don’t like cavillers or questioners; besides, there is something truly forbidding in a child\ntaking up her elders in that manner. Be seated somewhere; and until you can speak pleasantly,\nremain silent.”\nA breakfast-room adjoined the drawing-room, I slipped in there. It contained a bookcase: I soon\npossessed myself of a volume, taking care that it should be one stored with pictures. I mounted\ninto the window-seat: gathering up my feet, I sat cross-legged, like a Turk; and, having drawn the\nred moreen curtain nearly close, I was shrined in double retirement.\nFolds of scarlet drapery shut in my view to the right hand; to the left were the clear panes of\nglass, protecting, but not separating me from the drear November day. At intervals, while\nturning over the leaves of my book, I studied the aspect of that winter afternoon. Afar, it offered\na pale blank of mist and cloud; near a scene of wet lawn and storm-beat shrub, with ceaseless\nrain sweeping away wildly before a long and lamentable blast.\n"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(second_example)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "character"
\end{verbatim}

Again, we have a character vector. The end of each line is signalled by `\textbackslash n', but other than that it looks pretty good.

Finally, we consider the first two pages.

We use the same function as before.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{third_example <-}\StringTok{ }\NormalTok{pdftools}\OperatorTok{::}\KeywordTok{pdf_text}\NormalTok{(}\StringTok{"inputs/pdfs/third_example.pdf"}\NormalTok{)}

\NormalTok{third_example}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "CHAPTER I\nThere was no possibility of taking a walk that day. We had been wandering, indeed, in the\nleafless shrubbery an hour in the morning; but since dinner (Mrs. Reed, when there was no\ncompany, dined early) the cold winter wind had brought with it clouds so sombre, and a rain so\npenetrating, that further out-door exercise was now out of the question.\nI was glad of it: I never liked long walks, especially on chilly afternoons: dreadful to me was the\ncoming home in the raw twilight, with nipped fingers and toes, and a heart saddened by the\nchidings of Bessie, the nurse, and humbled by the consciousness of my physical inferiority to\nEliza, John, and Georgiana Reed.\nThe said Eliza, John, and Georgiana were now clustered round their mama in the drawing-room:\nshe lay reclined on a sofa by the fireside, and with her darlings about her (for the time neither\nquarrelling nor crying) looked perfectly happy. Me, she had dispensed from joining the group;\nsaying, “She regretted to be under the necessity of keeping me at a distance; but that until she\nheard from Bessie, and could discover by her own observation, that I was endeavouring in good\nearnest to acquire a more sociable and childlike disposition, a more attractive and sprightly\nmanner—something lighter, franker, more natural, as it were—she really must exclude me from\nprivileges intended only for contented, happy, little children.”\n“What does Bessie say I have done?” I asked.\n“Jane, I don’t like cavillers or questioners; besides, there is something truly forbidding in a child\ntaking up her elders in that manner. Be seated somewhere; and until you can speak pleasantly,\nremain silent.”\nA breakfast-room adjoined the drawing-room, I slipped in there. It contained a bookcase: I soon\npossessed myself of a volume, taking care that it should be one stored with pictures. I mounted\ninto the window-seat: gathering up my feet, I sat cross-legged, like a Turk; and, having drawn the\nred moreen curtain nearly close, I was shrined in double retirement.\nFolds of scarlet drapery shut in my view to the right hand; to the left were the clear panes of\nglass, protecting, but not separating me from the drear November day. At intervals, while\nturning over the leaves of my book, I studied the aspect of that winter afternoon. Afar, it offered\na pale blank of mist and cloud; near a scene of wet lawn and storm-beat shrub, with ceaseless\nrain sweeping away wildly before a long and lamentable blast.\nI returned to my book—Bewick’s History of British Birds: the letterpress thereof I cared little\nfor, generally speaking; and yet there were certain introductory pages that, child as I was, I could\nnot pass quite as a blank. They were those which treat of the haunts of sea-fowl; of “the solitary\nrocks and promontories” by them only inhabited; of the coast of Norway, studded with isles from\nits southern extremity, the Lindeness, or Naze, to the North Cape—\n“Where the Northern Ocean, in vast whirls,\nBoils round the naked, melancholy isles\n"
## [2] "Of farthest Thule; and the Atlantic surge\nPours in among the stormy Hebrides.”\nNor could I pass unnoticed the suggestion of the bleak shores of Lapland, Siberia, Spitzbergen,\nNova Zembla, Iceland, Greenland, with “the vast sweep of the Arctic Zone, and those forlorn\nregions of dreary space,—that reservoir of frost and snow, where firm fields of ice, the\naccumulation of centuries of winters, glazed in Alpine heights above heights, surround the pole,\nand concentre the multiplied rigours of extreme cold.” Of these death-white realms I formed an\nidea of my own: shadowy, like all the half-comprehended notions that float dim through\nchildren’s brains, but strangely impressive. The words in these introductory pages connected\nthemselves with the succeeding vignettes, and gave significance to the rock standing up alone in\na sea of billow and spray; to the broken boat stranded on a desolate coast; to the cold and ghastly\nmoon glancing through bars of cloud at a wreck just sinking.\nI cannot tell what sentiment haunted the quite solitary churchyard, with its inscribed headstone;\nits gate, its two trees, its low horizon, girdled by a broken wall, and its newly-risen crescent,\nattesting the hour of eventide.\nThe two ships becalmed on a torpid sea, I believed to be marine phantoms.\nThe fiend pinning down the thief’s pack behind him, I passed over quickly: it was an object of\nterror.\nSo was the black horned thing seated aloof on a rock, surveying a distant crowd surrounding a\ngallows.\nEach picture told a story; mysterious often to my undeveloped understanding and imperfect\nfeelings, yet ever profoundly interesting: as interesting as the tales Bessie sometimes narrated on\nwinter evenings, when she chanced to be in good humour; and when, having brought her ironing-\ntable to the nursery hearth, she allowed us to sit about it, and while she got up Mrs. Reed’s lace\nfrills, and crimped her nightcap borders, fed our eager attention with passages of love and\nadventure taken from old fairy tales and other ballads; or (as at a later period I discovered) from\nthe pages of Pamela, and Henry, Earl of Moreland.\nWith Bewick on my knee, I was then happy: happy at least in my way. I feared nothing but\ninterruption, and that came too soon. The breakfast-room door opened.\n“Boh! Madam Mope!” cried the voice of John Reed; then he paused: he found the room\napparently empty.\n“Where the dickens is she!” he continued. “Lizzy! Georgy! (calling to his sisters) Joan is not\nhere: tell mama she is run out into the rain—bad animal!”\n“It is well I drew the curtain,” thought I; and I wished fervently he might not discover my hiding-\nplace: nor would John Reed have found it out himself; he was not quick either of vision or\nconception; but Eliza just put her head in at the door, and said at once—\n"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(third_example)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "character"
\end{verbatim}

Now, notice that the first page is the first element of the character vector and the second page is the second element.

As we're most familiar with rectangular data we'll try to get it into that format as quickly as possible. And then we can use our regular tools to deal with it.

First we want to convert the character vector into a tibble. At this point we may like to add page numbers as well.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{jane_eyre <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{raw_text =}\NormalTok{ third_example,}
                    \DataTypeTok{page_number =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

We probably now want to separate the lines so that each line is an observation. We can do that by looking for the `\textbackslash n' remembering that we need to escape the backslash as it's a special character.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{jane_eyre <-}\StringTok{ }\KeywordTok{separate_rows}\NormalTok{(jane_eyre, raw_text, }\DataTypeTok{sep =} \StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{n"}\NormalTok{, }\DataTypeTok{convert =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{head}\NormalTok{(jane_eyre)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 2
##   raw_text                                                           page_number
##   <chr>                                                                    <int>
## 1 CHAPTER I                                                                    1
## 2 There was no possibility of taking a walk that day. We had been w~           1
## 3 leafless shrubbery an hour in the morning; but since dinner (Mrs.~           1
## 4 company, dined early) the cold winter wind had brought with it cl~           1
## 5 penetrating, that further out-door exercise was now out of the qu~           1
## 6 I was glad of it: I never liked long walks, especially on chilly ~           1
\end{verbatim}

\hypertarget{case-study-us-total-fertility-rate-by-state-and-year-2000-2018}{%
\section{Case-study: US Total Fertility Rate, by state and year (2000-2018)}\label{case-study-us-total-fertility-rate-by-state-and-year-2000-2018}}

\hypertarget{introduction-12}{%
\subsection{Introduction}\label{introduction-12}}

If you're married to a demographer it is not too long until you are asked to look at a US Department of Health and Human Services Vital Statistics Report. In this case we are interested in trying to get the total fertility rate (the average number of births per woman assuming that woman experience the current age-specific fertility rates throughout their reproductive years)\footnote{And if you'd like to know more about this then I'd recommend starting a PhD with \href{https://www.monicaalexander.com/}{Monica Alexander}.} for each state for nineteen years. Annoyingly, the US persists in only making this data available in PDFs, but it makes a nice case study.

In the case of the year 2000 the table that we are interested in is on page 40 of a PDF that is available \url{https://www.cdc.gov/nchs/data/nvsr/nvsr50/nvsr50_05.pdf} and it is the column labelled: ``Total fertility rate'' (Figure \ref{fig:dhsexample}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/dhs_example} \caption{Example Vital Statistics Report, from 2000}\label{fig:dhsexample}
\end{figure}

\hypertarget{begin-with-an-end-in-mind}{%
\subsection{Begin with an end in mind}\label{begin-with-an-end-in-mind}}

The first step when getting data out of a PDF is to sketch out what you eventually want. A PDF typically contains a lot of information, and so it is handy to be very clear about what you need. This helps keep you focused, and prevents scope creep, but it is also helpful when thinking about data checks. Literally write down on paper what you have in mind.

In this case, what is needed is a table with a column for state, year and TFR (Figure \ref{fig:tfrdesired}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/tfr_desired} \caption{Desired output from the PDF}\label{fig:tfrdesired}
\end{figure}

\hypertarget{start-simple-then-iterate.}{%
\subsection{Start simple, then iterate.}\label{start-simple-then-iterate.}}

There are 19 different PDFs and we are interested in a particular column in a particular table in each of them. Unfortunately there is nothing magical about what is coming. This first step requires working out the link for each, and the page and column name that is of interest. In the end, this looks like this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{monicas_data <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"inputs/tfr_tables_info.csv"}\NormalTok{)}

\NormalTok{monicas_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(year, page, table, column_name, url) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{gt}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\captionsetup[table]{labelformat=empty,skip=1pt}
\begin{longtable}{rrrll}
\toprule
year & page & table & column\_name & url \\ 
\midrule
2000 & 40 & 10 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr50/nvsr50\_05.pdf \\ 
2001 & 41 & 10 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr51/nvsr51\_02.pdf \\ 
2002 & 46 & 10 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr52/nvsr52\_10.pdf \\ 
2003 & 45 & 10 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr54/nvsr54\_02.pdf \\ 
2004 & 52 & 11 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr55/nvsr55\_01.pdf \\ 
2005 & 52 & 11 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr56/nvsr56\_06.pdf \\ 
2006 & 49 & 11 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr57/nvsr57\_07.pdf \\ 
2007 & 41 & 11 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr58/nvsr58\_24.pdf \\ 
2008 & 43 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr59/nvsr59\_01.pdf \\ 
2009 & 43 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr60/nvsr60\_01.pdf \\ 
2010 & 42 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr61/nvsr61\_01.pdf \\ 
2011 & 40 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr62/nvsr62\_01.pdf \\ 
2012 & 38 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr62/nvsr62\_09.pdf \\ 
2013 & 37 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr64/nvsr64\_01.pdf \\ 
2014 & 38 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr64/nvsr64\_12.pdf \\ 
2015 & 42 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr66/nvsr66\_01.pdf \\ 
2016 & 29 & 8 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr67/nvsr67\_01.pdf \\ 
2016 & 30 & 8 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr67/nvsr67\_01.pdf \\ 
2017 & 23 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr67/nvsr67\_08-508.pdf \\ 
2017 & 24 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr67/nvsr67\_08-508.pdf \\ 
2018 & 23 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr68/nvsr68\_13-508.pdf \\ 
\bottomrule
\end{longtable}

The first step is to get some code that works for one of them. I'll step through the code in a lot more detail than normal because we're going to use these pieces a lot.

We will choose the year 2000. We first download the data and save it.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{download.file}\NormalTok{(}\DataTypeTok{url =}\NormalTok{ monicas_data}\OperatorTok{$}\NormalTok{url[}\DecValTok{1}\NormalTok{], }
              \DataTypeTok{destfile =} \StringTok{"inputs/pdfs/dhs/year_2000.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We now want to read the PDF in as a character vector.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dhs_}\DecValTok{2000}\NormalTok{ <-}\StringTok{ }\NormalTok{pdftools}\OperatorTok{::}\KeywordTok{pdf_text}\NormalTok{(}\StringTok{"inputs/pdfs/dhs/year_2000.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Convert it to a tibble, so that we can use familiar verbs on it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dhs_}\DecValTok{2000}\NormalTok{ <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{raw_data =}\NormalTok{ dhs_}\DecValTok{2000}\NormalTok{)}

\KeywordTok{head}\NormalTok{(dhs_}\DecValTok{2000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 1
##   raw_data                                                                      
##   <chr>                                                                         
## 1 "Volume 50, Number 5                                                         ~
## 2 "2     National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\nH~
## 3 "                                                                            ~
## 4 "4     National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\nD~
## 5 "                                                                            ~
## 6 "6     National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\n ~
\end{verbatim}

Grab the page that is of interest (remembering that each page is a element of the character vector, hence a row in the tibble).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dhs_}\DecValTok{2000}\NormalTok{ <-}\StringTok{ }
\StringTok{  }\NormalTok{dhs_}\DecValTok{2000} \OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{slice}\NormalTok{(monicas_data}\OperatorTok{$}\NormalTok{page[}\DecValTok{1}\NormalTok{])}

\KeywordTok{head}\NormalTok{(dhs_}\DecValTok{2000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 1
##   raw_data                                                                      
##   <chr>                                                                         
## 1 "40 National Vital Statistics Report, Vol. 50, No. 5, Revised May 15, 20022\n~
\end{verbatim}

Now we want to separate the rows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dhs_}\DecValTok{2000}\NormalTok{ <-}\StringTok{ }
\StringTok{  }\NormalTok{dhs_}\DecValTok{2000} \OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{separate_rows}\NormalTok{(raw_data, }\DataTypeTok{sep =} \StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{n"}\NormalTok{, }\DataTypeTok{convert =} \OtherTok{FALSE}\NormalTok{)}

\KeywordTok{head}\NormalTok{(dhs_}\DecValTok{2000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 1
##   raw_data                                                                      
##   <chr>                                                                         
## 1 40 National Vital Statistics Report, Vol. 50, No. 5, Revised May 15, 20022    
## 2 Table 10. Number of births, birth rates, fertility rates, total fertility rat~
## 3 United States, each State and territory, 2000                                 
## 4 [By place of residence. Birth rates are live births per 1,000 estimated popul~
## 5 estimated in each area; total fertility rates are sums of birth rates for 5-y~
## 6 age group estimated in each area]
\end{verbatim}

Now we are searching for patterns that we can use. (If you have a lot of tables that you are interested in grabbing from PDFs then it may also be worthwhile considering the tabulizer package which is specifically designed for that. The issue is that it depends on Java and I always seem to run into trouble when I need to use Java so I avoid it when I can.)

Let's look at the first ten lines of content.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dhs_}\DecValTok{2000}\NormalTok{[}\DecValTok{13}\OperatorTok{:}\DecValTok{22}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 10 x 1
##    raw_data                                                                     
##    <chr>                                                                        
##  1 United States 1 ......................................................      ~
##  2 Alabama ...............................................................     ~
##  3 Alaska ...................................................................  ~
##  4 Arizona .................................................................   ~
##  5 Arkansas ...............................................................    ~
##  6 California ..............................................................   ~
##  7 Colorado ...............................................................    ~
##  8 Connecticut ...........................................................     ~
##  9 Delaware ..............................................................     ~
## 10 District of Columbia ..............................................         ~
\end{verbatim}

It doesn't get much better than this:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We have dots separating the states from the data.
\item
  We have a space between each of the columns.
\end{enumerate}

So we can now separate this in to separate columns. First we want to match on when there is at least two dots (remembering that the dot is a special character and so needs to be escaped).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dhs_}\DecValTok{2000}\NormalTok{ <-}\StringTok{ }
\StringTok{  }\NormalTok{dhs_}\DecValTok{2000} \OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{separate}\NormalTok{(}\DataTypeTok{col =}\NormalTok{ raw_data, }
           \DataTypeTok{into =} \KeywordTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{, }\StringTok{"data"}\NormalTok{), }
           \DataTypeTok{sep =} \StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{.\{2,\}"}\NormalTok{, }
           \DataTypeTok{remove =} \OtherTok{FALSE}\NormalTok{,}
           \DataTypeTok{fill =} \StringTok{"right"}
\NormalTok{           )}

\KeywordTok{head}\NormalTok{(dhs_}\DecValTok{2000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 3
##   raw_data                              state                              data 
##   <chr>                                 <chr>                              <chr>
## 1 40 National Vital Statistics Report,~ 40 National Vital Statistics Repo~ <NA> 
## 2 Table 10. Number of births, birth ra~ Table 10. Number of births, birth~ <NA> 
## 3 United States, each State and territ~ United States, each State and ter~ <NA> 
## 4 [By place of residence. Birth rates ~ [By place of residence. Birth rat~ <NA> 
## 5 estimated in each area; total fertil~ estimated in each area; total fer~ <NA> 
## 6 age group estimated in each area]     age group estimated in each area]  <NA>
\end{verbatim}

We get the expected warnings about the top and the bottom as they don't have multiple dots.

(Another option here is to use the \texttt{pdf\_data()} function which would allow us to use location rather than delimiters.)

We can now separate the data based on spaces. There is an inconsistent number of spaces, so we first squish any example of more than one space into just one.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dhs_}\DecValTok{2000}\NormalTok{ <-}\StringTok{ }
\StringTok{  }\NormalTok{dhs_}\DecValTok{2000} \OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{str_squish}\NormalTok{(data)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{tidyr}\OperatorTok{::}\KeywordTok{separate}\NormalTok{(}\DataTypeTok{col =}\NormalTok{ data, }
           \DataTypeTok{into =} \KeywordTok{c}\NormalTok{(}\StringTok{"number_of_births"}\NormalTok{, }
                    \StringTok{"birth_rate"}\NormalTok{, }
                    \StringTok{"fertility_rate"}\NormalTok{, }
                    \StringTok{"TFR"}\NormalTok{, }
                    \StringTok{"teen_births_all"}\NormalTok{, }
                    \StringTok{"teen_births_15_17"}\NormalTok{, }
                    \StringTok{"teen_births_18_19"}\NormalTok{), }
           \DataTypeTok{sep =} \StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{s"}\NormalTok{, }
           \DataTypeTok{remove =} \OtherTok{FALSE}
\NormalTok{           )}

\KeywordTok{head}\NormalTok{(dhs_}\DecValTok{2000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 10
##   raw_data state data  number_of_births birth_rate fertility_rate TFR  
##   <chr>    <chr> <chr> <chr>            <chr>      <chr>          <chr>
## 1 40 Nati~ 40 N~ <NA>  <NA>             <NA>       <NA>           <NA> 
## 2 Table 1~ Tabl~ <NA>  <NA>             <NA>       <NA>           <NA> 
## 3 United ~ Unit~ <NA>  <NA>             <NA>       <NA>           <NA> 
## 4 [By pla~ [By ~ <NA>  <NA>             <NA>       <NA>           <NA> 
## 5 estimat~ esti~ <NA>  <NA>             <NA>       <NA>           <NA> 
## 6 age gro~ age ~ <NA>  <NA>             <NA>       <NA>           <NA> 
## # ... with 3 more variables: teen_births_all <chr>, teen_births_15_17 <chr>,
## #   teen_births_18_19 <chr>
\end{verbatim}

This is all looking fairly great. The only thing left is to clean up.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dhs_}\DecValTok{2000}\NormalTok{ <-}\StringTok{ }
\StringTok{  }\NormalTok{dhs_}\DecValTok{2000} \OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(state, TFR) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{slice}\NormalTok{(}\DecValTok{13}\OperatorTok{:}\DecValTok{69}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{year =} \DecValTok{2000}\NormalTok{)}

\NormalTok{dhs_}\DecValTok{2000}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 57 x 3
##    state                   TFR      year
##    <chr>                   <chr>   <dbl>
##  1 "United States 1 "      2,130.0  2000
##  2 "Alabama "              2,021.0  2000
##  3 "Alaska "               2,437.0  2000
##  4 "Arizona "              2,652.5  2000
##  5 "Arkansas "             2,140.0  2000
##  6 "California "           2,186.0  2000
##  7 "Colorado "             2,356.5  2000
##  8 "Connecticut "          1,931.5  2000
##  9 "Delaware "             2,014.0  2000
## 10 "District of Columbia " 1,975.5  2000
## # ... with 47 more rows
\end{verbatim}

And we're done for that year. Now we want to take these pieces, put them into a function and then run that function over all 19 years.

\hypertarget{iterating}{%
\subsection{Iterating}\label{iterating}}

\hypertarget{get-the-pdfs}{%
\subsubsection{Get the PDFs}\label{get-the-pdfs}}

The first part is downloading each of the 19 PDFs that we need. We're going to build on the code that we used before. That code was:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{download.file}\NormalTok{(}\DataTypeTok{url =}\NormalTok{ monicas_data}\OperatorTok{$}\NormalTok{url[}\DecValTok{1}\NormalTok{], }\DataTypeTok{destfile =} \StringTok{"inputs/pdfs/dhs/year_2000.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To modify this we need:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  To have it iterate through each of the lines in the dataset that contains our CSVs (i.e.~where it says 1, we want 1, then 2, then 3, etc.).
\item
  Where it has a filename, we need it to iterate through our desired filenames (i.e.~year\_2000, then year\_2001, then year\_2002, etc).
\item
  We'd like for it to do all of this in a way that is a little robust to errors. For instance, if one of the URLs is wrong or the internet drops out then we'd like it to just move onto the next PDF, and then warn us at the end that it missed one, not to stop. (This doesn't really matter because it's only 19 files, but it's pretty easy to find yourself doing this for thousands of files).
\end{enumerate}

We will draw on the \texttt{purrr} package for this \citet{citepurrr}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(purrr)}
\NormalTok{monicas_data <-}\StringTok{ }
\StringTok{  }\NormalTok{monicas_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{pdf_name =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"inputs/pdfs/dhs/year_"}\NormalTok{, year, }\StringTok{".pdf"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{purrr}\OperatorTok{::}\KeywordTok{walk2}\NormalTok{(monicas_data}\OperatorTok{$}\NormalTok{url, monicas_data}\OperatorTok{$}\NormalTok{pdf_name, purrr}\OperatorTok{::}\KeywordTok{safely}\NormalTok{(}\OperatorTok{~}\KeywordTok{download.file}\NormalTok{(.x , .y)))}
\end{Highlighting}
\end{Shaded}

What this code does it take the function \texttt{download.file()} and give it two arguments: \texttt{.x} and \texttt{.y}. The function \texttt{walk2()} then applies that function to the inputs that we give it, in this case the URLs columns is the \texttt{.x} and the pdf\_names column is the \texttt{.y}. Finally, the \texttt{safely()} function means that if there are any failures then it just moves onto the next file instead of throwing an error.

We now have each of the PDFs saved and we can move onto getting the data from them.

\hypertarget{get-data-from-the-pdfs}{%
\subsubsection{Get data from the PDFs}\label{get-data-from-the-pdfs}}

Now we need to get the data from the PDFs. As before, we're going to build on the code that we used before. That code (overly condensed) was:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dhs_}\DecValTok{2000}\NormalTok{ <-}\StringTok{ }\NormalTok{pdftools}\OperatorTok{::}\KeywordTok{pdf_text}\NormalTok{(}\StringTok{"inputs/pdfs/dhs/year_2000.pdf"}\NormalTok{)}

\NormalTok{dhs_}\DecValTok{2000}\NormalTok{ <-}\StringTok{ }
\StringTok{  }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{raw_data =}\NormalTok{ dhs_}\DecValTok{2000}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{slice}\NormalTok{(monicas_data}\OperatorTok{$}\NormalTok{page[}\DecValTok{1}\NormalTok{]) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{separate_rows}\NormalTok{(raw_data, }\DataTypeTok{sep =} \StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{n"}\NormalTok{, }\DataTypeTok{convert =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{separate}\NormalTok{(}\DataTypeTok{col =}\NormalTok{ raw_data, }\DataTypeTok{into =} \KeywordTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{, }\StringTok{"data"}\NormalTok{), }\DataTypeTok{sep =} \StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{.\{2,\}"}\NormalTok{, }\DataTypeTok{remove =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{str_squish}\NormalTok{(data)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{separate}\NormalTok{(}\DataTypeTok{col =}\NormalTok{ data, }
           \DataTypeTok{into =} \KeywordTok{c}\NormalTok{(}\StringTok{"number_of_births"}\NormalTok{, }\StringTok{"birth_rate"}\NormalTok{, }\StringTok{"fertility_rate"}\NormalTok{, }\StringTok{"TFR"}\NormalTok{, }\StringTok{"teen_births_all"}\NormalTok{, }\StringTok{"teen_births_15_17"}\NormalTok{, }\StringTok{"teen_births_18_19"}\NormalTok{), }
           \DataTypeTok{sep =} \StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{s"}\NormalTok{, }
           \DataTypeTok{remove =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(state, TFR) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{slice}\NormalTok{(}\DecValTok{13}\OperatorTok{:}\DecValTok{69}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{year =} \DecValTok{2000}\NormalTok{)}

\NormalTok{dhs_}\DecValTok{2000}
\end{Highlighting}
\end{Shaded}

There are a bunch of aspects here that have been hardcoded, but the first thing that we want to iterate is the argument to \texttt{pdf\_text()}, then the number in in \texttt{slice()} will also need to change (that is doing the work to get only the page that we are interested in).

Two aspects are hardcoded and these may need to be updated. In particular: 1) The separate only works if each of the tables has the same columns in the same order; and 2) the slice (which restricts the data to just the states) only works. Finally, we add the year only at the end, whereas we'd need to bring that up earlier in the process.

We'll start by writing a function that will go through all the files, grab the data, get the page of interest, and then expand the rows. We'll then use a function from \texttt{purrr} to apply that function to all of the PDFs and to output a tibble.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get_pdf_convert_to_tibble <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(pdf_name, page, year)\{}
  
\NormalTok{  dhs_table_of_interest <-}\StringTok{ }
\StringTok{    }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{raw_data =}\NormalTok{ pdftools}\OperatorTok{::}\KeywordTok{pdf_text}\NormalTok{(pdf_name)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{slice}\NormalTok{(page) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{separate_rows}\NormalTok{(raw_data, }\DataTypeTok{sep =} \StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{n"}\NormalTok{, }\DataTypeTok{convert =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{separate}\NormalTok{(}\DataTypeTok{col =}\NormalTok{ raw_data, }
             \DataTypeTok{into =} \KeywordTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{, }\StringTok{"data"}\NormalTok{), }
             \DataTypeTok{sep =} \StringTok{"[�|}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{.]}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{s+(?=[[:digit:]])"}\NormalTok{, }
             \DataTypeTok{remove =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}
      \DataTypeTok{data =} \KeywordTok{str_squish}\NormalTok{(data),}
      \DataTypeTok{year_of_data =}\NormalTok{ year)}

  \KeywordTok{print}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{"Done with"}\NormalTok{, year))}
  
  \KeywordTok{return}\NormalTok{(dhs_table_of_interest)}
\NormalTok{\}}

\NormalTok{raw_dhs_data <-}\StringTok{ }\NormalTok{purrr}\OperatorTok{::}\KeywordTok{pmap_dfr}\NormalTok{(monicas_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(pdf_name, page, year),}
\NormalTok{                                get_pdf_convert_to_tibble)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Done with 2000"
## [1] "Done with 2001"
## [1] "Done with 2002"
## [1] "Done with 2003"
## [1] "Done with 2004"
## [1] "Done with 2005"
## [1] "Done with 2006"
## [1] "Done with 2007"
## [1] "Done with 2008"
## [1] "Done with 2009"
## [1] "Done with 2010"
## [1] "Done with 2011"
## [1] "Done with 2012"
## [1] "Done with 2013"
## [1] "Done with 2014"
## [1] "Done with 2015"
## [1] "Done with 2016"
## [1] "Done with 2016"
## [1] "Done with 2017"
## [1] "Done with 2017"
## [1] "Done with 2018"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(raw_dhs_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 4
##   raw_data                       state                        data  year_of_data
##   <chr>                          <chr>                        <chr>        <dbl>
## 1 40 National Vital Statistics ~ 40 National Vital Statistic~ 50, ~         2000
## 2 Table 10. Number of births, b~ Table 10. Number of births,~ <NA>          2000
## 3 United States, each State and~ United States, each State a~ <NA>          2000
## 4 [By place of residence. Birth~ [By place of residence. Bir~ <NA>          2000
## 5 estimated in each area; total~ estimated in each area; tot~ <NA>          2000
## 6 age group estimated in each a~ age group estimated in each~ <NA>          2000
\end{verbatim}

Now we need to clean up the state names and then filter on them.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{states <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Alabama"}\NormalTok{, }\StringTok{"Alaska"}\NormalTok{, }\StringTok{"Arizona"}\NormalTok{, }\StringTok{"Arkansas"}\NormalTok{, }\StringTok{"California"}\NormalTok{, }\StringTok{"Colorado"}\NormalTok{, }
            \StringTok{"Connecticut"}\NormalTok{, }\StringTok{"Delaware"}\NormalTok{, }\StringTok{"Florida"}\NormalTok{, }\StringTok{"Georgia"}\NormalTok{, }\StringTok{"Hawaii"}\NormalTok{, }\StringTok{"Idaho"}\NormalTok{, }
            \StringTok{"Illinois"}\NormalTok{, }\StringTok{"Indiana"}\NormalTok{, }\StringTok{"Iowa"}\NormalTok{, }\StringTok{"Kansas"}\NormalTok{, }\StringTok{"Kentucky"}\NormalTok{, }\StringTok{"Louisiana"}\NormalTok{, }
            \StringTok{"Maine"}\NormalTok{, }\StringTok{"Maryland"}\NormalTok{, }\StringTok{"Massachusetts"}\NormalTok{, }\StringTok{"Michigan"}\NormalTok{, }\StringTok{"Minnesota"}\NormalTok{, }
            \StringTok{"Mississippi"}\NormalTok{, }\StringTok{"Missouri"}\NormalTok{, }\StringTok{"Montana"}\NormalTok{, }\StringTok{"Nebraska"}\NormalTok{, }\StringTok{"Nevada"}\NormalTok{, }
            \StringTok{"New Hampshire"}\NormalTok{, }\StringTok{"New Jersey"}\NormalTok{, }\StringTok{"New Mexico"}\NormalTok{, }\StringTok{"New York"}\NormalTok{, }\StringTok{"North Carolina"}\NormalTok{, }
            \StringTok{"North Dakota"}\NormalTok{, }\StringTok{"Ohio"}\NormalTok{, }\StringTok{"Oklahoma"}\NormalTok{, }\StringTok{"Oregon"}\NormalTok{, }\StringTok{"Pennsylvania"}\NormalTok{, }
            \StringTok{"Rhode Island"}\NormalTok{, }\StringTok{"South Carolina"}\NormalTok{, }\StringTok{"South Dakota"}\NormalTok{, }\StringTok{"Tennessee"}\NormalTok{, }\StringTok{"Texas"}\NormalTok{, }
            \StringTok{"Utah"}\NormalTok{, }\StringTok{"Vermont"}\NormalTok{, }\StringTok{"Virginia"}\NormalTok{, }\StringTok{"Washington"}\NormalTok{, }\StringTok{"West Virginia"}\NormalTok{, }\StringTok{"Wisconsin"}\NormalTok{, }
            \StringTok{"Wyoming"}\NormalTok{, }\StringTok{"District of Columbia"}\NormalTok{)}

\NormalTok{raw_dhs_data <-}\StringTok{ }
\StringTok{  }\NormalTok{raw_dhs_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{state =} \KeywordTok{str_remove_all}\NormalTok{(state, }\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{."}\NormalTok{),}
         \DataTypeTok{state =} \KeywordTok{str_remove_all}\NormalTok{(state, }\StringTok{"�"}\NormalTok{),}
         \DataTypeTok{state =} \KeywordTok{str_remove_all}\NormalTok{(state, }\StringTok{""}\NormalTok{),}
         \DataTypeTok{state =} \KeywordTok{str_replace_all}\NormalTok{(state, }\StringTok{"United States 1"}\NormalTok{, }\StringTok{"United States"}\NormalTok{),}
         \DataTypeTok{state =} \KeywordTok{str_replace_all}\NormalTok{(state, }\StringTok{"United States1"}\NormalTok{, }\StringTok{"United States"}\NormalTok{),}
         \DataTypeTok{state =} \KeywordTok{str_replace_all}\NormalTok{(state, }\StringTok{"United States 2"}\NormalTok{, }\StringTok{"United States"}\NormalTok{),}
         \DataTypeTok{state =} \KeywordTok{str_replace_all}\NormalTok{(state, }\StringTok{"United States2"}\NormalTok{, }\StringTok{"United States"}\NormalTok{),}
         \DataTypeTok{state =} \KeywordTok{str_replace_all}\NormalTok{(state, }\StringTok{"United States²", "}\NormalTok{United States}\StringTok{"),}
\StringTok{         ) %>% }
\StringTok{  mutate(state = str_squish(state)) %>% }
\StringTok{  filter(state %in% states)}

\StringTok{head(raw_dhs_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 4
##   raw_data                              state   data                year_of_data
##   <chr>                                 <chr>   <chr>                      <dbl>
## 1 Alabama ............................~ Alabama 63,299 14.4 65.0 2~         2000
## 2 Alaska .............................~ Alaska  9,974 16.0 74.6 2,~         2000
## 3 Arizona ............................~ Arizona 85,273 17.5 84.4 2~         2000
## 4 Arkansas ...........................~ Arkans~ 37,783 14.7 69.1 2~         2000
## 5 California .........................~ Califo~ 531,959 15.8 70.7 ~         2000
## 6 Colorado ...........................~ Colora~ 65,438 15.8 73.1 2~         2000
\end{verbatim}

The next step is to separate the data and get the correct column from it. We're going to separate based on spaces once it is cleaned up.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{raw_dhs_data <-}\StringTok{ }
\StringTok{  }\NormalTok{raw_dhs_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{str_remove_all}\NormalTok{(data, }\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{*"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{separate}\NormalTok{(data, }\DataTypeTok{into =} \KeywordTok{c}\NormalTok{(}\StringTok{"col_1"}\NormalTok{, }\StringTok{"col_2"}\NormalTok{, }\StringTok{"col_3"}\NormalTok{, }\StringTok{"col_4"}\NormalTok{, }\StringTok{"col_5"}\NormalTok{, }
                          \StringTok{"col_6"}\NormalTok{, }\StringTok{"col_7"}\NormalTok{, }\StringTok{"col_8"}\NormalTok{, }\StringTok{"col_9"}\NormalTok{, }\StringTok{"col_10"}\NormalTok{), }
           \DataTypeTok{sep =} \StringTok{" "}\NormalTok{,}
           \DataTypeTok{remove =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{head}\NormalTok{(raw_dhs_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 14
##   raw_data state data  col_1 col_2 col_3 col_4 col_5 col_6 col_7 col_8 col_9
##   <chr>    <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>
## 1 Alabama~ Alab~ 63,2~ 63,2~ 14.4  65.0  2,02~ 62.9  37.9  97.3  <NA>  <NA> 
## 2 Alaska ~ Alas~ 9,97~ 9,974 16.0  74.6  2,43~ 42.4  23.6  69.4  <NA>  <NA> 
## 3 Arizona~ Ariz~ 85,2~ 85,2~ 17.5  84.4  2,65~ 69.1  41.1  111.3 <NA>  <NA> 
## 4 Arkansa~ Arka~ 37,7~ 37,7~ 14.7  69.1  2,14~ 68.5  36.7  114.1 <NA>  <NA> 
## 5 Califor~ Cali~ 531,~ 531,~ 15.8  70.7  2,18~ 48.5  28.6  75.6  <NA>  <NA> 
## 6 Colorad~ Colo~ 65,4~ 65,4~ 15.8  73.1  2,35~ 49.2  28.6  79.8  <NA>  <NA> 
## # ... with 2 more variables: col_10 <chr>, year_of_data <dbl>
\end{verbatim}

We can now grab the correct column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tfr_data <-}\StringTok{ }
\StringTok{  }\NormalTok{raw_dhs_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{TFR =} \KeywordTok{if_else}\NormalTok{(year_of_data }\OperatorTok{<}\StringTok{ }\DecValTok{2008}\NormalTok{, col_}\DecValTok{4}\NormalTok{, col_}\DecValTok{3}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(state, year_of_data, TFR) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{rename}\NormalTok{(}\DataTypeTok{year =}\NormalTok{ year_of_data)}
\KeywordTok{head}\NormalTok{(tfr_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 3
##   state       year TFR    
##   <chr>      <dbl> <chr>  
## 1 Alabama     2000 2,021.0
## 2 Alaska      2000 2,437.0
## 3 Arizona     2000 2,652.5
## 4 Arkansas    2000 2,140.0
## 5 California  2000 2,186.0
## 6 Colorado    2000 2,356.5
\end{verbatim}

Finally, we need to convert the case.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(tfr_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 3
##   state       year TFR    
##   <chr>      <dbl> <chr>  
## 1 Alabama     2000 2,021.0
## 2 Alaska      2000 2,437.0
## 3 Arizona     2000 2,652.5
## 4 Arkansas    2000 2,140.0
## 5 California  2000 2,186.0
## 6 Colorado    2000 2,356.5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tfr_data <-}\StringTok{ }
\StringTok{  }\NormalTok{tfr_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{TFR =} \KeywordTok{str_remove_all}\NormalTok{(TFR, }\StringTok{","}\NormalTok{),}
         \DataTypeTok{TFR =} \KeywordTok{as.numeric}\NormalTok{(TFR))}

\KeywordTok{head}\NormalTok{(tfr_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 3
##   state       year   TFR
##   <chr>      <dbl> <dbl>
## 1 Alabama     2000 2021 
## 2 Alaska      2000 2437 
## 3 Arizona     2000 2652.
## 4 Arkansas    2000 2140 
## 5 California  2000 2186 
## 6 Colorado    2000 2356.
\end{verbatim}

And run some checks.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# tfr_data %>% }
\CommentTok{#   skimr::skim()}
\end{Highlighting}
\end{Shaded}

In particular we want for there to be 51 states and for there to be 19 years.

And we're done.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(tfr_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 3
##   state       year   TFR
##   <chr>      <dbl> <dbl>
## 1 Alabama     2000 2021 
## 2 Alaska      2000 2437 
## 3 Arizona     2000 2652.
## 4 Arkansas    2000 2140 
## 5 California  2000 2186 
## 6 Colorado    2000 2356.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{write_csv}\NormalTok{(tfr_data, }\StringTok{"outputs/monicas_tfr.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{case-study-kenyan-census-data}{%
\section{Case-study: Kenyan census data}\label{case-study-kenyan-census-data}}


\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-219-1.pdf}

\hypertarget{optical-character-recognition}{%
\section{Optical Character Recognition}\label{optical-character-recognition}}

All of the above is predicated on having a PDF that is already `digitized'. But what if it is images? In that case you need to first use Optical Character Recognition (OCR). The go-to package is Tesseract \citet{citetesseract}.

\textbf{TBD}

\hypertarget{text}{%
\section{Text}\label{text}}

\emph{Aspects of this section have been previously published.}

\hypertarget{introduction-13}{%
\subsection{Introduction}\label{introduction-13}}

Text data is all around us, and in many cases is some of the earliest types of data that we are exposed to. Recent increases in computational power, the development of new methods, and the enormous availability of text, means that there has been a great deal of interest in using text as data. Initial methods tend to focus, essentially, on converting text into numbers and then analysing them using traditional methods. More recent methods have begun to take advantage of the structure that is inherent in text, to draw additional meaning. The difference is perhaps akin to a child who can group similar colors, compared with a child who knows what objects are; although both crocodiles and trees are green, and you can do something with that knowledge, you can do more by knowing that a crocodile could eat you, and a tree probably won't.

In this section we cover a variety of techniques designed to equip you with the basics of using text as data. One of the great things about text data is that it is typically not generated for the purposes of our analysis. That's great because it removes one of the unobservable variables that we typically have to worry about. The trade-off is that we typically have to do a bunch more work to get it into a form that we can work with.

\hypertarget{getting-text-data}{%
\subsection{Getting text data}\label{getting-text-data}}

Text as data is an exciting tool to apply. But many guides assume that you already have a nice dataset. Because we've focused on workflow in these notes, we know that's not likely to be true! In this section we will scrape some text from a website. We've already seen examples of scraping, but in general those were focused on exploiting tables in the website. Here we're going to instead focus on paragraphs of text, hence we'll focus on different html/css tags.

We're going to us the \texttt{rvest} package to make it easier to scrape data. We're also going to use the \texttt{purrr} package to apply a function to a bunch of different URLs. For those of you with a little bit of programming, this is an alternative to using a for loop. For those of you with a bit of CS, this is a package that adds functional programming to R.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rvest)}
\KeywordTok{library}\NormalTok{(tidyverse)}

\CommentTok{# Some websites}
\NormalTok{address_to_visit <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"https://www.rba.gov.au/monetary-policy/rba-board-minutes/2020/2020-03-03.html"}\NormalTok{,}
                    \StringTok{"https://www.rba.gov.au/monetary-policy/rba-board-minutes/2020/2020-02-04.html"}\NormalTok{,}
                    \StringTok{"https://www.rba.gov.au/monetary-policy/rba-board-minutes/2019/2019-12-03.html"}\NormalTok{,}
                    \StringTok{"https://www.rba.gov.au/monetary-policy/rba-board-minutes/2019/2019-11-05.html"}\NormalTok{,}
                    \StringTok{"https://www.rba.gov.au/monetary-policy/rba-board-minutes/2019/2019-10-01.html"}\NormalTok{,}
                    \StringTok{"https://www.rba.gov.au/monetary-policy/rba-board-minutes/2019/2019-09-03.html"}
\NormalTok{                    )}

\CommentTok{# Save names}
\NormalTok{save_name <-}\StringTok{ }\NormalTok{address_to_visit }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{str_remove}\NormalTok{(}\StringTok{"https://www.rba.gov.au/monetary-policy/rba-board-minutes/"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{str_remove}\NormalTok{(}\StringTok{".html"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{str_remove}\NormalTok{(}\StringTok{"20[:digit:]\{2\}/"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{str_c}\NormalTok{(}\StringTok{"inputs/rba/"}\NormalTok{, ., }\StringTok{".csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Create the function that will visit address\_to\_visit and save to save\_name files.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{visit_address_and_save_content <-}
\StringTok{  }\ControlFlowTok{function}\NormalTok{(name_of_address_to_visit,}
\NormalTok{           name_of_file_to_save_as) \{}
    \CommentTok{# The function takes two inputs}
\NormalTok{    name_of_address_to_visit <-}\StringTok{ }\NormalTok{address_to_visit[}\DecValTok{1}\NormalTok{]}
\NormalTok{    name_of_file_to_save_as <-}\StringTok{ }\NormalTok{save_name[}\DecValTok{1}\NormalTok{]}
    
    \KeywordTok{read_html}\NormalTok{(name_of_address_to_visit) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# Go to the website and read the html}
\StringTok{      }\KeywordTok{html_node}\NormalTok{(}\StringTok{"#content"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# Find the content part}
\StringTok{      }\KeywordTok{html_text}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\CommentTok{# Extract the text of the content part}
\StringTok{      }\KeywordTok{write_lines}\NormalTok{(name_of_file_to_save_as) }\CommentTok{# Save as a text file}
    \KeywordTok{print}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{"Done with"}\NormalTok{, name_of_address_to_visit, }\StringTok{"at"}\NormalTok{, }\KeywordTok{Sys.time}\NormalTok{()))  }
    \CommentTok{# Helpful so that you know progress when running it on all the records}
    \KeywordTok{Sys.sleep}\NormalTok{(}\KeywordTok{sample}\NormalTok{(}\DecValTok{30}\OperatorTok{:}\DecValTok{60}\NormalTok{, }\DecValTok{1}\NormalTok{)) }\CommentTok{# Space out each request by somewhere between }
    \CommentTok{# 30 and 60 seconds each so that we don't overwhelm their server}
\NormalTok{  \}}

\CommentTok{# If there is an error then ignore it and move to the next one}
\NormalTok{visit_address_and_save_content <-}
\StringTok{  }\KeywordTok{safely}\NormalTok{(visit_address_and_save_content)}
\end{Highlighting}
\end{Shaded}

We now apply that function to our list of URLs.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Walk through the addresses and apply the function to each}
\KeywordTok{walk2}\NormalTok{(address_to_visit,}
\NormalTok{      save_name,}
      \OperatorTok{~}\StringTok{ }\KeywordTok{visit_address_and_save_content}\NormalTok{(.x, .y))}
\end{Highlighting}
\end{Shaded}

The result is a bunch of files with saved text data.

In this case we used scraping, but there are, of course, many ways. We may be able to use APIs, for instance, In the case of the Airbnb dataset that we examined earlier in the notes. If you are lucky then it may simply be that there is a column that contains text data in your dataset.

\hypertarget{preparing-text-datasets}{%
\subsection{Preparing text datasets}\label{preparing-text-datasets}}

\emph{This section draws on Sharla Gelfand's blog post, linked in the required readings.}

As much as I would like to stick with Australian economics and politics examples, I realise that this is probably only of limited interest to most of you. As such, in this section we will consider a dataset of Sephora reviews. Please read Sharla's blog post (\url{https://sharla.party/post/crying-sephora/}) for another take on this dataset.

In this section we assume that there is some text data that you have gathered. At this point we need to change it into a form that we can work with. For some applications this will be counts of words. For others it may be some variant of this. The dataset that we are going to use is from Sephora, was scraped by \href{https://twitter.com/crabbage_/}{Connie} and I originally became aware of it because of \href{https://sharla.party/post/crying-sephora/}{Sharla}.

First let's read in the data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# This code is taken from https://sharla.party/post/crying-sephora/}
\KeywordTok{library}\NormalTok{(dplyr)}
\KeywordTok{library}\NormalTok{(jsonlite)}
\KeywordTok{library}\NormalTok{(tidytext)}

\NormalTok{crying <-}\StringTok{ }\KeywordTok{fromJSON}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/everestpipkin/datagardens/master/students/khanniie/5_newDataSet/crying_dataset.json"}\NormalTok{,}
  \DataTypeTok{simplifyDataFrame =} \OtherTok{TRUE}
\NormalTok{)}

\NormalTok{crying <-}\StringTok{ }\KeywordTok{as_tibble}\NormalTok{(crying[[}\StringTok{"reviews"}\NormalTok{]])}

\KeywordTok{head}\NormalTok{(crying)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 6
##   date  product_info$br~ $name $type $url  review_body review_title stars
##   <chr> <chr>            <chr> <chr> <chr> <chr>       <chr>        <chr>
## 1 29 M~ Too Faced        Bett~ Masc~ http~ "Now I can~ AWESOME      5 st~
## 2 29 S~ Too Faced        Bett~ Masc~ http~ "This hold~ if you're s~ 5 st~
## 3 23 M~ Too Faced        Bett~ Masc~ http~ "I just bo~ Hate it      1 st~
## 4 15 A~ Too Faced        Bett~ Masc~ http~ "To start ~ Nearly perf~ 5 st~
## 5 21 S~ Too Faced        Bett~ Masc~ http~ "This masc~ Amazing!!    5 st~
## 6 30 M~ Too Faced        Bett~ Masc~ http~ "Let's tal~ Tricky but ~ 5 st~
## # ... with 1 more variable: userid <dbl>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(crying)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "date"         "product_info" "review_body"  "review_title" "stars"       
## [6] "userid"
\end{verbatim}

We'll focus on the \texttt{review\_body} variable and the number of stars \texttt{stars} that the reviewer gave. Most of them are 5 stars, so we'll just focus on whether or not the review is five stars.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{crying <-}\StringTok{ }
\StringTok{  }\NormalTok{crying }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(review_body, stars) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{stars =} \KeywordTok{str_remove}\NormalTok{(stars, }\StringTok{" stars?"}\NormalTok{),  }\CommentTok{# The question mark at the end means it'l get rid of 'star' and 'stars'.}
         \DataTypeTok{stars =} \KeywordTok{as.integer}\NormalTok{(stars)}
\NormalTok{         ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{five_stars =} \KeywordTok{if_else}\NormalTok{(stars }\OperatorTok{==}\StringTok{ }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))}

\KeywordTok{table}\NormalTok{(crying}\OperatorTok{$}\NormalTok{stars)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  1  2  3  4  5 
##  6  2  4 14 79
\end{verbatim}

In this example we are going to split everything into separate words. When we do this it is just searching for a space, and so what other types of elements are going to be considered `words'?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{crying_by_words <-}\StringTok{ }
\StringTok{  }\NormalTok{crying }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{unnest_tokens}\NormalTok{(word, review_body, }\DataTypeTok{token =} \StringTok{"words"}\NormalTok{)}

\KeywordTok{head}\NormalTok{(crying_by_words)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 3
##   stars five_stars word 
##   <int>      <dbl> <chr>
## 1     5          1 now  
## 2     5          1 i    
## 3     5          1 can  
## 4     5          1 cry  
## 5     5          1 all  
## 6     5          1 i
\end{verbatim}

We now want to count the number of times each word is used by each of the star classifications.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{crying_by_words <-}\StringTok{ }
\StringTok{  }\NormalTok{crying_by_words }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(stars, word, }\DataTypeTok{sort =} \OtherTok{TRUE}\NormalTok{)}

\KeywordTok{head}\NormalTok{(crying_by_words)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 3
##   stars word      n
##   <int> <chr> <int>
## 1     5 i       348
## 2     5 and     249
## 3     5 the     239
## 4     5 it      211
## 5     5 a       193
## 6     5 this    178
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{crying_by_words }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(stars }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 3
##   stars word      n
##   <int> <chr> <int>
## 1     1 the      39
## 2     1 i        24
## 3     1 and      21
## 4     1 it       21
## 5     1 to       19
## 6     1 my       16
\end{verbatim}

So you can see that the most popular word for five star reviews is `i', and that the most popular word for one star reviews is `the'.

At this point, we can use the data to do a whole bunch of different things, but one nice measure to look at is term frequency e.g.~in this case how many times is a word used in reviews with a particular star rating. The issue is that there are a lot of words that are commonly used regardless of context. As such, we may also like to look at the inverse document frequency in which we `penalise' words that occur in many particular star ratings. For instance, `the' probably occurs in both one star and five star reviews and so its idf is lower than `hate' which probably only occurs in one star reviews. The term frequency--inverse document frequency (tf-idf) is then the product of these.

We can create this value using the \texttt{bind\_tf\_idf()} function from the \texttt{tidytext} package, and this will create a bunch of new columns, one for each word and star combination.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# This code, and the one in the next block, is from Julia Silge: https://juliasilge.com/blog/sherlock-holmes-stm/}
\NormalTok{crying_by_words_tf_idf <-}\StringTok{ }
\StringTok{  }\NormalTok{crying_by_words }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{bind_tf_idf}\NormalTok{(word, stars, n) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\OperatorTok{-}\NormalTok{tf_idf)}

\KeywordTok{head}\NormalTok{(crying_by_words_tf_idf)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 6
##   stars word              n      tf   idf tf_idf
##   <int> <chr>         <int>   <dbl> <dbl>  <dbl>
## 1     2 below             1 0.00826  1.61 0.0133
## 2     2 boy               1 0.00826  1.61 0.0133
## 3     2 choice            1 0.00826  1.61 0.0133
## 4     2 contrary          1 0.00826  1.61 0.0133
## 5     2 exceptionally     1 0.00826  1.61 0.0133
## 6     2 migrates          1 0.00826  1.61 0.0133
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{crying_by_words_tf_idf }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(stars) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{top_n}\NormalTok{(}\DecValTok{10}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\NormalTok{ungroup }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{word =} \KeywordTok{reorder_within}\NormalTok{(word, tf_idf, stars)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{stars =} \KeywordTok{as_factor}\NormalTok{(stars)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(stars }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(word, tf_idf, }\DataTypeTok{fill =}\NormalTok{ stars)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_col}\NormalTok{(}\DataTypeTok{show.legend =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{facet_wrap}\NormalTok{(}\KeywordTok{vars}\NormalTok{(stars), }\DataTypeTok{scales =} \StringTok{"free"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{scale_x_reordered}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Word"}\NormalTok{, }
         \DataTypeTok{y =} \StringTok{"tf-idf"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-229-1.pdf}

\hypertarget{hunting-data}{%
\chapter{Hunting data}\label{hunting-data}}

\textbf{Required reading}

\begin{itemize}
\tightlist
\item
  Banerjee, Abhijit Vinayak, 2020, `Field Experiments and the Practice of Economics', \emph{American Economic Review}, Vol. 110, No.~7, pp.~1937-1951.
\item
  Duflo, Esther, 2020, `Field Experiments and the Practice of Policy', \emph{American Economic Review}, Vol. 110, No.~7, pp.~1952-1973 (or watch the speech detailed below).
\item
  Fisher, Ronald, 1935, `The Design of Experiments', pp.~20-29, \url{https://archive.org/details/in.ernet.dli.2015.502684/page/n33/mode/2up}.
\item
  Fry, Hanna, 2020, `Experiments on Trial', \emph{The New Yorker}, 2 March, pp.~61-65, \url{https://www.newyorker.com/magazine/2020/03/02/big-tech-is-testing-you}.
\item
  Gertler, Paul, Sebastian Martinez, Patrick Premand, Laura Rawlings, and Christel Vermeersch, \emph{Impact Evaluation in Practice}, Chapters 3 and 4, \url{https://www.worldbank.org/en/programs/sief-trust-fund/publication/impact-evaluation-in-practice}.
\item
  Hill, Austin Bradford, 1965, `The Environment and Disease: Association or Causation?', \emph{Proceedings of the Royal Society of Medicine}, 58, 5, 295-300.
\item
  Kohavi, Ron and Stefan Thomke, 2017, `The Surprising Power of Online Experiments', \emph{Harvard Business Review}, September-October, \url{https://hbr.org/2017/09/the-surprising-power-of-online-experiments}.
\item
  Kohavi, Ron, Diane Tang, and Ya Xu, 2020, \emph{Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing}, Cambridge University Press. (This sounds like a lot, but it's a light book - it's more about providing examples of issues to think about.) (Freely available through the U of T library.)
\item
  Taback, Nathan, 2020, \emph{Design of Experiments and Observational Studies}, Chapter 8 - Completely Randomized Designs: Comparing More Than Two Treatments, \url{https://scidesign.github.io/designbook/completely-randomized-designs-comparing-more-than-two-treatments.html}.
\item
  Taylor, Sean, Dean Eckles, 2017, `Randomized experiments to detect and estimate social influence in networks', \emph{arXiv}, \url{https://arxiv.org/abs/1709.09636v1}.
\item
  Wu, Changbao and Mary E. Thompson, 2020, \emph{Sampling Theory and Practice}, Springer, Chapters 1-3, and 5 (freely available through the U of T library).
\end{itemize}

\textbf{Required viewing}

\begin{itemize}
\tightlist
\item
  Duflo, Esther, 2020, `Inteview with Esther Duflo', 12 October, \emph{Online Causal Inference Seminar}, \url{https://youtu.be/WWW9q3oMYxU}.
\item
  Duflo, Esther, 2019, `Nobel Prize Lecture', 8 December 2019, Stockholm: \url{https://www.nobelprize.org/prizes/economic-sciences/2019/duflo/lecture/}.
\item
  Register, Yim, 2020, `Introduction to Sampling and Randomization', \emph{Online Causal Inference Seminar}, 14 November, \url{https://youtu.be/U272FFxG8LE}.
\item
  Tipton, Elizabeth, 2020, `Will this Intervention Work in this Population? Designing Randomized Trials for Generalization', \emph{Online Causal Inference Seminar}, 14 April, \url{https://youtu.be/HYP32wzEZMA}.
\item
  Xu, Ya, 2020, `Causal inference challenges in industry, a perspective from experiences at LinkedIn', \emph{Online Causal Inference Seminar}, 16 July, \url{https://youtu.be/OoKsLAvyIYA}.
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
  Angrist, Joshua D., and Jörn-Steffen Pischke, 2008, \emph{Mostly harmless econometrics: An empiricist's companion}, Princeton University Press, Chapter 2.
\item
  Banerjee, Abhijit Vinayak, Esther Duflo, Rachel Glennerster, and Dhruva Kothari, 2010, `Improving immunisation coverage in rural India: clustered randomised controlled evaluation of immunisation campaigns with and without incentives', \emph{BMJ}, 340, c2220.
\item
  Beaumont, Jean-François, 2020, `Are probability surveys bound to disappear for the production of official statistics?', \emph{Survey Methodology}, 46 (1), Statistics Canada, Catalogue No.~12-001-X.
\item
  Christian, Brian, 2012, `The A/B Test: Inside the Technology That's Changing the Rules of Business', \emph{Wired}, 25 April, \url{https://www.wired.com/2012/04/ff-abtesting/}.
\item
  Dablander, Fabian, 2020, ``An Introduction to Causal Inference'', \emph{PsyArXiv}, 13 February, \url{doi:10.31234/osf.io/b3fkw}, \url{https://psyarxiv.com/b3fkw}.
\item
  Deaton, Angus, 2010, `Instruments, Randomization, and Learning about Development', \emph{Journal of Economic Literature}, vol.~48, no. 2, pp.~424-455.
\item
  Duflo, Esther, Rachel Glennerster, and Michael Kremer, 2007, `Using Randomization In Development Economics Research: A Toolkit', \url{https://economics.mit.edu/files/806}.
\item
  Gordon, Brett R., Florian Zettelmeyer, Neha Bhargava, and Dan Chapsky, 2019, `A Comparison of Approaches to Advertising Measurement: Evidence from Big Field Experiments at Facebook', \emph{Marketing Science}, Vol. 38, No.~2, March--April, pp.~193--225.
\item
  Groves, Robert M., 2011, `Three Eras of Survey Research', \emph{Public Opinion Quarterly}, 75 (5), pp.~861--871, \url{https://doi.org/10.1093/poq/nfr057}.
\item
  Hillygus, D. Sunshine, 2011, `The evolution of election polling in the United States', \emph{Public Opinion Quarterly}, 75 (5), pp.~962-981.
\item
  Imai, Kosuke, 2017, \emph{Quantitative Social Science: An Introduction}, Princeton University Press, Ch 2.3, 2.4, 4.3.
\item
  Jeffries, Adrianne, Leon Yin, and Surya Mattu, 2020, `Swinging the Vote?', \emph{The Markup}, 26 February, \url{https://themarkup.org/google-the-giant/2020/02/26/wheres-my-email}.
\item
  Kohavi, Ron, Alex Deng, Brian Frasca, Roger Longbotham, Toby Walker, and Ya Xu. 2012. Trustworthy online controlled experiments: five puzzling outcomes explained. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD '12). Association for Computing Machinery, New York, NY, USA, 786--794. \url{DOI:https://doi.org/10.1145/2339530.2339653}
\item
  Landesberg, Eddie, Molly Davies, and Stephanie Yee, 2019, `Want to make good business decisions? Learn causality', \emph{MultiThreaded, Stitchfix blog}, 19 December, \url{https://multithreaded.stitchfix.com/blog/2019/12/19/good-marketing-decisions/}.
\item
  Levay, Kevin E., Jeremy Freese, and James N. Druckman, 2016, `The demographic and political composition of Mechanical Turk samples', \emph{Sage Open}, 6 (1), 2158244016636433.
\item
  Lewis, Randall A., and David H. Reiley, 2014 `Online ads and offline sales: Measuring the effects of retail advertising via a controlled experiment on Yahoo!', \emph{Quantitative Marketing and Economics}, Vol 12, pp.~235--266.
\item
  Mullinix, Kevin J., Leeper, Thomas J., Druckman, James N. and Freese, Jeremy, 2015, `The generalizability of survey experiments', \emph{Journal of Experimental Political Science}, 2 (2), pp.~109-138.
\item
  Novak, Greg, Sven Schmit, and Dave Spiegel, 2020, Experimentation with resource constraints, 18 November, StitchFix Blog, \url{https://multithreaded.stitchfix.com/blog/2020/11/18/virtual-warehouse/}.
\item
  Prepared for the AAPOR Executive Council by a Task Force operating under the auspices of the AAPOR Standards Committee, with members including:, Reg Baker, Stephen J. Blumberg, J. Michael Brick, Mick P. Couper, Melanie Courtright, J. Michael Dennis, Don Dillman, Martin R. Frankel, Philip Garland, Robert M. Groves, Courtney Kennedy, Jon Krosnick, Paul J. Lavrakas, Sunghee Lee, Michael Link, Linda Piekarski, Kumar Rao, Randall K. Thomas, Dan Zahs, 2010, `Research Synthesis: AAPOR Report on Online Panels', \emph{Public Opinion Quarterly}, 74 (4), pp.~711--781, \url{https://doi.org/10.1093/poq/nfq048}.
\item
  Ryan, A. C., A. R. MacKenzie, S. Watkins, and R. Timmis, 2012, `World War II contrails: a case study of aviation‐induced cloudiness', \emph{International journal of climatology}, 32, no. 11, pp.~1745-1753.
\item
  Said, Chris, 2020, `Optimizing sample sizes in A/B testing, Part I: General summary', 10 January, \url{https://chris-said.io/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-I/}. (See also parts 2 and 3).
\item
  Stolberg, Michael, 2006, `Inventing the randomized double-blind trial: the Nuremberg salt test of 1835', \emph{Journal of the Royal Society of Medicine}, 99, no. 12, pp.~642-643.
\item
  Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel, 2019, popular science background, \url{https://www.nobelprize.org/uploads/2019/10/popular-economicsciencesprize2019-2.pdf}.
\item
  Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel, 2019, scientific background, \url{https://www.nobelprize.org/uploads/2019/10/advanced-economicsciencesprize2019.pdf}.
\item
  Taddy, Matt, 2019, \emph{Business Data Science}, Chapter 5.
\item
  Urban, Steve, Rangarajan Sreenivasan, and Vineet Kannan, 2016, `It's All A/Bout Testing: The Netflix Experimentation Platform', \emph{Netflix Technology Blog}, 29 April, \url{https://netflixtechblog.com/its-all-a-bout-testing-the-netflix-experimentation-platform-4e1ca458c15}.
\item
  VWO, `A/B Testing Guide', \url{https://vwo.com/ab-testing/}.
\item
  Yeager, David S., Jon A. Krosnick, LinChiat Chang, Harold S. Javitz, Matthew S. Levendusky, Alberto Simpser, Rui Wang, 2011, `Comparing the Accuracy of RDD Telephone Surveys and Internet Surveys Conducted with Probability and Non-Probability Samples', \emph{Public Opinion Quarterly}, 75 (4), pp.~709--747, \url{https://doi.org/10.1093/poq/nfr020}.
\item
  Yin, Xuan and Ercan Yildiz, 2020, `The Causal Analysis of Cannibalization in Online Products', \emph{Code as Craft, Etsy blog}, 24 February, \url{https://codeascraft.com/2020/02/24/the-causal-analysis-of-cannibalization-in-online-products/}.
\end{itemize}

\textbf{Recommended listening}

\begin{itemize}
\tightlist
\item
  Galef, Julia, 2020, `Episode 246: Deaths of despair / Effective altruism (Angus Deaton)', \emph{Rationally Speaking}, from 35:30 through to the end, available at: \url{http://rationallyspeakingpodcast.org/show/episode-246-deaths-of-despair-effective-altruism-angus-deato.html}.
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\tightlist
\item
  Treatment and control groups.
\item
  Internal and external validity.
\item
  Average treatment effect.
\item
  Generating simulated datasets.
\item
  Defining populations, frames and samples.
\item
  Distinguishing probability and non-probability sampling
\item
  Distinguishing strata and clusters.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{broom}
\item
  \texttt{ggplot2}
\item
  \texttt{tidyverse}
\end{itemize}

\textbf{Key functions/etc}

\begin{itemize}
\tightlist
\item
  \texttt{aov()}
\item
  \texttt{rnorm()}
\item
  \texttt{sample()}
\item
  \texttt{t.test()}
\end{itemize}

\textbf{Quiz}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In your own words, what is the role of randomisation in constructing a counterfactual (write two or three paragraphs)?
\item
  What is external validity (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Findings from an experiment hold in that setting.
  \item
    Findings from an experiment hold outside that setting.
  \item
    Findings from an experiment that has been repeated many times.
  \item
    Findings from an experiment for which code and data are available.
  \end{enumerate}
\item
  What is internal validity (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Findings from an experiment hold in that setting.
  \item
    Findings from an experiment hold outside that setting.
  \item
    Findings from an experiment that has been repeated many times.
  \item
    Findings from an experiment for which code and data are available.
  \end{enumerate}
\item
  If we have a dataset named `netflix\_data', with the columns `person' and `tv\_show' and `hours', (person is a character class uniqueID for every person, tv\_show is a character class name of a tv show, and hours is double expressing the number of hours that person watched that tv show). Could you please write some code that would randomly assign people into one of two groups? The data looks like this:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\NormalTok{netflix_data <-}\StringTok{ }
\StringTok{  }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{person =} \KeywordTok{c}\NormalTok{(}\StringTok{"Rohan"}\NormalTok{, }\StringTok{"Rohan"}\NormalTok{, }\StringTok{"Monica"}\NormalTok{, }\StringTok{"Monica"}\NormalTok{, }\StringTok{"Monica"}\NormalTok{, }
                    \StringTok{"Patricia"}\NormalTok{, }\StringTok{"Patricia"}\NormalTok{, }\StringTok{"Helen"}\NormalTok{),}
         \DataTypeTok{tv_show =} \KeywordTok{c}\NormalTok{(}\StringTok{"Broadchurch"}\NormalTok{, }\StringTok{"Duty-Shame"}\NormalTok{, }\StringTok{"Broadchurch"}\NormalTok{, }\StringTok{"Duty-Shame"}\NormalTok{, }
                     \StringTok{"Shetland"}\NormalTok{, }\StringTok{"Broadchurch"}\NormalTok{, }\StringTok{"Shetland"}\NormalTok{, }\StringTok{"Duty-Shame"}\NormalTok{),}
         \DataTypeTok{hours =} \KeywordTok{c}\NormalTok{(}\FloatTok{6.8}\NormalTok{, }\FloatTok{8.0}\NormalTok{, }\FloatTok{0.8}\NormalTok{, }\FloatTok{9.2}\NormalTok{, }\FloatTok{3.2}\NormalTok{, }\FloatTok{4.0}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{10.2}\NormalTok{)}
\NormalTok{         )}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  In the context of randomisation, what does stratification mean to you (write a paragraph or two)?
\item
  How could you check that your randomisation had been done appropriately (write two or three paragraphs)?
\item
  Identify three companies that conduct A/B testing commercially and write a short paper about how they work and the trade-offs of each. Are there any notable Toronto-based or Canadian companies? Why do you think this might be the case?
\item
  Pretend that you work as a junior analyst for a large consulting firm. Further, pretend that your consulting firm has taken a contract to put together a facial recognition model for the Canada Border Services Agency's Inland Enforcement branch. Taking a page or two, please discuss your thoughts on this matter. What would you do and why?
\item
  What are some types of probability sampling, and in what circumstances might you want to implement them (write two or three pages)?
\item
  There have been some substantial political polling `misses' in recent years (Trump and Brexit come to mind). To what extent do you think non-response bias was the cause of this (write a page or two, being sure to ground your writing with citations)?
\item
  What is an estimate (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A rule for calculating an estimate of a given quantity based on observed data.
  \item
    The quantity of interest.
  \item
    The result.
  \item
    Unknown numbers that determine a statistical model.
  \end{enumerate}
\item
  What is an estimator (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A rule for calculating an estimate of a given quantity based on observed data.
  \item
    The quantity of interest.
  \item
    The result.
  \item
    Unknown numbers that determine a statistical model.
  \end{enumerate}
\item
  What is an estimand (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A rule for calculating an estimate of a given quantity based on observed data.
  \item
    The quantity of interest.
  \item
    The result.
  \item
    Unknown numbers that determine a statistical model.
  \end{enumerate}
\item
  What is a parameter (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A rule for calculating an estimate of a given quantity based on observed data.
  \item
    The quantity of interest.
  \item
    The result.
  \item
    Unknown numbers that determine a statistical model.
  \end{enumerate}
\item
  It seems like a lot of businesses have closed in downtown Toronto since the pandemic. To investigate this, I decide to walk along some blocks downtown and count the number of businesses that are closed and open. To decide which blocks to walk, I open a map of Toronto, start at the lake, and then pick every 10th street. This type of sampling is (select all)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Cluster sampling.
  \item
    Systematic sampling.
  \item
    Stratified sampling.
  \item
    Simple random sampling.
  \item
    Convenience sampling.
  \end{enumerate}
\item
  Please name some reasons why you may wish to use cluster sampling (select all)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Balance in responses.
  \item
    Administrative convenience.
  \item
    Efficiency in terms of money.
  \item
    Underlying systematic concerns.
  \item
    Estimation of sub-populations.
  \end{enumerate}
\item
  Please consider Beaumont, 2020, `Are probability surveys bound to disappear for the production of official statistics?'. With reference to that paper, do you think that probability surveys will disappear, and why or why not (please write a paragraph or two)?
\end{enumerate}

\hypertarget{experiments-and-randomised-controlled-trials}{%
\section{Experiments and randomised controlled trials}\label{experiments-and-randomised-controlled-trials}}

\hypertarget{introduction-14}{%
\subsection{Introduction}\label{introduction-14}}

\emph{First a note on Ronald Fisher and Francis Galton. Fisher and Galton are the intellectual grandfathers of much of the work that we cover. In some cases it is directly their work, in other cases it is work that built on their contributions. Both of these men believed in eugenics, amongst other things that are generally reprehensible.}

This chapter is about experiments. This is a situation in which we can explicitly control and vary some aspects. The advantage of this is that identification should be clear. There is a treatment group that is treated and a control group that is not. These are randomly split. And so if they end up different then it must be because of the treatment. Unfortunately, life is rarely so smooth. Arguing about how similar the treatment and control groups were tends to carry on indefinitely, because our ability to speak to internal validity affects our ability to speak to external validity.

It's also important to note that the statistics of this were designed in agricultural settings `does fertilizer work?', etc. In those settings you can more easily divide a field into `treated' and `non-treated', and the magnitude of the effect is large. In general, these same statistical approaches are still used today (especially in the social sciences) but often inappropriately. If you hear someone talking about power to identify effects and similar terms, then it's \emph{not} necessarily that they're \emph{not} right, but it usually pays to take a step back and really think about what is being done and whether they really know what they're doing.

\begin{quote}
Never forget: if your sampling is in any way non-representative, your observe{[}d{]} data is not sufficient for population estimates. You \emph{must} deal with design, sampling issues, data quality, and misclassification. Otherwise you'll just be wrong.

Dan Simpson, \href{https://twitter.com/dan_p_simpson/status/1222924987667046400}{30 January 2020}.
\end{quote}

When Monica and I moved to San Francisco, the Giants immediately won the baseball, and the Warriors began a historic streak. We moved to Chicago and the Cubs won the baseball for the first time in a hundred years. We then moved to Massachusetts, and the Patriots won the Super Bowl again and again and again. Finally, we moved to Toronto, and the Raptors won the basketball. Should a city pay us to live there or could their funds be better spent elsewhere?

One way to get at the answer would be to run an experiment. Make a list of the North American cities with major sports teams, and then roll a dice and send us to live there for a year. If we had enough lifetimes, then we could work it out. The fundamental issue is that we cannot both live in a city and not live in a city. Experiments and randomised controlled trials are circumstances in which we try to randomly allocate some treatment, so as to have a belief that everything else was constant (or at least ignorable).

In the words of \citet[p.~3]{hernanrobins2020} an action, \(A\), is also known `as an intervention, an exposure, or a treatment.' I'll typically use `treated/control' language, reflecting whether an action was imposed or not. That treatment random variable will typically be binary, that is 0 or 1, `treated' or `not treated/control/comparison'. We'll then typically have some outcome random variable, \(Y\), which will typically be binary or continuous. An example of a binary outcome could vote choice - `Liberal' vs `not liberal' - noticing there that I grouped all the other parties into simply `not liberal' to force the binary outcome.

Further following \citet[p.~4]{hernanrobins2020}, but in the notation of \citet[p.~48]{gertler2016impact} we describe a treatment as `causal' when \((Y|a=0)\neq (Y|a=1)\). As discussed above, the fundamental problem of casual inference is that we cannot both treat and control the one individual. So when we want to know the effect of the treatment, we need to compare it with the counterfactual, which is what would have happened if the individual were not treated. So causal inference turns out to be fundamentally a missing data problem.\footnote{There's a joke in statistics, okay, well, TBH, I have a joke about statistics, and it's that at some point every professor is like `\ldots{} and so X really just boils down to a missing data problem' and it's funny because, that's kind of the fundamental issue of statistics, we'd not really need the science if we had all the data. In hindsight, this is not really a joke.}

To quote from \citet[p.48]{gertler2016impact}, in the context of evaluating income in response to an intervention program:

\begin{quote}
To put it another way, we would like to measure income at the same point in time for the same unit of observa- tion (a person, in this case), but in two different states of the world. If it were possible to do this, we would be observing how much income the same individual would have had at the same point in time both with and without the program, so that the only possible explanation for any difference in that person's income would be the program. By comparing the same individual with herself at the same moment, we would have managed to eliminate any outside factors that might also have explained the difference in outcomes. We could then be confident that the relationship between the vocational training program and the change in income is causal\ldots{} {[}A{]} unit either participated in the program or did not participate. The unit cannot be observed simultaneously in two different states (in other words, with and without the program).
\end{quote}

As we cannot compared treatment and control in one particular individual, we instead compare the average of two groups - all those treated and all those not. We are looking to estimate the counterfactual. We usually consider a default that there's no effect. As we're interested in what is happening in groups, we turn to expectations, and notions of probability to express ourselves. Hence, we'll make claims that talk, on average. Maybe wearing fun socks really does make you have a lucky day, but on average across the population, it's probably not the case.\footnote{As someone who oddly is somewhat superstitious, believes fully in the irony gods, and does have a pair of lucky, fun, socks, this example was not randomly chosen.}

It's worth pointing out that we don't just have to be interested in the average effect. We may consider the median, or variance, or whatever. Nonetheless, if we were interested in the average effect, then one way to proceed would be to divide the dataset into two - treated and not treated - have an effect column of 0s and 1s, sum the column and divide it by the length of the column, and then look at the ratio. This would be an estimator, which is a way of estimating something of interest. The estimand is the thing of interest, in this case the average effect, and the estimate is whatever our number is. To give another example, following \citet{gelmanhillvehtari2020}:

\begin{quote}
An estimand, or quantity of interest, is some summary of parameters or data that somebody is interested in estimating. For example, in the regression model, \(y = a + bx + \epsilon\), the parameters \(a\) and \(b\) might be of interest\ldots. We use the data to construct estimates of parameters and other quantities of interest.
\end{quote}

More broadly, \citet{Cunningham2021} defines causal inference as `\ldots the leveraging of theory and deep knowledge of institutional details to estimate the impact of events and choices on a given outcome of interest.' In the previous chapter we discussed gathering data which we observed about the world. In this chapter we are going to be more active. \citet{Cunningham2021} says about this experimental data that it `is collected in something akin to a laboratory environment. In a traditional experiment, the researcher participates actively in the process being recorded.' We have to go out and hunt our data, if you like.

\hypertarget{randomised-sampling}{%
\subsection{Randomised sampling}\label{randomised-sampling}}

Correlation can be enough in some settings, but in order to be able to make forecasts when things change and the circumstances are slightly different we need to understand causation. The key is the counterfactual - what would have happened in the absence of the treatment. Ideally we could keep everything else constant, randomly divide the world into two groups, and then treat one and not the other. Then we can be pretty confident that any difference between the two groups is due to that treatment. The reason for this is that if we have some population and we randomly select two groups from it, then our two groups (so long as they are both big enough) should have the same characteristics as the population. Randomised controlled trials (RCTs) and A/B testing attempts to get us as close to this `gold standard' as we can hope. (RCTs are often described as the `gold standard', for instance by \citet{athey2017state}. In doing so, we're not saying that RCTs are perfect, just that they're generally better than most of the other options. There is plenty that is wrong with RCTs.)

Remember that our challenge is: \citep[p.51-52]{gertler2016impact}:

\begin{quote}
\ldots to identify a treatment group and a comparison group that are statistically identical, on average, in the absence of the program. If the two groups are identical, with the sole exception that one group participates in the program and the other does not, then we can be sure that any difference in outcomes must be due to the program. Finding such comparison groups is the crux of any impact evaluation, regardless of what type of program is being evaluated. Simply put, without a comparison group that yields an accurate estimate of the counterfactual, the true impact of a program cannot be established.
\end{quote}

We might be worried about underlying trends (the issues with before/after comparison), or selection bias (the issue with self-selection), either of which would result in biased estimators. Our solution is randomisation.

To get started, let's generate a simulated dataset and then sample from it. In general, this is a good way to approach problems:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  generate a simulated dataset;
\item
  do your analysis on the simulated dataset; and
\item
  take your analysis to the real dataset.
\end{enumerate}

The reason this is a good approach is that you know roughly what the outcomes should be in step 2, whereas if you go directly to the real dataset then you don't know if unexpected outcomes are likely due to your own analysis errors, or actual results. The first time you generate a simulated dataset it will take a while, but after a bit of practice you'll get good at it. There are also packages that can help, including \texttt{DeclareDesign} \citep{citedeclaredesign} and \texttt{survey} \citep{citesurvey}. Another good reason it's useful to take this approach of simulation is that when you're working in teams the analysis can get started before the data collection and cleaning is completed. That simulation will also help the collection and cleaning team think about tests they should run on their data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}
\CommentTok{# Construct a population so that 25 per cent of people like blue and 75 per cent }
\CommentTok{# like white.}
\NormalTok{population <-}\StringTok{ }
\StringTok{  }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{person =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{10000}\NormalTok{),}
         \DataTypeTok{favourite_color =} \KeywordTok{sample}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\StringTok{"Blue"}\NormalTok{, }\StringTok{"White"}\NormalTok{), }
                                  \DataTypeTok{size  =} \DecValTok{10000}\NormalTok{, }
                                  \DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{,}
                                  \DataTypeTok{prob =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.75}\NormalTok{)),}
         \DataTypeTok{supports_the_leafs =} \KeywordTok{sample}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{), }
                                  \DataTypeTok{size  =} \DecValTok{10000}\NormalTok{, }
                                  \DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{,}
                                  \DataTypeTok{prob =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.80}\NormalTok{, }\FloatTok{0.20}\NormalTok{)),}
\NormalTok{         ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{in_frame =} \KeywordTok{sample}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{1}\NormalTok{),}
                        \DataTypeTok{size  =} \DecValTok{10000}\NormalTok{, }
                        \DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{group =} \KeywordTok{sample}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{),}
                        \DataTypeTok{size  =} \DecValTok{10000}\NormalTok{, }
                        \DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{group =} \KeywordTok{ifelse}\NormalTok{(in_frame }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{, group, }\OtherTok{NA}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The sampling frame is subset of the population that can actually be sampled, for instance they are listed somewhere. For instance, \href{https://jazzystats.com/}{Lauren Kennedy} likes to use the analogy of a city's population, and the phonebook - almost everyone is in there (or at least they used to be), so the population and the sampling frame are almost the same, but they are not.

Now look at the mean for two groups drawn out of the sampling frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{population }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(in_frame }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(group }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(group, favourite_color) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4 x 3
## # Groups:   group, favourite_color [4]
##   group favourite_color     n
##   <int> <chr>           <int>
## 1     1 Blue              114
## 2     1 White             420
## 3     2 Blue              105
## 4     2 White             369
\end{verbatim}

We are probably convinced by looking at it, but to formally test if there is a difference in the two samples, we can use a t-test.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(broom)}

\NormalTok{population <-}\StringTok{ }
\StringTok{  }\NormalTok{population }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{color_as_integer =} \KeywordTok{case_when}\NormalTok{(}
\NormalTok{    favourite_color }\OperatorTok{==}\StringTok{ "White"} \OperatorTok{~}\StringTok{ }\DecValTok{0}\NormalTok{,}
\NormalTok{    favourite_color }\OperatorTok{==}\StringTok{ "Blue"} \OperatorTok{~}\StringTok{ }\DecValTok{1}\NormalTok{,}
    \OtherTok{TRUE} \OperatorTok{~}\StringTok{ }\DecValTok{999}
\NormalTok{  ))}

\NormalTok{group_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }
\StringTok{  }\NormalTok{population }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(group }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(color_as_integer) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{as.vector}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{unlist}\NormalTok{()}

\NormalTok{group_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }
\StringTok{  }\NormalTok{population }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(group }\OperatorTok{==}\StringTok{ }\DecValTok{2}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(color_as_integer) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{unlist}\NormalTok{()}

\KeywordTok{t.test}\NormalTok{(group_}\DecValTok{1}\NormalTok{, group_}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Welch Two Sample t-test
## 
## data:  group_1 and group_2
## t = -0.30825, df = 988.57, p-value = 0.758
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.05919338  0.04312170
## sample estimates:
## mean of x mean of y 
## 0.2134831 0.2215190
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# We could also use the tidy function in the broom package.}
\KeywordTok{tidy}\NormalTok{(}\KeywordTok{t.test}\NormalTok{(group_}\DecValTok{1}\NormalTok{, group_}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 10
##   estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high
##      <dbl>     <dbl>     <dbl>     <dbl>   <dbl>     <dbl>    <dbl>     <dbl>
## 1 -0.00804     0.213     0.222    -0.308   0.758      989.  -0.0592    0.0431
## # ... with 2 more variables: method <chr>, alternative <chr>
\end{verbatim}

If properly done then not only will we get a `representative' share of people with the favourite color blue, but we should also get a representative share of people who support the Maple Leafs. Why should that happen when we haven't randomised on these variables? Let's start by looking at our dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{population }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(in_frame }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(group }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(group, supports_the_leafs) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4 x 3
## # Groups:   group, supports_the_leafs [4]
##   group supports_the_leafs     n
##   <int> <chr>              <int>
## 1     1 No                   102
## 2     1 Yes                  432
## 3     2 No                    81
## 4     2 Yes                  393
\end{verbatim}

This is very exciting. We have a representative share on `unobservables' (in this case we do `observe' them - to illustrate the point - but we didn't select on them). We get this because they were correlated. But it will breakdown in a number of ways that we will discuss. It also assumes large enough groups - if we sampled in Toronto are we likely to get a `representative' share of people who support the Canadiens? What about \href{https://www.fc-hansa.de/}{F.C. Hansa Rostock}? If we want to check that the two groups are the same then what can we do? Exactly what we did above - just check if we can identify a difference between the two groups based on observables (we looked at the mean, but we could look at other aspects as well).

\hypertarget{anova}{%
\subsection{ANOVA}\label{anova}}

Analysis of Variation (ANOVA) was introduced by Fisher while he was working on statistical problems in agriculture. To steal \href{https://darrendahly.github.io}{Darren L Dahly's} `favorite joke of all time' \citep{citeDahly}:

\begin{quote}
Q: ``What's the difference between agricultural and medical research?''

A: ``The former isn't conducted by farmers.''
\end{quote}

We need to cover ANOVA because of its importance historically, but in general you probably shouldn't actually use ANOVA day-to-day. There's nothing wrong with it, in the right circumstances, it's more just that it is a hundred years old and the number of modern use-case where it's still your best-bet is pretty small. In any case, typically, the null is that all of the groups are from the same distribution.

We can run ANOVA with the function built into R - \texttt{aov()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{just_two_groups <-}\StringTok{ }\NormalTok{population }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(in_frame }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(group }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}

\KeywordTok{aov}\NormalTok{(group }\OperatorTok{~}\StringTok{ }\NormalTok{favourite_color, }
    \DataTypeTok{data =}\NormalTok{ just_two_groups) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{tidy}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 6
##   term               df    sumsq meansq statistic p.value
##   <chr>           <dbl>    <dbl>  <dbl>     <dbl>   <dbl>
## 1 favourite_color     1   0.0238 0.0238    0.0952   0.758
## 2 Residuals        1006 251.     0.250    NA       NA
\end{verbatim}

In this case, we fail to reject the null that the samples are the same. This all said, it's just linear regression. So I'm not sure why it got a fancy name.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lm}\NormalTok{(group }\OperatorTok{~}\StringTok{ }\NormalTok{favourite_color, }
    \DataTypeTok{data =}\NormalTok{ just_two_groups) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{tidy}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 5
##   term                 estimate std.error statistic   p.value
##   <chr>                   <dbl>     <dbl>     <dbl>     <dbl>
## 1 (Intercept)            1.48      0.0338    43.8   1.67e-235
## 2 favourite_colorWhite  -0.0118    0.0382    -0.308 7.58e-  1
\end{verbatim}

My favourite discussion of ANOVA is \citep[Ch 8]{taback2020}.

\hypertarget{treatment-and-control}{%
\subsection{Treatment and control}\label{treatment-and-control}}

If the treated and control groups are the same in all ways and remain that way, then we have internal validity, which is to say that our control will work as a counterfactual and our results can speak to a difference between these groups in that study.

In the words of \citet[p.~71]{gertler2016impact}:

\begin{quote}
Internal validity means that the estimated impact of the program is net of all other potential confounding factors---or, in other words, that the comparison group provides an accurate estimate of the counterfactual, so that we are estimating the true impact of the program.
\end{quote}

If the group to which we applied our randomisation were representative of the broader population, and the experimental set-up were fairly similar to outside conditions, then we further have external validity. That means that the difference that we find does not just apply in our own experiment, but also in the broader population.

Again, in the words of \citet[p.~73]{gertler2016impact}:

\begin{quote}
External validity means that the evaluation sample accurately represents the population of eligible units. The results of the evaluation can then be generalized to the population of eligible units. We use random sampling to ensure that the evaluation sample accurately reflects the population of eligible units so that impacts identified in the evaluation sample can be extrapolated to the population.
\end{quote}

But this means we need randomisation twice. How does this trade-off happen and to what extent does it matter?

As such, we are interested in the effect of being `treated'. This may be that we charge different prices (continuous treatment variable), or that we compare different colours on a website (discrete treatment variable, and a staple of A/B testing). If we consider just discrete treatments (so that we can use dummy variables) then need to make sure that all of the groups are otherwise the same. How can we do this? One way is to ignore the treatment variable and to examine all other variables - can you detect a difference between the groups based on any other variables? In the website example, are there a similar number of:

\begin{itemize}
\tightlist
\item
  PC/Mac users?
\item
  Safari/Chrome/Firefox/other users?
\item
  Mobile/desktop users?
\item
  Users from certain locations?
\end{itemize}

These are all threats to the validity of our claims.

But if done properly, that is if the treatment is truly independent, then we can estimate an `average treatment effect', which in a binary treatment variable setting is:
\[\mbox{ATE} = \mbox{E}[y|d=1] - \mbox{E}[y|d=0].\]

That is, the difference between the treated group, \(d = 1\), and the control group, \(d = 1\), when measured by the expected value of some outcome variable, \(y\). So the mean causal effect is simply the difference between the two expectations!

Let's again get stuck into some code. First we need to generate some data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}
\NormalTok{example_data <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{person =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{1000}\NormalTok{),}
                       \DataTypeTok{treatment =} \KeywordTok{sample}\NormalTok{(}\DataTypeTok{x =} \DecValTok{0}\OperatorTok{:}\DecValTok{1}\NormalTok{, }\DataTypeTok{size  =} \DecValTok{1000}\NormalTok{, }\DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{                       )}
\CommentTok{# We want to make the outcome slightly more likely if they were treated than if not.}
\NormalTok{example_data <-}\StringTok{ }
\StringTok{  }\NormalTok{example_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{rowwise}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{outcome =} \KeywordTok{if_else}\NormalTok{(treatment }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{, }
                           \KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n =} \DecValTok{1}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{5}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{1}\NormalTok{),}
                           \KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n =} \DecValTok{1}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{6}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{1}\NormalTok{)}
\NormalTok{                           )}
\NormalTok{         )}

\NormalTok{example_data}\OperatorTok{$}\NormalTok{treatment <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(example_data}\OperatorTok{$}\NormalTok{treatment)}

\NormalTok{example_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ outcome, }
             \DataTypeTok{fill =}\NormalTok{ treatment)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{,}
                 \DataTypeTok{binwidth =} \FloatTok{0.2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Outcome"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Number of people"}\NormalTok{,}
       \DataTypeTok{fill =} \StringTok{"Person was treated"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-238-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example_regression <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(outcome }\OperatorTok{~}\StringTok{ }\NormalTok{treatment, }\DataTypeTok{data =}\NormalTok{ example_data)}

\KeywordTok{tidy}\NormalTok{(example_regression)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 5
##   term        estimate std.error statistic  p.value
##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>
## 1 (Intercept)     5.00    0.0430     116.  0.      
## 2 treatment1      1.01    0.0625      16.1 5.14e-52
\end{verbatim}

But then reality happens. Your experiment cannot run for too long otherwise people may be treated many times, or become inured to the treatment, but it cannot be too short otherwise you can't measure longer term outcomes. You cannot have a `representative' sample on every cross-tab, but if not then the treatment and control will be different. Practical difficulties may make it difficult to follow up with certain groups.

Questions to ask (if they haven't been answered already) include:

\begin{itemize}
\tightlist
\item
  How are the participants being selected into the frame for consideration?
\item
  How are they being selected for treatment? We would hope this is a lottery, but this term is applied to a variety of situations. Additionally, early `success' can lead to pressure to treat everyone.
\item
  How is treatment being assessed?
\item
  To what extent is random allocation ethical and fair? Some argue that shortages mean it is reasonable to randomly allocate, but that may depend on how linear the benefits are. It may also be difficult to establish boundaries. If we only want to include people in Ontario then that may be clear, but what about `students' in Ontario - who is a student, and who is making the decision?
\end{itemize}

Bias and other issues are not the end of the world. But you need to think about it carefully. In the famous example, Abraham Wald was given data on the planes that came back to Britain after being shot at in WW2. The question is where to place the armour. One option is to place it over the bullet holes. Wald recognised that there is a selection effect here - these are the planes that made it back - they didn't need the armour, but instead we should put the armour where there were no bullet holes.

To consider an example that may be closer to home - how would the results of a survey differ if I only asked students who completed this course what was difficult about it and not those who dropped out?

While, as Dan suggests, we should work to try to make the dataset as good as possible, it may be possible to use the model to control for some of the bias. If there is a variable that is correlated with say, attrition, then we can add it to the model. Either by itself, or as an interaction.

What if there is a correlation between the individuals? For instance, what if there were some `hidden variable' that we didn't know about, such as province, and it turned out that people from the same province were similar? In that case we could use `wider' standard errors.

But a better way to deal with this may be to change the experiment. For instance, we discussed stratified sampling - perhaps we should stratify by province? How would we implement this?

And of course, these days we'd not really use a 100-year-old method but would instead use Bayes-based approaches.

\textbf{TBD: Add code}

\hypertarget{case-study---fishers-tea-party}{%
\section{Case study - Fisher's tea party}\label{case-study---fishers-tea-party}}

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/drinking_tea} \caption{Afternoon Tea Party (1890–1891), by Mary Cassatt (American, 1844-1926), as downloaded from https://artvee.com/dl/afternoon-tea-party.}\label{fig:ladiesdrinkingtea}
\end{figure}

Fisher (see note above) introduced a, now, famous example of an experiment designed to see if a person can distinguish between a cup of tea when the milk was added first, or last\footnote{I'm personally very attached to this example as this issue also matters a lot to my father}.

From \citet[p.13]{fisherdesignofexperiments}:

\begin{quote}
A lady declares that by tasting a cup of tea made with milk she can discriminate whether the milk or the tea infusion was first added to the cup. We will consider the problem of designing an experiment by means of which this assertion can be tested.
\end{quote}

Fisher continues:

\begin{quote}
Our experiment consists in mixing eight cups of tea, four in one way and four in the other, and presenting them to the subject for judgment in a random order. The subject has been told in advance of what the test will consist, namely that she will be asked to taste eight cups, that these shall be four of each kind, and that they shall be presented to her in a random order, that is in an order not determined arbitrarily by human choice, but by the actual manipulation of the physical apparatus used in games of chance, cards, dice, roulettes, etc., or, more expeditiously, from a published collection of random sampling-numbers purporting to give the actual results of such manipulation. Her task is to divide the 8 cups into two sets of 4, agreeing, if possible, with the treatments received.
\end{quote}

To summarize, the set-up is:

\begin{itemize}
\tightlist
\item
  Eight randomly ordered cups of tea.
\item
  Four had tea put in first.
\item
  Four had milk put in first.
\item
  The person has to choose the four that are the same.
\item
  The person knows it's an experiment.
\end{itemize}

We'll now try this experiment. So brew some tea, grab eight cups, and pour eight cups of tea for a friend that you're isolating with\footnote{For posteriority, 2020 was quite a year.} - four where you put the milk in first and four where you put the milk in last. Make sure you use the same amount of tea and milk in each! Don't forget to randomise the order, possibly even using the following code:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sample}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{8}\NormalTok{), }\DataTypeTok{size =} \DecValTok{8}\NormalTok{, }\DataTypeTok{replace =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3 7 6 4 1 8 2 5
\end{verbatim}

Then have your friend guess which four you put milk in first and which four you put milk in last!

To decide if the person's choices were likely to have occurred at random or not, we need to think about the probability of this happening by chance. First count the number of successes out of the four that were chosen. \citet[p.14]{fisherdesignofexperiments} claims there are: \({8 \choose 4} = \frac{8!}{4!(8-4)!}=70\) possible outcomes.

By chance, there are two ways for the person to be perfectly correct (because we are only asking them to be grouped): correctly identify all the ones that were milk-first (one outcome out of 70) or correctly identify all the ones that were tea-first (one outcome out of 70), so the chance of that is \(2/70 \approx 0.028\). Now, as \citet[p.15]{fisherdesignofexperiments} says,

\begin{quote}
`{[}i{]}t is open to the experimenter to be more or less exacting in respect of the smallness of the probability he would require before he would be willing to admit that his observations have demonstrated a positive result'.
\end{quote}

You need to decide what evidence it takes for you to be convinced. If there's no possible evidence that will dissuade you from your view (that there is no difference between milk-first and tea-first) then what is the point of doing an experiment? In any case, if the null is that they can't distinguish, but they correctly separate them all, then at the five-per-cent level, we reject the null.

What if they miss one? Similarly, by chance there are 16 ways for a person to be `off-by-one'. Either they think there was one that was milk-first when it was tea-first - there are, \({4 \choose 1}\), four ways this could happen - or they think there was one that was tea-first when it was milk-first - again, there are, \({4 \choose 1}\), four ways this could happen. But these outcomes are independent, so the probability is \(\frac{4\times 4}{70} \approx 0.228\). And so on. So, we fail to reject the null.

Finally, an aside on this magical `5 per cent'. Fisher himself describes this as merely `usual and convenient' \citep[ p.15]{fisherdesignofexperiments}. \citet[p.16]{fisherdesignofexperiments} continues:

\begin{quote}
In order to assert that a natural phenomenon is experimentally demonstrable we need, not an isolated record, but a reliable method of procedure. In relation to the test of significance, we may say that a phenomenon is experimentally demonstrable when we know how to conduct an experiment which will rarely fail to give us a statistically significant result.
\end{quote}

At the start of these notes, I said that Fisher held views that we would consider reprehensible today. My guess is, were he around today, he would think our use of p-values as discrediting. Do not just go searching for meaning in constellations of stars. Thoroughly interrogate your data and think precisely about the statistical methods you are applying. For conclusions that you want to hold up in the long-run, aim to use as simple, and as understandable, statistical methods as you can. Ensure that you can explain and justify your statistical decisions without recourse to astrology.

\includegraphics[width=0.6\linewidth]{https://imgs.xkcd.com/comics/significant}
Source: \url{https://xkcd.com/882/}

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/wisdom_over_fortune} \caption{'The triumph of wisdom over fortune' by Otto van Veen (Flemish, 1556 - 1629), as downloaded from https://artvee.com/dl/the-triumph-of-wisdom-over-fortune.}\label{fig:unnamed-chunk-241}
\end{figure}

\hypertarget{case-study---tuskegee-syphilis-study}{%
\section{Case study - Tuskegee Syphilis Study}\label{case-study---tuskegee-syphilis-study}}

The Tuskegee Syphilis Study is an infamous medical trial in which Black Americans with syphilis (and a `control group' without) were not given appropriate treatment, nor even told they had syphilis, well after standard syphilis treatments were established in the mid-1940s \citep{marcella}. The study began in 1932 when poor Black Americans in the South were identified and offered compensation including `hot meals, the guise of treatment, and burial payments' \citep{marcella}. The men were not treated for syphilis. Further, and this is almost unbelievable, some of the men were drafted, told they had syphilis, and ordered to get treatment. This treatment was blocked. By the time the study was stopped, `the majority of the study's victims were deceased, many from syphilis-related causes.' \citep{marcella}.

The study continued through to 1972, only stopping when it was leaked and published in newspapers. In response the US established requirements for Institutional Review Boards and President Clinton made a formal apology in 1997. \citet{brandt1978racism} as quoted by \citet{marcella} says `\,``In retrospect the Tuskegee Study revealed more about the pathology of racism than the pathology of syphilis; more about the nature of scientific inquiry than the nature of the disease process\ldots. The degree of deception and the damages have been severely underestimated.''\,'

On the Tuskegee Syphilis Study \href{https://www.monicaalexander.com}{Professor Monica Alexander} says:

\begin{quote}
While it may be illegal to do this exact research these days, it doesn't mean that unethical research doesn't still happen, and we see it all the time in ML and health. Just because you can't explicitly discriminate when you design experiments, doesn't mean you can't implicitly discriminate.
\end{quote}

For an example of this, start with \citet{obermeyer2019dissecting}:

\begin{quote}
Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5\%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.
\end{quote}

\hypertarget{case-study---the-oregon-health-insurance-experiment}{%
\section{Case study - The Oregon Health Insurance Experiment}\label{case-study---the-oregon-health-insurance-experiment}}

\textbf{TBD}

\hypertarget{ab-testing}{%
\section{A/B testing}\label{ab-testing}}

\hypertarget{introduction-15}{%
\subsection{Introduction}\label{introduction-15}}

The past decade has probably seen the most experiments ever run by several orders of magnitude with the extensive use of A/B testing on websites. Every time you are online you are probably subject to tens, hundreds, or potentially thousands, of different A/B tests. If you use apps like TikTok then this could run to the tens of thousands. While they have several interesting features, which we will discuss, at their heart they are still just surveys that result in data that need to be analysed. In this section of the notes we augment our main textbooks with a less formal book focused on A/B testing that is very popular in industry at the moment.

I can't do much better than to quote their opening example \citet[p.~3]{kohavi}.

\begin{quote}
In 2012, an employee working on Bing, Microsoft's search engine, suggested changing how ad headlines display (Kohavi and Thomke 2017). The idea was to lengthen the title line of ads by combining it with the text from the first line below the title, as shown in Figure 1.1.

Nobody thought this simple change, among the hundreds suggested, would be the best revenue-generating idea in Bing's history!

The feature was prioritized low and languished in the backlog for more than six months until a software developer decided to try the change, given how easy it was to code. He implemented the idea and began evaluating the idea on real users, randomly showing some of them the new title layout and others the old one. User interactions with the website were recorded, including ad clicks and the revenue generated from them. This is an example of an A/B test, the simplest type of controlled experiment that compares two variants: A and B, or a Control and a Treatment.

A few hours after starting the test, a revenue-too-high alert triggered, indicating that something was wrong with the experiment. The Treatment, that is, the new title layout, was generating too much money from ads. Such ``too good to be true'' alerts are very useful, as they usually indicate a serious bug, such as cases where revenue was logged twice (double billing) or where only ads displayed, and the rest of the web page was broken.

For this experiment, however, the revenue increase was valid. Bing's revenue increased by a whopping 12\%, which at the time translated to over \$100M annually in the US alone, without significantly hurting key user-experience metrics. The experiment was replicated multiple times over a long period.

The example typifies several key themes in online controlled experiments:

\begin{itemize}
\tightlist
\item
  It is hard to assess the value of an idea. In this case, a simple change worth over \$100M/year was delayed for months.
\item
  Small changes can have a big impact. A \$100M/year return-on-investment (ROI) on a few days' work for one engineer is about as extreme as it gets.
\item
  Experiments with big impact are rare. Bing runs over 10,000 experiments a year, but simple features resulting in such a big improvement happen only once every few years.
\item
  The overhead of running an experiment must be small. Bing's engineers had access to ExP, Microsoft's experimentation system, which made it easy to scientifically evaluate the idea.
\item
  The overall evaluation criterion (OEC, described more later in this chapter) must be clear. In this case, revenue was a key component of the OEC, but revenue alone is insufficient as an OEC. It could lead to plastering the web site with ads, which is known to hurt the user experience. Bing uses an OEC that weighs revenue against user-experience metrics, including Sessions per user (are users abandoning or increasing engagement) and several other components. The key point is that user-experience metrics did not significantly degrade even though revenue increased dramatically.
\end{itemize}
\end{quote}

In these notes, I'm going to use A/B testing to strictly refer to the situation in which we're dealing with a tech firm, and some type of change in code. If we are dealing with the physical world then we'll stick with RCTs. You may think that it's easy to go to a workplace and say `hey, let's test stuff before we spend thousands/millions of dollars'. You'd be wrong. The hardest part of A/B testing isn't the science, it's the politics.

\hypertarget{unique-complications-of-ab-testing}{%
\subsection{Unique complications of A/B testing}\label{unique-complications-of-ab-testing}}

\hypertarget{delivery}{%
\subsubsection{Delivery}\label{delivery}}

\textbf{This is from Chapter 12 of \citet{kohavi}.}

In the case of a RCT it's fairly obvious how we deliver the treatment - for instance, make them come to a doctor's clinic and inject them with the drug or a placebo. In the case of A/B testing, it's less obvious - do you run it `server-side' or `client-side'? E.g. do you just change the website - `server side', or do you change an app - `client side'.

This may seem like a silly issue, but it affects two aspects:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Release.
\item
  Data transmission.
\end{enumerate}

In the case of the effect on release, it's easy and normal to update a website all the time, so small changes can be easily implemented in the case of server-side. However, in the case of client-side, let's say an app, it's likely a much bigger deal.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  It needs to get through an app store (a bigger or lesser deal depending on which one).
\item
  It need to go through a release cycle (a bigger or lesser deal depending on the specifics of the company and how it ships).
\item
  Users have the opportunity to not upgrade. Are they likely different to those that do upgrade? (Yes.)
\end{enumerate}

Now, in the case of the effect on data transmission, again server-side is less of a big deal - you kind of get the data as part of the user interacting. But in the case of client-side - it's not necessarily the case that the user will have the internet at the time they're using your application, and if they do they may have limitations on the data uploads. The phone may limit data transmission depending on its effect on battery, CPU, general performance, etc. So then you decide to cache, but then the user may find it weird that some minor app takes up as much size as their photos.

The effect of all this is that you need to plan, and build this into your expectations - don't promise results the day after a release if you're evaluating a client-side change. Adjust for the fact that your results are conditional and gather data on those conditions e.g.~battery level or whatever. Adjust in your analysis for different devices and platforms, etc. This is a lovely opportunity for multilevel regression.

\hypertarget{instrumentation}{%
\subsubsection{Instrumentation}\label{instrumentation}}

\textbf{This is from Chapter 13 of \citet{kohavi}.}

I would change the name of this from instrumentation, but I don't have a good replacement. The point of this is that you need to consider how you are getting your data in the first place. For instance, if we put a cookie on your device then different types of users will remove that at different rates. Using things like beacons can be great (this is when you force the user to `download' some tiny thing they don't notice so that you know they've gone somewhere - see `email' etc). But again, there are practical issues - do we force the beacon before the main content loads - which makes for a worse customer experience; or do we allow the beacon to load after the main content, in which case we may get a biased sample?

There are likely different servers and databases for different faces of the product. For instance, Twitter in Australia, compared with Twitter in Canada, compared with Twitter on my phone's app, compared with Twitter accessed via the browser. Joining these different datasets can be difficult and requires either a unique id or some probabilistic approach.

\citet[p.~165]{kohavi} recommend changing the culture of your workplace to ensure instrumentation is normalised, which I mean, yeah.

\hypertarget{randomisation-unit}{%
\subsubsection{Randomisation unit}\label{randomisation-unit}}

\textbf{This is from Chapter 13 of \citet{kohavi}.}

What are we actually randomising over? Okay, again, this is something that's kind of obvious in normal RCTs, but gets like really interesting in the case of A/B testing. Let's consider the malaria netting experiments - either a person/village/state gets a net or it doesn't. Easy (relatively). But in the case of server-side A/B testing - are we randomising the page, the session, or the user?

To think about this, let's think about colour. Let's say that we change our logo from red to blue on the `home' page. If we're randomising at the page level, then when the user goes to the `about' page the logo could be back to red. If we're randomising at the session level, then it'll be blue while they're using the website that time, but if they close it and come back then it'll be red. Finally, if we're randomising at a user level then it'll always be red for me, but always blue for my friend. That last bit assumes perfect identity tracking, which might be generally okay if you're Google or Facebook, but for anyone else is going to be a challenge - what if you visit cbc.ca on your phone and then on your laptop? You're likely considered a different `user'.

Does this matter? It's a trade-off between consistency and importance.

\hypertarget{case-study---upworthy}{%
\section{Case study - Upworthy}\label{case-study---upworthy}}

To see this in action let's look at the Upworthy dataset \citet{upworthy}.

\hypertarget{sampling-and-survey-essentials}{%
\section{Sampling and survey essentials}\label{sampling-and-survey-essentials}}

\hypertarget{introduction-16}{%
\subsection{Introduction}\label{introduction-16}}

Let's say that we have some data. For instance, a particular toddler goes to sleep at 6:00pm every night. We might be interested to know whether that bed-time is common more generally among all toddlers, or if we have an unusual toddler. We only have one toddler so our ability to use his bed time to speak about all toddlers is limited. But what about if we talk to our friends who also have toddlers? How many friends, and friends of friends, do we have to ask because we can begin to feel comfortable speaking about some underlying truth of toddler bedtime?

In the wonderful phrase of \citet[p.~3]{wuandthompson} `{[}s{]}tatistics is the science of how to collect and analyze data, and draw statements and conclusions about unknown populations. The term population usually refers to a real or hypothetical set of units with characteristics and attributes which can be modelled by random variables and their respective probability distributions.'. In my own much less wonderful phrasing, `statistics involves having some data and trying to say something sensible about it'. I mean, it's really up to you which one you want to go with.

In the case of surveys, our population is a finite set of \(N\) labels: `person 1', `person 2', `person 3', \ldots, `person \(N\)'. It is important here to recognise that there is a difference between the population of interest to a survey and a population in the sense that it is used when we talk of limits and similar infinity concepts in statistics. For instance, from time to time, you hear people who work with census data say that they don't need to worry about confidence intervals because they have the whole population of the country. Nothing could be further from the truth.

\citet[p.~4]{wuandthompson} have a lovely example of the ambiguity that surrounds the definition of a population. Let's consider the population of voters. In Canada that means anyone who is 18 or older. Fine. But what if we are interested in consumers - what is the definition of hipsters? I regularly eat avocado toast, (+1), but I've never had bullet coffee (-1). Am I in the population or not?

More things are formally defined than you may realise. For instance, the idea of a rural area is precisely defined. A property is either in a rural area or not. But then we come to the lovely example of \citet[p.~4]{wuandthompson} when it comes to whether someone is a smoker. If a 15 year old has had 100 cigarettes then it's pretty clear that we need to treat them differently than if they have had none. But if a 100 year old has had 100 cigarettes then we consider them to have none. That's fine, but what is the age at which this changes? Further, think about how this changes over time. At one point, parents used to be worried if children had more than two hours of screen time, now those same children (and possibly even the parents) regularly likely spend more than eight hours in front of a screen if they work in an office job.

So we come to some critical terminology:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Population: `The set of all units covered by the main objective of the study.' \citet[p.~5]{wuandthompson}.
\item
  Frame: `Lists of sampling units' \citet[p.~9]{wuandthompson} where sampling units are either the observational units themselves or the clusters.
\item
  Sample: Those who complete and return the survey.
\end{enumerate}

To be a little more concrete about this, consider that we are trying to conduct a survey about the attitudes Australians who live in Toronto. So the target population is all Australians who live in Toronto, the frame might be all those Australians who live in Toronto who use Facebook, because we are going to use Facebook to choose who to sample. And then finally, if we take that Facebook list of all Australians living in Canada and we gave each one a chance at being surveyed then that would be our sampled population, but if we just picked the ones that I know then it would just be Dan, Monica, and Liza (from New Zealand but we'll claim her because that's a thing that Australians do).

In that example the target population and the frame will be different because not all Australians who live in Toronto are on Facebook. Similarly, if not everyone that we gave the survey to actually completed the survey then the sample and the frame would be different.

Having identified a population of interest and a frame (i.e.~a list that gets the closest to that population) At this point we distinguish between probability and non-probability sampling.

With probability sampling, every member of the frame has some chance of being sampled. Consider the example of the Australian Election Study - they get a list of all the addresses in Australia, and then randomly choose some to send letters to. The `randomista' and RCT revolution that we discuss later, is needed because of a lack of probability sampling, but when it exists it plays a role here. Importantly it ensure that we are clear about the role of uncertainty \citep[p.~11]{wuandthompson}. The trade-off is that it is expensive and difficult. Note that each unit in the frame doesn't have to have the same probability necessarily, it just needs to be determined by a probability measure.

In contrast, with non-probability sampling we focus on populations that are `readily available' or convenient, satisfy certain quotas, based on judgement, or those that volunteer. The difference between probability and non-probability sampling is that of degree - we typically cannot force someone to take our survey, and hence, there is almost also as aspect of volunteering.

While acknowledging that it is a spectrum, most of statistics was developed based on probability sampling. But much of modern sampling is done using non-probability sampling. In particular, a common approach is to have a bunch of Facebook ads trying to recruit a panel of people in exchange for compensation. This panel is then the group that is sent various surveys as necessary. But think for a moment about the implications of this - what type of people are likely to respond to such an ad? I don't know who Canada's richest person is, but are they likely to be in this panel? Is your grandmother likely to respond to that ad? What about you - do you even use Facebook?

In some cases it is possible to do a census. Nation-states typically do one every five to ten years. But there is a reason that it is only nation states that do them - they are expensive, time-consuming, and surprisingly, they are sometimes not as accurate as we may hope because of how general they need to be. Hence, the role of surveys. Note, however that censuses will typically have many of the same concerns.

When we consider our population, it will typically have some ordering. This may be as simple as a country having states/provinces. We consider a stratified structure to be one in which we can divide the population into mutually exclusive and collectively exhaustive sub-populations, or strata. Examples of strata in \citet[p.~8]{wuandthompson} include provinces, federal electoral districts, or health regions. But strata need not be geographic, and it may be possible to use different majors. We use stratification to help with the efficiency of sampling or with the balance of the survey. For instance, if we surveyed provinces in proportion to their population, then even a survey of 10,000 responses would only expect to have 10 responses from the Yukon.

The other word that is used that takes advantage of the ordering of some population is clusters. Again, these are collectively exhaustive and mutually exclusive. Again, they may be geographically based, but need not be. The difference between stratified sampling and cluster sampling, is that `under stratified sampling, sample data are collected from every stratum, (whereas) under cluster sampling, only a portion of the clusters has members in the final sample' \citet[p.~8]{wuandthompson}. That all said, this difference can become less clear in practice, especially \emph{ex post} - what if you stratify then randomly sample within that strata, but no one is selected - but in terms of intention the difference is clear.

We now turn to the first of our claims, which is that if we have a perfect frame and no non-response, then our sample results will match that of the population. We'd of course be very worried if that weren't the case, but it's nice to have it stated. We establish some type of population mean for the study variable, \(\mu_y\), and population means for the auxiliary variables \(\mu_x\), which could be things like age, gender, etc. Remembering that when we do this in the real world, we may have many study variables, and indeed, some overlap. If a variable is an indicator then in this set-up all we have to do is to work out the proportion in order to estimate it, which is \(P\). And finally, we get a rule of thumb for large samples whereby the variance in this binary and perfect setting becomes \(\sigma_y^2 = P/(1-P)\) \citep[p.~11]{wuandthompson}.

Finally, we conclude with the steps that you should consider. These are all critical. Strong reports would grapple with all of these.

\hypertarget{simple-random-sampling}{%
\subsection{Simple random sampling}\label{simple-random-sampling}}

TBD

\hypertarget{stratified-and-cluster-sampling}{%
\subsection{Stratified and cluster sampling}\label{stratified-and-cluster-sampling}}

TBD

\hypertarget{implementing-surveys}{%
\section{Implementing surveys}\label{implementing-surveys}}

\hypertarget{google}{%
\subsection{Google}\label{google}}

\hypertarget{facebook}{%
\subsection{Facebook}\label{facebook}}

\hypertarget{survey-monkey}{%
\subsection{Survey Monkey}\label{survey-monkey}}

\hypertarget{mechanical-turk}{%
\subsection{Mechanical Turk}\label{mechanical-turk}}

\hypertarget{prolific}{%
\subsection{Prolific}\label{prolific}}

\hypertarget{qualtrics}{%
\subsection{Qualtrics}\label{qualtrics}}

\hypertarget{other-2}{%
\subsection{Other}\label{other-2}}

\hypertarget{next-steps-1}{%
\section{Next steps}\label{next-steps-1}}

Large scale experiments are happening all around us. These days I feel we all know a lot more about healthcare experiments than perhaps we'd like to know and the AstraZeneca/Oxford situation is especially interesting, for instance, \citet{OxfordAstraZeneca}, but see \citet{Bastian2020} for how this is actually possibly more complicated.

There are also well-known experiments that tried to see if big government programs are effective, such as:

\begin{itemize}
\tightlist
\item
  The RAND Health Insurance Experiment randomly gave health insurance to people in the US between 1974 and 1982 \citep{randhealth}.
\item
  The Oregon Health Study randomly gave health insurance in Oregon in 2008 \citep{finkelstein2012oregon}.
\end{itemize}

\hypertarget{other-sources}{%
\chapter{Other sources}\label{other-sources}}

\textbf{Required reading}

\textbf{Recommended reading}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\item
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\item
\end{itemize}

\textbf{Key functions/etc}

\begin{itemize}
\item
\end{itemize}

\textbf{Pre-quiz}

\textbf{Key sources of data}

\begin{itemize}
\tightlist
\item
  University of Toronto Dataverse: \url{https://dataverse.scholarsportal.info/dataverse/toronto}.
\item
  Data is Plural structured archive: \url{https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/edit\#gid=0}.
\item
  Kaggle Datasets: \url{https://www.kaggle.com/datasets}.
\item
  Figure Eight: \url{https://www.figure-eight.com/data-for-everyone/}.
\item
  Google dataset search: \url{https://datasetsearch.research.google.com/}.
\item
  Awesome Data: \url{https://github.com/awesomedata/awesome-public-datasets}.
\end{itemize}

\textbf{Quiz}

\begin{itemize}
\tightlist
\item
  Please identify three other sources of data that you are interested in and describe where are they available (please include a link if possible)?
\item
  Please focus on one of those sources. What steps do you have to go through in order to get a dataset that can be analysed in R?
\item
  Let's say you take a job at RBC (a Canadian bank) and they already have some quantitative data for you to use. What are some questions that you should explore when deciding whether that data will be useful to you?
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Please identify three sources of government data in your country and describe where are they available (please include a link if possible)?
\item
  Please focus on one of those sources. What steps do you have to go through in order to get a dataset such that it can be analysed in R?
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Please identify three other sources of data that you are interested in and describe where are they available (please include a link if possible)?
\item
  Please focus on one of those sources. What steps do you have to go through in order to get a dataset that can be analysed in R?
\item
  Let's say you take a job at RBC (a Canadian bank) and they already have some quantitative data for you to use. What are some questions that you should explore when deciding whether that data will be useful to you?
\end{itemize}

\hypertarget{open-government-data}{%
\section{Open Government Data}\label{open-government-data}}

\begin{itemize}
\tightlist
\item
  Canadian Government Open Data: \url{https://open.canada.ca/en/open-data}.
\end{itemize}

\hypertarget{canadian-census}{%
\subsection{Canadian Census}\label{canadian-census}}

\hypertarget{city-of-toronto-open-data-portal}{%
\subsection{City of Toronto Open Data Portal}\label{city-of-toronto-open-data-portal}}

\hypertarget{electoral-studies}{%
\section{Electoral Studies}\label{electoral-studies}}

\hypertarget{canadian-electoral-study}{%
\subsection{Canadian Electoral Study}\label{canadian-electoral-study}}

\hypertarget{australian-electoral-study}{%
\subsection{Australian Electoral Study}\label{australian-electoral-study}}

\hypertarget{part-clean}{%
\part{Clean}\label{part-clean}}

\hypertarget{cleaning-and-preparing-data}{%
\chapter{Cleaning and preparing data}\label{cleaning-and-preparing-data}}

\textbf{Required reading}

\textbf{Recommended reading}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\item
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\item
\end{itemize}

\textbf{Key functions/etc}

\begin{itemize}
\item
\end{itemize}

\textbf{Quiz}

\hypertarget{storing-and-retrieving-data}{%
\chapter{Storing and retrieving data}\label{storing-and-retrieving-data}}

\textbf{Required reading}

\textbf{Recommended reading}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\item
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\item
\end{itemize}

\textbf{Key functions/etc}

\begin{itemize}
\item
\end{itemize}

\textbf{Quiz}

\hypertarget{disseminating-and-protecting-data}{%
\chapter{Disseminating and protecting data}\label{disseminating-and-protecting-data}}

\begin{itemize}
\item
  Hawes, M. B. (2020). Implementing Differential Privacy: Seven Lessons From the 2020 United States Census. Harvard Data Science Review. \url{https://doi.org/10.1162/99608f92.353c6f99}
\item
  \url{https://hdsr.mitpress.mit.edu/pub/g9o4z8au/release/2}
\item
  \url{https://www.census.gov/newsroom/blogs/research-matters/2020/02/census_bureau_works.html}
\end{itemize}

\textbf{Required reading}

\textbf{Recommended reading}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\item
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\item
\end{itemize}

\textbf{Key functions/etc}

\begin{itemize}
\item
\end{itemize}

\textbf{Quiz}

\hypertarget{part-model}{%
\part{Model}\label{part-model}}

\hypertarget{exploratory-data-analysis}{%
\chapter{Exploratory data analysis}\label{exploratory-data-analysis}}

The \texttt{skim} function from the \texttt{skimr} package is just a quick way of looking at the data. It can be handy when you get a new dataset to quickly come to terms with it. The p0, p25, p50, etc, values that it provides are percentiles. So for instance, P50 is the 50th percentile which means that 50 per cent of the data are below this, and 50 per cent are above i.e.~the median. P0 should be the minimum, and p100 should be the maximum. The middle ones are similarly defined, so p25, means that 25 per cent of the data are below this point and 75 per cent of it is above this point. The general idea of both the percentiles and the histogram is to give you a quick idea of how skewed your data is.

\textbf{This chapter was written with \href{https://www.monicaalexander.com/}{Monica Alexander}.}

\textbf{Required reading}

\begin{itemize}
\tightlist
\item
  Wickham, Hadley, and Garrett Grolemund, 2017, `R for Data Science', Chapters 3 and 7, freely available here: \url{https://r4ds.had.co.nz/}.
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
  Hall, Megan, 2019, `Exploratory Data Analysis Using Tidyverse', freely available at: \url{https://hockey-graphs.com/2019/10/08/exploratory-data-analysis-using-tidyverse/}.
\item
  Jordan, Michael I, 2019, `AI - The revolution hasn't started yet', freely available at: \url{https://hdsr.mitpress.mit.edu/pub/wot7mkc1}.
\item
  Silge, Julia, 2018, `Understanding PCA using Stack Overflow data', freely available at: \url{https://juliasilge.com/blog/stack-overflow-pca/}.
\item
  Soetewey, Antoine, 2020, `Descriptive statistics in R', freely available at: \url{https://www.statsandr.com/blog/descriptive-statistics-in-r/}.
\item
  Stodulka, Jiri, 2019, `Toronto Crime and Folium', freely available at: \url{https://www.jiristodulka.com/post/toronto-crime/}.
\item
  Wong, Julia Carrie, 2020, `One year inside Trump's monumental Facebook campaign', The Guardian, 29 January, freely available at: \url{https://www.theguardian.com/us-news/2020/jan/28/donald-trump-facebook-ad-campaign-2020-election}.
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\tightlist
\item
  Quickly coming to terms with a new dataset
\end{itemize}

\textbf{Key libraries/functions/etc}

\begin{itemize}
\tightlist
\item
  \texttt{tidyverse}
\item
  \texttt{ggplot2}
\end{itemize}

\textbf{Pre-quiz}

\begin{itemize}
\tightlist
\item
  In your own words what is exploratory data analysis?
\item
  If you have a dataset called `my\_data', which has two columns: `first\_col' and `second\_col', then could you please write some rough R code that would generate a graph (the type of graph doesn't matter).
\item
  Consider a dataset that has 500 rows and 3 columns, so there are 1,500 cells. If 100 of the cells are missing data for at least one of the columns, then would you remove the whole row your dataset or try to run your analysis on the data as is, or some other procedure? What if your dataset had 10,000 rows instead, but the same number of missing cells?
\item
  Please note three ways of identifying unusual values.
\item
  What is the difference between a categorical and continuous variable?
\end{itemize}

\hypertarget{introduction-17}{%
\section{Introduction}\label{introduction-17}}

\begin{quote}
Exploratory data analysis is never finished, you just die.
\end{quote}

This chapter is about exploratory data analysis (EDA) and data visualization steps in R. The aim is to get you used to working with real data (that has issues) to understand the main characteristics and potential issues.

We will be using the \href{https://sharlagelfand.github.io/opendatatoronto/}{\texttt{opendatatoronto}} R package, which interfaces with the City of Toronto Open Data Portal.

\hypertarget{a-note-on-packages}{%
\section{A note on packages}\label{a-note-on-packages}}

If you are running this Rmd on your local machine, you may need to install various packages used (using the \texttt{install.packages} function).

Load in all the packages we need:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(opendatatoronto)}
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(stringr)}
\KeywordTok{library}\NormalTok{(skimr)}
\KeywordTok{library}\NormalTok{(visdat)}
\KeywordTok{library}\NormalTok{(janitor)}
\KeywordTok{library}\NormalTok{(lubridate)}
\KeywordTok{library}\NormalTok{(ggrepel)}
\end{Highlighting}
\end{Shaded}

\hypertarget{ttc-subway-delays}{%
\section{TTC subway delays}\label{ttc-subway-delays}}

This package provides an interface to all data available on the \href{https://open.toronto.ca/}{Open Data Portal} provided by the City of Toronto.

Use the \texttt{list\_packages} function to look at what's available

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all_data <-}\StringTok{ }\KeywordTok{list_packages}\NormalTok{(}\DataTypeTok{limit =} \DecValTok{500}\NormalTok{)}
\NormalTok{all_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 405 x 11
##    title id    topics civic_issues publisher excerpt dataset_category
##    <chr> <chr> <chr>  <chr>        <chr>     <chr>   <chr>           
##  1 Auto~ 6b4b~ Healt~ <NA>         Toronto ~ "Locat~ Document        
##  2 Earl~ 2619~ Commu~ Poverty red~ Children~ "Early~ Map             
##  3 Shor~ fc41~ Permi~ Affordable ~ Municipa~ "This ~ Table           
##  4 King~ 04f5~ City ~ Mobility     Transpor~ "King ~ Document        
##  5 King~ 5a4e~ City ~ Mobility     Transpor~ "King ~ Document        
##  6 King~ e051~ City ~ Mobility     Transpor~ "The K~ Document        
##  7 King~ b240~ City ~ Mobility     Transpor~ "The K~ Document        
##  8 Poll~ 7bce~ City ~ <NA>         City Cle~ "Polls~ Table           
##  9 Comm~ 741e~ Healt~ <NA>         Toronto ~ "Numbe~ Document        
## 10 Dail~ 8a6e~ City ~ Affordable ~ Shelter,~ "Daily~ Table           
## # ... with 395 more rows, and 4 more variables: num_resources <int>,
## #   formats <chr>, refresh_rate <chr>, last_refreshed <date>
\end{verbatim}

Let's download the data on TTC subway delays in 2019. There are multiple files for 2019 so we need to get them all and make them into one big dataframe.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res <-}\StringTok{ }\KeywordTok{list_package_resources}\NormalTok{(}\StringTok{"996cfe8d-fb35-40ce-b569-698d51fc683b"}\NormalTok{)}
\NormalTok{res <-}\StringTok{ }\NormalTok{res }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{year =} \KeywordTok{str_extract}\NormalTok{(name, }\StringTok{"201.?"}\NormalTok{))}
\NormalTok{delay_}\DecValTok{2019}\NormalTok{_ids <-}\StringTok{ }\NormalTok{res }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(year}\OperatorTok{==}\DecValTok{2019}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(id) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{pull}\NormalTok{()}

\NormalTok{delay_}\DecValTok{2019}\NormalTok{ <-}\StringTok{ }\KeywordTok{c}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(delay_}\DecValTok{2019}\NormalTok{_ids)) \{}
\NormalTok{  delay_}\DecValTok{2019}\NormalTok{ <-}\StringTok{ }\KeywordTok{bind_rows}\NormalTok{(delay_}\DecValTok{2019}\NormalTok{, }\KeywordTok{get_resource}\NormalTok{(delay_}\DecValTok{2019}\NormalTok{_ids[i]))}
\NormalTok{\}}

\CommentTok{# make the column names nicer to work with}
\NormalTok{delay_}\DecValTok{2019}\NormalTok{ <-}\StringTok{ }\KeywordTok{clean_names}\NormalTok{(delay_}\DecValTok{2019}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's also download the delay code and readme, as reference.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delay_codes <-}\StringTok{ }\KeywordTok{get_resource}\NormalTok{(}\StringTok{"fece136b-224a-412a-b191-8d31eb00491e"}\NormalTok{)}
\NormalTok{delay_data_codebook <-}\StringTok{ }\KeywordTok{get_resource}\NormalTok{(}\StringTok{"54247e39-5a7d-40db-a137-82b2a9ab0708"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This dataset has a bunch of interesting variables. You can refer to the readme for descriptions. Our outcome of interest is \texttt{min\_delay}, which give the delay in mins.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(delay_}\DecValTok{2019}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 10
##   date                time  day   station code  min_delay min_gap bound line 
##   <dttm>              <chr> <chr> <chr>   <chr>     <dbl>   <dbl> <chr> <chr>
## 1 2019-01-01 00:00:00 01:08 Tues~ YORK M~ PUSI          0       0 S     YU   
## 2 2019-01-01 00:00:00 02:14 Tues~ ST AND~ PUMST         0       0 <NA>  YU   
## 3 2019-01-01 00:00:00 02:16 Tues~ JANE S~ TUSC          0       0 W     BD   
## 4 2019-01-01 00:00:00 02:27 Tues~ BLOOR ~ SUO           0       0 N     YU   
## 5 2019-01-01 00:00:00 03:03 Tues~ DUPONT~ MUATC        11      16 N     YU   
## 6 2019-01-01 00:00:00 03:08 Tues~ EGLINT~ EUATC        11      16 S     YU   
## # ... with 1 more variable: vehicle <dbl>
\end{verbatim}

\hypertarget{eda-and-data-viz}{%
\section{EDA and data viz}\label{eda-and-data-viz}}

The following section highlights some tools that might be useful for you when you are getting used to a new dataset. There's no one way of exploration, but it's important to always keep in mind:

\begin{itemize}
\tightlist
\item
  what should your variables look like (type, values, distribution, etc)
\item
  what would be surprising (outliers etc)
\item
  what is your end goal (here, it might be understanding factors associated with delays, e.g.~stations, time of year, time of day, etc)
\end{itemize}

In any data analysis project, if it turns out you have data issues, surprising values, missing data etc, it's important you \textbf{document} anything you found and the subsequent steps or \textbf{assumptions} you made before moving onto your data analysis / modeling.

As always:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Start with an end in mind.
\item
  Be as lazy as possible.
\end{enumerate}

\hypertarget{data-checks}{%
\section{Data checks}\label{data-checks}}

\hypertarget{sanity-checks}{%
\subsection{Sanity Checks}\label{sanity-checks}}

We need to check variables should be what they say they are. If they aren't, the natural next question is to what to do with issues (recode? remove?)

E.g. check days of week

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{unique}\NormalTok{(delay_}\DecValTok{2019}\OperatorTok{$}\NormalTok{day)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Tuesday"   "Wednesday" "Thursday"  "Friday"    "Saturday"  "Sunday"   
## [7] "Monday"
\end{verbatim}

Check lines: oh no. some issues here. Some have obvious recodes, others, not so much.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{unique}\NormalTok{(delay_}\DecValTok{2019}\OperatorTok{$}\NormalTok{line)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "YU"                     "BD"                     "YU/BD"                 
##  [4] "SHP"                    "SRT"                    NA                      
##  [7] "YUS"                    "B/D"                    "BD LINE"               
## [10] "999"                    "YU/ BD"                 "YU & BD"               
## [13] "BD/YU"                  "YU\\BD"                 "46 MARTIN GROVE"       
## [16] "RT"                     "BLOOR-DANFORTH"         "YU / BD"               
## [19] "134 PROGRESS"           "YU - BD"                "985 SHEPPARD EAST EXPR"
## [22] "22 COXWELL"             "100 FLEMINGDON PARK"    "YU LINE"
\end{verbatim}

The \texttt{skimr} package might also be useful here

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# skim(delay_2019)}
\end{Highlighting}
\end{Shaded}

What are the different values of \texttt{bound} for each \texttt{line}?

For simplicity, just keep the correct line labels.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# delay_2019 %>%}
\CommentTok{#   filter(line %in% c("BD", "YU", "SHP", "SRT")) %>%}
\CommentTok{#   mutate(bound = as.factor(bound)) %>%}
\CommentTok{#   group_by(line) %>%}
\CommentTok{#   skim(bound)}
\end{Highlighting}
\end{Shaded}

\hypertarget{missing-values}{%
\subsection{Missing values}\label{missing-values}}

Look to see how many NAs by variable

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delay_}\DecValTok{2019} \OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise_all}\NormalTok{(}\DataTypeTok{.funs =} \KeywordTok{funs}\NormalTok{(}\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(.))}\OperatorTok{/}\KeywordTok{nrow}\NormalTok{(delay_}\DecValTok{2019}\NormalTok{)}\OperatorTok{*}\DecValTok{100}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 10
##    date  time   day station  code min_delay min_gap bound  line vehicle
##   <dbl> <dbl> <dbl>   <dbl> <dbl>     <dbl>   <dbl> <dbl> <dbl>   <dbl>
## 1     0     0     0       0     0         0       0  22.8 0.260       0
\end{verbatim}

The \texttt{visdat} package is also useful here, particularly to see how missing values are distributed.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{vis_dat}\NormalTok{(delay_}\DecValTok{2019}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-253-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{vis_miss}\NormalTok{(delay_}\DecValTok{2019}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-253-2.pdf}

\hypertarget{duplicates}{%
\subsection{Duplicates?}\label{duplicates}}

The \texttt{get\_dupes} function from the \texttt{janitor} package is useful for this.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{get_dupes}\NormalTok{(delay_}\DecValTok{2019}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 158 x 11
##    date                time  day   station code  min_delay min_gap bound line 
##    <dttm>              <chr> <chr> <chr>   <chr>     <dbl>   <dbl> <chr> <chr>
##  1 2019-01-01 00:00:00 08:18 Tues~ DONLAN~ MUESA         5      10 W     BD   
##  2 2019-01-01 00:00:00 08:18 Tues~ DONLAN~ MUESA         5      10 W     BD   
##  3 2019-02-01 00:00:00 05:51 Frid~ SCARB ~ MRTO         10      15 S     SRT  
##  4 2019-02-01 00:00:00 05:51 Frid~ SCARB ~ MRTO         10      15 S     SRT  
##  5 2019-02-01 00:00:00 06:45 Frid~ MIDLAN~ MRWEA         3       8 S     SRT  
##  6 2019-02-01 00:00:00 06:45 Frid~ MIDLAN~ MRWEA         3       8 S     SRT  
##  7 2019-02-01 00:00:00 06:55 Frid~ LAWREN~ ERDO          0       0 S     SRT  
##  8 2019-02-01 00:00:00 06:55 Frid~ LAWREN~ ERDO          0       0 S     SRT  
##  9 2019-02-01 00:00:00 07:16 Frid~ MCCOWA~ MRWEA         5      10 N     SRT  
## 10 2019-02-01 00:00:00 07:16 Frid~ MCCOWA~ MRWEA         5      10 N     SRT  
## # ... with 148 more rows, and 2 more variables: vehicle <dbl>, dupe_count <int>
\end{verbatim}

There are quite a few duplicates. Remove for now:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delay_}\DecValTok{2019}\NormalTok{ <-}\StringTok{ }\NormalTok{delay_}\DecValTok{2019} \OperatorTok{%>%}\StringTok{ }\KeywordTok{distinct}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{visualizing-distributions}{%
\subsection{Visualizing distributions}\label{visualizing-distributions}}

Histograms, barplots, and density plots are your friends here.

Let's look at the outcome of interest: \texttt{min\_delay}. First of all just a histogram of all the data:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Removing the observations that have non-standardized lines}

\NormalTok{delay_}\DecValTok{2019}\NormalTok{ <-}\StringTok{ }\NormalTok{delay_}\DecValTok{2019} \OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(line }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"BD"}\NormalTok{, }\StringTok{"YU"}\NormalTok{, }\StringTok{"SHP"}\NormalTok{, }\StringTok{"SRT"}\NormalTok{))}

\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ delay_}\DecValTok{2019}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ min_delay))}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-256-1.pdf}

To improve readability, could plot on logged scale:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ delay_}\DecValTok{2019}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ min_delay)) }\OperatorTok{+}\StringTok{ }\KeywordTok{scale_x_log10}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-257-1.pdf}

Our initial EDA hinted at an outlying delay time, let's take a look at the largest delays below. Join the \texttt{delay\_codes} dataset to see what the delay is. (Have to do some mangling as SRT has different codes).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delay_}\DecValTok{2019}\NormalTok{ <-}\StringTok{ }\NormalTok{delay_}\DecValTok{2019} \OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{left_join}\NormalTok{(delay_codes }\OperatorTok{%>%}\StringTok{ }\KeywordTok{rename}\NormalTok{(}\DataTypeTok{code =} \StringTok{`}\DataTypeTok{SUB RMENU CODE}\StringTok{`}\NormalTok{, }\DataTypeTok{code_desc =} \StringTok{`}\DataTypeTok{CODE DESCRIPTION...3}\StringTok{`}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(code, code_desc)) }


\NormalTok{delay_}\DecValTok{2019}\NormalTok{ <-}\StringTok{ }\NormalTok{delay_}\DecValTok{2019} \OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{code_srt =} \KeywordTok{ifelse}\NormalTok{(line}\OperatorTok{==}\StringTok{"SRT"}\NormalTok{, code, }\StringTok{"NA"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{left_join}\NormalTok{(delay_codes }\OperatorTok{%>%}\StringTok{ }\KeywordTok{rename}\NormalTok{(}\DataTypeTok{code_srt =} \StringTok{`}\DataTypeTok{SRT RMENU CODE}\StringTok{`}\NormalTok{, }\DataTypeTok{code_desc_srt =} \StringTok{`}\DataTypeTok{CODE DESCRIPTION...7}\StringTok{`}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(code_srt, code_desc_srt))  }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{code =} \KeywordTok{ifelse}\NormalTok{(code_srt}\OperatorTok{==}\StringTok{"NA"}\NormalTok{, code, code_srt),}
         \DataTypeTok{code_desc =} \KeywordTok{ifelse}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(code_desc_srt), code_desc, code_desc_srt)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{code_srt, }\OperatorTok{-}\NormalTok{code_desc_srt)}
\end{Highlighting}
\end{Shaded}

The 455 min delay due to `Rail Related Problem' is an outlier.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delay_}\DecValTok{2019} \OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{left_join}\NormalTok{(delay_codes }\OperatorTok{%>%}\StringTok{ }\KeywordTok{rename}\NormalTok{(}\DataTypeTok{code =} \StringTok{`}\DataTypeTok{SUB RMENU CODE}\StringTok{`}\NormalTok{, }\DataTypeTok{code_desc =} \StringTok{`}\DataTypeTok{CODE DESCRIPTION...3}\StringTok{`}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(code, code_desc)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\OperatorTok{-}\NormalTok{min_delay) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(date, time, station, line, min_delay, code, code_desc)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 18,697 x 7
##    date                time  station     line  min_delay code  code_desc        
##    <dttm>              <chr> <chr>       <chr>     <dbl> <chr> <chr>            
##  1 2019-06-25 00:00:00 18:48 WILSON TO ~ YU          455 PUTR  Rail Related Pro~
##  2 2019-02-12 00:00:00 20:28 LAWRENCE E~ SRT         284 MRWEA Weather Reports ~
##  3 2019-06-05 00:00:00 12:42 UNION TO S~ YU          250 MUPLA Fire/Smoke Plan A
##  4 2019-10-22 00:00:00 14:22 LAWRENCE S~ YU          228 PUTS  Structure Relate~
##  5 2019-09-26 00:00:00 11:38 YORK MILLS~ YU          193 MUPR1 Priority One - T~
##  6 2019-06-08 00:00:00 08:51 SPADINA BD~ BD          180 MUPLB Fire/Smoke Plan ~
##  7 2019-12-02 00:00:00 06:59 DUNDAS WES~ BD          176 MUPLB Fire/Smoke Plan ~
##  8 2019-01-29 00:00:00 05:46 VICTORIA P~ BD          174 MUWEA Weather Reports ~
##  9 2019-02-22 00:00:00 17:32 ELLESMERE ~ SRT         168 PRW   Rail Defect/Fast~
## 10 2019-02-10 00:00:00 07:53 BAYVIEW ST~ SHP         165 PUSI  Signals or Relat~
## # ... with 18,687 more rows
\end{verbatim}

\hypertarget{grouping-and-small-multiples}{%
\subsubsection{Grouping and small multiples}\label{grouping-and-small-multiples}}

A quick and powerful visualization technique is to group the data by a variable of interest, e.g.~\texttt{line}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ delay_}\DecValTok{2019}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ min_delay, }\DataTypeTok{y =}\NormalTok{ ..density.., }\DataTypeTok{fill =}\NormalTok{ line), }\DataTypeTok{position =} \StringTok{'dodge'}\NormalTok{, }\DataTypeTok{bins =} \DecValTok{10}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{scale_x_log10}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-260-1.pdf}

I switched to density above to look at the the distributions more comparably, but we should also be aware of differences in frequency, in particular, SHP and SRT have much smaller counts:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ delay_}\DecValTok{2019}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ min_delay, }\DataTypeTok{fill =}\NormalTok{ line), }\DataTypeTok{position =} \StringTok{'dodge'}\NormalTok{, }\DataTypeTok{bins =} \DecValTok{10}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{scale_x_log10}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-261-1.pdf}

If you want to group by more than one variable, facets are good:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ delay_}\DecValTok{2019}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_density}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ min_delay, }\DataTypeTok{color =}\NormalTok{ day), }\DataTypeTok{bw =} \FloatTok{.08}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_x_log10}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{facet_grid}\NormalTok{(}\OperatorTok{~}\NormalTok{line)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-262-1.pdf}

Side note: the station names are a mess. Try and clean up the station names a bit by taking just the first word (or, the first two if it starts with ``ST''):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delay_}\DecValTok{2019}\NormalTok{ <-}\StringTok{ }\NormalTok{delay_}\DecValTok{2019} \OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{station_clean =} \KeywordTok{ifelse}\NormalTok{(}\KeywordTok{str_starts}\NormalTok{(station, }\StringTok{"ST"}\NormalTok{), }\KeywordTok{word}\NormalTok{(station, }\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\KeywordTok{word}\NormalTok{(station, }\DecValTok{1}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

Plot top five stations by mean delay:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delay_}\DecValTok{2019} \OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(line, station_clean) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean_delay =} \KeywordTok{mean}\NormalTok{(min_delay), }\DataTypeTok{n_obs =} \KeywordTok{n}\NormalTok{()) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(n_obs}\OperatorTok{>}\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(line, }\OperatorTok{-}\NormalTok{mean_delay) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{slice}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(station_clean, mean_delay)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_col}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\NormalTok{line, }\DataTypeTok{scales =} \StringTok{"free_y"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-264-1.pdf}

\hypertarget{visualizing-time-series}{%
\subsection{Visualizing time series}\label{visualizing-time-series}}

Daily plot is messy (you can check for yourself). Let's look by week to see if there's any seasonality. The \texttt{lubridate} package has lots of helpful functions that deal with date variables. First, mean delay (of those that were delayed more than 0 mins):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delay_}\DecValTok{2019} \OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(min_delay}\OperatorTok{>}\DecValTok{0}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{week =} \KeywordTok{week}\NormalTok{(date)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(week, line) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean_delay =} \KeywordTok{mean}\NormalTok{(min_delay)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(week, mean_delay, }\DataTypeTok{color =}\NormalTok{ line)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{facet_grid}\NormalTok{(}\OperatorTok{~}\NormalTok{line)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-265-1.pdf}

What about proportion of delays that were greater than 10 mins?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delay_}\DecValTok{2019} \OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{week =} \KeywordTok{week}\NormalTok{(date)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(week, line) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{prop_delay =} \KeywordTok{sum}\NormalTok{(min_delay}\OperatorTok{>}\DecValTok{10}\NormalTok{)}\OperatorTok{/}\KeywordTok{n}\NormalTok{()) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(week, prop_delay, }\DataTypeTok{color =}\NormalTok{ line)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{facet_grid}\NormalTok{(}\OperatorTok{~}\NormalTok{line)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-266-1.pdf}

\hypertarget{visualizing-relationships}{%
\subsection{Visualizing relationships}\label{visualizing-relationships}}

Note that \textbf{scatter plots} are a good precursor to modeling, to visualize relationships between continuous variables. Nothing obvious to plot here, but easy to do with \texttt{geom\_point}.

Look at top five reasons for delay by station. Do they differ? Think about how this could be modeled.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delay_}\DecValTok{2019} \OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(line, code_desc) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean_delay =} \KeywordTok{mean}\NormalTok{(min_delay)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\OperatorTok{-}\NormalTok{mean_delay) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{slice}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ code_desc,}
             \DataTypeTok{y =}\NormalTok{ mean_delay)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\KeywordTok{vars}\NormalTok{(line), }
             \DataTypeTok{scales =} \StringTok{"free_y"}\NormalTok{,}
             \DataTypeTok{nrow =} \DecValTok{4}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-267-1.pdf}

\hypertarget{pca}{%
\subsection{PCA}\label{pca}}

Principal components analysis is a really powerful exploratory tool. It allows you to pick up potential clusters and/or outliers that can help to inform model building.

Let's do a quick (and imperfect) example looking at types of delays by station.

The delay categories are a bit of a mess, and there's hundreds of them. As a simple start, let's just take the first word:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delay_}\DecValTok{2019}\NormalTok{ <-}\StringTok{ }\NormalTok{delay_}\DecValTok{2019} \OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{code_red =} \KeywordTok{case_when}\NormalTok{(}
    \KeywordTok{str_starts}\NormalTok{(code_desc, }\StringTok{"No"}\NormalTok{) }\OperatorTok{~}\StringTok{ }\KeywordTok{word}\NormalTok{(code_desc, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{),}
    \KeywordTok{str_starts}\NormalTok{(code_desc, }\StringTok{"Operator"}\NormalTok{) }\OperatorTok{~}\StringTok{ }\KeywordTok{word}\NormalTok{(code_desc, }\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{),}
    \OtherTok{TRUE} \OperatorTok{~}\StringTok{ }\KeywordTok{word}\NormalTok{(code_desc,}\DecValTok{1}\NormalTok{))}
\NormalTok{         )}
\end{Highlighting}
\end{Shaded}

Let's also just restrict the analysis to causes that happen at least 50 times over 2019. To do the PCA, the dataframe also needs to be switched to wide format:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dwide <-}\StringTok{ }\NormalTok{delay_}\DecValTok{2019} \OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(line, station_clean) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{n_obs =} \KeywordTok{n}\NormalTok{()) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(n_obs}\OperatorTok{>}\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(code_red) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{tot_delay =} \KeywordTok{n}\NormalTok{()) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(tot_delay) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(tot_delay}\OperatorTok{>}\DecValTok{50}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(line, station_clean, code_red) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{n_delay =} \KeywordTok{n}\NormalTok{()) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{pivot_wider}\NormalTok{(}\DataTypeTok{names_from =}\NormalTok{ code_red, }\DataTypeTok{values_from =}\NormalTok{ n_delay) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate_all}\NormalTok{(}\DataTypeTok{.funs =} \KeywordTok{funs}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(.), }\DecValTok{0}\NormalTok{, .)))}
\end{Highlighting}
\end{Shaded}

Do the PCA:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delay_pca <-}\StringTok{ }\KeywordTok{prcomp}\NormalTok{(dwide[,}\DecValTok{3}\OperatorTok{:}\KeywordTok{ncol}\NormalTok{(dwide)])}

\NormalTok{df_out <-}\StringTok{ }\KeywordTok{as_tibble}\NormalTok{(delay_pca}\OperatorTok{$}\NormalTok{x)}
\NormalTok{df_out <-}\StringTok{ }\KeywordTok{bind_cols}\NormalTok{(dwide }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(line, station_clean), df_out)}
\KeywordTok{head}\NormalTok{(df_out)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 40
## # Groups:   line, station_clean [6]
##   line  station_clean    PC1     PC2    PC3    PC4    PC5      PC6   PC7   PC8
##   <chr> <chr>          <dbl>   <dbl>  <dbl>  <dbl>  <dbl>    <dbl> <dbl> <dbl>
## 1 BD    BATHURST        6.50   26.9   -2.71 -10.8  -8.40  -11.7    -3.33 -4.11
## 2 BD    BAY            24.8     7.63  -2.19  -7.05  0.714   3.90   -2.29 -4.14
## 3 BD    BLOOR         -62.4  -112.    57.3  -23.4  -5.09  -14.1    13.7   5.06
## 4 BD    BROADVIEW      -6.60   28.1   -1.06 -14.0  -6.49   -8.29   -6.29 -1.40
## 5 BD    CASTLE         23.8    11.8   -1.31  -7.93 -3.62   -3.37   -2.08 -3.48
## 6 BD    CHESTER        24.6    -1.87 -18.6    2.75  1.85    0.0736  3.79 -1.27
## # ... with 30 more variables: PC9 <dbl>, PC10 <dbl>, PC11 <dbl>, PC12 <dbl>,
## #   PC13 <dbl>, PC14 <dbl>, PC15 <dbl>, PC16 <dbl>, PC17 <dbl>, PC18 <dbl>,
## #   PC19 <dbl>, PC20 <dbl>, PC21 <dbl>, PC22 <dbl>, PC23 <dbl>, PC24 <dbl>,
## #   PC25 <dbl>, PC26 <dbl>, PC27 <dbl>, PC28 <dbl>, PC29 <dbl>, PC30 <dbl>,
## #   PC31 <dbl>, PC32 <dbl>, PC33 <dbl>, PC34 <dbl>, PC35 <dbl>, PC36 <dbl>,
## #   PC37 <dbl>, PC38 <dbl>
\end{verbatim}

Plot the first two PCs, and label some outlying stations:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(df_out,}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{PC1,}\DataTypeTok{y=}\NormalTok{PC2,}\DataTypeTok{color=}\NormalTok{line )) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_text_repel}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ df_out }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(PC2}\OperatorTok{>}\DecValTok{100}\OperatorTok{|}\NormalTok{PC1}\OperatorTok{<}\DecValTok{100}\OperatorTok{*-}\DecValTok{1}\NormalTok{), }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{label =}\NormalTok{ station_clean))}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-271-1.pdf}

Plot the factor loadings. Some evidence of public v operator?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df_out_r <-}\StringTok{ }\KeywordTok{as_tibble}\NormalTok{(delay_pca}\OperatorTok{$}\NormalTok{rotation)}
\NormalTok{df_out_r}\OperatorTok{$}\NormalTok{feature <-}\StringTok{ }\KeywordTok{colnames}\NormalTok{(dwide[,}\DecValTok{3}\OperatorTok{:}\KeywordTok{ncol}\NormalTok{(dwide)])}

\NormalTok{df_out_r}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 38 x 39
##         PC1      PC2      PC3      PC4      PC5      PC6      PC7     PC8
##       <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>   <dbl>
##  1 -0.0412   0.0638   1.33e-2 -4.67e-2  0.0246   0.0184  -0.00363  0.0198
##  2 -0.0332  -0.00469 -4.14e-2 -7.51e-3  0.0201  -0.0122  -0.0914  -0.0903
##  3 -0.135    0.207    2.37e-2 -1.44e-1  0.135   -0.0381  -0.00931 -0.320 
##  4 -0.0652   0.0475  -4.43e-2 -2.51e-2 -0.00139 -0.0748  -0.144   -0.428 
##  5 -0.00443  0.00878 -4.99e-5 -8.30e-4  0.00967  0.00954 -0.0160  -0.0144
##  6 -0.0268  -0.00722 -4.39e-3  5.34e-4 -0.0151  -0.0125  -0.00381 -0.0423
##  7 -0.0813   0.0960  -4.62e-2  4.79e-2 -0.0978  -0.0365  -0.0766   0.278 
##  8 -0.0117   0.0135   5.48e-3 -2.94e-2  0.0125   0.0377  -0.0790  -0.0321
##  9 -0.516    0.655   -1.77e-2 -1.62e-1 -0.221   -0.287   -0.184    0.0465
## 10 -0.151    0.0826   5.48e-2  3.52e-1 -0.397    0.281    0.110    0.477 
## # ... with 28 more rows, and 31 more variables: PC9 <dbl>, PC10 <dbl>,
## #   PC11 <dbl>, PC12 <dbl>, PC13 <dbl>, PC14 <dbl>, PC15 <dbl>, PC16 <dbl>,
## #   PC17 <dbl>, PC18 <dbl>, PC19 <dbl>, PC20 <dbl>, PC21 <dbl>, PC22 <dbl>,
## #   PC23 <dbl>, PC24 <dbl>, PC25 <dbl>, PC26 <dbl>, PC27 <dbl>, PC28 <dbl>,
## #   PC29 <dbl>, PC30 <dbl>, PC31 <dbl>, PC32 <dbl>, PC33 <dbl>, PC34 <dbl>,
## #   PC35 <dbl>, PC36 <dbl>, PC37 <dbl>, PC38 <dbl>, feature <chr>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(df_out_r,}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{PC1,}\DataTypeTok{y=}\NormalTok{PC2,}\DataTypeTok{label=}\NormalTok{feature )) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_text_repel}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-272-1.pdf}

\hypertarget{exercises}{%
\section{Exercises}\label{exercises}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Using the \texttt{opendatatoronto} package, download the data on mayoral campaign contributions for 2014. (note: the 2014 file you will get from \texttt{get\_resource}, so just keep the sheet that relates to the Mayor election).
\item
  Clean up the data format (fixing the parsing issue and standardizing the column names using \texttt{janitor})
\item
  Summarize the variables in the dataset. Are there missing values, and if so, should we be worried about them? Is every variable in the format it should be? If not, create new variable(s) that are in the right format.
\item
  Visually explore the distribution of values of the contributions. What contributions are notable outliers? Do they share a similar characteristic(s)? It may be useful to plot the distribution of contributions without these outliers to get a better sense of the majority of the data.
\item
  List the top five candidates in each of these categories:

  \begin{itemize}
  \tightlist
  \item
    total contributions
  \item
    mean contribution
  \item
    number of contributions
  \end{itemize}
\item
  Repeat 5 but without contributions from the candidates themselves.
\item
  How many contributors gave money to more than one candidate?
\end{enumerate}

\hypertarget{case-study---opinions-about-a-casino-in-toronto}{%
\section{Case study - Opinions about a casino in Toronto}\label{case-study---opinions-about-a-casino-in-toronto}}

\textbf{This was written by Michael Chong.}

\hypertarget{data-preparation}{%
\subsection{Data preparation}\label{data-preparation}}

\hypertarget{getting-data-from-opendatatoronto}{%
\subsubsection{\texorpdfstring{Getting data from \texttt{opendatatoronto}}{Getting data from opendatatoronto}}\label{getting-data-from-opendatatoronto}}

Here we use the \texttt{opendatatoronto} package again. See the previous example RMarkdown file for a deeper explanation of how the code below works.

The dataset I'm extracting below are the results from a survey in 2012 regarding the establishment of a casino in Toronto. More info available by following \href{https://open.toronto.ca/dataset/casino-survey-results/}{this link}. In this analysis, we'll be hoping to address the question, \textbf{which demographic (age/gender) groups are more likely to be supportive of a new casino in Toronto?}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Get the data}
\NormalTok{casino_resource <-}\StringTok{ }\KeywordTok{search_packages}\NormalTok{(}\StringTok{"casino survey"}\NormalTok{)}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{list_package_resources}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(name }\OperatorTok{==}\StringTok{ "toronto-casino-survey-results"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{get_resource}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{getting-the-right-kind-of-object}{%
\subsubsection{Getting the right kind of object}\label{getting-the-right-kind-of-object}}

The object \texttt{casino\_resource} isn't quite useable yet, because it's (inconveniently) stored as a \texttt{list} of 2 data frames:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Check what kind of object the casino_resource object is}
\KeywordTok{class}\NormalTok{(casino_resource)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "list"
\end{verbatim}

If we just return the object, we can see that the 2nd list item is empty, and we just want to keep the first one:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{casino_resource}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $tblSurvey
## # A tibble: 17,766 x 94
##    SurveyID Q1_A  Q1_B1 Q1_B2 Q1_B3 Q2_A  Q2_B  Q3_A  Q3_B  Q3_C  Q3_D  Q3_E 
##       <dbl> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>
##  1        1 Stro~ Do n~ Do n~ Do n~ Does~ "As ~ Not ~ Very~ Not ~ Not ~ Not ~
##  2        2 Stro~ Econ~ Jobs  Arts~ Fits~ "Cos~ Very~ Very~ Very~ Very~ Very~
##  3        3 Stro~ Ther~ If t~ <NA>  Fits~ "Big~ Very~ Very~ Very~ Very~ Very~
##  4        4 Some~ beli~ mone~ evid~ Does~ "My ~ Very~ Very~ Some~ Some~ Very~
##  5        5 Neut~ Like~ Conc~ <NA>  Neut~ "Aga~ Very~ Very~ Very~ Not ~ Very~
##  6        6 Stro~ have~ <NA>  <NA>  Does~ "Tor~ Not ~ Not ~ Not ~ Not ~ Not ~
##  7        7 Stro~ The ~ Peop~ We s~ Does~ "#3 ~ Not ~ Not ~ Not ~ Not ~ Not ~
##  8        8 Stro~ It w~ Mora~ <NA>  Does~ "Cas~ Very~ Very~ Very~ Very~ Very~
##  9        9 Stro~ It's~ traf~ heal~ Does~ "No ~ Not ~ Very~ Not ~ Not ~ Some~
## 10       10 Stro~ Toro~ Avoi~ Prov~ Fits~ "Tor~ Very~ Very~ Very~ Not ~ Very~
## # ... with 17,756 more rows, and 82 more variables: Q3_F <chr>, Q3_G <chr>,
## #   Q3_H <chr>, Q3_I <chr>, Q3_J <chr>, Q3_K <chr>, Q3_L <chr>, Q3_M <chr>,
## #   Q3_N <chr>, Q3_O <chr>, Q3_P <chr>, Q3_Q <chr>, Q3_Q_Other <chr>,
## #   Q3_Comments <chr>, Q4_A <chr>, Q5 <chr>, Q6 <chr>, Q6_Comments <chr>,
## #   Q7_A_StandAlone <chr>, Q7_A_Integrated <chr>, Q7_A1 <chr>, Q7_A2 <chr>,
## #   Q7_A3 <chr>, Q7_A_A <chr>, Q7_A_B <chr>, Q7_A_C <chr>, Q7_A_D <chr>,
## #   Q7_A_E <chr>, Q7_A_F <chr>, Q7_A_G <chr>, Q7_A_H <chr>, Q7_A_I <chr>,
## #   Q7_A_J <chr>, Q7_A_J_Other <chr>, Q7_B_StandAlone <chr>,
## #   Q7_B_Integrated <chr>, Q7_B1 <chr>, Q7_B2 <chr>, Q7_B3 <chr>, Q7_B_A <chr>,
## #   Q7_B_B <chr>, Q7_B_C <chr>, Q7_B_D <chr>, Q7_B_E <chr>, Q7_B_F <chr>,
## #   Q7_B_G <chr>, Q7_B_H <chr>, Q7_B_I <chr>, Q7_B_J <chr>, Q7_B_J_Other <chr>,
## #   Q7_C_StandAlone <chr>, Q7_C_Integrated <chr>, Q7_C1 <chr>, Q7_C2 <chr>,
## #   Q7_C3 <chr>, Q7_C_A <chr>, Q7_C_B <chr>, Q7_C_C <chr>, Q7_C_D <chr>,
## #   Q7_C_E <chr>, Q7_C_F <chr>, Q7_C_G <chr>, Q7_C_H <chr>, Q7_C_I <chr>,
## #   Q7_C_J <chr>, Q7_C_J_Other <chr>, Q8_A1 <chr>, Q8_A2 <chr>, Q8_B1 <chr>,
## #   Q8_B2 <chr>, Q8_B3 <chr>, Q9 <chr>, Q9_Considerations <chr>, Q10 <chr>,
## #   Q11 <chr>, Age <chr>, Gender <chr>, PostalCode <chr>, GroupName <chr>,
## #   DateCreated <dttm>, ...93 <lgl>, ...94 <lgl>
## 
## $Sheet1
## # A tibble: 0 x 0
\end{verbatim}

So, let's only keep the first item by indexing the list with double square brackets:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{casino_data <-}\StringTok{ }\NormalTok{casino_resource[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\hypertarget{cleaning-up-the-dataframe}{%
\subsubsection{Cleaning up the dataframe}\label{cleaning-up-the-dataframe}}

Let's check out what the first couple rows of the dataframe looks like. By default, \texttt{head()} returns the first 6 rows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(casino_data) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 94
##   SurveyID Q1_A  Q1_B1 Q1_B2 Q1_B3 Q2_A  Q2_B  Q3_A  Q3_B  Q3_C  Q3_D  Q3_E 
##      <dbl> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>
## 1        1 Stro~ Do n~ Do n~ Do n~ Does~ "As ~ Not ~ Very~ Not ~ Not ~ Not ~
## 2        2 Stro~ Econ~ Jobs  Arts~ Fits~ "Cos~ Very~ Very~ Very~ Very~ Very~
## 3        3 Stro~ Ther~ If t~ <NA>  Fits~ "Big~ Very~ Very~ Very~ Very~ Very~
## 4        4 Some~ beli~ mone~ evid~ Does~ "My ~ Very~ Very~ Some~ Some~ Very~
## 5        5 Neut~ Like~ Conc~ <NA>  Neut~ "Aga~ Very~ Very~ Very~ Not ~ Very~
## 6        6 Stro~ have~ <NA>  <NA>  Does~ "Tor~ Not ~ Not ~ Not ~ Not ~ Not ~
## # ... with 82 more variables: Q3_F <chr>, Q3_G <chr>, Q3_H <chr>, Q3_I <chr>,
## #   Q3_J <chr>, Q3_K <chr>, Q3_L <chr>, Q3_M <chr>, Q3_N <chr>, Q3_O <chr>,
## #   Q3_P <chr>, Q3_Q <chr>, Q3_Q_Other <chr>, Q3_Comments <chr>, Q4_A <chr>,
## #   Q5 <chr>, Q6 <chr>, Q6_Comments <chr>, Q7_A_StandAlone <chr>,
## #   Q7_A_Integrated <chr>, Q7_A1 <chr>, Q7_A2 <chr>, Q7_A3 <chr>, Q7_A_A <chr>,
## #   Q7_A_B <chr>, Q7_A_C <chr>, Q7_A_D <chr>, Q7_A_E <chr>, Q7_A_F <chr>,
## #   Q7_A_G <chr>, Q7_A_H <chr>, Q7_A_I <chr>, Q7_A_J <chr>, Q7_A_J_Other <chr>,
## #   Q7_B_StandAlone <chr>, Q7_B_Integrated <chr>, Q7_B1 <chr>, Q7_B2 <chr>,
## #   Q7_B3 <chr>, Q7_B_A <chr>, Q7_B_B <chr>, Q7_B_C <chr>, Q7_B_D <chr>,
## #   Q7_B_E <chr>, Q7_B_F <chr>, Q7_B_G <chr>, Q7_B_H <chr>, Q7_B_I <chr>,
## #   Q7_B_J <chr>, Q7_B_J_Other <chr>, Q7_C_StandAlone <chr>,
## #   Q7_C_Integrated <chr>, Q7_C1 <chr>, Q7_C2 <chr>, Q7_C3 <chr>, Q7_C_A <chr>,
## #   Q7_C_B <chr>, Q7_C_C <chr>, Q7_C_D <chr>, Q7_C_E <chr>, Q7_C_F <chr>,
## #   Q7_C_G <chr>, Q7_C_H <chr>, Q7_C_I <chr>, Q7_C_J <chr>, Q7_C_J_Other <chr>,
## #   Q8_A1 <chr>, Q8_A2 <chr>, Q8_B1 <chr>, Q8_B2 <chr>, Q8_B3 <chr>, Q9 <chr>,
## #   Q9_Considerations <chr>, Q10 <chr>, Q11 <chr>, Age <chr>, Gender <chr>,
## #   PostalCode <chr>, GroupName <chr>, DateCreated <dttm>, ...93 <lgl>,
## #   ...94 <lgl>
\end{verbatim}

Unfortunately the column names aren't very informative. For simplicity, we'll use the \texttt{.pdf} questionnaire that accompanies this dataset from the Toronto Open Data website. Alternatively, we could get and parse the \texttt{readme} through the R package.

\href{https://ckan0.cf.opendata.inter.prod-toronto.ca/dataset/427ca4cd-168a-4a37-883d-4a574277caf5/resource/ae135d6a-6921-4905-bc79-516fcd428b7b/download/toronto-casino-survey-feedback-form.pdf}{Here's a link to the questionnaire}.

Question 1 indicates the level of support for a casino in Toronto. We'll use this as the response variable.

Concerning potential predictor variables, most of the questions ask respondents about their opinions on different aspects of a potential casino development, which aren't particularly useful towards our cause. The only demographic variables are \texttt{Age} and \texttt{Gender}, so let's choose these.

Here I'm also going to rename the columns so that my resulting data frame has columns \texttt{opinion}, \texttt{age}, and \texttt{gender}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Narrow down the dataframe to our variables of interest}
\NormalTok{casino_data <-}\StringTok{ }\NormalTok{casino_data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(Q1_A, Age, Gender) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{rename}\NormalTok{(}\DataTypeTok{opinion =}\NormalTok{ Q1_A, }\DataTypeTok{age =}\NormalTok{ Age, }\DataTypeTok{gender =}\NormalTok{ Gender)}

\CommentTok{# Look at first couple rows:}
\KeywordTok{head}\NormalTok{(casino_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 3
##   opinion                   age   gender
##   <chr>                     <chr> <chr> 
## 1 Strongly Opposed          25-34 Male  
## 2 Strongly in Favour        35-44 Female
## 3 Strongly in Favour        55-64 Male  
## 4 Somewhat Opposed          25-34 Male  
## 5 Neutral or Mixed Feelings 25-34 Female
## 6 Strongly Opposed          45-54 Female
\end{verbatim}

\hypertarget{some-visual-exploration-and-more-cleanup-of-course}{%
\subsection{Some visual exploration (and more cleanup, of course)}\label{some-visual-exploration-and-more-cleanup-of-course}}

Let's first do some quick exploration to get a feel for what's going on in the data. We'll first calculate proportions of casino support for each age-gender combination:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Calculate proportions}
\NormalTok{casino_summary <-}\StringTok{ }\NormalTok{casino_data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(age, gender, opinion) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{n =} \KeywordTok{n}\NormalTok{()) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# Count the number in each group and response}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(age, gender) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{prop =}\NormalTok{ n}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(n)) }\CommentTok{# Calculate proportions within each group}
\end{Highlighting}
\end{Shaded}

Some notes:
* we use \texttt{geom\_col()} to make a bar chart,
* \texttt{facet\_grid()} modifies the plot so that the plot has panels that correspond only to certain values of discrete variables (in this case, we will ``facet'' by \texttt{age} and \texttt{gender}). This is helpful in this case because we are interested in how the distribution of \texttt{opinion}s changes by \texttt{age} and \texttt{gender}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(casino_summary) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ opinion, }\DataTypeTok{y =}\NormalTok{ prop)) }\OperatorTok{+}\StringTok{ }\CommentTok{# Specify a histogram of opinion responses}
\StringTok{  }\KeywordTok{facet_grid}\NormalTok{(age}\OperatorTok{~}\NormalTok{gender) }\OperatorTok{+}\StringTok{ }\CommentTok{#Facet by age and gender}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{90}\NormalTok{)) }\CommentTok{# Rotate the x-axis labels to be readable}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-280-1.pdf}

Some things to note:

\begin{itemize}
\tightlist
\item
  the x-axis labels are out of order in the sense that they are not in a monotone order of increasing/decreasing support
\item
  there are \texttt{NA} values in \texttt{opinion}, \texttt{age}, and \texttt{gender}, as well as ``Prefer not to disclose'' responses
\end{itemize}

\hypertarget{getting-the-data-into-a-more-model-suitable-format}{%
\subsubsection{Getting the data into a more model-suitable format}\label{getting-the-data-into-a-more-model-suitable-format}}

\hypertarget{get-rid-of-responses-that-arent-suitable}{%
\paragraph{Get rid of responses that aren't suitable}\label{get-rid-of-responses-that-arent-suitable}}

For simplicity we'll assume that \texttt{NA} values and ``Prefer not to disclose'' responses occur randomly, and remove them from our dataset (note in reality this assumption might not hold up and we might want to be more careful). Let's check how many rows are in the original dataset:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# nrow() returns the number of rows in a dataframe:}
\KeywordTok{nrow}\NormalTok{(casino_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 17766
\end{verbatim}

Now let's \texttt{filter()} accordingly to omit the responses we don't want. In case you're unfamiliar, I'm going to make use of:

\begin{itemize}
\tightlist
\item
  \texttt{is.na()}, which returns \texttt{TRUE} if the argument is \texttt{NA},
\item
  the \texttt{!} operator, which flips \texttt{TRUE} and \texttt{FALSE}. So for instance, \texttt{!is.na(x)} will return \texttt{TRUE} if \texttt{x} is NOT \texttt{NA}, which is what we want to keep.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{casino_data <-}\StringTok{ }\NormalTok{casino_data }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# Only keep rows with non-NA:}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(opinion), }\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(age), }\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(gender)) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# Only keep rows where age and gender are disclosed:}
\StringTok{  }\KeywordTok{filter}\NormalTok{(age }\OperatorTok{!=}\StringTok{ "Prefer not to disclose"}\NormalTok{, gender }\OperatorTok{!=}\StringTok{ "Prefer not to disclose"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's check how many rows of data we're left with:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{nrow}\NormalTok{(casino_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 13658
\end{verbatim}

\hypertarget{convert-response-variable-into-binary}{%
\paragraph{Convert response variable into binary}\label{convert-response-variable-into-binary}}

To clean up the first problem (response variables out of order), we might as well take this opportunity to convert these into a format suitable for our model. In a logistic regression, we would like our response variable to be binary, but in this case we have 5 possible categories ranging from ``Strongly Opposed'' to ``Strongly in Favour''. We'll recategorize them into a new \texttt{supportive\_or\_not} variable as follows.

\begin{itemize}
\tightlist
\item
  \texttt{supportive\ =\ 1} if ``Strongly in Favour'' or ``Somewhat in Favour''
\item
  \texttt{supportive\ =\ 0} if ``Neutral or Mixed Feelings'', ``Somewhat Opposed'', or ``Strongly Opposed''
\end{itemize}

We do this with the \texttt{mutate()} function, which creates new columns (possibly as functions of existing columns), and \texttt{case\_when()}, which provides a way to assign values conditional on if-statements. The syntax here is a little strange. On the LHS of the \texttt{\textasciitilde{}} is the ``if'' condition, and the RHS of the tilde is the value to return. For example, \texttt{x\ ==\ 0\ \textasciitilde{}\ 3} would return 3 when \texttt{x} is 0.

Another commonly used operator here is the \texttt{\%in\%} operator, which checks whether something is an element of a vector. E.g.:

\begin{itemize}
\tightlist
\item
  \texttt{1\ \%in\%\ c(1,\ 3,\ 4)} returns \texttt{TRUE}
\item
  \texttt{2\ \%in\%\ c(1,\ 3,\ 4)} returns \texttt{FALSE}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Store possible opinions in vectors}
\NormalTok{yes_opinions <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Strongly in Favour"}\NormalTok{, }\StringTok{"Somewhat in Favour"}\NormalTok{)}
\NormalTok{no_opinions <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Neutral or Mixed Feelings"}\NormalTok{, }\StringTok{"Somewhat Opposed"}\NormalTok{, }\StringTok{"Strongly Opposed"}\NormalTok{)}

\CommentTok{# Create `supportive` column:}
\NormalTok{casino_data <-}\StringTok{ }\NormalTok{casino_data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{supportive =} \KeywordTok{case_when}\NormalTok{(}
\NormalTok{    opinion }\OperatorTok{%in%}\StringTok{ }\NormalTok{yes_opinions }\OperatorTok{~}\StringTok{ }\OtherTok{TRUE}\NormalTok{, }\CommentTok{# Assign TRUE}
\NormalTok{    opinion }\OperatorTok{%in%}\StringTok{ }\NormalTok{no_opinions }\OperatorTok{~}\StringTok{ }\OtherTok{FALSE}  \CommentTok{# Assign FALSE}
\NormalTok{  ))}
\end{Highlighting}
\end{Shaded}

\hypertarget{convert-age-to-a-numeric-variable}{%
\paragraph{Convert age to a numeric variable}\label{convert-age-to-a-numeric-variable}}

Age in this survey is given in age groups. Let's instead treat it map it to a numeric variable so that we can more easily talk about trends with age. We'll map the youngest age to \texttt{1}, and so on:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{casino_data <-}\StringTok{ }\NormalTok{casino_data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{age_group =} \KeywordTok{case_when}\NormalTok{(}
\NormalTok{    age }\OperatorTok{==}\StringTok{ "Under 15"} \OperatorTok{~}\StringTok{ }\DecValTok{0}\NormalTok{,}
\NormalTok{    age }\OperatorTok{==}\StringTok{ "15-24"} \OperatorTok{~}\StringTok{ }\DecValTok{1}\NormalTok{,}
\NormalTok{    age }\OperatorTok{==}\StringTok{ "25-34"} \OperatorTok{~}\StringTok{ }\DecValTok{2}\NormalTok{,}
\NormalTok{    age }\OperatorTok{==}\StringTok{ "35-44"} \OperatorTok{~}\StringTok{ }\DecValTok{3}\NormalTok{,}
\NormalTok{    age }\OperatorTok{==}\StringTok{ "45-54"} \OperatorTok{~}\StringTok{ }\DecValTok{4}\NormalTok{,}
\NormalTok{    age }\OperatorTok{==}\StringTok{ "55-64"} \OperatorTok{~}\StringTok{ }\DecValTok{5}\NormalTok{,}
\NormalTok{    age }\OperatorTok{==}\StringTok{ "65 or older"} \OperatorTok{~}\StringTok{ }\DecValTok{6}
\NormalTok{  ))}
\end{Highlighting}
\end{Shaded}

Now let's make the same plot again, with our new processed data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{casino_summary2 <-}\StringTok{ }\NormalTok{casino_data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(age_group, gender, supportive) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{n =} \KeywordTok{n}\NormalTok{()) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# Count the number in each group and response}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(age_group, gender) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{prop =}\NormalTok{ n}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(n)) }\CommentTok{# Calculate proportions within each group}

\KeywordTok{ggplot}\NormalTok{(casino_summary2) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_grid}\NormalTok{(age_group }\OperatorTok{~}\StringTok{ }\NormalTok{gender) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ supportive, }\DataTypeTok{y =}\NormalTok{ prop)) }
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-286-1.pdf}

We can sort of see some difference in the distribution between different panels. To formalize this, we can run a logistic regression.

\hypertarget{logistic-regression}{%
\subsection{Logistic Regression}\label{logistic-regression}}

Now, we're set up to feed it to the regression. We can do this with \texttt{glm()}, which allows us to fit generalized linear models.

We use \texttt{family\ =\ "binomial"} to specify a logistic regression, and our formula is \texttt{supportive\ \textasciitilde{}\ age\_group\ +\ gender}, which indicates that \texttt{supportive} is the (binary) response variable since it's on the LHS, and \texttt{age\_group} and \texttt{gender} are our predictor variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{casino_glm <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(supportive }\OperatorTok{~}\StringTok{ }\NormalTok{age_group }\OperatorTok{+}\StringTok{ }\NormalTok{gender, }\DataTypeTok{data =}\NormalTok{ casino_data, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can take a look at the results of running the GLM using \texttt{summary()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(casino_glm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = supportive ~ age_group + gender, family = "binomial", 
##     data = casino_data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.0107  -0.8888  -0.6804   1.4249   1.8822  
## 
## Coefficients:
##                     Estimate Std. Error z value Pr(>|z|)    
## (Intercept)         -1.10594    0.05863 -18.862  < 2e-16 ***
## age_group           -0.07983    0.01376  -5.801 6.59e-09 ***
## genderMale           0.70036    0.04027  17.390  < 2e-16 ***
## genderTransgendered  0.69023    0.39276   1.757   0.0789 .  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 16010  on 13657  degrees of freedom
## Residual deviance: 15653  on 13654  degrees of freedom
## AIC: 15661
## 
## Number of Fisher Scoring iterations: 4
\end{verbatim}

\hypertarget{interpretation}{%
\subsubsection{Interpretation}\label{interpretation}}

Interpretation can be a little tricky. Here are some important things to note about our results:

\hypertarget{numeric-age-group-variable}{%
\paragraph{Numeric age group variable}\label{numeric-age-group-variable}}

Remember that we coded \texttt{age\_group} as numbers 1 to 5. Because we've used age groups instead of age, we have to be careful with how we phrase our conclusion. The coefficient estimate corresponds to the effect of moving up a unit on the age group scale (e.g.~from the 25-34 age group to the 35-44 age group), rather than 1 year in age (e.g.~from age 28 to 29).

\hypertarget{log-odds-ratios}{%
\paragraph{log-odds ratios}\label{log-odds-ratios}}

The effect estimates are on the log-odds scale. This means the effect of -0.07983 for \texttt{age\_group} is interpreted as: \emph{for each unit increase in \texttt{age\_group}, we estimate a 0.07983 decrease in the log-odds of being supportive of a casino}.

We could exponentiate the coefficient estimate to make this at least a little easier to interpret. The number we get is interpreted as a factor for the odds.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{exp}\NormalTok{(}\OperatorTok{-}\FloatTok{0.07983}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9232733
\end{verbatim}

So our (cleaner) interpretation is:
\emph{the odds of an individuals of the same gender being pro-casino are predicted to change by a factor of 0.9232733 for each unit increase in \texttt{age\_group}}

\hypertarget{baseline-category}{%
\subsubsection{Baseline category}\label{baseline-category}}

First, note that because we have categorical variables, the \texttt{gender} coefficients are \emph{relative} to a ``baseline'' category. The value of \texttt{gender} that doesn't appear in the table, \texttt{Female}, is implicitly used as our baseline gender category.

Technical note: if the variable is stored as a \texttt{character} class, then \texttt{glm()} will choose the alphabetically first value to use as the baseline.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{exp}\NormalTok{(}\FloatTok{0.70036}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.014478
\end{verbatim}

So, the interpretation of the \texttt{genderMale} coefficient is: \emph{the odds of a male individual supporting a casino is 2.0144778 times higher than a female individual of the same \texttt{age\_group}.}

\hypertarget{making-estimates}{%
\subsubsection{Making estimates}\label{making-estimates}}

\hypertarget{a-manual-way}{%
\paragraph{A manual way}\label{a-manual-way}}

Using the formula found in ISLR 4.3.3, we can make estimates for an individual of certain characteristics. Suppose we wanted to predict the the probability of supporting a Toronto casino for an individual who was 36 and identified as transgender. Then:

\begin{itemize}
\tightlist
\item
  \texttt{age\_group} takes a value of 3, since they are in the age group of 35-44 coded as 3,
\item
  \texttt{genderTransgendered} takes a value of 1
\end{itemize}

First, let's extract the coefficient estimates as a vector using \texttt{coefficients()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coefs <-}\StringTok{ }\KeywordTok{coefficients}\NormalTok{(casino_glm)}
\NormalTok{coefs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         (Intercept)           age_group          genderMale genderTransgendered 
##         -1.10593925         -0.07983372          0.70036199          0.69022910
\end{verbatim}

Since this vector is labelelled, we can index it using square brackets and names. For instance:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coefs[}\StringTok{"age_group"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   age_group 
## -0.07983372
\end{verbatim}

So first let's evaluate the exponent term \(e^{\beta_0 + \cdots + \beta_p X_p}\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{exp_term <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(coefs[}\StringTok{"(Intercept)"}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{coefs[}\StringTok{"age_group"}\NormalTok{]}\OperatorTok{*}\DecValTok{3} \OperatorTok{+}\StringTok{ }\NormalTok{coefs[}\StringTok{"genderTransgendered"}\NormalTok{]}\OperatorTok{*}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now evaluate the expression that gives the probability of casino support:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# The unname() command just takes off the label that it "inherited" from the coefs vector.}
\CommentTok{# (don't worry about it, doesn't affect any functionality)}
\KeywordTok{unname}\NormalTok{(exp_term }\OperatorTok{/}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{exp_term))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3418161
\end{verbatim}

\hypertarget{a-more-streamlined-way}{%
\paragraph{A more streamlined way}\label{a-more-streamlined-way}}

Thankfully R comes with a convenient function to make prediction estimates from a \texttt{glm()}. We do this using the \texttt{predict()} function. First, we need to make a dataframe that has the relevant variables and values that we're interested in predicting. We'll use the same values as before:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prediction_df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{age_group =} \DecValTok{3}\NormalTok{, }\DataTypeTok{gender =} \StringTok{"Transgendered"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The dataframe looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prediction_df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   age_group        gender
## 1         3 Transgendered
\end{verbatim}

Then we feed it into the \texttt{predict()} function, along with our \texttt{glm} object. To get the probability, we need to specify \texttt{type\ =\ "response"}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{predict}\NormalTok{(casino_glm, }\DataTypeTok{newdata =}\NormalTok{ prediction_df, }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         1 
## 0.3418161
\end{verbatim}

This matches the probability we got from doing this manually, yay!

\hypertarget{case-study---historical-canadian-elections}{%
\section{Case study - Historical Canadian elections}\label{case-study---historical-canadian-elections}}

\hypertarget{case-study---airbnb-listing-in-toronto}{%
\section{Case study - Airbnb listing in Toronto}\label{case-study---airbnb-listing-in-toronto}}

\hypertarget{essentials}{%
\subsection{Essentials}\label{essentials}}

In this case study we\ldots{}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
  Kommenda, Niko, Helen Pidd and Libby Brooks, 2020, `Revealed: the areas in the UK with one Airbnb for every four homes', \emph{The Guardian}, 20 February, freely available at: \url{https://www.theguardian.com/technology/2020/feb/20/revealed-the-areas-in-the-uk-with-one-airbnb-for-every-four-homes}.
\end{itemize}

\hypertarget{set-up-1}{%
\subsection{Set up}\label{set-up-1}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(broom) }\CommentTok{# Helps with model outputs etc}
\KeywordTok{library}\NormalTok{(here) }\CommentTok{# Helps with specifying path names}
\KeywordTok{library}\NormalTok{(janitor) }\CommentTok{# Helps with initial data cleaning and pretty tables}
\KeywordTok{library}\NormalTok{(tidymodels) }\CommentTok{# Help with modelling}
\KeywordTok{library}\NormalTok{(tidyverse) }
\KeywordTok{library}\NormalTok{(skimr) }\CommentTok{# Helps with initial data visualisation}
\KeywordTok{library}\NormalTok{(visdat) }\CommentTok{# Helps check missing values}
\end{Highlighting}
\end{Shaded}

\hypertarget{get-data}{%
\subsection{Get data}\label{get-data}}

The dataset is from Inside Airbnb (\url{http://insideairbnb.com}).

We can give \texttt{read\_csv()} a link to where the dataset is and it will download it. This helps with reproducibility because the source is clear. But, as that link could change at any time, longer-term reproducibility, as well as wanting to minimise the effect on the Inside Airbnb servers, suggest that we should also save a local copy of the data and then use that. (As the original data is not ours, we should not make that public without first getting written permission.)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# For reproducibility}
\CommentTok{# airbnb_data_reduced <- read_csv("http://data.insideairbnb.com/canada/on/toronto/2019-12-07/data/listings.csv.gz", guess_max = 20000)}
\CommentTok{# write_csv(airbnb_data_reduced, "week_6/data/airbnb_toronto_2019-12-07.csv")}

\CommentTok{# For actual work}
\NormalTok{airbnb_data <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(here}\OperatorTok{::}\KeywordTok{here}\NormalTok{(}\StringTok{"dont_push/airbnb_toronto_2019-12-07.csv"}\NormalTok{), }\DataTypeTok{guess_max =} \DecValTok{20000}\NormalTok{)}

\CommentTok{# The guess_max option in read_csv helps us avoid having to specify the column types. Usually read_csv takes a best guess at the column types based on the first few rows. But sometimes those first ones are misleading and so guess_max forces it to look at a larger number of rows to try to work out what is going on.}
\end{Highlighting}
\end{Shaded}

\hypertarget{clean-data-1}{%
\subsection{Clean data}\label{clean-data-1}}

There are an enormous number of columns, so we'll just select a few.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(airbnb_data) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{length}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 106
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_selected <-}\StringTok{ }
\StringTok{  }\NormalTok{airbnb_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(host_id, }
\NormalTok{         host_since, }
\NormalTok{         host_response_time, }
\NormalTok{         host_is_superhost, }
\NormalTok{         host_listings_count,}
\NormalTok{         host_total_listings_count,}
\NormalTok{         host_neighbourhood, }
\NormalTok{         host_listings_count, }
\NormalTok{         neighbourhood_cleansed, }
\NormalTok{         room_type, }
\NormalTok{         bathrooms, }
\NormalTok{         bedrooms, square_feet, }
\NormalTok{         price, }
\NormalTok{         number_of_reviews, }
\NormalTok{         has_availability, }
\NormalTok{         review_scores_rating, }
\NormalTok{         review_scores_accuracy, }
\NormalTok{         review_scores_cleanliness, }
\NormalTok{         review_scores_checkin, }
\NormalTok{         review_scores_communication, }
\NormalTok{         review_scores_location, }
\NormalTok{         review_scores_value}
\NormalTok{         )}
\end{Highlighting}
\end{Shaded}

We might like to have a brief look at the dataset to see if anything weird is going on. There are a bunch of ways of doing this, but one way is `skim' from the skimr package (Waring, Quinn, McNamara, Arino de la Rubia, Zhu and Ellis, 2020).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# skim(airbnb_data_selected)}
\end{Highlighting}
\end{Shaded}

A few things jump out:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  There are a character variables that should probably be numerics or dates/times: \texttt{host\_response\_time}, price, \texttt{weekly\_price}, \texttt{monthly\_price}, \texttt{cleaning\_fee.}
\item
  Weekly and monthly price is missing in an overwhelming number of observations.
\item
  Roughly a fifth of observations are missing a review score and there seems like there is some correlation between those review-type variables.
\item
  There are two variants of the neighbourhood name.
\item
  There are NAs in \texttt{host\_is\_superhost}.
\item
  The reviews seem really skewed.
\item
  There is someone who has 328 properties on Airbnb.
\end{enumerate}

\hypertarget{price}{%
\subsubsection{Price}\label{price}}

First we need to convert to a numeric. This is a common problem, and you need to be a little careful that it doesn't all just convert to NAs. In our case if we just force the price data to be a numeric then it will go to NA because there are a lot of characters that R doesn't know how to convert, e.g.~what is the numeric for `\$'? So we need to remove those characters first.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_selected}\OperatorTok{$}\NormalTok{price }\OperatorTok{%>%}\StringTok{ }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "$469.00" "$99.00"  "$66.00"  "$72.00"  "$199.00" "$54.00"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# First work out what is going on}
\NormalTok{airbnb_data_selected}\OperatorTok{$}\NormalTok{price }\OperatorTok{%>%}\StringTok{ }\KeywordTok{str_split}\NormalTok{(}\StringTok{""}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{unlist}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{unique}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "$" "4" "6" "9" "." "0" "7" "2" "1" "5" "3" "8" ","
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# It's clear that '$' needs to go. The only odd thing is ',' so take a look at those:}
\NormalTok{airbnb_data_selected }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(price) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\KeywordTok{str_detect}\NormalTok{(price, }\StringTok{","}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 87 x 1
##    price    
##    <chr>    
##  1 $1,988.00
##  2 $1,099.00
##  3 $2,799.00
##  4 $1,100.00
##  5 $1,450.00
##  6 $1,999.00
##  7 $1,060.00
##  8 $1,300.00
##  9 $2,142.00
## 10 $1,200.00
## # ... with 77 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# It's clear that the data is just nicely formatted, but we need to remove the comma:}
\NormalTok{airbnb_data_selected <-}\StringTok{ }
\StringTok{  }\NormalTok{airbnb_data_selected }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{price =} \KeywordTok{str_remove}\NormalTok{(price, }\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{$"}\NormalTok{),}
         \DataTypeTok{price =} \KeywordTok{str_remove}\NormalTok{(price, }\StringTok{","}\NormalTok{),}
         \DataTypeTok{price =} \KeywordTok{as.integer}\NormalTok{(price)}
\NormalTok{         )}
\end{Highlighting}
\end{Shaded}

Now we can have a look at prices.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Look at distribution of price}
\NormalTok{airbnb_data_selected }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{10}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Price per night"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Number of properties"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-303-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#  We use bins with a width of 10, so that's going to aggregate prices into 10s so that we don't get overwhelmed with bars.}
\end{Highlighting}
\end{Shaded}

So we have some outliers. Let's zoom in on prices that are more than \$1,000.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Look at distribution of high prices}
\NormalTok{airbnb_data_selected }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(price }\OperatorTok{>}\StringTok{ }\DecValTok{1000}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{10}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Price per night"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Number of properties"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-304-1.pdf}

Let's look in more detail at those with a price more than \$4,999

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_selected }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(price }\OperatorTok{>}\StringTok{ }\DecValTok{4999}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 22 x 22
##    host_id host_since host_response_t~ host_is_superho~ host_listings_c~
##      <dbl> <date>     <chr>            <lgl>                       <dbl>
##  1  9.91e7 2016-10-10 N/A              FALSE                           7
##  2  5.29e6 2013-03-02 within an hour   TRUE                            4
##  3  1.20e8 2017-03-07 N/A              FALSE                           1
##  4  8.46e6 2013-08-27 N/A              FALSE                           2
##  5  1.47e8 2017-08-22 N/A              FALSE                           2
##  6  6.74e7 2016-04-16 N/A              FALSE                           1
##  7  2.58e8 2019-04-24 within an hour   TRUE                            5
##  8  2.58e8 2019-04-24 within an hour   TRUE                            5
##  9  2.71e8 2019-06-24 a few days or m~ FALSE                           1
## 10  2.72e8 2019-06-27 within an hour   FALSE                           4
## # ... with 12 more rows, and 17 more variables:
## #   host_total_listings_count <dbl>, host_neighbourhood <chr>,
## #   neighbourhood_cleansed <chr>, room_type <chr>, bathrooms <dbl>,
## #   bedrooms <dbl>, square_feet <dbl>, price <int>, number_of_reviews <dbl>,
## #   has_availability <lgl>, review_scores_rating <dbl>,
## #   review_scores_accuracy <dbl>, review_scores_cleanliness <dbl>,
## #   review_scores_checkin <dbl>, review_scores_communication <dbl>,
## #   review_scores_location <dbl>, review_scores_value <dbl>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# It's pretty clear that there is something odd going on with some of these, but some of them seem legit.}
\end{Highlighting}
\end{Shaded}

Let's look at the distribution of prices that are in a `reasonable' range, which until Monica is a full professor, will be defined as a nightly price of less than \$1,000.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_selected }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(price }\OperatorTok{<}\StringTok{ }\DecValTok{1000}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{10}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Price per night"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Number of properties"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-306-1.pdf}

Interestingly it looks like there is some bunching of prices, possible around numbers ending in zero or nine? Let's just zoom in on prices between \$90 and \$210, out of interest, but change the bins to be smaller.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Look at distribution of price again}
\NormalTok{airbnb_data_selected }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(price }\OperatorTok{>}\StringTok{ }\DecValTok{90}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(price }\OperatorTok{<}\StringTok{ }\DecValTok{210}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ price)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Price per night"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Number of properties"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-307-1.pdf}

\hypertarget{superhosts}{%
\subsubsection{Superhosts}\label{superhosts}}

Airbnb \href{https://www.airbnb.ca/help/article/828/what-is-a-superhost}{says} that:

\begin{quote}
Superhosts are experienced hosts who provide a shining example for other hosts, and extraordinary experiences for their guests.

Once a host reaches Superhost status, a badge superhost badge will automatically appear on their listing and profile to help you identify them.

We check Superhosts' activity four times a year, to ensure that the program highlights the people who are most dedicated to providing outstanding hospitality.
\end{quote}

First we'll look at the NAs in \texttt{host\_is\_superhost}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_selected }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(host_is_superhost))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 285 x 22
##    host_id host_since host_response_t~ host_is_superho~ host_listings_c~
##      <dbl> <date>     <chr>            <lgl>                       <dbl>
##  1  2.27e6 NA         <NA>             NA                             NA
##  2  2.27e6 NA         <NA>             NA                             NA
##  3  2.27e6 NA         <NA>             NA                             NA
##  4  5.99e6 NA         <NA>             NA                             NA
##  5  8.73e6 NA         <NA>             NA                             NA
##  6  1.11e7 NA         <NA>             NA                             NA
##  7  1.62e7 NA         <NA>             NA                             NA
##  8  1.46e5 NA         <NA>             NA                             NA
##  9  1.56e7 NA         <NA>             NA                             NA
## 10  1.85e7 NA         <NA>             NA                             NA
## # ... with 275 more rows, and 17 more variables:
## #   host_total_listings_count <dbl>, host_neighbourhood <chr>,
## #   neighbourhood_cleansed <chr>, room_type <chr>, bathrooms <dbl>,
## #   bedrooms <dbl>, square_feet <dbl>, price <int>, number_of_reviews <dbl>,
## #   has_availability <lgl>, review_scores_rating <dbl>,
## #   review_scores_accuracy <dbl>, review_scores_cleanliness <dbl>,
## #   review_scores_checkin <dbl>, review_scores_communication <dbl>,
## #   review_scores_location <dbl>, review_scores_value <dbl>
\end{verbatim}

There are 285 of these and it's clear that there is something odd going on - maybe the host removed the listing or similar?

We'll also want to create a binary variable out of this. It's true/false at the moment, which is fine for the modelling, but there are a handful of situations where it'll be easier if we have a 0/1.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_selected <-}\StringTok{ }
\StringTok{  }\NormalTok{airbnb_data_selected }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{host_is_superhost_binary =} \KeywordTok{case_when}\NormalTok{(}
\NormalTok{    host_is_superhost }\OperatorTok{==}\StringTok{ }\OtherTok{TRUE} \OperatorTok{~}\StringTok{ }\DecValTok{1}\NormalTok{,}
\NormalTok{    host_is_superhost }\OperatorTok{==}\StringTok{ }\OtherTok{FALSE} \OperatorTok{~}\StringTok{ }\DecValTok{0}\NormalTok{,}
    \OtherTok{TRUE} \OperatorTok{~}\StringTok{ }\DecValTok{999}
\NormalTok{    )}
\NormalTok{  )}
\NormalTok{airbnb_data_selected}\OperatorTok{$}\NormalTok{host_is_superhost_binary[airbnb_data_selected}\OperatorTok{$}\NormalTok{host_is_superhost_binary }\OperatorTok{==}\StringTok{ }\DecValTok{999}\NormalTok{] <-}\StringTok{ }\OtherTok{NA}
\end{Highlighting}
\end{Shaded}

\hypertarget{reviews}{%
\subsubsection{Reviews}\label{reviews}}

Airbnb \href{https://www.airbnb.ca/help/article/1257/how-do-star-ratings-work-for-stays}{says} that:

\begin{quote}
In addition to written reviews, guests can submit an overall star rating and a set of category star ratings for their stay.

Hosts can view their star ratings on their Progress page, under Reviews. Hosts using professional hosting tools can find reviews and quality details on their Performance page, under Quality.

Guests can give ratings on:

\begin{itemize}
\tightlist
\item
  Overall experience. Overall, how was the stay?
\item
  Cleanliness. Did guests feel that the space was clean and tidy?
\item
  Accuracy. How accurately did the listing page represent the space? For example, guests should be able to find up-to-date info and photos in the listing description.
\item
  Value. Did the guest feel that the listing provided good value for the price?
\item
  Communication. How well did you communicate before and during the stay? Guests often care that their host responds quickly, reliably, and frequently to their messages and questions.
\item
  Check-in. How smoothly did check-in go?
\item
  Location. How did guests feel about the neighbourhood? This may mean that there's an accurate description for proximity and access to transportation, shopping centres, city centre, etc., and a description that includes special considerations, like noise, and family safety.
\item
  Amenities. How did guests feel about the amenities that were available during their stay? Guests often care that all the amenities listed are available, working, and in good condition.
\end{itemize}

In each category, hosts are able to see how often they get 5 stars, how guests rated nearby hosts, and, in some cases, tips to help improve the listing.

The number of stars displayed at the top of a listing page is an aggregate of the primary scores guests have given for that listing. At the bottom of a listing page, there's an aggregate for each category rating. A host needs to receive star ratings from at least 3 guests before their aggregate score appears.
\end{quote}

\textbf{TODO: I don't understand how these review scores are being constructed. Airbnb says it's a star rating, but how are they converting this into the 10 point scale, similarly, how are their constructing the overall one, which seems to be out of 100? There's a lot of clumping around 20, 40, 60, 80, 100 - could they be averaging a five-star scale and then rebasing it to 100?}

Now we'll deal with the NAs in \texttt{review\_scores\_rating}. This one is more complicated as there are a lot of them.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_selected }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(review_scores_rating))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4,676 x 23
##    host_id host_since host_response_t~ host_is_superho~ host_listings_c~
##      <dbl> <date>     <chr>            <lgl>                       <dbl>
##  1   48239 2009-10-25 N/A              FALSE                           1
##  2  545074 2011-04-29 N/A              FALSE                           2
##  3 1210571 2011-09-26 N/A              FALSE                           1
##  4 1408862 2011-11-15 N/A              FALSE                           1
##  5 1411076 2011-11-15 N/A              FALSE                           1
##  6 1409872 2011-11-15 N/A              FALSE                           1
##  7 1466410 2011-12-02 within a few ho~ FALSE                           1
##  8 1664812 2012-01-28 N/A              FALSE                           1
##  9 1828773 2012-02-28 N/A              FALSE                           1
## 10 1923052 2012-03-14 N/A              FALSE                           1
## # ... with 4,666 more rows, and 18 more variables:
## #   host_total_listings_count <dbl>, host_neighbourhood <chr>,
## #   neighbourhood_cleansed <chr>, room_type <chr>, bathrooms <dbl>,
## #   bedrooms <dbl>, square_feet <dbl>, price <int>, number_of_reviews <dbl>,
## #   has_availability <lgl>, review_scores_rating <dbl>,
## #   review_scores_accuracy <dbl>, review_scores_cleanliness <dbl>,
## #   review_scores_checkin <dbl>, review_scores_communication <dbl>,
## #   review_scores_location <dbl>, review_scores_value <dbl>,
## #   host_is_superhost_binary <dbl>
\end{verbatim}

Now see if it's just because they don't have any reviews.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_selected }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(review_scores_rating)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(number_of_reviews) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{table}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## .
##    0    1    2    3    4 
## 4389  251   26    8    2
\end{verbatim}

So it's clear that in almost all these cases they don't have a review yet because they don't have enough reviews. However, it's a large proportion of the total - almost a fifth of properties don't have any reviews (hence an NA in review\_scores\_rating).

We can use \texttt{vis\_miss} from the visdat package (Tierney, 2017) to make sure that all components of the review are missing. If the NAs are being driven by the Airbnb requirement of at least three reviews then we would expect they would all be missing.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# We'll just check whether this is the same for all of the different variants of reviews}
\NormalTok{airbnb_data_selected }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(review_scores_rating, }
\NormalTok{         review_scores_accuracy, }
\NormalTok{         review_scores_cleanliness, }
\NormalTok{         review_scores_checkin, }
\NormalTok{         review_scores_communication, }
\NormalTok{         review_scores_location, }
\NormalTok{         review_scores_value) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{vis_miss}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-312-1.pdf}

It looks pretty convincing that in almost all cases, all the different variants of reviews are missing. So let's just focus on the main review.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_selected }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(review_scores_rating)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ review_scores_rating)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Average review score"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Number of properties"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-313-1.pdf}

It's pretty clear that almost all the reviews are more than 80. Let's just zoom in on that 60 to 80 range to check what the distribution looks like in that range.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_selected }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(review_scores_rating)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(review_scores_rating }\OperatorTok{>}\StringTok{ }\DecValTok{60}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(review_scores_rating }\OperatorTok{<}\StringTok{ }\DecValTok{80}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ review_scores_rating)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Average review score"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Number of properties"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-314-1.pdf}

\hypertarget{response-time}{%
\subsubsection{Response time}\label{response-time}}

Airbnb \href{https://www.airbnb.ca/help/article/75/how-much-time-does-a-host-have-to-respond-to-my-reservation-request}{says} that:

\begin{quote}
Hosts have 24 hours to officially accept or decline reservation requests. You'll be updated by email about the status of your request.

More than half of all reservation requests are accepted within one hour of being received. The vast majority of hosts reply within 12 hours.

If a host confirms your request, your payment is processed and collected by Airbnb in full. If a host declines your request or the request expires, we don't process your payment.
\end{quote}

\textbf{TODO: I don't understand how you can get a response time of NA? It must be related to some other variable.}

Looking now at response time:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(airbnb_data_selected}\OperatorTok{$}\NormalTok{host_response_time)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## a few days or more                N/A       within a day within a few hours 
##                321               6324               1584               2542 
##     within an hour 
##              12341
\end{verbatim}

Interestingly it seems like what looks like `NAs' in the \texttt{host\_response\_time} variable are not being coded as proper NAs, but are instead being treated as another category. We'll recode them to be actual NAs.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_selected}\OperatorTok{$}\NormalTok{host_response_time[airbnb_data_selected}\OperatorTok{$}\NormalTok{host_response_time }\OperatorTok{==}\StringTok{ "N/A"}\NormalTok{] <-}\StringTok{ }\OtherTok{NA}
\end{Highlighting}
\end{Shaded}

So here we clearly have issues with NAs. We probably want to filter them away for this example because it's just a quick example, but there are an awful lot of them (more than 20 per cent) so we'll have a quick look at them in relation to the review score.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_selected }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(host_response_time)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ review_scores_rating)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-317-1.pdf}

There seem to be an awful lot that have an overall review of 100. There are also an awful lot that have a review score of NA.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_selected }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(host_response_time)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(review_scores_rating))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2,313 x 23
##    host_id host_since host_response_t~ host_is_superho~ host_listings_c~
##      <dbl> <date>     <chr>            <lgl>                       <dbl>
##  1   48239 2009-10-25 <NA>             FALSE                           1
##  2  545074 2011-04-29 <NA>             FALSE                           2
##  3 1210571 2011-09-26 <NA>             FALSE                           1
##  4 1408862 2011-11-15 <NA>             FALSE                           1
##  5 1411076 2011-11-15 <NA>             FALSE                           1
##  6 1409872 2011-11-15 <NA>             FALSE                           1
##  7 1664812 2012-01-28 <NA>             FALSE                           1
##  8 1828773 2012-02-28 <NA>             FALSE                           1
##  9 1923052 2012-03-14 <NA>             FALSE                           1
## 10 2291374 2012-05-04 <NA>             FALSE                           1
## # ... with 2,303 more rows, and 18 more variables:
## #   host_total_listings_count <dbl>, host_neighbourhood <chr>,
## #   neighbourhood_cleansed <chr>, room_type <chr>, bathrooms <dbl>,
## #   bedrooms <dbl>, square_feet <dbl>, price <int>, number_of_reviews <dbl>,
## #   has_availability <lgl>, review_scores_rating <dbl>,
## #   review_scores_accuracy <dbl>, review_scores_cleanliness <dbl>,
## #   review_scores_checkin <dbl>, review_scores_communication <dbl>,
## #   review_scores_location <dbl>, review_scores_value <dbl>,
## #   host_is_superhost_binary <dbl>
\end{verbatim}

\hypertarget{host-number-of-listings}{%
\subsubsection{Host number of listings}\label{host-number-of-listings}}

There are two versions of a variable telling you how many properties a host has on Airbnb, so to start just check whether there's a difference.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_selected }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{listings_count_is_same =} \KeywordTok{if_else}\NormalTok{(host_listings_count }\OperatorTok{==}\StringTok{ }\NormalTok{host_total_listings_count, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(listings_count_is_same }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 0 x 24
## # ... with 24 variables: host_id <dbl>, host_since <date>,
## #   host_response_time <chr>, host_is_superhost <lgl>,
## #   host_listings_count <dbl>, host_total_listings_count <dbl>,
## #   host_neighbourhood <chr>, neighbourhood_cleansed <chr>, room_type <chr>,
## #   bathrooms <dbl>, bedrooms <dbl>, square_feet <dbl>, price <int>,
## #   number_of_reviews <dbl>, has_availability <lgl>,
## #   review_scores_rating <dbl>, review_scores_accuracy <dbl>,
## #   review_scores_cleanliness <dbl>, review_scores_checkin <dbl>,
## #   review_scores_communication <dbl>, review_scores_location <dbl>,
## #   review_scores_value <dbl>, host_is_superhost_binary <dbl>,
## #   listings_count_is_same <dbl>
\end{verbatim}

There are none in this dataset so we can just remove one column for now and have a quick look at the other one.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_selected <-}\StringTok{ }
\StringTok{  }\NormalTok{airbnb_data_selected }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{host_listings_count)}

\NormalTok{airbnb_data_selected }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(host_total_listings_count)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 60 x 2
##    host_total_listings_count     n
##                        <dbl> <int>
##  1                         0  1750
##  2                         1  9727
##  3                         2  3424
##  4                         3  1889
##  5                         4  1049
##  6                         5   809
##  7                         6   530
##  8                         7   353
##  9                         8   284
## 10                         9   287
## # ... with 50 more rows
\end{verbatim}

So there are a large number who have somewhere in the 2-10 properties range, but the usual long tail. The number with 0 listings is unexpected and worth following up on. And there are a bunch with NA that we'll need to deal with.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_selected }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(host_total_listings_count }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 22
##   host_id host_since host_response_t~ host_is_superho~ host_total_list~
##     <dbl> <date>     <chr>            <lgl>                       <dbl>
## 1  140602 2010-06-08 within an hour   FALSE                           0
## 2  992557 2011-08-19 a few days or m~ FALSE                           0
## 3 2737251 2012-06-25 <NA>             FALSE                           0
## 4 3367744 2012-08-25 within a day     FALSE                           0
## 5 2139510 2012-04-14 <NA>             FALSE                           0
## 6 1786233 2012-02-21 within a few ho~ FALSE                           0
## # ... with 17 more variables: host_neighbourhood <chr>,
## #   neighbourhood_cleansed <chr>, room_type <chr>, bathrooms <dbl>,
## #   bedrooms <dbl>, square_feet <dbl>, price <int>, number_of_reviews <dbl>,
## #   has_availability <lgl>, review_scores_rating <dbl>,
## #   review_scores_accuracy <dbl>, review_scores_cleanliness <dbl>,
## #   review_scores_checkin <dbl>, review_scores_communication <dbl>,
## #   review_scores_location <dbl>, review_scores_value <dbl>,
## #   host_is_superhost_binary <dbl>
\end{verbatim}

There's nothing that immediately jumps out as odd about the people with zero listings, but there must be something going on.

Based on this dataset, there's a third way of looking at the number of properties someone has and that's to look at the number of times the unique ID occurs.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_selected }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(host_id) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\OperatorTok{-}\NormalTok{n) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 2
##     host_id     n
##       <dbl> <int>
## 1  10202618   121
## 2   1919294    82
## 3 152088065    53
## 4  33238402    50
## 5  63526506    48
## 6 164320990    48
\end{verbatim}

Again this makes it clear that there are many with multiple properties listed.

\hypertarget{decisions}{%
\subsubsection{Decisions}\label{decisions}}

The purpose of this document is to just give a quick introduction to using real-world data, so we'll just remove anything that is annoying, but if you're using this for research then you'd need to justify these decisions and/or possibly make different ones.

Get rid of prices more than \$999.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_filtered <-}\StringTok{ }
\StringTok{  }\NormalTok{airbnb_data_selected }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(price }\OperatorTok{<}\StringTok{ }\DecValTok{1000}\NormalTok{)}
\KeywordTok{dim}\NormalTok{(airbnb_data_filtered)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 23310    22
\end{verbatim}

Get rid of anyone with an NA for whether they are a super host.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Just remove host_is_superhost NAs for now.}
\NormalTok{airbnb_data_filtered <-}\StringTok{ }
\StringTok{  }\NormalTok{airbnb_data_filtered }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(host_is_superhost))}
\KeywordTok{dim}\NormalTok{(airbnb_data_filtered)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 23025    22
\end{verbatim}

Get rid of anyone with an NA in their main review score - this removes roughly 20 per cent of observations.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# We'll just get rid of them for now, but this is probably something that deserves more attention - possibly in an appendix or similar.}
\NormalTok{airbnb_data_filtered <-}\StringTok{ }
\StringTok{  }\NormalTok{airbnb_data_filtered }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(review_scores_rating))}
\CommentTok{# There are still some where the rest of the reviews are missing even though there is a main review score}
\CommentTok{# There seem to be an awful lot that have an overall review of 100. Does that make sense?}
\KeywordTok{dim}\NormalTok{(airbnb_data_filtered)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 18485    22
\end{verbatim}

Get rid of anyone with a main review score less than 70.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# We'll just get rid of them for now, but this is probably something that deserves more attention - possibly in an appendix or similar.}
\NormalTok{airbnb_data_filtered <-}\StringTok{ }
\StringTok{  }\NormalTok{airbnb_data_filtered }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(review_scores_rating }\OperatorTok{>}\StringTok{ }\DecValTok{69}\NormalTok{)}
\CommentTok{# There are still some where the rest of the reviews are missing even though there is a main review score}
\CommentTok{# There seem to be an awful lot that have an overall review of 100. Does that make sense?}
\KeywordTok{dim}\NormalTok{(airbnb_data_filtered)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 18143    22
\end{verbatim}

Get rid of anyone with a NA in their response time - this removes roughly another 20 per cent of the observations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_filtered <-}\StringTok{ }
\StringTok{  }\NormalTok{airbnb_data_filtered }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(host_response_time))}
\KeywordTok{dim}\NormalTok{(airbnb_data_filtered)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 14165    22
\end{verbatim}

(\textbf{TODO: We don't have to do this next step as we've already got rid of them at some other point. So there's something systematic going on and we should come back and look into it.})

Get rid of anyone with a NA in their number of properties.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_filtered <-}\StringTok{ }
\StringTok{  }\NormalTok{airbnb_data_filtered }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(host_total_listings_count))}
\KeywordTok{dim}\NormalTok{(airbnb_data_filtered)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 14165    22
\end{verbatim}

Get rid of anyone with a 100 for their review\_scores\_rating.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_filtered <-}\StringTok{ }
\StringTok{  }\NormalTok{airbnb_data_filtered }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(review_scores_rating }\OperatorTok{!=}\StringTok{ }\DecValTok{100}\NormalTok{)}
\KeywordTok{dim}\NormalTok{(airbnb_data_filtered)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10857    22
\end{verbatim}

Only keep people with one property:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_filtered <-}\StringTok{ }
\StringTok{  }\NormalTok{airbnb_data_filtered }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{add_count}\NormalTok{(host_id) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(n }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{n)}
\KeywordTok{dim}\NormalTok{(airbnb_data_filtered)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4880   22
\end{verbatim}

\hypertarget{explore-data}{%
\subsection{Explore data}\label{explore-data}}

We might like to make some graphs to see if we can any relationships jump out. Some aspects that come to mind is looking at prices and reviews and super hosts, and number of properties and neighbourhood.

\textbf{TODO: Come back and make this paragraph better.}

\hypertarget{price-and-reviews}{%
\subsubsection{Price and reviews}\label{price-and-reviews}}

Look at the relationship between price and reviews, and whether they are a super-host.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Look at both price and reviews}
\NormalTok{airbnb_data_filtered }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ price, }\DataTypeTok{y =}\NormalTok{ review_scores_rating, }\DataTypeTok{color =}\NormalTok{ host_is_superhost)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{1}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.1}\NormalTok{) }\OperatorTok{+}\StringTok{ }\CommentTok{# Make the points smaller and more transparent as they overlap considerably.}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Price per night"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Average review score"}\NormalTok{,}
       \DataTypeTok{color =} \StringTok{"Super host"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\CommentTok{# Probably should recode this to more meaningful than TRUE/FALSE.}
\StringTok{  }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-331-1.pdf}

\hypertarget{superhost-and-response-time}{%
\subsubsection{Superhost and response-time}\label{superhost-and-response-time}}

One of the aspects that may make someone a super host is how quickly they respond to enquiries. One could imagine that being a superhost involves quickly saying yes or no to enquiries. Let's look at the data. First, we want to look at the possible values of superhost by their response times.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_filtered }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{tabyl}\NormalTok{(host_is_superhost) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{adorn_totals}\NormalTok{(}\StringTok{"row"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{adorn_pct_formatting}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  host_is_superhost    n percent
##              FALSE 2617   53.6%
##               TRUE 2263   46.4%
##              Total 4880  100.0%
\end{verbatim}

Fortunately, it looks like when we removed the reviews rows we removed any NAs from whether they were a super host, but if we go back and look into that we may need to check again. The \texttt{tabyl} function within the janitor package (Firke, 2020) would list the NAs if there were any, but in case you don't trust it, another way of check this is to try to filter to just the NAs.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_filtered }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(host_is_superhost))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 0 x 22
## # ... with 22 variables: host_id <dbl>, host_since <date>,
## #   host_response_time <chr>, host_is_superhost <lgl>,
## #   host_total_listings_count <dbl>, host_neighbourhood <chr>,
## #   neighbourhood_cleansed <chr>, room_type <chr>, bathrooms <dbl>,
## #   bedrooms <dbl>, square_feet <dbl>, price <int>, number_of_reviews <dbl>,
## #   has_availability <lgl>, review_scores_rating <dbl>,
## #   review_scores_accuracy <dbl>, review_scores_cleanliness <dbl>,
## #   review_scores_checkin <dbl>, review_scores_communication <dbl>,
## #   review_scores_location <dbl>, review_scores_value <dbl>,
## #   host_is_superhost_binary <dbl>
\end{verbatim}

Now let's look at the response time.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_filtered }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{tabyl}\NormalTok{(host_response_time) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{adorn_totals}\NormalTok{(}\StringTok{"row"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{adorn_pct_formatting}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  host_response_time    n percent
##  a few days or more   78    1.6%
##        within a day  538   11.0%
##  within a few hours  833   17.1%
##      within an hour 3431   70.3%
##               Total 4880  100.0%
\end{verbatim}

So a vast majority respond within an hour.

Finally, we can look at the cross-tab.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_filtered }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{tabyl}\NormalTok{(host_response_time, host_is_superhost) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{adorn_percentages}\NormalTok{(}\StringTok{"row"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{adorn_pct_formatting}\NormalTok{(}\DataTypeTok{digits =} \DecValTok{0}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{adorn_ns}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{adorn_title}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                     host_is_superhost           
##  host_response_time             FALSE       TRUE
##  a few days or more        92%   (72)  8%    (6)
##        within a day        74%  (396) 26%  (142)
##  within a few hours        60%  (502) 40%  (331)
##      within an hour        48% (1647) 52% (1784)
\end{verbatim}

So if someone doesn't respond within an hour then it's unlikely that they are a super host.

\hypertarget{neighbourhood}{%
\subsubsection{Neighbourhood}\label{neighbourhood}}

Finally, let's look at neighbourhood. The data provider has attempted to clean the neighbourhood variable for us, so we'll just use this for now. If we were doing this analysis properly, we'd need to check whether they'd made any mistakes.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# We expect something in the order of 100 to 150 neighbourhoods, with the top ten accounting for a large majority of listings.}
\NormalTok{airbnb_data_filtered }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{tabyl}\NormalTok{(neighbourhood_cleansed) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{adorn_totals}\NormalTok{(}\StringTok{"row"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{adorn_pct_formatting}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{nrow}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 139
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_filtered }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{tabyl}\NormalTok{(neighbourhood_cleansed) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{adorn_pct_formatting}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\OperatorTok{-}\NormalTok{n) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(n }\OperatorTok{>}\StringTok{ }\DecValTok{100}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{adorn_totals}\NormalTok{(}\StringTok{"row"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               neighbourhood_cleansed    n percent
##    Waterfront Communities-The Island 1001   20.5%
##                              Niagara  273    5.6%
##                Church-Yonge Corridor  176    3.6%
##                                Annex  174    3.6%
##  Dovercourt-Wallace Emerson-Junction  163    3.3%
##                  Bay Street Corridor  130    2.7%
\end{verbatim}

\hypertarget{model-data}{%
\subsection{Model data}\label{model-data}}

We will now run some models on our dataset. We will first split the data into test/training groups, we do this using functions from the tidymodels package (Kuhn and Wickham, 2019) (which like the tidyverse package (Wickham et al., 2019) is a package of packages).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{airbnb_data_filtered_split <-}\StringTok{ }
\StringTok{  }\NormalTok{airbnb_data_filtered }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{initial_split}\NormalTok{(}\DataTypeTok{prop =} \DecValTok{3}\OperatorTok{/}\DecValTok{4}\NormalTok{)}

\NormalTok{airbnb_train <-}\StringTok{ }\KeywordTok{training}\NormalTok{(airbnb_data_filtered_split)}
\NormalTok{airbnb_test <-}\StringTok{ }\KeywordTok{testing}\NormalTok{(airbnb_data_filtered_split)}

\KeywordTok{rm}\NormalTok{(airbnb_data_filtered_split)}
\end{Highlighting}
\end{Shaded}

\hypertarget{logistic-regression-1}{%
\subsubsection{Logistic regression}\label{logistic-regression-1}}

We may like to look at whether we can forecast whether someone is a super host, and the factors that go into explaining that. As the dependent variable is binary, this is a good opportunity to look at logistic regression. We expect that better reviews will be associated with faster responses and higher reviews. Specifically, the model that we estimate is:

\[\mbox{Prob(Is super host} = 1) = \beta_0 + \beta_1 \mbox{Response time} + \beta_2 \mbox{Reviews} + \epsilon\]

We estimate the model using \texttt{glm} in the R language (R Core Team, 2019).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{logistic_reg_superhost_response_review <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(host_is_superhost }\OperatorTok{~}\StringTok{ }
\StringTok{                                                }\NormalTok{host_response_time }\OperatorTok{+}\StringTok{ }
\StringTok{                                                }\NormalTok{review_scores_rating,}
                                              \DataTypeTok{data =}\NormalTok{ airbnb_train,}
                                              \DataTypeTok{family =}\NormalTok{ binomial}
\NormalTok{                                              )}
\end{Highlighting}
\end{Shaded}

We can have a quick look at the results, for instance, the \texttt{summary} function. We could also use \texttt{tidy} or \texttt{glance} from the broom package (Robinson and Hayes, 2020). The details should be the same, but the broom functions are tibbles, which means that we can more easily deal with them within a tidy framework.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(logistic_reg_superhost_response_review)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = host_is_superhost ~ host_response_time + review_scores_rating, 
##     family = binomial, data = airbnb_train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9513  -0.7401  -0.0529   0.8562   4.4894  
## 
## Coefficients:
##                                       Estimate Std. Error z value Pr(>|z|)    
## (Intercept)                          -49.44238    1.90724 -25.924  < 2e-16 ***
## host_response_timewithin a day         1.13811    0.47746   2.384   0.0171 *  
## host_response_timewithin a few hours   1.99356    0.46831   4.257 2.07e-05 ***
## host_response_timewithin an hour       2.42867    0.46035   5.276 1.32e-07 ***
## review_scores_rating                   0.49249    0.01905  25.851  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 5051.2  on 3659  degrees of freedom
## Residual deviance: 3583.2  on 3655  degrees of freedom
## AIC: 3593.2
## 
## Number of Fisher Scoring iterations: 6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tidy}\NormalTok{(logistic_reg_superhost_response_review)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 5 x 5
##   term                                 estimate std.error statistic   p.value
##   <chr>                                   <dbl>     <dbl>     <dbl>     <dbl>
## 1 (Intercept)                           -49.4      1.91      -25.9  3.61e-148
## 2 host_response_timewithin a day          1.14     0.477       2.38 1.71e-  2
## 3 host_response_timewithin a few hours    1.99     0.468       4.26 2.07e-  5
## 4 host_response_timewithin an hour        2.43     0.460       5.28 1.32e-  7
## 5 review_scores_rating                    0.492    0.0191     25.9  2.39e-147
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{glance}\NormalTok{(logistic_reg_superhost_response_review)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 8
##   null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs
##           <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>
## 1         5051.    3659 -1792. 3593. 3624.    3583.        3655  3660
\end{verbatim}

We might like to look at what our model predicts, compared with whether the person was actually a super host. We can do that in a variety of ways, but one way is to use \texttt{augment} from the broom package (Robinson and Hayes, 2020). This will add the prediction and associated uncertainty to the data. For every row we will then have the probability that our model is estimating that they are a superhost. But ultimately, we need a binary forecast. There are a bunch of different options, but one is to just say that if the model estimates a probability of more than 0.5 then we bin it into a superhost, and other not.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_filtered_logistic_fit_train <-}\StringTok{ }
\StringTok{  }\KeywordTok{augment}\NormalTok{(logistic_reg_superhost_response_review, }
          \DataTypeTok{data =}\NormalTok{ airbnb_train }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(host_is_superhost, }
\NormalTok{                                         host_is_superhost_binary,}
\NormalTok{                                         host_response_time,}
\NormalTok{                                         review_scores_rating}
\NormalTok{                                         ),}
          \DataTypeTok{type.predict =} \StringTok{"response"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# We use the "response" option here so that the function does the work of worrying about the exponential and log odds for us. Our result will be a probability.}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{.hat, }\OperatorTok{-}\NormalTok{.sigma, }\OperatorTok{-}\NormalTok{.cooksd, }\OperatorTok{-}\NormalTok{.std.resid) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{predict_host_is_superhost =} \KeywordTok{if_else}\NormalTok{(.fitted }\OperatorTok{>}\StringTok{ }\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{), }\CommentTok{# How do things change if we change the 0.5 cutoff?}
         \DataTypeTok{host_is_superhost_binary =} \KeywordTok{as.factor}\NormalTok{(host_is_superhost_binary),}
         \DataTypeTok{predict_host_is_superhost_binary =} \KeywordTok{as.factor}\NormalTok{(predict_host_is_superhost)}
\NormalTok{         )}
\end{Highlighting}
\end{Shaded}

We can look at how far off the model is. There are a bunch of ways of doing this, but one is to look at what probability the model has given each person.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_filtered_logistic_fit_train }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ .fitted, }\DataTypeTok{fill =}\NormalTok{ host_is_superhost_binary)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \FloatTok{0.05}\NormalTok{, }\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Estimated probability that host is super host"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Number"}\NormalTok{,}
       \DataTypeTok{fill =} \StringTok{"Host is super host"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-342-1.pdf}

We can look at how the model probabilities change based on average review score, and their average time to respond.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(airbnb_data_filtered_logistic_fit_train, }
       \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ review_scores_rating, }
           \DataTypeTok{y =}\NormalTok{ .fitted, }
           \DataTypeTok{color =}\NormalTok{ host_response_time)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Average review score"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Predicted probability of being a superhost"}\NormalTok{,}
       \DataTypeTok{color =} \StringTok{"Host response time"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-343-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# What is missing from this graph?}
\end{Highlighting}
\end{Shaded}

This nice thing about this graph is that it illustrates nicely the effect of a host having an average response time of, say, `within an hour' compared with `within a few hours'.

We can focus on how the model does in terms of raw classification using \texttt{confusionMatrix} from the caret package (Kuhn, 2020). This also gives a bunch of diagnostics (the help file explains what they are). In general, they suggest this isn't the best model that's ever existed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{caret}\OperatorTok{::}\KeywordTok{confusionMatrix}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ airbnb_data_filtered_logistic_fit_train}\OperatorTok{$}\NormalTok{predict_host_is_superhost_binary,}
                       \DataTypeTok{reference =}\NormalTok{ airbnb_data_filtered_logistic_fit_train}\OperatorTok{$}\NormalTok{host_is_superhost_binary)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 1502  341
##          1  472 1345
##                                          
##                Accuracy : 0.7779         
##                  95% CI : (0.764, 0.7912)
##     No Information Rate : 0.5393         
##     P-Value [Acc > NIR] : < 2.2e-16      
##                                          
##                   Kappa : 0.5555         
##                                          
##  Mcnemar's Test P-Value : 5.132e-06      
##                                          
##             Sensitivity : 0.7609         
##             Specificity : 0.7977         
##          Pos Pred Value : 0.8150         
##          Neg Pred Value : 0.7402         
##              Prevalence : 0.5393         
##          Detection Rate : 0.4104         
##    Detection Prevalence : 0.5036         
##       Balanced Accuracy : 0.7793         
##                                          
##        'Positive' Class : 0              
## 
\end{verbatim}

In any case, to this point we've been looking at how the model has done on the training set. It's also relevant how it does on the test set. Again, there are a bunch of ways to do this, but one is to again use the augment function, but to include a newdata argument.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb_data_filtered_logistic_fit_test <-}\StringTok{ }
\StringTok{  }\KeywordTok{augment}\NormalTok{(logistic_reg_superhost_response_review, }
          \DataTypeTok{data =}\NormalTok{ airbnb_train }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(host_is_superhost, }
\NormalTok{                                         host_is_superhost_binary,}
\NormalTok{                                         host_response_time,}
\NormalTok{                                         review_scores_rating}
\NormalTok{                                         ),}
          \DataTypeTok{newdata =}\NormalTok{ airbnb_test }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(host_is_superhost, }
\NormalTok{                                         host_is_superhost_binary,}
\NormalTok{                                         host_response_time,}
\NormalTok{                                         review_scores_rating}
\NormalTok{                                         ), }\CommentTok{# I'm selecting just because the}
          \CommentTok{# dataset is quite wide, and so this makes it easier to look at.}
          \DataTypeTok{type.predict =} \StringTok{"response"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{predict_host_is_superhost =} \KeywordTok{if_else}\NormalTok{(.fitted }\OperatorTok{>}\StringTok{ }\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{), }
         \DataTypeTok{host_is_superhost_binary =} \KeywordTok{as.factor}\NormalTok{(host_is_superhost_binary),}
         \DataTypeTok{predict_host_is_superhost_binary =} \KeywordTok{as.factor}\NormalTok{(predict_host_is_superhost)}
\NormalTok{         )}
\end{Highlighting}
\end{Shaded}

We would expect the performance to be slightly worse on the test set. But it's actually fairly similar.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{caret}\OperatorTok{::}\KeywordTok{confusionMatrix}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ airbnb_data_filtered_logistic_fit_test}\OperatorTok{$}\NormalTok{predict_host_is_superhost_binary,}
                       \DataTypeTok{reference =}\NormalTok{ airbnb_data_filtered_logistic_fit_test}\OperatorTok{$}\NormalTok{host_is_superhost_binary)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 501 132
##          1 142 445
##                                           
##                Accuracy : 0.7754          
##                  95% CI : (0.7509, 0.7986)
##     No Information Rate : 0.527           
##     P-Value [Acc > NIR] : <2e-16          
##                                           
##                   Kappa : 0.5499          
##                                           
##  Mcnemar's Test P-Value : 0.5866          
##                                           
##             Sensitivity : 0.7792          
##             Specificity : 0.7712          
##          Pos Pred Value : 0.7915          
##          Neg Pred Value : 0.7581          
##              Prevalence : 0.5270          
##          Detection Rate : 0.4107          
##    Detection Prevalence : 0.5189          
##       Balanced Accuracy : 0.7752          
##                                           
##        'Positive' Class : 0               
## 
\end{verbatim}

We could compare the test with the training sets in terms of forecasts.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{training <-}\StringTok{ }\NormalTok{airbnb_data_filtered_logistic_fit_train }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(host_is_superhost_binary, .fitted) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{type =} \StringTok{"Training set"}\NormalTok{)}

\NormalTok{test <-}\StringTok{ }\NormalTok{airbnb_data_filtered_logistic_fit_test }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(host_is_superhost_binary, .fitted) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{type =} \StringTok{"Test set"}\NormalTok{)}

\NormalTok{both <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(training, test)}
\KeywordTok{rm}\NormalTok{(training, test)}

\NormalTok{both }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ .fitted, }
             \DataTypeTok{fill =}\NormalTok{ host_is_superhost_binary)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \FloatTok{0.05}\NormalTok{, }\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Estimated probability that host is super host"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Number"}\NormalTok{,}
       \DataTypeTok{fill =} \StringTok{"Host is super host"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\KeywordTok{vars}\NormalTok{(type),}
             \DataTypeTok{nrow =} \DecValTok{2}\NormalTok{,}
             \DataTypeTok{scales =} \StringTok{"free_y"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-347-1.pdf}

\textbf{HINT: How many aspects mentioned in the modelling criteria in rubric for PS3 have I gone through?}

\hypertarget{next-steps-2}{%
\subsection{Next steps}\label{next-steps-2}}

Ideas for the future:

\begin{itemize}
\tightlist
\item
  We could look at the relationship between superhosts and time - how long does it take to become one normally?
\item
  We could use price.
\item
  We could use neighbourhood.
\end{itemize}

\hypertarget{references-1}{%
\subsection{References}\label{references-1}}

\begin{itemize}
\tightlist
\item
  R Core Team (2019). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL \url{https://www.R-project.org/}.
\item
  David Robinson and Alex Hayes (2020). broom: Convert Statistical Analysis Objects into Tidy Tibbles. R package version 0.5.4., \url{https://CRAN.R-project.org/package=broom}
\item
  Kirill Müller (2017). here: A Simpler Way to Find Your Files. R package version 0.1. \url{https://CRAN.R-project.org/package=here}.
\item
  Sam Firke (2020). janitor: Simple Tools for Examining and Cleaning Dirty Data. R package version 1.2.1., \url{https://CRAN.R-project.org/package=janitor}
\item
  Max Kuhn and Hadley Wickham (2019). tidymodels: Easily Install and Load the `Tidymodels' Packages. R package, version 0.0.3. \url{https://CRAN.R-project.org/package=tidymodels}
\item
  Wickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686, \url{https://doi.org/10.21105/joss.01686}
\item
  Elin Waring, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu and Shannon Ellis (2020). skimr: Compact and Flexible Summaries of Data. R package version 2.1. \url{https://CRAN.R-project.org/package=skimr}
\item
  Tierney N (2017). ``visdat: Visualising Whole Data Frames.'' \emph{JOSS}, \emph{2}(16), 355. doi: 10.21105/joss.00355 (URL: \url{https://doi.org/10.21105/joss.00355}), \textless URL: \url{http://dx.doi.org/10.21105/joss.00355}\textgreater.
\item
  Yihui Xie (2020). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.27.
\item
  Inside Airbnb (2020) Available at: \url{http://insideairbnb.com}, as accessed 19 February 2020.
\item
  Max Kuhn (2020). caret: Classification and Regression Training. R package version 6.0-85. \url{https://CRAN.R-project.org/package=caret}
\end{itemize}

\hypertarget{regression-essentials}{%
\chapter{Regression essentials}\label{regression-essentials}}

\hypertarget{linear-regression}{%
\section{Linear regression}\label{linear-regression}}

This chapter is about quantitatively describing a linear relationship. You're meant to have taken an intro stats course, but will I briefly cover the basics. Then we'll get stuck into some data and try to see how much of our understanding holds up.

\textbf{Required reading}

(You are welcome to refer to your favourite linear regression textbook instead of these.)

\begin{itemize}
\tightlist
\item
  James, Gareth, Daniela Witten, Trevor Hastie and Robert Tibshirani, 2017, \emph{An Introduction to Statistical Learning with Applications in R}, Chapter 3, freely available at: \url{http://faculty.marshall.usc.edu/gareth-james/ISL/}.
\item
  Wickham, Hadley, and Garrett Grolemund, 2017, \emph{R for Data Science}, Chapter 23, freely available at: \url{https://r4ds.had.co.nz/}.
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
  Angrist, Joshua D., and Jörn-Steffen Pischke, 2008, \emph{Mostly harmless econometrics: An empiricist's companion}, Princeton University Press, Chapter 3.4.3.
\item
  Cunningham, Scott, \emph{Causal Inference: The Mixtape}, Chapters `Probability theory and statistics review' and `Properties of Regression', freely available at: \url{http://www.scunning.com/causalinference_norap.pdf}.
\item
  ElHabr, Tony, 2019, `A Bayesian Approach to Ranking English Premier League Teams (using R)', freely available at: \url{https://tonyelhabr.rbind.io/post/bayesian-statistics-english-premier-league/}.
\item
  Greenland, Sander, Stephen J. Senn, Kenneth J. Rothman, John B. Carlin, Charles Poole, Steven N. Goodman, and Douglas G. Altman. ``Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations.'' European journal of epidemiology 31, no. 4 (2016): 337-350.
\item
  Ioannidis, John PA, 2005, `Why most published research findings are false', PLos med 2, no. 8, e124.
\item
  Ronald L. Wasserstein and Nicole A. Lazar, 2016, `The ASA Statement on p-Values: Context, Process, and Purpose', \emph{The American Statistician}, 70:2, 129-133, DOI: 10.1080/00031305.2016.1154108.
\item
  Silge, Julia, 2019, `Modeling salary and gender in the tech industry', freely available at: \url{https://juliasilge.com/blog/salary-gender/}.
\item
  Silge, Julia, 2019, `Opioid prescribing habits in Texas', freely available at: \url{https://juliasilge.com/blog/texas-opioids/}.
\item
  Silge, Julia, 2019, `Tidymodels', freely available at: \url{https://juliasilge.com/blog/intro-tidymodels/}.
\item
  Taddy, Matt, 2019, \emph{Business Data Science}, Chapter 2.
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\tightlist
\item
  Linear regression.
\item
  Uncertainty.
\item
  Threats to validity.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{broom}
\item
  \texttt{ggplot2}
\item
  \texttt{modelr}
\item
  \texttt{tidyverse}
\end{itemize}

\textbf{Key functions}

\begin{itemize}
\tightlist
\item
  \texttt{augment()}
\item
  \texttt{glance()}
\item
  \texttt{lm()}
\item
  \texttt{tidy()}
\end{itemize}

\textbf{Pre-quiz}

\begin{itemize}
\tightlist
\item
  Please write a linear relationship between some response variable, Y, and some predictor, X. What is the intercept term? What is the slope term? What would adding a hat to these indicate?
\item
  What is the least squares criterion? Similarly, what is RSS and what are we trying to do when we run least squares regression?
\item
  What is bias? What is a confidence interval?
\item
  If there were three variables: Snow, Temperature, and Wind, please write R code that would fit a simple linear regression to explain Snow as a function of Temperature and Wind. What do you think about another explanatory variable - daily stock market returns - to your model?
\end{itemize}

\hypertarget{introduction-18}{%
\subsection{Introduction}\label{introduction-18}}

I am going to follow a combination of ISLR and R4DS. Sometimes the language that is used for specific terms differs over time and between disciplines. For instance, ISLR is written from a machine learning perspective, R4DS is written from a data science perspective, and if you come from a different discipline then the terminology may be slightly different. You're welcome to use whatever you are comfortable with, but here I'll follow ISLR and R4DS.

If we have two variables, \(Y\) and \(X\), then we could characterise the relationship between these as:
\[Y \sim \beta_0 + \beta_1 X.\]

There are two coefficients/parameters, and the intercept is \(\beta_0\), the slope is \(\beta_1\). We are saying that \(Y\) will have some value, \(\beta_0\), even when \(X\) is 0, and that \(Y\) will change by \(\beta_1\) units for every one unit change in \(X\).

We may then take this relationship to the data that we have about the relationship in order to estimate these coefficients for those particular values that we have:
\[\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x.\]

\hypertarget{competing-relationships}{%
\subsection{Competing relationships}\label{competing-relationships}}

I want to focus on data, so we'll make this example concrete, by generating some data and then discussing everything in the context of that. The example will be looking at someone's time for running five kilometers, compared with their time for running a marathon.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}
\NormalTok{number_of_observations <-}\StringTok{ }\DecValTok{100}
\NormalTok{running_data <-}\StringTok{ }
\StringTok{  }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{five_km_time =} \KeywordTok{rnorm}\NormalTok{(number_of_observations, }\DecValTok{20}\NormalTok{, }\DecValTok{3}\NormalTok{),}
         \DataTypeTok{noise =} \KeywordTok{rnorm}\NormalTok{(number_of_observations, }\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{),}
         \DataTypeTok{marathon_time =}\NormalTok{ five_km_time }\OperatorTok{*}\StringTok{ }\FloatTok{8.4} \OperatorTok{+}\StringTok{ }\NormalTok{noise,}
         \DataTypeTok{was_raining =} \KeywordTok{sample}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{), }
                              \DataTypeTok{size =}\NormalTok{ number_of_observations,}
                              \DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{, }
                              \DataTypeTok{prob =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.8}\NormalTok{)) }
\NormalTok{         )}

\NormalTok{running_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ five_km_time, }\DataTypeTok{y =}\NormalTok{ marathon_time)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Five km time (minutes)"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Marathon time (minutes)"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-348-1.pdf}

In this set-up we may like to use \(x\), which is the five-kilometer time, to produce estimates of \(y\), which is the marathon time. This would involve also estimating values of \(\beta_0\) and \(\beta_1\), which is why they have a hat on them.

But how should we estimate the coefficients? Even if we impose a linear relationship there are a lot of options, but here are some:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# This great idea of showing some possible fits is from Wickham, Hadley, and Garrett Grolemund, 2017.}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}
\NormalTok{models <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}
  \DataTypeTok{a1 =} \KeywordTok{runif}\NormalTok{(}\DecValTok{750}\NormalTok{, }\DecValTok{-40}\NormalTok{, }\DecValTok{400}\NormalTok{),}
  \DataTypeTok{a2 =} \KeywordTok{runif}\NormalTok{(}\DecValTok{750}\NormalTok{, }\DecValTok{-16}\NormalTok{, }\DecValTok{16}\NormalTok{)}
\NormalTok{  )}

\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_abline}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ models, }
              \KeywordTok{aes}\NormalTok{(}\DataTypeTok{intercept =}\NormalTok{ a1, }\DataTypeTok{slope =}\NormalTok{ a2), }
              \DataTypeTok{alpha =} \DecValTok{1}\OperatorTok{/}\DecValTok{4}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ running_data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ five_km_time, }
                                      \DataTypeTok{y =}\NormalTok{ marathon_time)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"five-kilometer time (minutes)"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Marathon time (minutes)"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-349-1.pdf}

Clearly some of these fits are not all that great. One way we may define being great would be to impose that they be as close as possible to each of the \(x\) and \(y\) combinations that we know. One way is to choose the one that minimises the sum of least squares. To do this we produce our estimates of \(\hat{y}\) based on some estimates of \(\hat{\beta}_0\) and \(\hat{\beta}_1\), given the \(x\), and then work out how `wrong', for every point \(i\), we were:
\[ e_i = y_i - \hat{y}_i.\]

The residual sum of squares (RSS) then requires summing across all the points:
\[ \mbox{RSS} = e^2_1+ e^2_2 +\dots + e^2_n.\]
This results in one `linear best-fit' line, but it is worth thinking about all of the assumptions and decisions that it took to get us to this point.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{running_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ five_km_time, }\DataTypeTok{y =}\NormalTok{ marathon_time)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{color =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"five-kilometer time (minutes)"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Marathon time (minutes)"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-350-1.pdf}

With the least squares criterion we want the values of \(\hat{\beta}_0\) and \(\hat{\beta}_1\) that result in the smallest RSS.

\hypertarget{implementing-this-in-r}{%
\subsection{Implementing this in R}\label{implementing-this-in-r}}

Within R, the main function for doing linear regression is \texttt{lm}. This is included in base R, so you don't need to call any packages, but in a moment we will call a bunch of packages that will surround \texttt{lm} within an environment that we are more familiar with. You specify the relationship with the dependent variable first, then \texttt{\textasciitilde{}}, then the independent variables. Finally, you should specify the dataset (or you could pipe to it as usual).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x, }\DataTypeTok{data =}\NormalTok{ dataset)}
\end{Highlighting}
\end{Shaded}

In general, you should assign this to an object:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{running_data_first_model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(marathon_time }\OperatorTok{~}\StringTok{ }\NormalTok{five_km_time, }
                               \DataTypeTok{data =}\NormalTok{ running_data)}
\end{Highlighting}
\end{Shaded}

To see the result of your regression you can then call \texttt{summary()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(running_data_first_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = marathon_time ~ five_km_time, data = running_data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -24.763  -5.686   0.722   6.650  16.707 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept)    0.4114     6.0610   0.068    0.946    
## five_km_time   8.3617     0.3058  27.343   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 8.474 on 98 degrees of freedom
## Multiple R-squared:  0.8841, Adjusted R-squared:  0.8829 
## F-statistic: 747.6 on 1 and 98 DF,  p-value: < 2.2e-16
\end{verbatim}

The first part of the result tells us the regression that we called, then information about the residuals, and the estimated coefficients. And then finally some useful diagnostics.

We are considering that there is some relationship between \(X\) and \(Y\) - \(Y = f(X) + \epsilon\) - and we are going to say that function is linear and so our relationship is:
\[\hat{Y} = \beta_0 + \beta_1 X + \epsilon.\]

There is some `true' relationship between \(X\) and \(Y\), but we don't know what it is. All we can do is use our sample of data to try to estimate it. But because our understanding depends on that sample, for every possible sample, we would get a slightly different relationship (as measured by the coefficients).

That \(\epsilon\) is a measure of our error - what does the model not know? There's going to be plenty that the model doesn't know, but we hope is that the error does not depend on \(X\).

The intercept is marathon time that we would expect with a five-kilometer time of 0 minutes. Hopefully this example illustrates the need to carefully interpret the intercept coefficient! The coefficient on five-kilometer run time shows how we expect the marathon time to change if five-kilometer run time changed by one unit. In this case it's about 8.4, which makes sense seeing as a marathon is roughly that many times longer than a five-kilometer run.

\hypertarget{tidy-up-with-broom}{%
\subsection{Tidy up with broom}\label{tidy-up-with-broom}}

While there is nothing wrong with the base approach, I want to introduce the \texttt{broom} package because that will provide us with outputs in a tidy framework. There are three key functions:

\begin{itemize}
\tightlist
\item
  \texttt{tidy}: Gives the coefficient estimates in a tidy output.
\item
  \texttt{glance}: Gives the diagnostics.
\item
  \texttt{augment}: Adds the forecasted values, and hence, residuals, to your dataset.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(broom)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tidy}\NormalTok{(running_data_first_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 5
##   term         estimate std.error statistic  p.value
##   <chr>           <dbl>     <dbl>     <dbl>    <dbl>
## 1 (Intercept)     0.411     6.06     0.0679 9.46e- 1
## 2 five_km_time    8.36      0.306   27.3    1.17e-47
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{glance}\NormalTok{(running_data_first_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 12
##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC
##       <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>
## 1     0.884         0.883  8.47      748. 1.17e-47     1  -355.  715.  723.
## # ... with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>
\end{verbatim}

Notice how the results are fairly similar to the base summary function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{running_data <-}\StringTok{ }
\StringTok{  }\KeywordTok{augment}\NormalTok{(running_data_first_model,}
          \DataTypeTok{data =}\NormalTok{ running_data)}
\end{Highlighting}
\end{Shaded}

We could now make plots of the residuals.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(running_data, }
       \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ .resid)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{y =} \StringTok{"Number of occurrences"}\NormalTok{,}
       \DataTypeTok{x =} \StringTok{"Residuals"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-357-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(running_data, }\KeywordTok{aes}\NormalTok{(five_km_time, .resid)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_hline}\NormalTok{(}\DataTypeTok{yintercept =} \DecValTok{0}\NormalTok{, }\DataTypeTok{linetype =} \StringTok{"dotted"}\NormalTok{, }\DataTypeTok{color =} \StringTok{"grey"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{y =} \StringTok{"Residuals"}\NormalTok{,}
       \DataTypeTok{x =} \StringTok{"five-kilometer time (minutes)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-357-2.pdf}

When we say our estimate is unbiased we are trying to say that even though with some sample our estimate might be too high, and with another sample our estimate might be too low, eventually if we have a lot of data then our estimate would be the same as the population. (A pro hockey player may sometimes shoot right of the net, and sometimes left of the net, but we'd hope that on average they'd be right in the middle of the net ;)).

\textbf{TODO: Add my favourite graph}

But we want to try to speak to the `true' relationship, so we need to try to capture how much we think our understanding depends on the particular sample that we have to analyse. And this is where standard error comes in. It tells us how off our estimate is compared with the actual.

From standard errors, we can compute a confidence interval. A 95 per cent confidence interval means that there is a 0.95 probability that the interval happens to contain the population parameter (which is typically unknown).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{running_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ five_km_time, }\DataTypeTok{y =}\NormalTok{ marathon_time)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }\DataTypeTok{se =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{color =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"five-kilometer time (minutes)"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Marathon time (minutes)"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-358-1.pdf}

\hypertarget{testing-hypothesis}{%
\subsection{Testing hypothesis}\label{testing-hypothesis}}

Now that we have an interval for which we can say there is a 95 per cent probability it contains the true population parameter we can test claims. For instance, a null hypothesis that there is no relationship between \(X\) and \(Y\) (i.e.~\(\beta_1 = 0\)), compared with an alternative hypothesis that there is some relationship between \(X\) and \(Y\) (i.e.~\(\beta_1 \neq 0\)).

We need to know whether our estimate of \(\beta_1\), which is \(\hat{\beta}_1\), is `far enough' away from zero for us to be comfortable claiming that \(\beta_1 \neq 0\). How far is `far enough'? If we were very confident in our estimate of \(\beta_1\) then it wouldn't have to be far, but if we were not then it would have to be substantial. So it depends on a bunch of things, but essentially the standard error of \(\hat{\beta}_1\).

We compare this standard error with \(\hat{\beta}_1\) to get the t-statistic:
\[t = \frac{\hat{\beta}_1 - 0}{\mbox{SE}(\hat{\beta}_1)}.\]
And we then compare our t-statistic to the t-distribution to compute the probability of getting this absolute t-statistic or a larger one, if \(\beta_1 = 0\). This is the p-value. A small p-value means it is unlikely that we would observe our association due to chance if there wasn't a relationship.

\hypertarget{adding-more-and-varied-explanatory-variables}{%
\subsection{Adding more and varied explanatory variables}\label{adding-more-and-varied-explanatory-variables}}

To this point we've just considered one explanatory variable. But we'll usually have more than one. One approach would be to run separate regressions for each explanatory variable. But compared with separate linear regressions for each, adding more explanatory variables allows us to have a better understanding of the intercept and accounts for interaction. Often the results will be quite different.

\begin{quote}
This slightly counterintuitive result is very common in many real life situations. Consider an absurd example to illustrate the point. Running a regression of shark attacks versus ice cream sales for data collected at a given beach community over a period of time would show a positive relationship, similar to that seen between sales and newspapers. Of course no one (yet) has suggested that ice creams should be banned at beaches to reduce shark attacks. In reality, higher temperatures cause more people to visit the beach, which in turn results in more ice cream sales and mores hark attacks. A multiple regression of attacks versus ice cream sales and temperature reveals that, as intuition implies, the former predictor is no longer significant after adjusting for temperature.

James, Gareth, Daniela Witten, Trevor Hastie and Robert Tibshirani, 2017, p.~74.
\end{quote}

We may also like to consider variables that do not have an inherent ordering. For instance, pregnant or not. When there are only two options then we can use a binary variable which is 0 or 1. If there are more than two levels then use a combination of binary variables, where the `missing' outcome (baseline) gets pushed onto the intercept.

In other languages you may need to explicitly construct dummy variables, but as R was designed as a language to do statistical programming, it does a lot of the work here for you and is fairly forgiving. For instance, if you have a column of character values that only had two values: \texttt{c("Monica",\ "Rohan",\ "Rohan",\ "Monica",\ "Monica",\ "Rohan")}, and you used this as a independent variable in your usual regression set up then R would treat it as a dummy variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{running_data_rain_model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(marathon_time }\OperatorTok{~}\StringTok{ }\NormalTok{five_km_time }\OperatorTok{+}\StringTok{ }\NormalTok{was_raining, }
                               \DataTypeTok{data =}\NormalTok{ running_data)}
\KeywordTok{summary}\NormalTok{(running_data_rain_model)}
\end{Highlighting}
\end{Shaded}

The result probably isn't too surprising if we look at a plot of the data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{running_data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ five_km_time, }\DataTypeTok{y =}\NormalTok{ marathon_time, }\DataTypeTok{color =}\NormalTok{ was_raining)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{color =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"five-kilometer time (minutes)"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Marathon time (minutes)"}\NormalTok{,}
       \DataTypeTok{color =} \StringTok{"Was raining"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-360-1.pdf}

In addition to wanting to include additional explanatory variables we may think that they are related with one another. For instance, if we were wanting to explain the amount of snowfall in Toronto, then we may be interested in the humidity and the temperature, but those two variables may also interact. We can do this by using \texttt{*} instead of \texttt{+} when we specify the model in R. If you do interact variables, then you should almost always also include the individual variables as well (Figure \ref{fig:trump}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{figures/trump} \caption{Don't leave out the main effects in an interactive model}\label{fig:trump}
\end{figure}

Source: By \href{https://twitter.com/kai_arzheimer/status/1228998718646607876}{Kai Arzheimer}, 16 February 2020.

\hypertarget{threats-to-validity-and-aspects-to-think-about}{%
\subsection{Threats to validity and aspects to think about}\label{threats-to-validity-and-aspects-to-think-about}}

There are a variety of weaknesses and aspects that you should discuss when you use linear regression. A quick list includes (James, Gareth, Daniela Witten, Trevor Hastie and Robert Tibshirani, 2017, p.~92):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Non-linearity of the response-predictor relationships.
\item
  Correlation of error terms.
\item
  Non-constant variance of error terms.
\item
  Outliers.
\item
  High-leverage points.
\item
  Collinearity
\end{enumerate}

These are also aspects that you should discuss if you use linear regression. Including plots tends to be handy here to illustrate your points. Other aspects that you may consider discussing include (James, Gareth, Daniela Witten, Trevor Hastie and Robert Tibshirani, 2017, p.~75):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Is at least one of the predictors \(X_1, X_2, \dots, X_p\) useful in predicting the response?
\item
  Do all the predictors help to explain \(Y\), or is only a subset of the predictors useful?
\item
  How well does the model fit the data?
\item
  Given a set of predictor values, what response value should we predict, and how accurate is our prediction?
\end{enumerate}

\hypertarget{more-credible-outputs}{%
\subsection{More credible outputs}\label{more-credible-outputs}}

Finally, after creating beautiful graphs and tables you may want your regression output to look just as nice. There are a variety of packages in R that will automatically format your regression outputs. You should try \texttt{huxtable}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# library(huxtable)}
\CommentTok{# huxreg(running_data_first_model)}
\end{Highlighting}
\end{Shaded}

\hypertarget{classification}{%
\section{Classification}\label{classification}}

asdf

\hypertarget{count-data}{%
\section{Count data}\label{count-data}}

\hypertarget{logistic-regression-2}{%
\subsection{Logistic regression}\label{logistic-regression-2}}

asdf

\hypertarget{poisson-regression}{%
\subsection{Poisson regression}\label{poisson-regression}}

asdf

\hypertarget{difference-in-differences}{%
\chapter{Difference in differences}\label{difference-in-differences}}

\textbf{TODO: Replace the arm matching with \url{https://kosukeimai.github.io/MatchIt/index.html}}

\textbf{Required reading}

\begin{itemize}
\tightlist
\item
  Angelucci, Charles, and Julia Cagé, 2019, `Newspapers in times of low advertising revenues', American Economic Journal: Microeconomics, vol.~11, no. 3, pp.~319-364, DOI: 10.1257/mic.20170306, available at: \url{https://www.aeaweb.org/articles?id=10.1257/mic.20170306}.
\item
  Gertler, Paul, Sebastian Martinez, Patrick Premand, Laura Rawlings, and Christel Vermeersch, `Impact Evaluation in Practice', Chapters 7 and 8.
\item
  Gelman, Andrew, Jennifer Hill and Aki Vehtari, 2020, Regression and Other Stories, Cambridge University Press, Chs 18 - 21.
\item
  McElreath, Richard, 2020, Statistical Rethinking, 2nd Edition, CRC Press, Ch 14.
\item
  Wong, Jeffrey, and Colin McFarland, 2020, `Computational Causal Inference at Netflix', Netflix Technology Blog, 11 Aug, \url{https://netflixtechblog.com/computational-causal-inference-at-netflix-293591691c62}
\end{itemize}

\textbf{Required viewing}

\begin{itemize}
\tightlist
\item
  Gelman, Andrew, 2020 `100 Stories of Causal Inference', 4 August, \url{https://www.youtube.com/watch?v=jnI5KI843Lk}.
\item
  King, Gary, 2020, `Research Designs', Lectures on Quantitative Social Science Methods 1, freely available: \url{https://youtu.be/27grU_VM5Ps}.
\item
  Kuriwaki, Shiro, 2020, `Difference-in-Differences Estimation in R (parts 1 and 2)', 18 April, \url{https://vimeo.com/409267138} and \url{https://vimeo.com/409267190}.
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
  Alexander, Monica, Polimis, Kivan, and Zagheni, Emilio, 2019,' The impact of Hurricane Maria on out-migration from Puerto Rico: Evidence from Facebook data', \emph{Population and Development Review}. (Example of using diff-in-diff to measure the effect of Hurricane Maria.)
\item
  Alexander, Rohan, and Zachary Ward, 2018, `Age at arrival and assimilation during the age of mass migration', \emph{The Journal of Economic History}, 78, no. 3, 904-937. (Example where I used differences between brothers to estimate the effect of education.)
\item
  Angrist, Joshua D., and Jörn-Steffen Pischke, 2008, \emph{Mostly harmless econometrics: An empiricist's companion}, Princeton University Press, Chapters 3.3.2 and 5.
\item
  Austin, Peter C., 2011, `An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies', \emph{Multivariate Behavioral Research}, vol.~46, no. 3, pp.399-424. (Broad overview of propensity score matching, with a nice discussion of the comparison to randomised controlled trials.)
\item
  Baker, Andrew, 2019, `Difference-in-Differences Methodology', 25 September, \url{https://andrewcbaker.netlify.app/2019/09/25/difference-in-differences-methodology/}.
\item
  Cunningham, Scott, \emph{Causal Inference: The Mixtape}, chapters `Matching and subclassifications' and `Differences-in-differences', \url{http://www.scunning.com/causalinference_norap.pdf}. (Very well-written notes on diff-in-diff.)
\item
  Gelman, Andrew, and Jennifer Hill, 2007, \emph{Data Analysis Using Regression and Muiltilevel/Hierarchical Models}, Chapter 10, pp.~207-212.
\item
  King, Gary, and Richard Nielsen, 2019, `Why Propensity Scores Should Not Be Used for Matching', \emph{Political Analysis}. (Academic paper on the limits of propensity score matching. Propensity score matching was a big thing in the 90s but everyone knew about these weaknesses and so it died off. Lately, there has been a resurgence because of the CS/ML folks using it without thinking so King and Nielsen wrote a nice paper about the flaws. I mean, you can't say you weren't warned.)
\item
  Saeed, Sahar, Erica E. M. Moodie, Erin C. Strumpf, Marina B. Klein, 2019, `Evaluating the impact of health policies: using a difference-in-differences approach', \emph{International Journal of Public Health}, 64, pp.~637--642, \url{https://doi.org/10.1007/s00038-018-1195-2}.
\item
  Taddy, Matt, 2019, \emph{Business Data Science}, Chapter 5. (Some brief notes on diff-in-diff that may appeal to some students.)
\item
  Tang, John, 2015, `Pollution havens and the trade in toxic chemicals: evidence from U.S. trade flows', \emph{Ecological Economics}, vol.~112, pp.~150-160. (Example of using diff-in-diff to estimate pollution.)
\item
  Valencia Caicedo, Felipe. `The mission: Human capital transmission, economic persistence, and culture in South America.' The Quarterly Journal of Economics 134.1 (2019): 507-556. (Data available at: Valencia Caicedo, Felipe, 2018, ``Replication Data for: `The Mission: Human Capital Transmission, Economic Persistence, and Culture in South America'\,'', \url{https://doi.org/10.7910/DVN/ML1155}, Harvard Dataverse, V1.).
\end{itemize}

\textbf{Key concepts/skills/etc}

\emph{Difference-in-differences}

\begin{itemize}
\tightlist
\item
  Essential matching methods.
\item
  Weaknesses of matching.
\item
  Difference-in-differences.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{broom}
\item
  \texttt{tidyverse}
\end{itemize}

\textbf{Key functions/etc}

\begin{itemize}
\tightlist
\item
  \texttt{tidy()}
\item
  \texttt{lm()}
\end{itemize}

\textbf{Pre-quiz}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sharla Gelfand has been `(s)haring two \#rstats functions most days - one I know and love, and one that's new to me!'. Please go to Sharla's GitHub page: \url{https://github.com/sharlagelfand/twofunctionsmostdays}. Please find a package that she mentions that you have never used. Please find the relevant website for the package. Please describe what the package does and a context in which it could be useful to you.
\item
  Sharla Gelfand has been `(s)haring two \#rstats functions most days - one I know and love, and one that's new to me!'. Please go to Sharla's GitHub page: \url{https://github.com/sharlagelfand/twofunctionsmostdays}. Please find a function that she mentions that you have never used. Please look at the help file for that function. Please detail the arguments of the function, and a context in which it could be useful to you.
\item
  What is propensity score matching? If you were matching people, then what are some of the features that you would like to match on? What sort of ethical questions does collecting and storing such information raise for you?
\item
  Putting to one side, the ethical issues, what are some statistical weaknesses with propensity score matching?
\item
  What is the key assumption when using diff-in-diff?
\item
  Please read the fascinating article in The Markup about car insurance algorithms: \url{https://themarkup.org/allstates-algorithm/2020/02/25/car-insurance-suckers-list}. Please read the article and tell me what you think. You may wish to focus on ethical, legal, social, statistical, or other, aspects.
\item
  Please go to the GitHub page related to the fascinating article in The Markup about car insurance algorithms: \url{https://github.com/the-markup/investigation-allstates-algorithm}. What is great about their work? What could be improved?
\end{enumerate}

\hypertarget{introduction-19}{%
\section{Introduction}\label{introduction-19}}

Life it grand when you can conduct experiments to be able to speak to causality. But what if you can only run the survey - you can't run an experiment? Here we begin our discussion of the circumstances and methods that would allow you to nonetheless speak to causality. We use (relatively) simple methods, in sophisticated, well-developed, ways (cf, much of what is done these days) and our applied statistics draw from a variety of social sciences including economics, and political science.

\hypertarget{matching-and-difference-in-differences}{%
\section{Matching and difference-in-differences}\label{matching-and-difference-in-differences}}

\hypertarget{introduction-20}{%
\subsection{Introduction}\label{introduction-20}}

The ideal situation, as described in the previous chapter, is rarely possible in a data science setting. Can we really reasonably expect that Netflix would allow us to change prices. And even if they did once, would they let us do it again, and again, and again? Further, rarely can we explicitly create treatment and control groups. Finally, experiments are really expensive and potentially unethical. Instead, we need to make do with what we have. Rather than our counterfactual coming to us through randomisation, and hence us knowing that the two are the same but for the treatment, we try to identify groups that were similar before the treatment, and hence any differences can be attributed to the treatment. In practice, we tend to even have differences between our two groups before we treat. Provided those pre-treatment differences satisfy some assumptions (basically that they were consistent, and we expect that consistency to continue in the absence of the treatment) -- the `parallel trends' assumption -- then we can look to any difference in the differences as the effect of the treatment. One of the lovely aspects of difference in differences analysis is that we can do it using fairly straight-forward quantitative methods - linear regression with a dummy variable is all that is needed to do a convincing job.

\hypertarget{motivation}{%
\subsection{Motivation}\label{motivation}}

Consider us wanting to know the effect of a new tennis racket on serve speed. One way to test this would be to measure the difference between Roger Federer's serve speed without the tennis racket and mine with the tennis racket. Sure, we'd find a difference but how do we know how much to attribute to the tennis racket? Another way would be to consider the difference between my serve speed without the tennis racket and my serve speed with the tennis racket. But what if serves were just getting faster naturally over time? Instead, let's combine the two to look at the difference in the differences!

In this world we measure Federer's serve and compare it to my serve without the new racket. We then measure Federer's serve again and measure my serve with the new racket. That difference in the differences would then be our estimate of the effect of the new racket.

What sorts of assumptions jump out at you that we are going to have to make in order for this analysis to be appropriate?

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Is there something else that may have affected only me, and not Roger that could affect my serve speed? Probably.
\item
  Is it likely that Roger Federer and I have the same trajectory of serve speed improvement? Probably not. This is the `parallel trends' assumption, and it dominates any discussion of difference in differences analysis. Finally, is it likely that the variance of our serve speeds is the same? Probably not.
\end{enumerate}

Why might this be powerful? We don't need the treatment and control group to be the same before the treatment. We just need to have a good idea of how they differ.

\hypertarget{simulated-example}{%
\subsection{Simulated example}\label{simulated-example}}

Let's generate some data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(broom)}
\KeywordTok{library}\NormalTok{(tidyverse)}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{diff_in_diff_example_data <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{person =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{1000}\NormalTok{), }\DataTypeTok{times =} \DecValTok{2}\NormalTok{),}
                       \DataTypeTok{time =} \KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DataTypeTok{times =} \DecValTok{1000}\NormalTok{), }\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{times =} \DecValTok{1000}\NormalTok{)),}
                       \DataTypeTok{treatment_group =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{sample}\NormalTok{(}\DataTypeTok{x =} \DecValTok{0}\OperatorTok{:}\DecValTok{1}\NormalTok{, }\DataTypeTok{size  =} \DecValTok{1000}\NormalTok{, }\DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{), }\DataTypeTok{times =} \DecValTok{2}\NormalTok{)}
\NormalTok{                       )}
\CommentTok{# We want to make the outcome slightly more likely if they were treated than if not.}
\NormalTok{diff_in_diff_example_data <-}\StringTok{ }
\StringTok{  }\NormalTok{diff_in_diff_example_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{rowwise}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{serve_speed =} \KeywordTok{case_when}\NormalTok{(}
\NormalTok{    time }\OperatorTok{==}\StringTok{ }\DecValTok{0} \OperatorTok{&}\StringTok{ }\NormalTok{treatment_group }\OperatorTok{==}\StringTok{ }\DecValTok{0} \OperatorTok{~}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n =} \DecValTok{1}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{5}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{1}\NormalTok{),}
\NormalTok{    time }\OperatorTok{==}\StringTok{ }\DecValTok{1} \OperatorTok{&}\StringTok{ }\NormalTok{treatment_group }\OperatorTok{==}\StringTok{ }\DecValTok{0} \OperatorTok{~}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n =} \DecValTok{1}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{6}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{1}\NormalTok{),}
\NormalTok{    time }\OperatorTok{==}\StringTok{ }\DecValTok{0} \OperatorTok{&}\StringTok{ }\NormalTok{treatment_group }\OperatorTok{==}\StringTok{ }\DecValTok{1} \OperatorTok{~}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n =} \DecValTok{1}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{8}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{1}\NormalTok{),}
\NormalTok{    time }\OperatorTok{==}\StringTok{ }\DecValTok{1} \OperatorTok{&}\StringTok{ }\NormalTok{treatment_group }\OperatorTok{==}\StringTok{ }\DecValTok{1} \OperatorTok{~}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n =} \DecValTok{1}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{14}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{1}\NormalTok{),}
\NormalTok{    )}
\NormalTok{    )}

\KeywordTok{head}\NormalTok{(diff_in_diff_example_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 4
## # Rowwise: 
##   person  time treatment_group serve_speed
##    <int> <dbl>           <int>       <dbl>
## 1      1     0               0        4.43
## 2      2     0               1        6.96
## 3      3     0               1        7.77
## 4      4     0               0        5.31
## 5      5     0               0        4.09
## 6      6     0               0        4.85
\end{verbatim}

Let's make a graph.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diff_in_diff_example_data}\OperatorTok{$}\NormalTok{treatment_group <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(diff_in_diff_example_data}\OperatorTok{$}\NormalTok{treatment_group)}
\NormalTok{diff_in_diff_example_data}\OperatorTok{$}\NormalTok{time <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(diff_in_diff_example_data}\OperatorTok{$}\NormalTok{time)}

\NormalTok{diff_in_diff_example_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ time,}
             \DataTypeTok{y =}\NormalTok{ serve_speed,}
             \DataTypeTok{color =}\NormalTok{ treatment_group)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{group =}\NormalTok{ person), }\DataTypeTok{alpha =} \FloatTok{0.2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Time period"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Serve speed"}\NormalTok{,}
       \DataTypeTok{color =} \StringTok{"Person got a new racket"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-363-1.pdf}

As it is a simple example, we could do this manually, by getting the average difference of the differences.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{average_differences <-}\StringTok{ }
\StringTok{  }\NormalTok{diff_in_diff_example_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{pivot_wider}\NormalTok{(}\DataTypeTok{names_from =}\NormalTok{ time,}
              \DataTypeTok{values_from =}\NormalTok{ serve_speed,}
              \DataTypeTok{names_prefix =} \StringTok{"time_"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{difference =}\NormalTok{ time_}\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{time_}\DecValTok{0}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(treatment_group) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{average_difference =} \KeywordTok{mean}\NormalTok{(difference))}

\NormalTok{average_differences}\OperatorTok{$}\NormalTok{average_difference[}\DecValTok{2}\NormalTok{] }\OperatorTok{-}\StringTok{ }\NormalTok{average_differences}\OperatorTok{$}\NormalTok{average_difference[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.058414
\end{verbatim}

Let's use OLS to do the same analysis. The general regression equation is:
\[Y_{i,t} = \beta_0 + \beta_1\mbox{Treatment group dummy}_i + \beta_2\mbox{Time dummy}_t + \beta_3(\mbox{Treatment group dummy} \times\mbox{Time dummy})_{i,t} + \epsilon_{i,t}\]

If we use \texttt{*} in the regression then it automatically includes the separate aspects as well as their interaction. It's the estimate of \(\beta_3\) which is of interest.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diff_in_diff_example_regression <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(serve_speed }\OperatorTok{~}\StringTok{ }\NormalTok{treatment_group}\OperatorTok{*}\NormalTok{time, }
                         \DataTypeTok{data =}\NormalTok{ diff_in_diff_example_data)}

\KeywordTok{tidy}\NormalTok{(diff_in_diff_example_regression)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4 x 5
##   term                   estimate std.error statistic  p.value
##   <chr>                     <dbl>     <dbl>     <dbl>    <dbl>
## 1 (Intercept)                4.97    0.0428     116.  0.      
## 2 treatment_group1           3.03    0.0622      48.7 0.      
## 3 time1                      1.01    0.0605      16.6 2.97e-58
## 4 treatment_group1:time1     5.06    0.0880      57.5 0.
\end{verbatim}

Fortunately, our estimates are the same!

\hypertarget{assumptions}{%
\subsection{Assumptions}\label{assumptions}}

If we want to use difference in differences, then we need to satisfy the assumptions. There were three that were touched on earlier, but here I want to focus on one: the `parallel trends' assumption. The parallel trends assumption haunts everything to do with diff-in-diff analysis because we can never prove it, we can just be convinced of it.

To see why we can never prove it, consider an example in which we want to know the effect of a new stadium on a professional sports team's wins/loses. To do this we consider two teams: the Warriors and the Raptors. The Warriors changed stadiums at the start of the 2019-20 season (the Raptors did not), so we will consider four time periods: the 2016-17 season, 2017-18 season, 2018-19 season, and finally we will compare the performance with the one after they moved, so in the 2019-20 season. The Raptors here act as our counterfactual. This means that we assume the relationship between the Warriors and the Raptors, in the absence of a new stadium, would have continued to change in a consistent way. But we can never know that for certain. We have to present sufficient evidence to assuage any concerns that a reader may have.

For a variety of reasons, it is worth having tougher than normal requirements around the evidence it would take to convince you of an effect.

There are four main `threats to validity' when you are using difference in differences and you should address all of these (Cunningham, 2020, pp.~272--277):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Non-parallel trends. The treatment and control groups may be based on differences. As such it can be difficult to convincingly argue for parallel trends. In this case, maybe try to find another factor to consider in your model that may adjust for some of that. This may require difference in difference in differences (in the earlier example, perhaps could add in the San Francisco 49ers as they are in the same broad geographic area as the Warriors). Or maybe re-think your analysis to see if you can make a different control group. Adding additional earlier time periods may help but may introduce more issues (see third point).
\item
  Compositional differences. This is a concern when working with repeated cross-sections. What if the composition of those cross-sections change? For instance, if we work at Tik Tok or some other app that is rapidly growing and want to look at the effect of some change. In our initial cross-section, we may have mostly young people, but in a subsequent cross-section, we may have more older people as the demographics of the app usage change. Hence our results may just be an age-effect, not an effect of the change that we are interested in.
\item
  Long-term effects vs.~reliability. As we discussed in the last chapter, there is a trade-off between the length of the analysis that we run. As we run the analysis for longer there is more opportunity for other factors to affect the results. There is also increased chance for someone who was not treated to be treated. But, on the other hand, it can be difficult to convincingly argue that short-term results will continue in the long-term.
\item
  Functional form dependence. This is less of an issue when the outcomes are similar, but if they are different then functional form may be responsible for some aspects of the results.
\end{enumerate}

\hypertarget{matching}{%
\subsection{Matching}\label{matching}}

\emph{This section draws on material from Gelman and Hill, 2007, pp.~207-212.}

Difference in differences is a powerful analysis framework. After I learnt about it I began to see opportunities to implement it everywhere. But it can be tough to identify appropriate treatment and control groups. In Alexander and Ward, 2018, we compare migrant brothers - one of whom had most of their education in a different country, and the other who had most of their education in the US. Is this really the best match?

We may be able to match based on observable variables. For instance, age-group or education. At two different times we compare smoking rates in 18-year-olds in one city with smoking rates in 18-year-olds in another city. That is fine, but it is fairly coarse. We know that there are differences between 18-year-olds, even in terms of the variables that we commonly observe, say gender and education. One way to deal with this may be to create sub-groups: 18-year-old males with a high school education, etc. But the sample sizes are likely to quickly become small. How do we deal with continuous variables? And also, is the difference between an 18-year-old and a 19-year-old really so different? Shouldn't we also compare with them?

One way to proceed is to consider a nearest neighbour approach. But there is limited concern for uncertainty in this approach. There is also an issue if you have a large number of variables because you end up with a high-dimension graph. This leads us to propensity score matching.

Propensity score matching involves assigning some probability to each observation. We construct that probability based on the observation's values for the independent variables, at their values before the treatment. That probability is our best guess at the probability of the observation being treated, regardless of whether it was treated or not. For instance, if 18-year-old males were treated but 19-year-old males were not, then as there is not much difference between 18-year-old males and 19-year-old males our assigned probability would be fairly similar. We can then compare the outcomes of observations with similar propensity scores.

One advantage of propensity score matching is that is allows us to easily consider many independent variables at once, and it can be constructed using logistic regression.

Let's generate some data to illustrate propensity score matching. Let's pretend that we work for Amazon. We are going to treat some individuals with free-shipping to see what happens to their average purchase.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample_size <-}\StringTok{ }\DecValTok{10000}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{amazon_purchase_data <-}
\StringTok{  }\KeywordTok{tibble}\NormalTok{(}
    \DataTypeTok{unique_person_id =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{sample_size),}
    \DataTypeTok{age =} \KeywordTok{runif}\NormalTok{(}\DataTypeTok{n =}\NormalTok{ sample_size,}
                \DataTypeTok{min =} \DecValTok{18}\NormalTok{,}
                \DataTypeTok{max =} \DecValTok{100}\NormalTok{),}
    \DataTypeTok{city =} \KeywordTok{sample}\NormalTok{(}
      \DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\StringTok{"Toronto"}\NormalTok{, }\StringTok{"Montreal"}\NormalTok{, }\StringTok{"Calgary"}\NormalTok{),}
      \DataTypeTok{size =}\NormalTok{ sample_size,}
      \DataTypeTok{replace =} \OtherTok{TRUE}
\NormalTok{      ),}
    \DataTypeTok{gender =} \KeywordTok{sample}\NormalTok{(}
      \DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\StringTok{"Female"}\NormalTok{, }\StringTok{"Male"}\NormalTok{, }\StringTok{"Other/decline"}\NormalTok{),}
      \DataTypeTok{size =}\NormalTok{ sample_size,}
      \DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{,}
      \DataTypeTok{prob =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.49}\NormalTok{, }\FloatTok{0.47}\NormalTok{, }\FloatTok{0.02}\NormalTok{)}
\NormalTok{      ),}
    \DataTypeTok{income =} \KeywordTok{rlnorm}\NormalTok{(}\DataTypeTok{n =}\NormalTok{ sample_size,}
                    \DataTypeTok{meanlog =} \FloatTok{0.5}\NormalTok{, }
                    \DataTypeTok{sdlog =} \DecValTok{1}\NormalTok{)}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

Now we need to add some probability of being treated with free shipping, which depends on our variables. Younger, higher-income, male and in Toronto all make it slightly more likely.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{amazon_purchase_data <-}
\StringTok{  }\NormalTok{amazon_purchase_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{age_num =} \KeywordTok{case_when}\NormalTok{(}
\NormalTok{           age }\OperatorTok{<}\StringTok{ }\DecValTok{30} \OperatorTok{~}\StringTok{ }\DecValTok{3}\NormalTok{,}
\NormalTok{           age }\OperatorTok{<}\StringTok{ }\DecValTok{50} \OperatorTok{~}\StringTok{ }\DecValTok{2}\NormalTok{,}
\NormalTok{           age }\OperatorTok{<}\StringTok{ }\DecValTok{70} \OperatorTok{~}\StringTok{ }\DecValTok{1}\NormalTok{,}
           \OtherTok{TRUE} \OperatorTok{~}\StringTok{ }\DecValTok{0}\NormalTok{),}
         \DataTypeTok{city_num =} \KeywordTok{case_when}\NormalTok{(}
\NormalTok{           city }\OperatorTok{==}\StringTok{ "Toronto"} \OperatorTok{~}\StringTok{ }\DecValTok{3}\NormalTok{,}
\NormalTok{           city }\OperatorTok{==}\StringTok{ "Montreal"} \OperatorTok{~}\StringTok{ }\DecValTok{2}\NormalTok{,}
\NormalTok{           city }\OperatorTok{==}\StringTok{ "Calgary"} \OperatorTok{~}\StringTok{ }\DecValTok{1}\NormalTok{,}
           \OtherTok{TRUE} \OperatorTok{~}\StringTok{ }\DecValTok{0}\NormalTok{),}
         \DataTypeTok{gender_num =} \KeywordTok{case_when}\NormalTok{(}
\NormalTok{           gender }\OperatorTok{==}\StringTok{ "Male"} \OperatorTok{~}\StringTok{ }\DecValTok{3}\NormalTok{,}
\NormalTok{           gender }\OperatorTok{==}\StringTok{ "Female"} \OperatorTok{~}\StringTok{ }\DecValTok{2}\NormalTok{,}
\NormalTok{           gender }\OperatorTok{==}\StringTok{ "Other/decline"} \OperatorTok{~}\StringTok{ }\DecValTok{1}\NormalTok{,}
           \OtherTok{TRUE} \OperatorTok{~}\StringTok{ }\DecValTok{0}\NormalTok{),}
         \DataTypeTok{income_num =} \KeywordTok{case_when}\NormalTok{(}
\NormalTok{           income }\OperatorTok{>}\StringTok{ }\DecValTok{3} \OperatorTok{~}\StringTok{ }\DecValTok{3}\NormalTok{,}
\NormalTok{           income }\OperatorTok{>}\StringTok{ }\DecValTok{2} \OperatorTok{~}\StringTok{ }\DecValTok{2}\NormalTok{,}
\NormalTok{           income }\OperatorTok{>}\StringTok{ }\DecValTok{1} \OperatorTok{~}\StringTok{ }\DecValTok{1}\NormalTok{,}
           \OtherTok{TRUE} \OperatorTok{~}\StringTok{ }\DecValTok{0}\NormalTok{)}
\NormalTok{         ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{rowwise}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{sum_num =} \KeywordTok{sum}\NormalTok{(age_num, city_num, gender_num, income_num),}
         \DataTypeTok{softmax_prob =} \KeywordTok{exp}\NormalTok{(sum_num)}\OperatorTok{/}\KeywordTok{exp}\NormalTok{(}\DecValTok{12}\NormalTok{),}
         \DataTypeTok{free_shipping =} \KeywordTok{sample}\NormalTok{(}
           \DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{1}\NormalTok{),}
           \DataTypeTok{size =} \DecValTok{1}\NormalTok{,}
           \DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{,}
           \DataTypeTok{prob =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{-}\NormalTok{softmax_prob, softmax_prob)}
\NormalTok{           )}
\NormalTok{         ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ungroup}\NormalTok{()}

\NormalTok{amazon_purchase_data <-}
\StringTok{  }\NormalTok{amazon_purchase_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{age_num, }\OperatorTok{-}\NormalTok{city_num, }\OperatorTok{-}\NormalTok{gender_num, }\OperatorTok{-}\NormalTok{income_num, }\OperatorTok{-}\NormalTok{sum_num, }\OperatorTok{-}\NormalTok{softmax_prob)}
\end{Highlighting}
\end{Shaded}

Finally, we need to have some measure of a person's average spend. We want those with free shipping to be slightly higher than those without.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{amazon_purchase_data <-}
\StringTok{  }\NormalTok{amazon_purchase_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{mean_spend =} \KeywordTok{if_else}\NormalTok{(free_shipping }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{, }\DecValTok{60}\NormalTok{, }\DecValTok{50}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{rowwise}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{average_spend =} \KeywordTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, mean_spend, }\DataTypeTok{sd =} \DecValTok{5}\NormalTok{)}
\NormalTok{    ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ungroup}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{mean_spend)}

\CommentTok{# Fix the class on some}
\NormalTok{amazon_purchase_data <-}
\StringTok{  }\NormalTok{amazon_purchase_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate_at}\NormalTok{(}\KeywordTok{vars}\NormalTok{(city, gender, free_shipping), }\OperatorTok{~}\KeywordTok{as.factor}\NormalTok{(.)) }\CommentTok{# Change some to factors}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(amazon_purchase_data}\OperatorTok{$}\NormalTok{free_shipping)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##    0    1 
## 9629  371
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(amazon_purchase_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 7
##   unique_person_id   age city     gender income free_shipping average_spend
##              <int> <dbl> <fct>    <fct>   <dbl> <fct>                 <dbl>
## 1                1  47.5 Calgary  Female  1.72  0                      41.1
## 2                2  27.8 Montreal Male    1.54  0                      55.7
## 3                3  57.7 Toronto  Female  3.16  0                      56.5
## 4                4  43.9 Toronto  Male    0.636 0                      50.5
## 5                5  21.1 Toronto  Female  1.43  0                      44.7
## 6                6  51.1 Calgary  Male    1.18  0                      48.8
\end{verbatim}

Now we construct a logistic regression model that `explains' whether a person was treated as a function of the variables that we think explain it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{propensity_score <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(free_shipping }\OperatorTok{~}\StringTok{ }\NormalTok{age }\OperatorTok{+}\StringTok{ }\NormalTok{city }\OperatorTok{+}\StringTok{ }\NormalTok{gender }\OperatorTok{+}\StringTok{ }\NormalTok{income, }
                        \DataTypeTok{family =}\NormalTok{ binomial,}
                        \DataTypeTok{data =}\NormalTok{ amazon_purchase_data)}
\end{Highlighting}
\end{Shaded}

We will now add our forecast to our dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{amazon_purchase_data <-}\StringTok{ }
\StringTok{  }\KeywordTok{augment}\NormalTok{(propensity_score, }
          \DataTypeTok{data =}\NormalTok{ amazon_purchase_data,}
          \DataTypeTok{type.predict =} \StringTok{"response"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{.resid, }\OperatorTok{-}\NormalTok{.std.resid, }\OperatorTok{-}\NormalTok{.hat, }\OperatorTok{-}\NormalTok{.sigma, }\OperatorTok{-}\NormalTok{.cooksd) }
\end{Highlighting}
\end{Shaded}

Now we use our forecast to create matches. There are a variety of ways to do this. In a moment I'll step through some code that does it all at once, but as this is a worked example and we only have a small number of possibilities, we can just do it manually.

For every person who was actually treated (given free shipping) we want the untreated person who was considered as similar to them (based on propensity score) as possible.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{amazon_purchase_data <-}\StringTok{ }
\StringTok{  }\NormalTok{amazon_purchase_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{arrange}\NormalTok{(.fitted, free_shipping)}
\end{Highlighting}
\end{Shaded}

Here we're going to use a matching function from the \texttt{arm} package. This finds which is the closest of the ones that were not treated, to each one that was treated.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{amazon_purchase_data}\OperatorTok{$}\NormalTok{treated <-}\StringTok{ }\KeywordTok{if_else}\NormalTok{(amazon_purchase_data}\OperatorTok{$}\NormalTok{free_shipping }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{amazon_purchase_data}\OperatorTok{$}\NormalTok{treated <-}\StringTok{ }\KeywordTok{as.integer}\NormalTok{(amazon_purchase_data}\OperatorTok{$}\NormalTok{treated)}

\NormalTok{matches <-}\StringTok{ }\NormalTok{arm}\OperatorTok{::}\KeywordTok{matching}\NormalTok{(}\DataTypeTok{z =}\NormalTok{ amazon_purchase_data}\OperatorTok{$}\NormalTok{treated, }\DataTypeTok{score =}\NormalTok{ amazon_purchase_data}\OperatorTok{$}\NormalTok{.fitted)}

\NormalTok{amazon_purchase_data <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(amazon_purchase_data, matches)}
\end{Highlighting}
\end{Shaded}

Now we reduce the dataset to just those that are matched. We had 371 treated, so we expect a dataset of 742 observations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{amazon_purchase_data_matched <-}\StringTok{ }
\StringTok{  }\NormalTok{amazon_purchase_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(match.ind }\OperatorTok{!=}\StringTok{ }\DecValTok{0}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{match.ind, }\OperatorTok{-}\NormalTok{pairs, }\OperatorTok{-}\NormalTok{treated)}

\KeywordTok{head}\NormalTok{(amazon_purchase_data_matched)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   unique_person_id      age     city gender     income free_shipping
## 1             5710 81.15636 Montreal Female 0.67505625             0
## 2             9458 97.04859 Montreal Female 9.49752179             1
## 3             6428 83.21262  Calgary   Male 0.05851482             0
## 4             2022 98.97504 Montreal   Male 1.66683768             1
## 5             9824 64.61936  Calgary Female 3.35263989             1
## 6             1272 97.09546  Toronto Female 0.71813784             0
##   average_spend     .fitted cnts
## 1      47.36258 0.001375987    1
## 2      61.15317 0.001376161    1
## 3      49.90080 0.001560150    1
## 4      57.75673 0.001560418    1
## 5      64.69709 0.002207195    1
## 6      56.64754 0.002207514    1
\end{verbatim}

Finally, we can examine the `effect' of being treated on average spend in the `usual' way.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{propensity_score_regression <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(average_spend }\OperatorTok{~}\StringTok{ }\NormalTok{age }\OperatorTok{+}\StringTok{ }\NormalTok{city }\OperatorTok{+}\StringTok{ }\NormalTok{gender }\OperatorTok{+}\StringTok{ }\NormalTok{income }\OperatorTok{+}\StringTok{ }\NormalTok{free_shipping, }
                                  \DataTypeTok{data =}\NormalTok{ amazon_purchase_data_matched)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{huxtable}\OperatorTok{::}\KeywordTok{huxreg}\NormalTok{(propensity_score_regression)}
\end{Highlighting}
\end{Shaded}

\label{tab:unnamed-chunk-377}

(1)

(Intercept)

49.694 ***

(0.809)~~~

age

0.005~~~~

(0.011)~~~

cityMontreal

0.169~~~~

(0.734)~~~

cityToronto

0.652~~~~

(0.623)~~~

genderMale

-0.968 *~~

(0.422)~~~

genderOther/decline

-1.973~~~~

(2.621)~~~

income

0.009~~~~

(0.021)~~~

free\_shipping1

10.488 ***

(0.380)~~~

N

742~~~~~~~~

R2

0.513~~~~

logLik

-2267.486~~~~

AIC

4552.971~~~~

*** p \textless{} 0.001; ** p \textless{} 0.01; * p \textless{} 0.05.

I cover propensity score matching here because it is widely used. Hence, you need to know how to use it. People would think it's weird if you didn't, in the same way that we have to cover ANOVA people would think it's weird if we had an entire experimental design course and didn't cover it even though there are more modern ways of looking at differences between two means. But at the same time you need to know that there are flaws with propensity score matching. I will now discuss some of them.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Matching. Propensity score matching cannot match on unobserved variables. This may be fine in a class-room setting, but in more realistic settings it will likely cause issues.
\item
  Modelling. The results tend to be specific to the model that is used. King and Nielsen, 2019, discuss this thoroughly.
\item
  Statistically. We are using the data twice.
\end{enumerate}

\hypertarget{case-study---lower-advertising-revenue-reduced-french-newspaper-prices-between-1960-and-1974}{%
\section{Case study - Lower advertising revenue reduced French newspaper prices between 1960 and 1974}\label{case-study---lower-advertising-revenue-reduced-french-newspaper-prices-between-1960-and-1974}}

\textbf{Required reading}

\begin{itemize}
\tightlist
\item
  Angelucci, Charles, and Julia Cagé, 2019, `Newspapers in times of low advertising revenues', \emph{American Economic Journal: Microeconomics}, vol.~11, no. 3, pp.~319-364, DOI: 10.1257/mic.20170306, available at: \url{https://www.aeaweb.org/articles?id=10.1257/mic.20170306}.
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\tightlist
\item
  Reading in foreign data.
\item
  Difference in differences.
\item
  Replicating work.
\item
  Displaying multiple regression results.
\item
  Discussing results.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{haven}
\item
  \texttt{huxtable}
\item
  \texttt{scales}
\item
  \texttt{tidyverse}
\end{itemize}

\textbf{Key functions/etc}

\begin{itemize}
\tightlist
\item
  \texttt{dollar\_format()}
\item
  \texttt{hux\_reg()}
\item
  \texttt{lm()}
\item
  \texttt{mutate\_at()}
\item
  \texttt{read\_dta()}
\end{itemize}

\hypertarget{introduction-21}{%
\subsection{Introduction}\label{introduction-21}}

In this case study we introduce Angelucci and Cagé, 2019, and replicate its main findings. Angelucci and Cagé, 2019, is a paper in which difference in differences is used to examine the effect of the reduction in advertising revenues on newspapers' content and prices. They create a dataset of `French newspapers between 1960 and 1974'. They `perform a difference-in-differences analysis' and exploit `the introduction of advertising on television' as this change `affected national newspapers more severely than local ones'. They `find robust evidence of a decrease in the amount of journalistic-intensive content produced and the subscription price.'

In order to conduct this analysis we will use the dataset that they provide alongside their paper. This dataset is available at: \url{https://www.openicpsr.org/openicpsr/project/116438/version/V1/view}. It is available for you to download after registration. As their dataset is in Stata data format, we will use the haven package to read it in (Wickham and Miller, 2019).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(here)}
\KeywordTok{library}\NormalTok{(haven)}
\KeywordTok{library}\NormalTok{(huxtable)}
\KeywordTok{library}\NormalTok{(scales)}
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\hypertarget{background}{%
\subsection{Background}\label{background}}

Newspapers are in trouble. We can probably all think of a local newspaper that has closed recently because of pressure brought on by the internet. But this issue isn't new. When television started, there were similar concerns. In this paper, Angelucci and Cagé use the introduction of television advertising in France, announced in 1967, to examine the effect of decreased advertising revenue on newspapers.

The reason this is important is because it allows us to disentangle a few competing effects. For instance, are newspapers becoming redundant because they can no longer charge high prices for their ads or because consumers prefer to get their news in other ways? Are fewer journalists needed because smartphones and other technology mean they can be more productive? Angelucci and Cagé look at advertising revenue and a few other features, when a new advertising platform arrives, in this case television advertising.

\hypertarget{data}{%
\subsection{Data}\label{data}}

\begin{quote}
(The) dataset contains annual data on local and national newspapers between 1960 and 1974, as well as detailed information on television content. In 1967, the French government announced it would relax long-standing regulations that prohibited television advertising. We provide evidence that this reform can be plausibly interpreted as an exogenous and negative shock to the advertising side of the newspaper industry\ldots{} {[}I{]}t is likely that the introduction of television advertising constituted a direct shock to the advertising side of the newspaper industry and only an indirect shock to the reader side\ldots{} (O)ur empirical setting constitutes a unique opportunity to isolate the consequences of a decrease in newspapers' advertising revenues on their choices regarding the size of their newsroom, the amount of information to produce, and the prices they charge to both sides of the market.
\end{quote}

The authors' argue that national newspapers were affected by the television advertising change, but local newspapers were not. So the national newspapers are the treatment group and the local newspapers are the control group.

The dataset can be read in using \texttt{read\_dta()}, which is a function within the \texttt{haven} package for reading in Stata dta files. This is equivalent to \texttt{read\_csv()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{newspapers <-}\StringTok{ }\KeywordTok{read_dta}\NormalTok{(here}\OperatorTok{::}\KeywordTok{here}\NormalTok{(}\StringTok{"inputs/data/116438-V1/data/dta/Angelucci_Cage_AEJMicro_dataset.dta"}\NormalTok{))}

\KeywordTok{dim}\NormalTok{(newspapers)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1196   52
\end{verbatim}

There are 1,196 observations in the dataset and 52 variables. The authors are interested in the 1960-1974 time period which has around 100 newspapers. There are 14 national newspapers at the beginning of the period and 12 at the end.

We just want to replicate their main results, so we don't need all their variables. As such we will just \texttt{select()} the ones that we are interested in and change the \texttt{class()} where needed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{newspapers <-}\StringTok{ }
\StringTok{  }\NormalTok{newspapers }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(year, id_news, after_national, local, national, }\CommentTok{# Diff in diff variables}
\NormalTok{         ra_cst, qtotal, ads_p4_cst, ads_s, }\CommentTok{# Advertising side dependents}
\NormalTok{         ps_cst, po_cst, qtotal, qs_s, rs_cst) }\OperatorTok{%>%}\StringTok{ }\CommentTok{#Reader side dependents}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{ra_cst_div_qtotal =}\NormalTok{ ra_cst }\OperatorTok{/}\StringTok{ }\NormalTok{qtotal) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# An advertising side dependents needs to be built}
\StringTok{  }\KeywordTok{mutate_at}\NormalTok{(}\KeywordTok{vars}\NormalTok{(id_news, after_national, local, national), }\OperatorTok{~}\KeywordTok{as.factor}\NormalTok{(.)) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# Change some to factors}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{year =} \KeywordTok{as.integer}\NormalTok{(year))}
\end{Highlighting}
\end{Shaded}

We can now have a look at the main variables of interest for both national (Figure \ref{fig:frenchnewspaperssummarystats}) and local daily newspapers (Figure \ref{fig:frenchlocalnewspaperssummarystats}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{figures/french_national_newspapers_summary_stats} \caption{Angelucci and Cagé, 2019, summary statistics: national daily newspapers}\label{fig:frenchnewspaperssummarystats}
\end{figure}

Source: Angelucci and Cagé, 2019, p.~333.

\begin{figure}
\includegraphics[width=0.9\linewidth]{figures/french_local_newspapers_summary_stats} \caption{Angelucci and Cagé, 2019, summary statistics: local daily newspapers}\label{fig:frenchlocalnewspaperssummarystats}
\end{figure}

Source: Angelucci and Cagé, 2019, p.~334.

Please read this section of their paper to see how they describe their dataset.

We are interested in the change from 1967 onward.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{newspapers }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{type =} \KeywordTok{if_else}\NormalTok{(local }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{, }\StringTok{"Local"}\NormalTok{, }\StringTok{"National"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ year, }\DataTypeTok{y =}\NormalTok{ ra_cst)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{labels =} \KeywordTok{dollar_format}\NormalTok{(}\DataTypeTok{prefix=}\StringTok{"$"}\NormalTok{, }\DataTypeTok{suffix =} \StringTok{"M"}\NormalTok{, }\DataTypeTok{scale =} \FloatTok{0.000001}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Year"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Advertising revenue"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\KeywordTok{vars}\NormalTok{(type),}
               \DataTypeTok{nrow =} \DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_classic}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_vline}\NormalTok{(}\DataTypeTok{xintercept =} \FloatTok{1966.5}\NormalTok{, }\DataTypeTok{linetype =} \StringTok{"dashed"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-381-1.pdf}

\hypertarget{model}{%
\subsection{Model}\label{model}}

The model that we are interested in estimating is:
\[\mbox{ln}(y_{n,t}) = \beta_0 + \beta_1(\mbox{National dummy}\times\mbox{1967 onward dummy}) + \lambda_n + \gamma_y + \epsilon.\]
The \(\lambda_n\) is a fixed effect for each newspaper, and the \(\gamma_y\) is a fixed effect for each year. We just use regular linear regression, with a few different dependent variables. It is the \(\beta_1\) coefficient that we are interested in.

\hypertarget{results}{%
\subsection{Results}\label{results}}

We can run the models using \texttt{lm()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Advertising side}
\NormalTok{ad_revenue <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(ra_cst) }\OperatorTok{~}\StringTok{ }\NormalTok{after_national }\OperatorTok{+}\StringTok{ }\NormalTok{id_news }\OperatorTok{+}\StringTok{ }\NormalTok{year, }\DataTypeTok{data =}\NormalTok{ newspapers)}
\NormalTok{ad_revenue_div_circulation <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(ra_cst_div_qtotal) }\OperatorTok{~}\StringTok{ }\NormalTok{after_national }\OperatorTok{+}\StringTok{ }\NormalTok{id_news }\OperatorTok{+}\StringTok{ }\NormalTok{year, }\DataTypeTok{data =}\NormalTok{ newspapers)}
\NormalTok{ad_price <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(ads_p4_cst) }\OperatorTok{~}\StringTok{ }\NormalTok{after_national }\OperatorTok{+}\StringTok{ }\NormalTok{id_news }\OperatorTok{+}\StringTok{ }\NormalTok{year, }\DataTypeTok{data =}\NormalTok{ newspapers)}
\NormalTok{ad_space <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(ads_s) }\OperatorTok{~}\StringTok{ }\NormalTok{after_national }\OperatorTok{+}\StringTok{ }\NormalTok{id_news }\OperatorTok{+}\StringTok{ }\NormalTok{year, }\DataTypeTok{data =}\NormalTok{ newspapers)}

\CommentTok{# Consumer side}
\NormalTok{subscription_price <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(ps_cst) }\OperatorTok{~}\StringTok{ }\NormalTok{after_national }\OperatorTok{+}\StringTok{ }\NormalTok{id_news }\OperatorTok{+}\StringTok{ }\NormalTok{year, }\DataTypeTok{data =}\NormalTok{ newspapers)}
\NormalTok{unit_price <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(po_cst) }\OperatorTok{~}\StringTok{ }\NormalTok{after_national }\OperatorTok{+}\StringTok{ }\NormalTok{id_news }\OperatorTok{+}\StringTok{ }\NormalTok{year, }\DataTypeTok{data =}\NormalTok{ newspapers)}
\NormalTok{circulation <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(qtotal) }\OperatorTok{~}\StringTok{ }\NormalTok{after_national }\OperatorTok{+}\StringTok{ }\NormalTok{id_news }\OperatorTok{+}\StringTok{ }\NormalTok{year, }\DataTypeTok{data =}\NormalTok{ newspapers)}
\NormalTok{share_of_sub <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(qs_s) }\OperatorTok{~}\StringTok{ }\NormalTok{after_national }\OperatorTok{+}\StringTok{ }\NormalTok{id_news }\OperatorTok{+}\StringTok{ }\NormalTok{year, }\DataTypeTok{data =}\NormalTok{ newspapers)}
\NormalTok{revenue_from_sales <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(rs_cst) }\OperatorTok{~}\StringTok{ }\NormalTok{after_national }\OperatorTok{+}\StringTok{ }\NormalTok{id_news }\OperatorTok{+}\StringTok{ }\NormalTok{year, }\DataTypeTok{data =}\NormalTok{ newspapers)}
\end{Highlighting}
\end{Shaded}

Looking at the advertising-side variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{omit_me <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"(Intercept)"}\NormalTok{, }\StringTok{"id_news3"}\NormalTok{, }\StringTok{"id_news6"}\NormalTok{, }\StringTok{"id_news7"}\NormalTok{, }\StringTok{"id_news13"}\NormalTok{, }
             \StringTok{"id_news16"}\NormalTok{, }\StringTok{"id_news25"}\NormalTok{, }\StringTok{"id_news28"}\NormalTok{, }\StringTok{"id_news34"}\NormalTok{, }\StringTok{"id_news38"}\NormalTok{, }
             \StringTok{"id_news44"}\NormalTok{, }\StringTok{"id_news48"}\NormalTok{, }\StringTok{"id_news51"}\NormalTok{, }\StringTok{"id_news53"}\NormalTok{, }\StringTok{"id_news54"}\NormalTok{, }
             \StringTok{"id_news57"}\NormalTok{, }\StringTok{"id_news60"}\NormalTok{, }\StringTok{"id_news62"}\NormalTok{, }\StringTok{"id_news66"}\NormalTok{, }\StringTok{"id_news67"}\NormalTok{, }
             \StringTok{"id_news70"}\NormalTok{, }\StringTok{"id_news71"}\NormalTok{, }\StringTok{"id_news72"}\NormalTok{, }\StringTok{"id_news80"}\NormalTok{, }\StringTok{"id_news82"}\NormalTok{, }
             \StringTok{"id_news88"}\NormalTok{, }\StringTok{"id_news95"}\NormalTok{, }\StringTok{"id_news97"}\NormalTok{, }\StringTok{"id_news98"}\NormalTok{, }\StringTok{"id_news103"}\NormalTok{, }
             \StringTok{"id_news105"}\NormalTok{, }\StringTok{"id_news106"}\NormalTok{, }\StringTok{"id_news118"}\NormalTok{, }\StringTok{"id_news119"}\NormalTok{, }\StringTok{"id_news127"}\NormalTok{, }
             \StringTok{"id_news136"}\NormalTok{, }\StringTok{"id_news138"}\NormalTok{, }\StringTok{"id_news148"}\NormalTok{, }\StringTok{"id_news151"}\NormalTok{, }\StringTok{"id_news153"}\NormalTok{, }
             \StringTok{"id_news154"}\NormalTok{, }\StringTok{"id_news157"}\NormalTok{, }\StringTok{"id_news158"}\NormalTok{, }\StringTok{"id_news161"}\NormalTok{, }\StringTok{"id_news163"}\NormalTok{, }
             \StringTok{"id_news167"}\NormalTok{, }\StringTok{"id_news169"}\NormalTok{, }\StringTok{"id_news179"}\NormalTok{, }\StringTok{"id_news184"}\NormalTok{, }\StringTok{"id_news185"}\NormalTok{, }
             \StringTok{"id_news187"}\NormalTok{, }\StringTok{"id_news196"}\NormalTok{, }\StringTok{"id_news206"}\NormalTok{, }\StringTok{"id_news210"}\NormalTok{, }\StringTok{"id_news212"}\NormalTok{, }
             \StringTok{"id_news213"}\NormalTok{, }\StringTok{"id_news224"}\NormalTok{, }\StringTok{"id_news225"}\NormalTok{, }\StringTok{"id_news234"}\NormalTok{, }\StringTok{"id_news236"}\NormalTok{, }
             \StringTok{"id_news245"}\NormalTok{, }\StringTok{"id_news247"}\NormalTok{, }\StringTok{"id_news310"}\NormalTok{, }\StringTok{"id_news452"}\NormalTok{, }\StringTok{"id_news467"}\NormalTok{, }
             \StringTok{"id_news469"}\NormalTok{, }\StringTok{"id_news480"}\NormalTok{, }\StringTok{"id_news20040"}\NormalTok{, }\StringTok{"id_news20345"}\NormalTok{, }
             \StringTok{"id_news20346"}\NormalTok{, }\StringTok{"id_news20347"}\NormalTok{, }\StringTok{"id_news20352"}\NormalTok{, }\StringTok{"id_news20354"}\NormalTok{, }
             \StringTok{"id_news21006"}\NormalTok{, }\StringTok{"id_news21025"}\NormalTok{, }\StringTok{"id_news21173"}\NormalTok{, }\StringTok{"id_news21176"}\NormalTok{, }
             \StringTok{"id_news33718"}\NormalTok{, }\StringTok{"id_news34689"}\NormalTok{, }\StringTok{"id_news73"}\NormalTok{)}

\KeywordTok{huxreg}\NormalTok{(}\StringTok{"Ad. rev."}\NormalTok{ =}\StringTok{ }\NormalTok{ad_revenue, }
       \StringTok{"Ad rev. div. circ."}\NormalTok{ =}\StringTok{ }\NormalTok{ad_revenue_div_circulation, }
       \StringTok{"Ad price"}\NormalTok{ =}\StringTok{ }\NormalTok{ad_price, }
       \StringTok{"Ad space"}\NormalTok{ =}\StringTok{ }\NormalTok{ad_space,}
        \DataTypeTok{omit_coefs =}\NormalTok{ omit_me, }
        \DataTypeTok{number_format =} \DecValTok{2}
\NormalTok{        )}
\end{Highlighting}
\end{Shaded}

\label{tab:unnamed-chunk-383}

Ad. rev.

Ad rev. div. circ.

Ad price

Ad space

after\_national1

-0.23 ***

-0.15 ***

-0.31 ***

0.01~~~~

(0.03)~~~

(0.03)~~~

(0.07)~~~

(0.05)~~~

year

0.05 ***

0.04 ***

0.04 ***

0.02 ***

(0.00)~~~

(0.00)~~~

(0.00)~~~

(0.00)~~~

N

1052~~~~~~~

1048~~~~~~~

809~~~~~~~

1046~~~~~~~

R2

0.99~~~~

0.90~~~~

0.89~~~~

0.72~~~~

logLik

345.34~~~~

449.52~~~~

-277.71~~~~

-164.01~~~~

AIC

-526.68~~~~

-735.05~~~~

705.43~~~~

478.02~~~~

*** p \textless{} 0.001; ** p \textless{} 0.01; * p \textless{} 0.05.

Similarly, we can look at the reader-side variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{omit_me <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"(Intercept)"}\NormalTok{, }\StringTok{"id_news3"}\NormalTok{, }\StringTok{"id_news6"}\NormalTok{, }\StringTok{"id_news7"}\NormalTok{, }\StringTok{"id_news13"}\NormalTok{, }
             \StringTok{"id_news16"}\NormalTok{, }\StringTok{"id_news25"}\NormalTok{, }\StringTok{"id_news28"}\NormalTok{, }\StringTok{"id_news34"}\NormalTok{, }\StringTok{"id_news38"}\NormalTok{, }
             \StringTok{"id_news44"}\NormalTok{, }\StringTok{"id_news48"}\NormalTok{, }\StringTok{"id_news51"}\NormalTok{, }\StringTok{"id_news53"}\NormalTok{, }\StringTok{"id_news54"}\NormalTok{, }
             \StringTok{"id_news57"}\NormalTok{, }\StringTok{"id_news60"}\NormalTok{, }\StringTok{"id_news62"}\NormalTok{, }\StringTok{"id_news66"}\NormalTok{, }\StringTok{"id_news67"}\NormalTok{, }
             \StringTok{"id_news70"}\NormalTok{, }\StringTok{"id_news71"}\NormalTok{, }\StringTok{"id_news72"}\NormalTok{, }\StringTok{"id_news80"}\NormalTok{, }\StringTok{"id_news82"}\NormalTok{, }
             \StringTok{"id_news88"}\NormalTok{, }\StringTok{"id_news95"}\NormalTok{, }\StringTok{"id_news97"}\NormalTok{, }\StringTok{"id_news98"}\NormalTok{, }\StringTok{"id_news103"}\NormalTok{, }
             \StringTok{"id_news105"}\NormalTok{, }\StringTok{"id_news106"}\NormalTok{, }\StringTok{"id_news118"}\NormalTok{, }\StringTok{"id_news119"}\NormalTok{, }\StringTok{"id_news127"}\NormalTok{, }
             \StringTok{"id_news136"}\NormalTok{, }\StringTok{"id_news138"}\NormalTok{, }\StringTok{"id_news148"}\NormalTok{, }\StringTok{"id_news151"}\NormalTok{, }\StringTok{"id_news153"}\NormalTok{, }
             \StringTok{"id_news154"}\NormalTok{, }\StringTok{"id_news157"}\NormalTok{, }\StringTok{"id_news158"}\NormalTok{, }\StringTok{"id_news161"}\NormalTok{, }\StringTok{"id_news163"}\NormalTok{, }
             \StringTok{"id_news167"}\NormalTok{, }\StringTok{"id_news169"}\NormalTok{, }\StringTok{"id_news179"}\NormalTok{, }\StringTok{"id_news184"}\NormalTok{, }\StringTok{"id_news185"}\NormalTok{, }
             \StringTok{"id_news187"}\NormalTok{, }\StringTok{"id_news196"}\NormalTok{, }\StringTok{"id_news206"}\NormalTok{, }\StringTok{"id_news210"}\NormalTok{, }\StringTok{"id_news212"}\NormalTok{, }
             \StringTok{"id_news213"}\NormalTok{, }\StringTok{"id_news224"}\NormalTok{, }\StringTok{"id_news225"}\NormalTok{, }\StringTok{"id_news234"}\NormalTok{, }\StringTok{"id_news236"}\NormalTok{, }
             \StringTok{"id_news245"}\NormalTok{, }\StringTok{"id_news247"}\NormalTok{, }\StringTok{"id_news310"}\NormalTok{, }\StringTok{"id_news452"}\NormalTok{, }\StringTok{"id_news467"}\NormalTok{, }
             \StringTok{"id_news469"}\NormalTok{, }\StringTok{"id_news480"}\NormalTok{, }\StringTok{"id_news20040"}\NormalTok{, }\StringTok{"id_news20345"}\NormalTok{, }
             \StringTok{"id_news20346"}\NormalTok{, }\StringTok{"id_news20347"}\NormalTok{, }\StringTok{"id_news20352"}\NormalTok{, }\StringTok{"id_news20354"}\NormalTok{, }
             \StringTok{"id_news21006"}\NormalTok{, }\StringTok{"id_news21025"}\NormalTok{, }\StringTok{"id_news21173"}\NormalTok{, }\StringTok{"id_news21176"}\NormalTok{, }
             \StringTok{"id_news33718"}\NormalTok{, }\StringTok{"id_news34689"}\NormalTok{, }\StringTok{"id_news73"}\NormalTok{)}

\KeywordTok{huxreg}\NormalTok{(}\StringTok{"Subscription price"}\NormalTok{ =}\StringTok{ }\NormalTok{subscription_price, }
       \StringTok{"Unit price"}\NormalTok{ =}\StringTok{ }\NormalTok{unit_price, }
       \StringTok{"Circulation"}\NormalTok{ =}\StringTok{ }\NormalTok{circulation, }
       \StringTok{"Share of sub"}\NormalTok{ =}\StringTok{ }\NormalTok{share_of_sub,}
       \StringTok{"Revenue from sales"}\NormalTok{ =}\StringTok{ }\NormalTok{revenue_from_sales,}
       \DataTypeTok{omit_coefs =}\NormalTok{ omit_me, }
       \DataTypeTok{number_format =} \DecValTok{2}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\label{tab:unnamed-chunk-384}

Subscription price

Unit price

Circulation

Share of sub

Revenue from sales

after\_national1

-0.04 *~~

0.06 **~

-0.06 **~

0.19 ***

-0.06 *~~

(0.02)~~~

(0.02)~~~

(0.02)~~~

(0.03)~~~

(0.03)~~~

year

0.05 ***

0.05 ***

0.01 ***

-0.01 ***

0.05 ***

(0.00)~~~

(0.00)~~~

(0.00)~~~

(0.00)~~~

(0.00)~~~

N

1044~~~~~~~

1063~~~~~~~

1070~~~~~~~

1072~~~~~~~

1046~~~~~~~

R2

0.88~~~~

0.87~~~~

0.99~~~~

0.97~~~~

0.99~~~~

logLik

882.14~~~~

907.28~~~~

759.57~~~~

321.91~~~~

451.11~~~~

AIC

-1600.28~~~~

-1650.57~~~~

-1355.15~~~~

-477.81~~~~

-738.22~~~~

*** p \textless{} 0.001; ** p \textless{} 0.01; * p \textless{} 0.05.

\hypertarget{other-points}{%
\subsection{Other points}\label{other-points}}

\begin{itemize}
\tightlist
\item
  We certainly find that in many cases there appears to be a difference from 1967 onward.
\item
  In general, we are able to obtain results that are similar to Angelucci and Cagé, 2019. If we spent more time, we could probably replicate their findings perfectly. Isn't this great! What else could do?
\item
  Parallel trends: Notice the wonderful way in which they test the `parallel trends' assumption on pp.~350-351.
\item
  Discussion: Look at their wonderful discussion (pp.~353-358) of interpretation, external validity, and robustness.
\end{itemize}

\hypertarget{tutorial---propensity-score-matching---lalonde}{%
\section{Tutorial - Propensity score matching - Lalonde}\label{tutorial---propensity-score-matching---lalonde}}

\url{http://sekhon.berkeley.edu/matching/lalonde.html}

\hypertarget{instrumental-variables}{%
\chapter{Instrumental variables}\label{instrumental-variables}}

\textbf{Required reading}

\begin{itemize}
\tightlist
\item
  Gertler, Paul, Sebastian Martinez, Patrick Premand, Laura Rawlings, and Christel Vermeersch, `Impact Evaluation in Practice', Chapter 5.
\end{itemize}

\textbf{Required viewing}

\begin{itemize}
\tightlist
\item
  Kuriwaki, Shiro, 2020, `Instrumental variables in R', 11 April, freely available at: \url{https://vimeo.com/406629459}.
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
  Angrist, Joshua D., and Jörn-Steffen Pischke, 2008, \emph{Mostly harmless econometrics: An empiricist's companion}, Princeton University Press, Chapter 4.
\item
  Cunningham, Scott, `Causal Inference: The Mixtape', Chapter `Instrumental variables', freely available at: \url{http://www.scunning.com/causalinference_norap.pdf}.
\item
  Grogger, Jeffrey, Andreas Steinmayr, Joachim Winter, 2020, `The Wage Penalty of Regional Accents', NBER Working Paper No.~26719.
\item
  Taddy, Matt, 2019, \emph{Business Data Science}, Chapter 5, pp.~152-162.
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\tightlist
\item
  Identifying opportunities for instrumental variables.
\item
  Implementing instrumental variables.
\item
  Challenges to the validity of instrumental variables.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{estimatr}
\item
  \texttt{tidyverse}
\end{itemize}

\textbf{Key functions/etc}

\begin{itemize}
\tightlist
\item
  \texttt{iv\_robust()}
\end{itemize}

\textbf{Pre-quiz}

\begin{itemize}
\tightlist
\item
  What is an instrumental variable?
\item
  What are some circumstances in which instrumental variables might be useful?
\item
  What conditions must instrumental variables satisfy?
\item
  Who were some of the early instrumental variable authors?
\item
  Can you please think of and explain an application of instrumental variables in your own life?
\end{itemize}

\hypertarget{introduction-22}{%
\section{Introduction}\label{introduction-22}}

Instrumental variables (IV) is an approach that can be handy when we have some type of treatment and control going on, but we have a lot of correlation with other variables and we possibly don't have a variable that actually measures what we are interested in. So adjusting for observables will not be enough to create a good estimate. Instead we find some variable - the eponymous instrumental variable - that is:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  correlated with the treatment variable, but
\item
  not correlated with the outcome.
\end{enumerate}

This solves our problem because the only way the instrumental variable can have an effect is through the treatment variable, and so we are able to adjust our understanding of the effect of the treatment variable appropriately. The trade-off is that instrumental variables must satisfy a bunch of different assumptions, and that, frankly, they are difficult to identify \emph{ex ante}. Nonetheless, when you are able to use them they are a powerful tool for speaking about causality.

The canonical instrumental variables example is smoking. These days we know that smoking causes cancer. But because smoking is correlated with a lot of other variables, for instance, education, it could be that it was actually education that causes cancer. RCTs may be possible, but they are likely to be troublesome in terms of speed and ethics, and so instead we look for some other variable that is correlated with smoking, but not, in and of itself, with lung cancer. In this case, we look to tax rates, and other policy responses, on cigarettes. As the tax rates on cigarettes are correlated with the number of cigarettes that are smoked, but not correlated with lung cancer, other than through their impact on cigarette smoking, through them we can assess the effect of cigarettes smoked on lung cancer.

To implement instrumental variables we first regress tax rates on cigarette smoking to get some coefficient on the instrumental variable, and then (in a separate regression) regress tax rates on lung cancer to again get some coefficient on the instrumental variable. Our estimate is then the ratio of these coefficients. \citep[p.~219]{gelmanandhill} describe this ratio as the `Wald estimate'.

Following the language of \citep[p.~216]{gelmanandhill} when we use instrumental variables we make a variety of assumptions including:

\begin{itemize}
\tightlist
\item
  Ignorability of the instrument.
\item
  Correlation between the instrumental variable and the treatment variable.
\item
  Monotonicity.
\item
  Exclusion restriction.
\end{itemize}

To summarise exactly what instrumental variables is about, I cannot do better than recommend the first few pages of the `Instrumental Variables' chapter in \citet{cunninghamnorap}, and this key paragraph in particular (by way of background, Cunningham has explained why it would have been impossible to randomly allocate `clean' and `dirty' water through a randomised controlled trial and then continues\ldots):

\begin{quote}
Snow would need a way to trick the data such that the allocation of clean and dirty water to people was not associated with the other determinants of cholera mortality, such as hygiene and poverty. He just would need for someone or something to be making this treatment assignment for him.

Fortunately for Snow, and the rest of London, that someone or something existed. In the London of the 1800s, there were many different water companies serving different areas of the city. Some were served by more than one company. Several took their water from the Thames, which was heavily polluted by sewage. The service areas of such companies had much higher rates of cholera. The Chelsea water company was an exception, but it had an exceptionally good filtration system. That's when Snow had a major insight. In 1849, Lambeth water company moved the intake point upstream along the Thames, above the main sewage discharge point, giving its customers purer water. Southwark and Vauxhall water company, on the other hand, left their intake point downstream from where the sewage discharged. Insofar as the kinds of people that each company serviced were approximately the same, then comparing the cholera rates between the two houses could be the experiment that Snow so desperately needed to test his hypothesis.
\end{quote}

\hypertarget{history}{%
\section{History}\label{history}}

The history of instrumental variables is a rare statistical mystery, and \citet{stock2003retrospectives} provide a brief overview. The method was first published in \citet{wright1928tariff}. This is a book about the effect of tariffs on animal and vegetable oil. So why might instrumental variables be important in a book about tariffs on animal and vegetable oil? The fundamental problem is that the effect of tariffs depends on both supply and demand. But we only know prices and quantities, so we don't know what is driving the effect. We can use instrumental variables to pin down causality.

Where is gets interesting, and becomes something of a mystery, is that the instrumental variables discussion is only in Appendix B. If you made a major statistical break-through would you hide it in an appendix? Further, Philip G. Wright, the book's author, had a son Sewall Wright, who had considerable expertise in statistics and the specific method used in Appendix B. Hence the mystery of Appendix B - did Philip or Sewall write it? Both \citet{cunninghamnorap} and \citet{stock2003retrospectives} go into more detail, but on balance feel that it is likely that Philip did actually author the work.

\hypertarget{simulated-example-1}{%
\section{Simulated example}\label{simulated-example-1}}

Let's generate some data. We will explore a simulation related to the canonical example of health status, smoking, and tax rates. So we are looking to explain how healthy someone is based on the amount they smoke, via the tax rate on smoking. We are going to generate different tax rates by provinces. My understanding is that the tax rate on cigarettes is now pretty much the same in each of the provinces, but that this is fairly recent. So we'll pretend that Alberta had a low tax, and Nova Scotia had a high tax.

As a reminder, we are simulating data for illustrative purposes, so we need to impose the answer that we want. When you actually use instrumental variables you will be reversing the process.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(broom)}
\KeywordTok{library}\NormalTok{(tidyverse)}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{number_of_observation <-}\StringTok{ }\DecValTok{10000}

\NormalTok{iv_example_data <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{person =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{number_of_observation),}
                          \DataTypeTok{smoker =} \KeywordTok{sample}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\OperatorTok{:}\DecValTok{1}\NormalTok{),}
                                          \DataTypeTok{size =}\NormalTok{ number_of_observation, }
                                          \DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{                          )}
\end{Highlighting}
\end{Shaded}

Now we need to relate the number of cigarettes that someone smoked to their health. We'll model health status as a draw from the normal distribution, with either a high or low mean depending on whether the person smokes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iv_example_data <-}\StringTok{ }
\StringTok{  }\NormalTok{iv_example_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{health =} \KeywordTok{if_else}\NormalTok{(smoker }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{,}
                          \KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n =} \KeywordTok{n}\NormalTok{(), }\DataTypeTok{mean =} \DecValTok{1}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{1}\NormalTok{),}
                          \KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n =} \KeywordTok{n}\NormalTok{(), }\DataTypeTok{mean =} \DecValTok{0}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{1}\NormalTok{)}
\NormalTok{                          )}
\NormalTok{         )}
\CommentTok{# So health will be one standard deviation higher for people who don't or barely smoke.}
\end{Highlighting}
\end{Shaded}

Now we need a relationship between cigarettes and the province (because in this illustration, the provinces have different tax rates).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iv_example_data <-}\StringTok{ }
\StringTok{  }\NormalTok{iv_example_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{rowwise}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{province =} \KeywordTok{case_when}\NormalTok{(smoker }\OperatorTok{==}\StringTok{ }\DecValTok{0} \OperatorTok{~}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\StringTok{"Nova Scotia"}\NormalTok{, }\StringTok{"Alberta"}\NormalTok{),}
                                                                       \DataTypeTok{size =} \DecValTok{1}\NormalTok{, }
                                                                       \DataTypeTok{replace =} \OtherTok{FALSE}\NormalTok{, }
                                                                       \DataTypeTok{prob =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\DecValTok{2}\NormalTok{)),}
\NormalTok{                              smoker }\OperatorTok{==}\StringTok{ }\DecValTok{1} \OperatorTok{~}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\StringTok{"Nova Scotia"}\NormalTok{, }\StringTok{"Alberta"}\NormalTok{),}
                                                                       \DataTypeTok{size =} \DecValTok{1}\NormalTok{, }
                                                                       \DataTypeTok{replace =} \OtherTok{FALSE}\NormalTok{, }
                                                                       \DataTypeTok{prob =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\DecValTok{4}\NormalTok{, }\DecValTok{3}\OperatorTok{/}\DecValTok{4}\NormalTok{)))) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ungroup}\NormalTok{()}

\NormalTok{iv_example_data <-}\StringTok{ }
\StringTok{  }\NormalTok{iv_example_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{tax =} \KeywordTok{case_when}\NormalTok{(province }\OperatorTok{==}\StringTok{ "Alberta"} \OperatorTok{~}\StringTok{ }\FloatTok{0.3}\NormalTok{,}
\NormalTok{                         province }\OperatorTok{==}\StringTok{ "Nova Scotia"} \OperatorTok{~}\StringTok{ }\FloatTok{0.5}\NormalTok{,}
                         \OtherTok{TRUE} \OperatorTok{~}\StringTok{ }\DecValTok{9999999}
\NormalTok{  )}
\NormalTok{  )}

\NormalTok{iv_example_data}\OperatorTok{$}\NormalTok{tax }\OperatorTok{%>%}\StringTok{ }\KeywordTok{table}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## .
##  0.3  0.5 
## 6206 3794
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(iv_example_data)}
\end{Highlighting}
\end{Shaded}

\label{tab:unnamed-chunk-387}

person

smoker

health

province

tax

1

0

1.11~~

Alberta

0.3

2

1

-0.0831

Alberta

0.3

3

1

-0.0363

Alberta

0.3

4

0

2.48~~

Alberta

0.3

5

0

0.617~

Alberta

0.3

6

0

0.748~

Nova Scotia

0.5

Now we can look at our data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iv_example_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{smoker =} \KeywordTok{as_factor}\NormalTok{(smoker)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ health, }\DataTypeTok{fill =}\NormalTok{ smoker)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{, }\DataTypeTok{binwidth =} \FloatTok{0.2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Health rating"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Number of people"}\NormalTok{,}
       \DataTypeTok{fill =} \StringTok{"Smoker"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\KeywordTok{vars}\NormalTok{(province))}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-388-1.pdf}

Finally, we can use the tax rate as an instrumental variable to estimate the effect of smoking on health.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{health_on_tax <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(health }\OperatorTok{~}\StringTok{ }\NormalTok{tax, }\DataTypeTok{data =}\NormalTok{ iv_example_data)}
\NormalTok{smoker_on_tax <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(smoker }\OperatorTok{~}\StringTok{ }\NormalTok{tax, }\DataTypeTok{data =}\NormalTok{ iv_example_data)}

\KeywordTok{coef}\NormalTok{(health_on_tax)[}\StringTok{"tax"}\NormalTok{] }\OperatorTok{/}\StringTok{ }\KeywordTok{coef}\NormalTok{(smoker_on_tax)[}\StringTok{"tax"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        tax 
## -0.8554502
\end{verbatim}

So we find, luckily, that if you smoke then your health is likely to be worse than if you don't smoke.

Equivalently, we can think of instrumental variables in a two-stage regression context.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{first_stage <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(smoker }\OperatorTok{~}\StringTok{ }\NormalTok{tax, }\DataTypeTok{data =}\NormalTok{ iv_example_data)}
\NormalTok{health_hat <-}\StringTok{ }\NormalTok{first_stage}\OperatorTok{$}\NormalTok{fitted.values}
\NormalTok{second_stage <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(health }\OperatorTok{~}\StringTok{ }\NormalTok{health_hat, }\DataTypeTok{data =}\NormalTok{ iv_example_data)}

\KeywordTok{summary}\NormalTok{(second_stage)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = health ~ health_hat, data = iv_example_data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.9867 -0.7600  0.0068  0.7709  4.3293 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  0.91632    0.04479   20.46   <2e-16 ***
## health_hat  -0.85545    0.08911   -9.60   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.112 on 9998 degrees of freedom
## Multiple R-squared:  0.009134,   Adjusted R-squared:  0.009034 
## F-statistic: 92.16 on 1 and 9998 DF,  p-value: < 2.2e-16
\end{verbatim}

\hypertarget{implementation}{%
\section{Implementation}\label{implementation}}

As with regression discontinuity, although it is possible to use existing functions, it might be worth looking at specialised packages. Instrumental variables has a few moving pieces, so a specialised package can help keep everything organised, and additionally, standard errors need to be adjusted and specialised packages make this easier. The package \texttt{estimatr} is a recommendation, although there are others available and you should try those if you are interested. The \texttt{estimatr} package is from the same team as DeclareDesign.

Let's look at our example using \texttt{iv\_robust()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(estimatr)}
\KeywordTok{iv_robust}\NormalTok{(health }\OperatorTok{~}\StringTok{ }\NormalTok{smoker }\OperatorTok{|}\StringTok{ }\NormalTok{tax, }\DataTypeTok{data =}\NormalTok{ iv_example_data) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## iv_robust(formula = health ~ smoker | tax, data = iv_example_data)
## 
## Standard error type:  HC2 
## 
## Coefficients:
##             Estimate Std. Error t value   Pr(>|t|) CI Lower CI Upper   DF
## (Intercept)   0.9163    0.04057   22.59 3.163e-110   0.8368   0.9958 9998
## smoker       -0.8555    0.08047  -10.63  2.981e-26  -1.0132  -0.6977 9998
## 
## Multiple R-squared:  0.1971 ,    Adjusted R-squared:  0.197 
## F-statistic:   113 on 1 and 9998 DF,  p-value: < 2.2e-16
\end{verbatim}

\hypertarget{assumptions-1}{%
\section{Assumptions}\label{assumptions-1}}

As discussed earlier, there are a variety of assumptions that are made when using instrumental variables. The two most important are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Exclusion Restriction. This assumption is that the instrumental variable only affects the dependent variable through the independent variable of interest.
\item
  Relevance. There must actually be a relationship between the instrumental variable and the independent variable.
\end{enumerate}

There is typically a trade-off between these two. There are plenty of variables that

When thinking about potential instrumental variables \citet{cunninghamnorap}, p.~211, puts it brilliantly:

\begin{quote}
But, let's say you think you do have a good instrument. How might you defend it as such to someone else? A necessary but not a sufficient condition for having an instrument that can satisfy the exclusion restriction is if people are confused when you tell them about the instrument's relationship to the outcome. Let me explain. No one is going to be confused when you tell them that you think family size will reduce female labor supply. They don't need a Becker model to convince them that women who have more children probably work less than those with fewer children. It's common sense. But, what would they think if you told them that mothers whose first two children were the same gender worked less than those whose children had a balanced sex ratio? They would probably give you a confused look. What does the gender composition of your children have to do with whether a woman works?

It doesn't -- it only matters, in fact, if people whose first two children are the same gender decide to have a third child. Which brings us back to the original point -- people buy that family size can cause women to work less, but they're confused when you say that women work less when their first two kids are the same gender. But if when you point out to them that the two children's gender induces people to have larger families than they would have otherwise, the person
``gets it'', then you might have an excellent instrument.
\end{quote}

Relevance can be tested using regression and other tests for correlation. The exclusion restriction cannot be tested. You need to present evidence and convincing arguments. As \citet{cunninghamnorap} p.~225 says `Instruments have a certain ridiculousness to them{[}.{]} That is, you know you have a good instrument if the instrument itself doesn't seem relevant for explaining the outcome of interest because that's what the exclusion restriction implies.'

\hypertarget{example---effect-of-police-on-crime}{%
\section{Example - Effect of Police on Crime}\label{example---effect-of-police-on-crime}}

\hypertarget{overview}{%
\subsection{Overview}\label{overview}}

Here we'll use an example of \citet{levitt2002using} that looks at the effect of police on crime. This is interesting because you might think, that more police is associated with lower crime. But, it could actually be the opposite, if more crime causes more police to be hired - how many police would a hypothetical country with no crime need? Hence there is a need to find some sort of instrumental variable that affects crime only through its relationship with the number of police (that is, not in and of itself, related to crime), and yet is also correlated with police numbers. \citet{levitt2002using} suggests the number of firefighters in a city.

\citet{levitt2002using} argues that firefighters are appropriate as an instrument, because `(f)actors such as the power of public sector unions, citizen tastes for government services, affirmative action initiatives, or a mayor's desire to provide spoils might all be expected to jointly influence the number of firefighters and police.'. \citet{levitt2002using} also argues that the relevance assumption is met by showing that `changes in the number of police officers and firefighters within a city are highly correlated over time'.

In terms of satisfying the exclusion restriction, \citet{levitt2002using} argues that the number of firefighters should not have a `direct impact on crime.' However, it may be that there are common factors, and so \citet{levitt2002using} adjusts for this in the regression.

\hypertarget{data-1}{%
\subsection{Data}\label{data-1}}

The dataset is based on 122 US cities between 1975 and 1995. Summary statistics are provided in Figure \ref{fig:levittcrime}.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/levitt_summary_stats} \caption{Summary statistics for Levitt 2002.}\label{fig:levittcrime}
\end{figure}

Source: \citet{levitt2002using} p.~1,246.

\hypertarget{model-1}{%
\subsection{Model}\label{model-1}}

In the first stage \citet{levitt2002using} looks at police as a function of firefighters, and a bunch of adjustment variables:
\[\ln(\mbox{Police}_{ct}) = \gamma \ln(\mbox{Fire}_{ct}) + X'_{ct}\Gamma + \lambda_t + \phi_c + \epsilon_{ct}.\]
The important part of this is the police and firefighters numbers which are on a per capita basis. There are a bunch of adjustment variables in \(X\) which includes things like state prisoners per capita, the unemployment rate, etc, as well as year dummy variables and fixed-effects for each city.

Having established the relationship between police and firefights, \citet{levitt2002using} can then use the estimates of the number of police, based on the number of firefighters, to explain crime rates:
\[\Delta\ln(\mbox{Crime}_{ct}) = \beta_1 \ln(\mbox{Police}_{ct-1}) + X'_{ct}\Gamma + \Theta_c + \mu_{ct}.\]

The typical way to present instrumental variable results is to show both stages. Figure \ref{fig:levittcrimefirst} shows the relationship between police and firefighters.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/levitt_first} \caption{The relationship between firefighters, police and crime.}\label{fig:levittcrimefirst}
\end{figure}

Source: \citet{levitt2002using} p.~1,247.

And then Figure \ref{fig:levittcrimesecond} shows the relationship between police and crime, where is it the IV results that are the ones of interest.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/levitt_second} \caption{The impact of police on crime.}\label{fig:levittcrimesecond}
\end{figure}

Source: \citet{levitt2002using} p.~1,248.

\hypertarget{discussion}{%
\subsection{Discussion}\label{discussion}}

The key finding of \citet{levitt2002using} is that there is a negative effect of the number of police on the amount of crime.

There are a variety of points that I want to raise in regard to this paper. They will come across as a little negative, but this is mostly just because this a paper from 2002, that I am reading today, and so the standards have changed.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  It's fairly remarkable how reliant on various model specifications the results are. The results bounce around a fair bit and that's just the ones that are reported. Chances are there are a bunch of other results that were not reported, but it would be of interest to see their impact.
\item
  On that note, there is fairly limited model validation. This is probably something that I am more aware of these days, but it seems likely that there is a fair degree of over-fitting here.
\item
  \citet{levitt2002using} is actually a response, after another researcher, \citet{mccrary2002using}, found some issues with the original paper: \citet{levitt87using}. While Levitt appears quite decent about it, it is jarring to see that Levitt was thanked by \citet{mccrary2002using} for providing `both the data and computer code.' What if Levitt had not been decent about providing the data and code? Or what if the code was unintelligible? In some ways it is nice to see how far that we have come - the author of a similar paper these days would be forced to make their code and data available as part of the paper, we wouldn't have to ask them for it. But it reinforces the importance of open data and reproducible science.
\end{enumerate}

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

Instrumental variables is a useful approach because one can obtain causal estimates even without explicit randomisation. Finding instrumental variables used to be a bit of a white whale, especially in academia. However, I will leave the final (and hopefully motivating) word to \citet{taddy2019}, p.~162:

\begin{quote}
As a final point on the importance of IV models and analysis, note that when you are on the inside of a firm---especially on the inside of a modern technology firm---explicitly randomised instruments are everywhere\ldots. But it is often the case that decision-makers want to understand the effects of policies that are not themselves randomised but are rather downstream of the things being AB tested. For example, suppose an algorithm is used to predict the creditworthiness of potential borrowers and assign loans. Even if the process of loan assignment is never itself randomised, if the parameters in the machine learning algorithms used to score credit are AB tested, then those experiments can be used as instruments for the loan assignment treatment. Such `upstream randomisation' is extremely common and IV analysis is your key tool for doing causal inference in that setting.'
\end{quote}

\hypertarget{regression-discontinuity-design}{%
\chapter{Regression discontinuity design}\label{regression-discontinuity-design}}

\textbf{Required reading}

\begin{itemize}
\tightlist
\item
  Better Evaluation, `Regression Discontinuity', \url{https://www.betterevaluation.org/en/evaluation-options/regressiondiscontinuity}
\item
  Eggers, Andrew C., Anthony Fowler, Jens Hainmueller, Andrew B. Hall, and James M. Snyder Jr, 2015, `On the validity of the regression discontinuity design for estimating electoral effects: New evidence from over 40,000 close races', American Journal of Political Science, 59 (1), pp.~259-274
\item
  Gelman, Andrew, 2019, `Another Regression Discontinuity Disaster and what can we learn from it', 25 June, \url{https://statmodeling.stat.columbia.edu/2019/06/25/another-regression-discontinuity-disaster-and-what-can-we-learn-from-it/}.
\item
  Gertler, Paul, Sebastian Martinez, Patrick Premand, Laura Rawlings, and Christel Vermeersch, `Impact Evaluation in Practice', Chapter 6.
\item
  Sekhon, Jasjeet and Rocio Titiunik, 2016, `Understanding Regression Discontinuity Designs As Observational Studies', Observational Studies 2 (2016) 174-182, \url{http://sekhon.berkeley.edu/papers/SekhonTitiunik2016-OS.pdf}.
\end{itemize}

\textbf{Required viewing}

\begin{itemize}
\tightlist
\item
  Kuriwaki, Shiro, 2020, `Regression Discontinuity in R (parts 1 and 2)', 25 March, freely available at: \url{https://vimeo.com/400826628} and \url{https://vimeo.com/400826660}.
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
  Angrist, Joshua D., and Jörn-Steffen Pischke, 2008, \emph{Mostly harmless econometrics: An empiricist's companion}, Princeton University Press, Chapter 6.
\item
  Coppock, Alenxader, and Donald P. Green, 2016, `Is Voting Habit Forming? New Evidence from Experiments and Regression Discontinuities', \emph{American Journal of Political Science}, Volume 60, Issue 4, pp.~1044-1062, available at: \url{https://onlinelibrary.wiley.com/doi/abs/10.1111/ajps.12210}. (Has code and data.)
\item
  Cunningham, Scott, `Causal Inference: The Mixtape', chapter `Regression discontinuity', freely available at: \url{http://www.scunning.com/causalinference_norap.pdf}.
\item
  Dell, Melissa, Pablo Querubin, 2018, `Nation Building Through Foreign Intervention: Evidence from Discontinuities in Military Strategies', \emph{The Quarterly Journal of Economics}, Volume 133, Issue 2, pp.~701--764, \url{https://doi.org/10.1093/qje/qjx037}.
\item
  Evans, David, 2013, `Regression Discontinuity Porn', \emph{World Bank Blogs}, 16 November, freely available at: \url{https://blogs.worldbank.org/impactevaluations/regression-discontinuity-porn}.
\item
  Imai, Kosuke, 2017, Quantitative Social Science: An Introduction, Princeton University Press, Ch 2.5.
\item
  Gelman, Andrew, and Jennifer Hill, 2007, \emph{Data Analysis Using Regression and Multilevel/Hierarchical Models}, Chapter 10, pp.~212-215.
\item
  Gelman, Andrew, and Guido Imbens, 2019, ``Why high-order polynomials should not be used in regression discontinuity designs'', \emph{Journal of Business \& Economic Statistics}, 37, pp.~447-456.
\item
  Gelman, Andrew, 2019, `Another Regression Discontinuity Disaster and what can we learn from it', \emph{Statistical Modeling, Causal Inference, and Social Science}, 25 June, freely available at: \url{https://statmodeling.stat.columbia.edu/2019/06/25/another-regression-discontinuity-disaster-and-what-can-we-learn-from-it/}.
\item
  Harris, Rich, Mlacki Migliozzi and Niraj Chokshi, `13,000 Missing Flights: The Global Consequences of the Coronavirus', \emph{New York Times}, 21 February 2020. freely available here (if you make an account): \url{https://www.nytimes.com/interactive/2020/02/21/business/coronavirus-airline-travel.html}.
\item
  Imbens, Guido W., and Thomas Lemieux, 2008, `Regression discontinuity designs: A guide to practice', \emph{Journal of Econometrics}, vol.~142, no. 2, pp.~615-635.
\item
  Myllyvirta, Lauri, 2020, `Analysis: Coronavirus has temporarily reduced China's CO2 emissions by a quarter', \emph{Carbon Brief}, 19 February, freely available at: \url{https://www.carbonbrief.org/analysis-coronavirus-has-temporarily-reduced-chinas-co2-emissions-by-a-quarter}.
\item
  Taddy, Matt, 2019, \emph{Business Data Science}, Chapter 5, pp.~146-152.
\item
  Travis, D.J., Carleton, A.M. and Lauritsen, R.G., 2004. `Regional variations in US diurnal temperature range for the 11--14 September 2001 aircraft groundings: Evidence of jet contrail influence on climate', Journal of climate, 17(5), pp.1123-1134.
\item
  Travis, David J., Andrew M. Carleton, and Ryan G. Lauritsen. ``Contrails reduce daily temperature range.'' Nature, 418, no. 6898 (2002): 601-601.
\item
  Zinovyeva, Natalia and Maryna Tverdostup, 2019, `Why are women who earn slightly more than their husbands hard to find?', 10 June, freely available at: \url{https://blogs.lse.ac.uk/businessreview/2019/06/10/why-are-women-who-earn-slightly-more-than-their-husbands-hard-to-find/}.
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\tightlist
\item
  Generating simulated data.
\item
  Understanding regression discontinuity and implementing it both manually and using packages.
\item
  Appreciating the threats to the validity of regression discontinuity.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{broom}
\item
  \texttt{rdrobust}
\item
  \texttt{tidyverse}
\end{itemize}

\textbf{Key functions/etc}

\begin{itemize}
\tightlist
\item
  \texttt{lm()}
\item
  \texttt{tidy()}
\item
  \texttt{rdrobust()()}
\end{itemize}

\textbf{Pre-quiz}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What are the fundamental features of regression discontinuity design?
\item
  What are the conditions that are needed in order for RDD to be able to be used?
\item
  Can you think of a situation in your own life where RDD may be useful?
\item
  What are some threats to the validity of RDD estimates?
\item
  Please look at the \texttt{performance} package: \url{https://easystats.github.io/performance/index.html}. What are some features of this package that may be useful in your own work?
\item
  What do you think about using COVID-19 in an RDD setting? Statistically? Ethically?
\item
  Please read and reproduce the main findings from Eggers, Fowler, Hainmueller, Hall, Snyder, 2015.
\end{enumerate}

\hypertarget{introduction-23}{%
\section{Introduction}\label{introduction-23}}

Regression discontinuity design (RDD) is a popular way to get causality when there is some continuous variable with cut-offs that determine treatment. Is there a difference between a student who gets 79 per cent and a student who gets 80 per cent? Probably not much, but one gets an A-, while the other gets a B+, and seeing that on a transcript could affect who gets a job which could affect income. In this case the percentage is a `forcing variable' and the cut-off for an A- is a `threshold'. As the treatment is determined by the forcing variable all you need to do is to control for that variable. And, these seemingly arbitrary cut-offs can be seen all the time. Hence, there has been an `explosion' in the use of regression discontinuity design (Figure \ref{fig:johnholbein}).

Please note that I've followed the terminology of Taddy, 2019. Gelman and Hill, 2007, and others use slightly different terminology. For instance, Cunningham refers to the forcing function as the running variable. It doesn't matter what you use so long as you are consistent. If you have a terminology that you are familiar with then please feel free to use it, and to share it with me!

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/johnholbein} \caption{The explosion of regression discontinuity designs in recent years.}\label{fig:johnholbein}
\end{figure}

Source: John Holbein, \href{https://twitter.com/JohnHolbein1/status/1228050675378069504}{13 February 2020}.

The key assumptions are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The cut-off is `known, precise and free of manipulation' (Cunningham, 2020, p.~163).
\item
  The forcing function should be continuous because this means we can say that people on either side of the threshold are the same, other than happening to just fall on either side of the threshold.
\end{enumerate}

\hypertarget{simulated-example-2}{%
\section{Simulated example}\label{simulated-example-2}}

Let's generate some data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(broom)}
\KeywordTok{library}\NormalTok{(tidyverse)}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{number_of_observation <-}\StringTok{ }\DecValTok{1000}

\NormalTok{rdd_example_data <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{person =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{number_of_observation),}
                           \DataTypeTok{grade =} \KeywordTok{runif}\NormalTok{(number_of_observation, }\DataTypeTok{min =} \DecValTok{78}\NormalTok{, }\DataTypeTok{max =} \DecValTok{82}\NormalTok{),}
                           \DataTypeTok{income =} \KeywordTok{rnorm}\NormalTok{(number_of_observation, }\DecValTok{10}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{                           )}

\CommentTok{# We want to make income more likely to be higher if they are have a grade over 80}
\NormalTok{rdd_example_data <-}\StringTok{ }
\StringTok{  }\NormalTok{rdd_example_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{income =} \KeywordTok{if_else}\NormalTok{(grade }\OperatorTok{>}\StringTok{ }\DecValTok{80}\NormalTok{, income }\OperatorTok{+}\StringTok{ }\DecValTok{2}\NormalTok{, income))}

\KeywordTok{head}\NormalTok{(rdd_example_data)}
\end{Highlighting}
\end{Shaded}

\label{tab:unnamed-chunk-392}

person

grade

income

1

79.4

9.43

2

78.5

9.69

3

79.9

10.8~

4

79.3

9.34

5

78.1

10.7~

6

79.6

9.83

Let's make a graph.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rdd_example_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ grade,}
             \DataTypeTok{y =}\NormalTok{ income)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ rdd_example_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(grade }\OperatorTok{<}\StringTok{ }\DecValTok{80}\NormalTok{), }
              \DataTypeTok{method=}\StringTok{'lm'}\NormalTok{,}
              \DataTypeTok{color =} \StringTok{"black"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ rdd_example_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(grade }\OperatorTok{>=}\StringTok{ }\DecValTok{80}\NormalTok{), }
              \DataTypeTok{method=}\StringTok{'lm'}\NormalTok{,}
              \DataTypeTok{color =} \StringTok{"black"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Grade"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Income ($)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-393-1.pdf}

We can use a dummy variable with linear regression to estimate the effect (we're hoping that it's 2 because that is what we imposed.)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rdd_example_data <-}\StringTok{ }
\StringTok{  }\NormalTok{rdd_example_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{grade_80_and_over =} \KeywordTok{if_else}\NormalTok{(grade }\OperatorTok{<}\StringTok{ }\DecValTok{80}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)) }

\KeywordTok{lm}\NormalTok{(income }\OperatorTok{~}\StringTok{ }\NormalTok{grade }\OperatorTok{+}\StringTok{ }\NormalTok{grade_}\DecValTok{80}\NormalTok{_and_over, }\DataTypeTok{data =}\NormalTok{ rdd_example_data) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{tidy}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\label{tab:unnamed-chunk-394}

term

estimate

std.error

statistic

p.value

(Intercept)

11.7~~

4.24~~

2.76~

0.00585~

grade

-0.021

0.0537

-0.391

0.696~~~

grade\_80\_and\_over

1.99~

0.123~

16.2~~

1.34e-52

There are various caveats to this estimate that we'll get into later, but the essentials are here.

The other great thing about regression discontinuity is that is can almost be as good as an RCT. For instance, (and I thank John Holbein for the pointer) \citet{bloombellreiman2020} compare randomized trials with RCTs and find that the RCTs compare favourably.

\hypertarget{different-slopes}{%
\subsection{Different slopes}\label{different-slopes}}

Figure \ref{fig:scotland} shows an example with different slopes.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/scotland} \caption{Effect of minimum unit pricing for alcohol in Scotland.}\label{fig:scotland}
\end{figure}

Source: John Burn-Murdoch, \href{https://twitter.com/jburnmurdoch/status/1225773931342303233}{7 February 2020}.

\hypertarget{overlap}{%
\section{Overlap}\label{overlap}}

In the randomised control trial and A/B testing section, because of randomised assignment of the treatment, we imposed that the control and treatment groups were the same but for the treatment. We moved to difference-in-differences, and we assumed that there was a common trend between the treated and control groups. We allowed that the groups could be different, but that we could `difference out' their differences. Finally, we considered matching, and we said that even if we the control and treatment groups seemed quite different we were able to match those who were treated with a group that were similar to them in all ways, apart from the fact that they were not treated.

In regression discontinuity we consider a slightly different setting - the two groups are completely different in terms of the forcing variable - they are on either side of the threshold. So there is no overlap at all. But we know the threshold and believe that those on either side are essentially matched. Let's consider the 2019 NBA Eastern Conference Semifinals - Toronto and the Philadelphia. Game 1: Raptors win 108-95; Game 2: 76ers win 94-89; Game 3: 76ers win 116-95; Game 4: Raptors win 101-96; Game 5: Raptors win 125-89; Game 6: 76ers win 112-101; and finally, Game 7: Raptors win 92-90, because of a ball that win in after bouncing on the rim four times. Was there really that much difference between the teams (Figure \ref{fig:kawai})?

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/torraptors13} \caption{It took four bounces to go in, so how different were the teams...?}\label{fig:kawai}
\end{figure}

Source: Stan Behal / \href{https://nationalpost.com/sports/kawhi-leonard-miracle-shot-toronto-raptors-game-7-scott-stinson-kawhi-leonards-miracle-shot-sends-toronto-raptors-into-game-7-elation}{Postmedia Network}.

\hypertarget{examples}{%
\section{Examples}\label{examples}}

As with difference-in-differences, after I learnt about it, I began to see opportunities to implement it everywhere. Frankly, I find it a lot easier to think of legitimate examples of using regression discontinuity than difference-in-differences. But, at the risk of mentioning yet another movie from the 1990s that none of you have seen, when I think of RDD, my first thought is often of Sliding Doors (Figure \ref{fig:slidingdoors}).

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/sliding_doors} \caption{Nobody expects the Spanish Inquisition.}\label{fig:slidingdoors}
\end{figure}

Source: Mlotek, Haley, 2018, `The Almosts and What-ifs of 'Sliding Doors'', \emph{The Ringer}, 24 April, freely available at: \url{https://www.theringer.com/movies/2018/4/24/17261506/sliding-doors-20th-anniversary}.

Not only did the movie have a great soundtrack and help propel Gwyneth Paltrow to super-stardom, but it features an iconic moment in which Paltrow's character, Helen, arrives at a tube station at which point the movie splits into two. In one version she just makes the train, and arrives home to find her boyfriend cheating on her; and in another she just misses the train and doesn't find out about the boyfriend.

I'd say, spoiler alert, but the movie was released in 1998, so\ldots{} Of course, that `threshold' turns out to be important. In the world in which she gets the train she leaves the boyfriend, cuts her hair, and changes everything about her life. In the world in which she misses the train she doesn't. At least initially. But, and I can't say this any better than Ashley Fetters:

\begin{quote}
At the end of Sliding Doors, the ``bad'' version of Helen's life elides right into the ``good'' version; even in the ``bad'' version, the philandering !@\#\$\%\^{}\& boyfriend eventually gets found out and dumped, the true love eventually gets met-cute, and the MVP friend comes through. According to the Sliding Doors philosophy, in other words, even when our lives take fluky, chaotic detours, ultimately good-hearted people find each other, and the bad boyfriends and home-wreckers of the world get their comeuppance. There's no freak turn of events that allows the cheating boyfriend to just keep cheating, or the well-meaning, morally upright soulmates to just keep floating around in the universe unacquainted.

Fetters, Ashley, 2018, `I Think About This a Lot: The Sliding Doors in Sliding Doors', \emph{The Cut}, 9 April, freely available at: \url{https://www.thecut.com/2018/04/i-think-about-this-a-lot-the-sliding-doors-in-sliding-doors.html}.
\end{quote}

I'm getting off-track here, but the point is, not only does it seem as though we have a `threshold', but it seems as though there's continuity!

Let's see some more legitimate implementations of regression discontinuity. (And thank you to \href{http://www.ryanbedwards.com/}{Ryan Edwards} for pointing me to these.)

\hypertarget{elections}{%
\subsection{Elections}\label{elections}}

Elections are a common area of application for regression discontinuity because if the election is close then arguably there's not much difference between the candidates. There are plenty of examples of regression discontinuity in an elections setting, but one recent one is George, Siddharth Eapen, 2019, `Like Father, Like Son? The Effect of Political Dynasties on Economic Development', freely available at: \url{https://www.dropbox.com/s/orhvh3n03wd9ybl/sid_JMP_dynasties_latestdraft.pdf?dl=0}.

In this paper George is interested in political dynasties. But is the child of a politician more likely to be elected because they are the child of a politician, or because they happen to also be similarly skilled at politics? Regression discontinuity can help because in a close election, we can look at differences between places where someone narrowly won with where a similar someone narrowly lost.

In the George, 2019, case he examines:

\begin{quote}
descendant effects using a close elections regression discontinuity (RD) design. We focus on close races between dynastic descendants (i.e.~direct relatives of former officeholders) and non-dynasts, and we compare places where a descendant narrowly won to those where a descendant narrowly lost. In these elections, descendants and non-dynasts have similar demographic and political characteristics, and win in similar places and at similar rates. Nevertheless, we find negative economic effects when a descendant narrowly wins. Villages represented by a descendant have lower asset ownership and public good provision after an electoral term: households are less likely to live in a brick house and to own basic amenities like a refrigerator, mobile phone, or vehicle. Moreover, voters assess descendants to perform worse in office. An additional standard deviation of exposure to descendants lowers a village's wealth rank by 12pp.
\end{quote}

The model that George, 2019, estimates is (p.~19:
\[y_i = \alpha_{\mbox{district}} + \beta \times \mbox{Years descendant rule}_i + f(\mbox{Descendant margin}) + \gamma X_i + \epsilon_{i,t}.\]
In this model, \(y_i\) is various development outcomes in village \(i\); \(\mbox{Years descendant rule}_i\) is the number of years a dynastic descendant has represented village \(i\) in the national or state parliament; \(\mbox{Descendant margin}\) is the vote share difference between the dynastic descendant and non-dynast; and \(\gamma X_i\) is a vector of village-level adjustments.

George, 2019, then conducts a whole bunch of tests of the validity of the regression discontinuity design (p.~19). These are critical in order for the results to be believed. There are a lot of different results but one is shown in Figure \ref{fig:descendantsinindia}.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/descendantsinindia} \caption{George, 2019, descendant effects identified using close elections RD design (p. 41).}\label{fig:descendantsinindia}
\end{figure}

\hypertarget{economic-development}{%
\subsection{Economic development}\label{economic-development}}

One of the issues with considering economic development is that a place typically is either subject to some treatment or not. However, sometimes regression discontinuity allows us to compare areas that were just barely treated with those that were just barely not.

One recent paper that does this Esteban Mendez-Chacon and Diana Van Patten, 2020, `Multinationals, monopsony and local development: Evidence from the United Fruit Company' available here: \url{https://www.dianavanpatten.com/}. They are interested in the effect of the United Fruit Company (UFCo), which was given land in Costa Rica between 1889 and 1984. They were given roughly 4 per cent of the national territory or around 4500 acres. They key is that this land assignment was redrawn in 1904 based on a river and hence the re-assignment was essentially random with regard to determinants of growth to that point. They compare areas that were assigned to UFCo with those that were not. They find:

\begin{quote}
We find that the firm had a positive and persistent effect on living standards. Regions within the UFCo were 26 per cent less likely to be poor in 1973 than nearby counterfactual locations, with only 63 per cent of the gap closing over the following three decades. Company documents explain that a key concern at the time was to attract and maintain a sizable workforce, which induced the firm to invest heavily in local amenities that likely account for our result.
\end{quote}

The model is:
\[y_{i,g,t} = \gamma\mbox{UFCo}_g + f(\mbox{geographic location}_g) + X_{i,g,t}\beta + X_g\Gamma + \alpha_t + \epsilon_{i,g,t}.\]
In this model, \(y_{i,g,t}\) is the development outcome for a household \(i\) in census-block \(g\) and year \(t\); \(\gamma\mbox{UFCo}_g\) is an indicator variable as to whether the census-block was in a UFCo area or not; \(f(\mbox{geographic location}_g)\) is a function of the latitude and longitude to adjust for geographic area; \(X_{i,g,t}\) is covariates for household \(i\); \(X_g\) is geographic characteristics for that census-block; and \(\alpha_t\) is a year fixed effect.

Again, there are a lot of different results but one is shown in Figure \ref{fig:ufco}.

\begin{figure}
\includegraphics[width=0.9\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/ufco} \caption{George, 2020, UFCo effect on the probability of being poor (p. 17).}\label{fig:ufco}
\end{figure}

\hypertarget{implementation-1}{%
\section{Implementation}\label{implementation-1}}

Although they are fairly conceptually similar to work that we have done in the past, if you are wanting to use regression discontinuity in your work then you might like to consider a specialised package. The package \texttt{rdrobust} is a one recommendation, although there are others available and you should try those if you are interested. (The \texttt{rdd} package had been the go-to for a while, but seems to have been taken off CRAN recently. If you use RDD, then maybe just follow up to see if it comes back on as that one is pretty nice.)

Let's look at our example using \texttt{rdrobust}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rdrobust)}
\KeywordTok{rdrobust}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ rdd_example_data}\OperatorTok{$}\NormalTok{income, }
         \DataTypeTok{x =}\NormalTok{ rdd_example_data}\OperatorTok{$}\NormalTok{grade, }
         \DataTypeTok{c =} \DecValTok{80}\NormalTok{, }\DataTypeTok{h =} \DecValTok{2}\NormalTok{, }\DataTypeTok{all =} \OtherTok{TRUE}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call: rdrobust
## 
## Number of Obs.                 1000
## BW type                      Manual
## Kernel                   Triangular
## VCE method                       NN
## 
## Number of Obs.                 497         503
## Eff. Number of Obs.            497         503
## Order est. (p)                   1           1
## Order bias  (q)                  2           2
## BW est. (h)                  2.000       2.000
## BW bias (b)                  2.000       2.000
## rho (h/b)                    1.000       1.000
## Unique Obs.                    497         503
## 
## =============================================================================
##         Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]       
## =============================================================================
##   Conventional     1.974     0.143    13.783     0.000     [1.693 , 2.255]     
## Bias-Corrected     1.977     0.143    13.805     0.000     [1.696 , 2.258]     
##         Robust     1.977     0.211     9.374     0.000     [1.564 , 2.390]     
## =============================================================================
\end{verbatim}

\hypertarget{fuzzy-rdd}{%
\section{Fuzzy RDD}\label{fuzzy-rdd}}

The examples to this point have been `sharp' RDD. That is, the threshold is strict. However, in reality, often the boundary is a little less strict. For instance, consider the drinking age. Although there is a legal drinking age, say 19. If we looked at the number of people who had drank, then it's likely to increase in the few years leading up to that age. Perhaps you went to Australia where the drinking age is 18 and drank. Or perhaps you snuck into a bar when you were 17, etc.

In a sharp RDD setting, if you know the value of the forcing function then you know the outcome. For instance, if you get a grade of 80 then we know that you got an A-, but if you got a grade of 79 then we know that you got a B+. But with fuzzy RDD it is only known with some probability. We can say that a Canadian 19-year-old is more likely to have drunk alcohol than a Canadian 18 year old, but the number of Canadian 18-year-olds who have drunk alcohol is not zero.

It may be possible to deal with fuzzy RDD settings with appropriate choice of model or data. It may also be possible to deal with them using instrumental variables, which we cover in the next section.

\hypertarget{threats-to-validity}{%
\section{Threats to validity}\label{threats-to-validity}}

The continuity assumption is fairly important, but we cannot test this as it is based on a counterfactual. Instead we need to convince people of it. Ways to do this include:

\begin{itemize}
\tightlist
\item
  Using a test/train set-up.
\item
  Trying different specifications (and be very careful if your results don't broadly persist when just consider linear or quadratic functions).
\item
  Considering different subsets of the data.
\item
  Consider different windows.
\item
  Be up-front about uncertainty intervals, especially in graphs.
\item
  Discuss and assuage concerns about the possibility of omitted variables.
\end{itemize}

The threshold is also important. For instance, is there an actual shift or is there a non-linear relationship?

We want as `sharp' an effect as possible, but if the thresholds are known, then they will be gamed. For instance, there is a lot of evidence that people run for certain marathon times, and we know that people aim for certain grades. Similarly, from the other side, it is a lot easier for an instructor to just give out As than it is to have to justify Bs. One way to look at this is to consider how `balanced' the sample is on either side of the threshold. Do this by using histograms with appropriate bins, for instance Figure \ref{fig:marathontimes}, which is from \citet{allen2017reference}.

\begin{figure}
\includegraphics[width=0.9\linewidth]{figures/marathontimes} \caption{Bunching around marathon times.}\label{fig:marathontimes}
\end{figure}

\hypertarget{weaknesses}{%
\section{Weaknesses}\label{weaknesses}}

\begin{itemize}
\tightlist
\item
  External validity may be hard - think about the A-/B+ example - do you think the findings generalise to B-/C+?
\item
  The important responses are those that are close to the cut-off. So even if we have a whole bunch of B- and A+ students, they don't really help much. Hence we need a lot of data.
\item
  There is a lot of freedom for the researcher, so open science best practice becomes vital.
\end{itemize}

\hypertarget{case-study---stiers-hooghe-and-dassonneville-2020}{%
\section{Case study - Stiers, Hooghe, and Dassonneville, 2020}\label{case-study---stiers-hooghe-and-dassonneville-2020}}

Paper: Stiers, D., Hooghe, M. and Dassonneville, R., 2020. Voting at 16: Does lowering the voting age lead to more political engagement? Evidence from a quasi-experiment in the city of Ghent (Belgium). Political Science Research and Methods, pp.1-8. Available at: \url{https://www.cambridge.org/core/journals/political-science-research-and-methods/article/voting-at-16-does-lowering-the-voting-age-lead-to-more-political-engagement-evidence-from-a-quasiexperiment-in-the-city-of-ghent-belgium/172A2D9B75ECB66E98C9680787F302AD\#fndtn-information}

Data: \url{https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/J1FQW9}

\hypertarget{case-study---caughey-and-sekhon.-2011}{%
\section{Case study - Caughey, and Sekhon., 2011}\label{case-study---caughey-and-sekhon.-2011}}

Paper: Caughey, Devin, and Jasjeet S. Sekhon. ``Elections and the regression discontinuity design: Lessons from close US house races, 1942--2008.'' Political Analysis 19.4 (2011): 385-408. Available at: \url{https://www.cambridge.org/core/journals/political-analysis/article/elections-and-the-regression-discontinuity-design-lessons-from-close-us-house-races-19422008/E5A69927D29BE682E012CAE9BFD8AEB7}

Data: \url{https://dataverse.harvard.edu/dataset.xhtml?persistentId=hdl:1902.1/16357\&version=1.0}

\hypertarget{poll-of-polls}{%
\chapter{Poll of polls}\label{poll-of-polls}}

\textbf{Required reading}

\begin{itemize}
\tightlist
\item
  Arnold, Jeffrey B., 2018, `Simon Jackman's Bayesian Model Examples in Stan', Ch 13, 7 May, \url{https://jrnold.github.io/bugs-examples-in-stan/campaign.html}.
\item
  Gelman, Andrew, Jessica Hullman, and Christopher Wlezien, 2020, `Information, incentives, and goals in election forecasts', 8 September, available at: \url{http://www.stat.columbia.edu/~gelman/research/unpublished/forecast_incentives3.pdf}
\item
  Gelman, Andrew, Merlin Heidemanns, and Elliott Morris, 2020, `2020 US POTUS model', The Economist, freely available: \url{https://github.com/TheEconomist/us-potus-model}.
\item
  Jackman, Simon, 2005, `Pooling the polls over an election campaign', \emph{Australian Journal of Political Science}, 40 (4), pp.~499-517.
\end{itemize}

\textbf{Required viewing}

\begin{itemize}
\tightlist
\item
  Jackman, Simon, 2020, `The triumph of the quants?: Model-based poll aggregation for election forecasting', \emph{Ihaka Lecture Series}, \url{https://youtu.be/MvGYsKIsLFs}.
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
  Imai, Kosuke, 2017, Quantitative Social Science: An Introduction, Princeton University Press, Ch 4.1, and 5.3.
\item
  Leigh, Andrew, and Justin Wolfers, 2006, `Competing approaches to forecasting elections: Economic models, opinion polling and prediction markets', Economic Record, 82 (258), pp.325-340.
\item
  Nickerson, David W., and Todd Rogers, 2014, `Political campaigns and big data', Journal of Economic Perspectives, 28 (2), pp.~51-74.
\item
  Shirani-Mehr, Houshmand, David Rothschild, Sharad Goel, and Andrew Gelman, 2018, `Disentangling bias and variance in election polls', Journal of the American Statistical Association, 113 (522), pp.~607-614.
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\item
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\item
\end{itemize}

\textbf{Key functions/etc}

\begin{itemize}
\item
\end{itemize}

\textbf{Quiz}

\begin{itemize}
\item
\end{itemize}

\textbf{Lab}

\begin{itemize}
\tightlist
\item
  Following the guidance of the TA, please examine various poll-of-polls models, make changes to them, and then see how their output changes.
\end{itemize}

\hypertarget{introduction-24}{%
\section{Introduction}\label{introduction-24}}

\begin{quote}
{[}The Presidential election of{]} 2016 was the largest analytics failure in US political history.

David Shor, 13 August 2020
\end{quote}

In this section we look at poll-of-polls (equally pooling-the-polls or poll aggregation) approaches which use a statistical model to bring together the outcomes of polls.

\hypertarget{multilevel-modelling-with-post-stratification}{%
\chapter{Multilevel modelling with post-stratification}\label{multilevel-modelling-with-post-stratification}}

\textbf{Required reading}

\begin{itemize}
\tightlist
\item
  Alexander, Monica, 2019, `Analyzing name changes after marriage using a non-representative survey', 7 August, \url{https://www.monicaalexander.com/posts/2019-08-07-mrp/}.
\item
  Gelman, Andrew, Jennifer Hill and Aki Vehtari, 2020, \emph{Regression and Other Stories}, Cambridge University Press, Ch 17.
\item
  Hanretty, Chris, 2019, `An introduction to multilevel regression and post-stratification for estimating constituency opinion', \emph{Political Studies Review}, \url{https://doi.org/10.1177/1478929919864773}.
\item
  Kastellec, Jonathan, Jeffrey Lax, and Justin Phillips, 2016, `Estimating State Public Opinion With Multi-Level Regression and Poststratification using R', \url{https://scholar.princeton.edu/sites/default/files/jkastellec/files/mrp_primer.pdf}.
\item
  Kennedy, Lauren and Jonah Gabry, 2019, `MRP with rstanarm', rstanarm vignettes, 8 October, \url{https://mc-stan.org/rstanarm/articles/mrp.html}.
\item
  Kennedy, Lauren, and Andrew Gelman, 2019, `Know your population and know your model: Using model-based regression and poststratification to generalize findings beyond the observed sample', \url{https://arxiv.org/abs/1906.11323}.
\item
  Wang, Wei, David Rothschild, Sharad Goel, and Andrew Gelman, 2015, `Forecasting elections with non-representative polls', \emph{International Journal of Forecasting}, 31, no. 3, pages 980-991.
\item
  Wu, Changbao and Mary E. Thompson, 2020, \emph{Sampling Theory and Practice}, Springer, Ch 17.
\end{itemize}

\textbf{Required viewing}

\begin{itemize}
\tightlist
\item
  Gelman, Andrew, 2020, `Statistical Models of Election Outcomes', \emph{CPSR Summer Program in Quantitative Methods of Social Research}, \url{https://youtu.be/7gjDnrbLQ4k}.
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
  Cohn, Nate, 2016, `We Gave Four Good Pollsters the Same Raw Data. They Had Four Different Results', \emph{The New York Times}, The Upshot, 20 September, \url{https://www.nytimes.com/interactive/2016/09/20/upshot/the-error-the-polling-world-rarely-talks-about.html}.
\item
  Gelman, Andrew, and Julia Azari, 2017, `19 things we learned from the 2016 election', \emph{Statistics and Public Policy}, 4 (1), pp.~1-10.
\item
  Ghitza, Yair, and Andrew Gelman, 2013, `Deep Interactions with MRP: Election Turnout and Voting Patterns Among Small Electoral Subgroups', \emph{American Journal of Political Science}, 57 (3), pp.~762-776.
\item
  Ghitza, Yair, and Andrew Gelman, 2020, `Voter Registration Databases and MRP: Toward the Use of Large-Scale Databases in Public Opinion Research', \emph{Political Analysis}, pp.~1-25.
\item
  Jackman, Simon, Shaun Ratcliff and Luke Mansillo, 2019, `Small area estimates of public opinion: Model-assisted post-stratification of data from voter advice applications', 4 January, \url{https://www.cambridge.org/core/membership/services/aop-file-manager/file/5c2f6ebb7cf9ee1118d11c0a/APMM-2019-Simon-Jackman.pdf}.
\item
  Lauderdale, Ben, Delia Bailey, Jack Blumenau, and Doug Rivers, 2020, `Model-based pre-election polling for national and sub-national outcomes in the US and UK', \emph{International Journal of Forecasting}, 36 (2), pp.~399-413.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{brms}
\item
  \texttt{broom}
\item
  \texttt{here}
\item
  \texttt{tidybayes}
\item
  \texttt{tidyverse}
\end{itemize}

\textbf{Quiz}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Your Mum asked you what you've been learning this term. You decide to tell her about multilevel modelling with post-stratification. Please explain what MRP is. Your Mum has a university-education, but has not necessarily taken any statistics, so you will need to explain any technical terms that you use. {[}Please write a paragraph or two.{]}
\item
  Please consider Wang, Rothschild, Goel, and Gelman, 2015, `Forecasting elections with non-representative polls'. Is this the most interesting paper ever written? Why or why not? What do you like about this paper? What do you wish it did better? Can you reproduce this paper? {[}Please write one or two paragraphs about each aspect.{]}
\item
  I am interested in studying how voting intentions in the recent US presidential election vary by an individual's income. I set up a logistic regression model to study this relationship. In my study, some possible independent variables would be: {[}Please check all that apply.{]} 1) Whether the respondent is registered to vote (yes/no). 2) Whether the respondent is going to vote for Biden (yes/no). 3) The race of the respondent (white/not white). 4) The respondent's marital status (married/not)
\item
  Please think about Cohn, 2016, `We Gave Four Good Pollsters the Same Raw Data. They Had Four Different Results'. Why is this type of exercise not carried out more? Why do you think that different groups, even with the same background and level of quantitative sophistication, could have such different estimates even when they use the same data? {[}Please write a paragraph or two about each aspect.{]}
\item
  Please consider Wang, Rothschild, Goel, and Gelman, 2015, `Forecasting elections with non-representative polls'.

  \begin{itemize}
  \tightlist
  \item
    What is not a feature they mention election forecasts need? 1) Explainable. 2) Accurate. 3) Cost-effective. 4) Relevant. 5) Timely.
  \item
    What is a weakness of MRP? 1) Detailed data requirement. 2) Allows use of biased data. 3) Expensive to conduct.
  \item
    What is concerning about the Xbox sample? 1) Non-representative. 2) Small sample size. 3) Multiple responses from the same respondent.
  \end{itemize}
\end{enumerate}

\hypertarget{introduction-25}{%
\section{Introduction}\label{introduction-25}}

Multilevel regression with post-stratification (MRP) is a popular way to adjust non-representative samples to better analyse opinion and other survey responses. It uses a regression model to relate individual-level survey responses to various characteristics and then rebuilds the sample to better match the population. In this way MRP can not only allow a better understanding of responses, but also allow us to analyse data that may otherwise be unusable. However, it can be a challenge to get started with MRP as the terminology may be unfamiliar, and the data requirements can be onerous.

Multilevel regression with post-stratification (MRP) is a handy approach when dealing with survey data. Essentially, it trains a model based on the survey, and then applies that trained model to another dataset. There are two main, related, advantages:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  It can allow us to `re-weight' in a way that includes uncertainty front-of-mind and isn't hamstrung by small samples.
\item
  It can allow us to use broad surveys to speak to subsets.
\end{enumerate}

From a practical perspective, it tends to be less expensive to collect non-probability samples and so there are benefits of being able to use these types of data. That said, it is not a magic-bullet and the laws of statistics still apply. We will have larger uncertainty around our estimates and they will still be subject to all the usual biases. As \href{https://twitter.com/jazzystats}{Lauren Kennedy} points out, `MRP has traditionally been used in probability surveys and had potential for non-probability surveys, but we're not sure of the limitations at the moment.'

One famous example is Wei Wang, David Rothschild, Sharad Goel, and Andrew Gelman, 2014, `Forecasting elections with non-representative polls', \emph{International Journal of Forecasting}. They used data from the Xbox gaming platform to forecast the 2012 US Presidential Election.

Key facts about the set-up:

\begin{itemize}
\tightlist
\item
  Data from an opt-in poll which was available on the Xbox gaming platform during the 45 days preceding the 2012 US presidential election.
\item
  Each day there were three to five questions, including voter intention: ``If the election were held today, who would you vote for?''
\item
  Respondents were allowed to answer at most once per day.
\item
  First-time respondents were asked to provide information about themselves, including their sex, race, age, education, state, party ID, political ideology, and who they voted for in the 2008 presidential election.
\item
  In total, 750,148 interviews were conducted, with 345,858 unique respondents - over 30,000 of whom completed five or more polls
\item
  Young men dominate the Xbox population: 18-to-29-year-olds comprise 65 per cent of the Xbox dataset, compared to 19 per cent in the exit poll; and men make up 93 per cent of the Xbox sample but only 47 per cent of the electorate.
\end{itemize}

Given the US electorate, they use a two-stage modelling approach. The details don't really matter too much, and essentially they model how likely a respondent is to vote for Obama, given various information such as state, education, sex, etc:
\[
Pr\left(Y_i = \mbox{Obama} | Y_i\in\{\mbox{Obama, Romney}\}\right) = \mbox{logit}^{-1}(\alpha_0 + \alpha_1(\mbox{state last vote share}) 
+ \alpha_{j[i]}^{\mbox{state}} + \alpha_{j[i]}^{\mbox{edu}} + \alpha_{j[i]}^{\mbox{sex}}...
)
\]

They run this in R using glmer() from lme4.

Having a trained model that considers the effect of these various independent variables on support for the candidates, they now post-stratify, where each of these ``cell-level estimates are weighted by the proportion of the electorate in each cell and aggregated to the appropriate level (i.e., state or national).''

This means that they need cross-tabulated population data. In general, the census would have worked, or one of the other large surveys available in the US, but the difficulty is that the variables need to be available on a cross-tab basis. As such, they use exit polls (not an option for Australia in general).

They make state-specific estimates by post-stratifying to the features of each state (Figure \ref{fig:states}).

\begin{figure}
\includegraphics[width=1\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/states} \caption{Post-stratified estimates for each state based on the Xbox survey and MRP}\label{fig:states}
\end{figure}

Similarly, they can examine demographic-differences (Figure \ref{fig:demographics}).

\begin{figure}
\includegraphics[width=1\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/demographics} \caption{Post-stratified estimates on a demographic basis based on the Xbox survey and MRP}\label{fig:demographics}
\end{figure}

Finally, they convert their estimates into electoral college estimates (Figure \ref{fig:electoralcollege}).

\begin{figure}
\includegraphics[width=1\linewidth]{/Users/rohanalexander/Documents/telling_stories_with_data/figures/electoral_college} \caption{Post-stratified estimates of electoral college outcomes based on the Xbox survey and MRP}\label{fig:electoralcollege}
\end{figure}

\hypertarget{hello-world-1}{%
\section{Hello world}\label{hello-world-1}}

The workflow that we are going to use is:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  read in the poll;
\item
  model the poll;
\item
  read in the post-stratification data; and
\item
  apply the model to the post-stratification data.
\end{enumerate}

We are going to use R \citep{citeR}. First load the packages: \texttt{broom} \citep{citebroom}, \texttt{here} \citep{citehere}, \texttt{tidyverse} \citep{citetidyverse}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Uncomment these (by deleting the #) if you need to install the packages}
\CommentTok{# install.packages("broom")}
\CommentTok{# install.packages("here")}
\CommentTok{# install.packages("skimr")}
\CommentTok{# install.packages("tidyverse")}

\KeywordTok{library}\NormalTok{(broom) }\CommentTok{# Helps make the regression results tidier}
\KeywordTok{library}\NormalTok{(here) }\CommentTok{# Helps make file referencing easier.}
\KeywordTok{library}\NormalTok{(tidyverse) }\CommentTok{# Helps make programming with R easier}
\end{Highlighting}
\end{Shaded}

Then load some sample polling data to analyse. I have generated this fictitious data so that we have some idea of what to expect from the model. The dependent variable is supports\_ALP, which is a binary variable - either 0 or 1. We'll just use two independent variables here: gender, which is either Female or Male (as that is what is available from the ABS); and age\_group, which is one of four groups: ages 18 to 29, ages 30 to 44, ages 45 to 59, ages 60 plus.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example_poll <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"outputs/data/example_poll.csv"}\NormalTok{) }\CommentTok{# Here we read in a }
\CommentTok{# CSV file and assign it to a dataset called 'example_poll'}

\KeywordTok{head}\NormalTok{(example_poll) }\CommentTok{# Displays the first 10 rows}
\end{Highlighting}
\end{Shaded}

(\#tab:initial\_model\_simulate\_data)

gender

age\_group

supports\_ALP

state

Male

ages30to44

0

NSW

Female

ages45to59

0

NSW

Female

ages60plus

1

VIC

Male

ages30to44

1

QLD

Female

ages30to44

1

QLD

Female

ages18to29

1

VIC

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Look at some summary statistics to make sure the data seem reasonable}
\KeywordTok{summary}\NormalTok{(example_poll)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     gender           age_group          supports_ALP       state          
##  Length:5000        Length:5000        Min.   :0.0000   Length:5000       
##  Class :character   Class :character   1st Qu.:0.0000   Class :character  
##  Mode  :character   Mode  :character   Median :1.0000   Mode  :character  
##                                        Mean   :0.5514                     
##                                        3rd Qu.:1.0000                     
##                                        Max.   :1.0000
\end{verbatim}

I generated this polling data to make both made males and older people less likely to vote for the Australian Labor Party; and females and younger people more likely to vote for the Labor Party. Females are over-sampled. As such, we should have an ALP skew on the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# The '%>%' is called a 'pipe' and it takes whatever the output is of the }
\CommentTok{# command before it, and pipes it to the command after it.}
\NormalTok{example_poll }\OperatorTok{%>%}\StringTok{ }\CommentTok{# So we are taking our example_poll dataset and using it as an }
\StringTok{  }\CommentTok{# input to 'summarise'.}
\StringTok{   }\CommentTok{# summarise reduces the dimensions, so here we will get one number from a column.}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{raw_ALP_prop =} \KeywordTok{sum}\NormalTok{(supports_ALP) }\OperatorTok{/}\StringTok{ }\KeywordTok{nrow}\NormalTok{(example_poll))}
\end{Highlighting}
\end{Shaded}

(\#tab:summarise\_model\_simulate\_data)

raw\_ALP\_prop

0.551

Now we'd like to see if we can get our results back (we should find females less likely than males to vote for Australian Labor Party and that people are less likely to vote Australian Labor Party as they get older). Our model is:

\[
\mbox{ALP support}_j = \mbox{gender}_j + \mbox{age\_group}_j + \epsilon_j
\]

This model says that the probability that some person, \(j\), will vote for the Australian Labor Party depends on their gender and their age-group. Based on our simulated data, we would like older age-groups to be less likely to vote for the Australian Labor Party and for males to be less likely to vote for the Australian Labor Party.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Here we are running an OLS regression with supports_ALP as the dependent variable }
\CommentTok{# and gender and age_group as the independent variables. The dataset that we are }
\CommentTok{# using is example_poll. We are then saving that OLS regression to a variable called 'model'.}
\NormalTok{model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(supports_ALP }\OperatorTok{~}\StringTok{ }\NormalTok{gender }\OperatorTok{+}\StringTok{ }\NormalTok{age_group, }
            \DataTypeTok{data =}\NormalTok{ example_poll}
\NormalTok{            )}

\CommentTok{# broom::tidy just displays the outputs of the regression in a nice table.}
\NormalTok{broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{(model) }
\end{Highlighting}
\end{Shaded}

(\#tab:initial\_model\_analyse\_example\_polling)

term

estimate

std.error

statistic

p.value

(Intercept)

0.9~~

0.0131

68.8

0~~~~~~~~

genderMale

-0.205

0.0142

-14.4

2.69e-46~

age\_groupages30to44

-0.186

0.0176

-10.6

6.5e-26~~

age\_groupages45to59

-0.402

0.0177

-22.7

8.29e-109

age\_groupages60plus

-0.585

0.0175

-33.4

5.2e-221~

Essentially we've got our inputs back. We just used regular OLS even though our dependent variable is a binary. (It's usually fine to start with an OLS model and then iterate toward an approach that may be more appropriate such as logistic regression or whatever, but where the results are a little more difficult to interpret.\footnote{\href{https://www.monicaalexander.com/}{Monica} is horrified by the use of OLS here, and wants it on the record that she regrets not making not doing this part of our marriage vows.}) If you wanted to do that then the place to start would be \texttt{glmer()} from the R package \texttt{lme4}, and we'll see that in the next section.

Now we'd like to see if we can use what we found in the poll to get an estimate for each state based on their demographic features.

First read in some real demographic data, on a seat basis, from the ABS.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{census_data <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"outputs/data/census_data.csv"}\NormalTok{)}
\KeywordTok{head}\NormalTok{(census_data)}
\end{Highlighting}
\end{Shaded}

(\#tab:initial\_model\_post\_stratify\_add\_coefficients)

state

gender

age\_group

number

cell\_prop\_of\_division\_total

ACT

Female

ages18to29

3.47e+04

0.125

ACT

Female

ages30to44

4.3e+04~

0.155

ACT

Female

ages45to59

3.38e+04

0.122

ACT

Female

ages60plus

3.03e+04

0.109

ACT

Male

ages18to29

3.42e+04

0.123

ACT

Male

ages30to44

4.13e+04

0.149

We're just going to do some rough forecasts. For each gender and age-group we want the relevant coefficient in the example\_data and we can construct the estimates.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Here we are making predictions using our model with some new data from the }
\CommentTok{# census, and we saving the results of those predictions by adding a new column }
\CommentTok{# to the census_data dataset called 'estimate'.}
\NormalTok{census_data}\OperatorTok{$}\NormalTok{estimate <-}\StringTok{ }
\StringTok{  }\NormalTok{model }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{predict}\NormalTok{(}\DataTypeTok{newdata =}\NormalTok{ census_data)}

\NormalTok{census_data }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{alp_predict_prop =}\NormalTok{ estimate}\OperatorTok{*}\NormalTok{cell_prop_of_division_total) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(state) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{alp_predict =} \KeywordTok{sum}\NormalTok{(alp_predict_prop))}
\end{Highlighting}
\end{Shaded}

(\#tab:initial\_model\_post\_stratify\_age\_sex\_specific)

state

alp\_predict

ACT

0.525

NSW

0.495

NT

0.541

QLD

0.496

SA

0.479

TAS

0.464

VIC

0.503

WA

0.503

We now have post-stratified estimates for each division. Our model has a fair few weaknesses. For instance small cell counts are going to be problematic. And our approach ignores uncertainty, but now that we have something working we can complicate it.

\hypertarget{your-turn}{%
\section{Your turn!}\label{your-turn}}

\emph{We're going to go through this all again, but this time with you doing it. If you run into issues then I am happy to help point you in the right direction.}

As a reminder, our workflow is:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  read in the poll;
\item
  model the poll;
\item
  read in the post-stratification data;
\item
  apply your model to the post-stratification data.
\end{enumerate}

Get started by opening a Rproj file and opening a new R script.

\hypertarget{extended-example}{%
\section{Extended example}\label{extended-example}}

We'd like to address some of the major issues with our approach, specifically being able to deal with small cell counts, and also taking better account of uncertainty. As we are dealing with survey data, prediction intervals or something similar are crticial, and it's not appropriate to only report central estimates. To do this we'll use the same broad approach as before, but just improving bits of our workflow.

First load the packages (you don't need to reload the earlier ones - I just do it here so that each section is self-contained in case people are lost). We additionally need: \texttt{brms} \citep{citebrms} and \texttt{tidybayes} \citep{citetidybayes}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Uncomment these if you need to install the packages}
\CommentTok{# install.packages("broom")}
\CommentTok{# install.packages("brms")}
\CommentTok{# install.packages("here") }
\CommentTok{# install.packages("tidybayes")}
\CommentTok{# install.packages("tidyverse") }

\KeywordTok{library}\NormalTok{(broom)}
\KeywordTok{library}\NormalTok{(brms) }\CommentTok{# Used for the modelling}
\KeywordTok{library}\NormalTok{(here)}
\KeywordTok{library}\NormalTok{(tidybayes) }\CommentTok{# Used to help understand the modelling estimates}
\KeywordTok{library}\NormalTok{(tidyverse) }
\end{Highlighting}
\end{Shaded}

As before, read in the polling dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example_poll <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"outputs/data/example_poll.csv"}\NormalTok{)}

\KeywordTok{head}\NormalTok{(example_poll)}
\end{Highlighting}
\end{Shaded}

(\#tab:brms\_model\_simulate\_data)

gender

age\_group

supports\_ALP

state

Male

ages30to44

0

NSW

Female

ages45to59

0

NSW

Female

ages60plus

1

VIC

Male

ages30to44

1

QLD

Female

ages30to44

1

QLD

Female

ages18to29

1

VIC

Now, using the same basic model as before, but we move it to a setting that acknowledges the dependent variable as being binary, and in a Bayesian setting.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model <-}\StringTok{ }\KeywordTok{brm}\NormalTok{(supports_ALP }\OperatorTok{~}\StringTok{ }\NormalTok{gender }\OperatorTok{+}\StringTok{ }\NormalTok{age_group, }
             \DataTypeTok{data =}\NormalTok{ example_poll, }
             \DataTypeTok{family =} \KeywordTok{bernoulli}\NormalTok{(),}
             \DataTypeTok{file =} \StringTok{"outputs/model/brms_model"}
\NormalTok{             )}

\NormalTok{model <-}\StringTok{ }\KeywordTok{read_rds}\NormalTok{(}\StringTok{"outputs/model/brms_model.rds"}\NormalTok{)}

\KeywordTok{summary}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Family: bernoulli 
##   Links: mu = logit 
## Formula: supports_ALP ~ gender + age_group 
##    Data: example_poll (Number of observations: 5000) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept               2.07      0.09     1.91     2.23 1.00     2240     2194
## genderMale             -1.06      0.07    -1.20    -0.91 1.00     3403     2595
## age_groupages30to44    -1.10      0.10    -1.29    -0.91 1.00     2483     2805
## age_groupages45to59    -2.04      0.10    -2.23    -1.85 1.00     2521     3061
## age_groupages60plus    -2.88      0.10    -3.09    -2.68 1.00     2517     2858
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
\end{verbatim}

We've moved to the Bernoulli distribution, so we have to do a bit more work to understand our results, but we are broadly getting back what we'd expect.

As before, we'd like an estimate for each state based on their demographic features and start by reading in the data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{census_data <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"outputs/data/census_data.csv"}\NormalTok{)}
\KeywordTok{head}\NormalTok{(census_data)}
\end{Highlighting}
\end{Shaded}

(\#tab:brms\_model\_post\_stratify\_add\_coefficients)

state

gender

age\_group

number

cell\_prop\_of\_division\_total

ACT

Female

ages18to29

3.47e+04

0.125

ACT

Female

ages30to44

4.3e+04~

0.155

ACT

Female

ages45to59

3.38e+04

0.122

ACT

Female

ages60plus

3.03e+04

0.109

ACT

Male

ages18to29

3.42e+04

0.123

ACT

Male

ages30to44

4.13e+04

0.149

We're just going to do some rough forecasts. For each gender and age\_group we want the relevant coefficient in the example\_data and we can construct the estimates (this code is from \href{https://www.monicaalexander.com/posts/2019-08-07-mrp/}{Monica Alexander}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{post_stratified_estimates <-}\StringTok{ }
\StringTok{  }\NormalTok{model }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{tidybayes}\OperatorTok{::}\KeywordTok{add_predicted_draws}\NormalTok{(}\DataTypeTok{newdata =}\NormalTok{ census_data) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{rename}\NormalTok{(}\DataTypeTok{alp_predict =}\NormalTok{ .prediction) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{alp_predict_prop =}\NormalTok{ alp_predict}\OperatorTok{*}\NormalTok{cell_prop_of_division_total) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(state, .draw) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{alp_predict =} \KeywordTok{sum}\NormalTok{(alp_predict_prop)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(state) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(alp_predict), }
            \DataTypeTok{lower =} \KeywordTok{quantile}\NormalTok{(alp_predict, }\FloatTok{0.025}\NormalTok{), }
            \DataTypeTok{upper =} \KeywordTok{quantile}\NormalTok{(alp_predict, }\FloatTok{0.975}\NormalTok{))}

\NormalTok{post_stratified_estimates}
\end{Highlighting}
\end{Shaded}

(\#tab:brms\_model\_post\_stratify\_age\_sex\_specific)

state

mean

lower

upper

ACT

0.53~

0.245

0.791

NSW

0.494

0.214

0.767

NT

0.544

0.253

0.852

QLD

0.498

0.215

0.768

SA

0.482

0.201

0.75~

TAS

0.465

0.183

0.757

VIC

0.502

0.224

0.769

WA

0.503

0.219

0.765

We now have post-stratified estimates for each division. Our new Bayesian approach will enable us to think more deeply about uncertainty. We could complicate this in a variety of ways including adding more coefficients (but remember that we'd need to get new cell counts), or adding some layers.

\hypertarget{your-turn-1}{%
\section{Your turn!}\label{your-turn-1}}

\emph{We're going to go through this all again, but this time with you doing it. If you run into issues then I am happy to help point you in the right direction.}

As a reminder, our workflow is:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  read in the poll;
\item
  model the poll;
\item
  read in the post-stratification data;
\item
  apply your model to the post-stratification data.
\end{enumerate}

\hypertarget{adding-layers}{%
\section{Adding layers}\label{adding-layers}}

We may like to try to add some layers to our model. For instance, we may like a different intercept for each state.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model_states <-}\StringTok{ }\KeywordTok{brm}\NormalTok{(supports_ALP }\OperatorTok{~}\StringTok{ }\NormalTok{gender }\OperatorTok{+}\StringTok{ }\NormalTok{age_group }\OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{|}\NormalTok{state), }
                    \DataTypeTok{data =}\NormalTok{ example_poll, }
                    \DataTypeTok{family =} \KeywordTok{bernoulli}\NormalTok{(),}
                    \DataTypeTok{file =} \StringTok{"outputs/model/brms_model_states"}\NormalTok{,}
                    \DataTypeTok{control =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{adapt_delta =} \FloatTok{0.90}\NormalTok{)}
\NormalTok{                    )}
\KeywordTok{summary}\NormalTok{(model_states)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Family: bernoulli 
##   Links: mu = logit 
## Formula: supports_ALP ~ gender + age_group + (1 | state) 
##    Data: example_poll (Number of observations: 5000) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~state (Number of levels: 8) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.06      0.05     0.00     0.20 1.00     1553     2072
## 
## Population-Level Effects: 
##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept               2.07      0.09     1.90     2.26 1.00     1660     2273
## genderMale             -1.06      0.08    -1.21    -0.91 1.00     4106     2833
## age_groupages30to44    -1.10      0.10    -1.30    -0.90 1.00     2110     2566
## age_groupages45to59    -2.04      0.10    -2.24    -1.84 1.00     2058     2347
## age_groupages60plus    -2.89      0.10    -3.10    -2.69 1.00     2201     2581
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# broom::tidy(model_states, par_type = "varying")}
\CommentTok{# broom::tidy(model_states, par_type = "non-varying", robust = TRUE)}
\end{Highlighting}
\end{Shaded}

One interesting aspect is that our multilevel approach will allow us to deal with small cell counts by borrowing information from other cells.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example_poll }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(state)}
\end{Highlighting}
\end{Shaded}

(\#tab:brms\_model\_analyse\_extended\_state\_counts)

state

n

ACT

107

NSW

1622

NT

50

QLD

982

SA

359

TAS

105

VIC

1285

WA

490

At the moment we have 50 respondents in the Northern Territory, 105 in Tasmania, and 107 in the ACT. Even if we were to remove most of the, say, 18 to 29 year old, male respondents from Tasmania our model would still provide estimates. It does this by pooling, in which the effect of these young, male, Tasmanians is partially determined by other cells that do have respondents.

\hypertarget{communication}{%
\section{Communication}\label{communication}}

There are many interesting aspects that we may like to communicate to others. For instance, we may like to show how the model is affecting the results. We can make a graph that compares the raw estimate with the model estimate.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{post_stratified_estimates }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ mean, }\DataTypeTok{x =}\NormalTok{ forcats}\OperatorTok{::}\KeywordTok{fct_inorder}\NormalTok{(state), }\DataTypeTok{color =} \StringTok{"MRP estimate"}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_errorbar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{ymin =}\NormalTok{ lower, }\DataTypeTok{ymax =}\NormalTok{ upper), }\DataTypeTok{width =} \DecValTok{0}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Proportion ALP support"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"State"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ example_poll }\OperatorTok{%>%}\StringTok{ }
\StringTok{               }\KeywordTok{group_by}\NormalTok{(state, supports_ALP) }\OperatorTok{%>%}
\StringTok{               }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{n =} \KeywordTok{n}\NormalTok{()) }\OperatorTok{%>%}\StringTok{ }
\StringTok{               }\KeywordTok{group_by}\NormalTok{(state) }\OperatorTok{%>%}\StringTok{ }
\StringTok{               }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{prop =}\NormalTok{ n}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(n)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{               }\KeywordTok{filter}\NormalTok{(supports_ALP}\OperatorTok{==}\DecValTok{1}\NormalTok{), }
             \KeywordTok{aes}\NormalTok{(state, prop, }\DataTypeTok{color =} \StringTok{"Raw data"}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"bottom"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.title =} \KeywordTok{element_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-396-1.pdf}

Similarly, we may like to plot the distribution of the coefficients.\footnote{You can work out which coefficients to be pass to gather\_draws by using tidybayes::get\_variables(model). (In this example I passed `b\_.', but the ones of interest to you may be different.)}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{gather_draws}\NormalTok{(}\StringTok{`}\DataTypeTok{b_.*}\StringTok{`}\NormalTok{, }\DataTypeTok{regex=}\OtherTok{TRUE}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ungroup}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{coefficient =}\NormalTok{ stringr}\OperatorTok{::}\KeywordTok{str_replace_all}\NormalTok{(.variable, }\KeywordTok{c}\NormalTok{(}\StringTok{"b_"}\NormalTok{ =}\StringTok{ ""}\NormalTok{))) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{coefficient =}\NormalTok{ forcats}\OperatorTok{::}\KeywordTok{fct_recode}\NormalTok{(coefficient,}
                                           \DataTypeTok{Intercept =} \StringTok{"Intercept"}\NormalTok{,}
                                           \StringTok{`}\DataTypeTok{Is male}\StringTok{`}\NormalTok{ =}\StringTok{ "genderMale"}\NormalTok{,}
                                           \StringTok{`}\DataTypeTok{Age 30-44}\StringTok{`}\NormalTok{ =}\StringTok{ "age_groupages30to44"}\NormalTok{,}
                                           \StringTok{`}\DataTypeTok{Age 45-59}\StringTok{`}\NormalTok{ =}\StringTok{ "age_groupages45to59"}\NormalTok{,}
                                           \StringTok{`}\DataTypeTok{Age 60+}\StringTok{`}\NormalTok{ =}\StringTok{ "age_groupages60plus"}
\NormalTok{                                           )) }\OperatorTok{%>%}\StringTok{ }

\CommentTok{# both %>% }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y=}\KeywordTok{fct_rev}\NormalTok{(coefficient), }\DataTypeTok{x =}\NormalTok{ .value)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\NormalTok{ggridges}\OperatorTok{::}\KeywordTok{geom_density_ridges2}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{height =}\NormalTok{ ..density..),}
                                 \DataTypeTok{rel_min_height =} \FloatTok{0.01}\NormalTok{, }
                                 \DataTypeTok{stat =} \StringTok{"density"}\NormalTok{,}
                                 \DataTypeTok{scale=}\FloatTok{1.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Distribution of estimate"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Coefficient"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Dataset: "}\NormalTok{, }\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{panel.grid.major =} \KeywordTok{element_blank}\NormalTok{(),}
        \DataTypeTok{panel.grid.minor =} \KeywordTok{element_blank}\NormalTok{()) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"bottom"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-397-1.pdf}

\hypertarget{concluding-remarks}{%
\section{Concluding remarks}\label{concluding-remarks}}

In general, MRP is a good way to accomplish specific aims, but it's not without trade-offs. If you have a good quality survey, then it may be a way to speak to disaggregated aspects of it. Or if you are concerned about uncertainty then it is a good way to think about that. If you have a biased survey then it's a great place to start, but it's not a panacea.

There's not a lot of work that's been done using Canadian data, so there's plenty of scope for exciting work. I look forward to seeing what you do with it!

\hypertarget{text-as-data}{%
\chapter{Text as data}\label{text-as-data}}

\textbf{Required reading}

\begin{itemize}
\item
\end{itemize}

\textbf{Required viewing}

\begin{itemize}
\item
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\item
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\item
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\item
\end{itemize}

\textbf{Key functions/etc}

\begin{itemize}
\item
\end{itemize}

\textbf{Pre-quiz}

\begin{itemize}
\item
\end{itemize}

\hypertarget{introduction-26}{%
\section{Introduction}\label{introduction-26}}

TBD

\hypertarget{lasso-regression}{%
\section{Lasso regression}\label{lasso-regression}}

\textbf{This subsection, and much of the code that is used, directly draws on Julia Silge's notes, in particular: \url{https://juliasilge.com/blog/tidy-text-classification/} \citep{silge2018}.}

One of the nice aspects of text is that we can adapt our existing methods to use it as an input. Here we are going to use a variation of logistics regression, along with text inputs, to forecast. If you want to learn more about Lasso regression, then you should consider taking Arik's course over the summer, where he will dive into machine learning using Python.

In this section we are going to have two different text inputs, train a model on a sample of text from each of them, and then try to use that model to forecast the text in a training set. Although this is a arbitrary example, you could imagine many real-world applications. For instance, if you work at Twitter then you may want to know if a tweet was likely written by a bot, or by a human. Or similarly, imagine that you work for a political party - you may like to know if an email was likely from an email campaign organised by a group, or from an individual.

First we need to get some data. Julia Silge's example, nicely, uses book text as input. Seeing as I am jointly appointed at a Faculty of Information, that seems especially nice. The wonderful thing about this is that there is an R package - \texttt{gutenbergr} - that makes it easy to get text from Project Gutenberg into R. The key function is \texttt{gutenberg\_download()}, which needs a key for the book that you want. We'll consider Jane Eyre and Alice's Adventures in Wonderland, which have the keys of 1260 and 11, respectively.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(gutenbergr)}

\NormalTok{alice_and_jane <-}\StringTok{ }\KeywordTok{gutenberg_download}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1260}\NormalTok{, }\DecValTok{11}\NormalTok{), }\DataTypeTok{meta_fields =} \StringTok{"title"}\NormalTok{)}
\CommentTok{# Save the dataset so that we don't need to overwhelm the servers each time}
\KeywordTok{write_csv}\NormalTok{(alice_and_jane, }\StringTok{"inputs/books/alice_and_jane.csv"}\NormalTok{)}

\KeywordTok{head}\NormalTok{(alice_and_jane)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(gutenbergr)}

\NormalTok{alice_and_jane <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"inputs/books/alice_and_jane.csv"}\NormalTok{)}

\KeywordTok{head}\NormalTok{(alice_and_jane)}
\end{Highlighting}
\end{Shaded}

\label{tab:unnamed-chunk-399}

gutenberg\_id

text

title

11

ALICE'S ADVENTURES IN WONDERLAND

Alice's Adventures in Wonderland

11

Alice's Adventures in Wonderland

11

Lewis Carroll

Alice's Adventures in Wonderland

11

Alice's Adventures in Wonderland

11

THE MILLENNIUM FULCRUM EDITION 3.0

Alice's Adventures in Wonderland

11

Alice's Adventures in Wonderland

One of the great things about this is that the dataset is a tibble. So we can just work with all our familiar skills. The package has a lot more functionality, so I'd encourage you to look at the package's website: \url{https://github.com/ropensci/gutenbergr}. Each line of the book is read in as a different row in the dataset. Notice that we have downloaded two books here at once, and so we added the title. The two books are one after each other. You can see that we have both by looking at some summary statistics.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(alice_and_jane}\OperatorTok{$}\NormalTok{title)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Alice's Adventures in Wonderland      Jane Eyre: An Autobiography 
##                             3339                            20659
\end{verbatim}

So it looks like Jane Eyre is much longer than Alice in Wonderland, which isn't a surprise to those who have read them. I don't want to step into Digital Humanities too much, as I don't know anything about it, but looking at things like the broader context of when these books were written, or other books that were written at similar times, is likely a fascinating area.

We'll just get rid of blank lines

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(janitor)}
\CommentTok{# }\AlertTok{TODO}\CommentTok{ There's a way to do this within janitor, but I forget, need to look it up.}
\NormalTok{alice_and_jane <-}\StringTok{ }
\StringTok{  }\NormalTok{alice_and_jane }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{blank_line =} \KeywordTok{if_else}\NormalTok{(text }\OperatorTok{==}\StringTok{ ""}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(blank_line }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{blank_line)}

\KeywordTok{table}\NormalTok{(alice_and_jane}\OperatorTok{$}\NormalTok{title)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Alice's Adventures in Wonderland      Jane Eyre: An Autobiography 
##                             2481                            16395
\end{verbatim}

There's still an overwhelming amount of Jane Eyre in there. So we'll just sample from Jane Eyre to make it more equal.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{alice_and_jane}\OperatorTok{$}\NormalTok{rows <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(alice_and_jane))}
\NormalTok{sample_from_me <-}\StringTok{ }\NormalTok{alice_and_jane }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(title }\OperatorTok{==}\StringTok{ "Jane Eyre: An Autobiography"}\NormalTok{)}
\NormalTok{keep_me <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ sample_from_me}\OperatorTok{$}\NormalTok{rows, }\DataTypeTok{size =} \DecValTok{2481}\NormalTok{, }\DataTypeTok{replace =} \OtherTok{FALSE}\NormalTok{)}

\NormalTok{alice_and_jane <-}\StringTok{ }
\StringTok{  }\NormalTok{alice_and_jane }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(title }\OperatorTok{==}\StringTok{ "Alice's Adventures in Wonderland"} \OperatorTok{|}\StringTok{ }\NormalTok{rows }\OperatorTok{%in%}\StringTok{ }\NormalTok{keep_me) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{rows)}

\KeywordTok{table}\NormalTok{(alice_and_jane}\OperatorTok{$}\NormalTok{title)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Alice's Adventures in Wonderland      Jane Eyre: An Autobiography 
##                             2481                             2481
\end{verbatim}

There's a bunch of issues here, for instance, we have the whole of Alice, but we only have random bits of Jane, but nonetheless let's continue and we'll try to do something about that in a moment.

Now we want to get a sample of text from each book. We will use the lines to distinguish these samples. So we use a counter that will add a line number.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alice_and_jane <-}\StringTok{ }
\StringTok{  }\NormalTok{alice_and_jane }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(title) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{line_number =} \KeywordTok{paste}\NormalTok{(gutenberg_id, }\KeywordTok{row_number}\NormalTok{(), }\DataTypeTok{sep =} \StringTok{"_"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

We now want to sepearate out the words. We'll just use tidytext, because the focus here is on modelling, but there are a bunch of alternatives and one especially good one is the \texttt{quanteda} package, specifically, the \texttt{tokens()} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidytext)}
\NormalTok{alice_and_jane_by_word <-}\StringTok{ }
\StringTok{  }\NormalTok{alice_and_jane }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{unnest_tokens}\NormalTok{(word, text) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(word) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\KeywordTok{n}\NormalTok{() }\OperatorTok{>}\StringTok{ }\DecValTok{10}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Notice here that we removed any word that wasn't used more than 10 times. Nonetheless we still have a lot of unique words. (If we didn't require that the word be used by the author at least 10 times then we end up with more than 6,000 words.)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alice_and_jane_by_word}\OperatorTok{$}\NormalTok{word }\OperatorTok{%>%}\StringTok{ }\KeywordTok{unique}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{length}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 585
\end{verbatim}

The reason this is relevant is because these are our independent variables. So where you may be used to having something less than 10 explanatory variables, in this case we are going to have 585 As such, we need a model that can handle this.

However, as mentioned before, we are going to have some rows that essentially just had one word. While we could allow that, it might also be nice to give the model at least a few words to work with.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alice_and_jane_by_word <-}\StringTok{ }
\StringTok{  }\NormalTok{alice_and_jane_by_word }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(title, line_number) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{number_of_words_in_line =} \KeywordTok{n}\NormalTok{()) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ungroup}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(number_of_words_in_line }\OperatorTok{>}\StringTok{ }\DecValTok{2}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{number_of_words_in_line)}
\end{Highlighting}
\end{Shaded}

We'll create a test/training split, and load in \texttt{tidymodels}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidymodels)}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{alice_and_jane_by_word_split <-}\StringTok{ }
\StringTok{  }\NormalTok{alice_and_jane_by_word }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(title, line_number) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{distinct}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{initial_split}\NormalTok{(}\DataTypeTok{prop =} \DecValTok{3}\OperatorTok{/}\DecValTok{4}\NormalTok{, }\DataTypeTok{strata =}\NormalTok{ title)}

\CommentTok{# alice_and_jane_by_word_train <- training(alice_and_jane_by_word_split) %>% select(line_number)}
\CommentTok{# alice_and_jane_by_word_test <- testing(alice_and_jane_by_word_split)}
\CommentTok{# }
\CommentTok{# rm(alice_and_jane_by_word_split)}
\end{Highlighting}
\end{Shaded}

Now we need to create a document-term matrix.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alice_and_jane_dtm_training <-}\StringTok{ }
\StringTok{  }\NormalTok{alice_and_jane_by_word }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{(line_number, word) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{inner_join}\NormalTok{(}\KeywordTok{training}\NormalTok{(alice_and_jane_by_word_split) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(line_number)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{cast_dtm}\NormalTok{(}\DataTypeTok{term =}\NormalTok{ word, }\DataTypeTok{document =}\NormalTok{ line_number, }\DataTypeTok{value =}\NormalTok{ n)}

\KeywordTok{dim}\NormalTok{(alice_and_jane_dtm_training)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3415  585
\end{verbatim}

So we have our independent variables sorted, now we need our binary dependent variable, which is whether the book is Alice in Wonderland or Jane Eyre.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{response <-}\StringTok{ }
\StringTok{  }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{id =} \KeywordTok{dimnames}\NormalTok{(alice_and_jane_dtm_training)[[}\DecValTok{1}\NormalTok{]]) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{separate}\NormalTok{(id, }\DataTypeTok{into =} \KeywordTok{c}\NormalTok{(}\StringTok{"book"}\NormalTok{, }\StringTok{"line"}\NormalTok{, }\DataTypeTok{sep =} \StringTok{"_"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{is_alice =} \KeywordTok{if_else}\NormalTok{(book }\OperatorTok{==}\StringTok{ }\DecValTok{11}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)) }
  

\NormalTok{predictor <-}\StringTok{ }\NormalTok{alice_and_jane_dtm_training[] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Now we can run our model.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(glmnet)}

\NormalTok{model <-}\StringTok{ }\KeywordTok{cv.glmnet}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ predictor,}
                   \DataTypeTok{y =}\NormalTok{ response}\OperatorTok{$}\NormalTok{is_alice,}
                   \DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{,}
                   \DataTypeTok{keep =} \OtherTok{TRUE}
\NormalTok{                   )}

\KeywordTok{save}\NormalTok{(model, }\DataTypeTok{file =} \StringTok{"outputs/models/alice_vs_jane.rda"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{load}\NormalTok{(}\StringTok{"outputs/models/alice_vs_jane.rda"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(glmnet)}
\KeywordTok{library}\NormalTok{(broom)}

\NormalTok{coefs <-}\StringTok{ }\NormalTok{model}\OperatorTok{$}\NormalTok{glmnet.fit }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{tidy}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(lambda }\OperatorTok{==}\StringTok{ }\NormalTok{model}\OperatorTok{$}\NormalTok{lambda}\FloatTok{.1}\NormalTok{se)}

\NormalTok{coefs }\OperatorTok{%>%}\StringTok{ }\KeywordTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\label{tab:unnamed-chunk-411}

term

step

estimate

lambda

dev.ratio

(Intercept)

36

-0.335~~

0.00597

0.562

in

36

-0.144~~

0.00597

0.562

she

36

0.39~~~

0.00597

0.562

so

36

0.00249

0.00597

0.562

a

36

-0.117~~

0.00597

0.562

about

36

0.279~~

0.00597

0.562

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coefs }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(estimate }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{top_n}\NormalTok{(}\DecValTok{10}\NormalTok{, }\KeywordTok{abs}\NormalTok{(estimate)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ungroup}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\KeywordTok{fct_reorder}\NormalTok{(term, estimate), estimate, }\DataTypeTok{fill =}\NormalTok{ estimate }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.8}\NormalTok{, }\DataTypeTok{show.legend =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_minimal}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Coefficient"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Word"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_brewer}\NormalTok{(}\DataTypeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{telling_stories_with_data_files/figure-latex/unnamed-chunk-412-1.pdf}

Perhaps unsurprisingly, if you mention Alice then it's likely to be a Alice in Wonderland and if you mention Jane then it's likely to be Jane Eyre.

\hypertarget{topic-models}{%
\section{Topic models}\label{topic-models}}

\textbf{A version of these notes was previously circulated as part of \citet{Alexander2020}.}

\hypertarget{overview-1}{%
\subsection{Overview}\label{overview-1}}

Sometimes we have a statement and we want to know what it is about. Sometimes this will be easy, but we don't always have titles for statements, and even when we do, sometimes we do not have titles that define topics in a well-defined and consistent way. One way to get consistent estimates of the topics of each statement is to use topic models. While there are many variants, one way is to use the latent Dirichlet allocation (LDA) method of \citet{Blei2003latent}, as implemented by the R package `topicmodels' by \citet{Grun2011}.

The key assumption behind the LDA method is that each statement, `a document', is made by a person who decides the topics they would like to talk about in that document, and then chooses words, `terms', that are appropriate to those topics. A topic could be thought of as a collection of terms, and a document as a collection of topics. The topics are not specified \emph{ex ante}; they are an outcome of the method. Terms are not necessarily unique to a particular topic, and a document could be about more than one topic. This provides more flexibility than other approaches such as a strict word count method. The goal is to have the words found in documents group themselves to define topics.

\hypertarget{document-generation-process}{%
\subsection{Document generation process}\label{document-generation-process}}

The LDA method considers each statement to be a result of a process where a person first chooses the topics they want to speak about. After choosing the topics, the person then chooses appropriate words to use for each of those topics. More generally, the LDA topic model works by considering each document as having been generated by some probability distribution over topics. For instance, if there were five topics and two documents, then the first document may be comprised mostly of the first few topics; the other document may be mostly about the final few topics (Figure \ref{fig:topicsoverdocuments}).

\includegraphics{telling_stories_with_data_files/figure-latex/topicsoverdocuments-1.pdf} \includegraphics{telling_stories_with_data_files/figure-latex/topicsoverdocuments-2.pdf}

Similarly, each topic could be considered a probability distribution over terms. To choose the terms used in each document the speaker picks terms from each topic in the appropriate proportion. For instance, if there were ten terms, then one topic could be defined by giving more weight to terms related to immigration; and some other topic may give more weight to terms related to the economy (Figure \ref{fig:topicsoverterms}).

\includegraphics{telling_stories_with_data_files/figure-latex/topicsoverterms-1.pdf} \includegraphics{telling_stories_with_data_files/figure-latex/topicsoverterms-2.pdf}

Following \citet{BleiLafferty2009}, \citet{blei2012} and \citet{GriffithsSteyvers2004}, the process by which a document is generated is more formally considered to be:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  There are \(1, 2, \dots, k, \dots, K\) topics and the vocabulary consists of \(1, 2, \dots, V\) terms. For each topic, decide the terms that the topic uses by randomly drawing distributions over the terms. The distribution over the terms for the \(k\)th topic is \(\beta_k\). Typically a topic would be a small number of terms and so the Dirichlet distribution with hyperparameter \(0<\eta<1\) is used: \(\beta_k \sim \mbox{Dirichlet}(\eta)\).\footnote{The Dirichlet distribution is a variation of the beta distribution that is commonly used as a prior for categorical and multinomial variables. If there are just two categories, then the Dirichlet and the beta distributions are the same. In the special case of a symmetric Dirichlet distribution, \(\eta=1\), it is equivalent to a uniform distribution. If \(\eta<1\), then the distribution is sparse and concentrated on a smaller number of the values, and this number decreases as \(\eta\) decreases. A hyperparameter is a parameter of a prior distribution.} Strictly, \(\eta\) is actually a vector of hyperparameters, one for each \(K\), but in practice they all tend to be the same value.
\item
  Decide the topics that each document will cover by randomly drawing distributions over the \(K\) topics for each of the \(1, 2, \dots, d, \dots, D\) documents. The topic distributions for the \(d\)th document are \(\theta_d\), and \(\theta_{d,k}\) is the topic distribution for topic \(k\) in document \(d\). Again, the Dirichlet distribution with the hyperparameter \(0<\alpha<1\) is used here because usually a document would only cover a handful of topics: \(\theta_d \sim \mbox{Dirichlet}(\alpha)\). Again, strictly \(\alpha\) is vector of length \(K\) of hyperparameters, but in practice each is usually the same value.
\item
  If there are \(1, 2, \dots, n, \dots, N\) terms in the \(d\)th document, then to choose the \(n\)th term, \(w_{d, n}\):

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Randomly choose a topic for that term \(n\), in that document \(d\), \(z_{d,n}\), from the multinomial distribution over topics in that document, \(z_{d,n} \sim \mbox{Multinomial}(\theta_d)\).
  \item
    Randomly choose a term from the relevant multinomial distribution over the terms for that topic, \(w_{d,n} \sim \mbox{Multinomial}(\beta_{z_{d,n}})\).
  \end{enumerate}
\end{enumerate}

Given this set-up, the joint distribution for the variables is (\citet{blei2012}, p.6):
\[p(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}) = \prod^{K}_{i=1}p(\beta_i) \prod^{D}_{d=1}p(\theta_d) \left(\prod^N_{n=1}p(z_{d,n}|\theta_d)p\left(w_{d,n}|\beta_{1:K},z_{d,n}\right) \right).\]

Based on this document generation process the analysis problem, discussed in the next section, is to compute a posterior over \(\beta_{1:K}\) and \(\theta_{1:D}\), given \(w_{1:D, 1:N}\). This is intractable directly, but can be approximated (\citet{GriffithsSteyvers2004} and \citet{blei2012}).

\hypertarget{analysis-process}{%
\subsection{Analysis process}\label{analysis-process}}

After the documents are created, they are all that we have to analyse. The term usage in each document, \(w_{1:D, 1:N}\), is observed, but the topics are hidden, or `latent'. We do not know the topics of each document, nor how terms defined the topics. That is, we do not know the probability distributions of Figures \ref{fig:topicsoverdocuments} or \ref{fig:topicsoverterms}. In a sense we are trying to reverse the document generation process -- we have the terms and we would like to discover the topics.

If the earlier process around how the documents were generated is assumed and we observe the terms in each document, then we can obtain estimates of the topics (\citet{SteyversGriffiths2006}). The outcomes of the LDA process are probability distributions and these define the topics. Each term will be given a probability of being a member of a particular topic, and each document will be given a probability of being about a particular topic. That is, we are trying to calculate the posterior distribution of the topics given the terms observed in each document (\citet{blei2012}, p.7):
\[p(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N} | w_{1:D, 1:N}) = \frac{p\left(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}\right)}{p(w_{1:D, 1:N})}.\]

The initial practical step when implementing LDA given a corpus of documents is to remove `stop words'. These are words that are common, but that don't typically help to define topics. There is a general list of stop words such as: ``a''; ``a's''; ``able''; ``about''; ``above''\ldots{} We also remove punctuation and capitalisation. The documents need to then be transformed into a document-term-matrix. This is essentially a table with a column of the number of times each term appears in each document.

After the dataset is ready, the R package `topicmodels' by \citet{Grun2011} can be used to implement LDA and approximate the posterior. It does this using Gibbs sampling or the variational expectation-maximization algorithm. Following \citet{SteyversGriffiths2006} and \citet{Darling2011}, the Gibbs sampling process attempts to find a topic for a particular term in a particular document, given the topics of all other terms for all other documents. Broadly, it does this by first assigning every term in every document to a random topic, specified by Dirichlet priors with \(\alpha = \frac{50}{K}\) and \(\eta = 0.1\) (\citet{SteyversGriffiths2006} recommends \(\eta = 0.01\)), where \(\alpha\) refers to the distribution over topics and \(\eta\) refers to the distribution over terms (\citet{Grun2011}, p.7). It then selects a particular term in a particular document and assigns it to a new topic based on the conditional distribution where the topics for all other terms in all documents are taken as given (\citet{Grun2011}, p.6):
\[p(z_{d, n}=k | w_{1:D, 1:N}, z'_{d, n}) \propto \frac{\lambda'_{n\rightarrow k}+\eta}{\lambda'_{.\rightarrow k}+V\eta} \frac{\lambda'^{(d)}_{n\rightarrow k}+\alpha}{\lambda'^{(d)}_{-i}+K\alpha} \]
where \(z'_{d, n}\) refers to all other topic assignments; \(\lambda'_{n\rightarrow k}\) is a count of how many other times that term has been assigned to topic \(k\); \(\lambda'_{.\rightarrow k}\) is a count of how many other times that any term has been assigned to topic \(k\); \(\lambda'^{(d)}_{n\rightarrow k}\) is a count of how many other times that term has been assigned to topic \(k\) in that particular document; and \(\lambda'^{(d)}_{-i}\) is a count of how many other times that term has been assigned in that document. Once \(z_{d,n}\) has been estimated, then estimates for the distribution of words into topics and topics into documents can be backed out.

This conditional distribution assigns topics depending on how often a term has been assigned to that topic previously, and how common the topic is in that document (\citet{SteyversGriffiths2006}). The initial random allocation of topics means that the results of early passes through the corpus of document are poor, but given enough time the algorithm converges to an appropriate estimate.

\hypertarget{warnings-and-extensions}{%
\subsection{Warnings and extensions}\label{warnings-and-extensions}}

The choice of the number of topics, \emph{k}, affects the results, and must be specified \emph{a priori}. If there is a strong reason for a particular number, then this can be used. Otherwise, one way to choose an appropriate number is to use a test and training set process. Essentially, this means running the process on a variety of possible values for \emph{k} and then picking an appropriate value that performs well.

One weakness of the LDA method is that it considers a `bag of words' where the order of those words does not matter (\citet{blei2012}). It is possible to extend the model to reduce the impact of the bag-of-words assumption and add conditionality to word order. Additionally, alternatives to the Dirichlet distribution can be used to extend the model to allow for correlation. For instance, in Hansard topics related the army may be expected to be more commonly found with topics related to the navy, but less commonly with topics related to banking.

\hypertarget{word-embedding}{%
\section{Word embedding}\label{word-embedding}}

\hypertarget{conclusion-1}{%
\section{Conclusion}\label{conclusion-1}}

Using text as data is exciting because of the quantity and variety of text that is available to us. In general, dealing with text datasets is messy. There is a lot of cleaning and preparation that is typically required. Often text datasets are large. As such, having a workflow in place, in which you work in a reproducible way, simulating data first, and then clearly communicating your findings becomes critical, if only to keep everything organised in your own mind. Nonetheless, it is an exciting area, and I encourage you to regularly use text analysis where possible.

In terms of next steps there are two, related, concerns: data and analysis.

In terms of data there are many places to get large amounts of text data relatively easily, including:

\begin{itemize}
\tightlist
\item
  The r package \texttt{rtweets} makes it easy to get Twitter data (although typically this is going to be looking forward from when you start using it, rather than being able to look back). Plenty of people at U of T work with Twitter data including Jia Xue in the iSchool, and Ludovic Rheault in political science.
\item
  The inside Airbnb dataset that we used earlier provides text from reviews.
\item
  We've seen the \texttt{gutenbergr} package already in these notes, which provides easy access to text from Project Gutenberg.
\item
  We've seen scraping of Wikipedia, but if you are going to do a bit of this then you may find it better to use a package, for instance \texttt{WikipediR}.
\end{itemize}

In terms of analysis:

\begin{itemize}
\tightlist
\item
  Start by going through the tidytext book, \texttt{tidytext}, as it has a lot of nice explanations, code, and examples.
\item
  It would then be worthwhile working through the Quanteda package \texttt{quanteda} tutorials.
\item
  Finally, consider packages such as \texttt{text2vec}, and \texttt{spacyr}.
\end{itemize}

\hypertarget{part-scale}{%
\part{Scale}\label{part-scale}}

\hypertarget{cloud}{%
\chapter{Cloud}\label{cloud}}

\textbf{Required reading}

\begin{itemize}
\item
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\tightlist
\item
  Edmondson, Mark, 2020, `googleComputeEngineR documentation', version 0.3.0.9000, freely available at: \url{https://cloudyr.github.io/googleComputeEngineR/}.
\item
  McDermott, Grant R., 2020, `Cloud computing with Google Compute Engine', \emph{Data Science for Economists}, freely available at: \url{https://raw.githack.com/uo-ec607/lectures/master/14-gce/14-gce.html}.
\item
  Morris, Mitzi, 2020, `Stan Notebooks in the Cloud', freely available at: \url{https://mc-stan.org/users/documentation/case-studies/jupyter_colab_notebooks_2020.html}.
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\tightlist
\item
  Benefits/costs of cloud.
\item
  Getting started in the cloud.
\item
  Starting virtual machines with R Studio.
\item
  Stopping virtual machines.
\end{itemize}

\hypertarget{introduction-27}{%
\section{Introduction}\label{introduction-27}}

Cloud benefits:
- Costs can be reduced, or more easily amortized.
- Can scale as you need.
- Many platforms are already sorted out e.g.~R Studio just works.

I stole this from someone and I can't remember who, but the cloud is another name for `someone else's computer'. That's it. Nonetheless, learning to use someone else's computer can be great for a number of reasons including:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Scalability: It can be quite expensive to buy a new computer, especially if you only need it to run something every now and then, but by using someone else's computer, you can just rent for a few hours or days.
\item
  Portability: If you can shift your analysis workflow from your laptop to the cloud, then that suggests that you are likely doing good things in terms of reproducibility and portability. At the very least, your code is capable of running on your laptop and the cloud.
\item
  Set-and-forget: If you are doing something that will take a while, then it can be great to not have to worry about your laptop's fan running overnight, or your partner/baby/pet/housemate/etc accidently closing your computer, or not being able to watch Netflix on that same computer.
\end{enumerate}

When you use the cloud you are running your code on a `virtual machine'. This is a part of a larger bunch of computers that has been designed to act like a computer with specific features. For instance you may specify that your virtual machine has 8 GB RAM, 128 storage, and 4 CPUs. Your VM would then act like a computer with those specifications. The cost to use cloud options increases based on the specifications of the virtual machine that you choose.

There are a few downsides:

\begin{itemize}
\tightlist
\item
  Cost: While most cloud options are cheap, they are rarely free. (While there are free options, they tend to not be very powerful, and so you end up having to pay to get a computer that is better than your laptop.) To give you an idea of cost, when I use AWS, I typically end up spending five to ten dollars for a couple of days. So it's fairly cheap, but it's not nothing. It's also pretty easy to accidently forget about something and run up an unexpected bill, especially initially.
\item
  Public: It is pretty easy to make mistakes and accidently make everything public.
\item
  Time: It takes time to get set-up and comfortable on the cloud.
\end{itemize}

In these notes we are going to introduce the cloud starting with some options that pretty much anyone can (and should) take advantage of: Google Colab; and then moving to more general cloud options including Google Compute Engine, AWS, and Azure, which may be useful to some of you in some cases. If you want to get a job in industry, then the advice of pretty much every speaker from industry at the Toronto Data Workshop is that you learn at least one of those cloud options. For instance, Munich Re is an Azure shop, Receptiviti uses AWS, etc.

\hypertarget{google-colab}{%
\section{Google Colab}\label{google-colab}}

Google Colab is similar to R Studio Cloud, in that it is set-up to allow you to just log in and get started. In this case, you need a Google account. It's better than R Studio because they have more resources to put into its development and you can use GPUs, but but on the other hand it is designed for Python, and while we can use it for R, it's not really focused on that.

To get started you need to tell Google Colab that you want to use R. You can do this by using this: \url{https://colab.research.google.com/notebook\#create=true\&language=r}.

At this point you have a Jupyter notebook open that will run R. (But it is not a R Markdown document.) You can install packages as normal, e.g.~\texttt{install.packages("tidyverse")}, and then call the package e.g.~\texttt{library(tidyverse)}.

Google Colab is a good option if you have a good reason for using the broader capabilities that it has. If you want to go deeper into that then the Morris reading has a bunch of options that you can explore, but as Morris puts it `Colab is a gateway drug - for large-scale processing pipelines you'll need to move up to Google Cloud Platform or one of its competitors AWS, Azure, etc.' and that is what we will do now.

\hypertarget{aws}{%
\section{AWS}\label{aws}}

Amazon Web Services is a cloud service from Amazon. To get started you need an AWS Developer account which you can create here: \url{https://aws.amazon.com/developer/}.

After you have created an account, you need to select a region where the computer that you will access is located. After this, you will want to ``Launch a virtual machine'' (with EC2).

The first step is to choose an Amazon Machine Image (AMI). This provides the details of the computer that you will be using. For instance, your local computer may be a MacBook running Catalina. Helpfully, Louis Aslett provides a bunch of these already set up - \url{http://www.louisaslett.com/RStudio_AMI/}. You can either select the code for the region that you registered for, or you can click on the link. The benefit of this AMI is that they are set-up specifically for R Studio, however the trade-off is that they are a little out-dated, as they were compiled in May 2019.

In the next step you can choose how powerful the computer will be. The free tier has a fairly basic computer, but you can choose better ones when you need them. At this point you can pretty much just launch the instance. If you start using AWS more seriously then you should look into different security settings.

Your instance is now running. You can go to it by pasting the `public DNS' into a browser. The username is `rstudio' and the password is your instance ID.

You should have R Studio running, which is exciting. The first thing to do is probably to change the default password using the instructions in the instance.

You don't need to install, say, the tidyverse, instead you can just call the library and keep going. You can see the list of packages that are installed with \texttt{installed.packages()}. For instance, \texttt{rstan} is already installed. And you can use GPUs if you want.

Perhaps as important as being able to start an AWS instance is being able to stop it (so that you don't get billed). The free tier is pretty great, but you do need to turn it off. To stop an instance, in the AWS instances page, select it, then `Actions -\textgreater{} Instance State -\textgreater{} Terminate'.

\hypertarget{google-compute-engine}{%
\section{Google Compute Engine}\label{google-compute-engine}}

The main R package related to Google Compute Engine seems to be: \texttt{googleComputeEngineR}.

The reading from Grant McDermott is a pretty good walk-through.

\hypertarget{azure}{%
\section{Azure}\label{azure}}

There are a bunch of R packages related to Azure here: \url{https://github.com/Azure/AzureR}.

\hypertarget{deploy}{%
\chapter{Deploy}\label{deploy}}

\textbf{Required reading}

\begin{itemize}
\item
\end{itemize}

\textbf{Recommended reading}

\begin{itemize}
\item
\end{itemize}

\textbf{Key concepts/skills/etc}

\begin{itemize}
\item
\end{itemize}

\hypertarget{introduction-28}{%
\section{Introduction}\label{introduction-28}}

\hypertarget{part-assessment}{%
\part{Assessment}\label{part-assessment}}

\hypertarget{papers}{%
\chapter{Papers}\label{papers}}

\hypertarget{mandatory-minimums}{%
\section{`Mandatory Minimums'}\label{mandatory-minimums}}

\hypertarget{task}{%
\subsection{Task}\label{task}}

\begin{itemize}
\tightlist
\item
  Working individually and in an entirely reproducible way, please find a dataset of interest on Open Data Toronto and write a short paper telling a story about the data.
\end{itemize}

\hypertarget{guidance}{%
\subsection{Guidance}\label{guidance}}

\begin{itemize}
\tightlist
\item
  Find a dataset of interest on \href{https://open.toronto.ca}{Open Data Toronto} and download it in a reproducible way using the R package \texttt{opendatatoronto} \citep{citeSharla}.
\item
  Create a folder with appropriate sub-folders, add it to GitHub, and then prepare a PDF using \texttt{R\ Markdown} with these sections (you are welcome to use this starter folder: \url{https://github.com/RohanAlexander/starter_folder}):

  \begin{itemize}
  \tightlist
  \item
    title,
  \item
    author,
  \item
    date,
  \item
    abstract,
  \item
    introduction,
  \item
    data, and
  \item
    references.
  \end{itemize}
\item
  In the data section thoroughly and precisely discuss the source of the data and the bias this brings (ethical, statistical, and otherwise). Comprehensively describe and summarize the data using text and at least one graph and one table. Graphs must be made in \texttt{ggplot} \citep{citeggplot} and tables must be made using \texttt{knitr::kable()} (with or without \texttt{kableExtra}) or \texttt{gt} \citep{citeGT}. Make sure to cross-reference graphs and tables.
\item
  Use \texttt{bibtex} to add references. Be sure to reference R and any R packages you use, as well as the dataset. Check that you have referenced everything. Strong submissions will draw on related literature and would be sure to also reference those. There are various options in R Markdown for references style; just pick one that you are used to.
\item
  Go back and write an introduction. This should be two or three paragraphs. The last paragraph should set out the remainder of the paper.
\item
  Add an abstract. This should be three or four sentences. And then add a descriptive title (Hint: `Paper 1' is not descriptive.)
\item
  Add a link to your GitHub repo via a footnote.
\item
  Check that your GitHub repo is well-organized, and add an informative README. (Hint: Comment. Your. Code.). Make sure that you've got at least one R script in there, in addition, to your R Markdown file.
\item
  Pull this all together as a PDF and check that the paper is well-written and able to be understood by the average reader of, say, FiveThirtyEight. This means that you are allowed to use mathematical notation, but you must explain all of it in plain language. All statistical concepts and terminology must be explained. Your reader is someone with a university education, but not necessarily someone who understands what a p-value is - explain everything that you use.
\item
  Check there is no evidence that this is a class assignment.
\item
  Via Quercus, submit the PDF.
\end{itemize}

\hypertarget{check-offs-points}{%
\subsection{Check offs points}\label{check-offs-points}}

\begin{itemize}
\tightlist
\item
  Check you've not included any R code or raw R output in the final PDF.
\item
  Check that although you'll probably have most of your code in the R Markdown, make sure that you have at least one R script in the \texttt{scripts} folder.
\item
  Check there is thoroughly commented code that directly creates your PDF. Do not knit to html and then save as a PDF. Do not knit to Word and then save as a PDF
\item
  Check that your graph and discussion are extremely clear, and of comparable quality to those of FiveThirtyEight.
\item
  Check that the date is updated.
\item
  Check your entire workflow is entirely reproducible.
\item
  Check for typos.
\end{itemize}

\hypertarget{faq}{%
\subsection{FAQ}\label{faq}}

\begin{itemize}
\tightlist
\item
  Can I use a dataset from Kaggle instead? No, because too many people use Kaggle datasets so employers are sick of them.
\item
  I can't use code to download my dataset, can I just manually download it? No, because your entire workflow needs to be reproducible. Please fix the download problem or pick a different dataset.
\item
  How much should I write? Most students submit something in the two-to-six-page range, but it's really up to you. Be precise and thorough.
\item
  My data is about apartment blocks/NBA/League of Legends so there's no ethical or bias aspect, what do I do? Please re-read the readings to better understand bias and ethics. If you really can't think of something, then it might be worth picking a different dataset.
\item
  Can I use Python? No.~If you already know Python then it doesn't hurt to learn another language.
\item
  Why do I need to cite R, when I don't need to cite Word? R is a free statistical programming language with academic origins so it's appropriate to acknowledge the work of others. It's also important for reproducibility.
\end{itemize}

\hypertarget{these-numbers-mean-dial-it-up}{%
\section{`These numbers mean dial it up'}\label{these-numbers-mean-dial-it-up}}

\hypertarget{task-1}{%
\subsection{Task}\label{task-1}}

Please consider this scenario:

\begin{itemize}
\tightlist
\item
  `You are employed as a junior data scientist at Petit Poll - a Canadian polling company. Petit Poll has a contract with a 'client' - an Ontario government department - to provide them with advice. In particular, the client wants to understand the effect of COVID shut-downs on restaurant businesses and has asked Petit Poll to design an experiment where some restaurants are shutdown.'
\item
  Working as part of a small team of 1-3 people, and in an entirely reproducible way, please decide on an intervention, and some measurement strategies, and then write a short paper telling a story about the effect of shut-downs on restaurants.
\end{itemize}

\hypertarget{guidance-1}{%
\subsection{Guidance}\label{guidance-1}}

\begin{itemize}
\tightlist
\item
  Working as part of a team of 1-3 people, prepare a PDF in R Markdown with the following features (you are welcome to use this starter folder: \url{https://github.com/RohanAlexander/starter_folder}):

  \begin{itemize}
  \tightlist
  \item
    title,
  \item
    author/s,
  \item
    date,
  \item
    abstract,
  \item
    introduction,
  \item
    data,
  \item
    discussion, and
  \item
    references.
  \end{itemize}
\item
  In the data section you should specify the intervention and data gathering methodology,
\item
  In the discussion section and any other relevant section, please be sure to discuss ethics and bias with reference to relevant literature.
\item
  Decide on an intervention. Some aspects to address include:

  \begin{itemize}
  \tightlist
  \item
    How will it be designed and implemented?
  \item
    What will be random about it?
  \item
    How will you ensure the separation of treatment and non-treatment?
  \item
    How long will it run?
  \end{itemize}
\item
  Decide on a survey methodology. Some aspects to address include:

  \begin{itemize}
  \tightlist
  \item
    What is the population, frame, and sample?
  \item
    What sampling methods will you use and why? What are some of the statistical properties that the method brings to the table?
  \item
    How are you going to reach your desired respondents?
  \item
    How much do you estimate this will cost?
  \item
    What steps will you take to deal with non-response and how will non-response affect your survey?
  \item
    How are you going to protect respondent privacy?
  \end{itemize}
\item
  Remember to consider all of this in the context of your `client' - for instance, what are they interested in?
\item
  Develop a survey on a platform that was introduced in class. Be sure to test it yourselves. You will want to test this as much as possible, maybe even swap informally with another group?
\item
  Now release the surveys into the (simulated) `field'. Please do this by simulating an appropriate number of responses to your survey in R. Don't forget to simulate in relation to the intervention that you proposed. Do you need two, or even more, surveys? Show the results and discuss your `findings'. Everything must be entirely reproducible.
\item
  You may wish to scrape some data and/or use open data sources to appropriately parameterize your simulations. Don't forget to cite them when you do this.
\item
  Use R Markdown to write a PDF report about all of this. Discuss your intervention, results and findings, your survey design and motivations, etc - all of it. You are writing a report that will eventually go to the client, so you must set the scene, and use language that demonstrates your command of statistical concepts but brings the reader along with you. Be sure to include graphs and tables and reference them in your discussion. Be sure to be clear about weaknesses and biases, and opportunities for future work.
\item
  Your report must be well written. You are allowed to, and should, use mathematical notation, but you must explain all of it in plain language. Similarly, you can, and should, use experimental/survey/sampling/observational data terminology, but again, you need to explain it.
\item
  Your graphs and tables must be of an incredibly high standard. Graphs and tables should be well formatted and report-ready. They should be clean and digestible. Furthermore, you should label and describe each table/figure.
\item
  Your client has stats graduates working for it who need to be impressed by the main content of the report, but also has people who barely know what an average is and these people need to be impressed also.
\item
  Your graphs must be of an extremely high standard.
\item
  Check that you have referenced everything, including R, R packages, and datasets. Strong submissions will draw on related literature and would be sure to also reference those. The style of references does not matter, provided it is consistent.
\item
  Via Quercus, submit your PDF report. You must provide a link to the GitHub repo where the code that you used for this assignment lives (hint: Comment. Your. Code.). Your entire workflow must be entirely reproducible. Your repo should be clearly organised and a useful README included. And you must include the R Markdown file that produced the PDF in that repo.
\item
  Please be sure to include a link to your survey/s in your report and screenshots of the survey/s in the appendix of your report.
\item
  Everyone in the team receives the same mark.
\item
  There should be no evidence that this is a class assignment.
\end{itemize}

\hypertarget{check-offs-points-1}{%
\subsection{Check offs points}\label{check-offs-points-1}}

\hypertarget{faq-1}{%
\subsection{FAQ}\label{faq-1}}

\begin{itemize}
\tightlist
\item
  Can I work by myself? Yes. But I recommend forming a group and the workload for the course assumes you'll work on the second and third paper as part of a group of four.
\item
  Can we switch groups for the third paper? Yes.
\item
  How can I find a group? I will randomly create groups of four in Quercus. You are welcome to shift out of those groups and form your own groups if you'd like.
\item
  Can I get a different mark to the rest of my group? No.~Everyone in the group gets the same mark.
\item
  I wrote my paper by myself, so can I be graded on a different scale? No.~All papers are graded in the same way.
\item
  How much should I write? Most students submit something in the 10-to-15-page range, but it's really up to you. Be precise and thorough.
\end{itemize}

\hypertarget{the-short-list}{%
\section{`The Short List'}\label{the-short-list}}

\hypertarget{task-2}{%
\subsection{Task}\label{task-2}}

\begin{itemize}
\tightlist
\item
  Working as part of a small team of 1-3 people, and in an entirely reproducible way, please pick a paper to reproduce from an approved list and then write a short paper telling a story based on this. Your story should both talk about the (reproduced) findings, but also (a bit more `meta') about what you learnt from the process.
\end{itemize}

\hypertarget{guidance-2}{%
\subsection{Guidance}\label{guidance-2}}

\begin{itemize}
\tightlist
\item
  Working as part of a team of 1-3 people, prepare a PDF in R Markdown with the following features:

  \begin{itemize}
  \tightlist
  \item
    title,
  \item
    author/s,
  \item
    date,
  \item
    abstract,
  \item
    introduction,
  \item
    data,
  \item
    model,
  \item
    results,
  \item
    discussion, and
  \item
    references.
  \end{itemize}
\item
  In the discussion section and any other relevant section, please be sure to discuss ethics and bias with reference to relevant literature.
\item
  You should reproduce one of the following papers:

  \begin{itemize}
  \tightlist
  \item
    Liran Einav, Amy Finkelstein, Tamar Oostrom, Abigail Ostriker, Heidi Williams, 2020, `Screening and Selection: The Case of Mammograms', \emph{American Economic Review}.
  \item
    Pons, Vincent, 2018, `Will a Five-Minute Discussion Change Your Mind? A Countrywide Experiment on Voter Choice in France' \emph{American Economic Review}.
  \item
    Barari, Soubhik, Christopher Lucas, and Kevin Munger, 2021, `Political Deepfake Videos Misinform the Public, But No More than Other Fake Media', 13 January, \url{https://osf.io/cdfh3/}.
  \item
    \textbf{Others TBD}
  \item
    If you have a favourite paper and want to reproduce it, then please submit it to me for consideration before Reading Week.
  \end{itemize}
\item
  You should follow the lead of the author/s of the paper you're reproducing, but thoroughly think about, and discuss, what is being done. Regardless of the particular model that you are using, and the (possibly lack of) extent to which this is done in the paper, your model must be well explained, thoroughly justified, explained as appropriate to the task at hand, and the results must be beautifully described.
\item
  You must include a DAG (probably in the model section).
\item
  You must have a discussion of power and experimental design (probably in the data section)
\item
  Your paper must be well-written, draw on relevant literature, and show your statistical skills by explaining all statistical concepts that you draw on.
\item
  You are welcome to use appendices for supporting, but not critical, material. Your discussion must include sub-sections that focus on three or four interesting points, and also sub-sections on weaknesses and next steps.
\item
  In your report you must provide a link to a GitHub repo that fully contains your analysis. Your code must be entirely reproducible, documented, and readable. Your repo must be well-organised and appropriately use folders.
\item
  Your graphs and tables must be of an incredibly high standard. Graphs and tables should be well formatted and report-ready. They should be clean and digestible. Furthermore, you should label and describe each table/figure.
\item
  When you discuss the dataset (in the data section) you should make sure to discuss (at least):

  \begin{itemize}
  \tightlist
  \item
    Its key features, strengths, and weaknesses generally.
  \item
    A discussion of the questionnaire - what is good and bad about it?
  \item
    A discussion of the methodology including how they find people to take the survey; what their population, frame, and sample were; what sampling approach they took and what some of the trade-offs may be; what they do about non-response; the cost.
  \item
    A discussion of the intervention and experimental design.
  \item
    These are just some of the issues strong submissions will consider. Show off your knowledge. If this becomes too detailed then you should push some of this to footnotes or an appendix.
  \end{itemize}
\item
  When you discuss your model (in the model section), you must be extremely careful to spell out the statistical model that you are using, defining and explaining each aspect and why it is important. (For a Bayesian model, a discussion of priors and regularization is almost always important.) You should mention the software that you used to run the model. You should be clear about model convergence, model checks, and diagnostic issues. How do the sampling and survey aspects that you discussed assert themselves in the modelling decisions that you make? Again, if it becomes too detailed then push some of the details to footnotes or an appendix. You have the original paper to guide you, but you'll likely need to go well-beyond what is included.
\item
  You should present model results, graphs, figures, etc, in the results section. This section should strictly relay results. Interpretation of these results and conclusions drawn from the results should be left for the discussion section.
\item
  Your discussion should focus on your model results. Interpret them and explain what they mean. Put them in context. What do we learn about the world having understood your model and its results? What caveats could apply? To what extent does your model represent the small world and the large world (to use the language of McElreath, Ch 2)? What are some weaknesses and opportunities for future work? Additionally, as this is a reproduction you should include a sub-section on differences you found and difficulties that you had.
\item
  Check that you have referenced everything. Strong submissions will draw on related literature in the discussion (and other sections) and would be sure to also reference those. The style of references does not matter, provided it is consistent.
\item
  As a team, via Quercus, submit a PDF of your paper. Again, in your paper you must have a link to the associated GitHub repo. And you must include the R Markdown file that produced the PDF in that repo. And you must include the R Markdown file that produced the PDF in that repo. The repo must be well-organized and have a detailed README.
\item
  A good way to work as a team would be to split up the work, so that one person is doing each section. The people doing the sections that rely on data (such as the analysis and the graphs) could just simulate it while they are waiting for the person putting together the data to finish.
\item
  It is expected that your submission be well written and able to be understood by the average reader of say 538. This means that you are allowed to use mathematical notation, but you must be able to explain it all in plain English. Similarly, you can (and hint: you should) use survey, sampling, observational, and statistical terminology, but again you need to explain it. Your work should have flow and should be easy to follow and understand. To communicate well, anyone at the university level should be able to read your report once and relay back the methodology, overall results, findings, weaknesses and next steps without confusion.
\item
  Everyone in the team receives the same mark.
\item
  There should be no evidence that this is a class assignment.
\end{itemize}

\hypertarget{check-offs-points-2}{%
\subsection{Check offs points}\label{check-offs-points-2}}

\hypertarget{faq-2}{%
\subsection{FAQ}\label{faq-2}}

\begin{itemize}
\tightlist
\item
  Do I have to stay in the same group as the second paper? No.~You're welcome to change. However, it's important that you don't change the second paper group on Quercus - be sure to only change the third paper group.
\item
  Can we switch groups for the third paper? Yes.
\item
  How much should I write? Most students submit something in the 10-to-15-page range, but it's really up to you. Be precise and thorough.
\item
  My paper doesn't have a DAG, what do I do? You need to make the DAG.
\end{itemize}

\hypertarget{two-cathedrals}{%
\section{`Two Cathedrals'}\label{two-cathedrals}}

\hypertarget{task-3}{%
\subsection{Task}\label{task-3}}

\begin{itemize}
\tightlist
\item
  Working individually, please conduct original research that applies methods from statistics to a question that involves an experiment.
\end{itemize}

\hypertarget{guidance-3}{%
\subsection{Guidance}\label{guidance-3}}

You have various options for topics (pick one):
- Develop a research question that is of interest to you and obtain or create a relevant dataset. This option involves developing your own research question based on your own interests, background, and expertise. I encourage you to take this option, but please discuss your plans with me. How does one come up with ideas? One way is to be question-driven, where you keep an informal log of small ideas, questions, and puzzles, that you have as you're reading and working. Often, after dwelling on it for a while you can manage to find some questions of interest. Another way is to be data-driven - try to find some interesting dataset and then work backward. Finally, yet another way, is to be methods-driven - let's say that you happen to understand Gaussian processes, then just apply that expertise.
- Others \textbf{TBA}

\begin{itemize}
\tightlist
\item
  You should know the expectations by now. If you need a refresher then review the past problem sets. But essentially:

  \begin{itemize}
  \tightlist
  \item
    Everything is entirely reproducible.
  \item
    Your paper must be written in R Markdown.
  \item
    Your paper must have the following sections:

    \begin{itemize}
    \tightlist
    \item
      Title, date, author, keywords, abstract, introduction, data, model, results, discussion, appendix (optional, for supporting, but not critical, material), and a reference list.
    \end{itemize}
  \item
    Your paper must be well-written, draw on relevant literature, and show your statistical skills by explaining all statistical concepts that you draw on.
  \item
    The discussion needs to be substantial. For instance, if the paper were 10 pages long then a discussion should be at least 2.5 pages. In the discussion, the paper must include subsections on weaknesses and next steps - but these must be in proportion.
  \item
    The report must provide a link to a GitHub repo that contains everything (apart from any raw data that you git ignored if it is not yours to share). The code must be entirely reproducible, documented, and readable. The repo must be well-organised and appropriately use folders and README files.
  \end{itemize}
\end{itemize}

\hypertarget{peer-review-submission}{%
\subsection{Peer review submission}\label{peer-review-submission}}

\begin{itemize}
\tightlist
\item
  My expectations for this paper are very high. I'm very excited to read what you submit. To help you achieve this standard, there is an initial `submission' where you can get comments and feedback and then the final, actual, submission.
\item
  Submit initial materials for peer-review.

  \begin{itemize}
  \tightlist
  \item
    As an individual, via Quercus, submit a PDF of your rough draft on Quercus.
  \item
    At a minimum this must include:
  \item
    All top-matter (title, author (you can use a pseudonym if you want), date, keywords, abstract) completely filled out.
  \item
    A fully written Introduction section.
  \end{itemize}
\item
  All other sections must be present in your paper, but don't have to be filled out (e.g.~you must have a `Data' heading, but you don't need to have content in that section).
\item
  To be clear - it is fine to later change any aspect of what you submit at this check-point.
\item
  You will be awarded two percentage points just for submitting a draft that meets this minimum.
\item
  The point of this is to get feedback on your work (and to make sure you have at least started thinking about this project) so you are more than welcome to include other sections that you wish to get feedback on.
\item
  There will be no extensions granted for this submission since the following submission is dependent on this date.
\end{itemize}

\hypertarget{conduct-peer-review}{%
\subsection{Conduct peer-review}\label{conduct-peer-review}}

\begin{itemize}
\tightlist
\item
  As an individual, you will randomly be assigned a handful of rough drafts to provide feedback. You have three days to provide feedback to your peers.
\item
  If you provide feedback to one peer you will receive one percentage point, if you provide feedback to two peers you will receive two percentage points, if you provide feedback to three (or more) peers you will receive the full three percentage points.
\item
  Your feedback must include at least five comments (meaningful/useful bullet points). These must be well-written and thoughtful.
\item
  There will be no extensions granted for this submission since the following submission is dependent on this date.
\item
  Please remember that you are providing feedback here to help your colleagues. All comments should be professional and kind. It is challenging to receive criticism. Please remember that your goal here is to help your peers advance their writing/analysis. Any feedback that is inappropriate or not up to standard will receive a 0 and cannot be redeemed later.
\end{itemize}

\hypertarget{check-offs-points-3}{%
\subsection{Check offs points}\label{check-offs-points-3}}

\hypertarget{faq-3}{%
\subsection{FAQ}\label{faq-3}}

\begin{itemize}
\tightlist
\item
  Can I work as part of a team? No.~It's important that you have some work that is entirely your own. You really need your own work to show off for job applications etc.
\item
  How much should I write? Most students submit something in the 10-to-15-page range, but it's really up to you. Be precise and thorough.
\end{itemize}

\hypertarget{a-proportional-response}{%
\section{`A Proportional Response'}\label{a-proportional-response}}

\hypertarget{task-4}{%
\subsection{Task}\label{task-4}}

Working in teams of one to four people, please consider this scenario:

\begin{itemize}
\tightlist
\item
  `You are employed as a junior statistician at Petit Poll - a Canadian polling company. Petit Poll has a contract with a Canadian political party to provide them with monthly polling updates.'
\item
  Working as part of a small team of 1-4 people, and in an entirely reproducible way, please write a short paper that tells the client a story about their standing.
\end{itemize}

\hypertarget{recommended-steps}{%
\subsection{Recommended steps}\label{recommended-steps}}

\begin{itemize}
\tightlist
\item
  Please pick a political party that you are `working for', and pick a geographic focus: 1) the overall election, 2) a particular province, or 3) a specific riding.
\item
  Then decide on a survey methodology (hint: p.~13 of Wu \& Thompson provides a handy checklist). Some questions you should address include:
\item
  What is the population, frame, and sample?
\item
  What sampling methods will you use and why (e.g.~you could choose SRSWOR, stratified, etc). What are some of the statistical properties that the method brings to the table (e.g.~for SRSWOR you could discuss Wu \& Thompson, Theorem 2.2, etc, as appropriate)?
\item
  How are you going to reach your desired respondents?
\item
  How much do you estimate this will cost?
\item
  What steps will you take to deal with non-response and how will non-response affect your survey?
\item
  How are you going to protect respondent privacy?
\item
  Remember to consider all of this in the context of your `client' - for instance, who would be more interested in Alberta ridings: Bloc Québécois or the Conservatives? Who likely has more money to spend - the Liberals or the Greens?
\item
  Develop a survey on a platform that was introduced in class. Be sure to test it yourselves. You will want to test this as much as possible, maybe even swap informally with another group?
\item
  Now release the surveys into the (simulated) `field'. Please do this by simulating an appropriate number of responses to your survey in R. Don't forget to simulate in relation to the survey methodology that you proposed. Show the results and discuss your `findings'. Everything must be entirely reproducible. You may like to consider linking your survey `responses' with other data such as the census or GSS.
\item
  Use R Markdown to write a PDF report about all of this. Discuss your results and findings, your survey design and motivations, etc - all of it. You are writing a report that will eventually go to the `client', so you must set the scene, and use language that demonstrates your command of statistical concepts but brings the reader along with you. Be sure to include graphs and tables and reference them in your discussion. Be sure to be clear about weaknesses and biases, and opportunities for future work.
\item
  Your report must be well written. You are allowed to, and should, use mathematical notation, but you must explain all of it in plain english. Similarly, you can, and should, use surveys/sampling/observational data terminology, but again, you need to explain it.
\item
  Your report must include at least the following aspects: title, date, authorship, non-technical executive summary, introduction, survey methodology, results, discussion, appendices that detail the survey, and references. Your `client' has stats graduates working for it who need to be impressed by the main content of the report, but also has people who barely know what an average is and these people need to be impressed also. This is why your report should include a non-technical executive summary. In terms of length, this would typically be roughly 10 per cent of the report. It would be more detailed than an introduction, but still at a high level.
\item
  Your graphs must be of an extremely high standard.
\item
  Check that you have referenced everything. Strong submissions will draw on related literature in the discussion and would be sure to also reference those. The style of references does not matter, provided it is consistent.
\item
  Via Quercus, submit a link to your PDF report which is hosted on GitHub. At some point in the introduction to your report, you must provide a link to the GitHub repo where the code that you used for this assignment lives (Hint: Comment. Your. Code.). Your entire workflow must be entirely reproducible.
\item
  Please be sure to include a link to your survey in your report and screenshots of the survey in the appendix of your report.
\item
  There should be no evidence that this is a class assignment.
\end{itemize}

\hypertarget{check-offs-points-4}{%
\subsection{Check offs points}\label{check-offs-points-4}}

\hypertarget{faq-4}{%
\subsection{FAQ}\label{faq-4}}

\hypertarget{mr-willis-of-ohio}{%
\section{`Mr Willis of Ohio'}\label{mr-willis-of-ohio}}

\hypertarget{task-5}{%
\subsection{Task}\label{task-5}}

\begin{itemize}
\tightlist
\item
  Working in teams of one to four people, and in an entirely reproducible way, please use the Canadian General Social Survey (GSS) and a regression model to tell a story.
\end{itemize}

\hypertarget{recommended-steps-1}{%
\subsection{Recommended steps}\label{recommended-steps-1}}

\begin{itemize}
\tightlist
\item
  Depending on your focus and background, you may like to use a Bayesian hierarchical model, but regardless of the particular model that you use it must be well explained, thoroughly justified, appropriate to the task at hand, and the results must be beautifully described.
\item
  You may focus on any year, aspect, or geography that is reasonable given the focus and constraints of the GSS. As a reminder, the GSS `program was designed as a series of independent, annual, cross-sectional surveys, each covering one topic in-depth.' So please consider the topic and the year.
\item
  The GSS is available to University of Toronto students via the library. In order to use it you need to clean and prepare it. Code to do this for one year is being distributed alongside this problem set and was discussed in lectures.
\item
  You are welcome to simply use this code and this year, but the topic of that year will constrain your focus. Naturally, you are welcome to adapt the code to other years. If you use the code exactly as is then you must cite it. If you adapt the code then you don't have to cite it, as it has a MIT license, but it would be appropriate to at least mention and acknowledge it, depending on how close your adaption is.
\item
  Using R Markdown, please write a paper about your analysis and compile it into a PDF.
\item
  Your paper must be well-written, draw on relevant literature, and show your statistical skills by explaining all statistical concepts that you draw on.
\item
  Your paper must have the following sections: title, name/s, and date, abstract, introduction, data, model, results, discussion, and references.
\item
  You are welcome to use appendices for supporting, but not critical, material. Your discussion must include sub-sections on weaknesses and next steps.
\item
  In your report you must provide a link to a GitHub repo that fully contains your analysis. Your code must be entirely reproducible, documented, and readable. Your repo must be well-organised and appropriately use folders.
\item
  Your graphs and tables must be of an incredibly high standard. Graphs and tables should be well formatted and report-ready. They should be clean and digestible. Furthermore, you should label and describe each table/figure.
\item
  When you discuss the dataset (in the data section) you should make sure to discuss (at least):

  \begin{itemize}
  \tightlist
  \item
    Its key features, strengths, and weaknesses generally.
  \item
    A discussion of the questionnaire - what is good and bad about it?
  \item
    A discussion of the methodology including how they find people to take the survey; what their population, frame, and sample were; what sampling approach they took and what some of the trade-offs may be; what they do about non-response; the cost.
  \item
    This is just some of the issues strong submissions will consider. Show off your knowledge. If this becomes too detailed then you should push some of this to footnotes or an appendix.
  \end{itemize}
\item
  When you discuss your model (in the model section), you must be extremely careful to spell out the statistical model that you are using, defining and explaining each aspect and why it is important. (For a Bayesian model, a discussion of priors and regularization is almost always important.) You should mention the software that you used to run the model. You should be clear about model convergence, model checks, and diagnostic issues. How do the sampling and survey aspects that you discussed assert themselves in the modelling decisions that you make? Again, if it becomes too detailed then push some of the details to footnotes or an appendix.
\item
  You should present model results, graphs, figures, etc, in the results section. This section should strictly relay results. Interpretation of these results and conclusions drawn from the results should be left for the discussion section.
\item
  Your discussion should focus on your model results. Interpret them and explain what they mean. Put them in context. What do we learn about the world having understood your model and its results? What caveats could apply? To what extent does your model represent the small world and the large world (to use the language of McElreath, Ch 2)? What are some weaknesses and opportunities for future work?
\item
  Check that you have referenced everything. Strong submissions will draw on related literature in the discussion (and other sections) and would be sure to also reference those. The style of references does not matter, provided it is consistent.
\item
  As a team, via Quercus, submit a PDF of your paper. Again, in your paper you must have a link to the associated GitHub repo in an appendix. And you must include the R Markdown file that produced the PDF in that repo.
\item
  A good way to work as a team would be to split up the work, so that one person is doing each section. The people doing the sections that rely on data (such as the analysis and the graphs) could just simulate it while they are waiting for the person putting together the data to finish.
\item
  It is expected that your submission be well written and able to be understood by the average reader of say 538. This means that you are allowed to use mathematical notation, but you must be able to explain it all in plain English. Similarly, you can (and hint: you should) use survey, sampling, observational, and statistical terminology, but again you need to explain it. Your work should have flow and should be easy to follow and understand. To communicate well, anyone at the university level should be able to read your report once and relay back the methodology, overall results, findings, weaknesses and next steps without confusion.
\end{itemize}

\hypertarget{check-offs-points-5}{%
\subsection{Check offs points}\label{check-offs-points-5}}

\begin{itemize}
\tightlist
\item
  It is recommended that you (informally) proofread one another's sections - why not exchange papers with another group?
\item
  Everyone in the team receives the same mark.
\item
  There should be no evidence that this is a class assignment.
\end{itemize}

\hypertarget{faq-5}{%
\subsection{FAQ}\label{faq-5}}

\hypertarget{five-votes-down}{%
\section{`Five Votes Down'}\label{five-votes-down}}

\hypertarget{task-6}{%
\subsection{Task}\label{task-6}}

\begin{itemize}
\tightlist
\item
  The primary goal of this paper is to predict the overall popular vote of the 2020 American presidential election using multilevel regression with post-stratification.
\end{itemize}

\hypertarget{recommended-steps-2}{%
\subsection{Recommended steps}\label{recommended-steps-2}}

\begin{itemize}
\tightlist
\item
  We expect you to work as part of a group of 4 people, but groups of size 1-4 are fine. We have suggested a split of the work based on a 4-person group, but these are just suggestions.
\item
  Individual-level survey data:

  \begin{itemize}
  \tightlist
  \item
    Request access to the Democracy Fund + UCLA Nationscape `Full Data Set': \url{https://www.voterstudygroup.org/publication/nationscape-data-set}. This could take a day or two. Please start early.
  \item
    Given the expense of collecting this data, and the privilege of having access to it, if you don't properly cite this dataset then you will get zero for this problem set.
  \item
    Once you have access then pick a survey of interest. We will use ``ns20200102.dta'' in the example (your number may be different).
  \item
    This will be a large file and is not yours to share. Do not push it to GitHub (use the .gitignore file - see here: \url{https://carpentries-incubator.github.io/git-Rstudio-course/02-ignore/index.html}).
  \item
    Use the example R code to get started preparing this dataset, and then go on cleaning and preparing it based on what you need.
  \item
    Make graphs and tables about the survey data and write beautiful sentences and paragraphs explaining everything.
  \end{itemize}
\item
  Post-stratification data:

  \begin{itemize}
  \tightlist
  \item
    We will use the American Community Surveys (ACS).
  \item
    Please create an account with IPUMS: \url{https://usa.ipums.org/usa/index.shtml}
  \item
    You want the 2018 1-year ACS. Then you need to select some variables. This will depend on what you want to model and the survey data, but some options include: REGION, STATEICP, AGE, SEX, MARST, RACE, HISPAN, BPL, CITIZEN, EDUC, LABFORCE, INCTOT. Have a look around and see what you are interested in, remembering that you will need to establish a correspondence to the survey.
  \item
    Download the relevant post-stratification data (it's probably easiest to change the data format to .dta). Again, this can take some time. Please start this early.
  \item
    This will be a large file and is not yours to share. Do not push it to GitHub (use the .gitignore file - see here: \url{https://carpentries-incubator.github.io/git-Rstudio-course/02-ignore/index.html}).
  \item
    Given the expense of collecting this data, and the privilege of having access to it, if you don't properly cite this dataset then you will get zero for this problem set.
  \item
    Clean and prepare the post-stratification dataset.
  \item
    Remember that you need cell counts for the sub-populations in your model. See examples in the readings.
  \end{itemize}
\item
  (It may be efficient to start with simulated data while waiting for the real data) Modelling.

  \begin{itemize}
  \tightlist
  \item
    You will want to explain vote intention based on a variety of explanatory variables. Construct the vote intention variable so that it is binary (either `supports Trump' or `supports Biden').
  \item
    You are welcome to use lm() but you would need to explain the nuances of this decision in the model section (Hint: start here: \url{https://statmodeling.stat.columbia.edu/2020/01/10/linear-or-logistic-regression-with-binary-outcomes/}).
  \item
    That said, you should probably use logistic regression if it is at all possible for you. If you don't know where to start then look at (in increasing levels of complexity) glm(), lme4::glmer(), or brms::brm(). There are examples of each in the readings.
  \item
    Think very deeply about model fit, diagnostics, and other similar things that you need in order to convince someone that your model is appropriate.
  \item
    You have flexibility of the model that you use, (and hence the cells that you'll need to create next). In general, the more cells the better, but you may want fewer cells for simplicity in the writing process and to ensure a decent sample in each cell.
  \item
    Apply your trained model to the post-stratification dataset to make the best estimate of the election result that you can. The specifics will depend on your modelling approach but will likely involve predict(), add\_predicted\_draws(), or similar. See the examples in the readings. We are primarily interested in the distribution of your forecast of the overall Presidential popular vote, and how the explanatory variables affect this. But great submissions would go beyond that. Also, you're taking a statistics course, so if you just gave a central estimate and nothing else, then that would not be great.
  \item
    Create beautiful graphs and tables of your model and results.
  \item
    Create wonderful paragraphs talking about and explaining everything.
  \end{itemize}
\item
  (Again, it's probably efficient to start with simulated data/results while waiting)

  \begin{itemize}
  \tightlist
  \item
    Write up.
  \item
    Using R Markdown, please write a very thorough paper about your analysis and compile it into a PDF.
  \item
    The paper must be well-written, draw on relevant literature, and show your statistical skills by explaining all statistical concepts that you draw on.
  \item
    The paper must have the following sections: title, name/s, and date, abstract and keywords, introduction, data, model, results, discussion, and references.
  \item
    The paper may use appendices for supporting, but not critical, material.
  \item
    The discussion needs to be substantial. For instance, if the paper were 10 pages long then a discussion should be at least 2.5 pages. In the discussion, the paper must include subsections on weaknesses and next steps - but these must be in proportion.
  \item
    The report must provide a link to a GitHub repo that contains everything (apart from the raw data that you git ignored because it is not yours to share). The code must be entirely reproducible, documented, and readable. The repo must be well-organised and appropriately use folders and README files.
  \item
    The graphs and tables must be of an incredibly high standard, well formatted, and report-ready. They should be clean and digestible. Furthermore, you should label and describe each table/figure.
  \item
    When you discuss the datasets (in the data section) (remember there will be at least two datasets to discuss) you should make sure to discuss (at least):

    \begin{itemize}
    \tightlist
    \item
      Their key features, strengths, and weaknesses generally.
    \item
      The survey questionnaire - what is good and bad about it?
    \item
      A discussion of the methodology including how they find people to take the survey; what their population, frame, and sample were; what sampling approach they took and what some of the trade-offs may be; what they do about non-response; the cost.
    \item
      This is just some of the issues strong submissions will consider. Show off your knowledge. If this becomes too detailed then you should push some of this to footnotes or an appendix.
    \end{itemize}
  \item
    The dataset section is probably an appropriate place to include an explanation of what post-stratification is (in non-statistical language) and the strengths and weaknesses of it, although this discussion may fit more naturally in another section. Regardless, be sure to justify the inclusion of each explanatory variable.
  \item
    When you discuss your model (in the model section), you must be extremely careful to spell out the statistical model that you are using, defining and explaining each aspect and why it is important. (For a Bayesian model, a discussion of priors and regularization is almost always important.) You should mention the software that you used to run the model. You should be clear about model convergence, model checks, and diagnostic issues, although you may push the details of this to an appendix depending on how detailed you get. How do the sampling and survey aspects that you discussed assert themselves in the modelling decisions that you make? How can you convince a reader that you've neither overfit nor underfit the data? Again, if it becomes too detailed then push some of the details to footnotes or an appendix.
  \item
    You should present model results, graphs, figures, etc, in the results section. This section should strictly relay results. It must include text explaining all of these and summary statistics and similar. However, interpretation of these results and conclusions drawn from the results should be left for the discussion section.
  \item
    Your discussion should focus on your model results, but this time interpreting them, and explaining what they mean. Put them in context. What do we learn about the world having understood your model and its results? What caveats could apply? To what extent does your model represent the small world and the large world (to use the language of McElreath, Ch 2)? What are some weaknesses and opportunities for future work? Who is going to win the election? How confident are you in that forecast? Do you have a small or large distribution? What could that mean? Are you more confident in certain states? Do certain explanatory variables carry more weight than others? Etc.
  \item
    Check that you have referenced everything. Strong submissions will draw on related literature in the discussion (and other sections) and would be sure to also reference those. The style of references does not matter, but it must be consistent.
  \item
    If you don't cite R then you will get zero for this problem set.
  \item
    As a team, via Quercus, submit a PDF of your paper. Again, in your paper you must have a link to the associated GitHub repo. And you must include the R Markdown file that produced the PDF in that repo.
    The- R Markdown file must exactly produce the PDF. Don't edit it manually ex post - that isn't reproducible.
  \end{itemize}
\item
  A good way to work as a team would be to split up the work, so that one person is doing each section. The people doing the sections that rely on data (such as the analysis and the graphs) could just simulate it while they are waiting for the person putting together the data to finish. We have recommended a split above, but you do what works for you.
\end{itemize}

\hypertarget{check-offs-points-6}{%
\subsection{Check offs points}\label{check-offs-points-6}}

\begin{itemize}
\tightlist
\item
  It is expected that your submission be well written and able to be understood by the average reader of say 538. This means that you are allowed to use mathematical notation, but you must be able to explain it all in plain English. Similarly, you can (and hint: you should) use survey, sampling, observational, and statistical terminology, but again you need to explain it. The average person doesn't know what a p-value is nor what a confidence interval is. You need to explain all of this in plain language the first time you use it. Your work should have flow and should be easy to follow and understand. To communicate well, anyone at the university level should be able to read your report once and relay back the methodology, overall results, findings, weaknesses and next steps without confusion.
\item
  It is recommended that you (informally) proofread one another's work - why not exchange papers with another group?
\item
  Everyone in the team receives the same mark.
\item
  There should be no evidence that this is a class assignment.
\end{itemize}

\hypertarget{faq-6}{%
\subsection{FAQ}\label{faq-6}}

\hypertarget{whats-next}{%
\section{`What's next?'}\label{whats-next}}

\hypertarget{task-7}{%
\subsection{Task}\label{task-7}}

Please work individually. In this paper, you will conduct original research that applies methods from statistics to a question involving surveys, sampling or observational data.

\hypertarget{recommended-steps-3}{%
\subsection{Recommended steps}\label{recommended-steps-3}}

You have various options for topics (pick one):
Develop a research question that is of interest to you and obtain or create a relevant dataset. This option involves developing your own research question based on your own interests, background, and expertise. I encourage you to take this option, but please discuss your plans with me. How does one come up with ideas? One way is to be question-driven, where you keep an informal log of small ideas, questions, and puzzles, that you have as you're reading and working. Often, after dwelling on it for a while you can manage to find some questions of interest. Another way is to be data-driven - try to find some interesting dataset and then work backward. Finally, yet another way, is to be methods-driven - let's say that you happen to understand Gaussian processes, then just apply that expertise.
(Thanks to Jack Bailey for this idea.) Build a MRP model based on the CES and a post-stratification dataset that you obtain, to identify how the 2019 Canadian Federal Election would have been different if `everyone' had voted. What do we learn about the importance of turnout based on your model and results? (This option involves logistic regression in either frequentist or Bayesian settings.)
Reproduce a paper. This means that you download the data and then write your own code (using their code and paper as a guide if it's available) to try to get their results and then write up what you did and found. Options include:
Angelucci, Charles, and Julia Cagé, 2019, `Newspapers in times of low advertising revenues', American Economic Journal: Microeconomics, please see: \url{https://www.openicpsr.org/openicpsr/project/116438/version/V1/view}. (This option can be accomplished with just OLS. It is a `safe' pick. I even already provided you with some code in class to get started - see the notes! ).
Bailey, Michael A., Daniel J. Hopkins \& Todd Rogers, 2016, `Unresponsive and Unpersuaded: The Unintended Consequences of a Voter Persuasion Effort', Political Behavior.
Clark, Sam, 2019, `A General Age-Specific Mortality Model With an Example Indexed by Child Mortality or Both Child and Adult Mortality', Demography, please see: \url{https://github.com/sinafala/svd-comp}. (This is an ambiguous pick!)
Skinner, Ben, 2019, `Making the connection: Broadband access and online course enrollment at public open admissions institutions', Research in Higher Education, please see: \url{https://github.com/btskinner/oa_online_broadband_rep}.
Pons, Vincent, 2018, `Will a Five-Minute Discussion Change Your Mind? A Countrywide Experiment on Voter Choice in France' American Economic Review.
Valencia Caicedo, Felipe, 2019, `The Mission: Human Capital Transmission, Economic Persistence, and Culture in South America', The Quarterly Journal of Economics, please see: \url{https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ML1155}.
If you have a favourite paper and want to reproduce it, then please let me know by the end of Week 12 so that I can check that it's appropriate.
Pretend that you work for Upworthy. Request the Upworthy dataset and then use it to evaluate the result of an A/B test. This request could take a week. Please plan ahead if you choose this option.
Critique the following paper: AlShebli, Bedoor, Kinga Makovi \& Talal Rahwan, 2020, `The association between early career informal mentorship in academic collaborations and junior author performance', Nature Communications. You should be able to download the data here: \url{https://github.com/bedoor/Mentorship} and the paper here: \url{https://www.nature.com/articles/s41467-020-19723-8}. For background and starting points for your critique, please see: \url{https://statmodeling.stat.columbia.edu/2020/11/19/are-female-scientists-worse-mentors-this-study-pretends-to-know/} and \url{https://danieleweeks.github.io/Mentorship/\#summary}. (This option involves extensive data exploration and thinking really hard about what they are trying to do and how they are doing it.)
Use this post by Andrew Whitby - \url{https://andrewwhitby.com/2020/11/24/contact-tracing-biased/} - as a starting point to explore biased sampling and its effects on what we know about COVID and how this affects public policy. (This option involves extensive simulation.)
In `Bias Behind Bars', data journalist Tom Cardoso finds that `(a)fter controlling for a number of variables, \ldots{} Black and Indigenous inmates are more likely to get worse scores than white inmates, based solely on their race.'
The main story is here: \url{https://www.theglobeandmail.com/canada/article-investigation-racial-bias-in-canadian-prison-risk-assessments/}
The methodology discussion is here: \url{https://www.theglobeandmail.com/canada/article-investigation-racial-bias-in-canadian-prisons-methodology/}
The observational data is available here: \url{https://www.theglobeandmail.com/files/editorial/News/nw-na-risk-1023/The_Globe_and_Mail_CSC_OMS_2012-2018_20201022235635.zip}
Your task is to follow the methodology that Tom published and attempt to replicate the results. Are you able to replicate them? Do the results change significantly under slightly different assumptions? (This option involves only frequentist logistic regression, although doing everything in a Bayesian setting would be lovely too).
You should know the expectations by now. If you need a refresher then review the past problem sets.
Everything is entirely reproducible.
Your paper must be written in R Markdown.
Your paper must have the following sections:
Title, date, author, keywords, abstract, introduction, data, model, results, discussion, appendix (optional, for supporting, but not critical, material), and a reference list.
Your paper must be well-written, draw on relevant literature, and show your statistical skills by explaining all statistical concepts that you draw on.
The discussion needs to be substantial. For instance, if the paper were 10 pages long then a discussion should be at least 2.5 pages. In the discussion, the paper must include subsections on weaknesses and next steps - but these must be in proportion.
The report must provide a link to a GitHub repo that contains everything (apart from any raw data that you git ignored if it is not yours to share). The code must be entirely reproducible, documented, and readable. The repo must be well-organised and appropriately use folders and README files.
My expectations for this paper are very high. I'm very excited to read what you submit. To help you achieve this standard, there are two initial `submissions' where you can get comments and feedback and then the final, actual, submission.
Due dates:
(Optional) December 9 11:59pm ET
Submit initial materials for peer-review.
As an individual, via Quercus, submit a PDF of your rough draft on Quercus by 11:59pm ET on Wednesday, December 9, 2020.
At a minimum this must include:
All top-matter (title, author (you can use a pseudonym if you want), date, keywords, abstract) completely filled out.
A fully written Introduction section.
All other sections must be present in your paper, but don't have to be filled out (e.g.~you must have a `Data' heading, but you don't need to have content in that section).
To be clear - it is fine to later change any aspect of what you submit at this check-point.
You will be awarded 1 percentage point just for submitting a draft that meets this minimum (that is 1 out of the 30 that are available for the final paper). If you don't submit, then the percentage point will be pushed to part d).
The point of this is to get feedback on your work (and to make sure you have at least started thinking about this project) so you are more than welcome to include other sections that you wish to get feedback on.
There will be no extensions granted for this submission since the following submission is dependent on this date.
(Optional) December 12 11:59pm ET
Conduct peer-review.
As an individual, on December 10, you will randomly be assigned a handful of rough drafts to provide feedback. You have until December 12, 2020 11:59pm ET to provide feedback to your peers.
If you provide feedback to one peer you will receive 1 percentage point, if you provide feedback to two peers you will receive 2 percentage points, if you provide feedback to three (or more) peers you will receive the full 3 percentage points.
You may complete this aspect whether or not you submitted something in part a). If you don't complete it then the percentage points will be pushed to part d).
Your feedback must include at least six comments (meaningful/useful bullet points). These must be well-written and thoughtful.
There will be no extensions granted for this submission since the following submission is dependent on this date.
Please remember that you are providing feedback here to help your colleagues. All comments should be professional and kind. It is challenging to receive criticism. Please remember that your goal here is to help your peers advance their writing/analysis. Any feedback that is inappropriate or not up to standard will receive a 0 and cannot be redeemed later.
(Optional) December 16 11:59pm ET
Submit materials for TA review.
Submit a PDF to Quercus. The TA will provide high-level comments on December 17.
At a minimum this must include:
All top-matter.
Fully written Introduction, Data, Model, and Results sections.
All other sections must be present in your paper, but don't have to be filled out (e.g.~you must have a `Discussion' heading, but you don't need to have content in that section).
To be clear - it is fine to later change any aspect of what you submit at this check-point.
You receive 1 percentage point for submitting something that meets that minimum. If you don't submit anything then this is pushed to the final paper.
There are no extensions possible on this aspect.
(Compulsory) December 20 11:59pm ET
As an individual, via Quercus, submit a PDF of your paper. Again, in your paper, you must have a link to the associated GitHub repo.
This submission will be graded based on a rubric that will be posted on Quercus and will be worth 25-30 percentage points depending on parts a) - c).

\hypertarget{check-offs-points-7}{%
\subsection{Check offs points}\label{check-offs-points-7}}

\hypertarget{faq-7}{%
\subsection{FAQ}\label{faq-7}}

  \bibliography{bibliography.bib,packages.bib}

\end{document}
