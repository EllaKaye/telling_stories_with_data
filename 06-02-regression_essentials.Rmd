---
date: February 2, 2021
bibliography: bibliography.bib
output:
  pdf_document:
    citation_package: natbib
  bookdown::pdf_book:
    citation_package: biblatex
---

# It’s Just A Linear Model

*Notes compiled: `r format(Sys.time(), "%d %B %Y")`.*


**Required reading**

- Greenland, Sander, Stephen J. Senn, Kenneth J. Rothman, John B. Carlin, Charles Poole, Steven N. Goodman, and Douglas G. Altman, 2016, 'Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations', *European journal of epidemiology*, 31, no. 4, pp. 337-350.
- James, Gareth, Daniela Witten, Trevor Hastie and Robert Tibshirani, 2017, *An Introduction to Statistical Learning with Applications in R*, Chapters 3 and 4, https://www.statlearning.com.
- Obermeyer, Z., Powers, B., Vogeli, C., & Sendhill, M., 2019, 'Dissecting racial bias in an algorithm used to manage the health of populations', *Science*, (366): 447-453.
- Wickham, Hadley, and Garrett Grolemund, 2017, *R for Data Science*, Chapter 23, https://r4ds.had.co.nz/.



**Recommended reading**

- Angrist, Joshua D., and Jörn-Steffen Pischke, 2008, *Mostly harmless econometrics: An empiricist's companion*, Princeton University Press, Chapter 3.4.3.
- Cunningham, Scott, *Causal Inference: The Mixtape*, Chapters 'Probability theory and statistics review' and 'Properties of Regression', http://www.scunning.com/causalinference_norap.pdf.
- ElHabr, Tony, 2019, 'A Bayesian Approach to Ranking English Premier League Teams (using R)', https://tonyelhabr.rbind.io/post/bayesian-statistics-english-premier-league/.
- Ioannidis, John PA, 2005, 'Why most published research findings are false', PLos med 2, no. 8, e124.
- Pavlik, Kaylin, 2018, 'Exploring the Relationship Between Dog Names and Breeds', https://www.kaylinpavlik.com/dog-names-tfidf/.
- Pavlik, Kaylin, 2019, 'Understanding + classifying genres using Spotify audio features', https://www.kaylinpavlik.com/classifying-songs-genres/.
- Ronald L. Wasserstein and Nicole A. Lazar, 2016, 'The ASA Statement on p-Values: Context, Process, and Purpose', *The American Statistician*, 70:2, 129-133, DOI: 10.1080/00031305.2016.1154108.
- Silge, Julia, 2019, 'Modeling salary and gender in the tech industry', https://juliasilge.com/blog/salary-gender/.
- Silge, Julia, 2019, 'Opioid prescribing habits in Texas', https://juliasilge.com/blog/texas-opioids/.
- Silge, Julia, 2019, 'Tidymodels', https://juliasilge.com/blog/intro-tidymodels/.
- Silge, Julia, 2020, '#TidyTuesday hotel bookings and recipes', https://juliasilge.com/blog/hotels-recipes/.
- Silge, Julia, 2020, 'Hyperparameter tuning and #TidyTuesday food consumption', https://juliasilge.com/blog/food-hyperparameter-tune/.
- Taddy, Matt, 2019, *Business Data Science*, Chapter 2.
- Taddy, Matt, 2019, *Business Data Science*, Chapter 4.


**Fun reading**

- Chellel, Kit, 2018, 'The Gambler Who Cracked the Horse-Racing Code', *Bloomberg Businessweek*, 3 May, https://www.bloomberg.com/news/features/2018-05-03/the-gambler-who-cracked-the-horse-racing-code.


<!-- https://rviews.rstudio.com/2020/03/10/comparing-machine-learning-algorithms-for-predicting-clothing-classes-part-3/ -->

<!-- https://juliasilge.com/blog/tuition-resampling/ -->

<!-- https://easystats.github.io/performance/index.html -->


**Key concepts/skills/etc**

- Linear regression.
- Uncertainty.
- Threats to validity.
- Logistic regression.
- Overfitting.

**Key libraries**

- `broom`
- `huxtable`
- `tidymodels`
- `tidyverse`

**Key functions**

- `broom::augment()`
- `broom::glance()`
- `broom::tidy()`
- `glm()`
- `huxtable::huxreg()`
- `lm()`
- `parsnip::fit()`
- `parsnip::linear_reg()`
- `parsnip::logistic_reg()`
- `parsnip::set_engine()`
- `poissonreg::poisson_reg()`
- `rnorm()`
- `rpois()`
- `rsample::initial_split()`
- `rsample::testing()`
- `rsample::training()`
- `sample()`
- `set.seed()`
- `summary()`


**Quiz**

1. Please write a linear relationship between some response variable, Y, and some predictor, X. What is the intercept term? What is the slope term? What would adding a hat to these indicate?
2. What is the least squares criterion? Similarly, what is RSS and what are we trying to do when we run least squares regression?
3. What is statistical bias?
4. If there were three variables: Snow, Temperature, and Wind, please write R code that would fit a simple linear regression to explain Snow as a function of Temperature and Wind. What do you think about another explanatory variable - daily stock market returns - to your model?
5. According to @greenland2016statistical, p-values test (pick one)?
    a. All the assumptions about how the data were generated (the entire model), not just the targeted hypothesis it is supposed to test (such as a null hypothesis).
    b. Whether the hypothesis targeted for testing is true or not.
    c. A dichotomy whereby results can be declared 'statistically significant'.
6. According to @greenland2016statistical, a p-value may be small because (select all)?
    a. The targeted hypothesis is false.
    b. The study protocols were violated.
    c. It was selected for presentation based on its small size. 
7. According to @obermeyer2019dissecting, why does racial bias occur in an algorithm used to guide health decisions in the US (pick one)?
    a. The algorithm uses health costs as a proxy for health needs.
    b. The algorithm was trained on Reddit data.
8. When should we use logistic regression (pick one)?
    a. Continuous dependent variable.
    b. Binary dependent variable.
    c. Count dependent variable.
9. I am interested in studying how voting intentions in the recent US presidential election vary by an individual's income. I set up a logistic regression model to study this relationship. In my study, one possible dependent variable would be (pick one)?
    a. Whether the respondent is a US citizen (yes/no)
    b. The respondent's personal income (high/low)
    c. Whether the respondent is going to vote for Trump (yes/no)
    d. Who the respondent voted for in 2016 (Trump/Clinton)
10. I am interested in studying how voting intentions in the recent US presidential election vary by an individual's income. I set up a logistic regression model to study this relationship. In my study, one possible dependent variable would be (pick one)?
    a. The race of the respondent (white/not white)
    b. The respondent's marital status (married/not)
    c. Whether the respondent is registered to vote (yes/no)
    d. Whether the respondent is going to vote for Biden (yes/no)
11. Please explain what a p-value is, using only the term itself (i.e. 'p-value') and words that are amongst the 1,000 most common in the English language according to the XKCD Simple Writer - https://xkcd.com/simplewriter/. (Please write one or two paragraphs.)



## Linear regression

```{r logregression, echo=FALSE, fig.cap="Oh my.", out.width = '70%'}
knitr::include_graphics(here::here("figures/lolregression.png"))
```
Source: [Mijke Rhemtulla](https://twitter.com/mijkenijk/status/1234899588311474176), 3 March 2020.





### Brief reminder

If we have two variables, $Y$ and $X$, then we could characterise the relationship between these as:
$$Y \sim \beta_0 + \beta_1 X.$$

There are two coefficients/parameters: the 'intercept' is $\beta_0$, and the 'slope' is $\beta_1$. We are saying that $Y$ will have some value, $\beta_0$, even when $X$ is 0, and that $Y$ will change by $\beta_1$ units for every one unit change in $X$. 

We may then take this relationship to the data that we have about the relationship in order to estimate these coefficients for those particular values that we have:
$$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x.$$

We are saying this is a linear regression because we assume that if $x$ doubles then $y$ would also double. Linear regressions considers how the average of a dependent variable changes based on the independent variables.

I want to focus on data, so we'll make this example concrete, by generating some data and then discussing everything in the context of that. The example will be looking at someone's time for running five kilometers, compared with their time for running a marathon. 

```{r, warning = FALSE, message = FALSE}
library(tidyverse)
set.seed(853)
number_of_observations <- 100
running_data <- 
  tibble(five_km_time = rnorm(number_of_observations, 20, 3),
         noise = rnorm(number_of_observations, 0, 10),
         marathon_time = five_km_time * 8.4 + noise,
         was_raining = sample(c("Yes", "No"), 
                              size = number_of_observations,
                              replace = TRUE, 
                              prob = c(0.2, 0.8)) 
         )

running_data %>% 
  ggplot(aes(x = five_km_time, y = marathon_time)) +
  geom_point() +
  labs(x = "Five-kilometer time (minutes)",
       y = "Marathon time (minutes)") +
  theme_classic()
```

In this set-up we may like to use $x$, which is the five-kilometer time, to produce estimates of $y$, which is the marathon time. This would involve also estimating values of $\beta_0$ and $\beta_1$, which is why they have a hat on them.

But how should we estimate the coefficients? Even if we impose a linear relationship there are a lot of options (how many straight lines can you fit on a piece of paper?). But clearly some of the fits are not all that great. 

One way we may define being great would be to impose that they be as close as possible to each of the $x$ and $y$ combinations that we know. One way is to choose the one that minimises the sum of least squares. To do this we produce our estimates of $\hat{y}$ based on some estimates of $\hat{\beta}_0$ and $\hat{\beta}_1$, given the $x$, and then work out how 'wrong', for every point $i$, we were:
$$ e_i = y_i - \hat{y}_i.$$

The residual sum of squares (RSS) then requires summing across all the points:
$$ \mbox{RSS} = e^2_1+ e^2_2 +\dots + e^2_n.$$
This results in one 'linear best-fit' line, but it is worth thinking about all of the assumptions and decisions that it took to get us to this point.
 
```{r}
running_data %>% 
  ggplot(aes(x = five_km_time, y = marathon_time)) +
  geom_point() + 
  geom_smooth(method = "lm", 
              se = FALSE, 
              color = "black", 
              linetype = "dashed",
              formula = 'y ~ x') +
  labs(x = "Five-kilometer time (minutes)",
       y = "Marathon time (minutes)") +
  theme_classic()
```

With the least squares criterion we want the values of $\hat{\beta}_0$ and $\hat{\beta}_1$ that result in the smallest RSS.


### Implementing it in R

Within R, the main function for doing linear regression is `lm`. This is included in base R, so you don't need to call any packages, but in a moment, we will call a bunch of packages that will surround `lm` within an environment that we are more familiar with. You specify the relationship with the dependent variable first, then `~`, then the independent variables. Finally, you should specify the dataset (or you could pipe to it as usual).

```{r, include = TRUE, eval = FALSE}
lm(y ~ x, data = dataset)
```

In general, you should assign this to an object:
```{r}
running_data_first_model <- lm(marathon_time ~ five_km_time, 
                               data = running_data)
```

To see the result of your regression you can then call `summary()`.

```{r}
summary(running_data_first_model)
```

The first part of the result tells us the regression that we called, then information about the residuals, and the estimated coefficients. And then finally some useful diagnostics. 

We are considering that there is some relationship between $X$ and $Y$ - $Y = f(X) + \epsilon$ -  and we are going to say that function is linear and so our relationship is:
$$\hat{Y} = \beta_0 + \beta_1 X + \epsilon.$$

There is some 'true' relationship between $X$ and $Y$, but we don't know what it is. All we can do is use our sample of data to try to estimate it. But because our understanding depends on that sample, for every possible sample, we would get a slightly different relationship (as measured by the coefficients). 

That $\epsilon$ is a measure of our error - what does the model not know? There's going to be plenty that the model doesn't know, but we hope is that the error does not depend on $X$. 

The intercept is marathon time that we would expect with a five-kilometer time of 0 minutes. Hopefully this example illustrates the need to carefully interpret the intercept coefficient! The coefficient on five-kilometer run time shows how we expect the marathon time to change if five-kilometer run time changed by one unit. In this case it's about 8.4, which makes sense seeing as a marathon is roughly that many times longer than a five-kilometer run.


### Tidy up with broom

While there is nothing wrong with the base approach, I want to introduce the `broom` package because that will provide us with outputs in a tidy framework [@citebroom]. There are three key functions:

- `broom::tidy`: Gives the coefficient estimates in a tidy output.
- `broom::glance`: Gives the diagnostics.
- `broom::augment`: Adds the forecasted values, and hence, residuals, to your dataset.


```{r}
library(broom)
tidy(running_data_first_model)
glance(running_data_first_model)
```

Notice how the results are fairly similar to the base summary function.

```{r}
running_data <- 
  augment(running_data_first_model,
          data = running_data)
head(running_data)
```

We could now make plots of the residuals.

```{r}
ggplot(running_data, 
       aes(x = .resid)) + 
  geom_histogram(binwidth = 1) +
  theme_classic() +
  labs(y = "Number of occurrences",
       x = "Residuals")

ggplot(running_data, aes(five_km_time, .resid)) + 
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dotted", color = "grey") +
  theme_classic() +
  labs(y = "Residuals",
       x = "Five-kilometer time (minutes)")
```

When we say our estimate is unbiased, we are trying to say that even though with some sample our estimate might be too high, and with another sample our estimate might be too low, eventually if we have a lot of data then our estimate would be the same as the population. (A pro hockey player may sometimes shoot right of the net, and sometimes left of the net, but we'd hope that on average they'd be right in the middle of the net ;)).

But we want to try to speak to the 'true' relationship, so we need to try to capture how much we think our understanding depends on the particular sample that we have to analyse. And this is where standard error comes in. It tells us how off our estimate is compared with the actual. 

From standard errors, we can compute a confidence interval. A 95 per cent confidence interval means that there is a 0.95 probability that the interval happens to contain the population parameter (which is typically unknown).

```{r}
running_data %>% 
  ggplot(aes(x = five_km_time, y = marathon_time)) +
  geom_point() + 
  geom_smooth(method = "lm", 
              se = TRUE, 
              color = "black", 
              linetype = "dashed",
              formula = 'y ~ x') +
  labs(x = "Five-kilometer time (minutes)",
       y = "Marathon time (minutes)") +
  theme_classic()
```



### Testing hypothesis

Now that we have an interval for which we can say there is a 95 per cent probability it contains the true population parameter we can test claims. For instance, a null hypothesis that there is no relationship between $X$ and $Y$ (i.e. $\beta_1 = 0$), compared with an alternative hypothesis that there is some relationship between $X$ and $Y$ (i.e. $\beta_1 \neq 0$).

We need to know whether our estimate of $\beta_1$, which is $\hat{\beta}_1$, is 'far enough' away from zero for us to be comfortable claiming that $\beta_1 \neq 0$. How far is 'far enough'? If we were very confident in our estimate of $\beta_1$ then it wouldn't have to be far, but if we were not then it would have to be substantial. So it depends on a bunch of things, but essentially the standard error of $\hat{\beta}_1$. 

We compare this standard error with $\hat{\beta}_1$ to get the t-statistic:
$$t = \frac{\hat{\beta}_1 - 0}{\mbox{SE}(\hat{\beta}_1)}.$$ 
And we then compare our t-statistic to the t-distribution to compute the probability of getting this absolute t-statistic or a larger one, if $\beta_1 = 0$. This is the p-value. A small p-value means it is unlikely that we would observe our association due to chance if there wasn't a relationship.



### Adding more and varied explanatory variables

To this point we've just considered one explanatory variable. But we'll usually have more than one. One approach would be to run separate regressions for each explanatory variable. But compared with separate linear regressions for each, adding more explanatory variables allows us to have a better understanding of the intercept and accounts for interaction. Often the results will be quite different.

> This slightly counterintuitive result is very common in many real life situations. Consider an absurd example to illustrate the point. Running a regression of shark attacks versus ice cream sales for data collected at a given beach community over a period of time would show a positive relationship, similar to that seen between sales and newspapers. Of course no one (yet) has suggested that ice creams should be banned at beaches to reduce shark attacks. In reality, higher temperatures cause more people to visit the beach, which in turn results in more ice cream sales and mores hark attacks. A multiple regression of attacks versus ice cream sales and temperature reveals that, as intuition implies, the former predictor is no longer significant after adjusting for temperature.
>
> [@islr, p. 74].

We may also like to consider variables that do not have an inherent ordering. For instance, pregnant or not. When there are only two options then we can use a binary variable which is 0 or 1. If there are more than two levels then use a combination of binary variables, where the 'missing' outcome (baseline) gets pushed onto the intercept.

In other languages you may need to explicitly construct dummy variables, but as R was designed as a language to do statistical programming, it does a lot of the work here for you and is fairly forgiving. For instance, if you have a column of character values that only had two values: `c("Monica", "Rohan", "Rohan", "Monica", "Monica", "Rohan")`, and you used this as a independent variable in your usual regression set up then R would treat it as a dummy variable.

```{r, include = TRUE}
running_data_rain_model <- 
  lm(marathon_time ~ five_km_time + was_raining, 
     data = running_data)
summary(running_data_rain_model)
```

The result probably isn't too surprising if we look at a plot of the data.
```{r}
running_data %>%
  ggplot(aes(x = five_km_time, y = marathon_time, color = was_raining)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "black", linetype = "dashed") +
  labs(x = "Five-kilometer time (minutes)",
       y = "Marathon time (minutes)",
       color = "Was raining") +
  theme_classic() +
  scale_color_brewer(palette = "Set1")
```

In addition to wanting to include additional explanatory variables we may think that they are related with one another. For instance, if we were wanting to explain the amount of snowfall in Toronto, then we may be interested in the humidity and the temperature, but those two variables may also interact. We can do this by using `*` instead of `+` when we specify the model in R. If you do interact variables, then you should almost always also include the individual variables as well (Figure \@ref(fig:trump)).

```{r trump, echo=FALSE, fig.cap="Don't leave out the main effects in an interactive model", out.width = '90%'}
knitr::include_graphics("figures/trump.jpg")
```
Source: By [Kai Arzheimer](https://twitter.com/kai_arzheimer/status/1228998718646607876), 16 February 2020.



### Threats to validity and aspects to think about 

There are a variety of weaknesses and aspects that you should discuss when you use linear regression. A quick list includes [@islr, p. 92]:

1. Non-linearity of the response-predictor relationships.
2. Correlation of error terms.
3. Non-constant variance of error terms.
4. Outliers.
5. High-leverage points.
6. Collinearity

These are also aspects that you should discuss if you use linear regression. Including plots tends to be handy here to illustrate your points. Other aspects that you may consider discussing include [@islr, p. 75]:

1. Is at least one of the predictors $X_1, X_2, \dots, X_p$ useful in predicting the response?
2. Do all the predictors help to explain $Y$, or is only a subset of the predictors useful?
3. How well does the model fit the data?
4. Given a set of predictor values, what response value should we predict, and how accurate is our prediction?



### More credible outputs

Finally, after creating beautiful graphs and tables you may want your regression output to look just as nice. There are a variety of packages in R that will automatically format your regression outputs. One that is particularly nice is `huxtable` [@citehuxtable].

```{r, warning = FALSE, message = FALSE}
library(huxtable)
huxreg(running_data_first_model, running_data_rain_model)
```



### Tidymodels

The reason that we went to all that trouble to do simple regression is that we often want to fit a bunch of models. One way is to copy/paste code a bunch of times. There's nothing wrong with that. And that's the way that most people get started, but you may want to take an approach that scales more easily. We also need to think more carefully about over-fitting, and being able to evaluate our models.

`Tidymodels` [@citeTidymodels] is what all the cool kids are using these days. It's an attempt to bring some order to the chaos that has been different modelling packages in R. (There have been other attempts in the past and they've crashed and burned, but hopefully this time is different.) The issue is that let's say you want to run a simple linear regression and then run a random forest. The language that you'd use to code these models is fairly different. `Tidymodels` is the latest attempt to bring a coherent grammar to this. It's also a package of packages.

We'll create test and training datasets.

```{r}
set.seed(853)
library(tidymodels)

running_data_split <- rsample::initial_split(running_data, prop = 0.80)
running_data_split
```

So we have 81 points in our training set, 19 in our test set and 100 in total.

We can then make datasets for the test and training samples.

```{r}
running_data_train <- rsample::training(running_data_split)
running_data_test  <-  rsample::testing(running_data_split)
```

If we have a look at the dataset that we made we can see that it's got fewer rows. We could have reached the same outcome with something like:

```{r}
running_data <- 
  running_data %>% 
  mutate(magic_number = sample(x = c(1:nrow(running_data)), size = nrow(running_data), replace = FALSE))

running_data_test <- 
  running_data %>% 
  filter(magic_number <= 20)

running_data_train <- 
  running_data %>% 
  filter(magic_number > 20)
```



```{r}
first_go <- 
  parsnip::linear_reg() %>%
  parsnip::set_engine(engine = "lm") %>% 
  parsnip::fit(marathon_time ~ five_km_time + was_raining, 
               data = running_data_train
               )
```









<!-- ### Bayesian approaches -->

<!-- The `tidymodels` will be fine for specific types of tasks. For instance if you are doing machine learning then chances are you are interested in forecasting. That's the role of thing that `tidymodels` is really built for. If you want equivalent firepower for explanatory modelling then one option is to use Bayesian approaches more directly. -->

<!-- There are a variety of ways of getting started, but essentially what you need is a probabilistic programming language. That is one that is specifically designed for this sort of thing, in comparison to `R`, which is designed for more general statistical computing. We will use Stan in these notes within the context of our familiar R environment. -->





<!-- ## Tutorial -->

<!-- Let's do this in the context of the Paspaley dataset that we were introduced to in an earlier lecture. -->

<!-- Read in the data. -->

<!-- ```{r} -->
<!-- paspaley_data <- read_csv(here::here("notes/inputs/data/paspaley_cleaned_dataset.csv")) -->

<!-- head(paspaley_data) -->
<!-- ``` -->

<!-- Take a brief look: -->

<!-- ```{r} -->
<!-- skimr::skim(paspaley_data) -->



<!-- paspaley_data$keshi %>% table() -->
<!-- paspaley_data$metal %>% table() -->
<!-- paspaley_data$category %>% table() -->
<!-- ``` -->

<!-- Clean up and remove some anonying other variables. -->

<!-- ```{r} -->
<!-- paspaley_data <-  -->
<!--   paspaley_data %>%  -->
<!--   filter(metal != "Other") %>%  -->
<!--   filter(category != "Other") -->
<!-- ``` -->

<!-- Run a ordinary linear regression. -->

<!-- ```{r} -->
<!-- model_1 <- lm(price ~ keshi, data = paspaley_data) -->
<!-- model_2 <- lm(price ~ metal, data = paspaley_data) -->
<!-- model_3 <- lm(price ~ category, data = paspaley_data) -->
<!-- model_4 <- lm(price ~ keshi + metal + category, data = paspaley_data) -->
<!-- ``` -->

<!-- `broom` is great to look at the models, in particular `glance` and `tidy`.  -->

<!-- ```{r} -->
<!-- summary(model_1) -->
<!-- broom::glance(model_1) -->
<!-- broom::tidy(model_1) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- broom::glance(model_4) -->
<!-- broom::tidy(model_4) -->

<!-- paspaley_data %>%  -->
<!--   ggplot(aes(x = price)) + -->
<!--   geom_histogram() + -->
<!--   facet_wrap(vars(metal), scales = "free_y") -->
<!-- ``` -->


<!-- You can add the fitted variables easily too. -->

<!-- ```{r} -->
<!-- paspaley_data_with_model_4 <- broom::augment(model_4) -->

<!-- head(model_4) -->
<!-- ``` -->









## Binary and count data 

### Logistic regression

To steal a joke from someone, 'it's AI when you're fundraising, machine learning when you're hiring, and logistic regression when you're implementing.'

When the dependent variable is a binary outcome, that is 0 or 1, then instead of linear regression we may like to use logistic regression. Although a binary outcome may sound limiting, there are a lot of circumstances in which your outcome either naturally falls into this situation, or can be adjusted into it (e.g. a voter supports the liberals or not the liberals). 

The reason that we use logistic regression is that we'll be modelling a probability and so it will be bounded between 0 and 1. Whereas with linear regression we may end up with values outside this. In practice it is usually fine to start with linear regression and then move to logistic regression as you build confidence.

This all said, logistic regression, as Daniella Witten teaches us, is just a linear model!

As with linear regression, logistic regression is built into R, with the `glm` function. In this case, we'll try to work out if it was raining based on a person's marathon time and their five kilometer time.

```{r}
running_data <- 
  running_data %>% 
  mutate(was_raining_binary = if_else(was_raining == "No", 0, 1))

rain_model <- 
  glm(was_raining_binary ~ five_km_time + marathon_time, 
    data = running_data, 
    family = 'binomial')

summary(rain_model)
```

One reason that logistic regression can be a bit of a pain initially is because the coefficients take a bit of work to interpret. In particular, our estimate on marathon time is 0.00969. This is the odds. So the odds that it was raining increase by 0.01 as your marathon time increase. We can have our model make forecasts in terms of a probability, by asking for that.

```{r}
running_data <- 
  broom::augment(rain_model,
          data = running_data,
          type.predict = "response")
head(running_data)

```




<!-- https://jtcies.com/2020/03/when-does-garbage-time-start/ -->
<!-- https://meghan.rbind.io/post/tidymodels-intro/ -->
<!-- https://alison.rbind.io/post/2020-02-27-better-tidymodels/#tidymodels-101 -->

We can use `tidymodels` to run this if we wanted.

```{r}
set.seed(853)

running_data <- 
  running_data %>% 
  mutate(was_raining_binary = as_factor(was_raining_binary))

running_data_split <- rsample::initial_split(running_data, prop = 0.80)
running_data_train <- rsample::training(running_data_split)
running_data_test  <-  rsample::testing(running_data_split)

rain_model_tidymodels <-
  parsnip::logistic_reg(mode = "classification") %>%
  parsnip::set_engine("glm") %>%
  fit(was_raining_binary ~ five_km_time + marathon_time, 
      data = running_data_train)

rain_model_tidymodels
```



### Poisson regression

When we have count data, we use Poisson distribution. From @pitman[p. 121] 'The Poisson distribution with parameter $\mu$ or Poisson ($\mu$) distribution is the distribution of probabilities  $P_{\mu}(k)$ over ${0, 1, 2, ...}$ defined by: 
$$P_{\mu}(k) = e^{-\mu}\mu^k/k!\mbox{, for }k=0,1,2,...$$
We can simulate $n$ data points from the Poisson distribution with `rpois` where $lambda$ is the mean and the variance.

```{r}
rpois(n = 20, lambda = 3)
```

That $\lambda$ parameter governs the shape of the distribution.

```{r}
set.seed(853)
number_of_each <- 1000
tibble(lambda = c(rep(0, number_of_each), rep(1, number_of_each), rep(2, number_of_each), rep(5, number_of_each), rep(10, number_of_each)),
       draw = c(rpois(n = number_of_each, lambda = 0), rpois(n = number_of_each, lambda = 1), rpois(n = number_of_each, lambda = 2), rpois(n = number_of_each, lambda = 5), rpois(n = number_of_each, lambda = 10))) %>% 
  ggplot(aes(x = draw)) +
  geom_density() +
  facet_wrap(vars(lambda)) +
  theme_classic()
```

For instance, if we look at the number of A+ grades that are awarded in each university course in a given term then for each course we would have a count.

```{r}
set.seed(853)
count_of_A_plus <- 
  tibble( 
    # https://stackoverflow.com/questions/1439513/creating-a-sequential-list-of-letters-with-r
    department = c(rep.int("1", 26), rep.int("2", 26)),
    course = c(paste0("DEP_1_", letters), paste0("DEP_2_", letters)),
    number_of_A_plus = c(sample(c(1:10), 
                              size = 26,
                              replace = TRUE),
                         sample(c(1:50), 
                              size = 26,
                              replace = TRUE)
    )
  )
```

```{r}
grades_model <- 
  glm(number_of_A_plus ~ department, 
    data = count_of_A_plus, 
    family = 'poisson')

summary(grades_model)
```


We can use `tidymodels` to run this if we wanted although we first need to install a helper package.

```{r}
# install.packages("poissonreg")

set.seed(853)

count_of_A_plus_split <- rsample::initial_split(count_of_A_plus, prop = 0.80)
count_of_A_plus_train <- rsample::training(count_of_A_plus_split)
count_of_A_plus_test  <-  rsample::testing(count_of_A_plus_split)

a_plus_model_tidymodels <-
  poissonreg::poisson_reg(mode = "regression") %>%
  parsnip::set_engine("glm") %>%
  parsnip::fit(number_of_A_plus ~ department, 
      data = count_of_A_plus_train)

a_plus_model_tidymodels
```




<!-- ## Classification -->


<!-- **Add real classification content** -->

