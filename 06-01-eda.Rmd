---
date: May 17, 2020
bibliography: bibliography.bib
output:
  pdf_document:
    citation_package: natbib
  bookdown::pdf_book:
    citation_package: biblatex
---

# (PART) Model {-}

# Exploratory data analysis


**This chapter was written with [Monica Alexander](https://www.monicaalexander.com/).**



**Required reading**

- Wickham, Hadley, and Garrett Grolemund, 2017, *R for Data Science*, Chapters 3 and 7, freely available here: https://r4ds.had.co.nz/.

**Recommended reading**

- Hall, Megan, 2019, 'Exploratory Data Analysis Using Tidyverse', freely available at: https://hockey-graphs.com/2019/10/08/exploratory-data-analysis-using-tidyverse/.
- Jordan, Michael I, 2019, 'AI - The revolution hasn't started yet', freely available at: https://hdsr.mitpress.mit.edu/pub/wot7mkc1.
- Silge, Julia, 2018, 'Understanding PCA using Stack Overflow data', freely available at: https://juliasilge.com/blog/stack-overflow-pca/.
- Soetewey, Antoine, 2020, 'Descriptive statistics in R', freely available at: https://www.statsandr.com/blog/descriptive-statistics-in-r/.
- Stodulka, Jiri, 2019, 'Toronto Crime and Folium', freely available at: https://www.jiristodulka.com/post/toronto-crime/.
- Wong, Julia Carrie, 2020, 'One year inside Trump's monumental Facebook campaign', The Guardian, 29 January, freely available at: https://www.theguardian.com/us-news/2020/jan/28/donald-trump-facebook-ad-campaign-2020-election.


**Key concepts/skills/etc**

- Quickly coming to terms with a new dataset

**Key libraries/functions/etc**

- `tidyverse`
- `ggplot2`

**Quiz**

- In your own words what is exploratory data analysis?
- If you have a dataset called 'my_data', which has two columns: 'first_col' and 'second_col', then could you please write some rough R code that would generate a graph (the type of graph doesn't matter).
- Consider a dataset that has 500 rows and 3 columns, so there are 1,500 cells. If 100 of the cells are missing data for at least one of the columns, then would you remove the whole row your dataset or try to run your analysis on the data as is, or some other procedure? What if your dataset had 10,000 rows instead, but the same number of missing cells?
- Please note three ways of identifying unusual values.
- What is the difference between a categorical and continuous variable?


## Introduction

> Exploratory data analysis is never finished, you just die.

This chapter is about exploratory data analysis (EDA) and data visualization steps in R. The aim is to get you used to working with real data (that has issues) to understand the main characteristics and potential issues. 

We will be using the [`opendatatoronto`](https://sharlagelfand.github.io/opendatatoronto/) R package, which interfaces with the City of Toronto Open Data Portal. 




## A note on packages

If you are running this Rmd on your local machine, you may need to install various packages used (using the `install.packages` function). 

Load in all the packages we need:

```{r, echo = TRUE}
library(opendatatoronto)
library(tidyverse)
library(stringr)
library(visdat)
library(janitor)
library(lubridate)
library(ggrepel)
```


## TTC subway delays 

This package provides an interface to all data available on the [Open Data Portal](https://open.toronto.ca/) provided by the City of Toronto. 

Use the `list_packages` function to look at what's available


```{r, echo = TRUE}
all_data <- list_packages(limit = 500)
all_data
```

Let's download the data on TTC subway delays in 2019. There are multiple files for 2019 so we need to get them all and make them into one big dataframe. 


```{r, echo = TRUE}
res <- list_package_resources("996cfe8d-fb35-40ce-b569-698d51fc683b")
res <- res %>% mutate(year = str_extract(name, "201.?"))
delay_2019_ids <- res %>% filter(year==2019) %>% select(id) %>% pull()

delay_2019 <- c()
for(i in 1:length(delay_2019_ids)) {
  delay_2019 <- bind_rows(delay_2019, get_resource(delay_2019_ids[i]))
}

# make the column names nicer to work with
delay_2019 <- clean_names(delay_2019)
```

Let's also download the delay code and readme, as reference. 

```{r, echo = TRUE}
delay_codes <- get_resource("fece136b-224a-412a-b191-8d31eb00491e")
delay_data_codebook <- get_resource("54247e39-5a7d-40db-a137-82b2a9ab0708")
```

This dataset has a bunch of interesting variables. You can refer to the readme for descriptions. Our outcome of interest is `min_delay`, which give the delay in mins. 

```{r, echo = TRUE}
head(delay_2019)
```

## EDA and data viz

The following section highlights some tools that might be useful for you when you are getting used to a new dataset. There's no one way of exploration, but it's important to always keep in mind: 

- what should your variables look like (type, values, distribution, etc)
- what would be surprising (outliers etc)
- what is your end goal (here, it might be understanding factors associated with delays, e.g. stations, time of year, time of day, etc)

In any data analysis project, if it turns out you have data issues, surprising values, missing data etc, it's important you **document** anything you found and the subsequent steps or **assumptions** you made before moving onto your data analysis / modeling. 

As always: 

1) Start with an end in mind.
2) Be as lazy as possible.


## Data checks

### Sanity Checks

We need to check variables should be what they say they are. If they aren't, the natural next question is to what to do with issues (recode? remove?)

E.g. check days of week 

```{r, echo = TRUE}
unique(delay_2019$day)
```

Check lines: oh no. some issues here. Some have obvious recodes, others, not so much. 

```{r, echo = TRUE}
unique(delay_2019$line)
```


What are the different values of `bound` for each `line`?

For simplicity, just keep the correct line labels.

```{r, echo = TRUE}
# delay_2019 %>%
#   filter(line %in% c("BD", "YU", "SHP", "SRT")) %>%
#   mutate(bound = as.factor(bound)) %>%
#   group_by(line) %>%
#   skim(bound)
```


### Missing values

Look to see how many NAs by variable

```{r, echo = TRUE}
delay_2019 %>% 
  summarise_all(.funs = funs(sum(is.na(.))/nrow(delay_2019)*100))
```

The `visdat` package is also useful here, particularly to see how missing values are distributed. 

```{r, echo = TRUE}
vis_dat(delay_2019)
vis_miss(delay_2019)
```

### Duplicates?

The `get_dupes` function from the `janitor` package is useful for this. 

```{r, echo = TRUE}
get_dupes(delay_2019)
```

There are quite a few duplicates. Remove for now:

```{r, echo = TRUE}
delay_2019 <- delay_2019 %>% distinct()
```



### Visualizing distributions

Histograms, barplots, and density plots are your friends here. 

Let's look at the outcome of interest: `min_delay`. First of all just a histogram of all the data:

```{r, echo = TRUE}
## Removing the observations that have non-standardized lines

delay_2019 <- delay_2019 %>% filter(line %in% c("BD", "YU", "SHP", "SRT"))

ggplot(data = delay_2019) + 
  geom_histogram(aes(x = min_delay))
```

To improve readability, could plot on logged scale:

```{r, echo = TRUE}
ggplot(data = delay_2019) + 
  geom_histogram(aes(x = min_delay)) + scale_x_log10()
```


Our initial EDA hinted at an outlying delay time, let's take a look at the largest delays below. Join the `delay_codes` dataset to see what the delay is. (Have to do some mangling as SRT has different codes).

```{r, echo = TRUE}
delay_2019 <- delay_2019 %>% 
  left_join(delay_codes %>% rename(code = `SUB RMENU CODE`, code_desc = `CODE DESCRIPTION...3`) %>% select(code, code_desc)) 


delay_2019 <- delay_2019 %>%
  mutate(code_srt = ifelse(line=="SRT", code, "NA")) %>% 
  left_join(delay_codes %>% rename(code_srt = `SRT RMENU CODE`, code_desc_srt = `CODE DESCRIPTION...7`) %>% select(code_srt, code_desc_srt))  %>% 
  mutate(code = ifelse(code_srt=="NA", code, code_srt),
         code_desc = ifelse(is.na(code_desc_srt), code_desc, code_desc_srt)) %>% 
  select(-code_srt, -code_desc_srt)
```


The 455 min delay due to 'Rail Related Problem' is an outlier. 

```{r, echo = TRUE}
delay_2019 %>% 
  left_join(delay_codes %>% rename(code = `SUB RMENU CODE`, code_desc = `CODE DESCRIPTION...3`) %>% select(code, code_desc)) %>% 
  arrange(-min_delay) %>% 
  select(date, time, station, line, min_delay, code, code_desc)
```

#### Grouping and small multiples

A quick and powerful visualization technique is to group the data by a variable of interest, e.g. `line`

```{r, echo = TRUE}
ggplot(data = delay_2019) + 
  geom_histogram(aes(x = min_delay, y = ..density.., fill = line), position = 'dodge', bins = 10) + scale_x_log10()
```

I switched to density above to look at the the distributions more comparably, but we should also be aware of differences in frequency, in particular, SHP and SRT have much smaller counts:

```{r, echo = TRUE}
ggplot(data = delay_2019) + 
  geom_histogram(aes(x = min_delay, fill = line), position = 'dodge', bins = 10) + scale_x_log10()
```



If you want to group by more than one variable, facets are good:

```{r, echo = TRUE}
ggplot(data = delay_2019) + 
  geom_density(aes(x = min_delay, color = day), bw = .08) + 
  scale_x_log10() + facet_grid(~line)
```

Side note: the station names are a mess. Try and clean up the station names a bit by taking just the first word (or, the first two if it starts with "ST"):

```{r, echo = TRUE}
delay_2019 <- delay_2019 %>% 
  mutate(station_clean = ifelse(str_starts(station, "ST"), word(station, 1,2), word(station, 1)))
```

Plot top five stations by mean delay:

```{r, echo = TRUE}
delay_2019 %>% 
  group_by(line, station_clean) %>% 
  summarise(mean_delay = mean(min_delay), n_obs = n()) %>% 
  filter(n_obs>1) %>% 
  arrange(line, -mean_delay) %>% 
  slice(1:5) %>% 
  ggplot(aes(station_clean, mean_delay)) + geom_col() + coord_flip() + facet_wrap(~line, scales = "free_y")
```

### Visualizing time series

Daily plot is messy (you can check for yourself). Let's look by week to see if there's any seasonality. The `lubridate` package has lots of helpful functions that deal with date variables. First, mean delay (of those that were delayed more than 0 mins):


```{r, echo = TRUE}
delay_2019 %>% 
  filter(min_delay>0) %>% 
  mutate(week = week(date)) %>% 
  group_by(week, line) %>% 
  summarise(mean_delay = mean(min_delay)) %>% 
  ggplot(aes(week, mean_delay, color = line)) + geom_point() + geom_smooth() + facet_grid(~line)
```

What about proportion of delays that were greater than 10 mins?

```{r, echo = TRUE}
delay_2019 %>% 
  mutate(week = week(date)) %>% 
  group_by(week, line) %>% 
  summarise(prop_delay = sum(min_delay>10)/n()) %>% 
  ggplot(aes(week, prop_delay, color = line)) + geom_point() + geom_smooth() + facet_grid(~line)
```

### Visualizing relationships

Note that **scatter plots** are a good precursor to modeling, to visualize relationships between continuous variables. Nothing obvious to plot here, but easy to do with `geom_point`.

Look at top five reasons for delay by station. Do they differ? Think about how this could be modeled. 

```{r, echo = TRUE}
delay_2019 %>%
  group_by(line, code_desc) %>%
  summarise(mean_delay = mean(min_delay)) %>%
  arrange(-mean_delay) %>%
  slice(1:5) %>%
  ggplot(aes(x = code_desc,
             y = mean_delay)) +
  geom_col() + 
  facet_wrap(vars(line), 
             scales = "free_y",
             nrow = 4) +
  coord_flip()
```

### PCA
Principal components analysis is a really powerful exploratory tool. It allows you to pick up potential clusters and/or outliers that can help to inform model building. 

Let's do a quick (and imperfect) example looking at types of delays by station. 

The delay categories are a bit of a mess, and there's hundreds of them. As a simple start, let's just take the first word: 

```{r, echo = TRUE}
delay_2019 <- delay_2019 %>% 
  mutate(code_red = case_when(
    str_starts(code_desc, "No") ~ word(code_desc, 1, 2),
    str_starts(code_desc, "Operator") ~ word(code_desc, 1,2),
    TRUE ~ word(code_desc,1))
         )
```

Let's also just restrict the analysis to causes that happen at least 50 times over 2019. To do the PCA, the dataframe also needs to be switched to wide format:

```{r, echo = TRUE}

dwide <- delay_2019 %>% 
  group_by(line, station_clean) %>% 
  mutate(n_obs = n()) %>% 
  filter(n_obs>1) %>% 
  group_by(code_red) %>% 
  mutate(tot_delay = n()) %>% 
  arrange(tot_delay) %>% 
  filter(tot_delay>50) %>% 
  group_by(line, station_clean, code_red) %>% 
  summarise(n_delay = n()) %>% 
  pivot_wider(names_from = code_red, values_from = n_delay) %>% 
  mutate_all(.funs = funs(ifelse(is.na(.), 0, .)))

```

Do the PCA:

```{r, echo = TRUE}
delay_pca <- prcomp(dwide[,3:ncol(dwide)])

df_out <- as_tibble(delay_pca$x)
df_out <- bind_cols(dwide %>% select(line, station_clean), df_out)
head(df_out)
```


Plot the first two PCs, and label some outlying stations:

```{r, echo = TRUE}
ggplot(df_out,aes(x=PC1,y=PC2,color=line )) + geom_point() + geom_text_repel(data = df_out %>% filter(PC2>100|PC1<100*-1), aes(label = station_clean))
```

Plot the factor loadings. Some evidence of public v operator?

```{r, echo = TRUE}
df_out_r <- as_tibble(delay_pca$rotation)
df_out_r$feature <- colnames(dwide[,3:ncol(dwide)])

df_out_r

ggplot(df_out_r,aes(x=PC1,y=PC2,label=feature )) + geom_text_repel()
```



## Exercises

1. Using the `opendatatoronto` package, download the data on mayoral campaign contributions for 2014. (note: the 2014 file you will get from `get_resource`, so just keep the sheet that relates to the Mayor election). 
2. Clean up the data format (fixing the parsing issue and standardizing the column names using `janitor`)
3. Summarize the variables in the dataset. Are there missing values, and if so, should we be worried about them? Is every variable in the format it should be? If not, create new variable(s) that are in the right format.
4. Visually explore the distribution of values of the contributions. What contributions are notable outliers? Do they share a similar characteristic(s)? It may be useful to plot the distribution of contributions without these outliers to get a better sense of the majority of the data. 
5. List the top five candidates in each of these categories:
    + total contributions
    + mean contribution
    + number of contributions
6. Repeat 5 but without contributions from the candidates themselves.
7. How many contributors gave money to more than one candidate? 










## Case study - Opinions about a casino in Toronto

**This was written by Michael Chong.**

### Data preparation

#### Getting data from `opendatatoronto`

Here we use the `opendatatoronto` package again. See the previous example RMarkdown file for a deeper explanation of how the code below works.

The dataset I'm extracting below are the results from a survey in 2012 regarding the establishment of a casino in Toronto. More info available by following [this link](https://open.toronto.ca/dataset/casino-survey-results/). In this analysis, we'll be hoping to address the question, **which demographic (age/gender) groups are more likely to be supportive of a new casino in Toronto?**

```{r, echo = TRUE}
# Get the data
casino_resource <- search_packages("casino survey")%>%
  list_package_resources() %>%
  filter(name == "toronto-casino-survey-results") %>%
  get_resource()
```

#### Getting the right kind of object

The object `casino_resource` isn't quite useable yet, because it's (inconveniently) stored as a `list` of 2 data frames:

```{r, echo = TRUE}
# Check what kind of object the casino_resource object is
class(casino_resource)
```

If we just return the object, we can see that the 2nd list item is empty, and we just want to keep the first one:

```{r, echo = TRUE}
casino_resource
```

So, let's only keep the first item by indexing the list with double square brackets:

```{r, echo = TRUE}
casino_data <- casino_resource[[1]]
```

#### Cleaning up the dataframe

Let's check out what the first couple rows of the dataframe looks like. By default, `head()` returns the first 6 rows:

```{r, echo = TRUE}
head(casino_data) 
```

Unfortunately the column names aren't very informative. For simplicity, we'll use the `.pdf` questionnaire that accompanies this dataset from the Toronto Open Data website. Alternatively, we could get and parse the `readme` through the R package.

[Here's a link to the questionnaire](https://ckan0.cf.opendata.inter.prod-toronto.ca/dataset/427ca4cd-168a-4a37-883d-4a574277caf5/resource/ae135d6a-6921-4905-bc79-516fcd428b7b/download/toronto-casino-survey-feedback-form.pdf). 

Question 1 indicates the level of support for a casino in Toronto. We'll use this as the response variable.

Concerning potential predictor variables, most of the questions ask respondents about their opinions on different aspects of a potential casino development, which aren't particularly useful towards our cause. The only demographic variables are `Age` and `Gender`, so let's choose these.

Here I'm also going to rename the columns so that my resulting data frame has columns `opinion`, `age`, and `gender`.

```{r, echo = TRUE}
# Narrow down the dataframe to our variables of interest
casino_data <- casino_data %>%
  select(Q1_A, Age, Gender) %>%
  rename(opinion = Q1_A, age = Age, gender = Gender)

# Look at first couple rows:
head(casino_data)
```

### Some visual exploration (and more cleanup, of course)

Let's first do some quick exploration to get a feel for what's going on in the data. We'll first calculate proportions of casino support for each age-gender combination:

```{r, echo = TRUE}
# Calculate proportions
casino_summary <- casino_data %>%
  group_by(age, gender, opinion) %>%
  summarise(n = n()) %>% # Count the number in each group and response
  group_by(age, gender) %>%
  mutate(prop = n/sum(n)) # Calculate proportions within each group
```

Some notes:
* we use `geom_col()` to make a bar chart,
* `facet_grid()` modifies the plot so that the plot has panels that correspond only to certain values of discrete variables (in this case, we will "facet" by `age` and `gender`). This is helpful in this case because we are interested in how the distribution of `opinion`s changes by `age` and `gender`.

```{r, fig.height = 8}
ggplot(casino_summary) +
  geom_col(aes(x = opinion, y = prop)) + # Specify a histogram of opinion responses
  facet_grid(age~gender) + #Facet by age and gender
  theme(axis.text.x = element_text(angle = 90)) # Rotate the x-axis labels to be readable
```

Some things to note:

* the x-axis labels are out of order in the sense that they are not in a monotone order of increasing/decreasing support
* there are `NA` values in `opinion`, `age`, and `gender`, as well as "Prefer not to disclose" responses

#### Getting the data into a more model-suitable format

##### Get rid of responses that aren't suitable

For simplicity we'll assume that `NA` values and "Prefer not to disclose" responses occur randomly, and remove them from our dataset (note in reality this assumption might not hold up and we might want to be more careful). Let's check how many rows are in the original dataset:

```{r, echo = TRUE}
# nrow() returns the number of rows in a dataframe:
nrow(casino_data)
```

Now let's `filter()` accordingly to omit the responses we don't want. In case you're unfamiliar, I'm going to make use of:

* `is.na()`, which returns `TRUE` if the argument is `NA`,
* the `!` operator, which flips `TRUE` and `FALSE`. So for instance, `!is.na(x)` will return `TRUE` if `x` is NOT `NA`, which is what we want to keep. 

```{r, echo = TRUE}
casino_data <- casino_data %>%
  # Only keep rows with non-NA:
  filter(!is.na(opinion), !is.na(age), !is.na(gender)) %>%
  # Only keep rows where age and gender are disclosed:
  filter(age != "Prefer not to disclose", gender != "Prefer not to disclose")
```

Let's check how many rows of data we're left with:

```{r, echo = TRUE}
nrow(casino_data)
```


##### Convert response variable into binary

To clean up the first problem (response variables out of order), we might as well take this opportunity to convert these into a format suitable for our model. In a logistic regression, we would like our response variable to be binary, but in this case we have 5 possible categories ranging from "Strongly Opposed" to "Strongly in Favour". We'll recategorize them into a new `supportive_or_not` variable as follows. 

* `supportive = 1` if "Strongly in Favour" or "Somewhat in Favour"
* `supportive = 0` if "Neutral or Mixed Feelings", "Somewhat Opposed", or "Strongly Opposed"

We do this with the `mutate()` function, which creates new columns (possibly as functions of existing columns), and `case_when()`, which provides a way to assign values conditional on if-statements. The syntax here is a little strange. On the LHS of the `~` is the "if" condition, and the RHS of the tilde is the value to return. For example, `x == 0 ~ 3` would return 3 when `x` is 0.

Another commonly used operator here is the `%in%` operator, which checks whether something is an element of a vector. E.g.:

* `1 %in% c(1, 3, 4)` returns `TRUE`
* `2 %in% c(1, 3, 4)` returns `FALSE`

```{r, echo = TRUE}
# Store possible opinions in vectors
yes_opinions <- c("Strongly in Favour", "Somewhat in Favour")
no_opinions <- c("Neutral or Mixed Feelings", "Somewhat Opposed", "Strongly Opposed")

# Create `supportive` column:
casino_data <- casino_data %>%
  mutate(supportive = case_when(
    opinion %in% yes_opinions ~ TRUE, # Assign TRUE
    opinion %in% no_opinions ~ FALSE  # Assign FALSE
  ))
```

##### Convert age to a numeric variable

Age in this survey is given in age groups. Let's instead treat it map it to a numeric variable so that we can more easily talk about trends with age. We'll map the youngest age to `1`, and so on:

```{r, echo = TRUE}
casino_data <- casino_data %>%
  mutate(age_group = case_when(
    age == "Under 15" ~ 0,
    age == "15-24" ~ 1,
    age == "25-34" ~ 2,
    age == "35-44" ~ 3,
    age == "45-54" ~ 4,
    age == "55-64" ~ 5,
    age == "65 or older" ~ 6
  ))
```

Now let's make the same plot again, with our new processed data:

```{r, fig.height = 9}
casino_summary2 <- casino_data %>%
  group_by(age_group, gender, supportive) %>%
  summarise(n = n()) %>% # Count the number in each group and response
  group_by(age_group, gender) %>%
  mutate(prop = n/sum(n)) # Calculate proportions within each group

ggplot(casino_summary2) +
  facet_grid(age_group ~ gender) +
  geom_col(aes(x = supportive, y = prop)) 
```

We can sort of see some difference in the distribution between different panels. To formalize this, we can run a logistic regression.

### Logistic Regression

Now, we're set up to feed it to the regression. We can do this with `glm()`, which allows us to fit generalized linear models.

We use `family = "binomial"` to specify a logistic regression, and our formula is `supportive ~ age_group + gender`, which indicates that `supportive` is the (binary) response variable since it's on the LHS, and `age_group` and `gender` are our predictor variables.

```{r, echo = TRUE}
casino_glm <- glm(supportive ~ age_group + gender, data = casino_data, family = "binomial")
```

We can take a look at the results of running the GLM using `summary()`:

```{r, echo = TRUE}
summary(casino_glm)
```


#### Interpretation

Interpretation can be a little tricky. Here are some important things to note about our results:

##### Numeric age group variable

Remember that we coded `age_group` as numbers 1 to 5. Because we've used age groups instead of age, we have to be careful with how we phrase our conclusion. The coefficient estimate corresponds to the effect of moving up a unit on the age group scale (e.g. from the 25-34 age group to the 35-44 age group), rather than 1 year in age (e.g. from age 28 to 29).

##### log-odds ratios

The effect estimates are on the log-odds scale. This means the effect of -0.07983 for `age_group` is interpreted as: *for each unit increase in `age_group`, we estimate a 0.07983 decrease in the log-odds of being supportive of a casino*.  

We could exponentiate the coefficient estimate to make this at least a little easier to interpret. The number we get is interpreted as a factor for the odds.

```{r, echo = TRUE}
exp(-0.07983)
```

So our (cleaner) interpretation is:
*the odds of an individuals of the same gender being pro-casino are predicted to change by a factor of `r exp(-0.07983)` for each unit increase in `age_group`*

#### Baseline category

First, note that because we have categorical variables, the `gender` coefficients are *relative* to a "baseline" category. The value of `gender`  that doesn't appear in the table, `Female`, is implicitly used as our baseline gender category. 

Technical note: if the variable is stored as a `character` class, then `glm()` will choose the alphabetically first value to use as the baseline.

```{r, echo = TRUE}
exp(0.70036)
```

So, the interpretation of the `genderMale` coefficient is: *the odds of a male individual supporting a casino is `r exp(0.70036)` times higher than a female individual of the same `age_group`.*

#### Making estimates

##### A manual way

Using the formula found in ISLR 4.3.3, we can make estimates for an individual of certain characteristics. Suppose we wanted to predict the the probability of supporting a Toronto casino for an individual who was 36 and identified as transgender. Then:

* `age_group` takes a value of 3, since they are in the age group of 35-44 coded as 3,
* `genderTransgendered` takes a value of 1

First, let's extract the coefficient estimates as a vector using `coefficients()`:

```{r, echo = TRUE}
coefs <- coefficients(casino_glm)
coefs
```

Since this vector is labelelled, we can index it using square brackets and names. For instance:

```{r, echo = TRUE}
coefs["age_group"]
```

So first let's evaluate the exponent term $e^{\beta_0 + \cdots + \beta_p X_p}$:

```{r, echo = TRUE}
exp_term <- exp(coefs["(Intercept)"] + coefs["age_group"]*3 + coefs["genderTransgendered"]*1)
```

Now evaluate the expression that gives the probability of casino support:

```{r, echo = TRUE}
# The unname() command just takes off the label that it "inherited" from the coefs vector.
# (don't worry about it, doesn't affect any functionality)
unname(exp_term / (1 + exp_term))
```

##### A more streamlined way

Thankfully R comes with a convenient function to make prediction estimates from a `glm()`. We do this using the `predict()` function. First, we need to make a dataframe that has the relevant variables and values that we're interested in predicting. We'll use the same values as before:

```{r, echo = TRUE}
prediction_df <- data.frame(age_group = 3, gender = "Transgendered")
```

The dataframe looks like this:

```{r, echo = TRUE}
prediction_df
```


Then we feed it into the `predict()` function, along with our `glm` object. To get the probability, we need to specify `type = "response"`.

```{r, echo = TRUE}
predict(casino_glm, newdata = prediction_df, type = "response")
```

This matches the probability we got from doing this manually, yay!











## Case study - Historical Canadian elections


<!-- https://twitter.com/semrasevi/status/1122889166008745985?s=21  -->




<!-- ```{r, include = TRUE, eval = FALSE} -->
<!-- library(tidyverse) -->

<!-- elections_data <- read_csv("inputs/federal_candidates-1.csv") -->


<!-- elections_data$Province %>% table()  -->

<!-- # There are inconsistencies in the province names -->
<!-- elections_data$Province[elections_data$Province == "Québec"] <- "Quebec" -->


<!-- elections_data <- elections_data %>%  -->
<!--   filter(!is.na(Province)) -->

<!-- # Check gender -->
<!-- elections_data$Gender %>% table() -->

<!-- # Check occupation -->
<!-- elections_data$occupation %>% table() -->
<!-- # Get a count of how many uniques there are -->
<!-- elections_data$occupation %>% unique() %>% length() -->

<!-- # Check party -->
<!-- elections_data$party_short %>% table() -->
<!-- elections_data$party_short %>% unique() %>% length() -->

<!-- # Check incumbency -->
<!-- elections_data$incumbent_candidate %>% table() -->
<!-- elections_data$party_short %>% unique() %>% length() -->


<!-- # Add year -->
<!-- elections_data <-  -->
<!--   elections_data %>%  -->
<!--   mutate(year = year(edate)) -->

<!-- # Add a counter for year -->
<!-- elections_data <-  -->
<!--   elections_data %>%  -->
<!--   mutate(counter = year - 1867) -->



<!-- #### Save #### -->
<!-- write_csv(elections_data, "outputs/elections.csv") -->
<!-- ``` -->



<!-- ```{r, include = TRUE, eval = FALSE} -->
<!-- library(lubridate) -->
<!-- library(tidyverse) -->

<!-- elections_data <- read_csv("outputs/elections.csv") -->

<!-- elections_data %>%  -->
<!--   ggplot(aes(x = edate)) + -->
<!--   geom_histogram() -->

<!-- elections_data %>%  -->
<!--   ggplot(aes(x = edate, y = Result, color = Gender)) + -->
<!--   geom_point() + -->
<!--   facet_wrap(vars(Province))  -->


<!-- elections_data %>%  -->
<!--   ggplot(aes(x = Province, fill = Gender)) + -->
<!--   geom_bar(position = "dodge") -->
<!-- # We discovered WW1! -->




<!-- ``` -->

<!-- ```{r, include = TRUE, eval = FALSE} -->

<!-- #### Set up workspace ### -->
<!-- library(broom) -->
<!-- library(tidyverse) -->

<!-- #### Read in data #### -->
<!-- elections_data <- read_csv("outputs/elections.csv") -->

<!-- #### Model #### -->
<!-- # Just gender -->
<!-- model1 <- lm(Result ~ Gender, data = elections_data) -->

<!-- tidy(model1) -->

<!-- # Gender and incumbent -->
<!-- model2 <- lm(Result ~ Gender + incumbent_candidate, data = elections_data) -->

<!-- tidy(model2) -->

<!-- # Gender and incumbent and province -->
<!-- model3 <- lm(Result ~ Gender + incumbent_candidate + Province, data = elections_data) -->

<!-- tidy(model3) -->

<!-- # Gender and incumbent and province and year -->
<!-- model4 <- lm(Result ~ Gender + incumbent_candidate + Province + year, data = elections_data) -->
<!-- model5 <- lm(Result ~ Gender + incumbent_candidate + Province + year*Gender, data = elections_data) -->

<!-- tidy(model4) -->
<!-- tidy(model5) -->

<!-- # CHange the year into a counter that increments one for each year -->
<!-- model6 <- lm(Result ~ Gender + incumbent_candidate + Province + counter + counter*Gender, data = elections_data) -->
<!-- tidy(model6) -->


<!-- ``` -->











